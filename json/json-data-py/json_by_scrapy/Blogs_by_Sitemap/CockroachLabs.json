[
{"website": "CockroachLabs", "title": "Global Financial Data Firm's Database Migration from Oracle to CockroachDB", "author": ["Jessica Edwards", "Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/global-financial-data-firm-migrates-off-oracle-to-cockroachdb/", "abstract": "A global financial data firm migrated off its legacy Oracle infrastructure, creating a single hybrid, geo-partitioned deployment of CockroachDB. Frustrated by its legacy Oracle database architecture, a major American financial data firm wanted to consolidate its legacy databases and migrate to public cloud infrastructure. Through numerous acquisitions and organic growth, the firm had accumulated a multitude of Oracle databases and application platforms spread across the U.S., Canada, Australia, and Europe. They wanted to consolidate this infrastructure into a single platform and migrate many of their applications to the cloud. The financial data firm evaluated multiple options, including Google Spanner, but found that Spanner couldn’t bridge both cloud and on-prem servers. The financial data firm ultimately chose CockroachDB because it offered them the flexibility and high performance they required, in addition to enabling a hybrid cloud deployment. In order to provide high read and write performance, they used geo-partitioned replicas , a capability that allows for the partitioning of data by location . After migrating its identity and access management (IAM) system, the firm plans to migrate more apps from Oracle to CockroachDB. Challenge: Legacy Database Migration A large financial and customer data firm headquartered in the United States was facing the tasks of consolidating its legacy databases and migrating to public cloud infrastructure. The team decided they needed to simplify and modernize their application architecture. They planned to migrate many of their systems to Google Cloud Platform (GCP), while keeping some systems in traditional data centers. Their first priority was to modernize their identity and access management (IAM) system, followed by a user service that tracks application authorization. After that, there were various additional applications they needed to modernize. The team ruled out direct lift-and-shift migration of their legacy database systems since their Oracle databases were inordinately expensive and poorly suited for multi-datacenter cloud deployments. They decided to explore new solutions. Requirements: Provide a hybrid cloud deployment that spans on-prem and Google Cloud The team had several requirements for their replacement database. Principal among these requirements was the ability to provide a single hybrid cloud deployment that could span their nine physical and cloud regions around the globe. They planned to keep some data on-prem and migrate other data to Google Cloud (GCP). However, across this global setting, the team needed to provide low latency reads and writes to their customers . Solution: A Multi-Region, Hybrid Cloud Deployment of CockroachDB *Diagram: An example of a CockroachDB cluster similar to the financial data firm’s. The cluster uses CockroachDB’s geo-partitioned replicas feature, where all replicas for a set of data are constrained to a region, and each replica is pinned to separate datacenter.* CockroachDB’s architecture allows developers to deploy the same software across on-prem servers and cloud regions, creating a single logical cluster that can address all data around the globe and can be accessed from any node. With this characteristic, the financial data firm could create the hybrid cloud deployment they needed. First Application Migrated: Identity Access Management (IAM) Microservice The first application the firm migrated, their identity access management (IAM) microservice. The application’s requirement of high read and write performance, was a compelling fit for CockroachDB’s geo-partitioning capability because the entities being authenticated are globally distributed and strongly tied to specific geographic regions. The team deployed CockroachDB across four GCP regions and two on-prem datacenters. Within each region, there are three different availability zones, each housing one CockroachDB node. They used CockroachDB’s geo-partitioned replicas, allowing the team to pin replicas of data to locations to minimize latency. Geo-Partitioning for low latency and resiliency By default, CockroachDB replicates data and distributes the replicas in a way that maximizes geographic diversity. This replication and distribution allows the database to remain available in the event of an outage. With the geo-partitioned replicas feature, all replicas are constrained to the region closest to the end-user, in order to reduce read and write latency. The replicas are then pinned to different datacenters within that region so the data can survive availability zone failures. This pattern provides high availability without sacrificing performance. (For an indepth explainer, check out this video .) With CockroachDB, the financial data firm has found a solution for its new cloud-first database strategy, consolidating its datacenter footprint and moving services to the cloud. CockroachDB is supporting them in this process, enabling them to scale and migrate their databases across physical data centers and cloud regions, without causing any downtime or requiring application changes. Read the full case study here . Further reading: [demo] Reduce Latency 10x by Geo-Partitioning Your Data [case study] Top U.S. financial software company turns to CockroachDB to improve its application login experience [video] Geo-distributed Data, Residency, Performance and Compliance [docs] Database Migration Overview", "date": "2021-02-08"},
{"website": "CockroachLabs", "title": "Build an App with Pony ORM and CockroachDB", "author": ["Charlotte Dillon", "Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/ponyorm-for-cockroachdb/", "abstract": "What you build and how you build it should not be dictated by your database. The tools and frameworks that you’re familiar with should be compatible with your database of choice. This is why modern database solutions have implemented support for third part tools like Pony ORM, Hibernate, GORM, jOOQ and others. Developers are beginning to expect to be able to bend the database to their will. And they should - because the flexibility leads to better applications. What is Pony Orm? Pony ORM is a free, open-source Python language library that makes database interactions easy. It’s comparable to Django and SQLAlchemy by virtue of being tied to Python, but it’s distinct because of automatic query optimization, how it solves for the N+1 problem, and the online database schema editor . Software developer Gabriel Llie explains why he uses Pony ORM, “Pony is by far the best SQL ORM for fast prototyping, thanks to its Entity-Relationship Diagram Editor and its pythonic style. It’s my first choice when dealing with a SQL database oriented application. The learning curve is very lean and you can start using it really fast.” How Does Pony ORM Work? Developers can query a SQL database in Python syntax, and allow Pony to translate Python queries into SQL and execute them in the most efficient way. But database interactions are only half the battle. Even though Pony ORM simplifies interactions with a database, there are still other database complexities developers have to deal with, like scaling and ensuring the database is always on. Why Build an App with Pony ORM and CockroachDB? With Pony ORM for CockroachDB, you’ll get simplicity on both fronts: easy database interactions by virtue of Pony ORM and easy database operations, compliments of CockroachDB. CockroachDB shards automatically, is naturally resilient, and is low-maintenance. It never breaks because it’s architected to survive any kind of failure (including whole availability zone failures). There is a large user base and an engaged community where developers can ask questions and get quick answers so that they don’t lose momentum with the build. There are examples of real use cases in production on CockroachDB today. With Pony ORM for CockroachDB, you can more efficiently build your apps in Python and gain all the benefits of a distributed SQL database , which include horizontal scale without operational complexity and the luxury of being able to rest easy knowing that your database will survive outages . Getting Started with Pony ORM, Python, and CockroachDB The best place to get started would be to check out how to build a Python app with CockroachDB and PonyORM. Another useful resource that you might be interested in is our free course on how to build Python apps with CockroachDB . If you’re interested in using an ORM with CockroachDB but Pony ORM isn’t your top choice then you can check out all the third party database tools that we support including GORM, Hibernate, jOOQ, Django, SQLAlchemy and more. Once you start building you can join the CockroachDB community slack to ask questions and get answers from other users of CockroachDB as well as CockroachDB engineers.", "date": "2021-02-02"},
{"website": "CockroachLabs", "title": "Improved Backup & Restore Capabilities in Distributed Database", "author": ["Charlotte Dillon", "Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/backup-restore/", "abstract": "CockroachDB is designed to deliver bulletproof resilience. But even the world’s safest boat needs to carry life jackets. The same principle applies to your data, which is why we’ve baked a breadth of Backup and Restore capabilities into CockroachDB. Moving Backup & Restore into the Free, Open Source Version of CockroachDB Since this blog was originally published there has been one massive change to our backup & restore functionality - we moved it into open source. This decision was influenced by the community of CockroachDB users who helped us understand that modern data intensive applications need more support than what we were providing with our single machine disaster recovery. For context, when we made the decision to create two versions of our backup and restore features (one core and one enterprise) there was not the same proliferance of data intensive applications in the open source market that needed to be supported across multiple machines. That is, of course, no longer the case. And we’re grateful to the community for communicating their needs with us. You made this an easy decision. I’m very happy that @CockroachDB has included distributed backup/restore to the OSS version. I’m very passionate about core functionality to run an OSS DB successfully. Fundamental features like backup/restore being paywalled really undermine OSS adoption/contrib. Good job! — Kelly Sommers (@kellabyte) November 10, 2020 Backup Data & Restore Data in a Resilient, Distributed Database The responsibilities of backup and restore functionalities are different in a database that is designed to survive anything (and is named after the “cockroach”). For example, isolated issues like small-scale node failures don’t require action on your part. In a resilient database like CockroachDB backups are for disaster recovery situations where a cluster has lost a majority of its nodes. In CockroachDB the distributed backup and restore features help the users quickly recover from coding accidents, they also add more protection for customer data, and they help applications stay in regulatory compliance with industry specific data regulations (through backup archival). You also may want to take regular backups to get point-in-time snapshots of your database. Backup: CockroachDB’s Backup statement allows you to create full or incremental backups of your cluster’s schema and data that are consistent as of a given timestamp. Restore: The Restore statement restores your clusters schemas and data from a backup stored on services such as AWS S3, Google Cloud Storage, NFS, or HTTP storage (you can restore a full cluster, databases, or tables). CockroachDB Backup & Restore Tutorial In this video tutorial, Senior Product Manager Michael Wang goes through some improvements made to Backup and Restore that are intended to make the process faster, easier and more secure. In this tutorial, Michael gets into the command line to demonstrate some improvements we’ve made, and he takes the time to explain what CockroachDB is doing under the hood to make the developer experience more seamless. Here are some of the highlights from the tutorial: An updated incremental backup UX Introducing full cluster backup/restore Increased speed with time bound iterators Added security with encrypted backups As we mentioned above these Backup & Restore features became open source as of version 20.2. So you can start building on CockroachDB by clicking here to get a free 30-day trial . If you’d like to learn what other CockroachDB open source users are building you can ask them directly in our community slack channel .", "date": "2021-02-11"},
{"website": "CockroachLabs", "title": "Why Deploying on Kubernetes is Like Flying With an Alligator", "author": ["Chris Seto"], "link": "https://www.cockroachlabs.com/blog/kubernetes-scheduler/", "abstract": "As long as you’re willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will “just work”. However, if one is interested in traveling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one’s own plane or database for that matter. Traveling with reptiles aside, scaling a highly available stateful system is no trivial task. Scaling any system has two main components: Adding or removing infrastructure that the system will run on, and Ensuring that the system knows how to handle additional instances of itself being added and removed. Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones). One of Kubernetes' responsibilities is to place “resources” (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: “I must be in availability zone A ”, or “I can’t be placed onto the same node as this other Pod”. As an addition to those constraints, Kubernetes offers Statefulsets that provide identity to Pods as well as persistent storage that “follows” these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod’s name. It’s important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist. Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see Orchestrate CockroachDB in a Single Kubernetes Cluster .\nIn this article, I’ll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones. A three-node CockroachCloud cluster would look something like this: When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet. Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones. To remove resources from a cluster, we perform these operations in reverse order. We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod. Now, remember that pods in a StatefulSet of size n must have ids in the range [0,n) . When scaling down a StatefulSet by m , Kubernetes removes m pods, starting from the highest ordinals and moving towards the lowest, the reverse in which they were added.\nConsider the cluster topology below: As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones. However, Kubernetes' scheduler doesn’t guarantee the placement above as we expected at first. Our combined knowledge of the following is what lead to this misconception. Kubernetes' ability to automatically spread Pods across zone The behavior that a StatefulSet with n replicas, when Pods are being deployed, they are created sequentially, in order from {0..n-1} . Consider the following topology: These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C! Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in. To correct the latter issue, we now employ a “hunt and peck” approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler. A session of brainstorming left us with 3 options: 1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.\nFurthermore, pod topology spread constraints were still a (beta feature in 1.18) which meant that it wasn’t guaranteed to be available in managed clusters even when v1.18 became available.\nThe entire endeavour was concerningly reminiscent of checking caniuse.com when Internet Explorer 8 was still around. 2. Deploy a statefulset per zone . Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.\nOur team had considered this as an option in the past which made it particularly appealing.\nUltimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking. 3. Write a custom Kubernetes scheduler. Thanks to an example from Kelsey Hightower and a blog post from Banzai Cloud , we decided to dive in head first and write our own custom Kubernetes scheduler.\nOnce our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.\nThe output of kubectl get events had led us to believe there was another system at play.\nIn our journey to find the component responsible for storage claim mapping, we discovered the kube-scheduler plugin system. Our next POC was a Filter plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly! Our custom scheduler plugin is open source and runs in all of our CockroachCloud clusters.\nHaving control over how our StatefulSet pods are being scheduled has let us scale out with confidence.\nWe may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.\nBetter still: the plugin’s implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the schedulerName field in our StatefulSet definitions. Further reading: Gotchas Running Distributed Systems on Kubernetes Kubernetes Deployments Explained Run CockroachDB on Kubernetes A version of this post originally appeared on the Kubernetes Blog .", "date": "2021-02-09"},
{"website": "CockroachLabs", "title": "Database Schema Migration Tools: Flyway & Liquibase + CockroachDB", "author": ["Charlotte Dillon", "Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/flyway/", "abstract": "If you use a database, at some point you’ll need to modify your schema design. Maybe you need to store new kinds of information. Perhaps you’ll add a field to a table or even need to change a primary key. Regardless of the reason, application requirements tend to change over time - and you’ll need to migrate. We know how important schema modification is, which is why we’ve built online schema changes and online primary key changes into CockroachDB. The Benefits of Schema Migration Tools: Flyway & Liquibase Over the last year, we’ve realized that many of our customers like using third-party schema migration tools to ease these transitions and to provide structure and control over the process. For this reason, CockroachDB has built official support for FlywayDB and Liquibase - the two most popular schema migration tools. Flyway and Liquibase both deliver version control for your database - which makes schema migrations more simple. Here is a short list of ways that schema migration tools increase developer efficiency: They automatically order scripts for deployment They do branching and merging for teams They embed easily into products or build tools (think Jenkins ) They make it easy to roll back changes They let you reproduce schema changes across multiple environments They provide automation so you can do schema changes automatically without having to remember all the commands With Flyway developers write migration files in SQL or Java. With Liquibase (an open source project) developers can write changes in SQL, XML, YAML and JSON formats. Another benefit of using a tool like Flyway or Liquibase is that it can be used as a single source of truth of schema across multiple environments. This is helpful when you need to deploy the same set of changes to different databases, such as your development, staging, and production instances. Database Schema Migration Tutorials The process of migrating schema is, for some reason, excluded from most computer science program curriculums. As a result a lot of young developers approach schema migrations with fear. The truth is that migrations can be confusing, but the tools are extremely helpful, and after you do one or two you’ll never worry about it again. Here is a short video tutorial in which Rafi Shamim, engineer on our App Dev team, demonstrates how you would use Flyway to perform a schema migration on a local CockroachDB instance: (Sneaky pro tip from the tutorial: Don’t store passwords in plain text. Instead you should hash and salt them .) Our Docs team has written a comprehensive tutorial on how to use Flyway with CockroachDB , as well as how to use Liquibase with CockroachDB . After you’ve digested either the video tutorial or the written tutorials head over to our community slack channel to ask followup questions relevant to your particular application architecture. CockroachDB engineers are in the slack channel and will respond with recommended solutions. Of note: People use the term “Schema Migration” and “Schema change” interchangeably. If you didn’t already know this, hopefully it clears up some confusion.", "date": "2021-02-10"},
{"website": "CockroachLabs", "title": "UNIwise Delivers a Frictionless Experience for Remote Learners with Kubernetes & CockroachDB", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-helps-universities-power-remote-learning-in-pandemic/", "abstract": "Since the COVID-19 pandemic began, there has been a massive traffic spike in everything digital--from online shopping to remote learning. Under the hood, companies responsible for these technologies have had to quickly adapt to handle the growth in users. Having an elastically scalable and cloud-native stack--all the way down to the database --has helped ease this transition. One such company that experienced a pandemic-driven surge in popularity was UNIwise, an education tech company based in Denmark that sells an online test platform called WISEflow . WISEflow has been popular for years in Scandinavia and Northern Europe. But when universities shut down this spring and pivoted to remote learning, WISEflow's popularity spiked in new markets including France, Germany, and Korea. UNIwise was able to handle the increase in demand because they already had a fully cloud-native, scalable stack. DevOps Engineers Shoulder Scale & Resiliency Demands in the Midst of the COVID-19 Pandemic The responsibility for scaling infrastructure to accommodate increased demand falls squarely on the shoulders of the DevOps engineers at UNIwise. The team follows a fully cloud-native ethos and deploys its services in Kubernetes, running in AWS. When stay-at-home orders prompted the sudden increase in traffic across different countries in Europe and Asia, the DevOps team was able to scale effortlessly to meet the demand. Legacy databases like Postgres or MySQL can cause issues in cloud-native stacks, because they don’t deliver elastic, horizontal scale. With a cloud-native database like CockroachDB, the data layer scales along with the application. As you add more Kubernetes instances to meet increased demand, CockroachDB scales in tandem. Scale is one challenge during this digital surge, resilience is another: DevOps teams must provide always-on services for their customers. CockroachDB is aptly named on account of the fact that it’s architected to withstand any outage . For DevOps engineers running CockroachDB with Kubernetes, that means that if a Kubernetes instance goes down, the CockroachDB instance in that pod can safely go down too, without impacting the availability of the database or application as a whole. This approach provided UNIwise the confidence that they could continue serving customers, without fear of a service outage. Kubernetes + PostgreSQL Compatibility “We like the comfort of knowing that CockroachDB can scale without any hassle.”\n-Jonas Tranberg, DevOps Engineer at UNIwise UNIwise evaluated several SQL, NoSQL, and document databases. One of their DevOps engineers, Jonas Tranberg, had heard about CockroachDB at KubeCon in Copenhagen back in 2018. When Tranberg and his team revisited CockroachDB, they were pleased with the ​PostgreSQL compatibility a​nd with the ability to scale automatically. And since the DevOps team deploys their infrastructure on Kubernetes, they also appreciated that they could run CockroachDB within StatefulSets pods. “We liked the fact that CockroachDB is built with a distributed high availability architecture in mind from the beginning, which makes it ideal to incorporate into our current Kubernetes cluster,” says Tranberg. UNIwise has built a variety of key applications on it CockroachDB, deploying each on Kubernetes in a single-region AWS cluster. UNIwise uses CockroachDB to store various types of data, including messaging history, text analysis, biometric analysis for their facial recognition technology, and document versions within the lockdown browser. To learn more about how the DevOps engineers at UNIwise scaled to survive a traffic spike and continued to deliver a frictionless experience for universities across Europe and Asia, read the full case study.", "date": "2021-02-19"},
{"website": "CockroachLabs", "title": "Stargazers: A Tool For Analyzing Your GitHub Stars", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/what-can-we-learn-from-our-github-stars/", "abstract": "It’s been over six years since CockroachDB became a GitHub project. In that time, the project has racked up more than 20,000 GitHub stars , which is a simple way for GitHub users to bookmark repositories that interest them. Naturally, we’ve wondered how people find out about our project. Are there things we could do to accelerate awareness and interest? Years ago I dedicated a Flex Friday (our version of 20% time) to stargazers , a tool to query the CockroachDB repository for information about its GitHub stars and analyze the results. At the time of writing, we had 6,000+ stars (which felt like a lot), and the data in this blog will be based on that original set of 6,000 stargazers. Github and the New Era of Open Source Community I’m going to wax rhapsodic about open source for a moment. My first experience with “open source” was typing in BASIC programs on an Apple IIc. I was soaking up the smarts of other software engineers by manually transcribing lines of code published in Byte magazine. Years later, I discovered an open source implementation of Pascal on a BBS. I spent hours trying to puzzle out how it worked using a translation dictionary (the comments were all in German). But wow, not having to type the whole thing in from a printout was a big step forward. At UC Berkeley, the internet was suddenly available for the first time, and I discovered there was a whole universe beyond the blinkered world of Borland and Microsoft. gcc, bash, emacs, X11, Linux…all there for the looking, and for the taking. A true embarrassment of riches! You’d search for code tarballs located in FTP archives using archie, or by searching Usenet. Archaic by today’s standards, but I assure you, another big step forward. Which brings us very roughly to the present, and another evolutionary step. GitHub, by some potent combination of critical mass and ease of use, has added a significant community dimension to open source projects. OSS projects have become living things, growing and evolving through the attention and ministration of many intelligences. If you take a little time to dig around using GitHub’s API, you can start to get some idea of how interconnected they are, and how they inform each other. What drives GitHub stars? The first thing I did was to take a look at the data and try to match up any notable discontinuities in the accumulation of GitHub stars with exogenous events. Turns out that press matters! Our first ever mention was in Hacker News , then Wired , then another pivotal Hacker News story followed by a glut of news when we announced that we were starting a company and had received funding ( VB , Wired , WSJ ). Interestingly, the announcement of FoundationDB being acquired by Apple drove interest as well. Conference talks work well: there was a presentation by Tobias at FOSDEM and a talk I gave at CoreOS Fest . Needle-moving press mentions are difficult to gin up for small startups but you can create new content, this blog being one example, and if it’s interesting enough , it might get a mention on Hacker News and even spawn a discussion. In fact, the positive impact Hacker News can have on your project’s GitHub stars was analyzed in this Reddit post . Contentious topics aren’t something to fear either; they get noticed . What else are our stargazers starring? People seem to really like starring projects. At the time this blog was originally published we had 6,000+ “stargazers” (GitHub’s term for them) that had starred more than 1.2M repos (for this analysis, we stopped counting at 300 stars per user, as some people apparently star many thousands). 227K of the 1.2M are unique, so there’s significant overlap in interests. If you think about it, that overlap represents other repositories which are correlated to CockroachDB in terms of our stargazers’ interests. Again, curious to learn more, we did an analysis counting the overlap and ranking the most commonly co-starred repositories. The top 15: What projects do our stargazers contribute to? But what about our stargazers themselves? What exactly are they up to? To answer that question, we took a look at what repositories our stargazers subscribe to. A subscription often implies contributory activity, and at the very least a high level of interest. Here are the top repositories sorted by the number of subscribed stargazers who are also committers, with total commits, additions, and deletions made by our stargazers: How much do our stargazers contribute to open source projects? Turns out roughly 2,200 of the 6,000 stargazers have made at least one commit to a repository. To make the numbers meaningful, we included only those repositories with at least 25 of their own stargazers, 10 forks, or 10 open issues. All told, our stargazers made 728K commits to 36K repositories meeting those minimum thresholds! The average number of commits was 325 and the median was 64. The top 15 most prolific stargazers: Another thing we can look at is our stargazers’ followers. Turns out that in total, our stargazers are themselves followed by 216K other GitHub users. 112K of the 216K are unique, so again, there’s significant overlap. Perhaps not surprising, but that’s a lot of connectedness. Certainly to the extent we have any luck getting these people to contribute to or use CockroachDB, it’s difficult to imagine a better way to drive developer adoption. Here’s a histogram of follower counts for our stargazers (note that the follower counts are logarithmic, so 1=10, 2=100, 3=1000): Have our stargazers changed over time? Is there an evolution in the attributes of new stargazers as they discover the CockroachDB project? We looked at the average counts for followers and total commits. Turns out earlier interest in the project is correlated with more GitHub involvement, with both number of commits and number of followers. But what if we correct by normalizing by the GitHub age of the stargazer? The suspicion is that earlier stargazers have had more time to gain followers and merge pull requests. However, even when normalized by age, both average followers and commits are still positively correlated with earlier interest in CockroachDB. Interested in looking at another repository? You can use the stargazers app yourself to analyze the composition of starring users on any GitHub repository.  In the case of cockroachdb/cockroach, it required querying 24G of data from the API. At 5,000 API requests per hour, it took a couple of days to run, so not exactly for the faint of heart if you have a repository with a substantial following.", "date": "2021-02-22"},
{"website": "CockroachLabs", "title": "3 Ways to Master Stateful Apps in Kubernetes", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/kubernetes-orchestrate-sql-with-cockroachdb/", "abstract": "Kubernetes adoption has massively accelerated, leading the way to a new, cloud-native approach to building and delivering the software that businesses need to make users happy and employees successful. Slow and heavy lifting has been replaced with interchangeable, self-contained software objects that can be configured by a simple configuration and scaled through automated replication. If an object fails, it is replaced. To deliver new software, objects are replaced while still in motion. But not all applications are built the same. While Kubernetes container orchestration is a natural fit for stateless apps, stateful app management—and its inherent dependencies—presents a special challenge to the orchestration paradigm. This post looks at the solution paths available for organizations using Kubernetes to orchestrate stateful apps, and walks through some of the factors that might help a dev team choose between the three. The challenge of state in Kubernetes The magic of orchestration—being able to quickly swap and update containers on the fly—is a big obstacle to stateful applications and databases. The whole promise of containerization, being able to move and manage quickly and freely, breaks down when an application database is chained to local storage. Why? Because Kubernetes deployments win through replication. This is how DevOps can build, deploy, scale, and fall back with less effort and more confidence. But replication doesn’t work for stateful apps: Database replicas aren’t interchangeable like other containers, they have unique states Databases also require tight coordination with other nodes running the same application to ensure version and other and require careful coordination Solving for stateful: three paths to success The road to stateful Kubernetes has three big intersections, with some other minor navigational options. We’ll review each of the three paths: Running outside Kubernetes, Using cloud services, or Running in native Kubernetes Maximum Choice, Maximum Effort: Run your database outside of Kubernetes The most straight-forward approach is to simply spin up a new VM and run the database outside of the Kubernetes environment. The high cost of comfort here, though, unfortunately, is the additional operations workload you’re incurring. Because even though Kubernetes has a high-quality, automated version of each of the following, you’ll wind up duplicating effort across: process monitoring configuration management in-datacenter load balancing service discovery monitoring and logging The result is maximum choice, but also a full stack of management tools that you’ll have to run outside of Kubernetes. Less control, less effort: running your database via cloud services. You can also leverage cloud services to run your database outside of Kubernetes. This would eliminate the need to manage spinning up, scaling, and managing the database, and eliminates that redundant infrastructure stack through external services. The downside is you’re stuck with the DBaaS as offered by your cloud services provider, which makes even less sense for those running things in house or on prem. And since you don’t have direct access to the infrastructure running the database, fine-tuning performance and managing compliance can be an issue. Native control, minimum effort: running your database inside Kubernetes Kubernetes does have two integrated, native controllers for running the database inside the container, just as deployment works with stateless apps. These maximize integration and automation, retain more workload controls, and eliminate the time, cost, and complexity of maintaining a separate stack of “around the database services” as listed earlier. The StatefulSet controller The first control for stateful apps is the StatefulSet controller. Like a Deployment , a StatefulSet manages pods that are based on an identical container spec but not interchangeable. By assigning each pod a persistent and unique ID (by way of an easy to build Headless Service) both application and database maintain connection regardless of which node they’re assigned to. It also means that as an application is scaled up and down, connections are maintained, and persistence achieved. This makes them ideal for applications that need stable, persistent storage and ordered, automated scaling and updates. This includes distributed controllers like ZooKeeper as well as workloads such as MySQL clusters, Redis, Kafka, MongoDB, and others. To learn more about how StatefulSet supports local storage by way of LocalPersistentVolume, read here . The DaemonSet controller The second native stateful control is the DaemonSet controller . Where StatefulSets used unique IDs to keep application and database connected across nodes, DaemonSets ensure that all (or some) nodes run a copy of a pod. As a node is added, so is the required database pod. As the node is removed, the pod is removed via the garbage collector. As you might guess from the name, the DaemonSet controller is especially useful when running background processes (or daemons), especially around performance monitoring or log collection and/or analysis. By restricting nodes to database support, DaemonSets eliminate the potential performance issues of StatefulSets caused by resource contention and competition. Choosing the Right Controller: DaemonSets vs StatefulSets As we already mentioned, the nature of the workload, and adherence to other Kubernetes best practices, must drive the choice between stateful Kubernetes controllers. Transactional database applications like PostgreSQL are ideal for the more nimble StatefulSet controller, while scheduled background processes are typically a better fit for DaemonSets. A guide to Managing State in Kubernetes If you want to take all your stateful momentum into a deployment of a stateful application you can use this step-by-step guide that demonstrates a couple of different ways to manage state in Kubernetes. CockroachDB’s architecture mirrors Kubernetes architecture which makes CockroachDB an excellent fit for the third path mentioned above, “Natrive control, minimum effort: running your database in Kubernetes”.", "date": "2021-02-23"},
{"website": "CockroachLabs", "title": "Contact Tracing COVID-19 With An Open Source App Built on CockroachDB", "author": ["Cassie McAllister"], "link": "https://www.cockroachlabs.com/blog/contact-tracing-covid-19/", "abstract": "The thought of developing a solution for “epidemic management” was not what Quarano engineers had in mind this time last year. In fact, Quarano ’s story started just a few months ago at a government-sponsored hackathon in Germany. At the hackathon, over 40,000+ participants were asked to build applications and services to help manage and decrease the spread of COVID-19. As we now know, contact tracing is an important tactic of managing a crisis. The Quarano team focused their project on making the COVID-19 documentation process more effective. COVID-19 Management in Germany The German government operates on a Health Authority basis meaning that each region in Germany has its own authority that makes its own rules for tracing the virus. Health Authorities used (and in some cases are still using) pen and paper to record people’s symptoms and who they have been in contact with. Not only is this manual documentation method inefficient and unreliable, it also significantly slows down the process of tracking the virus. After the 48-hour hackathon, the team had a prototype for Quarano which would become an open-source application for epidemic management. The application helps German health departments trace the spread of COVID-19, organize quarantines, and offer health services for citizens. While their initial mission was to help the German government, Quarano can actually be used by anyone, anywhere. From the beginning, the team at Quarano knew that they needed a secure, stable, and modern foundation for their application. These requirements initiated the search for a distributed database that would serve as the foundation of their infrastructure. Safe, scalable application application After designing the prototype, the team knew that they were dealing with the most sensitive kind of data and would have to host the application in an extremely secure environment. Deutsche Telekom, a German telecom, donated the hardware that includes state of the art encryption and resides in Germany. Next, they had to find an encrypted database solution that could be hosted as a cluster in their private cloud. They wanted the application to have the ability to easily scale to new regions should other authorities want to use it. The application is under an open source license (EUPL-12) meaning that it is free of charge. As such, all the technologies that make up Quarano needed to be open source including the infrastructure and database. A summary of Quarano’s requirements include: Built-in encryption Ability to scale Great performance Open source foundation The Quarano team started evaluating distributed databases, testing various performance metrics. Eventually they came across CockroachDB, a cloud-native distributed SQL database. After some initial testing, they were impressed by CockroachDB’s ability to scale and were interested in it’s geo-partitioning capabilities , should their product grow beyond Germany. An application backend built on a reliable infrastructure The Quarano backend is a Spring Boot based application exposing a REST API to track cases of COVID-19 infections. The application backend uses a standard Maven project based on a series of open-source technologies. This system allows patients to log a diary of symptoms which is then monitored by the Health Authorities. (For detailed information, view Quarano’s GitHub page ). The application collects a lot of data about users (age, gender, symptoms, location, etc), and this metadata is managed and stored in CockroachDB. Because CockroachDB has built-in encryption, Quarano did not have to implement a supplemental encryption solution. The database’s data model is managed with Flyway migration scripts to keep different development and customer environments in sync. As mentioned above, Deutsche Telekom donated the hardware that hosts CockroachDB. The multi-region deployment uses 3 nodes (one per region) in Germany and each region has 4 vCPUs. This 3-node deployment ensures that Quarano will will always be on and available, even if they encounter a node failure. If Quarano were to add an additional Health Authority located in a new region, they would simply scale CockroachDB by adding more nodes . When a node is pointed at the cluster, it instantly rebalances the data across all of the nodes to incorporate this new resource. With the addition of a new node, they could also scale out the volume of transactions the single logical database can handle. As they add nodes, they are able to scale both reads and writes which would ensure that Quarano is performant. Now, if the application’s reach expands beyond Germany, that’s when the team at Quarano would leverage CockroachDB’s geo-partitioning features . Geo-partitioning allows Quarano to tie data to a location at the row level. The purpose of this feature is to help Quarano meet latency requirements by keeping data close to its users, and to comply with data regulations like the GDPR that require data to reside in a particular geographic jurisdiction. Distributed technology for contact tracing Just 10 short weeks after the initial Hackathon, Quarano was live. The team has made tremendous progress in developing the application and is planning to sign additional Health Authorities in the future. They are currently working on adding more functionality to the app itself to make it more user friendly for citizens, not just healthcare workers. Since the team at Quarano built their application on a distributed, reliable, modern infrastructure, they will have no problem scaling to accommodate new users in the future. We look forward to watching Quarano achieve its mission of helping Germany (& beyond) slow the spread of COVID-19!", "date": "2021-02-23"},
{"website": "CockroachLabs", "title": "SQL Compatibility in CockroachDB: Spatial Data, Enums, Materialized Views", "author": ["Vy Ton"], "link": "https://www.cockroachlabs.com/blog/sql-updates-in-cockroachdb-spatial-data-enums-materialized-views/", "abstract": "CockroachDB empowers developers to build fast, scalable applications, and one of the ways it does this is by providing rich, Postgres-compatible SQL. And while CockroachDB follows the Postgres wire protocol, the database also has a custom SQL implementation designed for a distributed database. Over the years, we’ve expanded our distributed SQL implementation to include a cost-based optimizer (CBO) and vectorized execution engine - all built to tackle the complexity of distributed data for developers. In CockroachDB 20.2, we’re excited to provide developers with an increasingly rich SQL feature set that includes support for spatial data, materialized views, Enums, ALTER TABLE , and user-defined schema changes. Let’s dive into the new capabilities. More powerful, flexible data models in CockroachDB Applications today need to define expressive data models and be able to continuously evolve them as application requirements change. Spatial data in CockroachDB CockroachDB now supports the ability to model spatial data which includes both geometric, related to a map, data and geographic, related to the Earth’s surface, data. To read more about combining PostGIS compatible SQL with CockroachDB’s scalability, check out spatial data tutorials or take a look at this distributed spatial data demo . User-defined schemas in CockroachDB Developers using Postgres can leverage a hierarchy of database → user-defined schemas → database objects (tables, views, etc&mldr;) to organize their data. Data organization patterns can support multiple users or multiple applications working within the same database cluster isolated from each other. In addition, Postgres has a large third-party tool ecosystem that can leverage user-defined schemas in their native internal implementation. Prior to 20.2, we recommended users create a database wherever they would create a user-defined schema, and third-party tools needed to use workarounds. With 20.2, user-defined schemas are supported in CockroachDB. Developers can use the example below to convert databases with only the default public schema to be user-defined schemas. CockroachDB developers can now implement a data hierarchy similar to what they would use in Postgres. In addition, CockroachDB developers still have the flexibility to execute statements against databases beyond the current connected database with fully qualified names . With the additional level of namespacing provided by user-defined schemas, developers can create more complex and secure data hierarchies. The example below shows that a single database for an application can support user-defined schemas for different users. Support for user-defined schemas also removes a known blocker for developers who are adding third-party tool support for CockroachDB. For example, Prisma - the Javascript data access framework - ran into this very issue. demo@127.0.0.1:53365/defaultdb> show databases;\n  database_name | owner\n----------------+--------\n  defaultdb     | root\n  teamA         | demo\n  movr          | demo\n  postgres      | root\n  system        | node\n(5 rows)\n\ndemo@127.0.0.1:53365/defaultdb> create database my_app;\nCREATE DATABASE\n\ndemo@127.0.0.1:53365/defaultdb> use my_app;\nSET\n\ndemo@127.0.0.1:53365/my_app> ALTER DATABASE teamA CONVERT TO SCHEMA WITH PARENT my_app;\nCONVERT TO SCHEMA\n\ndemo@127.0.0.1:53365/my_app> create role teamA;\nCREATE ROLE\n\ndemo@127.0.0.1:53365/my_app> alter schema teamA owner to teamA;\nALTER SCHEMA\n\ndemo@127.0.0.1:53365/my_app> show schemas;\n     schema_name     | owner\n---------------------+--------\n  crdb_internal      | NULL\n  information_schema | NULL\n  teamA              | teamA\n  pg_catalog         | NULL\n  pg_extension       | NULL\n  public             | admin\n(6 rows) Enumerated types (Enums) Using Enums in a data model provides stronger data integrity and can be combined with Enums available in most programming languages to validate data at both the application and database layer. Stronger data validation allows developers to think about other technical challenges. An Enum defines a list of static values and can be set as a column’s type. Enum columns permit only values in a user-defined set, such as items in a user interface dropdown. teamA@127.0.0.1:53365/my_app> CREATE TYPE movie_genre AS ENUM ('drama', 'comedy', 'horror');\nCREATE TYPE\n\nteamA@127.0.0.1:53365/my_app> CREATE TABLE employees (favorite_movie_genre movie_genre);\nCREATE TABLE\n\nteamA@127.0.0.1:53365/my_app> INSERT INTO employees VALUES ('action');\nERROR: invalid input value for enum movie_genre: \"action\"\nSQLSTATE: 22P02 Online schema changes with ALTER TABLE In 20.2, we’ve expanded CockroachDB’s support for online schema changes. These features allow developers to model the relationships their application needs and change data models over time. When combined with CockroachDB’s online schema changes , developers can perform such changes without application downtime. With ALTER TABLE...ALTER COLUMN...TYPE , developers can now change a column’s type. ALTER TABLE...ADD COLUMN...REFERENCES syntax supports adding foreign keys to newly created tables in the same transaction as shown in the example below. Schema changes are a natural part of the application development lifecycle. The initial schema needs to evolve to support future business needs or to add previously unknown business constraints. demo@127.0.0.1:53365/my_app> BEGIN;\nBEGIN\n\ndemo@127.0.0.1:53365/my_app> CREATE TABLE employees (id INT PRIMARY KEY);\nCREATE TABLE\n\ndemo@127.0.0.1:53365/my_app  OPEN> CREATE TABLE laptops (id INT);\nCREATE TABLE\n\ndemo@127.0.0.1:53365/my_app  OPEN> ALTER TABLE laptops ADD COLUMN employee_id INT REFERENCES employees (id);\nALTER TABLE\n\ndemo@127.0.0.1:53365/my_app  OPEN> COMMIT;\nCOMMIT\n\ndemo@127.0.0.1:53365/my_app> SHOW CREATE laptops;\n  table_name |                                            create_statement\n-------------+---------------------------------------------------------------------------------------------------------\n  laptops    | CREATE TABLE public.laptops (\n             |     id INT8 NULL,\n             |     employee_id INT8 NULL,\n             |     CONSTRAINT fk_employee_id_ref_employees FOREIGN KEY (employee_id) REFERENCES public.employees(id),\n             |     FAMILY \"primary\" (id, rowid, employee_id)\n             | )\n(1 row) Improve application performance CockroachDB performance improved significantly on real-world OLTP workloads as evident by our TPC-C benchmarking results, the industry standard for OLTP benchmarks. In addition, Cockroach Labs continues to invest in servicing queries that read a lot of data, such as complex joins and aggregations found within transactional workloads. Using TPC-H queries as representative analytical queries, we saw a decrease in query latency for 20 out of the 22 queries with query 9 latency improving by 80x. In 20.1, CockroachDB’s vectorized execution engine was only on for a subset of queries. 20.2 will have the vectorized execution engine on by default for many more complex joins and aggregations. The vectorized execution engine allows for better memory accounting that was not possible with the row-oriented execution engine. To read more about how we built a vectorized execution engine, check out this post . We also improved the performance of lookup join and join reordering, two critical database components whose performance affects many queries. While changes to lookup joins benefitted most TPC-H queries, it negatively affected Query 17 increasing the latency. This highlights the tricky path of optimizing execution across many queries, and Cockroach Labs continues to analyze for new improvements. The join-reordering algorithm is a part of CockroachDB’s cost-based optimizer (CBO) , which continues to become more intelligent with every release as our team incorporates field learnings to select the best query execution plan. With vectorized query execution , better lookup joins, and improved join-reordering, CockroachDB has increased its execution efficiency allowing developers to do more with the same database resources. CBO enhancements mean that queries will have better performance without developers having to understand the complexities of query execution. Instead, developers can focus on their application technical challenges. CockroachDB supports materialized views and partial indexes CockroachDB now supports materialized views and partial indexes that developers can employ to improve their application performance. Materialized views allow developers to store query results as a queryable database object. A materialized view acts as a cache of a query’s results, which can be refreshed using REFRESH MATERIALIZED VIEW . With materialized views, developers can efficiently access query results with the tradeoff that materialized view data will be out of date as soon as the underlying query data changes. This data model can support use cases such as a daily report of business activity for which having real-time data is not important. For an application, certain query access patterns can be well-defined and only operate against a subset of data. Partial indexes can help optimize these query patterns with a lower impact on write performance compared to full indexes. Since an index is another copy of data, a partial index means that not every write to a table would incur the overhead of updating a full index and stores less data. Most applications have queries that are critical to their use case or help achieve service-level objectives (SLOs). Once these scenarios have been identified, CockroachDB provides developers with a wide range of strategies including materialized views and partial indexes to help achieve application requirements. Troubleshooting CockroachDB In 20.2, Cockroach Labs increased the ways to introspect the database both from our cockroach sql command line and Admin UI. This includes new ways to introspect database sessions and transactions. Conclusion CockroachDB provides developers with the familiar interface of Postgres-compatible SQL against a database that is designed to scale without additional application complexity. Developers can take advantage of the SQL features above to create and alter complex, performant data models. Improvements to our Postgres-compatible SQL reduces overhead for third-party developer tools to add CockroachDB support. In 20.2, Cockroach Labs was excited to partner with the developer community to add Hibernate and Active Record support. As Cockroach Labs continues to provide developers with more solutions to their problems, we are committed to investing in CockroachDB’s CBO, vectorized execution engine, and other internal mechanisms that improve overall database performance. Check out the power capabilities discussed above by trying out CockroachCloud .", "date": "2021-02-18"},
{"website": "CockroachLabs", "title": "Why Interns Get Equity at Cockroach Labs", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/equity-for-interns/", "abstract": "The benefits of hiring the best engineering interns into full-time positions are well known by top technology recruiters at Facebook, Apple, and Google. CNBC reported that engineering interns at these companies “ earn almost twice what the average American worker makes ,” which often includes an array of mind-blowing perks. This puts pre-IPO companies like Cockroach Labs at a disadvantage. We often can’t provide the extensive compensation and perks that top technology companies are offering. However, we can get creative. Our CEO, Spencer Kimball, and I had a lengthy discussion about how we can attract engineering interns and have them come back to our company when they graduate. To attract more candidates, we give interns the opportunity to have direct exposure to high-impact projects. During the onboarding process, we expose interns to a smaller, manageable part of the system and set a scope for their first project to last about a month. During the first project, they learn not only how to navigate the code base and engineering environment, but gain confidence in the work they are doing. After they complete their first project, we set them up with a project for the duration of their internship that is in line with their interests and the company’s needs. Starting with a project that is smaller in scope allows for an intern to feel they are making impact sooner as they have reached productivity more quickly than they would if they were working on a larger project. In addition, we pair our interns with a mentor and have checkpoints as they progress. These efforts can include 1:1s, 30-day check-ins, mid-intern reviews, etc. To encourage our top engineering interns to want to come back to Cockroach Labs in the future, we have approached the intern-employer in two ways. First, we provide ongoing mentorship. One of our past interns had a desire to work at Google on a large-scale system in his future internship. Our VP of Engineering and past Google employee, Peter Mattis, advised on the benefits of working at Google and what the intern could expect while working at a larger, more structured company. For example, at a company like Google, you get the benefit of what it is like working at a company at scale but your internship project may have less impact on the overall business as it could be a smaller piece of the product area. At Cockroach Labs, we want interns to have all the data points possible to make the best decision for them while they are considering full-time opportunities. If that involves exploring internships at other companies, we want to provide support in any way possible, all while leaving an offer open to return to Cockroach Labs. Second, subject to board approval, we may extend options (pre-IPO shares) to our interns that will vest if they choose to return to us after they graduate. If interns who receive such options return to the company within a specified period of time, they will be recognized for the work that they have done previously by accelerating their option vesting schedule by the time spent at Cockroach Labs during their internship. For example, our standard option vesting schedule is that 25% of the option vests after 12 months of service from an employee’s start date (the “cliff”), and the remaining option vests in equal installments over the following 36 months of continuous service. However, if an intern spent four months with us, on their hire date they would only have eight months until they hit their one-year cliff date and vest 25%. We started extending these options to a number of our interns in 2017, and thus far the response has been very positive. It’s now been nearly four years since we started offering interns equity if they came back to Cockroach Labs full-time. Back then, we were a Series B startup with $53 million in funding. Today, we’re a Series E company with $355m in funding and a $2 billion valuation. Interns who joined us back then will benefit from the lower cost to exercise stock options as a Series B employee even though those options have grown in valuation. Paul Bardea , a former intern who joined us full-time in 2019, shared his perspective on this benefit: “As an intern, I felt like I had the opportunity to work on impactful projects, as would a full-time engineer. Being offered equity for my time as an intern further recognized the value of my work. It was also an exciting and unique way to be involved with an early-stage startup.” Our efforts to attract and retain our engineering interns are different from later stage or larger technology companies. Our approach stresses supporting our engineering interns as they evaluate long-term career decisions, even if that takes them in another direction. However, should they decide to return to Cockroach Labs, we ensure we are recognizing the impact that the intern has already made even before their transition to full-time employment. This approach shows our commitment to our intern’s lifelong career. Humans be humans. ;) Illustration by Jared Oriel", "date": "2021-02-25"},
{"website": "CockroachLabs", "title": "POC@CRL: How Our CREWS for People of Color Builds Community", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/poc-at-crl/", "abstract": "CREWS (Cockroach Employees Who Support) are groups intended to create an inclusive environment for Roachers from underrepresented backgrounds and help employees connect with their peers on a deeper level. Groups host events and develop programs to support and advocate for their members, both internally and externally. While these groups are company-sponsored, they are run by employees for employees. What is POC@CRL? Our CREWS is a group for people of color at Cockroach Labs and their allies. We get together for everything from happy hours to planning month-long celebrations, all in the name of community. What is the POC@CRL mission? Why? The POC@CRL mission is to build a community of people of color and allies at Cockroach Labs that promotes professional and personal development and belonging through peer-led events and initiatives. We arrived at this mission statement because our founding group recognized the importance of creating space for people of color and driving the events and engagements that are important to us. We sought to empower every member of our intersectional group to use the CREWS resources to thrive and celebrate our cultures. Can you give us an example of how POC@CRL supports its mission? POC@CRL was founded just before the coronavirus pandemic made us all home-bound for over a year; this threw a wrench in some of our plans to host speakers and after-work events in person. We did find other ways to connect, though! During Black Women’s History Month last year, we established a book fund to cover each member’s purchase of a book written by a woman of color and discussed our picks during a virtual happy hour. This month, we’re celebrating Black History Month by celebrating Black civil rights leaders, artists, technologists, and more. Aside from events and programming, our Slack channel serves as a place for members to communicate throughout the year and share ideas, news, and spark conversation. Tell us about events/programs you’re planning for POC@CRL Though creating a community within our own company is important, we want to connect with more people of color in the tech world and those who want to get started in tech, too. We’re looking at ways that we can host events which include folks from outside of our work environment to join a wider network. Who can join POC@CRL? Everyone is welcome to join our CREWS; whether you are a person of color or an ally, it’s in the name. Cockroach Employees Who Support our mission make POC@CRL possible. If you see yourself joining our CREWS and helping to keep our community strong, check out our careers page to see if there’s a job that fits your interests!", "date": "2021-02-26"},
{"website": "CockroachLabs", "title": "What Banks Have in Common with Google", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/what-banks-have-in-common-with-google/", "abstract": "Innovative fintech organizations built from the ground up to be agile, data-driven, and cloud-native are driving traditional banks and financial services organizations to adopt cloud-native infrastructure. In order to remain competitive and relevant, banks are modernizing operations, replacing legacy systems like Oracle, IBM DB2, and others. Moreover, to keep pace with fintech organizations, banks are turning to the cloud . Financial services organizations have different infrastructure requirements than most companies undergoing a digital transformation. For one, the stakes are much higher due to the highly-regulated nature of banking and the critical role these organizations play in global economies. They process a huge number of transactions which means their applications are typically distributed and use many different types of data. We have a highly complex transaction environment where we are continually searching for simplicity, scalability, and robustness. –Gabriel Melo, Engineering Manager at Nubank in a case study for Cockroach Labs Another differentiator for banks is that they typically run on a complicated base of legacy systems and mainframes that have been around for 20 plus years. The organization’s mission-critical, high-risk workloads are deeply ingrained in these systems. This was the case for an American financial data firm when it started its journey to modernize its application architecture. The company was frustrated by its legacy Oracle database architecture, which had grown in size and number through acquisitions over the years and spanned data centers across the globe. It’s Time for Banks to Embrace the Cloud Though their requirements may differ, the cloud offers financial services organizations numerous benefits over traditional on-premises models. It empowers organizations to keep up with demand and competition with flexible and scalable solutions for developing, deploying, and maintaining applications. The cloud also provides more accessible data which organizations can use for timely insights to guide decision-making and enable automation. As many experts have said, every company is now an IT company. The financial services industry is no exception . Feature-rich applications and websites from the likes of Facebook and Google have driven customer expectations sky-high. Consumers demand seamless experiences driven by real-time data. For financial services organizations, this means that if an existing customer applies for a loan, they expect an instant quote on rates, or, if a new customer tries to set up an account, they want it created on the spot. Banks need to scale just like today’s biggest IT companies. To do this they must modernize their applications and infrastructure and attract the top talent to stay ahead of the competition. A search of LinkedIn finds that the number of people employed at top financial services organizations with the title of engineer, database administrator (DBA), or developer is: 36,000 at JPMC 26,000 at Wells Fargo 8,800 at Deutsche Bank 8,600 at Goldman Sachs Sticking with legacy systems like Oracle and IBM DB2 comes with inherent risks. These include a lack of staff who can manage these older systems, high costs associated with keeping them going, and the risks that come when migrating off of them, including lost data or data ending up in the wrong hands. The only option now is for banks to move to the cloud to take advantage of the latest technologies that deliver unparalleled performance and enable them to offer more services and meet fast-changing customer demands. Focus on infrastructure Since financial services organizations need to interface and interact with legacy systems that contain critical, deeply integrated backend infrastructure, their digital transformations often look different. Unlike in other industries, many of the mainframes deployed decades ago in financial services are still regarded for the high performance, availability, and security they provide. Banks need to find a balance between incorporating the data and services from these distributed systems and harnessing the improved performance and reduced latency provided by the cloud. This may mean integrating the old with the new or completely writing off old infrastructure to start from scratch. In the case of the American financial data firm, the team ruled out a direct lift-and-shift migration of their legacy Oracle database systems since they were very expensive and were not for multi-cloud deployments. They turned to CockroachDB for a completely new solution to modernize applications that covered their nine physical and cloud regions worldwide. They decided on a hybrid approach, keeping some data on-premise and moving other data to Google Cloud Platform (GCP). Like this data firm, many banks are moving to cloud-native or hybrid cloud application strategies based on containers and microservices. Compared to traditional monolithic financial services infrastructure, these tools and technologies offer the flexibility to scale up to meet future demand by adding new services. This means organizations can plan not just for now but for three to five years in the future. Nubank kicked off its transformation by setting up on-premise CockroachDB deployments in three locations. However, as the company continues to grow and expand into new markets, it left itself the option of moving to the cloud and creating a hybrid deployment. In the future, the company is considering migrating some apps to Amazon Web Services (AWS) to create a hybrid deployment across its on-premise servers and cloud regions. When migrating to the cloud, the main challenge for any organization is to maintain peak application performance. This new breed of distributed applications requires infrastructure that is always-on, resilient, and able to support real-time processing of transactions while ensuring compliance with any local data protection and privacy laws. CockroachDB helps organizations on this path by providing easy-to-use databases that are built to address cloud scalability and performance issues. Financial Services and CockroachDB CockroachDB has helped some of the world’s largest banks accelerate their transition to a cloud-first infrastructure. It provides an easy way for developers to take advantage of high availability and elastic scalability features to deliver data-intensive applications that guarantee consistent transactions and low latency. CockroachDB helped Nubank migrate away from storing data on-premise with an in-memory data store. As the organization was growing quickly it needed a transactional solution for some of its credit card authorization services. CockroachDB provided the availability, scale, resilience, and easy maintenance the digital bank was looking for. The financial data firm adopted CockroachDB as a solution for its cloud-first database strategy which supports the firm as it continues consolidating its datacenter footprint. This migration will enable them to scale and migrate their databases across physical data centers and cloud regions without any downtime. Global banks, financial services organizations, and fintech companies trust CockroachDB with their critical, transaction-heavy workloads. Download How Financial Service Companies Can Successfully Migrate Critical Applications to the Cloud to learn more.", "date": "2021-02-25"},
{"website": "CockroachLabs", "title": "Come Work on CockroachDB in Sydney, Australia!", "author": ["Oliver Tan"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-in-australia/", "abstract": "G’day! I’m Oliver, a Member of Technical Staff here at Cockroach Labs. After spending the better part of 5 years in the United States, I decided to come back home to Sydney, Australia. With my homecoming, I’m happy to announce that Cockroach Labs is hiring people to work with us from Sydney, Australia! If you’re curious about me, my journey, and why I’m at Cockroach, read on. Of course, if the news of an opening sounds good, you can jump straight to the job openings we have in Sydney . My journey before Cockroach Labs Born and raised in Sydney, I graduated with a degree in Computer Science from the University of New South Wales. During my university years, I did an internship at Google in Sydney, Australia, where I worked on tooling for detecting network hardware failures on the NetSoft team. I also worked at Facebook in Menlo Park, California where I built tooling for diagnosing traffic infrastructure issues behind internet.org. After graduating, I went to work at Dropbox in San Francisco, where I was on the filesystem team. The filesystem team was responsible for the abstraction that handled the metadata associated with syncing your files . This data was stored on thousands of MySQL shards managed in-house, altogether containing trillions of rows of data. One of my main projects involved moving the filesystem abstraction away to its own service. This enabled desirable traits, such as centralised rate limiting. It also ensured we had controlled, well behaved SQL queries to the database. This was a long running multi-month project that involved moving calls away from multiple services into one. Of course, there was reasonable effort spent to ensure the new service served traffic at reasonable latency, and with no downtime compared to speaking with the database directly from our monolithic server. The service today serves over a million requests per second with high availability. To ensure we had a well performing service, we needed to ensure the APIs provided by our service could serve traffic with reasonable latencies. To do this, we had to ensure we had performant queries for each API endpoint that we offered. While designing our APIs, we looked at various callsites and found unoptimised queries which could devolve into large table scans - some of which had caused reasonably large downtime. Tracking things down during a downtime was terrible - trying in a mad panic to find the source and then blocking these queries from execution was quite stressful. To ensure our queries were performant, we turned to a few techniques: Pagination of queries - We had some calls which could request and scan millions of rows. By ensuring each SQL query was constrained to read a maximum number of rows on each call, we were able to reduce a lot of load on the database. Adding indexes to support faster lookups - Some of our common queries turned out to not be backed by an index, meaning they scanned excess rows and took longer than they should have. Where it made sense, new indexes were added to improve performance. Performing offline work when able - Some queries required heavy computation on the live path, such as file searching. If search results didn’t have to be totally consistent with live state, we could migrate file search queries to utilise a separate search infrastructure that could process results offline, and store these results in a database that was friendlier to search type queries. That way, we could handle file search queries on the live path in an optimised fashion. Denormalising certain attributes to speed up access times for commonly answered queries - We recognised that certain read queries could be sped up significantly if we did some extra calculations and storage at write time. For example, one query we commonly had to answer was “what is the total size of all the files in your Dropbox”. Doing a `SELECT sum(size) &mldr;` over your entire Dropbox would require reading all the rows on your Dropbox account, a potentially large table scan! But if we added or subtracted from a separate denormalised value on a separate table every time a file is added or deleted (e.g. UPDATE denormalised_table SET size = size + delta &mldr;), this query could be answered quickly from a single row read. Of course, some queries just needed to be fixed to utilise indexing by forcing indexes or rewriting them! Another challenge we faced was having data that didn’t follow invariants we assumed to be true. This is especially painful if data is denormalised incorrectly, hence serving incorrect data. In the denormalisation of size case mentioned above, we had users with their Dropbox size deemed to be a higher value, meaning they couldn’t upload any files as they seemingly exceeded their quota - even when their Dropbox was completely empty! Broken invariants like these led to a broken user experience, culminating in support tickets or customers simply walking away from your product (most people just leave your product without filing a ticket if it is broken!). Unfortunately, there were lots of broken invariants hiding, which we couldn’t find without performing expensive SQL scans and joins across the database! To detect broken invariants, we built a verification system that regularly scanned every row in our database to ensure our data was always consistent. The system reads these rows in batches and from replica databases to minimise availability impact. We found that we had millions of inconsistencies in our data, and the job to resolve these and bring these incorrect invariants down to zero involved much labour. However, the reward of building such a system and ensuring zero inconsistencies was significant, as going forward we were able to make changes and add new features with a greater deal of confidence. Another effort I was part of involved the migration of billions of rows from our global single-shard MySQL database to a different database called Edgestore , an in-house graph-based database built on top of sharded MySQL with its own API. This was necessary to support the rising user base and to avoid the dreaded single point of failure. However, it was not straightforward - but more on that later! As you can see, a few projects - mostly working on top of large scale database systems. Did it have any bearing on why I joined Cockroach? Well&mldr; How did you like working with large scale database systems? My experiences at Dropbox were mostly spent working with in-house database systems that continued to work at scale with high availability. Having been involved in the projects above, there were lots of technical and operational challenges involved: Seemingly simple tasks such as creating a new index or adding a new column were tricky and surprisingly lengthy, especially if we wanted no observable customer impact (availability hits, performance changes or incorrect results) when performing these operations. These operations needed to be orchestrated in a complex order. In our setup, each shard contained a leader with two followers serving as backup. To action any schema change, we needed to apply the change on each follower, before promoting a follower to the leader. As each promotion resulted in a small availability hit, these promotions needed to be done one at a time. Since shards complete the schema change at different times, performance or correctness between these periods may be affected as shards may have indexes or columns that others don’t. As a result, this was a slow, heavily babysat shard-by-shard process. Schema changes ended up taking several weeks and multiple engineers to pull off. Having data sharded into separate MySQL databases meant we had to give up some powerful features built into the database to enforce invariants. For example, we couldn’t have foreign keys as MySQL was not able to enforce validation across different shards. This means we missed out on a cheap way to avoid the inconsistencies we found beforehand. We were only able to find and enforce these invariants via a large effort on building a verification system for every relation. Another example is giving up “ON DROP CASCADE”, which would have made tasks such as deleting all data related to a user much simpler. Instead, we had to invest in systems to vacuum and purge user-related entries. There was tremendous difficulty in committing to multiple shards without transactionality. Before the advent of cross-shard transactions on Edgestore (which came after some migrations were completed), many teams found that working in a world where commits could be made on one shard but not on another were tricky. This could have gnarly effects for the end user, in which some data may appear to be missing for short periods of time. Engineers needed to write large scale consistency scripts and auto-fixers to correct any inconsistencies in this new model, making for more complex code to manage this state across the stack. Introducing a new database API also had huge costs. In the case of Edgestore, teams needed to migrate from SQL to a custom API with a restricted KV-style interface - variations of Get, Set and List. This required all database statements to be completely re-learnt and re-written (which got even more tiring if there were complex joins unsupported by the new API). Combined with changes in transactional guarantees, these migrations meant lots of work and long periods of validation, with each relation taking at least several weeks to migrate. In the case of Edgestore, the migration efforts were a multi-year, full organisation effort. Work was needed to create a database system from scratch and keep it up and running under scale, compounded by needing every relation to be migrated. Maintaining our in-house database systems was also an operational burden, requiring dedicated engineers to perfect these in-house systems as they matured. Though hard and in some cases arduous, I found the database work I was doing enjoyable as it was the kind of large-scale infrastructure problems I was interested in solving. I felt fortunate to have worked with many talented engineers at Dropbox who managed to pull off such complex technical projects to keep us operational at scale. What made you decide to join Cockroach? Almost every company needs a database to house their data. Successful companies need databases that grow with their success. But not every company can afford to spend the amount of resources that companies such as Dropbox, Facebook or Google do on databases so they can survive massive growth. When I saw that CockroachDB had the power to abstract these problems away while still using the PostgreSQL syntax developers know and love, I was immediately sold. For me, it was the database that grew with you. If I could contribute to a product that took these pain points away, I would be helping others focus more on their mission of shipping their own product instead of worrying and reasoning about complex database-related issues at scale. In that sense, I felt I would be a part of every product that would be shipped and powered by CockroachDB. I applied straight away. Walking into the interviews, I already thought the product sold itself. I was even more impressed when I talked to everyone working at Cockroach. I was excited by the upcoming projects, the vision of the company and the people I talked to. Not long after that, I signed! How do you like working at Cockroach Labs? Working on CockroachDB has been an incredible experience. It’s been just over a year and I felt like I’ve been involved in so much - some highlights include dealing with the mindblow-yness of time , adding spatial features and indexing and simplifying our multi-region user experience in the upcoming release. While there are many aspects that I enjoy at Cockroach, here are a few big ones (in no particular order) that keep me going: The Product. As I mentioned before, it makes me proud to be working on such a powerful product that lets others innovate instead of spending time on their metadata storage solution. The Tech. Databases are so massively cross-field and interesting. Sometimes you think you’ve seen it all - but I’ve been proven wrong time and again as I’ve seen some truly pioneering stuff whilst working here. Being Open Source. In particular, the community has made some outstanding contributions and it’s scary to think how much further behind we’d be without them. Though I am worried that my wife likes to check up on me on Github! The People. There is some amazing talent here at Cockroach and that knowledge is shared around well. Everyone has been inviting and open with their time - it’s been easy to jump on quick calls to debug or solve issues with one another. Furthermore, the people team have been great at making sure we are still all connected and thriving as an intrusion with social events and initiatives to keep us feeling together, even as we work remotely. Flex Fridays. Well, Monday, in the case of Australia. But having that day with no meetings and no obligations, with time to focus on other endeavours, has really been refreshing on my week to week. What brings you back to Sydney? When the virus-that-must-not-be-named came along, my wife and I decided that it was time to move home. The US was a great adventure and scratched our traveller itch, but coming back home to the familiar suburbia of Sydney (and the wonderful fresh sea breezes) was always our long term plan. Of course, coming back was complicated, as I still wanted to be involved with Cockroach Labs but there was no Aussie presence yet. Fortunately, the team at Cockroach Labs was interested in the talent that could be tapped in Australia and I was given the go ahead to come back and spin up a new office. We are already growing rapidly with a Series E round under our belt , which puts us in a great position to aggressively grow. Come join us down here in Sydney! I’m happy to be home, but I’d be even happier if you decided to join us down here in Sydney! We are currently looking for Site Reliability Engineers and Software Engineers to join our team and help build out the start of a new shiny office in the land down under. You can see the current positions on our careers page - there will be more to come in the future. If you’re interested and are already in Australia, don’t hesitate to apply or reach out to me directly .", "date": "2021-03-01"},
{"website": "CockroachLabs", "title": "How Levven Keeps Your Smart Home On When The Internet Goes Out", "author": ["Lewis Gunsch"], "link": "https://www.cockroachlabs.com/blog/resilient-smart-appliances/", "abstract": "Remember when people’s vacuum cleaners and doorbells stopped working because a region of AWS went down? That won’t happen to anyone using our smart home products. Levven had been in the electronics manufacturing industry for decades before “smart technology” paved the way for “smart homes”. When that happened we saw an opportunity to help make homes more affordable by designing and manufacturing smart home controls that reduce build costs. By eliminating the wire between the wall switch and the light we can offer consumers more eco-friendly home construction saving precious resources. At Levven we added smart home controls manufacturing 5 years ago - but our cloud connectivity is new. Levven’s goal is to make homes more affordable to build, purchase, and operate for the masses with smart controls. In order to do that we need to build scale-out, low maintenance infrastructure that keeps the cost to the end user low, while also guaranteeing that our smart home controls will always work. To help us address this IoT challenge we chose CockroachDB’s managed service offering: CockroachCloud . In this blog we’ll explain how we’ve built IoT application architecture that keeps smart devices available at all times. Why Levven chose CockroachCloud Our goal at Levven is to make smart home technology affordable to the masses. We do this by keeping our infrastructure overhead costs as small as possible. Our requirements for the database were simple: • The database should speak SQL • The database should scale horizontally • The database should be highly available • The database should be cost efficient Those requirements swiftly eliminated Oracle ( price /scale issues), Postgres (scale), and MongoDB (I didn’t want to immediately jump to NoSQL solutions when there are fantastic relational solutions available). We did, however, spend some time considering Cassandra. But the complexity of managing the cluster and the machines would have added too much operational overhead for us. Because we’re a small team, aiming for a very low cost per customer, the lack of manual operations is an area where we need to spend efficiently. We were interested in what CockroachDB could do from a scale and availability perspective. It was tremendously helpful that they offer a self-service managed version of CockroachDB (CockroachCloud). But the moment I knew for sure that we would use CockroachCloud was when I read about their active-active availability . That was my holy grail! I was thrilled to see that someone had solved it so well. IoT Smart Controls Use-Case Tech Stack Right now we’re in the beginning stages of growing out this side of our business. The install base is growing steadily and with a simple tech stack we can deliver reliable robust service. While we use a number of AWS solutions to power our iOS and Android apps, we use CockroachCloud to manage and store data. Two Kinds of Smart Controls Data There are basically two different kinds of data being stored on CockroachCloud: Live operational information and metadata. For us the operational data keeps the system running and allows users to personalize their home, experience, and control all the connected electrical loads. Utilizing metadata such as how many times a lightswitch is being pressed, or what temperature a room is, and what the voltage is on a battery allows us to deliver more advanced control and energy saving features. How Levven’s Smart Controls Work When the Internet is Down In our industry there are two different data paths. One path depends on internet connection and the other depends on radio frequency (RF). We chose RF because we wanted to architect a system in which our smart home controls still work even when the internet does not. Why did everyone’s Roomba stop working when the AWS region went out? The Roomba stopped working because Roomba (and many other smart electronics companies) chose the other path - the one that relies on an internet connection at all times. Here is a high-level diagram of Levven’s architecture. The “Levven Cloud” contains a lot of details like our load balancers, scaling groups, and such - but, for the purposes of this blog we’ve chosen not to go into all those details. A light switch and the load controller are not internet connected. They are connected over a 900 megahertz RF network. The gateway is what bridges the controls with the cloud and access for mobile apps. It listens in on the radio communications and sends it to Levven’s servers. That is where the app connects to the home and can perform actions. What’s important to note here is that even without the internet all your home controls will continue to function intelligently. So if a region fails, the lightswitch will continue to work. Surprise Benefit of CockroachCloud: DB-Console The two best things about CockroachCloud are that you don’t have to worry about it because it’s a managed service and it scales easily so you know you can add machines to solve usage problems. But beyond those obvious benefits we’ve been pleasantly surprised by the look of the DB-Console and how much information I can get out of it. There is a lot of debug information in there that is just running without me having to do anything. I use this to make sure we maintain our p99 latency. What’s Next: Analytics & Scale with CockroachCloud We know that we could ask a time-series database to run our analytics but we’re going to ask CockroachCloud to do it. We know that this is not a priority use case for CockroachDB. But they’re perfectly capable of running what we need (particularly with improvements made to the cost based optimizer and vectorized execution in the latest version of 20.2 ). For us the time, cost, and complexity of adding another database just to run some simple analytics workloads is not worth the effort. This is just the beginning of our entry into the smart home industry. Our intention is to establish relationships with all the major builders in North America and scale out our users across the continent. Right now we do not require a multi-region configuration. But expect that to happen rather quickly and we’re ready for it because of CockroachCloud.", "date": "2021-03-02"},
{"website": "CockroachLabs", "title": "Hello World", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/hello-world/", "abstract": "Databases are the beating heart of every business in the world, running the gamut from humble spreadsheets to thousands of servers linked into vast supercomputers. And they’ve been evolving rapidly. Most of us at Cockroach Labs have spent our careers watching them progress, often actively struggling to overcome their limitations when the task at hand outstripped their capabilities. But first, why “Cockroach”? If you can get past their grotesque outer aspect, you’ve got to give them credit for sheer resilience. You’ve heard the theory that cockroaches will be the only survivors post-apocalypse? Turns out modern database systems have a lot to gain by emulating one of nature’s oldest and most successful designs. Survive, replicate, proliferate. That’s been the cockroach model for geological ages, and it’s ours too. It doesn’t hurt that the name itself is resilient to being forgotten. In the early 2000s when we were working at Google, it wasn’t long before we ran smack into a database headache all too common in the industry: application-level sharding. What happens when you have too much data to store in a database? Easy, you split it up and put some into one database, some into another, and so on, creating as many shards (i.e. partitions) as necessary. But what happens when the data growth doesn’t stop? The challenge was, succinctly: more shards, more problems, which created customer downtime, overloaded servers, and mounting complexity. We developed a healthy appreciation for the difficulties of scale. Fast forward to the mid-2000s, when Google ushered in the NoSQL movement. NoSQL sacrificed some capabilities of traditional RDBMS systems for simplicity. A simpler design allowed transparent scalability on commodity hardware. Now applications could be developed as though against a single database, but one capable of reaching formerly unheard-of sizes. We then watched as those simplifications became an Achilles’ heel for developers, leading to increasingly complex application logic to workaround missing functionality, like transactions. These pain points in turn required new designs to restore consistency and transactionality, as next-generation systems replaced the old. In 2012 we left Google to create a private photo sharing platform called Viewfinder. In this day and age, dreams of providing a service for billions of users are not misplaced fantasies, and our dreams were big. But they collided with reality as we struggled to find open source replacements for the excellent infrastructure we’d left behind. We muddled through, built decent, sometimes complex workarounds where possible, used duct tape where necessary. Turns out there’s simply never enough time to solve this problem when it’s not your primary goal. So we decided to make it our priority. We started Cockroach Labs with a clear objective: build the database we’d been itching after for years, and build it as open source, from the start. The mission is simple, but as big and as broad as any we could happily commit to: Make Data Easy . The database we want scales, survives disasters, is always consistent, and supports abstractions that make it easy to use. We’d rather spend time quickly building and iterating products, not engineering solutions to database shortcomings. This is the database we want when we start our next company… And the one after that. Today, we’re launching CockroachDB for everyone. Use it. Build on it. Contribute to it! Ben, Peter & Spencer", "date": "2015-06-04"},
{"website": "CockroachLabs", "title": "Accessibility Improvements & Interactive SQL Shell Now in CockroachDB Docs", "author": ["Lucia Cozzi", "Lauren Hirata Singh"], "link": "https://www.cockroachlabs.com/blog/accessibility-improvements-interactive-sql-shell-now-in-cockroachdb-docs/", "abstract": "The CockroachDB Docs site is home to our product documentation for CockroachDB and CockroachCloud , our two product offerings. It includes reference documentation, conceptual documentation, developer guides, tutorials, and much more. Cockroach Labs has always prioritized its documentation, investing in it from very early in the company’s existence. We believe clear, concise, and helpful docs are important in enabling our users to be self-sufficient in testing, deploying, and interacting with our products. They’re an important tool that internal users, such as Support and Sales, also reference. Each document goes through a thorough research, draft, and review process– and while our Docs team owns the process, it’s a collaborative effort. Many times, a cross-functional group made up of engineers, product managers, designers, customer success– even members from our community!–  will contribute. No matter how good our content is though, that means very little if you can’t find what you’re looking for. We decided to redesign our site to improve its functionality, make our site easier for users to discover relevant content, and fine tune and add features within the docs. As you explore the new site, you’ll see a number of things have changed. You Are Here: Better Navigation through the CockroachDB Docs We’ve taken a new direction with our navigation. To navigate through Docs, there is now a left-panel that presents the following: A search box The quickest way to get started Our products ( CockroachCloud and CockroachDB ) An interactive SQL shell to test features We believe this will make it easier for users to find the content they are looking for when exploring our docs. We’ve also added completely redesigned product area landing pages, as entry points for new users to learn about each product, and for returning users to find out about new releases. There’s a landing page for CockroachCloud : As well as CockroachDB : Digital Accessibility Improvements to the CockroachDB Docs Those with an eye for details might also notice that minor usability bugs have been improved. This includes updates to colors for accessibility, code-blocks enhanced for visual contrast, and sortable tables. These updates are small, but important– you might only notice them when they aren’t working right. With that in mind, if you find a usability bug in our Docs, please file an issue ! In-browser Interactive SQL Shell in the CockroachDB Docs A really exciting addition to our docs is our new in-browser interactive SQL shell , also known as the “SQL Playground”. This playground gives you an interactive SQL shell to a single-node CockroachDB cluster, and is pre-loaded with a sample dataset. This will allow you to test features on your browser before launching into the “real” thing. We also now offer interactive tutorials as more guided experiences within the SQL Playground. Goodbye, Green: A Refresh on Our Branding Lastly, the branding has been fully updated. We are so excited about this! Our new colors, fonts and images are fabulous, you can explore them more on our main website . Along with visual updates, we have also added a new library of UI (user-interface) components that are used across the site. This includes buttons, fields, navigation bars, callouts, etc. Now, we are consistently styling and using these components, which not only makes it a cohesive and consistent experience for our users, but it also makes it easier for our engineering team to maintain. Docs Sites We Admire You might be wondering how we settled on what to change. As with many design projects, we started out by doing research. We connected with our colleagues and users to learn about what they liked or disliked about the site, how they navigated to find the content they were looking for, and what functionality they wished for. We were looking to define the needs that users have when they come to the docs site, so we could design solutions for those needs. We also analyzed other company’s documentation sites– like MongoDB and Stripe – that we considered successful, and critically evaluated what it was about them that we enjoyed. Then, we embarked on a design exploration, testing different prototypes (ranging from low-fidelity sketches to high-fidelity walkthroughs) until we landed on the final concept. Usability Testing Throughout our testing, we paid particular attention to making sure users felt oriented when navigating through the Docs Site, and prioritized the ability to locate content quickly while still having an enjoyable reading experience. We are very excited to finally share these updates with everyone, knowing that while this will be a significant improvement, good design never stops. In the months following the redesign we will be monitoring our CSAT scores, page analytics, and gathering qualitative feedback to see how our users respond to the updates, and measure whether, in fact, our goals have been achieved. In the future, we will use these metrics and qualitative feedback to drive next steps and future improvements including exciting updates like Dark Mode, an improved search experience, and more. Stay tuned! Contribute to the CockroachDB Docs Our docs are open source and we encourage anyone to help improve them by contributing content, testing examples and tutorials, or filing issues for improvement suggestions.", "date": "2021-02-26"},
{"website": "CockroachLabs", "title": "Scaling Raft", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/scaling-raft/", "abstract": "In CockroachDB , we use the Raft consensus algorithm to ensure that your data remains consistent even when machines fail. In most systems that use Raft, such as etcd and Consul , the entire system is one Raft consensus group. In CockroachDB, however, the data is divided into ranges , each with its own consensus group. This means that each node may be participating in hundreds of thousands of consensus groups. This presents some unique challenges, which we have addressed by introducing a layer on top of Raft that we call MultiRaft . With a single range, one node (out of three or five) is elected leader, and it periodically sends heartbeat messages to the followers. As the system grows to include more ranges, so does the amount of traffic required to handle heartbeats. The number of ranges is much larger than the number of nodes (keeping the ranges small helps improve recovery time when a node fails), so many ranges will have overlapping membership. This is where MultiRaft comes in: instead of allowing each range to run Raft independently, we manage an entire node’s worth of ranges as a group. Each pair of nodes only needs to exchange heartbeats once per tick, no matter how many ranges they have in common. In addition to reducing heartbeat network traffic, MultiRaft can improve efficiency in other areas. For example, MultiRaft only needs a small, constant number of goroutines (currently 3) instead of one goroutine per range. Implementing and testing a consensus algorithm is a daunting task, so we are pleased to be working closely with the etcd team from CoreOS instead of starting from scratch. The raft implementation in etcd is built around clean abstractions that we found easy to adapt to our rather unusual requirements, and we have been able to contribute improvements back to etcd and the community.", "date": "2015-06-12"},
{"website": "CockroachLabs", "title": "SQL in CockroachDB: Mapping Table Data to Key-Value Storage", "author": ["Peter Mattis", "Tamir Duberstein"], "link": "https://www.cockroachlabs.com/blog/sql-in-cockroachdb-mapping-table-data-to-key-value-storage/", "abstract": "<!–– Outdated blog post alert! CockroachDB no longer stores each non-primary-key column in a separate value by default. By default, all data in a table is stored in the same column family . Gory details available here. ––!> SQL? I thought CockroachDB was a key-value store?!? In the past we described CockroachDB as a distributed, transactionally consistent, key-value store. We knew that a key-value API was not the endpoint we wanted to provide and a few months ago started work on a higher level structured data API that would support tables and indexes. Along with supporting such rich structures, we anticipated eventually supporting SQL for manipulating and accessing this structured data. After a bit of soul searching we embraced the inevitable and moved forward full-speed with SQL as the core of our structured data layer. There are a lot of components to an SQL system: query parsing, query analysis, query planning, query execution, transactions, and persistent storage – to name a few. The CockroachDB SQL system is built on top of the internal CockroachDB key-value store and leverages the monolithic sorted key-value map to store all of the SQL table data and indexes. This post will focus on CockroachDB’s mapping of SQL data to the key-value store* and show how that mapping helps implement SQL functionality. Future posts will talk about query analysis, planning, and execution. An SQL table is a set of rows where each row is a set of columns. Each column has an associated type (bool, int, float, string, bytes). A table also has associated indexes which allow for the efficient retrieval of a range of rows from a table. This sounds nothing at all like a key-value API where string keys map to string values. How do we map the SQL table data to KV storage? First, a primer: CockroachDB’s internal key-value API supports a number of operations, but we only need to know about a few of them for this post: ConditionalPut(key, value, expected-value) - Conditionally set the value at the specified key if the existing value matches the expected value. Scan(start-key, end-key) - Retrieve all of the keys between start-key (inclusive) and end-key (exclusive). In CockroachDB, keys and values are both strings which can contain unrestricted byte values. OK, let’s move on! Key Encoding A foundational piece of the puzzle for mapping SQL table data to keys and values is encoding typed column data into strings. For example, given a tuple of values <1, 2.3, \"four”> , we would like to encode this to a string that conceptually looks like: /1/2.3/\"four” We’re using forward-slash as a visual separator between values, though this is only for readability purposes. We could dedicate an entire blog post to how these encodings work; in the interest of brevity, this post only discusses their properties, not their implementation. The encoded keys sort such that each field of the key is considered separately: /1/2.3/\"four” /2/3.1/\"six” /10/4.6/\"seven” If you were to naively sort these strings you’d discover that /10/… sorts before /2/… . How the encoding works can appear a bit magical if you haven’t encountered it before. If you’re interested in the nitty-gritty details, see {Encode,Decode}{Varint,Float,Bytes,Null} in the util/encoding package. With this encoding tool at our disposal, we’re ready to take a peek at the encoding of SQL table data. In CockroachDB, each table has a unique 64-bit integer ID assigned to it at creation. This table ID is used as the prefix for all keys associated with that table. Now let’s consider the following table and data: CREATE TABLE test (\n      key       INT PRIMARY KEY,\n      floatVal  FLOAT,\n      stringVal STRING\n)\n\nINSERT INTO test VALUES (10, 4.5, \"hello”) Every table in CockroachDB must have a primary key. The primary key is composed of one or more columns; in our test table above, it is composed of a single column. CockroachDB stores each non-primary key column under a separate key that is prefixed by the primary key and suffixed by the column name. The row <10, 4.5, \"hello”> would be stored in our test table as: Key Value /test/10/floatVal 4.5 /test/10/stringVal \"hello” In this depiction, we’re using /test/ as a placeholder for the table ID and /floatVal and /stringVal as placeholders for the column ID (every column in a table has an ID that is unique within the table). Note that the primary key immediately follows the table ID in our encoding. This is the basis for index-scans in CockroachDB’s SQL implementation. If we were to look under the hood, we would see the table metadata: test Table ID 1000 key Column ID 1 floatVal Column ID 2 stringVal Column ID 3 In numeric form, the key-value pairs for our table look like: Key Value /1000/10/2 4.5 /1000/10/3 \"hello” We’ll stick with the symbolic form for keys for the rest of this post. [You might be thinking that the common prefixes ( /1000/10 ) for the keys are wasting storage, but RocksDB our underlying storage engine, eliminates almost all of the overhead via prefix compression of keys.] Astute readers will note that storing key-value pairs for columns which are contained in the primary key is not necessary, as these columns’ values are already encoded in the key itself. Indeed, CockroachDB elides these. Notice that all of the keys for a particular row will be stored next to each other due to the primary key prefix (remember, key-value data is stored in a sorted monolithic map in CockroachDB, so this property is “free”). This allows retrieval of the values for a particular row using a Scan on the prefix. And this is exactly what CockroachDB does internally. The query: SELECT * FROM test WHERE key = 10 Will be translated into: Scan(/test/10/, /test/10/Ω) This scan will retrieve only the two keys for the row. The Ω represents the last possible key suffix. The query execution engine will then decode the keys to reconstruct the row. Null Column Values There is a small twist to this story: column values can be NULL unless explicitly marked as NOT NULL . CockroachDB does not store NULL values and instead uses the absence of a key-value pair for a column to indicate NULL . The observant might notice a wrinkle here: if all of the non-primary key columns for a row are NULL we won’t store any keys for the row. To address this case, CockroachDB always writes a sentinel key for each row that is the primary key without a column ID suffix. For our example row of <10, 4.5, \"hello”> , the sentinel key would be: /test/10 . Huzzah! Secondary Indexes So far we’ve ignored secondary indexes. Let’s rectify that oversight: CREATE INDEX foo ON test (stringVal) This creates a secondary index on the column stringVal. We haven’t declared the index as unique, so duplicate values are allowed. Similar to the rows for a table, we’ll be storing all of the index data in keys prefixed by the table ID. But we want to separate the index data from the row data. We accomplish that by introducing an index ID which is unique for each index in the table, including the primary key index (sorry, we lied earlier!): /tableID/indexID/indexColumns[/columnID] The keys we used as examples above get slightly longer: Key Value /test/primary/10 Ø /test/primary/10/floatVal 4.5 /test/primary/10/stringVal \"hello” And now we’ll also have a single key for the row for our index foo: Key Value /test/foo/”hello”/10 Ø You might be wondering why we suffixed this encoding with the primary key value ( /10 ). For a non-unique index like foo , this is necessary to allow the same value to occur in multiple rows. Since the primary key is by definition unique for the table, appending it as a suffix to a non-unique key results in a unique key. In general, for a non-unique index, CockroachDB appends the values of all columns which are contained in the primary key but not contained in the index in question. Now let’s see what happens if we insert <4, NULL, \"hello”> into our table: Key Value /test/primary/4 Ø /test/primary/4/stringVal \"hello” /test/foo/\"hello”/4 Ø All of the table data together looks like: Key Value /test/primary/4 Ø /test/primary/4/stringVal \"hello” /test/primary/10 Ø /test/primary/10/floatVal 4.5 /test/primary/10/stringVal \"hello” /test/foo/\"hello”/4 Ø /test/foo/\"hello”/10 Ø Secondary indexes are used during SELECT processing to scan a smaller set of keys. Consider: SELECT key FROM test WHERE stringVal = \"hello” The query planner will notice there is an index on stringVal and translate this into: Scan(/test/foo/”hello”/, /test/foo/”hello\"/Ω) Which will retrieve the keys: Key Value /test/foo/”hello”/4 Ø /test/foo/”hello”/10 Ø Notice that these keys contain not only the index column stringVal, but also the primary key column key. CockroachDB will notice the primary key column key and avoid an unnecessary lookup of the full row. Finally, let’s look at how unique indexes are encoded. Instead of the index foo we created earlier, we’ll create uniqueFoo : CREATE UNIQUE INDEX uniqueFoo ON test (stringVal) Unlike non-unique indexes, the key for a unique index is composed of only the columns that are part of the index. The value stored at the key is an encoding of all primary key columns that are not part of the index. The two rows in our test table would encode to: Key Value /test/uniqueFoo/\"hello” /4 /test/uniqueFoo/\"hello” /10 We use ConditionalPut when writing the key in order to detect if the key already exists which indicates a violation of the uniqueness constraint. So that’s how CockroachDB maps SQL data to the key-value store in a nutshell. Keep an eye out for upcoming posts on query analysis, planning, and execution. * The idea of implementing SQL on top of a key-value store isn’t unique to CockroachDB. This is essentially the design of MySQL on InnoDB, Sqlite4, and other databases, as well. Does distributed SQL make you giddy? Then good news — we're hiring! Check out our open positions here .", "date": "2015-09-16"},
{"website": "CockroachLabs", "title": "The New Stack: Meet CockroachDB, the Resilient SQL Database", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/the-new-stack-meet-cockroachdb-the-resilient-sql-database/", "abstract": "Editor’s Note : Since this post was originally published on The New Stack in 2015 , CockroachDB has grown into a $2B evaluation and can publicly claim customers like DoorDash, Bose, SpaceX, Comcast and many others. For an up to date description of how CockroachDB compares to incumbent database products check out this short video: The goal of CockroachDB is to make data easy. If you could take all the energy wasted wrestling with database shortcomings, and invest that time, money and engineering into making your company stronger and your product better, everyone would be better off. Databases are the life-blood of every modern business, yet despite nearly 45 years of evolution, developers are left with a small range of options: RDBMS : Fully featured with transactions and indexes, but are not scalable and fault-tolerant. If your business grows beyond the capacity of a single node, you’ll have to resort to sharding, which is not a good option. NoSQL : Designed for massive horizontal scale, these systems sacrifice transactions and consistency, and eschew explicit schemas. These are features which increasingly complex applications require. Hosted Services : These are storage services that require vendor lock-in and are all proprietary. Their functional guarantees are difficult to verify. The biggest frustration with these options is that if a company succeeds, and especially if it succeeds wildly, it will be extremely expensive to scale, both in financial and engineering resources. None of these solutions are scalable, transactional, and available, which is why we were inspired to build CockroachDB. CockroachDB is a distributed SQL database built on top of a transactional and consistent key-value store. The primary design goals are support for ACID transactions, horizontal scalability and survivability (hence the name). CockroachDB implements a Raft consensus algorithm for consistency and aims to tolerate disk, machine, rack and even data center failures with minimal disruption and no manual intervention. So how does CockroachDB work? Let’s start with a look at the architecture. A Layered Architecture CockroachDB’s architecture is structured in layers that make complexity an easier task to manage. Each higher level in the architecture treats the lower levels as functional black boxes, while the lower layers remain completely unaware of the higher ones. SQL Layer: The highest level of abstraction and a well-established API for developers. Provides familiar relational concepts such as schemas, tables and indexes using a derivative of the Postgres grammar with some modern touches. Distributed Key:Value Store: We implement our distributed key-value store as a monolithic sorted map, making it easy to develop large tables and indexes (Hbase, BigTable, and Spanner all use similar architectures). Distributed Transactions : Not necessarily a part of the layered architecture, but a fundamental part of the system. The implementation of distributed transactions suffuse the layers of the architecture: from SQL to stores and ranges. Nodes: The physical machines, virtual machines, or containers that contain stores. The distributed KV store routes messages to nodes. Store: Each node contains one or more stores, and each store contains potentially many ranges. Every store is managed with RocksDB , an open source storage engine from Facebook based on Google’s LevelDB . Range: Every store contains ranges, which are the lowest-level unit of key-value data. Each range covers a contiguous segment of the larger key-space — one at the start of the alphabet, the next in the middle of the alphabet, and so on. Together, they make up the entire monolithic sorted map. The range is where we do synchronous replication, usually three or five way, using the Raft consensus algorithm, a variant of Paxos. From here, we’ll walk through the design of CockroachDB starting at the lowest level and working our way up to show how it achieves three key properties: Transactionality, Scalability, and Survivability. Horizontal Scaling Let’s consider CockroachDB at its lowest level, running on a single machine. Even though the data is logically organized into tables, rows, columns, etc., at the lowest level, individual pieces of data (think of a single column value) are stored on-disk in a sorted key-value map. CockroachDB starts off with a single, empty range of key-value data encompassing the entire key space. As you put more data into this range, it will eventually reach a threshold size (64MB by default). When that happens, the data splits into two ranges, each of which can be accessed as a separate units, each covering a contiguous segment of the key space. This process continues indefinitely as data is added (see figure below). The monolithic sorted map in CockroachDB is made up of the sorted set of all ranges —  one at the start of the alphabet, the next in the middle of the alphabet, and so on. We opt for small ranges as they’re easily moved between machines when repairing or rebalancing data. As new data flows in, existing ranges will continue to split into new ranges, aiming to keep a relatively consistent range size somewhere between 32MB and 64MB. When our cluster spans multiple nodes and stores, newly split ranges are rebalanced to stores with more capacity available. This design gives you a scalable database that can run across many different nodes. To make the database survivable, CockroachDB has to replicate the data. Replication By default, each range is replicated on three nodes. Any number can be configured, but three and five are often considered the most useful. Range replicas are intended to be located in disparate datacenters for survivability (e.g. { New York, Virginia, California }, { Ireland, New York, California}, { Ireland, New York, California, Japan, Australia }). When you have data that is stored across multiple machines, it’s important that the data be consistent across replicas. CockroachDB uses Raft as its consensus protocol. Each range is an independent instance of the Raft protocol, so we have many ranges all independently running Raft. This allows a considerable degree of flexibility, including arbitrary replication configurations for different segments of the key space. In any group of replicas, one of the nodes will be elected the leader, which then coordinates the decisions for the group. Whenever you want to write something to the database, it goes to the leader to be proposed to the cluster, and is committed as soon as it has been acknowledged by a majority (either by 2 out of 3 or by 3 out of 5 of the replicas). We actually share this implementation of the Raft protocol with the etcd team, but add another layer for scalability. In CockroachDB, each Range corresponds to a Raft instance, and their number is much higher than that of the physical nodes hosting them. To help scale Raft we built MultiRaft , a layer on top of Raft which groups messages sent by the replicas. Distributed Transactions One of the foundations of CockroachDB is strong consistency and full support of distributed ACID transactions. CockroachDB provides distributed transactions using multi-version concurrency control (MVCC). MVCC data is stored and managed on each local storage device with an instance of RocksDB . Mutations to MVCC data are consistently replicated using Raft . CockroachDB provides snapshot isolation (SI) and serializable snapshot isolation (SSI), allowing externally consistent, lock-free reads and writes, both from a historical snapshot timestamp and from the current wall clock time. SSI is provided as the default isolation level; SSI eliminates some rare anomalies which are present with SI, but suffers worse performance in the event of contention. Clients must consciously decide to trade correctness for performance. For more information on how CockroachDB approaches transactions, read this blog post . CockroachDB’s SQL implementation leverages the distributed transactions and strong consistency provided by the monolithic sorted key-value map to consistently encode, store, and retrieve the SQL table data and indexes. The SQL grammar supported is a derivative of PostgreSQL with some modern touches, and supports any Object-Relational Mapping (ORM). For more information on CockroachDB’s SQL implementation, read this blog post and check out our SQL branch on Github. Deployment and Management We built CockroachDB to have a simple deployment that fits well with the container model. Nodes are symmetric and self-organize — there’s no single point of failure, no complicated configuration (in fact, only a handful of command-line options), nor is there a tangle of different roles to be managed. All that’s required is a single binary (easily put into a container) which runs on each node in order to join the cluster and export the local stores for accepting new writes and rebalances. CockroachDB self-organizes by using a gossip network, which is a peer-to-peer network where nodes communicate with each other. A new node can join the gossip network as long as it can talk to any other node which has already joined. The gossip network continually balances itself to to minimize RPC traffic and the number of hops required to communicate new information to each node in the network. Nodes exchange their network addresses as well as store capacity information. If a node discovers that it has more capacity than the mean of its neighbors, it will start to rebalance the range to the other nodes. Similarly, if a node goes down, the replicas of its constituent ranges will notice its absence and look for another node to which they can re-replicate. CockroachDB can be run locally on a laptop, corporate dev cluster or private cloud, as well as on any public cloud infrastructure. We’ve developed cockroach-prod which, at the time of this writing, lets you set up a cluster on Amazon Web Services and in Google Cloud Engine. Support for other public cloud infrastructure and additional features are in the works. What’s Next At the time this post was originally written CockroachDB was in Alpha. That is, of course, no longer the case. With companies like DoorDash, SpaceX, and Tesla using CockroachDB in production today the next step for CockroachDB is to continue pushing it’s DBaaS offering, CockroachCloud , into the minds of developers so that they can take advantage of the distributed computing advantages like scale, resilience, and data locality. Aditionally, to further cement itself as the best database for Kubernetes, CockroachDB is polishing the Kubernetes Operator that is available in the open source version of CockroachDB today.", "date": "2015-10-30"},
{"website": "CockroachLabs", "title": "The Cost and Complexity of Cgo", "author": ["Tobias Grieger"], "link": "https://www.cockroachlabs.com/blog/the-cost-and-complexity-of-cgo/", "abstract": "Cgo is a pretty important part of Go : It’s your window to calling anything that isn’t Go (or, more precisely, anything that has C bindings). For CockroachDB , cgo lets us delegate a lot of the heavy lifting at the storage layer to RocksDB , for which no suitable replacement within the Go ecosystem exists, at least to the best of our knowledge. After some iterations, we’ve found that the right way to deal with these external libraries – of which we have quite a few – is to outsource them in Go wrapper packages: c-rocksdb c-snappy c-protobuf c-jemalloc c-lz4 But as well as using cgo has worked for us, it hasn’t come for free. The experienced cgo-er will probably know this (and might prefer to lightly skim over the remainder of this post absentmindedly), but using cgo comes with some caveats that we’ll discuss below along with our suggested workarounds. Call Overhead The overhead of a cgo call will be orders of magnitude larger than that of a call within Go. That sounds horrible, but isn’t actually an issue in many applications. Let’s take a look via this toy cgobench package:: func BenchmarkCGO(b *testing.B) {\n    CallCgo(b.N) // call `C.(void f() {})` b.N times\n}\n\n// BenchmarkGo must be called with `-gcflags -l` to avoid inlining.\nfunc BenchmarkGo(b *testing.B) {\n    CallGo(b.N) // call `func() {}` b.N times\n} $ go test -bench . -gcflags '-l'    # disable inlining for fairness\nBenchmarkCGO-8  10000000              171 ns/op\nBenchmarkGo-8   2000000000           1.83 ns/op In other words, in this (admittedly minimal) example there’s approximately a factor of 100 involved. Let’s not be crazy though. In absolute time, 171ns is often a perfectly acceptable price to pay, especially if your C code does substantial work. In our case, however, we clocked in the high tens of thousands of cgo calls during some tests, so we looked at pushing some of the code down to C to cut down on the number of iterations. Our conclusion was that the call overhead did not matter – equivalent C++ and Go implementations were indistinguishable performance-wise. However, we still ended up moving some operations to C++ with a fat improvement due to being able to write a more efficient implementation. Manual-ish Memory Management Go is a garbage-collected runtime, but C is not. That means that passing data from C into Go and vice versa must not be done carelessly, and that copies are often not avoidable. Especially when dealing with byte strings and interface crossing at high frequency (as we are), the prescribed usage of C.CString and C.GoBytes can increase memory pressure considerably – and of course copying data eats up CPU noticeably. In some cases, we were able to  avoid some of these copies. For example, when iterating over keys, we use something like : func (r *rocksDBIterator) Key() []byte {\n   return C.GoBytes(unsafe.Pointer(r.key), s.len)\n}\n\nfunc (r *rocksDBIterator) Next() {\n   // The memory referenced by r.key stays valid until the next operation\n   // on the iterator.\n   r.key = C.DBNext(r.iter) // cgo call\n} If all we want to do is check the current key against  a criterion, we know the underlying memory isn’t going to be freed while we need it. Hence, this (made-up) bit of code seems wasteful: for ; iter.Valid(); iter.Next() {\n    if bytes.HasPrefix(iter.Key(), someKey) { // copy!\n        // ...\n    }\n} To mitigate all of these copies, we add (and use) a zero-copy (and unsafe) version of Key() : // unsafeKey() returns the current key referenced by the iterator. The memory\n// is invalid after the next operation on the iterator.\nfunc (r *rocksDBIterator) unsafeKey() []byte {\n    // Go limits arrays to a length that will fit in a (signed) 32-bit\n    // integer. Fall back to copying if our slice is larger.\n    const maxLen = 0x7fffffff\n    if s.len > maxLen {\n        return C.GoBytes(unsafe.Pointer(r.key), s.len)\n    }\n    return (*[maxLen]byte)(unsafe.Pointer(s.data))[:s.len:s.len]\n} While this is going to be more efficient and is safe when properly used, it looks and is much more involved. We’re creating a slice which is backed by memory allocated by C. We need to be careful that the C memory is not freed while our slice (or any derivative slices) are still in use. We can get away with it since it’s in our low-level code, but this is certainly not an option for any type of public-facing API; it would be guaranteed that some users would not honor the subtle contract tacked on to the returned byte slice and experience null pointer exceptions randomly. Cgoroutines != Goroutines This one can be a serious issue and while it’s obvious when you think about it, it can come as a surprise when you don’t. Consider the following: func main() {\n  for i := 0; i < 1000; i++ {\n    go func() {\n        time.Sleep(time.Second)\n    }()\n  }\n  time.Sleep(2*time.Second)\n} This boring program wouldn’t do much. 1000 goroutines come pretty much for free in Go; the “stack” allocated to each of them is only a few kilobytes. What if we brought cgo into the game? The code below is a simplified version of an example in cgobench : //#include <unistd.h>\nimport \"C\"\n\nfunc main() {\n  for i := 0; i < 1000; i++ {\n    go func() {\n        C.sleep(1 /* seconds */)\n    }()\n  }\n  time.Sleep(2*time.Second)\n} The “surprise” is that this behaves very differently. A blocking cgo call occupies a system thread; the Go runtime can’t schedule them like it can a goroutine, and the stack, being a real stack, is on the order of megabytes! Again, not a big deal if you’re calling into cgo with appropriately bounded concurrency. But if you’re writing Go, chances are you’re used to not thinking about Goroutines too much. A blocking cgo call in the critical request path could leave you with hundreds and hundreds of threads which might well lead to issues . In particular, ulimit -r or debug.SetMaxThreads can lead to a quick demise. Or, in the words of Dave Cheney , “Excessive cgo usage breaks Go’s promise of lightweight concurrency.” Cross Out Cross-Compilation With cgo, you lose (or rather, you don’t win) the ease with which cross-compilation works in Go 1.5 and higher. This can’t be surprising (since cross-compiling Go with a C dependency certainly must entail cross-compiling the C dependency) but can be a criterion if you have the luxury of choosing between a Go-native package or an external library. Dave Cheney’s posts on this are usually about the best source of information available. Static Builds This is a similar story to cross-compilation, though the situation is a little better. Building static binaries is still possible using cgo, but needs some tweaking. Prior to Go 1.5, the most prominent example of this was having to use the netgo build tag to avoid linking in glibc for DNS resolution. This has since become the default, but there are still subtleties such as having to specify a custom -installsuffix (to avoid using cached builds from a non- static build), passing the right flags to the external linker (in our case, -extldflags “-static” ), and building with -a to enforce a complete rebuild. Not all of this may be necessary any more, but you get the idea: It gets more manual and, with all the rebuilding, slower. For anyone interested, here’s my first (and since dated) wrestle with cgo and a mysterious bug which we may pick up again in a future post. Debugging Debugging your code will be harder. The portions residing in C aren’t as readily accessed through Go’s tooling. PProf, runtime statistics, line numbers, stack traces – all will sort of feather out as you cross the boundary. GoRename and its friends may occasionally litter your source code with identifiers that postdate the translation to cgo-generated code. The loss can feel jarring since the tooling usually works so well . But, of course, gdb still works. Summary All in all, cgo is a great tool with limitations. We’ve recently begun moving some of the low-level operations down to C++, which gave some impressive speed-ups . Other attempts produced no benefit. Isn’t performance work fun?", "date": "2015-12-10"},
{"website": "CockroachLabs", "title": "How online schema changes are possible in CockroachDB", "author": ["Vivek Menezes"], "link": "https://www.cockroachlabs.com/blog/how-online-schema-changes-are-possible-in-cockroachdb/", "abstract": "I periodically need to make changes to tables… mostly adding columns. Very simple with the alter table command… But my tables have up to 40 million rows now and they are growing fast… So those alter table commands take several hours… Since I’m using amazon RDS [sic], I can’t have slave servers to play with and then promote to master. So my question is if there’s a way to do this with minimal downtime? I don’t mind an operation taking hours or even days if users can still use the db…” — Stack Overflow serverfault [^1] The above query was posted back in 2010, but the anxiety around downtime for schema changes remains. When we were designing CockroachDB’s schema change engine, we wanted to build it better. We wanted to provide a simple way to update a table schema (just run ALTER TABLE) and not have an application suffer any negative consequences – downtime included. We wanted schema changes to be a built-in feature of CockroachDB, requiring no additional tools, resources, or ad hoc sequencing of operations; and all this, with no effect on application read/write latency. In this post, I’ll explain our online schema change strategy. I’ll discuss how we manage changes to schema elements like columns and indexes without forcing a downtime. Here’s what we do A schema change involves updating the schema and adding or deleting the table data associated with the change. Two fundamental distributed database features make this particularly tricky: High performance: The schema must be cached across database nodes in order to optimize database performance. Keeping a distributed cache consistent is challenging. Large tables: Database tables can be very large. Backfilling or purging table data associated with the schema change takes time, and doing it correctly without disabling access to the database is challenging. Our solution for maintaining a consistent distributed schema cache and consistent table data embraces the concurrent use of multiple versions of the schema, allowing the rollout of a new schema while the older version is still in use. It backfills (or deletes) the underlying table data without holding locks. This solution is derived from the work done by the F1 team at Google. Safe schema rollout A schema element (which could be an index or column, although in the rest of this post we’ll focus on the case of an index) has associated data that can be deleted, written, or read by the SQL DML commands (e.g., INSERT, UPDATE, DELETE, SELECT). The strategy CockroachDB uses to rollout a new index involves granting delete, write, and read capabilities to these DML commands, one after the other in a staged manner rather than all at once. Adding a new index follows these discrete stages explained later: Grant delete capability. Grant write capability. Backfill the index data. Grant read capability. A capability is granted along with all prior granted capabilities in a new version of the schema. For correctness, a new capability must be granted only after the entire cluster is using a schema containing all prior granted capabilities. The process therefore pauses before each stage, allowing prior granted capabilities to propagate to the entire cluster before the next one can be granted. Dropping an index runs the process in reverse: Revoke read capability. Revoke write capability. Purge the index data. Revoke delete capability. Similarly, a capability is revoked only after the previous revocation has propagated across the entire cluster. Delete capability: Avoid spurious index entries This capability is granted by placing the index in the DELETE_ONLY state. Armed with this capability the SQL DML commands limit themselves: DELETE is fully functional deleting the row along with the underlying index data. UPDATE deletes the old index entry, but limits itself to not write a new one. INSERT and SELECT ignore the index. A node granted the write capability for an index at the next stage can trust the entire cluster to be using the delete capability for the index. Thus, when it receives an INSERT command and inserts an index entry for a row, another node with only the delete capability on seeing a DELETE command for the same row will correctly delete the index entry for the row. The index will avoid getting poisoned with a dangling index entry. When a schema change drops an index, the associated index data is purged only after the successful withdrawal of the write capability across the cluster; the entire cluster has only the delete capability allowing a safe purge. Write capability: Avoid missing index entries This capability is granted by placing the index in the WRITE AND DELETE_ONLY state, granting both delete and write capabilities. INSERT, UPDATE, and DELETE commands function normally, adding or deleting index entries as needed. SELECT, on the other hand, requires the read capability and ignores the index. The index backfill runs only after the entire cluster is capable of writing. An INSERT command received on any node while a backfill is in progress creates a new row with a legitimate index entry, and doesn’t depend on the separate backfill process to create an index entry for the row. There will be no missing index entries after the backfill is complete. Read capability This last capability is granted by making the index active, and turns on the full use of the index by all commands. Fast schema rollout Between each stage of the schema change process, the entire cluster is allowed to converge to the newest version of a table schema. A simple schema caching mechanism might use a 5 minute TTL, but that forces the schema change process to wait for many minutes before trusting that the latest version is the sole one active and a capability fully granted/revoked. In CockroachDB, we developed leases on schema versions to speed up cluster convergence to the latest schema version, which in turn accelerates the schema change process. Before using a table schema for a SQL DML command, the node running the command obtains a read lease for the schema valid for a significant period of time (on the order of minutes). When an updated table schema version becomes active, it is broadcast across the cluster, alerting nodes to the new version and causing them to actively release their leases on the old version. If some nodes are unhealthy and do not actively release their leases, the rollout will wait for the leases to expire and will simply get delayed. The schema change strategy is kept simple by having the leases follow two rules: New leases are granted only on the latest schema version. Valid leases are only on the two latest schema versions. For a more detailed discussion on table leases, check out this RFC on our Github repo. Reliable schema rollout The schema change is guided to completion by the node executing the schema change SQL command. Since schema change operations are potentially long-running, they need to be restartable when the node performing them dies. Every node runs a schema changer goroutine capable of running any incomplete schema change. Before executing a schema change, a schema changer goroutine first acquires an exclusive write lease on the table, giving it license to be the only one guiding the schema change. Conclusion In CockroachDB, online schema changes are are easy to use, and the schema change rollout process is safe, fast, and reliable. Change is inevitable, and it’s time that you not worry about it! We’d like to thank the folks from the F1 team at Google for publishing a similar implementation for online schema changes from which we gained much of the inspiration. [^1]: Apptree. Stack Overflow. “ Modifying columns of a very large mysql table with little or no downtime .” August 26, 2010.", "date": "2016-01-20"},
{"website": "CockroachLabs", "title": "How to Optimize Garbage Collection in Go", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/how-to-optimize-garbage-collection-in-go/", "abstract": "When we shared a post a few weeks back about why we chose Go for CockroachDB, we received a number of questions about how we deal with some of Go’s known issues, specifically those related to performance, garbage collection, and deadlocks. In this post, we’ll share a few powerful optimizations that mitigate many of the performance problems common to Go’s garbage collection (we will cover “fun with deadlocks” in a follow-up). In particular, we’ll share how embedding structs, using sync.Pool, and reusing backing arrays can minimize memory allocations and reduce garbage collection overhead. Minimizing memory allocation & optimizing garbage collection Something that sets Go apart from, say, Java, is that Go gives you the ability to manage your memory layout. With Go, you can combine things that would be separate allocations in other garbage collected languages. Take a look at the snippet below, which is a bit of code from CockroachDB that reads data from disk and decodes it: metaKey := mvccEncodeMetaKey(key)\nvar meta MVCCMetadata\nif err := db.GetProto(metaKey, &meta); err != nil {\n    // Handle err\n}\n...\nvalueKey := makeEncodeValueKey(meta)\nvar value MVCCValue\nif err := db.GetProto(valueKey, &value); err != nil {\n    // Handle err\n} In order to read the data, we’ve performed 4 allocations: the MVCCMetadata structure, the MVCCValue structure, and two keys. Go gives us the ability to reduce this to a single allocation by bundling the structures together and preallocating space for the keys. type getBuffer struct {\n    meta  MVCCMetadata\n    value MVCCValue\n    key   [1024]byte\n}\n\nvar buf getBuffer\nmetaKey := mvccEncodeKey(buf.key[:0], key)\nif err := db.GetProto(metaKey, &buf.meta); err != nil {\n    // Handle err\n}\n...\nvalueKey := makeEncodeValueKey(buf.key[:0], meta)\nif err := db.GetProto(valueKey, &buf.value); err != nil {\n    // Handle err\n} Here we declare a type getBuffer , which includes two different structs inside it: MVCCMetadata and MVCCValue (both protobuf objects). The third member is an array, which you don’t see in Go as often as you see slices. When you have an array of a fixed size (1024 bytes), it can be done directly in the struct without requiring an extra allocation. This allows us to embed three objects in the getBuffer structs, reducing our allocations from four to one. Note we reuse the array for both keys which is fine in this usage as the keys are not used simultaneously. We’ll return to the array later. sync.Pool: var getBufferPool = sync.Pool{\n       New: func () interface{} {\n              return &getBuffer{}\n       },\n} Truth be told, it took us a while to figure out what sync.Pool was actually for and why we would want to use it. It’s a free list that reuses allocations between garbage collection cycles, so that you don’t have to allocate another object that’s going to have to be collected by the garbage collector later. Each time a garbage collection cycle starts, it clears items out of the pool. An example of how to use sync.Pool: buf := getBufferPool.Get().(*getBuffer)\ndefer getBufferPool.Put(buf)\n\nkey := append(buf.key[0:0], …) First you declare a global sync.Pool object with a factory function, which in this case allocates a getBuffer struct and returns it. Instead of making a new getBuffer, we can get one from the pool. Pool.Get returns an empty interface, which we then type assert to the correct pointer type. When we’re done with it, we put it back in the pool. The end result is that we don’t have to do even the one allocation to get the Buffer struct. Arrays & Slices One thing worth noting is that arrays and slices are distinct types in Go, and that nearly everything deals in slices rather than arrays. You can get a slice from an array just by using the square bracket syntax [:0] key := append (buf.key[0:0], …) This creates a zero-length slice backed by an array. The fact that this slice already has a backing storage behind it means that any appends will actually go into that array instead of creating a new allocation. So when we are decoding a key, we can append it to a slice created out of this buffer. As long as that key is less than 1 KB, we don’t have to allocate anything. It just reuses the array that we already allocated. In the case of a key over 1 KB, which is possible but less common, it transparently allocates a new backing array, and our code doesn’t have to be aware of it. Gogoprotobuf vs Google protobuf And finally, we use protocol buffers to store everything on disk. However, instead of using Google’s own protobuf library, we use a fork of it called gogoprotobuf which we highly recommend. Gogoprotobuf follows many of the principles we outlined above to avoid unnecessary allocations. In particular, it allows marshalling into a byte slice which can be backed by an array to avoid allocations. Further, the non-nullable annotation allows embedding a message without an allocation, which is useful when the embedded message is known to always be present. A final bit of optimization with gogoprotobuf is to use the generated marshalling and unmarshalling routines, which provide a nice performance boost over the reflection-based marshalling and unmarshalling that are present in the standard Google protobuf library. Wrapping Up By combining the above techniques, we have been able to minimize the performance overhead of Go’s garbage collection and optimize for better performance. As we approach beta and focus more heavily on memory profiling, we’ll share our results in a follow-up post. And of course if you’ve learned other performance optimizations for Go, we’re all ears. Illustration by Mei-Li Nieuwland", "date": "2015-11-23"},
{"website": "CockroachLabs", "title": "Can a 4-Day Work Week Work?", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/can-a-4-day-work-week-work/", "abstract": "Note: As of May 4, 2021, this post is under active revision to bring it up to date with the current practices at Cockroach Labs. We will revise this note as soon as the post is brought up to date. Thank you! ------ Collectively, the founding team at Cockroach Labs has been involved in the early stages of nearly ten companies, covering the full spectrum of outcomes. And while a great culture will not guarantee a successful company, there’s no question that both the rides into outer space and the crash landings are respectively more fun and a lot less painful when a company is built on a foundation of trust, respect, balance, and of course friendship. When Ben, Peter, and I first agreed we’d take the plunge and build another company, we were determined to make culture a founding priority instead of an afterthought. Even before we’d found office space, or incorporated, we drew up a list of steps we’d take to build a lasting cultural foundation — something we hoped would make Cockroach Labs a great place to work and, by extension, increase its chances of success. Those cultural benefits and values are summarized on our website and run the gamut from flexible working hours, unlimited vacation, weekly team dinners , monthly offsites, twice-yearly company trips , and our version of a 4-day work week: “Free Fridays.” But when we recently conducted an employee engagement survey, it became clear that Free Fridays were confusing. There was anxiety about using them for any purpose other than core job responsibilities. Would it look bad if you never made it to the office? Were non-CockroachDB side projects fair game? Was it necessary to let your manager know your plans? Why Invest in Free Fridays? To understand why we instituted them in the first place, consider what sort of product we’re building. CockroachDB requires sustained abstract thinking, the kind of work better facilitated by a clear and relaxed mind, rather than 14-hour work days and lots of caffeine. But the exigencies of a modern, bog standard workday – commutes, meetings, email overload, endless social media cues – make finding time to think expansively a wistful pipe dream. Free Fridays are meant to be a new kind of day that breaks the simple dichotomy most of us have lived our entire lives. They’re neither strictly for working nor strictly the start of the weekend. Waking up on a Friday as an employee at Cockroach Labs should feel a lot more like that special Saturday morning realization that you don’t have to rush into the daily grind. Turn off the alarm Thursday night. Sleep in. Brew coffee and enjoy a lazy morning in a sunny kitchen. Spend time with the kids and walk them to school. Be there after school to watch soccer practice. See the doctor, get a haircut, run errands which are impossible to do on busy weekends. One of the greatest things about Free Fridays is you get to move through the normal world in an unconventional way. But there’s more to it. The reality is that Cockroach Labs is a group of curious, very bright people with lots of ideas. Turns out many of us appreciate a chance to leisurely experiment, to build prototypes of functionality far off the product roadmap, and to approach high priority work from a more relaxed and expansive perspective. Most people reading this are aware of Google’s 20% time policy, which was touted in the press, but became increasingly rare as the company grew. In the early days some pretty cool things came out of the policy, and we’d like to capture our share of those good outcomes. Free Fridays is a 20% time policy where the “when?” is fully specified: Fridays, all day. The “why?” has hopefully been answered in the preceding paragraphs. The “what?” is up to each of us individually, and there aren’t many wrong answers. A few more points: don’t tell anyone you’re working from home or not coming in to run errands; just do it. Don’t answer emails unless you want to. Don’t hesitate to get on a plane Thursday night and ignore work all weekend. Do be creative, do work on what you imagine the next killer feature might be. Do embrace opportunities for greater balance and personal happiness. We’re going to start a series of posts which cover activities we’ve collectively pursued on Free Fridays. You can follow the Flex-Fridays tag here. We hope the format yields exciting and interesting results. We’ll also do quarterly surveys and assess how this benefit affects employee satisfaction. Free Fridays is an experiment, not something written in stone. If it doesn’t work or doesn’t scale, we may abandon it. We’re eleven months into it, and cautiously optimistic.", "date": "2016-01-27"},
{"website": "CockroachLabs", "title": "Adventures in Performance Debugging", "author": ["Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/adventures-performance-debugging/", "abstract": "As we’ve built CockroachDB, correctness has been our primary concern. But as we’ve drawn closer to our beta launch, we’ve had to start paying significantly more attention to performance. The design of CockroachDB always kept performance and scalability in mind, but when you start measuring performance, there are inevitably surprises. This is the story of the detection, investigation, and fix of just one performance bug . First, a little context about CockroachDB for those new to the project. CockroachDB is a distributed SQL database which employs RocksDB to store data locally at each node. A basic performance test is to write random data into a table and test how many operations per second can be achieved. This is exactly what the block_writer example program does. It uses a simple table schema: CREATE TABLE IF NOT EXISTS blocks (\n    block_id BIGINT NOT NULL,\n    writer_id STRING NOT NULL,\n    block_num BIGINT NOT NULL,\n    raw_bytes BYTES NOT NULL,\n    PRIMARY KEY (block_id, writer_id, block_num)\n) And then spawns a number of workers to insert data into the table: INSERT INTO blocks (block_id, writer_id, block_num, raw_bytes) \n    VALUES ($1, $2, $3, $4) The block_id is randomly chosen and writer_id is uniquely assigned to each worker. The block_num field is monotonically increasing and ensures that there will never be duplicate rows inserted into the table. The effect is that we’re inserting random rows into the table and never experiencing contention. What could go wrong? 1. The Bug: Rapid performance deterioration A few weeks ago my colleague Matt Tracy ran the block_writer and discovered rapid performance deterioration: 1s: 1944.7/sec\n    2s: 1067.3/sec\n    3s:  788.8/sec\n    4s:  632.8/sec\n    5s:  551.5/sec\n…\n  1m0s:  105.2/sec Oh my, that isn’t good. Performance starts out at a reasonable 2000 ops/sec, but quickly falls by a factor of 20x. Matt suspected there was some scalability limitation with tables. He noted that once a table fell into this bad performance regime it stayed there. But if he dropped the blocks table and created a new one, performance reset only to degrade rapidly again. Like any good engineer, Matt turned to cpu profiling to try and determine what was going on. Was there some loop with horrible algorithmic complexity based on the table size? Unfortunately, the profiles didn’t reveal any culprits. Most of the cpu time was being spent inside RocksDB, both during the good performance regime and the bad performance regime. The builtin Go profiling tools are quite good, but they are unable to cross the cgo boundary (RocksDB is written in C++). 2. Snowball tracing to the rescue Matt was a bit stumped for how to proceed at this point. Conveniently, another engineer, Tobias Grieger, was experimenting with adding “snowball” tracing to SQL queries. Unlike sampling-based profilers which periodically stop a program and determine what code is running, a tracing system records implicit or explicit events associated with a specific request. The support Tobias was adding was a new EXPLAIN (TRACE) mode. After the addition of some more tracing events, here is what Matt saw: EXPLAIN (TRACE) INSERT INTO \nblocks (block_id, writer_id, block_num, raw_bytes) \nVALUES (1, 100, 1, ‘’)\n…\n92.947µs    |   9 | node      | execute\n3.653µs     |  10 | node      | executing\n3.129µs     |  11 | node      | BeginTransaction\n2.088µs     |  12 | node      | Transaction\n4.573606ms  |  13 | node      | Transaction was not present\n9.721µs     |  14 | node      | checksum\n417ns       |  15 | node      | Got put buffer\n7.501µs     |  16 | node      | mvccGetMetadata\n2.847048ms  |  17 | node      | mvccGetMetadata\n660ns       |  18 | node      | getMetadata\n12.128µs    |  19 | node      | Put internal\n352ns       |  20 | node      | Wrote transaction\n2.207µs     |  21 | node      | command complete\n5.902µs     |  22 | node      | executing\n30.517µs    |  23 | node      | Got put buffer I’ve edited the output for clarity and highlighted two lines which shed light on the problem. It should be clear that you can’t achieve 2000 ops/sec, or 1 op every 0.5 ms, if part of the time to perform an operation takes >7ms. It is interesting that this time is being consumed in writing the transaction record at the start of a transaction. Matt continued to add more instrumentation until the problem was narrowed down to a single RocksDB operation. At this point Matt tagged me in since I’ve had the most experience with our usage of RocksDB. I came onto the field swinging and wrote a micro-benchmark that duplicated the behavior of the BeginTransaction and utterly failed to find any performance degradation. Hmm, curious. I decided to verify I could reproduce the block_writer performance degradation (trust, but verify) and, of course, the problem reproduced immediately. I also verified that checking to see if the transaction record was present at the start of a transaction was the time consuming operation. 3. RocksDB, CockroachDB, and MVCC Now to provide a bit of background on CockroachDB’s MVCC (multi-version concurrency control) layout and transactions. In an MVCC system data is never overwritten; instead it is tagged with a version number (in CockroachDB this is a timestamp). Newer data “shadows” older data, but that older data is still present and is deleted via a background garbage collection process. What does this look like? Consider we have a key a at the timestamps 1 and 2 and key b at timestamp 3: a@2\na@1\nb@3 Notice that the keys are sorted by descending timestamp. This ordering plays nicely with the RocksDB iterator abstraction. There is also a special timestamp used for keys which hold info about an in-process write to a key. These “write intents” have a timestamp which sorts before any other timestamp. Let’s say there’s a write intent on key b: a@2\na@1\nb@intent\nb@3 If we want to find the latest version of key b we can seek the iterator to b@intent . Iterator seeking is defined to return the next key greater than or equal to the seek key. So by seeking to b@intent we’ll either find b@intent (if it exists) or b@3 or possibly c@ if there are no versions of b . Lastly, when a key is deleted a deletion marker is added: a@4(deleted)\na@2\na@1\nb@intent\nb@3 4. Dead ends: bloom filters and skip lists Ok, back to the performance degradation. We know that a RocksDB iterator seek of txn-key@intent is taking milliseconds to return. My mental model says this should be a very quick operation. Let’s figure out why it isn’t. The first thing I checked was whether the RocksDB bloom filters were being utilized. Bloom filters are a probabilistic data structure that can determine whether an element is a member of a set. When starting a transaction, the transaction key has not been seen before, so the bloom filters should be able to quickly reject any more expensive operation. When I turned on the logging of the bloom filter hit rate, I saw that the bloom filters were not being used at all. Huh? I scratched my head for a bit before turning on the logging for sstable creation, which is the structure where the bloom filter resides. Hmm, no sstables are being generated. This meant that the performance degradation was happening while all the data still resided in memory and before anything had been flushed to disk. This points the finger at the RocksDB MemTable which holds recent writes and periodically flushes them to disk. Lookups in the MemTable should be very fast. The MemTable is implemented using a skip list – a neat data structure similar to a b-tree but that can be implemented in a lock-free manner. When you see “skip list” you should be thinking “fast, ordered data structure”. How can the seeks within the skip list be taking milliseconds? A quick bit of instrumentation in SkipListRep::Iterator::Seek absolved it of blame. Individual seeks within the skip list were taking a handful of microseconds which was in line with my expectations. 5. Deletion tombstones A quick recap of where we are at: We have only a single memtable containing all of the data. An iterator into this memtable is sometimes taking milliseconds to perform a seek operation. The memtable is implemented on top of a skip list and seeking inside the skip list is taking microseconds. The memtable and skip list are both concurrent for readers and no locks are being taken. So where is the time going? This is the easy part of performance debugging. We have a reproducible test case and we have timing instrumentation which shows the problem at one level of the code and not at another. Ideally we’d perform a binary search of the code in between, narrowing our timing instrumentation until we identified by the culprit. Unfortunately, we have no easy tools to perform such a search so we have to do it manually. I won’t bore you with the specifics of the binary search and instead take you directly to the culprit: deletion tombstones. What is a deletion tombstone? RocksDB is a log-structured merge-tree . Unlike a b-tree where mutations are performed “in-place”, a log-structured merge-tree performs edits similar to the way an accountant maintains finances: by adding records to indicate changes to the past instead of rewriting history. In RocksDB, when you overwrite a key you don’t actually delete the old key but instead add a record for the new key that has a newer timestamp (sequence number) that the old key. When you delete a key you add a record (a deletion tombstone) that shadows the existence of the old key. The processing of deletion tombstones in RocksDB happens during the seek within the memtable. In pseudo-code this looks like: for iter = skiplist.seek(key); !iter.done(); iter.next() {\n  if !isDeletionTombstone(iter) {\n    break\n  }\n} Instrumentation revealed that the above loop in RocksDB (located in DBIter::FindNextUserEntryInternal ) was iterating upwards of 20,000 times when performance deterioration became noticeable. 6. The full story and the fix The block_writer tool writes random rows in a table. The performance of writing the transaction record associated with each row write degrades rapidly. Transaction records are written at a key derived from the first key written within the transaction and that key is deleted when the transaction is committed. So during random insertion into a table we have a segregated part of the key space where we’re randomly inserting keys and almost immediately deleting them creating a large expanse of deletion tombstones. Before inserting the transaction key we’re seeking within this deletion tombstone expanse and the seek needs to iterate from the (non-existent) transaction key to the next non-deleted key. For example, let’s say we’re creating txn100 (txn’s have a UUID associated with them, I’m using sequential numbers here for conciseness) and we have the existing set of keys: txn99@T(deleted)\ntxn101@T(deleted)\ntxn102@T(deleted)\ntxn103@T(deleted)\n...\ntxn40000@T(deleted) We can very quickly seek to find txn101@deleted but then we have to iterate until we find the next non-deleted key. It should be fairly clear that performance will quickly be dominated by that iteration. So seeking into a memtable with a large contiguous region of deleted keys is a bad idea. What can we do to fix the problem? It turns out that RocksDB has a solution readily at hand: when you create an iterator you can specify an upper-bound past which the iterator will not iterate (see rocksdb::ReadOptions::iterate_upper_bound ). CockroachDB’s MVCC layer was already indicating when it wanted to restrict iteration to a specific range of keys so it was straightforward to modify the code to allow specification of the iteration upper-bound. Recall the performance we were seeing earlier: 1s: 1944.7/sec\n    2s: 1067.3/sec\n    3s:  788.8/sec\n    4s:  632.8/sec\n    5s:  551.5/sec\n...\n  1m0s:  105.2/sec After the fix, we see: 1s: 3028.0/sec\n    2s: 2929.2/sec\n    3s: 2742.2/sec\n    4s: 2921.9/sec\n    5s: 2859.4/sec\n…\n  1m0s: 2579.0/sec Still a bit of a fall-off, but nothing close to the 20x decrease we were seeing before. That concludes our performance debugging adventure. It’s interesting that not only did the fix improve stability of performance in this benchmark, but it also improved top end performance on the benchmark and performance on a number of other benchmarks that created similar swaths of deletion tombstones.", "date": "2016-03-11"},
{"website": "CockroachLabs", "title": "How CockroachDB Does Distributed, Atomic Transactions", "author": ["Matt Tracy"], "link": "https://www.cockroachlabs.com/blog/how-cockroachdb-distributes-atomic-transactions/", "abstract": "Editor's Note - April 23, 2021: This article was written in 2015 when CockroachDB was pre-beta. The product has evolved significantly since then. We will be updating this post to reflect the current status of CockroachDB. In the meantime, the transaction section of the Architecture Document provides a more current description of CockroachDB's transaction model. --------------- One of the headline features of CockroachDB is its full support for ACID transactions across arbitrary keys in a distributed database. CockroachDB transactions apply a set of operations to the database while maintaining some key properties: Atomicity, Consistency, Isolation, and Durability (ACID). In this post, we’ll be focusing on how CockroachDB enables atomic transactions without using locks. Atomicity can be defined as: For a group of database operations, either all of the operations are applied or none of them are applied. Without atomicity, a transaction that is interrupted may only write a portion of the changes it intended to make; this may leave your database in an inconsistent state. Strategy The strategy CockroachDB uses to provide atomic transactions follows these basic steps: Switch : Before modifying the value of any key, the transaction creates a switch, which is a writeable value distinct from any of the real values being changed in the batch. The switch cannot be concurrently accessed – reads and writes of the switch are strictly ordered. The switch is initially “off,” and it can be switched to “on.” Stage : The writer prepares several changes to the database, but does not overwrite any existing values; the new values are instead staged in proximity to the original values. Filter : For any key with a staged value, reads for that key must check the state of the transaction’s switch before returning a value. If the switch is “off,” the reader returns the original value of the key. If the switch is “on,” the reader returns the staged value. Thus, all reads of a key with a staged value are filtered through the switch’s state. Flip : When the writer has prepared all changes in the transaction, the writer flips the switch to the “on” position. In combination with the filtering, all values staged as part of the transaction are immediately returned by any future reads. Unstage : Once a transaction is completed (either aborted or committed), the staged values are cleaned up as soon as possible. If the transaction succeeded, then the original values are replaced by the staged values; on failure, the staged values are discarded. Note that unstaging is done asynchronously and does not need to have finished before the transaction is considered committed. The Detailed Transaction Process Switch: CockroachDB Transaction Record To begin a transaction, a writer first needs to create a transaction record . The transaction record is used by CockroachDB to provide the switch in our overall strategy. Each transaction record has the following fields: A Unique ID (UUID) which identifies the transaction. A current state of PENDING , ABORTED , or COMMITTED . A cockroach K/V key . This determines where the “switch” is located in the distributed data store. The writer generates a transaction record with a new UUID in the PENDING state. The writer then uses a special CockroachDB command BeginTransaction() to store the transaction record. The record is co-located (i.e. on the same nodes in the distributed system) with the key in the transaction record. Because the record is stored at a single cockroach key, operations on it are strictly ordered (by a combination of raft and our underlying storage engine). The state of the transaction is the “on/off” state of switch, with states of PENDING or ABORTED representing “off,” and COMMITTED representing “on.” The transaction record thus meets the requirements for our switch. Note that the transaction state can move from PENDING to either ABORTED or COMMITTED , but cannot change in any other way (i.e. ABORTED and COMMITTED are permanent states). Stage: Write Intents To stage the changes in a transaction, CockroachDB uses a structure called a write intent . Any time a value is written to a key as part of a transaction, it is written as a write intent. This write intent structure contains the value that will be written if the transaction succeeds. The write intent also contains the key where the transaction record is stored. This is crucial: If a reader encounters a write intent, it uses this key value to locate the transaction record (the switch). As a final rule, there can only be a single write intent on any key. If there were multiple concurrent transactions, it would be possible for one transaction to try to write to a key which has an active intent from another transaction on it. However, transaction concurrency is a complicated topic which we will cover in a later blog post (on transaction isolation); for now, we will assume that there is only one transaction at a time, and that an existing write intent must be from an abandoned transaction. When writing to a key which already has a write intent: Move the transaction record for the existing intent to the ABORTED state if it is still in the PENDING state. If the earlier transaction was COMMITTED or ABORTED , do nothing. Clean up the existing intent from the earlier transaction, which will remove the intent. Add a new intent for the concurrent transaction. Filter: Reading an Intent When reading a key, we must follow principle 3 of our overall strategy and consult the value of any switch before returning a value. If the key contains a plain value (i.e. not a write intent), the reader is assured that there is no transaction in progress that involves this key, and that it contains the most recent committed value. The value is thus returned verbatim. However, if the reader encounters a write intent, it means that a previous transaction was abandoned at some point before removing the intent (remember: we are assuming that there is only one transaction at a time) . The reader needs to check the state of the transaction’s switch (the transaction record) before proceeding. Move the transaction record for the existing intent to the ABORTED state if it is still in the PENDING state. Clean up the existing intent from the earlier transaction, which will remove the intent. Return the plain value for the key. If the earlier transaction was COMMITTED , the cleanup operation will have upgraded the staged value to the plain value; otherwise, this will return the original value of the key before the transaction. Flip: Commit the Transaction To commit the transaction, the transaction record is updated to a state of COMMITTED . All write intents written by the transaction are immediately valid: any future reads which encounters a write intent for this transaction will filter through the transaction record, see that it is committed, and return the value that was staged in the intent. Aborting a Transaction A transaction can be aborted by updating the state of the transaction record to ABORTED . At this point, the transaction is permanently aborted and future reads will ignore write intents created by this transaction. Unstage: Cleaning up Intents The system above already provides the property of atomic commits; however, the filtering step is expensive, because it requires writes across the distributed system to filter through a central location (the transaction record). This is undesirable behavior for a distributed system. Therefore, after a transaction is completed, we remove the write intents it created as soon as possible: if a key has a plain value without a write intent, read operations do not need to be filtered and thus complete in a properly distributed fashion. Cleanup Operation The cleanup operation can be called on a write intent when the associated transaction is no longer pending. It follows these simple steps: If the transaction is ABORTED , the write intent is removed. If the transaction is COMMITTED , the write intent’s staged value is converted into the plain value of the key, and then the write intent is removed. The cleanup operation is idempotent; that is, if two processes try to clean up an intent for the same key and transaction, the second operation will be a no-op. Cleanup is performed in the following cases: After a writer commits or aborts a transaction, it attempts to clean up every intent it wrote immediately. When a write encounters another write intent from an earlier transaction. When a read encounters a write intent from an earlier transaction. By aggressively cleaning up expired write intents through multiple avenues, the necessary performance impact of filtering is minimized. Wrap Up With that, we have covered CockroachDB’s basic strategy for ensuring the atomicity of its distributed, lockless transactions. But there’s more to the story than just what I’ve covered here. CockroachDB supports concurrent transactions which may write to overlapping sets of keys. Allowing overlapping, concurrent transactions is the “I” in ACID, which guarantees isolation. We’ll cover the details of how we accomplish isolation in a future post . Stay tuned! Are distributed transactions your jam? Our engineering team is hiring! Check out our open positions here .", "date": "2015-09-02"},
{"website": "CockroachLabs", "title": "Efficient Documentation Using SQL Grammar Diagrams", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/efficient-documentation-using-sql-grammar-diagrams/", "abstract": "As CockroachDB approaches beta, user documentation has become increasingly important, and one of the meatiest requirements is documentation of our SQL implementation. For inspiration, I researched how other databases have documented SQL. The most effective example I found was SQLite’s grammar diagrams . Figure 1: Example of the alter table statement in SQLite’s grammar diagrams. These diagrams feature easy-to-understand railroad diagrams showing the possible options for a SQL statement. Compared to a text representation , these visual diagrams give users an intuitive way to explore the grammar and discover features. Figure 2: Example of SQL documentation as represented by text. Converting Grammar into Images with Yacc and EBNF There are various programs that can take a well-specified grammar file and convert it into images. Of the ones I saw, I was most impressed with the Railroad Diagram Generator . It produces linked SVG images that can easily be embedded into a web page and manipulated. However, this generator requires input in EBNF form. The CockroachDB grammar is defined in a yacc file, from which the yaac program generates source code that parses SQL. As yacc has a specified format, it is straightforward to parse and convert to EBNF. One program that does this is yyextract from the cutils package on many Linux distributions. yyextract produces just BNF files. But with some short regexes, it was possible to convert our sql.y into a valid EBNF file that the generator could understand. Figure 3: Grant statement represented through a SQLite grammar diagram. Inlining and Simplification AKA Documentation for Humans With the proof-of-concept complete, I had much more work left to make these diagrams useful to humans. We now had one huge HTML page with every possible option, but what we really needed was something similar to what SQLite provides: a single image that displays top-level, useful information with options to go deeper. Taking ALTER TABLE as an example, it was clear where this would get tricky. ALTER TABLE contains a reference to alter_table_cmds , which allows any number of alter_table_cmd references separated by commas. That’s at least three different statements just to figure out what ALTER TABLE can do. Instead of clicking through to each of those, the useful ones should be inlined into the top ALTER TABLE statement. That is, instead of a referencing other statements, they should be included directly. I accomplished this by writing my own parser for EBNF, parsing the output of yyextract , modifying it, and then feeding it into the diagram generator. This reduced the depth of the statements and made them much more usable. I worked in other helpful simplifications as well. For example, I used a simplification rule to convert awkwardly-defined lists into a nice form with a feedback loop. Figure 4: Alter table commands represented using SQLite grammar diagram. However, there are other simplifications I would still like to implement. For example, many statements have IF EXISTS expressions. Currently, these statements have two expressions: one with and one without the IF EXISTS clause. A simplification that combines these two expressions into one would further reduce the complexity of some diagrams. How to Diagram Unimplemented SQL Statements As CockroachDB is a new project, many esoteric or difficult parts of the full SQL grammar are not yet implemented. We allow for them in our parser, but they will always produce an error describing them as unimplemented. We want our documentation to be accurate and concise, not cluttered with notes about whether something displayed works or not, so we want to filter unimplemented expressions out of our generated diagrams. The yyextract tool used in the initial proof-of-concept outputs all of the parsing rules listed in the sql.y file, but not their implementations (or lack thereof). Thus, we needed a yacc parser that allows us to fully inspect the grammar. I was not able to find a Go package that could successfully parse our sql.y file. The Go tool itself has a yacc parser and generator , but it is translated from a C program and was not built for this kind of inspection. Yacc is not a complicated language, so it made some sense to build a custom parser ourselves. I used the Go text/template/parse package as a boilerplate, and modified it to produce a yacc AST. With the parsed yacc file in memory, it was possible to remove any expressions that were marked unimplemented as well as statements containing only unimplemented expressions. Summary These tools allow us to automatically generate all of the SQL diagrams in our documentation. We have a document describing the full grammar , as well as smaller pages listing single statements . All diagrams link references to the full grammar, making it simple to explore. The code for this is in our documentation repository . Now, anytime we modify the SQL grammar to add a new feature, all the diagrams can be regenerated with a single command. Figure 5: CockroachDB’s SQL grammar represented using SQL grammar diagrams. (Also posted at Matt Jibson’s blog .)", "date": "2016-03-16"},
{"website": "CockroachLabs", "title": "CockroachDB Skitters into Beta", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-skitters-beta/", "abstract": "``` We introduced Cockroach Labs last June with a simple yet ambitious mission: Make Data Easy . ``` We’ve spent the intervening months moving CockroachDB from an alpha stage product to launching CockroachDB beta. In the process, the team has nearly tripled in size and development has accelerated to a blistering pace. We’ve supplemented our original investment led by Peter Fenton of Benchmark with an additional round of funding, led by Mike Volpi of Index Ventures. We’re lucky to also count GV (formerly Google Ventures), Sequoia, FirstMark, and Work–Bench as investors. Why CockroachDB? It’s bog standard in our industry for the data architecture you start with to remain the one you still have in five years, love it or hate it. In light of this, it makes sense to choose your initial architecture carefully as it can save tremendous resources down the road. Our goal in building CockroachDB was an open source database better suited to the fast-evolving challenges that companies will face over the next decade. We believe those needs encompass three crucial capabilities: scalability, survivability, and SQL, while always maintaining strong consistency. Scalability The data underlying most businesses continues to expand faster than traditional databases can keep up with. The challenge goes beyond what’s immediately obvious. It turns out that data has no trouble expanding to fill available capacity: most companies are busy collecting data on their data! With data growing faster than improvements to the underlying database hardware, horizontal scalability is a requirement. Survivability In today’s leveraged business environment, downtime has never been so expensive. A SaaS company with 500 enterprise customers experiencing a five minute outage is actually causing 2,500 minutes, or nearly two days of disruption. A database should survive even datacenter outages, and it should do so without manual intervention and with perfect data fidelity (strong consistency). SQL SQL is the lingua franca of the database world. Or at least it was for about 35 years until 2004, when Google announced a new database called BigTable which eschewed the old standards in pursuit of simplicity and scalability. BigTable and the many databases which followed in its footsteps never envisioned or agreed on a consistent API for developers. It’s our industry’s own tower of Babel, with similar results. But providing a common standard is not all SQL is good for, especially as project complexity increases. Developers need transactions and well-defined schemas, and while many database users are able to write SQL queries, they may not be able to program map reduces. SQL has been getting the job done for 45 years now, it’s widely understood, and it’s a platform in its own right, with a substantial ecosystem of tools and educational resources built around it. For these reasons, we believe it’s the most productive API, and our ongoing challenge is to make it work well enough to justify that claim. The Road to CockroachDB Beta In addition to our open source contributors, Cockroach Labs currently has more than 20 full time employees building CockroachDB. The system has been evolving so quickly that it’s become increasingly difficult to keep up . The project has rapidly matured in functionality, stability and performance over the past three quarters. Our original intent was to deliver the beta end of summer last year, as a transactional key-value store. We reconsidered after becoming convinced that SQL was a necessary part of CockroachDB’s identity. The consensus was that without it, developers would end up having to build too much of the missing functionality themselves. We also worried about missing the opportunity to precisely define CockroachDB, instead leaving our users with more questions than answers. Will it have SQL eventually? Is it NoSQL? Am I supposed to build my own indexes? The decision to include SQL in our beta release added two quarters to our timeline. And before you wonder how that’s possible in only two quarters, it’s important to note that we stopped short of supporting joins or of parallelizing the execution of distributed SQL queries. Nevertheless, the stage is set: we are a scalable SQL database. The beta SQL support is a functional and appropriate starting point. What Does “Beta” Mean? We’re deliberately announcing a beta for CockroachDB because nothing is better than supporting real world use cases to sharpen focus and efficiently direct resources. ”Beta” means software with more bugs and potential performance or stability issues. That’s a pretty good description of CockroachDB right now, and this is the expectation we’d like you to start with. However, the key differentiator of CockroachDB beta from our alpha release is a commitment that future changes will be backwards compatible. What’s in CockroachDB Beta? We’re excited to announce that this beta release contains nearly everything found in the original design document and many previously-unanticipated features, not least of which is the addition of SQL. Crucially, everything necessary to support scalability, survivability, and strong consistency is in place. The system self organizes without requiring external services, self heals as nodes are lost or damaged, and automatically rebalances to maintain equilibrium as new nodes are added. We provide distributed transactions with serializable and snapshot isolation and have implemented an online schema change system that allows indexes to be added without requiring any downtime or table locking. We’ve also added extensive support for keeping time series of metrics including op latency and counts, network and disk I/O, and host memory and cpu usage. We surface this information to operators through our fast-evolving administrative UI: One of the most notable features of CockroachDB is just how simple it is to deploy . CockroachDB is a single binary which requires only the location of one or more storage devices to manage. Starting a multi-node cluster is as simple as starting the first node on its own and then pointing each additional node at the first node or any other node which has already joined the cluster. There are no external dependencies required. No global configuration, no distributed file system, no bundle of resources or install scripts. There are no config files and we’ve strictly limited the available command line flags to those which are useful and not just tunable knobs for the sake of having knobs. Securing a cluster is similarly straightforward. What Comes Next? For those of us working on The Roach, this is where the fun truly begins. We’re proud to have implemented a design with such promise for building the next generation of products and services. We can’t imagine a future without a sane, scalable, and performant database solution, and we intend to build it. CockroachDB is meant to work as well for a three-node cluster as a Silicon Valley data farm, and to provide a straight path between the two. In the near future we’ll focus on expanding the SQL capabilities to include joins and distributed query execution; we’ll continue to add better production and administration features; and we’ll improve stability and performance. True success will mean CockroachDB joining the ranks of other open source platforms and geek household names like Postgres, MySQL, and Hadoop. We think we’re on to something here and can’t wait for new users to help shape the direction. Download the CockroachDB beta , deploy a test cluster , and build a test app . The team is committed to supporting new users and debugging issues as they arise, so please don’t hesitate to contact us with questions or concerns. The best way to ask questions in real time is in the CockroachDB Slack Community . If you’d like to file an issue or feature request, please use our GitHub issues . Happy Roaching!", "date": "2016-03-30"},
{"website": "CockroachLabs", "title": "Why Go Was the Right Choice for CockroachDB", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/why-go-was-the-right-choice-for-cockroachdb/", "abstract": "The first question many developers ask us is what our experience has been writing a distributed database in Go, a garbage-collected language. JVM garbage collection is notoriously expensive, so wouldn’t we be risking CockroachDB’s performance by building it in Go? The fact is, when you’re building a high performance, distributed system, you’ve only got a handful of languages to choose from, with C++, Java, and Go topping the list. Java’s known performance issues made it unappealing, and while many of us spent our careers developing in C++, the effort required to build our own libraries further complicated the already daunting task of writing a distributed database. Despite Go being a brand new language for nearly every developer on the project, including the founders, its support for libraries, interfaces, and tooling positioned it as the right choice for CockroachDB. Perhaps most telling that Go is a good fit is that a lack of previous exposure to the language has not been a barrier for contributors: Go is picked up quickly by anyone with Java or C++ experience. We now have 67 contributors to the project, and CockroachDB has gone from an empty Github project to 125,000 lines of non-generated Go code, with a smattering of C++ and .proto files. Managing code complexity is undeniably affected by the choice of language, and it’s especially important in an open source context. It’s difficult to quantify the effect on productivity that Go brings over C++ or even Java. Go was designed to scale to large code bases with an emphasis on simplicity and orthogonality of features. The enforced code style, the simple imports and automated import management, the wide variety of linters, the straightforward (and minimal) set of programmatic idioms…all of these attributes of Go are important for clean, understandable code. When comparing to Java, we appreciate the tight focus on implementation instead of OOP and abstraction: interfaces can be added when needed, not as an initial, often unnecessary, step. When comparing to C++, we appreciate the automatic memory management and how there’s rarely more than one way to get something done, for example with static and one-time initializers. We’ve made good use of channels for synchronization, although we’ll note there is art to using them effectively. What remains to be seen, of course, is how all of this Go code will perform. We are still building out core functionality in CockroachDB, so much of the performance profiling is yet to come. However, in our past experience, we ported a large system from Java to Go, which greatly decreased its memory footprint and garbage collection overhead. As we approach beta and focus more heavily on performance, we’ll share our results in a follow-up post.", "date": "2015-11-03"},
{"website": "CockroachLabs", "title": "DIY Jepsen Testing CockroachDB", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/diy-jepsen-testing-cockroachdb/", "abstract": "[As of February 23, 2017, CockroachDB Beta Passed Jespen Testing ] We at Cockroach Labs absolutely love Aphyr’s work. We are avid readers of the Jepsen series – which some know as a high quality review of the correctness and consistency claims of modern database systems, but which we really know as “Aphyr’s hunting tales about the highest profile bugs in our industry.” Most of us read each new blog entry with a mix of thrill, excitement, and curiosity about which new system will be eviscerated and which exotic error will be discovered next. Aphyr’s Jepsen posts have changed the dialogue about distributed data stores, placing correctness on equal footing with scalability and performance. (Peter Mattis, Co-Founder of Cockroach Labs) We are grateful for that. That the Jepsen tool box is open source should be an inspiration to the entire industry. Additional gratitude is owed to Aphyr for his accompanying series on how to use Clojure , without which we would not have been able to appreciate how Clojure so elegantly simplifies writing the Jepsen event generators . Seriously, wow. We’re looking forward to seeing CockroachDB put under Aphyr’s microscope one day, but for the time being we decided to apply the Jepsen testing tools ourselves. We are completely aware of the shortcomings of self-testing the correctness of a system we built ourselves. We aren’t fooling ourselves thinking we could do this and announce afterwards to the world “Look! Aphyr’s magic wand hasn’t found any bugs!” That’s not really how things go in our industry, now, is it? Independent validation and all that, yet as we’ll see in this post we found value in running Jepsen tests. And we learned so much in the process! It gave us both a boost in confidence and the opportunity to fix important issues in time for our Beta release. Contents First steps What’s in a transaction? The ACID model A simple consistency test: unique appends Snapshot isolation: banking transactions Client-side retries Intermezzo: Clojure’s threading gave us a cold sweat From snapshot isolation to serializable isolation From serializability to linearizability Snapshot isolation over multiple ranges? Linearizability on single ranges Woes with CockroachDB timestamps Linearizability vs. serializability Linearizability vs. network partitions Linearizability vs. clock skews More nemeses! Wrapping up First steps Our first steps using Jepsen in January and February 2016 were embarrassing, though you might find them entertaining. CockroachDB supports the same client protocol as PostgreSQL. Doing so allows us to use existing drivers written in almost every language. Unfortunately, our testing of these various drivers was not sufficient and, until two weeks ago, CockroachDB wasn’t smart enough to understand all the combinations of SQL and Postgres wire protocol messages generated by Clojure’s java.jdbc module. For example, we would try a straightforward JDBC connection set-up and send the most basic queries ; even this code would bluntly fail with a server crash , a SQL error , obscure network issues , and all sorts of bugs. We knew we would fix these errors (and we eventually did), but at that point we really wanted to get something working so that we could start doing consistency checks! So in January we got an absolutely ugly kludge running: we sent all the SQL queries over SSH to our command-line client which resulted in rather verbose Jepsen clients . Still, it worked! The latencies were not great, but it got us some results and provided motivation to improve our command-line SQL client. That said, SQL-over-SSH got tiring pretty fast, so we ended up fixing the relevant CockroachDB bugs and rewriting our test code to use plain JDBC instead. And we even got JDBC working with SSL too! Now our test framework supports running the same test queries against either a CockroachDB cluster, a single local CockroachDB instance, and even PostgreSQL to serve as reference . Once that worked, setting up CockroachDB became rather trivial: fire up a cluster and create the test database . What’s in a transaction? The ACID model The ACID model requires transactions to be Atomic, Consistent, Isolated and Durable. When you work on Jepsen, you usually only look at problems (like claims to serializability and linearizability) that involve a combination of A, C, and I in a distributed environment because you can assume the database engine already worked very hard on A, C, I, and D individually. After all, these engines have been around for a while and have plenty of users already who would have complained earlier if their transactions were not atomic or durable, or if they could see gross consistency anomalies. In contrast, CockroachDB is still rather young, so a few more ACID checks were warranted. We figured we could learn how to use Jepsen by implementing a few simple tests first. A simple consistency test: unique appends CockroachDB allows programmers to perform updates to the database from different nodes on the same tables. It claims that the effects of a transaction committed on one node are automatically visible from the other nodes. There are actually some underlying claims here: That a committed transaction is only committed once (if the client added 1 row, we don’t end up adding 2 rows in storage). This is part of the A in ACID.; That it actually remains committed, and its effects don’t get canceled. This is part of the D in ACID. You could say that if a database doesn’t get these right, then it’s not much of a database, and you would be correct. But it’s not so obvious how to achieve this in a distributed environment: if the node that processes a transaction sends the updated data to other nodes, and there are network disturbances during this propagation, you want to ensure there’s consensus at the end. If nodes can decide that other nodes are really responsible for storing the data, then there must be at least one that takes the responsibility so you don’t lose any data. To illustrate this, we devised a simple test. We used a table with a single int column representing an integer set, and then we added different values via different nodes. What we wanted to see is that we could 1) retrieve all the values at the end ( persistence ) and 2) observe that all the values were still unique . (This is the same sets-test that you can find in Aphyr’s code for testing Galera, RobustIRC and Percona; they were not written about on the blog, probably because they didn’t uncover anything unwieldy.) We first define a sets-client in charge of creating the table , responding to add events by adding the specified value in the table, and responding to read events to read the final state . Then we implemented a checker that verifies there are no duplicates and that no confirmed INSERT is missing from the final set. We also checked that we don’t see values that were never inserted. Finally we implemented a test definition which generates add events, spreads them across clients to different nodes, and issues a final read event [1] . It uses our checker defined above but also Jepsen’s convenient performance analysis checker which can emit latency graphs. As a final touch, CockroachDB indicates failure to commit a transaction as a SQL error, which is reported as an exception; a transaction may stall for a bit while resolving a conflict, and we’d rather let the test continue without waiting. So we wrap our client code in a try…catch block and a timeout checker to report these states properly when they occur. We ran this test and were happy to see no error (yay!) and also this beautiful latency distribution: This tells us that on this particular test, most requests were served in less than 6 milliseconds except for the final reads which took 40-60ms. Notes [1] The code actually issues 2 read events at the end and on each client, which may seem overkill at this point but is actually motivated by an unrelated issue which we describe later on. Snapshot isolation: pseudo-banking In our experience, programmers have a rather straightforward, simple, healthy, and usually correct expectation that transactions should be well isolated: while a complex transaction is ongoing, another transaction should not be able to observe an intermediate state of the database. Historically, database engines tried (and often failed) to solve this via locking, and this gave us a nightmare of silly transaction levels in the SQL standard. Meanwhile, modern databases have learned to use things in addition to locking, like Multi-Version Concurrency Control (MVCC) , which ensures that even the weakest isolation level at least caters to programmers’ basic expectations. Usually, this is called snapshot isolation , although that level is not part of the SQL standard. CockroachDB implements snapshot isolation (SI). (It also implements, and defaults to, the stronger serializable snapshot isolation (SSI), but let’s make small steps here.) We wanted to test snapshot isolation and verify that expected anomalies are found and unexpected ones are not. For this we reused a particular test from Aphyr’s previous experiments with Percona, Galera and Postgres RDS. Its model is a restricted, unrealistic [2] “bank” that maintains accounts on different rows of a table with a “balance” column. The clients issue money transfers from one account to another, and each transaction prevents itself from moving money if that leaves an account with a negative balance. Each transaction contains two SELECTs and two UPDATEs behind a condition. Without a proper implementation of snapshot isolation, several invalid scenarios could occur, for example: Account A starts at 50 Txn1 checks that 30 can be transferred from A to B, sees the transfer is possible Txn2 checks that 30 can be transferred from A to C, sees the transfer is possible Txn2 updates A and C to effect the transfer Txn1 goes through and updates A and B, sees (incorrectly) the new data for A and makes A negative. Or, it (incorrectly) blindly overwrites Txn1’s previous write to A with its own computed value, leaving A positive but accidentally creating extra money! We implemented this like in previous Jepsen tests. The client first instantiates a couple of accounts (rows) in the test table. Then it processes incoming read and transfer events using the model rules described above. Meanwhile, the event generator issues randomly distributed read and transfer events. We used only transfers between different accounts . Finally, the checker ensures that at every read operation, the sum of all balances is preserved and that no balance has turned negative [3] . We finished it by checking that no errors were detected (none were). Here you could (and perhaps should) stop us and shout: “Wait!” The code we wrote for this test’s client uses a macro with-txn which always uses the isolation level “serializable.” So even if “serializable” implies “snapshot,” we’re not really testing “snapshot”! Perhaps CockroachDB wouldn’t work properly if we ran the same test with transactions that use snapshot isolation explicitly? Do not worry, we tested that too, but we couldn’t do so easily with Jepsen: even though our dialect of SQL allows “SNAPSHOT” in the syntax, Clojure’s java.jdbc driver doesn’t know how to use it. It only knows about the standard SQL levels, unfortunately… So we wrote a piece of Go code that talks to our database directly to test this [4] , and there were no surprises. Notes [2] This “banking” test was designed by Aphyr so that it merely need snapshot isolation to maintain its model (positive balances, sum stays constant). This is not to say that snapshot isolation is sufficient to implement a real banking application! “Real” banking transactions usually require serializable isolation, for otherwise you cannot implement eg. correlated accounts (authorize transfer from one account only if the sum of this account and another is sufficient). [3] We noticed that the Jepsen test code didn’t actually check for negative balances, contradicting what has been previously written about it, so we fixed that. [4] Code left unpublished as an exercise to the reader. Client-side retries Let’s look at the latency distribution graph generated for the previous banking test: As you can see, there is some red in there. Errors? Uh-oh. Looking at the query error log we can see the errors correspond to occurrences of the following messages: ERROR: read at time 1456878225.401016975,0 encountered previous write with future timestamp 1456878225.401436392,33 within uncertainty interval\n\nERROR: retry txn \"sql/executor.go:597 sql implicit: false\" id=e3528ef3 key=/Table/51/1/120611882267213829/0 [...]\n\nERROR: txn aborted \"sql/executor.go:597 sql implicit: false\" id=693720a3 key=/Table/51/1/120611978022649859/2/1 [...]\n\nERROR: txn \"sql/executor.go:597 sql implicit: false\" id=45d793a1 key=/Table/51/1/120611888869081090/2/1 [...] failed to push \"sql/executor.go:597 sql implicit: false\" id=c880a706 key=/Table/51/1/120611888869081090/2/1 [...] What does this mean? All are verbose ways CockroachDB tells us that a transaction could not be committed and that it should be retried. The first error (“write with future timestamp”) means that we are trying to “read into the future” by reading data written by a transaction on a node that appears to be slightly ahead in time (see Spencer’s blog post on CockroachDB and time for an explanation). The second error means the transaction tried to write something, but encountered a conflict with another read transaction. The remaining two errors are similar but report on write-write conflict in which our transaction lost. Clients must deal with these errors in real-world apps by retrying the transaction. In our Jepsen tests, we decided to do this as follows: First, we captured these specific cases using a handy wrapper macro . Then we implemented another macro that wraps a transaction and retries it, avoiding storms of transaction conflicts using exponential back-off . (CockroachDB’s documentation suggests there is a better approach , however we did not implement it here for simplicity.) With this extra infrastructure in place, the latency distribution becomes much cleaner: This reveals there are still “transfer failures,” but looking at the log, we see they are simply cases where a transfer would otherwise result in a negative balance, and are thus disallowed by the model. Intermezzo: Clojure’s threading gave us a cold sweat While testing bank transfers, we did see something that looked an awful lot like an anomaly: some transfers appeared to be incomplete, with money retrieved from one account but not deposited on another. That sounded really bad, and we spent a whole weekend investigating . While looking at the trace of SQL queries arriving at the server nodes, we found something very odd: it appeared as if multiple transfers were occurring in the same transaction . It was as if the network between the clients and the server was messing with the traffic and mixing the streams from multiple clients together. Obviously, TCP/IP is not that broken and we found the problem eventually: an open JDBC connection was being re-allocated to a new thread while the previous thread that was using it was still running. How did this happen? We were catching timeouts using Aphyr’s Jepsen macro for reconnects , and we found it to be a little sketchy. In the JVM, threads only check their interrupt status during certain operations, so it is possible for a thread to continue for some time after being interrupted. We fixed this by ensuring the connection is closed and re-opened on each timeout [5] . Notes [5] It would be possible, if the underlying JDBC driver checks the interruption status in all the right places, for the interrupted thread to rollback its transaction and then acknowledge the interrupt so the connection could be used again, but that’s tricky to get right, and also it is somewhat out of our hands. From snapshot isolation to serializable isolation As said above, CockroachDB also supports serializable isolation, which is one step up from snapshot isolation: transactions must appear to be processed in some order from the perspective of one client. This implies snapshot isolation (transactions cannot observe unfinished work) but also that write skews are not possible: concurrent transactions cannot appear to work starting from the same initial database state, perform disjoint updates, then achieve incompatible database states after they commit. Aphyr’s pseudo-bank test we were using earlier is insufficient to stress serializable isolation: two transfers involving at least a common bank account would get a write-write conflict, which is already disallowed by snapshot isolation, but that would also prevent write skews without further attention from the database engine. So we wrote another test. Our monotonic client uses a single table with an int column and initializes it with a single row with the value 0. It then responds to add events by reading the current max of all rows already in the table and adding a new row with the maximum + 1, and to read events by fetching all rows currently in the table. Meanwhile the event generator produces staggered add events during the test and a read event at the end. If write skews were possible, then some transactions would start from the same current maximum and insert the value of this maximum + 1 twice in the table (disjoint updates). This is not possible if they appear to run one after the other, as required by serializable isolation. So when the test completes, our checker verifies, among other things, that the table doesn’t contain duplicates . We ran this test in many configurations and didn’t find duplicates. The latency distribution looks like this: From serializability to linearizability Perhaps surprisingly, the order of serializable transactions may appear to be different depending on which client observes it. For example, if a client A creates a user, and then creates a post for that user in successive transactions T1 and T2, it would be possible for client B to see the blog post before the user exists. This is possible because the commit timestamps assigned by the database are acausal. That’s where serializability differs from linearizability: with linearizability, you observe the same order from all clients. By default, CockroachDB guarantees serializability in general for transactions, and guarantees linearizability only for special circumstances (technically, for all transactions that touch overlapping sets of nodes). Side Note: The ACID model does not refer to serializability or linearizability. Why is that? Well, ACID was developed at a time when people were more concerned that transactions not see partial results from other transactions than anything else. Since database engines were mostly sequential, linearizability came for free with serializability, so there was no need to distinguish between them. Today, everything is different, which is why the ACID model is becoming insufficient to talk about distributed databases. Given this claim, it’s interesting to evaluate CockroachDB’s limited linearizability guarantee and determine in which circumstances it breaks down to mere serializability. Snapshot isolation over multiple ranges? This is the moment where you could stop us and say: “Wait a minute! You said earlier you don’t need to make a lot of effort for isolation over small key ranges, because everything is resolved in chronological order at a single node, the range leader. But your banking test about snapshot isolation earlier uses only a few accounts, so they probably lie on the same key range. Wasn’t that a bit too easy?” You would be right, of course. The banking test from earlier used a small enough amount of data for it all to fit on a single range and thus not stress the multi-range transaction implementation. In other words, we weren’t testing much. We noticed this too and wrote another version of the banking test that uses a different table for each account (CockroachDB splits ranges at table boundaries). We haven’t found errors using that test either. Linearizability on single ranges One test Aphyr often uses to check linearizability is concurrent compare-and-set. The idea is to send concurrent read, write and compare-and-set (CAS) operations from multiple clients, collect the trace of observed results, then analyze these traces using the linearizability checker in Knossos . The checker validates that the trace is possible if the history was linearizable; if an invalid ordering is detected, it can conclude that the database does not guarantee linearizability. CockroachDB implements linearizability on single key ranges. SQL rows are mapped to a set of keys in the data store, but it also guarantees that all keys in the same row will belong to the same range. In other words, as long as our CAS test uses a single row for each atomic register, we should observe only linearizable histories. To do this, our Jepsen client initially sets up a table with two columns id and val . Each row represents a single atomic counter. Meanwhile, we also took good note of Aphyr’s and Peter Alvaro’s recommendations in this recent blog entry by Aphyr about RethinkDB : “Tests of longer than ~100 seconds would bring the checker to its knees. Peter Alvaro suggested a key insight: we may not need to analyze a single register over the lifespan of the whole test. If linearizability violations occur on short timescales, we can operate over several distinct keys and analyze each one independently. Each key’s history is short enough to analyze, while the test as a whole encompasses tens to hundreds of times more operations–each one a chance for the system to fail.” As in the RethinkDB tests, we use the independent/sequential-generator to construct operations over integer keys, and for each key, emitting a mix of reads, writes, and compare-and-set operations, one per second, for sixty events. Then sequential-generator begins anew with the next key. Also, like Aphyr suggested, we used reserve to ensure there are always some threads running reads when all threads running writes and CAS operations are blocked, so that we can detect transient read anomalies if any. We then handled in our client the fact that each operation may operate on different counters. On each operation we first looked at whether the row for the requested counter already existed then upon writes, depending on whether it existed or not, we either create it or update the existing row (CockroachDB does not yet support “upsert”). CAS operations are handled as simple updates , and one operation is known to have failed or succeeded depending on the updated row count returned for the UPDATE statement. Soon enough it was time to run the test, which gave us the following: Knossos did not find any linearizability errors in this history, which is good. Neither did it in many more histories. Note : despite “CAS failures” being reported in red in the figure, they are expected occurrences in the test: if the compare part of compare-and-set doesn’t find the expected value, it registers as a fail, but the test generates CAS randomly so that’s bound to happen sometimes. Woes with CockroachDB timestamps We tried to determine cases where transactions have serializable behavior but not linearizable. To test this, we thought we could use something time-related, but while designing a test we ran into a serious issue. The issue was discovered using another test described above already: the monotonic test which adds the current maximum + 1 on each client event. As a sanity check, we added a second timestamp column in that test’s table that would also capture the database’s perception of time on each insert. We figured that if the behavior were properly serializable, then each newly added maximum and its accompanying timestamp necessarily had to grow together. Our first approach was to use NOW() as a measurement of time. Unfortunately, when we tested that, we observed the contrary: sometimes we would see the maximum value decrease over “time.” Uh-oh. Well, the explanation is rather simple: NOW() uses each database node’s clock as a time source. If their clocks are slightly desynchronized, a transaction occurring earlier on node A in absolute time could pick a value for NOW() slightly larger than a transaction occurring later on node B. This is really a fundamental lesson with distributed databases: unless the database engine makes an extra effort, there is no guarantee that NOW() is monotonic in transaction order. We struggled for a while thinking about what to do about this. On the one hand, we had to acknowledge there is a user need for some function that evolves monotonically in transaction order. At least for our Jepsen experiments! We also argued a lot about whether SQL’s NOW() should fill this role, since the local physical time also has a use in many applications. We iterated back and forth and eventually settled on creating a CockroachDB-specific function called cluster logical timestamp() that provides this guarantee. This Jepsen exercise helped us understand we had to be very careful about our understanding of time and what properties to advertise to users about our time functions. Linearizability vs. serializability Since linearizability and serializability seem so close to each other, we used the opportunity offered by the Jepsen tooling to distinguish them further here. We do this as follows. Our monotonic-multitable test defines multiple tables , each with columns (val int, ts int, client int) . It then instantiates multiple clients that share a single atomically increasing counter, and for each input event, each client inserts that counter’s value, the database’s time as per cluster_logical_timestamp() , and its client ID in a randomly selected table . The different tables increase the probability that transactions from the same client are processed on separate nodes with no common key range and no internal causality. Since each client individually commits the previous transaction before starting the next one, we should see the same order for both columns val and ts . We implement this as a check that filters the sub-sequences of results for each client and checks the monotonicity on each sub-sequence . Meanwhile, since the isolation level is serializable and not linearizable, the database may decide to effect the transactions in a different order than the order they were issued and thus the combined table contents as observed by another client may have conflicting orderings between both columns. We implement this as a result field that reports ordering anomalies on the combined table contents . When running this test, we observe results like the following: {...\n  :details {...\n  :valid? true,\n  :value-reorders-perclient\n    (()   ()   ()   ()   ()   ()   ()   ()   ()   ()\n     ()   ()   ()   ()   ()   ()   ()   ()   ()   ()\n     ()   ()   ()   ()   ()   ()   ()   ()   ()   ()),\n  :value-reorders\n    (((22 14586953315945740430000000000N 3)\n      (21 14586953315962445260000000000N 0))\n     ((40 14586953321193014420000000000N 13)\n      (39 14586953321264351870000000000N 27))\n     ((94 14586953338036856600000000000N 9)\n      (93 14586953338081737300000000000N 17))\n     ((114 14586953344009692430000000000N 9)\n      (113 14586953344034096610000000000N 22))\n     ...\n    ),\n  },\n  :valid? true\n} The empty lists for value-reorders-perclients indicate that no per-client sub-sequence contained ordering anomalies. In other words, each client’s transaction history is serializable. Meanwhile, the field value-reorder shows that pairs of clients observe conflicting orderings. For example, client 0 has inserted value 21 before client 3 has inserted value 22 (the client-side atomic counter is guaranteed to increase monotonically), yet the database chose timestamps in the opposite order. This can happen when the client’s transactions are processed by nodes with separate physical clocks: separate clocks are always slightly desynchronized, even with NTP. At the end, to the client observing the database contents, the history appears as if client 3 has inserted before client 0, i.e. a different order. The global history is not serializable, ie. transactions are not linearizable. Just as predicted! To further drive the case in point, this is a good opportunity to reveal the existence of CockroachDB’s hidden database-wide “linearizable” flag. This flag, which can be set for a cluster globally, makes CockroachDB behave like Google’s Spanner database: each transaction waits some time before a successful commit is reported to the client, and this guarantees a global transaction ordering — i.e. linearizability. (The mechanism is described further in Spencer’s blog post on CockroachDB and time . We don’t advertise it yet because it makes the database very slow without a high-precision time source like TrueTime. However it is useful for testing.) When we set the linearizable flag, the linearizability anomaly revealed by the test above disappears! … except that it doesn’t, at least not always. In some test runs, linearizability anomalies still appear even when the database runs “linearizable.” There may be a bug in the implementation for the linearizable flag, we’re not sure yet. This is another reason why we do not advertise it in our documentation just yet. Until then, you should still get serializability per client, as advertised. Linearizability vs. network partitions Since CockroachDB also claims correctness in the face of adversity, we wanted to let Bad Things happen to our database and see how it responds. Like Aphyr often does, we started subjecting our 5-node database cluster to random network partitions while running the compare-and-set test. When we do so, we see the following happening: Still no linearizability errors detected by Knossos, but then we start seeing some (but not all!) operations timing out during a partition. What’s going on? We can look at the history log for what happens soon after a partition starts: :nemesis    :info   :start  \"Cut off {:n1l #{:n3l :n4l :n2l},\n:n5l #{:n3l :n4l :n2l}, :n3l #{:n1l :n5l},\n:n4l #{:n1l :n5l}, :n2l #{:n1l :n5l}}\"\n[...]\n7   :info   :read   [:timeout :url \"//n3l:26257/jepsen\"]\n2   :info   :write  [:timeout :url \"//n3l:26257/jepsen\"]\n[...]\n56  :info   :read   [:timeout :url \"//n2l:26257/jepsen\"]\n51  :info   :cas    [:timeout :url \"//n2l:26257/jepsen\"]\n[...]\n58  :info   :read   [:timeout :url \"//n4l:26257/jepsen\"]\n53  :info   :cas    [:timeout :url \"//n4l:26257/jepsen\"]\n[...] The first line indicates the nemesis has created two node groups {n1, n5} and {n2, n4, n3}. Following this, all transactions started on nodes n2, n3, and n4 fail and cause a reconnect — whereas any operation on n1 and n5 still succeeds. The reason for this is that a quorum is needed for updates. During the partition, n1 and n5 are in a replication group with one of the other 3 nodes (either n2, n4 or n3). When a transaction arrives on that 3rd node to modify data under responsibility of this replication group, it must wait until it obtains a quorum before it can resolve. Meanwhile, transactions arriving on n1 or n5 have their quorum and can succeed. To summarize, CockroachDB successfully protects the consistency of the data during network partitions by stalling or aborting the queries until the partition resolves. In a real-world application, clients can probabilistically avoid orphan nodes by reconnecting to the cluster via a load balancer when a timeout is detected. Linearizability vs. clock skews CockroachDB uses wall clock time to maintain ordering between transactions. We’ve written about that before. But this means it is particularly sensitive to time differences between nodes – with too much clock drift, nodes may start to have conflicting ideas about transaction order (e.g. a client might try to read data it just wrote and find it missing if the read is served through a node with a “slow” clock). This warrants some extra testing. We ran multiple tests using Jepsen’s clock-scrambler generator which uses “ date --set-time ” to add random positive or negative offsets to the system clock on each node. What does that give? For one, we started getting more read and write errors: These errors are not retries any more: storage/store.go:1399: Rejecting command with timestamp in the future: 1456997985090162004 (1.018244243s ahead) This occurs when CockroachDB tries to propagate a transaction across multiple replicas and the RPC communication layer detects the clocks of the replicas have drifted too much (more than set via the –max-offset parameter, which defaults to 250ms). But also as the drifts increase past a few seconds, CockroachDB really becomes confused about time and… we start seeing linearizability errors. We’d like to defend this as not a serious bug in CockroachDB. We’ve always said that we only guarantee consistency when the maximum clock drift doesn’t exceed the MaxOffset parameter configured at database start-up. CockroachDB actively tries to estimate clock drift and nodes stop themselves if they detect that their own time is likely to have drifted in excess of MaxOffset. We are really doing our best: it is theoretically impossible to guarantee the detection of clock offsets in general. Our default tolerance of 250ms drifts is generous, and caters for everyone but the most terrible deployments. Most applications can reliably avoid drifts larger than 250ms by ensuring that database nodes synchronize their time with NTP. That said, smaller clock skews are still possible even with NTP, if the query rate is high enough. This can happen due to VM migrations in clouds, or even on multi-core systems when the OS does not fully synchronize time counters between different cores (eg. in OS X). So we had to look at smaller skews, too, to exercise CockroachDB’s MaxOffset tolerance. For this, we implemented our own scrambler that can introduce drifts of +/- 100 milliseconds, ramped up the request rate and we observed the results: Actually this is pretty uneventful and the various errors are not any different than earlier. We ran this test with this nemesis many times, and we didn’t find any linearizability errors. More nemeses! We found the Jepsen tools to be incredibly useful at making Bad Things happen. They support network partitions, majority rings, starting and stopping the database at unexpected moments, and many other nemesis scenarios. For example, besides the “big skews” nemesis already discussed above, we also added another one that would “violently” kill one or two servers (using Unix’ SIGKILL ) and restart them randomly. We wanted to test them all and even in combination, but writing a Jepsen test definition for each nemesis quickly proved tedious. So we took some inspiration from the Jepsen code for MongoDB and RethinkDB and baked our own nemesis mix-and-match factory. We did this by defining each nemesis using a name, a generator for nemesis events, and a client for nemesis events. For example we wrapped the random partition nemesis as follows : (def parts\n  {:name \"parts\"\n   :generator nemesis-single-gen\n   :client (nemesis/partition-random-halves)}) For a single nemesis, the nemesis-single-gen generator simply generates start and stop events one after another with configurable delays. Using this pattern we wrapped several of Jepsen’s nemeses with some of our own. When we compose two nemeses, we do so using another event generator that generates two sequences of start/stop events with overlapping intervals, and use that in our custom nemesis composition operator which returns a new nemesis definition with the same interface . Finally, once all our nemeses had the same interface, we made them parameterizable in our tests then generated test definitions automatically for all combinations of interest to us! It works like this: (defn check\n  [test nemesis]\n  (is (:valid? (:results (jepsen/run! (test nodes nemesis))))))\n\n(defmacro def-tests\n  [base]\n  (let [name# (string/replace (name base) \"cl/\" \"\")]\n    `(do\n       (deftest ~(symbol name#)                (check ~base cln/none))\n       (deftest ~(symbol (str name# \"-parts\")) (check ~base cln/parts))\n       (deftest ~(symbol (str name# \"-skews\")) (check ~base cln/skews))\n       ...\n       (deftest ~(symbol (str name# \"-parts-skews\"))\n                      (check ~base (cln/compose cln/parts cln/skews)))\n       ...)))\n(def-tests cl/atomic-test) Using this, we can now run separately the check for atomic-test , atomic-test-parts , atomic-test-skews , atomic-test-parts-skews and so forth. We hope this might be of interest to other Jepsen users. Once this infrastructure was in place, we used it to check whether CockroachDB would get confused if we injected clock skews during a network partition, or vice-versa. It didn’t — at least not any more than determined earlier. We then automated the generation of a handy test result overview: The red items reveal inconsistencies found with big clock jumps, as explained in the previous section. A public instance of these tests results is available here . Note that since we only run these tests occasionally at significant development milestones, they may not reflect the latest published version of CockroachDB. Wrapping up The incentive to run the Jepsen tools compelled us to fix numerous limitations in our client wire protocol, which now provides pretty good support to Clojure (and Java) JDBC clients. We had a lot of fun adding extensions to the Jepsen tool box, including a few bug fixes, additional utilities, and the ability to easily test all combinations of multiple tests and multiple nemeses without too much boilerplate. And finally, thanks to Jepsen, we also found two consistency-related bugs in CockroachDB: #4884 and #4393 . This is exactly the kind of discovery we had expected from the ordeal, and we are thrilled to join the club of Jepsen advocates! Although, of course, we won’t forget… Does building and stress-testing distributed SQL engines put a spring in your step? If so, we're hiring! Check out our open positions here .", "date": "2016-04-14"},
{"website": "CockroachLabs", "title": "Could CockroachDB Ever Replace Redis? A Flex Fridays Experiment", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/could-cockroachdb-ever-replace-redis-a-free-fridays-experiment/", "abstract": "The goal of CockroachDB is to “make data easy,” and while it seems like a stretch now, we eventually want CockroachDB to be able to act as the entire state layer for web applications. We are currently addressing the SQL layer, and a full-text search like ElasticSearch is somewhere ahead on the product horizon. Since Cockroach Labs has a Flex Fridays policy for work on experimental projects, I decided to use mine to experiment with implementing the Redis protocol on top of CockroachDB , attempting to answer the question: Could CockroachDB ever replace Redis? Redis is used in many organizations as a fast and feature-full cache, but not often as a persistent data store. This is because of its primarily in-memory nature and asynchronous replication strategy (where multi-node consistency is traded for higher performance). For organizations that do use Redis in a multi-node setup (either master/slave or Redis Sentinel ), they must run additional software and configure their clients to support failover between nodes. Redis supports sharding (horizontal scaling) through Redis Cluster , which has similar limitations. Although I have only used Redis occasionally, I have been a part of organizations that use it heavily . They and others have expended significant engineering effort to create software that can correctly failover between master and slave nodes during a node failure. And even with all that effort, they are still limited to Redis’ asynchronous replication that can lose data. Redis, it appears to me, made some performance tradeoffs from the start: be fast by being single threaded with no synchronous replication to slave nodes. CockroachDB made the opposite tradeoffs: consistency and scaling first, then optimize to make up for performance loss. Experimenting with Redis Over CockroachDB I have now been working on this Redis over CockroachDB implementation for a few weeks with satisfying results. Currently, it is compiled directly into the main CockroachDB binary and binds to its own port that acts like a Redis server. The README file documents the supported commands, which include most of the list, set, and key commands. Various features aren’t implemented yet (key expiration, pub/sub, other data types), but my plan is to continue work to implement these over time. In order to ensure correctness, the tests are structured so they can be run against a real Redis server and this one. The tests don’t test all possible combinations of commands (for example I had a bug where the rename command would only work for string types), but being able to compare to a real Redis instance, including error messages and types, has made implementing new commands and types fairly straightforward. Performance and Implementation [Note: We are still in Alpha and have lots of performance work to do.] Similar to the test environment, a benchmark environment was created that easily runs against a real Redis server to compare with the CockroachDB one. I had no idea what to expect during my first benchmarks. I was going to be happy if a single-node CockroachDB cluster was only 100 times slower than Redis. I was pleasantly surprised when the benchmarks showed that performance was between 10 and 20 times slower, depending on the operation. String and key read operations like get are on the better side of that (nearer 10x). Write operations like set are nearer 20x. These commands have mappings in CockroachDB to native operations. incr is notably as fast as set because CockroachDB can atomically increment. Other data structures like lists and sets are slower, but that is more due to my poor implementation than CockroachDB. These data types are stored as gob -encoded bytes, and so must be decoded on every use. This means that any list or set takes only one key in the key-value store, but must be fully sent and decoded for every operation. As the size of the list or set increases, performance worsens. This is a bad use of gob and a poor way to store data, however it was a quick solution for a proof-of-concept implementation. A better, future solution would be to store each item of a list as an individual item in some key space. Some other features will be difficult to do at all. CockroachDB now has no event notifiers. blpop is implemented as a loop with a timeout that polls. Implementing something like pub/sub would have to use the same method. If there was some event system that broadcast to listeners on key put, these could be done well. This may happen in the future. Future Use: Could CockroachDB Ever Replace Redis? I am working on Redis over CockroachDB only as an experiment. For now, it looks as though Redis is pretty safely in a different performance tier. That outcome isn’t a big surprise, as one is a featured-full cache and the other a strongly consistent database. However, it will be interesting to see how CockroachDB’s performance improves relative to Redis over time. My personal goal is to make this a viable drop-in replacement for low- to medium-load Redis servers, but this requires support for nearly all Redis commands at reasonable performance. If it can get there, I hope that CockroachDB’s reduction in admin time compared to managing a Redis cluster offers enough benefit to be useful. This would make it possible to use a single CockroachDB cluster as both a SQL and Redis backend. GitHub: https://github.com/mjibson/cockroach/tree/redis Also posted at Matt Jibson’s blog .", "date": "2016-02-04"},
{"website": "CockroachLabs", "title": "Creating a Digestible GitHub Digest", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/creating-a-digestible-github-digest/", "abstract": "``` If you’ve ever “watched” a busy GitHub repository, your email inbox has discovered what it feels like to step in front of a firehose. If the project in question has active code reviewers, the problem is often worse by an order of magnitude. Every comment yields another email to all watchers. The CockroachDB repository’s weekly average is at 81 pull requests and 440 notification-generating comments . ``` Most of us who once paid close attention to incoming changes have since lost the ability to do so; these days, monitoring the stream requires a superhuman effort. The mere mortals among us can only pay attention to the pull requests we’ve authored or are tasked with reviewing. What’s surprising is that the watching functionality provided by GitHub is so coarse-grained. The dial apparently only has settings for “0” and “11”. GitHub Digest Options A search for GitHub digests yields some choices. Diffmatic is neat, as is this open source Ruby digest from the folks at Heroku. Taking a cue from these, I decided to spend a Flex Friday repurposing my previous efforts to analyze our GitHub stargazers to build repo-digest , a GitHub pull request digester which provides a daily appraisal of PRs in a concise and parsable format. repo-digest provides daily digesting of one or more GitHub repos. By default, the digest includes all pull requests which were opened or closed within the past 24 hours, but this is something you can tinker with to suit your preferences via the –since command line flag. The digest sorts all pull requests – both open and closed – in descending order, by total changes, to highlight consequential PRs. Example usage: repo-digest --since=2016-02-24T19:00:00-05:00 \n--repos=cockroachdb/cockroach,cockroachdb/docs \n--token=f87456b1112dadb2d831a5792bf2ca9a6afca7bc How does it work? It’s a straightforward usage of the GitHub API. Pull requests are queried in reverse date order from the comma-separated list of repositories specified via the –repos flag. Each pull request is queried in turn to get more details and its list of changed files. Changed files are again queried in turn to get addition and deletion counts for each. The digest provides additional insight into the focus of a pull request by listing the most important subdirectories. File changes are tallied for each subdirectory, and those which comprise the top 80th percentile are deemed representative of the pull request and listed immediately underneath the additions and deletions totals: Customizing the look of the digest The images shown here have been styled to match Cockroach Labs branding, but the template is flexible and very easy to customize. I used golang’s nifty templating language . The default template is generically styled. You can create and specify your own template using –template= . repo-digest automatically inlines CSS styles to make the output suitable for sending via email.", "date": "2016-03-23"},
{"website": "CockroachLabs", "title": "From 5 to 500: Lessons Learned Hiring for Startups", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/5-500-lessons-learned-hiring-for-startups/", "abstract": "Most founders agree that one of the greatest challenges that they face isn’t raising money or closing deals or finding partners.  It’s finding people. In particular, finding great people who are interested and eager to take a chance on a startup. Turns out, hiring for startups is hard. Early in my career, I was at Google and had a candidate who had offers from DE Shaw, Two Sigma, and Jane Street Capital. This was circa 2008, and Google’s compensation was not competitive against financial institutions. My manager and I strategized ways to close the candidate and decided that the Vice President of Google’s Engineering team, Stuart Feldman, should do a closing call. As I walked in to Stuart’s office for a prep meeting, I apologized for taking up his time. He responded with, “Don’t apologize. Hiring will always be 50% of my job.” I have been hiring for startups and building teams at tech companies for the last 10+ years, and I’ve learned that the only thing that will change for founders through the life-cycle of their company is the percent of time they spend on recruiting. At an early-stage business, it will feel like you are recruiting non-stop – an additional 80-90% of your time, at your already 100% job. For later stage companies, even with a robust team, founders will still be focused on recruiting at least 50% of their time. The key takeaway here: founders need to always put a concentrated effort towards hiring efforts, regardless of the stage of their business. So how do you go from good hiring to great? Hiring is not just about finding amazing people, it’s about creating a human experience. Regardless of if you are a hot, consumer-facing company or an enterprise tech company like Cockroach Labs, there is an immense pressure to hire quickly, and what is so often lost is the human experience. This matters because the way you approach your hiring efforts impacts the experience that an individual candidate has with your company and creates an impact on your future hiring. Great hiring is when you listen to your candidate and create an experience for them, conveying the day to day feel and heartbeat of your company. What’s your company’s story? When I first joined Cockroach Labs, I needed to figure out what the heartbeat of the company was. I met with all the employees and asked them several questions about why the joined and what keeps them here. Very specifically, I asked them what made them accept their offer. I heard three commonalities: the product, the people, and the founders. From this feedback, I was able to shape a story for our candidates about how our founders first were exposed to open source products and how their experience has driven the formation of Cockroach Labs and our culture. The story was not my own, rather a story that I told through the eyes of our founders and each of our employees. It’s a story that all of us at Cockroach Labs live and breathe. Regardless of if you are a 5 person startup, or a 500+ person company, every person who works for you shapes your story. They bring humanity to your company, which you can then share with your candidates. And if all goes well, it will hopefully help you to close them, adding one more voice to the story.", "date": "2016-04-27"},
{"website": "CockroachLabs", "title": "A Tale of Two Ports | Cockroach Labs", "author": ["Tamir Duberstein"], "link": "https://www.cockroachlabs.com/blog/a-tale-of-two-ports/", "abstract": "CockroachDB is pretty easy to deploy . We’ve done our best to avoid the need for configuration files, mandatory environment variables, and copious command line flags, and it shows; we’ve already had testimonials from folks who were able to deploy a 20-node cluster in just half a day. That’s something to be proud of! However, there is still one wrinkle in the fabric, and that’s our use of network ports. As of this writing, CockroachDB requires two ports, but why, and can we do better? CockroachDB started out as a distributed key-value store (read: NoSQL). In those days, all internode communication used golang’s standard net/rpc . This worked quite nicely thanks to net/rpc.(*Server) implementing http.Handler ; that implementation meant that we could multiplex our admin UI with our RPC traffic on a single port: package main import ( \" log \" \" net/http \" \" net/rpc \" ) type Args struct { A , B int } type Arith struct {} func (* Arith ) Multiply ( args * Args , reply * int ) error {\n    *reply = args. A * args. B return nil } func main () { s := rpc. NewServer ()\n    s. Register (&Arith{}) mux := http. NewServeMux ()\n    mux. Handle ( \" / \" , adminUIHandler)\n    mux. Handle (rpc. DefaultRPCPath , s)\n\n    log. Fatal (http. ListenAndServe ( \" :0 \" , mux))\n} Great, we’re done…right? Almost. As we’ve previously discussed on this blog, CockroachDB is now a relational SQL database , and because we’re not masochists, we decided to implement PostgreSQL’s wire protocol (PGWire) rather than write and maintain client libraries for every language. This presents a problem: we need another port. But why? net/rpc can share, why can’t we? turns out the answer is “by definition”; we can’t piggyback on net/http because PGWire isn’t HTTP. The net/rpc protocol is specially designed to support this use case – its handshake occurs over HTTP, after which the connection is http.Hijacker ed and only then passed to the net/rpc.(*Server) , which proceeds over plain TCP. Since PGWire wasn’t designed with these considerations in mind, it is implemented in plain TCP end-to-end. Bummer. Astute readers will note that HTTP and PGWire are different protocols! In other words, it should be possible to read a few bytes from an incoming connection, figure out which protocol is being used, and delegate to the appropriate handler. This is exactly what we did using a small library called cmux . cmux provides a custom net.Listener implementation which supports lookahead on its connections, along with the notion of “matchers” – boolean functions used to produce “child” listeners which yield only those connections for which the matcher returns true . CockroachDB supports TLS in both HTTP/RPC and PGWire, but the two protocols implement TLS differently. In HTTP the TLS handshake is the first thing sent on the connection (which is handled by tls.NewListener , while PGWire has a brief cleartext negotiation phase before encryption starts. Therefore, we use the following arrangement of listeners: non-TLS case:\n\nnet.Listen -> cmux.New -> pgwire.Match -> pgwire.Server.ServeConn\n              |\n              - -> cmux.HTTP2 -> http2.(*Server).ServeConn\n              - -> cmux.Any -> http.(*Server).Serve\n\nTLS case:\n\nnet.Listen -> cmux.New -> pgwire.Match -> pgwire.Server.ServeConn\n              |\n              - -> cmux.Any -> tls.NewListener -> http.(*Server).Serve Phew! That was a lot. Time to pat ourselves and have a drink, our database is running on one port. Have you heard of gRPC? It’s pretty nice. It’s built on HTTP2, so it supports such nicities as streaming RPCs, multiplexed streams over a single connection, flow control, and more. It’s also a great fit for our Raft implementation because it allows us to avoid blocking normal Raft messages behind (slow) snapshots by using different streams for the two types of traffic. Just a few weeks after the cmux work landed, we switched our RPC system to gRPC . This worked pretty well since grpc.(*Server) also implements net/http.Handler . Note: While grpc-go supports insecure gRPC (that is, h2c), Go’s net/http does not. This means that using grpc.(*Server).ServeHTTP requires some trickery – here’s our solution , and its later refinement using cmux . OK, let’s get back to the story. Unfortunately for us, grpc.(*Server).ServeHTTP turned out to have serious performance problems , so we had to get creative. Fortunately, gRPC identifies itself through a content-type header (it’s HTTP2, remember?), which means we can use cmux to sniff out the header and then dispatch to the much faster grpc.(*Server).Serve , so we did that : non-TLS case:\nnet.Listen -> cmux.New\n              |\n              - -> pgwire.Match -> pgwire.Server.ServeConn\n              - -> cmux.HTTP2HeaderField(\"content-type\", \"application/grpc\") -> grpc.(*Server).Serve\n              - -> cmux.HTTP2 -> http2.(*Server).ServeConn\n              - -> cmux.Any -> http.(*Server).Serve\n\nTLS case:\nnet.Listen -> cmux.New\n              |\n              - -> pgwire.Match -> pgwire.Server.ServeConn\n              - -> cmux.Any -> tls.NewListener -> cmux.New\n                                                   |\n                                                   - -> cmux.HTTP2HeaderField(\"content-type\", \"application/grpc\") -> grpc.(*Server).Serve\n                                                   - -> cmux.Any -> http.(*Server).Serve A little more complicated, but it worked, and allowed us to keep our straight-line performance from regressing too badly. That is, until we discovered that this broke our admin UI when using TLS . The TL;DR is that gRPC behaves differently from most HTTP2 clients (including Chrome); where those clients wait for an acknowledgement from the server before sending headers, gRPC does not, which is why the solution above worked in the first place. However, when serving Chrome over TLS (HTTP2 is not used unless TLS is used as well), our cmux matcher hangs as it waits for the headers to come through (cmux matchers may not write, only “sniff”), which causes Chrome’s cryptic error. This is where we are today. We’ve had to separate our HTTP port from the gRPC+PGWire port , where the diagram for gRPC+PGWire is now: non-TLS case:\nnet.Listen -> cmux.New\n              |\n              - -> pgwire.Match -> pgwire.Server.ServeConn\n              - -> cmux.Any -> grpc.(*Server).Serve\nTLS case:\nnet.Listen -> cmux.New\n              |\n              - -> pgwire.Match -> pgwire.Server.ServeConn\n              - -> cmux.Any -> grpc.(*Server).Serve and the HTTP admin UI has its own dedicated port. This is an interim solution; we hope that the gRPC maintainers are able to fix the performance problems in grpc.(*Server) , which will allow us to return to a single port. Until then, we’ll have one port to rule them all – and a second port for the Admin UI.", "date": "2016-05-11"},
{"website": "CockroachLabs", "title": "Trust, But Verify: How CockroachDB Checks Replication", "author": ["Vivek Menezes"], "link": "https://www.cockroachlabs.com/blog/trust-but-verify-cockroachdb-checks-replication/", "abstract": "We built survivability into the DNA of CockroachDB. And while we had a lot of fun doing so, and are confident that we have built a solution on a firm foundation, we felt a nagging concern: Does CockroachDB really survive? When data is written to the database, will a failure really not end up in data loss? So to assuage those concerns, we adopted a Russian maxim: “Dovorey, no provorey – Trust, but Verify.” To understand CockroachDB’s survivability promise, you must first understand our key-value store and replication model, as they form the foundation for survivability. CockroachDB is a SQL database built on top of a distributed consistent key-value store. The entire key-value space is split into multiple contiguous key-value ranges spread across many nodes. CockroachDB uses Raft for consensus-based replicated writes, guaranteeing that a write to any key is durable, or in other words, that it survives. On each write to a key-value range, the range is synchronously replicated from a Raft leader to multiple replicas residing on different nodes. Database writes are replayed on all replicas in the same order as on the leader, guaranteeing consistent replication. This model forms the basis of CockroachDB survivability: if a node dies, a few others have an exact copy of the data that is lost. While we’d like to trust our survivability model, we went one step further: we added verification. We built a subsystem that periodically verifies that all replicas of a range have identical data, so that when one of them has to step up to replace a lost node, the data served is what is expected. How CockroachDB Checks Replication Every node in the cluster runs an independent consistency checker. The checker scans through all the local replicas in a continuous loop over a 24 hour cycle, and runs a consistency check on each range for which it is the Raft leader: The leader and replicas agree on the snapshot of the data to verify and assign a unique ID to it. A SHA-512 based checksum on the selected snapshot is computed in parallel on the leader and all replicas. The leader shares its checksum and the unique snapshot ID with all replicas. A replica compares the supplied checksum with the one it has computed on its own. On seeing a different checksum, it declares a failure in replication. In the rare circumstance that it finds a replication problem, the checker logs an error or panics. Unfortunately, it is not possible for the system to identify whether the Raft leader or its replica is responsible for a faulty replication, and a replication problem cannot be repaired. Logging or panicking on detecting an inconsistency problem is great, but how does one debug such problems? To aid the user in debugging, on seeing a checksum mismatch, a second consistency check with an advanced diff setting is triggered on the range. The second consistency check publishes the entire range snapshot from the leader to all replicas, so that they can diff the entire snapshot against their own version and log a diff of the keys that are inconsistent. We were able to use this second consistency check mechanism to debug and fix many problems in replication. While we had hoped we had built the perfect system, the verifier uncovered a few bugs! Bugs and Fixes We fixed a number of hairy bugs: The same data can look different: The order of entries in protocol buffer maps is undefined in the standard. For some cockroach internal data, we were using protocol buffer maps and replicating it via Raft. All the replicas would separately convert the protocol buffers into a wire format while writing them to disk. Since the wire format is non-deterministic in the standard and implementation, we saw the same data with a different wire encoding on different replicas (fixed via gogo/protobuf #156 ). Backdoor writes on the replicas ( #5090 ): We collect statistics on every range in the system and replicate them. Writes to the statistics are allowed only on the leader, with replicas simply replaying the operations. We were occasionally updating the statistics on the replicas outside of Raft consensus (fixed via #5133 ). Internal time series data was being merged incorrectly: The CockroachDB UI keeps track of monitoring time series data which is replicated. The replicas merge the time series data when read, but an occasional legitimate read would creep in on the leader, causing a bug in the merge functionality (fixed via #5515 ). Floating point addition might be non-deterministic: While fixing the above time series issue, we got super paranoid and as a defensive measure decided to not depend on the replica replay of floating point addition. We were aware of floating point addition being non-associative , and although we knew our floating point additions were being replayed in a definite order and didn’t depend on the associativity property, we adhered to the mantra, “only the paranoid survive,” and got rid of them (fixed via #5905 ). Conclusion We’ve built a new database that you can trust to survive. With trust comes verification, and we built that into CockroachDB. CockroachDB offers other features like indexes, unique columns, and foreign keys, that you can trust to work properly. We plan on building automatic online verification mechanisms for them, too. We look forward to discussing them in the future.", "date": "2016-05-19"},
{"website": "CockroachLabs", "title": "Building an Application With CockroachDB and SQLAlchemy", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/building-application-cockroachdb-sqlalchemy-2/", "abstract": "CockroachDB’s support for SQLAlchemy is currently in beta, but we’re actively developing new features to improve the integration. You can find the documentation here . One of the great things about CockroachDB’s support for SQL is the wide variety of frameworks and tools for working with SQL data. Today, we’ll demonstrate this by building a simple application in Python, using SQLAlchemy and Flask . Adapting SQLAlchemy to CockroachDB Every SQL database is a little bit different, so a library like SQLAlchemy requires some code (called a dialect ) to adapt its interface to the database in use. CockroachDB is similar enough to PostgreSQL that SQLAlchemy’s built-in PostgreSQL dialect gets us most of the way there, but we still need a few tweaks that can be found in our cockroachdb python package . As of this writing, those tweaks are: Reflection in CockroachDB uses commands like SHOW TABLES instead of the pg_tables database. The best type for an automatic ID column is INT DEFAULT unique_rowid() instead of SERIAL . We don’t yet support foreign keys . We require special SAVEPOINT statements for the most efficient transaction retries . This will be discussed in detail below. To use this package, simply pip install cockroachdb and configure SQLAlchemy with a URL that begins with cockroachdb:// instead of postgresql:// . Getting Started We’re going to start with Flask-SQLAlchemy’s example app . You can follow along as we make our changes, or get the finished product from our repo. Install pip if you don’t already have it. Install flask-sqlalchemy and the cockroachdb python package: $ pip install flask-sqlalchemy cockroachdb If you haven’t already, install CockroachDB and start a server . The rest of this tutorial assumes that you’re running a local cluster in insecure mode. Clone the cockroachdb/examples-python repository and move into the flask-sqlalchemy directory: $ git clone https://github.com/cockroachdb/examples-python $ cd examples-python/flask-sqlalchemy/ Run this shell script to create a database for the application, grant an example user access to the database, and create the tables: #!/bin/sh\nset -ex\n\ncockroach sql --insecure -e 'DROP DATABASE IF EXISTS example flask sqlalchemy'\ncockroach sql --insecure -e 'CREATE DATABASE example flask sqlalchemy'\ncockroach sql --insecure -e 'GRANT ALL ON DATABASE example flask sqlalchemy TO example'\n\npython -c 'import hello; hello.db.create_all()' Finally, run the application: $ python hello.py Visit http://localhost:5000 in your browser to see the application running. Handling restarted transactions Unlike most relational databases, CockroachDB uses optimistic concurrency control instead of locking . This means that when there is a conflict between two transactions one of them is forced to restart, instead of waiting for the other to complete. Transactions are sometimes forced to restart due to deadlocks even in databases that don’t use optimistic concurrency control, but it’s much less common, so many applications just return an error and don’t even attempt to retry. In CockroachDB, restarted transactions are common enough that it’s important to handle them correctly. We provide a function to help with this: cockroachdb.sqlalchemy.run_transaction . It’s a little more cumbersome to use, because you can no longer use the global db.session or Model.query objects, but this protects you from accidentally reusing objects via the Session from outside the transaction. Here is one function modified to use run_transaction ; you can see the rest of the changes in this diff : from flask import Flask\nimport sqlalchemy.orm\nfrom cockroachdb.sqlalchemy import run_transaction\n\napp = Flask(__name__)\napp.config.from_pyfile('hello.cfg')\ndb = SQLAlchemy(app)\nsessionmaker = sqlalchemy.orm.sessionmaker(db.engine)\n\n@app.route('/new', methods=['GET', 'POST'])\ndef new():\n    if request.method == 'POST':\n        if not request.form['title']:\n            flash('Title is required', 'error')\n        elif not request.form['text']:\n            flash('Text is required', 'error')\n        else:\n            def callback(session):\n                todo = Todo(request.form['title'], request.form['text'])\n                session.add(todo)\n            run_transaction(sessionmaker, callback)\n            flash(u'Todo item was successfully created')\n            return redirect(url_for('show_all'))\n    return render_template('new.html') Under the hood, run_transaction() is using the SAVEPOINT statement. This is a standard SQL statement (normally used to support nested transactions) that CockroachDB uses in a special way. We don’t support nested transactions, but we do support the special case of a single SAVEPOINT that covers the entire transaction: BEGIN;\nSAVEPOINT cockroach_restart;\nINSERT INTO todos VALUES (...);  -- first attempt\nROLLBACK TO SAVEPOINT cockroach_restart;  -- failed; try again\nINSERT INTO todos VALUES (...);  -- second attempt\nRELEASE SAVEPOINT cockroach_restart;  -- success!\nCOMMIT; RELEASE SAVEPOINT is a kind of COMMIT . In fact, for the special case of SAVEPOINT cockroach_restart , RELEASE SAVEPOINT is the COMMIT . The transaction is fully committed at this point; the final COMMIT is just to match the first BEGIN . By structuring the transaction in this way, the server is able to preserve some information about the previous attempts to allow the retries to complete more easily, which it couldn’t do if the retries were independent top-level transactions. Conclusion There’s much more that we haven’t covered (for example, SQLAlchemy’s “core” API is supported as well if you don’t want to use the ORM/Session layer), but we hope this post serves as an introduction to using CockroachDB with an existing framework. Fans of other frameworks may also want to check out the implementation of the cockroachdb package to see what is involved in adapting a new framework to CockroachDB.", "date": "2016-06-01"},
{"website": "CockroachLabs", "title": "Serializable, Lockless, Distributed: Isolation in CockroachDB", "author": ["Matt Tracy"], "link": "https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/", "abstract": "Editor's Note : This post was originally authored when CockroachDB was pre-1.0. CockroachDB's architecture has undergone many changes since then. One of the most significant, as it relates to this post which focuses on our previous \"lockless\" design, is that we now use more locking and lock-like structures to provide SERIALIZABLE isolation. For more current details about CockroachDB's transaction model, read our transaction layer architecture documentation . --------------- Several months ago, I discussed how CockroachDB’s distributed transactions are executed atomically. However, that discussion was incomplete; it ignored the concept of concurrency, where multiple transactions are active on the same data set at the same time. CockroachDB, like all database systems, tries to allow as much concurrency as possible in order to maximize access to the data set. Unfortunately, our atomicity guarantee is not sufficient to keep the database consistent in a world of concurrent transactions. Recall that guarantee: For a group of database operations, either all of the operations are applied or none of them are applied. What this does not address is the way that concurrent transactions may interleave. The individual operations (reads and writes) in a transaction do not happen simultaneously; there is time in between the individual operations. In a concurrent system, one transaction may commit during the execution window of a second transaction; even if the first transaction (T1) commits atomically, this can still allow operations later in the second transaction (T2) to see the results of T1, even though earlier operations on T2 did not see the results of T1. This interleaving can create a number of undesired anomalies , ultimately breaking the consistency of the database. To protect against these anomalies, we require an Isolation guarantee: For a group of atomic, concurrent transactions, the commit of one transaction may not interleave with the operations of another transaction. Perfect isolation can be trivially achieved through serial execution: executing all transactions on the system one at a time, with no concurrency. This has terrible performance implications; fortunately, it is also unnecessary to achieve perfect isolation. Many concurrent databases, including CockroachDB, instead offer serializable execution, which is equivalent to serial execution while allowing a considerable level of concurrent transactions. CockroachDB’s default isolation level is called Serializable Snapshot. It is an optimistic, multi-version, timestamp-ordered concurrency control system with the following properties: Serializable: The resulting database state is equivalent to a serial execution of component transactions. Recoverable: A set of database transactions is considered recoverable if aborted or abandoned transactions will have no effect on the database state. Our atomic commit system already guarantees that individual transactions are recoverable; our Isolation system uses strict scheduling to ensure that any combination of transactions is also recoverable. Lockless: Operations execute without taking locks on resources. Correctness is enforced by aborting transactions which would violate either serializability or strict scheduling. Distributed: There is no central oracle, coordinator or service involved in this system. Providing Serializable Execution CockroachDB uses a multi-version timestamp ordering to guarantee that its complete transaction commit history is serializable. The basic technique has been textbook material for three decades, but we will briefly go over how it works: Serializability Graphs To demonstrate the correctness of timestamp ordering, we look to serializability theory , and specifically one of its core concepts, the serializability graph. This graph is used to analyze a history of database transactions in terms of operation conflicts . In the theory, a conflict occurs when two different transactions perform an operation on the same piece of data (one after the other), where at least one of the operations is a write. The second operation is said to be in conflict with the first operation. There are three types of conflicts: Read-Write (RW) – Second operation overwrites a value that was read by the first operation. Write-Read (WR) – Second operation reads a value that was written by the first operation. Write-Write (WW) – Second operation overwrites a value that was written by first operation. For any given transaction history, these conflicts can be used to create a serializability graph, which is a directed graph linking all transactions. Transactions are nodes in the graph. Whenever an operation conflicts with an operation from a different transaction, draw a directed edge from the conflicting operation to the conflicted operation. Figure 1: Example of a serializability graph for a simple transaction history. And we now arrive at a key statement of this theory: a history is guaranteed to be serializable if (and only if) its serializability graph is acyclic.  ( Proof , for those interested). Figure 2: Example of a transaction history with a cyclic serializability graph. This history is not serializable. CockroachDB’s timestamp ordering guarantees an acyclic serializability graph , and this is straightforward to demonstrate: Every transaction is assigned a timestamp (from the node on which it starts) when it begins. All operations in that transaction take place at this same timestamp, for the duration of the transaction. Individual operations can locally determine when they conflict with another operation, and what the transaction timestamp of the conflicted operation is. Operations are only allowed to conflict with earlier timestamps; a transaction is not allowed to commit if doing so would create a conflict with a later timestamp. By disallowing any conflicts that flow against the timestamp-ordered direction, cyclic serializability graphs are impossible. However, let’s explore in detail how CockroachDB actually goes about detecting and disallowing these conflicts. Write-Read Conflicts – MVCC Database This is where the “multi-version” aspect of our control mechanism comes into play. CockroachDB keys do not store a single value, but rather store multiple timestamped versions of that value. New writes do not overwrite old values, but rather create a new version with a later timestamp. Figure 3: Comparison of multi-versioned value store with a single-value store. Note that the multi-version store is sorted by timestamp. Read operations on a key return the most recent version with a lower timestamp than the operation: Thus, it is not possible in CockroachDB to form WR conflicts with later transactions; read operations will never read a value with a later timestamp. Note: This is the “Snapshot” in serializable snapshot; in-progress transactions essentially see a temporal snapshot of the database, ignoring anything committed later. Read-Write Conflicts – Read Timestamp Cache On any read operation, the timestamp of that read operation is recorded in a node-local timestamp cache . This cache will return the most recent timestamp at which the key was read. All write operations consult the timestamp cache for the key they are writing; if the returned timestamp is greater than the operation timestamp, this indicates a RW conflict with a later timestamp.  To disallow this, the operation (and its transaction) must be aborted and restarted with a later timestamp. The timestamp cache is an interval cache, meaning that its keys are actually key ranges. If a read operation is actually a predicate operating over a range of keys (such as a scan), then the entire scanned key range is written to the timestamp cache. This prevents RW conflicts where the key being written was not present during the scan operation. The timestamp cache is a size-limited, in-memory LRU (least recently used) data structure, with the oldest timestamps being evicted when the size limit is reached. To deal with keys not in the cache, we also maintain a “low water mark”, which is equivalent to the earliest read timestamp of any key that is present in the cache. If a write operation writes to a key not present in the cache, the “low water mark” is returned instead. Write-Write Conflicts – Can only write the most recent version of a key If a write operation attempts to write to a key, but that key already has a version with a later timestamp than the operation itself, allowing the operation would create a WW conflict with the later transaction.  To ensure serializability, the operation (and its transaction) must be aborted and restarted with a later timestamp. By choosing a timestamp-based ordering, and rejecting all conflicts which disagree with that ordering, CockroachDB’s Serializable Snapshot guarantees a serializable schedule. Recoverable with Strict Scheduling While the previous conflict rules are sufficient to guarantee a serializable history, a different concern arises when two uncommitted transactions have a conflict: even if that conflict is allowed by our timestamp ordering rules, additional rules are required to ensure that the transaction schedule remains recoverable . The issue of can be explained with an example: consider two transactions [T1, T2], where timestamp(T1) < timestamp(T2). T1 writes to a key ‘A’. Later, T2 reads from key ‘A’, before T1 has committed. This conflict is allowed according to our timestamp ordering rules. However, what value should T2’s read retrieve from ‘A’? Assume it ignores the uncommitted value written by T1, and retrieves the previous value instead. If T1 and T2 both commit, this will create a WR conflict with T2 (T1 will have overwritten a value read by T2). This violates our timestamp ordering guarantee, and thus serializability. Assume it retrieves the value written by T1. If T2 commits, but T1 later aborts, this will have violated the atomicity of T1: T1 still will have had an effect on the database state, even though it aborted. Thus, neither possibility can allowed: in this situation, there is no way that T2 can be safely committed before T1 while maintaining a recoverable schedule. CockroachDB uses strict scheduling to handle this situation: operations are only allowed to read or overwrite committed values; operations are never allowed to act on an uncommitted value. Enforcing Strict Scheduling As established in our atomicity post , uncommitted data is staged in intents on each key, for the purpose of atomic commits. In an MVCC data store, the intent on a key (if present) is stored in a special value which sorts immediately before the most recent committed value: In our previous post on atomicity, we assumed that any intent encountered by a transaction was the result of an abandoned transaction; however, in a concurrent environment, the intent might instead be from a concurrent transaction which is still running. Strict scheduling actions are required in two situations: if a read operation encounters an intent with a lower timestamp, or if a write encounters any intent from another transaction (regardless of timestamp ordering). In these situations, there are two options available to CockroachDB: If the second transaction has a higher timestamp, it can wait for the first transaction to commit or abort before completing the operation. One of the two transactions can be aborted. As an optimistic system (no waiting), CockroachDB always chooses to abort one of the transactions. The process of determining which transaction is as follows: The second transaction (which is encountering an intent) looks up the first transaction’s transaction record, the location of which is present in the intent. The transaction performs a “ push ” on the discovered transaction record. The push operation is as follows: If the first transaction is already committed (the intent was not yet cleaned up), then the second transaction can clean up the intent and proceed as if the intent were a normal value. Likewise, if the other transaction already aborted, the intent can be removed and the second transaction can proceed as if the intent were not present. Otherwise, the surviving transaction is deterministic according to priority . It is not optimal to always abort either the pusher or pushee; there are cases where both transactions will attempt to push the other, so “victory” must be deterministic between any transaction pair. Each transaction record is thus assigned a priority ; priority is an integer number. In a push operation, the transaction with the lowest priority is always aborted (if priority is equal, the transaction with the higher timestamp is aborted. In the extremely rare case where both are equal, the pushing transaction is aborted). New transactions have a random priority. If a transaction is aborted by a push operation and is restarted, its new priority is max(randomInt(), [priority of transaction that caused the restart] - 1]); this has the effect of probabilistically ratcheting up a transaction’s priority if it is restarted multiple times. In this way, all conflicts between uncommitted transactions are immediately resolved by aborting one of the transactions, thus enforcing strict scheduling and guaranteeing that all transaction histories are recoverable. Note on Abandoned Transactions As mentioned earlier, in a concurrent environment we can no longer assume that unresolved write intents belong to abandoned transactions; we must deal with abandoned transactions in a different way. The priority system already aborts abandoned transactions probabilistically – transactions blocked by the abandoned transaction will eventually have a high enough priority to usurp it. However, we additionally add a heartbeat timestamp to every transaction. While in progress, an active transaction is responsible for periodically updating the heartbeat timestamp on its central transaction record; if a push operation encounters a transaction with an expired heartbeat timestamp, then it is considered abandoned and can be aborted regardless of priority. Wrap Up We have now demonstrated how CockroachDB’s Isolation system is able to provide a serializable and recoverable transaction history in a completely distributed fashion. Combined with our atomic commit post, we have already described a fairly robust system for executing concurrent, distributed ACID transactions. That said, there are still many aspects to CockroachDB’s transaction system that we have not yet covered. For example, CockroachDB offers another, more relaxed isolation level known as Snapshot (without the “serializable”) . Like relaxed isolation levels in other database systems, this mode increases concurrency performance by allowing transactions to interleave in certain cases; for some applications, this is an acceptable tradeoff. Another aspect is how CockroachDB provides linearizable access to its data. Linearizability is a property that can be difficult to provide in a distributed system. Spencer Kimball has already written this blog post demonstrating how CockroachDB deals with this in some detail (contrasting it with the way a similar system, Google’s Spanner, does the same); however, we may eventually write an additional linearizability blog post focused more directly on our transaction system. Stay tuned, and please let us know if any of these potential topics are of particular interest. And if distributed transactions are your jam, check out our open positions here .", "date": "2016-05-04"},
{"website": "CockroachLabs", "title": "Index selection in CockroachDB", "author": ["Radu Berinde"], "link": "https://www.cockroachlabs.com/blog/index-selection-cockroachdb-2/", "abstract": "In an earlier post we discussed how CockroachDB maps SQL table data and table indexes to key-value storage. In this post, we will go over some of the factors involved in choosing the best index to use for running a certain query. Introduction to indexes Tables are internally organized according to a certain column (or group of columns); this makes searching for rows according to values in that column or column group very efficient, even if the table contains a large number of rows. But what if sometimes we need to search according to values for a different column? To find such results the system would have to go through all the rows in the table, which can be painfully slow for large tables. We can avoid this by telling the system to maintain an index : an additional structure which organizes the data according to a different column (or group of columns). We illustrate this with a simple example. Consider that we are maintaining a database of songs. We may have a Songs table as follows: CREATE TABLE Songs (\n    ID     INT PRIMARY KEY,\n    Title  STRING,\n    Artist STRING,\n    Album  STRING,\n    Year   INT\n) The ID is a number used to uniquely identify a certain song. The rows of this table are internally stored in order according to the ID, which allows us to retrieve the details of a certain song very efficiently: EXPLAIN SELECT * FROM Songs WHERE ID = 123 Level Type Description 0 scan Songs@primary /123-/124 The EXPLAIN statement is used to look at the details of how the SQL engine is going to run the query (known as a plan ). In this case, the range /123-/124 is showing that we will scan rows with IDs at least 123 and strictly less than 124 – so just the row with ID 123. Ignore Level for now, we’ll get to that later. Unfortunately, if we want to find the songs of a certain artist, CockroachDB will need to scan the entire table and compare each row against the artist name: EXPLAIN SELECT ID FROM Songs WHERE Artist = 'Rick Astley' Level Type Description 0 scan Songs@primary - The unbounded - range means we have to scan the entire table. This is no good if we have a sizable database of songs. The solution is to use an index on the artist: CREATE INDEX ByArtist ON Songs (Artist) This index allows us to efficiently find things by the artist name, at the cost of keeping the index updated whenever we make a change to the Songs table. The details of how the index is stored internally are discussed in another blog post . For the purposes of this discussion, we can think of the index as a list of entries, one for each song in the original table. Each entry contains the artist and the ID (“primary key”) of the song. The entries are available in sorted order. With this, the query above looks better: Level Type Description 0 scan Songs@ByArtist /\"Rick Astley\"-/\"Rick Astley\\x00\" In English: we will directly retrieve the entries that exactly match the given artist name. Let’s take a closer look at this example. We mentioned that each entry in the ByArtist index contains the artist and the song ID; and our sample query only asks for the ID of the song we are looking for – how (suspiciously) convenient! But what if we want more information, like the title of the song? Covering vs non-covering indexes Let’s see what happens when our query requests a column that is not part of the index: EXPLAIN SELECT ID, Title FROM Songs WHERE Artist = 'Rick Astley' Level Type Description 0 index-join 1 scan Songs@ByArtist /\"Rick Astley\"-/\"Rick Astley\\x00\" 1 scan Songs@primary What just happened? Because the index only contains the song IDs for each artist, we cannot retrieve the song title from the index. We need to “join” the data from the index with the data in the main ( primary ) table, which involves getting the song IDs from the index and then retrieving the corresponding titles for those IDs from the main table. The implementation of this query is divided into reusable pieces of functionality that form a tree. The root of the tree (level 0) is the index-join node, which coordinates the retrieval of rows performed by the two scan nodes (leaves) on level 1. Retrieving the titles from the main table happens in batches: each batch corresponds to a set of song IDs from the ByArtist index. In general, the rows we are looking for in the main table are at arbitrary positions (and not sequential), making this retrieval more expensive than a regular scan. For this query, ByArtist is what we call a non-covering index, in that it doesn’t “cover” all the columns we need to retrieve. For the previous query (where we only wanted the IDs), the same index was covering . A covering index is preferable to a non-covering index in terms of efficiency. What if the query above is frequent and we want to make it more efficient? We can change the index to make it store the title as well: CREATE INDEX ByArtist ON Songs (Artist) STORING (Title) Level Type Description 0 scan Songs@ByArtist /\"Rick Astley\"-/\"Rick Astley\\x00\" Alternatively we could index both the artist and the title, with a similar result: CREATE INDEX ByArtist ON Songs (Artist, Title) The difference is that this index not only makes the song titles available, but it makes the songs of any given artist available in sorted order – which can be useful for some queries (as we will see). Of course, regardless of the method, changing the index to store an extra column is not free: the space used by the index will increase accordingly. One needs to be careful when adding columns to indexes, especially if the amount of data stored in those columns can be large. Because using a non-covering index is more expensive, this is a factor that we care about during index selection – but it is not the most important factor. Restricting the search space When considering different indexes, the main objective is to minimize the amount of row data we have to scan; the more restrictive our search ranges are, the better. Suppose we want to find all albums starting with the letter W that came out either in 1987 or 1989 and we have an index where the songs are primarily ordered by album: CREATE INDEX ByAlbum ON Songs (Album, Year, Title)\nEXPLAIN SELECT Title FROM Songs\n    WHERE Year IN (1987, 1989) AND Album >= 'W' AND Album < 'X' Level Type Description 0 scan Songs@ByAlbum /\"W\"-/\"X\" We can narrow down the search space to those albums with the correct starting letter – all strings that are lexicographically ordered after W and before X . But if we have an index where songs are primarily ordered by year, and within the same year ordered by the album: CREATE INDEX Discographies ON Songs (Year, Album, Title) Then the search can be much more focused; the same query yields the following plan: Level Type Description 0 scan Songs@Discographies /1987/\"W\"-/1987/\"X\" /1989/\"W\"-/1989/\"X\" The first range maps to albums released in the year 1987 which are lexicographically ordered between W and X . This index is preferable (for this query), and it will be chosen by the system when both indexes in this example are available. Index ordering Another factor involved in choosing an index is the desired result ordering. Suppose we add another index: CREATE INDEX ByYear ON Songs (Year, Title) Say we want to look for all songs with a certain substring in the title and we want the results ordered by artist: EXPLAIN SELECT ID, Title FROM Songs WHERE Title LIKE '%Give You Up%' ORDER BY Artist Level Type Description 0 nosort +Artist 1 scan Songs@ByArtist - Or by year: EXPLAIN SELECT ID, Title FROM Songs WHERE Title LIKE '%Give You Up%' ORDER BY Year Level Type Description 0 nosort +Year 1 scan Songs@ByYear - Unfortunately we can’t do anything better than going through all the songs and checking the title. But we can at least choose the index that gives us the results in the order we want, avoiding any resorting (as signified by the nosort node). Seems simple enough conceptually, but there are some subtle nuances to matching an ordering. For example, the ByArtist index can be used to provide an order by title if we know all results have the same artist: EXPLAIN SELECT ID FROM Songs WHERE Artist = 'Rick Astley' ORDER BY Title Level Type Description 0 nosort +Title 1 scan Songs@ByArtist /\"Rick Astley\"-/\"Rick Astley\\x00\" This is because ByArtist is an index on (Artist, Title) so it not only stores entries in artist order, but within the same artists they are in title order. Of course, we cannot avoid sorting if our query can potentially involve multiple artists: EXPLAIN SELECT ID FROM Songs WHERE Artist >= 'R' AND Artist < 'S' ORDER BY Title Level Type Description 0 nosort +Title 1 scan Songs@ByArtist /\"R\"-/\"S\" A particular ordering might also be desired when using aggregate functions MIN and MAX . If we want to get the first artist name (in lexicographical order), using the ByArtist index is immensely useful: we only need to get first entry in the index: EXPLAIN SELECT MIN(Artist) FROM Songs Level Type Description 0 group MIN(Artist) 1 scan Songs@ByArtist 1:- The 1:- signifies that within the otherwise unbounded range - , we only need the first entry. As above, the same index is as useful if we want the first song of a certain artist: Level Type Description 0 group MIN(Title) 1 scan Songs@ByArtist 1:/\"Rick Astley\"/#-/\"Rick Astley\\x00\" Limits Given the multitude of factors to take into consideration, it is not surprising that many times we have to make a non-trivial choice between indexes. We may have an index that provides the ordering we are looking for, but might be less restrictive in terms of search space, or might be non-covering. Sometimes the LIMIT clause of a SELECT statement can tip the scale and change the outcome. Getting back to the first index we looked at: CREATE INDEX ByArtist ON Songs (Artist) As discussed, If our query must return more than the song ID and the artist, this index is non-covering and we must cross-reference each result with the row in the original table. So if we want song titles, even if we want them in the order that matches this index, the system will choose not to use it: EXPLAIN SELECT Title FROM Songs ORDER BY Artist Level Type Description 0 sort +Artist 1 scan Songs@primary - Because we have to go through all the songs anyway, it is cheaper to sort the results ourselves rather than using the non-covering index. But a LIMIT can throw a wrench in the works of this reasoning: if we only want the first few songs, it will be very wasteful to scan the entire table, sort the rows, and then throw away everything except the top results. In this case, using the index is preferable – despite the overhead associated with non-covering indexes – as we will only have to go through a handful of rows: EXPLAIN SELECT Title FROM Songs ORDER BY Artist LIMIT 10 Level Type Description 0 limit count: 10, offset: 0 1 nosort +Artist 2 index-join 3 scan Songs@ByArtist - 3 scan Songs@primary Because we don’t have to sort the results – they just flow through the nosort node – we can stop scanning for rows as soon as we get the 10 results we are looking for (as orchestrated by the limit node). Index hints What if we hit a case where the index algorithms don’t correctly identify the best index? Hopefully that is rare, but CockroachDB provides a way to force the use of a specific index by appending @index_name or, equivalently, @{FORCE_INDEX=index_name} to the table name. Take the following indexes and query: CREATE INDEX ByArtist ON Songs (Artist)\nCREATE INDEX ByTitle ON Songs (Title)\nEXPLAIN SELECT Title FROM Songs\n    WHERE Artist >= 'A' AND Artist < 'Z' AND Title >= 'A' AND Title < 'F' Level Type Description 0 index-join 1 scan Songs@ByArtist /\"A\"-/\"Z\" 1 scan Songs@primary This is a query where both indexes would work well. If the ByTitle index is more efficient, we can force it using either one of these statements: EXPLAIN SELECT Title FROM Songs@ByTitle\n    WHERE Artist >= 'A' AND Artist < 'Z' AND Title >= 'A' AND Title < 'F'\nEXPLAIN SELECT Title FROM Songs@{FORCE_INDEX=ByTitle};\n    WHERE Artist >= 'A' AND Artist < 'Z' AND Title >= 'A' AND Title < 'F' Level Type Description 0 index-join 1 scan Songs@ByTitle /\"A\"-/\"F\" 1 scan Songs@primary If we want to disallow use of non-covering indexes, we can use @{NO_INDEX_JOIN} . We can also disallow using any index by specifying @primary : EXPLAIN SELECT Title FROM Songs@{NO_INDEX_JOIN}\n    WHERE Artist >= 'A' AND Artist < 'Z' AND Title >= 'A' AND Title < 'F'\nEXPLAIN SELECT Title FROM Songs@primary\n    WHERE Artist >= 'A' AND Artist < 'Z' AND Title >= 'A' AND Title < 'F' Level Type Description 1 scan Songs@primary Future directions CockroachDB is starting out with a solid index selection implementation that takes into account all the factors we discussed. But the choices we make are “static” in the sense that they don’t depend on the data itself – only on its structure. In the future we plan to keep track of various metrics like table sizes and distribution of values to make more intelligent decisions. We will also consider feedback mechanisms for frequent queries, where we sometimes try out a different plan, analyze its efficiency, and potentially switch to that plan in subsequent instances of similar queries. Do all things distributed SQL and indexing make you giddy? Then good news — we're hiring! Check out our open positions here .", "date": "2016-04-21"},
{"website": "CockroachLabs", "title": "Outsmarting Go Dependencies in Testing Code", "author": ["Andrei Matei", "Radu Berinde"], "link": "https://www.cockroachlabs.com/blog/outsmarting-go-dependencies-testing-code/", "abstract": "Reading time: 9 minutes Writing good tests is tricky when the system has a lot of moving parts. When using Go ’s testing infrastructure, tests that involve multiple modules can cause dependency cycles which are not allowed by the compiler. In this post we will go over a technique we devised to break these dependency cycles. Background The CockroachDB Go code base is split up into various packages; some of the major ones are: storage : interfaces with the local stores kv : key-value store sql : SQL layer (on top of the key-value store) server : high level code for setting up a CockroachDB node exposing a PostgreSQL interface on a network port. A node is, among other things, both a kv and a sql server. We will focus on just the sql and server packages. The server package depends on the sql package – as it should, since the server code sets up the SQL server part of the CockroachDB node. Most sql tests involve setting up a test server, running some SQL statements and potentially peeking at or poking some internal implementation detail. To start a test server, we want to be able to leverage the code in server which sets everything up for us. But tests in the sql package cannot depend on the server package because that creates a circular dependency. This problem is not specific to CockroachDB – we suspect many large go codebases could run into this problem as tests tends to use shortcuts that cross logical boundaries. After all, all’s fair in love, war, and testing code. The Initial Solution Our first solution was to use Go ’s facility for black box testing (testing only through a package’s public interface). Go allows tests in a package like sql to be declared as being part of a sql_test package. This is a separate package as far as dependencies are concerned so it breaks the dependency cycle, allowing us to import server . The downside is that we don’t have access to the internals of sql from this package! So we were forced to export various internals for the sole purpose of accessing them from tests, or split off parts of the sql_test code and put them in sql test code.* This got to be more and more annoying as time went on. When we started work on a new distsql package for what will become our distributed-SQL implementation, we again were forced to expose a lot of package internals for tests. It was time to investigate a better solution. Toward a Better Solution What we really wanted was to write tests in the sql package from where we can directly access the sql internals. The only way to call out to server code for instantiating test servers would be indirectly, through a shim layer – a module that does not depend on either sql or server but which can be used to indirectly interface between them: We worked on a simple proof-of-concept which illustrates the idea. The server and sql packages represent the real packages as described so far. The testingshim defines an interface for the server functionality that we want to access from sql tests, but it doesn’t actually depend on either server or sql . Methods that need to use (or return) types defined in sql can do so indirectly, using interface{} : package testingshim\n\n// TestServerInterface defines test server functionality that tests need.\ntype TestServerInterface interface {\n  SQLSrv() interface{}\n  // Other needed stuff goes here.\n}\n\n// TestServerFactory encompasses the actual implementation of the shim\n// service.\ntype TestServerFactory interface {\n  // New instantiates a test server instance.\n  New() TestServerInterface\n} This layer also holds a key piece of global state: serviceImpl can be set to an external implementor of the interface defined here (via InitTestServerFactory ): var serviceImpl TestServerFactory\n\n// InitTestServerFactory should be called once to provide the implementation\n// of the service. It will be called from a xx_test package that can import the\n// server package.\nfunc InitTestServerFactory(impl TestServerFactory) {\n  serviceImpl = impl\n}\n\nfunc NewTestServer() TestServerInterface {\n  return serviceImpl.New()\n} The idea would be that a type in server implements the TestServerFactory interface, and something that has access to both server and testingshim calls InitTestServerFactory , allowing sql tests to call functions like NewTestServer . “ Something” was where we got stuck for a while, until.. The Hack The final piece of the puzzle also revolves around the black box testing facility that allows for a sql_test package, but used in a more ingenious way. The go test documentation states: Test files that declare a package with the suffix “_test” will be compiled as a separate package, and then linked and run with the main test binary. So if we had sql_test code that used server , the server code would be in there somewhere; Go just won’t allow us to access it from tests declared as part of sql . The “aha” moment was when someone pointed out TestMain() . TestMain is an optional function that can be used for doing extra setup before testing; a single TestMain can live in either the sql or sql_test package. By putting it in sql we are able to run initialization code which has access to server before running sql tests! Note: A possible alternative to TestMain would be to use an init() function in a sql_test file. This is again illustrated in our proof-of-concept : in the sql_test package, TestMain has access to both the server code and the testingshim . It can initialize the TestSrvInstance global with a type implemented by server : func TestMain(m *testing.M) {\n  ..\n  testingshim.InitTestServerFactory(server.TestServerFactory)\n  ..\n} And that allows sql tests to use testingshim.NewTestServer() : package sql\n..\nfunc TestFoo(t *testing.T) {\n   testingshim.NewTestServer().SQLSrv().(*SQLServer).Woof()\n} The dependency graph is: The full fledged change was of course more involved, but it follows this simple recipe. This one-time effort of creating the dependency-free testingshim package was worth the ease of writing tests going forward, especially as we can easily make use of the same framework in other packages . Go coders out there – if you hit the same problem and find this trick useful, let us know in the comments below! *_Updated on June 17, 2016_", "date": "2016-06-16"},
{"website": "CockroachLabs", "title": "Modesty in Simplicity: CockroachDB's JOIN", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/cockroachdbs-first-join/", "abstract": "CockroachDB’s JOIN: An Early Implementation When our VP of engineering, Peter Mattis, made the decision in 2015 to support SQL, little did he know that the team would get as far as shipping the first implementation of CockroachDB’s JOIN exactly one year after that. A celebration is in order! The good news is that CockroachDB’s JOIN seems to work, as in, “it returns correct results.” However, we’d like to underline that this is just our first, unoptimimized implementation. Like, really not optimized. As in “this will cause your client to wait forever and/or your server to stop with an out-of-memory-error if you join large tables” not optimized.  And we are sharing this blog post today to explain why this is so and what we are going to do about it. Of course, a better JOIN is on the roadmap. There will be a version 2, then perhaps 3 and further, as we figure how our users would best like their JOINs to behave. We are providing the SQL JOIN operation in CockroachDB today first and foremost to establish a baseline. A baseline for testing, for experimentation, for benchmarking. Even if this implementation probably isn’t ready for most production uses yet, by including it now, we’ll be able to ensure that the feature is tested and maintained as we work to improve and expand it. Under the hood The primary motivation for this first implementation is to serve as a baseline reference point during tests. While preparing this first shot, we aimed for the simplest, most natural implementation that provides correct result rows, deliberately ignoring the last 40 years of database research on how to perform fast JOINs. One advantage of this approach is that it’s rather simple to analyze and validate manually. The other advantage is that it is also easy to show and explain! For example, there is really not much more in the code that the following flowchart does not show: What this flowchart tells you: the algorithm reads all the rows from the JOIN’s right operand into memory and then, for each row read from the left operand, looks up the corresponding row in memory and produces the joined row if a match is found. Some extra conditions here and there deal with the NULL rows produced by OUTER JOIN when no match is found. The only allowance for cleverness in this code was to make it even simpler: when a query specifies RIGHT OUTER JOIN, we swap the operands and run it with the logic of LEFT OUTER JOIN. So this time, we optimized for simplicity and thus not for performance. Concretely, given N rows in the left operand and M in the right operand, the time complexity of a single JOIN is O(N×M) for now, and thus quadratic for self-joins, and the space overhead is O(M). This is far from the usual linear time complexity of a moderately optimized ordered join, or the constant space overhead when joining using a key or index. In short, unless your JOIN right operand has very few rows, you probably don’t want to use this for production workloads yet. What this first JOIN brings to the table By supporting SQL JOIN, we enable developers to follow many existing SQL tutorials and courses with CockroachDB. It enables more diverse evaluations of CockroachDB and creates a richer environment for learning and tinkering. For us internally, it provides a basis for comparison during tests. We could not easily reuse existing JOIN tests from other databases because our typing system is slightly different and their test/reference queries do not behave exactly the same with CockroachDB. So we prepared this version to build our own tests. There is another advantage to our users as well. Through the rest of 2016, we aim to increase compatibility between CockroachDB and popular ORM s. These ORMs make heavy use of database introspection, querying the information schema with JOINs to infer table structure. Our first implementation of JOINs will enable ORM compatibility . The information schema contains mostly small tables, for which performance is not a significant concern. We like to illustrate our progress during Beta development like this: Image Credit: Henrik Kniberg Our JOIN may not be production-ready yet, but at least it’s functional and you can already start using it to discover CockroachDB. Next steps on the horizon: performance optimizations Our overall implementation strategy for CockroachDB is to start with those features of SQL that are most useful to our users. In other words, we’d want our JOIN in version 1.0 to be reasonably fast for the most common use cases. In OLTP applications, we see that most requests join products with categories, prices with tax rates, customers with organisations, etc. The common characteristic of these use cases is that the join column is either a primary key or has an index. That’s where we plan to optimize first: use the ordering information from the JOIN operands and reduce the operation’s time complexity back to linear. From that point forward, we will also start applying some of the state-of-the-art in database theory. For cases where the database doesn’t provide a native ordering for joined data, we’ll perform sorting and merging instead of nested loops, and prepare the sorted data on-disk instead of keeping it all in memory where it doesn’t fit. Likely, hash joins will also appear in CockroachDB before long. Meanwhile, we have already started working on a new distributed query execution engine a few months ago. This feature is intended to parallelize the execution of complex operations over multiple nodes in the cluster. In particular, sorting and merging and hash joins have “ natural” extensions in a distributed implementation . There are caveats of course, like the fact we can’t promise fast results if you simultaneously issue complex JOINs from clients connected to all nodes in the cluster! (Parallelization brings more speed only if there are more available nodes to execute the query than there are nodes demanding query results.) However, we expect a performance boost for common workloads using distributed SQL execution. Expect more blog posts to explain our distributed query engine later in 2016. For now, we can already share that we used Google’s research on Sawzall for inspiration. Beyond that, we’ll work on optimization as we learn how our users like to use CockroachDB. We do not believe it is wise to already start implementing classical optimizations before we know which are applicable to the sort of workloads our users want to throw at a distributed database. We suspect we may need to develop new types of optimizations for CockroachDB because the bottlenecks will not appear in the same places as in traditional RDBMs. Stay tuned! A note about feature parity If you’re following the development of CockroachDB, you may have noticed several choices that seem to follow a pattern. We support SQL. We support the same client/server SQL protocol as PostgreSQL. We support SQL functions with the same names as PostgreSQL, and we even often reuse the same error messages! And we now support JOINs, including FULL OUTER JOINs which are not common but are supported by PostgreSQL. The temptation might be great to deduce that we want to match PostgreSQL feature-wise, and perhaps suggest we’ll soon claim to become a drop-in replacement! However, please resist this temptation. First of all, we believe that attempting to reach feature parity this early would be rather unwise, not to say unrealistic. CockroachDB has less than ten man-years invested in its SQL features, where PostgreSQL already has accumulated hundreds. Also, we are currently spending quality time on CockroachDB’s internals to stabilize things and guarantee scalability; making our SQL front-end much larger now would create inertia against productive work on the core technology. Furthermore, our goal for CockroachDB version 1.0 is that the product can support new businesses and projects built around OLTP workloads. A basic functional JOIN was a strong demand for the “minimum relational toolbox”; we now plan to make it fast for those queries where our users need that. This also means we’d love to hear from you and learn how you like to use JOIN. That said, we’re not actively bumping compatibility, including performance compatibility, off the road map, either. We promise you will see compatibility increase over time. Take correlated sub-queries , for example, which are not that common in application code (they can be replaced by JOINs in many workloads). Yet we acknowledge they make the life of developers easier. Supporting them with proper performance optimizations will enable more tutorial / course examples to work right away with CockroachDB. So that too will come to CockroachDB, in time. Closing words On this one year anniversary of the decision to support SQL,  JOINs were added to CockroachDB. This first implementation is simple and straightforward; it may be slow and memory-hungry, but it is designed to serve a particular purpose. And we already started working on better algorithms for 1.0. Of course, we could have kept this simple implementation to ourselves, for internal testing only. Or we could have released without any promotion or documentation. But that’s just not our way. We are committed to keeping our progress transparent to the community. This is where we are now, and from here on we can only do better. If building out distributed SQL engines puts a spring in your step, then good news — we're hiring! Check out our open positions here .", "date": "2016-07-20"},
{"website": "CockroachLabs", "title": "Revisiting SQL Typing in CockroachDB", "author": ["Nathan VanBenschoten", "Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/revisiting-sql-typing-in-cockroachdb/", "abstract": "Adopting a SQL interface for CockroachDB had an unexpected consequence; it forced us to dabble in language design. Most developers working with SQL have heard rumors of a SQL standard, pages upon pages of norms and requirements for all SQL compliant dialects to respect. Based on its existence, it’s natural to draw the conclusion that SQL is fully specified and straightforward to implement. A developer need only carefully follow each step laid out in the standard until they arrive at a working database. It’s a lot like building a couch from IKEA. However, in the months following our decision to create a SQL layer on top of our distributed consistent key-value store, we’ve come to realize that this is far from the truth. SQL does not provide implementation hints and design details are fully left to developer discretion. If this isn’t enough, most modern SQL databases only comply with a subset of the specification, and their implementation decisions diverge in all directions. So while beginning to construct the SQL layer of CockroachDB, we also had to determine how we wanted our flavor of SQL to look. Our goal was to balance compatibility with other databases with conciseness of our distributed SQL dialect. Specifically, we needed to figure out which parts of the spec we would implement, how we would approach unspecified behavior, and how we would balance robustness with usability and performance. Of all design decisions that this implies, the one that would be hardest to revisit later and which therefore deserved most attention upfront was how we wanted to deal with typing. We knew we wanted to define a straightforward type system, but the question remained: which one? In this post, we will detail our pre-beta approach to SQL typing in CockroachDB and what prompted a redesign; our evaluation of the Hindley-Milner type system and a vision for its use in SQL; what Rick & Morty (and an older sister named Summer) have to do with the the SQL typing system CockroachDB currently employs; and how a suitable selection of typing rules is essential to query optimization and thus performance. Table of Contents Roles of a SQL type system SQL Typing in CockroachDB, pre-Beta Back to school! Design iterations Types and performance What’s next for Summer Roles of a SQL type system In general, a type system is a set of rules that determines types for all valid combinations of values and expressions within a program. The application of these rules then becomes the process of semantic analysis, verifying the syntactic structure of provided code. This analysis assigns meaning to a program by determining what needs to be done at run-time, while making sure to reject invalid queries for which computation would not make sense. In a SQL dialect, the type system also performs the role of determining how a query should be evaluated based on the types at play, resolving the desired implementation for overloaded functions and operators. INSERT INTO foo (float_col) VALUES (‘3.15’ || ‘hello’);\n    -- this should probably be rejected\nINSERT INTO foo (int_col) VALUES (2 + 3.15);\n    -- this should probably be accepted\n    -- but what value gets computed by ‘+’? An additional role specific to SQL is type inference for placeholders. In SQL query protocols, like Postgres’ wire protocol implemented by CockroachDB, it is customary to separate a “query preparation phase”, where a client declares its intent to run a query to the server, and a “query execution phase”, where a previously declared query is run one or more times. In these prepared queries, a client can declare placeholders to be filled during execution. This provides both safety from security vulnerabilities like SQL injection and improved reusability of queries. For example, a client can prepare the following query, and only provide the two missing parameters during execution: INSERT INTO Customers (Name, Address) VALUES ($1, $2);\n    --sometimes also noted “VALUES (?, ?)” However, the client must be informed which types are to be used to provide values to these parameters, as the types may depend on the database schema. For this reason, the SQL typing system must also infer and report the types of placeholders. SQL Typing in CockroachDB, pre-Beta The initial approach for our typing system adopted a simple and commonly used tactic. During type checking of a given query, we first assigned types to all leaves of a syntax tree. For literal values, these types were based on textual representation. For instance, 1 would be assigned the type INT, while ‘str’ would be assigned the type STRING. For column references or other relationship values, types were assigned to the reference’s type. For instance, referencing a column with type FLOAT would type the syntax node as a FLOAT. Once types were determined for all leaves in our syntax tree, the types were then propagated up through the expression tree, resolving intermediate expression types and eventually resolving the types of the query’s root expressions, all in a single pass. All in all, this “bottom-up” approach was simple to implement and easy to reason about. Let’s take a look at how this system works for simple queries. Given the query: SELECT LENGTH(‘Cockroach’) + 12; We can first construct a syntax tree that looks like: With this syntax tree in hand, our next step is to validate that the expression tree (the white nodes) is semantically sound. This is accomplished by traversing the tree and following our “bottom-up” algorithm described above. 1. Type the ‘Cockroach’ string literal as a STRING. 2. Pick the LENGTH() overload which has a STRING parameter. This overload returns an INT. 3. Type 12 numeric literal as an INT. 4. Pick the INT + INT operator overload. This overload returns an INT. 5. The SELECT expression tree returns a single INT. In our pre-beta implementation, placeholders were handled on a case by case basis, depending on how they were used. For instance, if present as argument to a function with only a single overload, a placeholder would receive the type accepted by that function at that argument position. Placeholders providing values for columns in INSERT statements would receive the type of the columns. As children of cast expressions, placeholders would receive a type of STRING. However, for many types of expressions, placeholders were not handled at all. At the end of this process, we had a working type system that was straightforward and reasonably functional, while still remaining similar enough to other SQL dialects to permit seamless migration in many cases. For the time being, this was sufficient and we were satisfied. The itch that made us reconsider Fast forward a few months, and we began to feel discontent with the rigidity of our initial type system. We identified a few distinct limitations that we wanted to address. The first was a limitation caused by the bottom-up analysis, wherein context of an expression was not taken into account when determining the types of expressions. Take for instance: INSERT INTO float_col VALUES (2); The above would fail during type checking. Let’s take a look at the syntax tree to get an idea of why: 1. Type numeric literal 2 as an INT. 2. The INT type is propagated up as the single value in the VALUES tuple. 3. The INT type cannot be inserted into a FLOAT column, so type checking fails. In the above example, we created a situation where we had an INT value when we wanted to insert a FLOAT instead. Because of this, our old system would be forced to throw a typing error. The problem was that we were failing to take into account any context when assigning types to the “2” numeric literal, and our system lacked any support for ability to perform implicit type conversions to alleviate this limitation. To get this query to work with the old system, the numeric literal 2.0 was necessary so that it will be typed as a FLOAT and all of the types would “line up”. As this demonstrates, the system often required full explicitness when declaring literal values. On top of this, some types like DECIMAL and TIMESTAMP were missing literal representations entirely, so the only way to use them as literal values was to use the verbose SQL syntax for explicitly typed literals (name of type followed by value between quotes) or perform a manual cast on the literal values. This meant that a query which used these types would end up looking something like: INSERT INTO t (dec_col, ts_col) VALUES (DECIMAL ‘1.234’, TIMESTAMP ‘2010-09-28’); Both our users and colleagues found this a little too verbose. The next problem was a limited ability to properly determine the types of placeholders, which often resulted in placeholder ambiguity errors from queries that should not be ambiguous. To illustrate this, check out the query SELECT ATAN2(FLOOR($1), $1); Attempting to infer the placeholder types during type checking followed the process: 1. Recurse into ATAN2, which has only the single overload of ATAN2(FLOAT, FLOAT). 2. Recurse into FLOOR, which is overloaded for FLOAT and DECIMAL. 3. Because there are multiple overloads for FLOOR, we are unable to determine a type for the $1 placeholder, and therefore throw an ambiguity error. In this example, the multiple overloads for FLOOR caused an ambiguity which our typing system could not handle. However, a quick examination of this query reveals that the type of placeholder $1 can uniquely be inferred as FLOAT. The logic behind this is that because ATAN2 only takes FLOAT arguments, both $1 and the return value of FLOOR need to be FLOAT types. Since only one overload for FLOOR returns a FLOAT, we can infer that this is the only overload that can be used in this situation. Finally, because the FLOOR overload which returns a FLOAT type also takes a FLOAT type, we can infer that $1 should be typed as a FLOAT. Unfortunately, our initial type system, with its bottom-up approach, failed to resolve type constraints in a way which would permit such analysis, and as such, we failed to type check the query. Lastly, we began to feel constrained by a somewhat pessimistic typing of numeric literals. Consider for example: INSERT INTO float_col VALUES (1e10000 * 1e-9999); This query would fail during type checking with a complaint that the two numbers do not fit in the FLOAT data type, despite the fact the result after the multiplication would. While it is important that we exhibit expected overflow behavior and data type range limits when performing arithmetic on dynamic data, there was no pressing reason why we needed to limit arithmetic on numeric literals. We wondered if it would be possible to perform exact arithmetic on all numeric literals, and only face these data type limitations when the literal values needed to be used as a specific data type. With these limitations in mind, we began rethinking how we could improve the flexibility of our typing system. Back to school! {#back-to-school} We knew that the early typing of value literals in the leaves of our expression tree was failing to take into account its surrounding context. To remedy this, we turned towards the literature for type inference [a] [b] [c], which is the automatic deduction of data types within an expression. Using the literature on type inference in the context of SQL is no small enterprise. Language researchers working on typing traditionally work with some flavor of lambda calculus, where all language features can be expressed as assemblies of function definitions, function application, let bindings (and/or pattern matching), and higher-order function composition. The literature then explains typing in that context, and the exercise of implementation is left to the reader. This is OK for most programming languages where the relationship with lambda calculus is reasonably straightforward, and largely documented already . In contrast, SQL dialects have largely escaped study of programming language and typing theorists, and it was not immediately clear to us how principles of lambda calculus could be applied to the salient constructs of SQL. Nevertheless, we started by assuming that SQL semantics could be expressed in the same formalism as the one used by the typing literature, and we began exploring what was out there. How CockroachDB flew to Hindley-Milner like a moth to the fire Our original typing algorithm worked in a single “direction,” where the type of each subexpression is fully resolved before moving to its parent. This is too limited, as we explain above, so we wanted to introduce some notion of parent-child collaboration to determine a child expression’s type. Meanwhile, a few of us had experience with the languages ML, Haskell, and Rust, of which one of the hallmarks is their use of the Hindley-Milner type system [c] . Hindley-Milner (HM) has two key features: it can find the most general type for any part of a program based on its context, and it can do so almost in linear time with respect to the size of the program (as opposed to other iterative typing algorithms which can become quadratically slower as programs become bigger). We found the idea of applying HM in CockroachDB highly attractive: not only would it make our SQL flavor much more flexible than many other RDBMs, but it would also set a solid foundation for future efforts towards stored procedures and other forms of server-side evaluation of client-provided functions. In order to use HM in CockroachDB, three steps would be needed: We would need to learn how to express the SQL semantics in the lambda calculus formalism We would implement this mapping, ie. translate our SQL syntax tree into an intermediate representation (IR) with functional semantics We would (re-)implement HM in Go As we analyzed the technical steps needed to get from here to there, we realized that it would be unwise of us to do this so early in the lifecycle of CockroachDB. First, it would require a lot of work. Second, it may not be needed after all. The work argument can sound like a cop-out, and to some extent, it is. Expressing SQL in the formal system of lambda calculus and implementing a functional IR actually are on our wishlist, because they will enable us to implement powerful query optimizations. Implementing HM in Go might be a highly unpleasant experience (Go is arguably inadequate for implementing complex pattern matching rules on abstract syntax trees), but it would attract a lot of attention and love from the programming language community. So investing in these work items may actually be the right thing to do in the long term! However, we also wanted needed to improve the SQL experience of our users in the very short term. While we do have quite a few smart cookies on the team, getting HM to work in CockroachDB in just a few weeks/months in Go would probably require a crew of X-Men instead. Meanwhile, the stronger argument against implementing HM is that we don’t really need it at this point. Indeed, the power of HM and the motivation behind the complexity of its design is its ability to handle arbitrary algebraic types and higher-order functions properly. Algebraic types in SQL are limited to product types in the form of tuples, and the SQL syntax already clearly delineates when a tuple type is needed and when it is not. So we don’t need extra intelligence to infer tuple types. Moreover, there is no such thing as higher-order functions in SQL, at least, not in our dialect so far! Applying HM for typing SQL would be unnecessarily heavyweight, at least until a later time when we provide support for custom client-provided functions (i.e. stored procedures or similar) and client-defined types. Additional peculiarities of typing for SQL Besides these two main arguments, there was a third argument that made us realise that HM was attractive but not appropriate for CockroachDB after all. For our readers who already have experience with typing systems in other programming languages, this argument may add insight about what makes SQL different. This is one of design trade-offs. Hindley-Milner, like many type systems, has been designed with the assumption of “compile-once, run-many.” The cost and complexity of type inference and checking, if any, is assumed to be largely offset by the time the program will run, especially when a program is compiled once and ran many times. The client-server architecture of SQL databases and the basic concept of a “SQL query as interface,” in contrast, precludes offsetting the cost of type checking in this way. Even though SQL client/server protocols support a “query preparation” phase run only once for multiple subsequent “query executions,” this is not sufficient. The main obstacle is that the types in a SQL query depend on the database schema, and the schema can change at arbitrary points in a sequence of query executions. Therefore, a SQL execution engine must re-compute and check types each time a query is reused in a new transaction. You could suggest here that a cache could help amortize the cost of query analysis, however from our perspective the situation is not so clear: from experience, in SQL it is more likely than other languages that queries are unique and run only one time. In practice, the conservative (and often used) implementation route is to pay the overhead of query analysis for each session anew. This implies a specific extra-functional requirement for SQL type systems not commonly found in other PLs, that their typing algorithm should be relatively lightweight! Next to performance, another cost trade-off makes SQL distinctly unique as programming languages go: SQL is largely read and written by machines! HM, like many modern results in programming language theory, has been designed with a human programmer in mind. The flexibility gain in a language equipped with HM is measured by increased productivity of the human programmer: how much less time is needed to write new code and then read and understand existing code, because a powerful type inference algorithms makes programs concise. Furthermore, the ability of HM to find the most general type ensures that a written piece of code is maximally reusable across applications. In contrast, SQL queries are often assembled programmatically. The trend nowadays is to make SQL largely invisible from the programmers, hidden behind database-agnostic data abstraction layers in application frameworks. These client-side libraries are designed to construct valid, unambiguous SQL queries from the get-go, again and again for each application and use case. Assuming the trend continues, we could even expect these frameworks to become sufficiently intelligent to decide typing client-side and then communicate a fully disambiguated, type-annotated SQL query to the database server. (Granted, we are not there yet, but the argument is not a stretch.) Along the way, we expect to see the amount of work needed by SQL execution engines to analyze queries decrease over time. To summarize, any effort put into typing for a SQL dialect is intended to aid in raw / command-line use by a DB administrator or for experimentation by the programmers of said client frameworks. The queries written in these contexts are an infinitesimally small proportion of all the queries that reach a SQL server! In other words, SQL type systems should be just complex (smart) enough to avoid surprising and disgruntling DB admins and framework developers; they do not need to minimize the amount of syntax needed to express all queries that applications may need to use nor maximize the reusability of a query across multiple applications / use cases. Acknowledging the essentials: starting points for the new design Considerations about Hindley-Milner notwithstanding, we settled pretty early on the following two requirements for our new typing system. The first was that the typing algorithm should propagate types from bottom (syntax leaves) to top, as we felt this was a convention least surprising to the majority of our audience. While this requirement would have minimal effect on well-constructed queries, it would vastly improve the usefulness of errors we would be able to provide on poorly-constructed queries. Following this, we wanted our dialect to continue avoiding implicit type coercions, as our prior programming experience suggests that nothing good comes out of a language that allows them. Within this framework, the design exercise is restricted in scope to the following specific issues: If a function is overloaded, and no overload prototype matches exactly the argument types, what to do? There are two aspects to consider: We can’t populate our library with all combinations of overloads that may perhaps make sense (e.g. float + int , decimal + int , int + float , int + decimal , etc.). If we did so, we would break placeholders: an expression like “ 1 + $1 ” would become ambiguous for the type of “ $1 ”. But then if we only support e.g. int + int and float + float , what to do if the user enters “ 4.2 + 69 ” if we also dislike implicit coercions? Some SQL functions require “homogeneous types.” For example COALESCE(x, y, z) is generic on the type of its arguments but require them all to be of the same type. Meanwhile, we felt that COALESCE(1, 1.2) or COALESCE(1, float_col) is well-defined enough that we ought to accept it somehow. The question was how to do this? (Again, without implicit coercions.) Given our self-selected requirements, we didn’t have a ton of freedom for how to solve these issues. Whenever a sub-expression has a fixed type, for example because it is the name of a table column or the result of a function with a fixed return type, our solution would need to accept this type as a given and either reject or accept the surrounding context based on that type. For example, we knew early on that an expression of the form “ float_col + int_col ” would be rejected unless we had this specific overload in our library, which we don’t (for now), and that “ COALESCE(float_col, int_col) ” would be rejected on the same grounds. However, we are convinced that this limitation is actually a good thing, as it reduces the amount of “magic” in the typing rules and thus the cognitive effort needed for the SQL programmer to understand what is going on. But then, what remains? Placeholders Obviously, placeholders are special and need to “learn from their context”. In a pure bottom-to-top algorithm, an expression of the form COALESCE(int_col, $1) would always be rejected because the recursion would consider $1 in isolation and simply not know what to do with it. So the typing rules must necessarily make a dent on the bottom-to-top principle for placeholders. Constant Literals In addition to this, we invested extra effort into constant literals, which are the textual representations of constants in the SQL query source text. In many “simple” languages like C, C++, Java, ML, and others, constant literals are assigned a type independent of context, based only on their lexical form. For example, “ 123 ” receives (or is interpreted using) type INT because it contains only digits and no decimal separator; “ 123.23 ” receives type FLOAT because it has a decimal point, and “ ‘abc’ “ receives type STRING because it is enclosed by quote characters. (Technically speaking these languages’ literals are monomorphic .) But this is not the only way to do this. In more elaborate languages with type classes like Rust or Haskell, constant literals do not have intrinsic types and instead only receive a type when they are assigned to (or used as argument to) a context that demands a particular type. The way this works is that the language places these literals in a “class” of many possible types that the literal value could be interpreted as, and the literal stays there until context restricts the class further. (These languages handle literals as if they were polymorphic even though they eventually become monomorphic after constant folding.) The benefit of this approach is that a literal that looks like one type can be freely used in a context that requires another type without the need for an implicit coercion. Some members in our team knew this from previous experience with other languages. Incidentally, Go has a very similar concept which it refers to as untyped constants , so our team of Go programmers was comfortable with this idea. So we decided to reuse this principle in CockroachSQL. Yet that wasn’t enough, because we also wished our typing rules to accept constructs like INSERT INTO foo (float_col) VALUES (2 + 3) . With implicit type selection activated only for literals, the typing rule for “+” would see two arguments that have many possible types and bail out with an ambiguity error. Even if we had thought of implementing some sort of “ranking” of overloads based on argument types (a terrible idea, but bear with us for the sake of this example), this would not be sufficient since the best ranking would likely select the variant returning INT and then the INSERT would reject inserting an integer into a FLOAT column. Instead, we generalized the typeclass-based handling of constant literals to any composite expression involving only constant literals as leaves and operators for which we can compute a result that would be valid in all the possible types chosen by the context. For this, we observed that we can perform nearly all number arithmetic in a suitably chosen representation that always computes numerically exact results, like the type Value provided by Go’s Constant package (used by the Go compiler for Go constant literals); doing what we wanted was simply a matter of integrating constant folding in an early phase of our algorithm. To summarize, we knew early on that the key features of our typing algorithms would be: Constant folding in an exact type to delay typing constant literals as much as possible Inferring placeholder types based on their context Resolving overloads based on argument types Design iterations Early attempts: Rick & Morty Our first design for a “new type system” was relatively complicated. Its design is fully described here ; the salient feature is that it would go through multiple traversals of an expression tree containing placeholders until it could find a satisfying types for all placeholders, or detect that it is unable to make progress and then abort with an ambiguity error. This complexity appeared because we thought we wanted to solve the following use case: Suppose three functions f, g and h each have 2 overloads, as follows: f:int,float->int\nf:string,string->int\ng:float,decimal->int\ng:string,string->int\nh:decimal,float->int\nh:string,string->int Given this, the problem was to infer types for the the placeholders in the expression “ f($1,$2) + g($2,$3) + h($3,$1) ”. With a naive algorithm, there is not enough information locally to each of the three individual function calls to infer a specific type for each placeholder. With the new design – an iterative algorithm – we could infer STRING for $1, $2 and $3 correctly. However, of course, an iterative algorithm wasn’t satisfactory; we felt typing ought to be linear with respect to the size of the syntax. Also, truth be told, the driving examples were relatively far-fetched and rather unrealistic. Then we thought about another design which wasn’t iterative and which would instead use heuristics to make things work in most cases even though it would fail in more complex (unrealistic) situations, like the one just above. This second design is detailed here . Since this was rather different from our previous idea, we wanted to differentiate them during technical discussions, and thus we chose the names “ Rick & Morty ” for first and second designs respectively. We described both in our typing RFC and stated our preference for Morty. The RFC was discussed with the team, accepted, and then awaited implementation. When Nathan began implementing the new CockroachSQL type system, something unexpected happened. He thought he was implementing Morty , but when he explained his new system we realized he actually had implemented something else entirely. In strict engineering terms, this could have been interpreted as an error, as his implementation diverged from the spec; however we were quick to realize his implementation was much better than Morty (and Rick) [1] : it could type most SQL queries we were interested about with a simpler set of rules than Morty (and thus Rick). And so did we rediscover that Worse is Better . [1] There is a lesson here about the relative order of specification RFCs and their implementation. We learned from experience. Summer: Bi-directional typing with untyped constant literals CockroachSQL’s new type system is called “Summer,” after Morty’s sister in the eponymous show. Summer fits the requirements set forth earlier, by integrating early constant folding; typeclass-based inference of constant literals; bottom-to-top typing; and extra logic for overload resolution and homogeneous typing. Summer’s salient feature is how it solves ambiguity. In a twist to our requirement for bottom-to-top typing, Summer’s typing algorithm propagates a type “preference” down the recursion (as input to the recursive step) decided by an expression’s parent node. This enables Summer to infer the desired types of constant values in SQL queries based on how that constant gets used. We were surprised and happy to find that this simple mechanism resolves ambiguity in most interesting cases, without the complex rules or heuristics needed by Rick and Morty to achieve the same. Furthermore, we confirmed that this approach obviates the need to introduce implicit coercions during type checking. Let check out how these new rules play together when typing the query: INSERT INTO float_col VALUES (SQRT(ABS(1-4.5))); 1. Before type checking begins, we fold all constant operations using exact arithmetic. 2. We then propagate the desired FLOAT type down the expression tree during the type checking recursion. 3. -3.5 is a constant so we check that FLOAT is in its resolvable type set. Because it is, we resolve the constant as a FLOAT datum type. 4. As the recursion goes back up, we have a pair of straightforward overloads that are resolved to their FLOAT implementations. 5. The entire expression is resolved to a FLOAT, which matches the insert column type. To give more insight into the intermediate and final types during type checking, we also added a new EXPLAIN mode called EXPLAIN(TYPES) . In this mode, EXPLAIN shows the types of all sub-expressions involved in a statement, as well as the derived types of result columns in each intermediate step of the query plan. If we run this explain mode on the previous query, we get: > EXPLAIN (TYPES) INSERT INTO float_col VALUES (SQRT(ABS(1-4.5)));\n\n+-------+--------+---------------+-----------------------------------------------------------------+\n| Level |  Type  |    Element    |                           Description                           |\n+-------+--------+---------------+-----------------------------------------------------------------+\n|     0 | insert | result        | ()                                                              |\n|     1 | values | result        | (column1 float)                                                 |\n|     1 | values | row 0, expr 0 | (SQRT((ABS((-3.5000000000000000)[float]))[float]))[float]       |\n+-------+--------+---------------+-----------------------------------------------------------------+ For most expression types, Summer is a seamless extension of CockroachDB’s original type system. For instance, the AND expression now passes down a desired type of BOOLEAN during the downward pass into both its left and right subexpressions. Then, during the upward pass, the AND expression asserts that each subexpression did in fact type itself as a BOOLEAN , throwing a typing error if not, and typing itself as a BOOLEAN if so. Beyond the mechanism of “preferred” types, Summer provides straightforward rules for required homogeneity and overload resolution. Type inference for functions requiring homogeneous arguments Summer defines rules for how to determine a mutually-shared type, given an arbitrary set of expressions. This primitive comes into play for a number of classes of expressions: COALESCE : all parameters need to be the same type CASE : all conditions need to be the same type and all values need to be the same type NULLIF : both subexpressions need to be the same type RANGE : all three subexpressions need to be the same type A few other expression types… To support type inference in all of these cases, we built a set of rules that can be applied to determine the homogeneous shared type for a set of expressions. To see these rules in play, let’s inspect how we would handle the following SQL query: SELECT CASE 1 WHEN 1.5 THEN $1 WHEN dec_col THEN 1 END FROM t; 1. First look at the input and WHEN expressions, which must all be a homogenous type. This set of expressions includes two constant literals (‘1’ and ‘1.5’) and the column reference ‘dec_col’, which is referencing a column with the type decimal. 2. Because ‘ dec_col ’ can only be used as a decimal, we verify that DECIMAL is in the resolvable type set of the two constants. Because it is, we are able to infer that all three values have a type of DECIMAL. 3. Next, we look at the two THEN expressions, which also much all be a homogeneous type. The placeholder ‘$1’ has all types in its resolvable type set, so it exerts no preference for the shared type. If the placeholder was alone here, we would have to throw an ambiguity error. 4. However, the placeholder must have the same type as the literal value ‘9’. This value has a natural type of INT, so INT is assigned for both of their types, and is propagated up as the return type of the entire CASE expression. Again, we can see this rule in action if we use Explain(TYPES): > EXPLAIN (TYPES) SELECT CASE 1 WHEN 1.5 THEN $1 WHEN dec_col THEN 1 END FROM t;\n\n+-------+---------------+----------+-------------------------------------------------------------------------------------------------------+\n| Level |     Type      | Element  |                                             Description                                               |\n+-------+---------------+----------+-------------------------------------------------------------------------------------------------------+\n|     0 | select        | result   | (\"CASE 1 WHEN 1.5 THEN $1 WHEN dec_col THEN 1 END\" int)                                               |\n|     1 | render/filter | result   | (\"CASE 1 WHEN 1.5 THEN $1 WHEN dec_col THEN 1 END\" int)                                               |\n|     1 | render/filter | render 0 | (CASE (1)[decimal] WHEN (1.5)[decimal] THEN ($1)[int] WHEN (dec_col)[decimal] THEN (1)[int] END)[int] |\n|     2 | scan          | result   | (dec_col decimal)                                                                                     |\n+-------+---------------+----------+-------------------------------------------------------------------------------------------------------+ Type Inference of function calls and Overload Resolution Summer also provides rules for determining which function to choose when given a set of candidate overloads. This is critical, because many functions in SQL are overloaded to support multiple types of inputs. On top of this, unary operations, binary operations, and comparison operations all require some form of overload resolution to decide which runtime implementation of them to use. Flexibility While Summer does not support implicit type casts, it does provide an improved level of flexibility through type inference without compromising on clarity. A recent change we made exemplifies this point. After the completion of Summer, we received a request to allow implicit casts between TEXT and TIMESTAMP values. The use case driving this proposal was a simple query: INSERT INTO ts_col VALUES ('2016-5-17 18:22:4.303000 +2:0:0'); Implicit casts were a specific anti-goal of our type system, but we also realize that our type system could fully support this class of query. Instead, all we needed to do was to extend the type class of constant literal strings to include the TIMESTAMP type. In fact, following this train of thought, we extended their type class to include DATE and INTERVAL as well. With this small extension, string literal values which can become one of these types during type resolution are now allowed to be interpreted as their context demands. Types and performance Intelligent typing is not only a matter of improving the experience of programmers writing SQL queries. It’s also about providing the query engine with the right information to choose a good query plan. Consider for example a table with a column initially defined as floating point: > CREATE TABLE Item (length REAL, desc TEXT);\n> CREATE INDEX lIdx ON Item(length); Using this schema, let’s investigate the query plan for a query like the following: > EXPLAIN SELECT desc FROM Item WHERE length = 3.5;\n\n+-------+------+------------------------------------+\n| Level | Type |            Description             |\n+-------+------+------------------------------------+\n|     0 | scan | Item@lIdx /3.5-/3.5000000000000004 |\n+-------+------+------------------------------------+ We can confirm that the engine properly selects lIdx for the lookup. (EXPLAIN’s output is explained in a previous blog post .) Now suppose that as this application lives on, it appears that most items in this database actually have lengths that are round (integer) numbers. It may then look interesting to change the DB schema to use the type INT for the “length” column without having to change the client, so as to conserve time (INT comparisons are cheaper). After all, if the client only provides round number for lookups, the query engine should “know” that the constant round value is really an INT and use the index, right? We can investigate the query plan for the modified schema and query: > CREATE TABLE Item (length INTEGER, desc TEXT);\n> CREATE INDEX lIdx ON Item(length);\n> EXPLAIN SELECT desc FROM Item WHERE length = 3.0;\n\n+-------+------+-------------+\n| Level | Type | Description |\n+-------+------+-------------+\n|     0 | scan | Item@lIdx - |\n+-------+------+-------------+ What gives? CockroachDB, prior to Summer, did not (and could not ) consider heterogeneous comparisons as a candidate for index selection. So when the value “3.0” would get typed as REAL, the comparison in the WHERE clause was heterogeneous and the query planner would choose (unfortunately) a full index scan with post-scan filtering. (And incidentally, so do several competing DBMS, too; some even don’t use an index at all.) This problem, in general, offsets the benefits of schema changes to exploit application-level knowledge about actual values until clients are modified to use the updated column types too. With the new Summer type system, the constant 3.0 is interpreted using the type demanded by its context, and is simplified to an INT before index selection because representing 3.0 as an INT is possible without truncation or rounding. The query is thus optimized again, preserving the lookup time complexity of the table prior to the schema change. The benefits of the schema change are thus fully exploited without changes to client code and queries. To summarize, we’d like to offer the thought that CockroachDB makes it easier to write complex WHERE clauses on columns of various types using arbitrary literal formats, because the query optimizer won’t need to consider under which circumstances the database knows how to convert constant literals. What’s next for Summer Summer, CockroachDB’s new type system, is now implemented and available for use. You can even explore how it behaves with your SQL code using the EXPLAIN(TYPES) statement. We are continuing to improve the SQL implementation in CockroachDB, and a big part of that work will include refining our type system. So far we have already confirmed that our typing rules behave adequately for a variety of SQL benchmarks and reference test suites, including SQLLite’s Sqllogictest . We do have a few minor deviations (such as our number division returning DECIMAL even for integer operands) but they are deliberate choices for CockroachDB rather than unavoidable limitations of our type system. In the coming months, we will invest in ensuring CockroachDB can be used from popular ORMs, and we intend to adapt our typing rules as far as necessary to make these tools happy. While holding on to our goal of straightforward rules and no surprises , we will continue to iterate on making our type system even more usable and expressive. If distributed SQL makes you giddy, check out our open positions here . References [a] Hal Abelson’s, Jerry Sussman’s and Julie Sussman’s Structure and Interpretation of Computer Programs (MIT Press, 1984; ISBN 0-262-01077-1) [b] Benjamin C. Pierce’s Types and Programming Languages , (MIT Press, 2002; ISBN 0-262-16209-1) [c] Amir Gumar Gupta’s So you still don’t understand Hindley-Milner , 2013. Part 1 & Part 2 & Part 3", "date": "2016-06-09"},
{"website": "CockroachLabs", "title": "Critters in a Jar: Running CockroachDB in a FreeBSD Jail", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/critters-in-a-jar-running-cockroachdb-in-a-freebsd-jail/", "abstract": "Note: this blog post was updated on September 21, 2017. Jails are FreeBSD’s native solution to contain and isolate server processes. They are an alternative to (and predate) Linux cgroups , Solaris zones , and other OS-level process isolation technologies (the technologies that underlie Docker , CoreOS and a few others) . This blog post will explain how to natively run CockroachDB in a FreeBSD jail. This is lighter weight and as secure as running Docker on FreeBSD . Manually encapsulating CockroachDB using Linux cgroups is no easy task, which is why tools like Docker exist in the first place. By comparison, running server processes natively in FreeBSD jails is straightforward and robust. Call for contributions : We welcome patches to increase the portability of CockroachDB, as well as external support to create and maintain installable CockroachDB packages for various Unix flavors and distributions, including a BSD source port . Disclaimer : We do not yet claim to support FreeBSD compatibility. The solution below has been tested on FreeBSD 10.3-STABLE with today’s version of CockroachDB. Tomorrow, YMMV. Prerequisites to build CockroachDB Pending an official package for CockroachDB on FreeBSD, you will need to build from source. For now, CockroachDB requires a 64-bit machine. Your system should provide the C and C++ development tools. FreeBSD’s native C and C++ compilers are appropriate. You also need the additional packages for Bash, Git, GNU make, and Go. These can be installed with: pkg install bash go gmake git Ensure that go version 1.8 (for CockroachDB 1.0 or 1.1) or 1.9 (for CockroachDB 1.2) or higher is installed as a result: go version . Creating a CockroachDB binary The official instructions (select Linux and then Build from Source ) apply with some tweaks: Ensure that /tmp is mounted as tmpfs or from a ramdisk prior to the build. We’ve seen that builds can be very slow otherwise. You’ll need to run the build with gmake instead of make . Testing your build Ensure that $GOPATH/bin is in your PATH and the cockroach binary is there. Ensure that the Linux “/proc emulation” is mounted in /compat/linux/proc . If not done yet, do this with: sudo mkdir -p /compat/linux/proc\nsudo mount -t linprocfs /dev/null /compat/linux/proc (Note: Linux compatibility files / packages / libraries are not needed further. CockroachDB uses Linux’s procfs to inspect system properties via gosigar **. If/when gosigar evolves to read FreeBSD properties natively, CockroachDB will not need linprocfs any more.)** Then as per the official CockroachDB docs: cockroach start --insecure --host=localhost Then in another terminal: cockroach sql --insecure There, play around with SQL, for example: CREATE DATABASE test;\nCREATE TABLE test.t (x INT);\nINSERT INTO test.t(x) VALUES (42);\nSELECT * FROM test.t; If the program works as expected, the last query should return the inserted row from the table. Starting/stopping CockroachDB as a system daemon FreeBSD uses scripts in /etc/rc.d and /usr/local/etc/rc.d for starting and stopping system daemons. Which daemons are actually enabled to run during startup is specified using definitions in the common file /etc/rc.conf . To integrate CockroachDB into a standard FreeBSD startup, we need to create a suitable startup script in /usr/local/etc/rc.d or /etc/rc.d . One such script is the following: #!/bin/sh\n#\n# PROVIDE: cockroach\n# REQUIRE: LOGIN\n# KEYWORD: shutdown\n\n# Add the following lines to /etc/rc.conf to enable cockroachdb:\n# cockroach_enable=\"YES\"\n# cockroach_flags=\"<set as needed>\"\n\n. /etc/rc.subr\n\nname=\"cockroach\"\nrcvar=cockroach_enable\npidfile=/var/run/cockroach.pid\n\nload_rc_config $name\n\n: ${cockroach_enable=\"NO\"}\n: ${cockroach_user=\"cockroach\"}\n: ${cockroach_flags=\"--insecure --host=localhost --store=path=/home/cockroach\"}\n\nstart_cmd=${name}_start\nstop_cmd=${name}_stop\n\ncockroach_start() {\n  daemon -u $cockroach_user -p $pidfile /usr/local/bin/cockroach start $cockroach_flags\n}\n\ncockroach_stop() {\n [ -f $pidfile ] \\\n   && /usr/local/bin/cockroach quit --insecure \\\n   || echo ${name} not running? \\(check ${pidfile}\\)\n}\n\nrun_rc_command \"$1\" This script runs a CockroachDB node using account cockroach . The default settings for the configurable variable cockroach_flags instructs it to store database files ( --store ) in /home/cockroach and only listen to a TCP port on the localhost interface. The latter is only necessary until you follow the instructions to secure your cluster and remove --insecure . The flags specified via cockroach_flags in /etc/rc.conf are passed as-is to the database start command, cockroach start , so we can also use this later to set certificates, addresses and the like. It uses daemon(8) because the CockroachDB server does not daemonize itself. If you want to use this script, be sure to name it cockroach (without \" .sh \"), ensure it is executable ( chmod +x ) and add the appropriate enabling flags to /etc/rc.conf (see comments at start of script). Running CockroachDB in a FreeBSD Jail Jails are FreeBSD’s solution for process encapsulation, more commonly called “containers.” Jails are both older than most similar technologies and comparatively more feature-complete. In a nutshell, running CockroachDB in a FreeBSD jail amounts to the following steps: Create a standard FreeBSD jail as per the FreeBSD handbook. Ensure that the virtual filesystem linprocfs is mounted in the jail’s /compat/linux/proc Create a user account cockroach for CockroachDB in the jail. Copy the above cockroach binary in the jail’s /usr/local/bin Copy the above startup script in the jail’s as /usr/local/etc/rc.d/cockroach Add cockroach_enable=YES to the jail’s /etc/rc.conf, and tune cockroach_flags and cockroach_db_directory as desired. In particular you probably want to change --host=localhost to --host=127.0.1.1 to match the Jail internal address. Note that by default CockroachDB runs in “insecure” mode, i.e. with SSL to clients and between nodes disabled. To change this, you need to set up certificates and configure cockroach_flags as per the CockroachDB documentation . Start the jail. While these instructions should be sufficient to seasoned jails users, here is a step-by-step tutorial for jails beginners. This uses ezjail-admin from the ezjail package. Create an alias to the network loopback interface: Add cloned_interfaces=\"lo1\" to /etc/rc.conf Activate the interface with: service netif cloneup If not installed yet, install ezjail: pkg install ezjail If not created yet, create the jail template; note that we do not need FreeBSD ports to run CockroachDB so we do not need to pass the parameter -p : ezjail-admin install Create a new jail, using the aliased loopback interface to attach the jail: ezjail-admin create crjail “lo1:127.0.1.1” This creates the new jail in /usr/jails/crjail . From now on we use “$J” in the examples to refer to this directory. Start the jail manually for initial configuration: ezjail-admin onestart crjail Edit $J/etc/hosts and change the localhost address to 127.0.1.1. Edit $J/etc/resolv.conf to add a DNS resolver, for example Google’s: echo nameserver 8.8.8.8 >$J/etc/resolv.conf Mount the linprocfs filesystem onto the jail: mkdir -p $J/compat/linux/proc\nmount -t linprocfs linproc $J/compat/linux/proc Create a new cockroach user in the jail: chroot $J adduser You can choose a random password for the account. It will not be used by CockroachDB. We assume below that you keep the default home directory setting /home/cockroach here. Copy the cockroach binary generated above to $J/usr/local/bin . You may need to create this directory yourself. (use mkdir -p) Copy the cockroach startup script above to $J/usr/local/etc/rc.d . Again, you may need to create this directory yourself. Add cockroach_enable=YES to $J/etc/rc.conf : echo cockroach_enable=YES >>$J/etc/rc.conf Restart your jail to start CockroachDB: ezjail-admin onestop crjail\nezjail-admin onestart crjail Ensure that CockroachDB has started and is reachable via 127.0.1.1: curl http://127.0.1.1:8080 If CockroachDB does not appear to be running, you can investigate whether it has started by logging into the jail console with ezjail-admin console crjail and looking into /home/cockroach from there. If this works, congratulations! You successfully jailed the critter. Where to go from here? The steps above isolate the CockroachDB server process from other processes in the same system. The server becomes reachable only from the local address 127.0.1.1. From there you can move forward in various directions: Automate starting up your jail(s) when the host system comes up (e.g. add ezjail_enable=YES to the host system’s /etc/rc.conf ). Adjust cockroach_flags in the jail’s /etc/rc.conf to spread the data store over multiple disks and/or ramdisks using CockroachDB’s --store flag . Set up resource limits on the jail, for example memory or disk usage, see jail(8) . Add an alias to the LAN network interface to the jail, so that the CockroachDB server can connect to other nodes in the cluster. (be sure to configure security certificates !) Set up Mandatory Access Control (MAC) to further restrict CockroachDB’s security attack surface. Monitor the database server using Nagios (or your own favorite monitor tool) to set up administrator notifications and automatic restarts whenever the database process fails. Really from this point CockroachDB becomes one among your other contained server processes. All standard FreeBSD best practices and literature apply. References The CockroachDB documentation The FreeBSD handbook", "date": "2016-07-07"},
{"website": "CockroachLabs", "title": "Time-Travel Queries: SELECT witty_subtitle FROM THE FUTURE", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/time-travel-queries-select-witty_subtitle-the_future/", "abstract": "Update as of July 10, 2019 Since publishing this post back in 2016, we have added two new features that make time travel queries far easier to use. You can now specify relative times. For example, select * from t as of system time '-10s' will select from 10s ago. When this post was written in 2016, a user had to specify a full timestamp. You can now specify the AOST time in a BEGIN . In our most recent beta, we added a new feature: time-travel queries. These are `SELECT` queries where you can specify a timestamp, and the data returned will be the data as it was at that time. This has various uses including backups, undo, and historical reporting. The SQL:2011 standard describes this feature, and a few SQL databases (Oracle, MSSQL) have implemented it, in addition to various non-SQL DBs (Datomic). I’d like to introduce this feature: what it is, why we built it, and details about how it works for those interested in CockroachDB’s lower layers. The Feature: Time-Travel Queries A time-travel query returns data as it appeared at a specific time. It is enabled by adding `AS OF SYSTEM TIME` after the table name in a FROM . SELECT * FROM t AS OF SYSTEM TIME '2016-06-15 12:45:00' This returns all the rows of `t` as they appeared at the specified time. The CockroachDB implementation of this feature requires that the timestamp be a literal string in a valid time format. We do not support generic expressions or even placeholder values here (sadly 1 ). However, we are able to support schema changes between the present and the “as of” time. For example, if a column is deleted in the present but exists at some time in the past, a time-travel query requesting data from before the column was deleted will successfully return it. This data is kept in the MVCC layer and garbage collected (GC) at some configurable rate (default is 24 hours and configurable by table). Time travel is supported at any time within the GC threshold. A final restriction is that these statements are not supported in SQL transactions or subqueries. They may be in the future if there’s enough need, and if we can figure out how to correctly implement them. Purpose Time-travel queries were made as part of work being done on backups. CockroachDB provides lockless transactions , which allows operations to proceed without locking resources at the possibility of being aborted and forced to retry. Currently, a naive backup script performing a `SELECT *` on a large, write-heavy table could reasonably be restarted many times. Instead, our upcoming backup tool pages over the data in a table with separate queries but all at the same time using our above feature. These queries can still be aborted, but they can then retry without re-reading the entire table, and resume backup from where they left off. As an added benefit, these also allow for backups of multiple tables to be done in parallel. Improving the speed of backup (and restore) is actively being worked on. Implementation Details Broadly, this feature works by setting the timestamp of a request to the specified historical timestamp. All operations in CockroachDB have a timestamp, which is usually populated to the current time or start of the current transaction. With time-travel queries, this timestamp is modified, and the existing machinery operates as if it’s gotten a normal command. Implementing it required three main changes: First , the GC was changed so that instead of removing any values older than its threshold, it kept any data needed to respond to queries within its threshold. Instead of removing data older than 24 hours, anything valid within the last 24 hours was kept. Second , commands outside the GC threshold for a given range (shard) return an error. Previously, if a read was sent with a timestamp before the oldest value that the GC kept, an empty (but successful) result was returned. I’m not aware of a situation where this could ever happen, but now it certainly can’t. Third , the AS OF SYSTEM TIME syntax was hooked up to some new logic. This logic sets the timestamp of the transaction to the specified time and executes it (note that a single statement outside a SQL transaction gets converted into an implicit, single-statement transaction). The table schema fetch process uses this timestamp and gets the correct version of the schema. Type checking is performed. If everything is ok, the reads are executed. We didn’t have to change any of the underlying logic in any non-SQL layer. Conclusion Time-travel queries are awesome because they enable new features to be built and old data is cleaned up automatically by the system. They can be used to provide a consistent view of an entire cluster without using a SQL transaction, and thus have a much lower risk of being aborted by other queries. We built them for backups, but we think they will enable broader applications to be built on top of them. 1 Placeholders are not supported because an expression needs to be type checked so the server can return to the client the expected types of the placeholders. Since we only know the timestamp, and thus the schema of the table, at statement execution time, we cannot always type check a statement. Consider SELECT a+$1 FROM t AS OF SYSTEM TIME $2 . Here, $1 could be a variety of types depending on the type of a . But until we know the “as of” time, we don’t know which schema t had and thus can’t determine the type of a . We could theoretically support placeholders here if there’s exactly one placeholder and it is the “as of” time by delaying type checking until execution time, but we don’t yet. Are time travel[^*] and distributed SQL your jam? Then good news — we're hiring! Check out our open positions here . [^*] Actual time travel not guaranteed. YMMV. (Also posted at Matt Jibson's blog .) [ Image Credit: Joe Roberts ]", "date": "2016-06-22"},
{"website": "CockroachLabs", "title": "Squashing a Schrödinbug With Strong Typing", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/squashing-a-schroedinbug-with-strong-typing/", "abstract": "Until recently, CockroachDB’s SQL was suffering from a serious, long-standing bug – a schrödinbug, in fact – in its handling of table and column references. This blog post outlines how fuzz testing uncovered the error, how we discovered that our way of using Go was partly to blame, and how we addressed the issue using a form of strong typing. tl;dr: Fuzz testing is good. Don’t use the same type for different things. It crashes rockets and CockroachDB, too. Tale of a Schrödinbug Schrödinbug _: a bug that manifests only after someone reading source code or using the program in an unusual way notices that it never should have worked in the first place, at which point the program promptly stops working for everybody until fixed._ Like in many programming languages, SQL execution is separated in phases: first the text of the query is lexed and parsed ; then the parsed syntax tree is analyzed for semantic correctness ; then the checked tree is transformed into a query plan for execution. Until a few weeks ago, the team at CockroachDB was only looking at “deep” issues in the SQL code related to how queries were executed, optimized etc.: We assumed that parsing and semantic analysis were done deals. That is, until Matt decided it was a good idea to throw a random SQL syntax generator at our engine to check the coverage of our existing tests. This is part of our ongoing effort to test SQL coverage as a part of readying CockroachDB for the 1.0 release. The way his generator works is that the generator looks at the same SQL grammar as used by CockroachDB’s parser , and generates random strings that are also accepted by the grammar. This is actually a form of fuzz testing , a strategy that has recently become popular as an extremely powerful tool for detecting long-standing but obscure bugs in code that handles external input. The tests were meant to exercise the error checking code in CockroachDB. Many resulting strings were purposefully semantically erroneous (e.g. SELECT 2 / 0 , SELECT non_existent_column FROM non_existent_table ). The expected outcome was that each query would: Be accepted by the parser. Either run successfully, or report an error to the client from a semantic check in CockroachDB’s SQL code. Instead of the expected client errors, this generator promptly detected cases where the entire database server running the query would fail dramatically, with a non-recoverable panic. There were even multiple errors in different places, so we knew we were facing either multiple bugs or a more fundamental issue with a larger scope. The queries causing these errors were conspicuously out of this world, for example: SELECT TREAT . * [ PLACEHOLDER IS UNKNOWN ] FROM d.t;\nSELECT t.* IS NOT TRUE\nSELECT TRIM ( TRAILING LOW . * [ ROW ( ) ] ); And thus the first task was to narrow down the root cause(s). After a short investigation, we discovered this was a fundamental issue, and as soon as we had understood this it was easy to imagine many other rather straightforward queries that would fail the entire server too: SELECT COUNT(nonexistent_column.*) FROM table;\nSELECT some_column.*[10] FROM table;\nINSERT INTO table(col.*) VALUES (1) We even found queries that should cause an error didn’t and would run with unpredictable results: INSERT INTO table VALUES(1) RETURNING x[1]\nINSERT INTO table VALUES(1) ON CONFLICT DO UPDATE SET x.* = 10 In other words, as soon as we knew the bug was there, we could see it all over the place. We made a mistake and perhaps Go did, too The cause of the problem was a combination of a design flaw in a critical part of CockroachDB and an inconvenience caused by Go. A flaw in CockroachDB hidden in plain sight There are at least five very different things that can be named in SQL: a table, a function, a column group, a single column, and, if the database supports composite types, a part of a larger object. However, “for simplicity,” it was decided early on that CockroachDB would use a single Go data type for all these things , the `QualifiedName`. This type was a bit complicated. It was a struct with two states, “just parsed” and “normalized.” The data stored in the struct was to be interpreted very differently depending on the state. The parser would populate the struct with raw name parts found using the grammar, and depending where the `QualifiedName` was placed in the syntax tree, it was to be normalized in different ways by the semantic checker handling the syntax tree. Here the first flaw can be found. Although there are five categories of objects that can be named in SQL, CockroachDB only internally used only two normalization algorithms: table name or column name. Column groups (those written with `tablename.*`) were not handled directly during normalization but rather at each point where a column group was to be accepted or rejected. Of course (hindsight = 20/20). We had made mistakes and there were several places where the check for column groups was not implemented correctly. Moreover, we didn’t check that function names had the proper structure. We had simply forgotten, and we didn’t catch this in tests earlier because, after all, who would in their right mind write function calls like “ a.*[3](d) ” ? The better design would have been to use more normalization states, one for each kind of name. Go encourages bad practices, too When we implemented the `QualifiedName` struct, we used the same fields to carry the data in both states (pre- and post-normalization). This is because Go supports neither unions like in C nor sum types , which is by itself a major shortcoming but not directly a problem here. The crux instead is that the correctness of the code using `QualifiedName` was very much dependent on using its fields properly, at each point of use. This made the code using `QualifiedName` responsible for not only checking the normalization state but also testing explicitly that the data in the fields was appropriate (or not) in every context where a name was used. In this particular case, our parser accepts names with extra subscripts (e.g. `x[1]`) because we want to support this in the future . However, they are not yet supported by CockroachDB, so for now, we need to reject them before execution. Since this syntax can be supported in different ways depending on context, we didn’t place the check in the common normalization function; instead the code using the struct was responsible for checking whether subscripts were accepted or not. Now, realize that meanwhile, the Go language and its designers very much promote direct access to struct fields for “simple uses,” instead of suggesting to systematically encapsulate objects and use method accessors as is done in Java or C++. This is the current tune in CockroachDB’s code. And all the “simple uses” of `QualifiedName` we had written throughout CockroachDB’s source code were actually all missing a check that subscript notation wasn’t used. What we really needed was an accessor method with a common check and we hadn’t so realized; in other words, this problem was almost bound to happen ever since we followed Go’s suggestion to avoid encapsulation by default. Strong typing to the rescue Trying to address the issue by manually adding the missing checks at every use of `QualifiedName` was risky (there were many; how could we be sure to not miss any?) and foremost too brittle: What about the next person adding a new use of `QualifiedName` somewhere? Instead we went for what all good programming textbooks and best practices had told us to do from the beginning: use different types for different things! The full solution is outside of the scope of this blog post and can be explored here . In summary, we now have one type called `UnresolvedName` produced by the parser; then, during semantic analysis, objects of this type are normalized into objects of different types, either `TableName`, `FunctionName`, `AllColumnsSelector` / `UnqualifiedStar` (for column groups) or `ColumnItem` (for individual columns or sub-fields/subscripts). Since each type now has its own field and interface, it becomes more difficult to use a name improperly after it was normalized. As a result, the previous type `QualifiedName` could be removed entirely. Implementing this fix involved changes to pretty much the entire SQL code base. The review of this patch was grueling due to the large number of changes. However each part of the change was localized and thus checking its correctness was still tractable. In the process we fixed all the corresponding bugs found by our random generator, as well as a couple of other bugs that only became obvious during the change, for example latent errors with: CREATE TABLE newtable (x INT, CHECK (COUNT(*) = 1))\nCREATE TABLE db1.t (x INT, FOREIGN KEY (x) REFERENCES db2.t(x)) (The last two statements would only cause the server to fail at the first INSERT on the new table). Lesson learned Imagine you have a toy, and the object of the toy is to fill it by passing its contents through appropriate slots. Ideally, you’d want the toy to tell you that you’re doing it wrong by using different shapes for the different objects and holes. Instead, with CockroachDB, all the objects and holes were shaped the same and the rule was instead to “pay attention to the color.” What’s worse is that Go didn’t help us realize we were also color blind. Strong(er) typing really helped us put things back in shape. We’ll do it more from now on.", "date": "2016-08-18"},
{"website": "CockroachLabs", "title": "How to Run CockroachDB on a Raspberry Pi", "author": ["Nathan VanBenschoten"], "link": "https://www.cockroachlabs.com/blog/run-cockroachdb-on-a-raspberry-pi/", "abstract": "Scaling effortlessly over multiple nodes is one of the defining properties of CockroachDB. By maintaining a strongly-consistent database state across a network of machines, the distributed system can provide reliability and availability, while transparently tolerating disk, machine, and even datacenter failures. But not everyone has access to a datacenter at their fingertips, so we recently began looking into what it would take to run CockroachDB on a Raspberry Pi, one of the go-to tools of the modern day computer tinkerer. Running CockroachDB on a Raspberry Pi Going into it, we knew we were in for a challenge. CockroachDB had only ever been tested on the x86 computer architecture, and we were already aware of a few hurdles we would need to jump in order to get it working on the ARM architecture sported by Raspberry Pis. On top of this, we had also never successfully gotten CockroachDB working on a 32-bit architecture, and were afraid of subtle differences between the memory address sizes coming back to haunt us. Still, we felt we were up to the task, and after a lot trial and error and a little bit of luck, we’re happy to say that we were able to run CockroachDB on a Raspberry Pi. The rest of this post will walk through the steps of compiling and running CockroachDB on a Raspberry Pi. While we were able to get the database running, we can make no guarantees that it does not contain hidden bugs, that it won’t delete all of your data, or that it won’t set your Pi on fire, because we do not officially support the ARM family of architectures or 32-bit CPU architectures in general. As such, your mileage may vary. If you run into any issues while following these steps, hit us up on GitHub by filing an issue. Hardware/Software Specs Below are the specs of the Raspberry Pi that we used to get CockroachDB up and running. Other models will probably work as well, but again, no guarantees. Model: Raspberry Pi 1 Model B Architecture: ARMv6 (32-bit) CPU: 700 MHz single-core ARM1176JZF-S Memory: 512 MB (shared with GPU) Note: Models with 1 GB of RAM instead of 512 MB may be able to skip step #5, as memory constraints may no longer be an issue. OS: Raspbian Jessie (May 2016) Storage: 8 GB SD Card Cockroach SHA: fd3b6cfe118e182bf3564da481774c0adff30851 Installation Steps 1. Download Golang We are going to be compiling CockroachDB from source on the Raspberry Pi itself to avoid having to deal with cross-compilation. The reason for this is that while cross-compiling pure Go applications is a breeze , the task suddenly gets a lot trickier when the applications begin dealing with cgo . To avoid this, we’ll just compile CockroachDB directly on the Raspberry Pi. Because we’re compiling from source on our Pi, our first step is to get the Go toolchain up and running. Download and install the ARMv6 archive , and then follow the installation instructions . Note that at the time of writing this, go version 1.7.x is required to compile CockroachDB. After this step, you should have a working GOPATH. 2. Download CockroachDB’s source This step is nice and simple, just run go get -d github.com/cockroachdb/cockroach in your GOPATH. 3. Modify constants that assume a 64-bit architecture In github.com/cockroachdb/cockroach/storage/engine/rocksdb.go there are two identical lines which both say: const maxLen = 0x7fffffff These constants are used to specify the maximum length array that Go will allow. This maximum length is 2^31 because the length must fit inside a signed 32-bit integer. However, because we are working on a 32-bit architecture, and because the elements in these arrays are multiple bytes in size, these arrays will exceed the size of the memory address space. To address this, we can just reduce the constant values by replacing them with: const maxLen = 0x7fffff 4. Modify RocksDB compiler flags CockroachDB’s cgo package wrapping RocksDB uses a compiler flag that is not available to the PI’s version of g++. Luckily, it is safe to simply remove this flag. In github.com/cockroachdb/c-rocksdb/cgo_flags.go replace the line // #cgo CXXFLAGS: -std=c++11 -fno-omit-frame-pointer -momit-leaf-frame-pointer with // #cgo CXXFLAGS: -std=c++11 -fno-omit-frame-pointer Thanks to @linuxerwang for pointing this out in #6091 . 5. Increase available RAM Compiling and linking Go programs (especially big ones like CockroachDB) can take up more memory than we have available by default on the Raspberry Pi, so we need to work around this. We use the two tricks listed below to give the Go compiler enough memory. 5a. Configure Memory Split The Pi (Model B) comes with 512 MB of memory shared between the video subsystem and the main processor. To assist during the compilation, we adjust this memory split in favor of the main processor by using the `raspi-config` utility. sudo raspi-config This will open up a configuration tool. Go to `Advanced Options`, then `Memory Split`, and give the GPU 64 MB of memory instead of 128 MB. After this, go to `Ok`, `Finish`, and you will be prompted to reboot your Pi. Go ahead and reboot. 5b. Increase Raspberry Pi’s Swapfile Size By default, the Raspbian distribution comes with a 100 MB swapfile. This is on the small side, so we increase this size to 1024 MB in the file /etc/dphys-swapfile : sudo vim /etc/dphys-swapfile The default value in Raspbian is: CONF_SWAPSIZE=100 We need to change this to: CONF_SWAPSIZE=1024 Then we need to stop and start the service that manages the swapfile on Rasbian: sudo /etc/init.d/dphys-swapfile stop\nsudo /etc/init.d/dphys-swapfile start When this is complete, we should be able to verify that the swap size has increased by running the command free -m . Because swapping on flash-based memory is generally considered bad, you might want to reset this swapfile size after completing the compilation. 6. Compile using the standard system allocator If we tried to compile CockroachDB now, we would run into a large number of warnings. By default, CockroachDB chooses to replace the standard glibc memory allocator with jemalloc , and these warnings are coming from our cockroachdb/c-jemalloc package. While the warnings may be safe to ignore, we can compile CockroachDB with the standard allocator instead using the command: cd $GOPATH/src/github.com/cockroachdb/cockroach\nmake build TAGS='stdmalloc' If all goes well, CockroachDB should successfully compile and we will be left with a `cockroach` binary. You now have a distributed database ready to run on your Raspberry Pi! We can now start a local cluster by following the steps listed here on our docs . If we’re feeling ambitious, we can even drop this binary onto a few more Raspberry Pis and create a Raspberry Pi cluster . When you get your cluster up and running, let us know what you’re using it for. Hit us up in the comments or email us at projects@cockroachlabs.com . If you get stuck somewhere, feel free to reach out for help on our forum or on GitHub . Helpful Sources http://dave.cheney.net/2012/09/25/installing-go-on-the-raspberry-pi https://www.bitpi.co/2015/02/11/how-to-change-raspberry-pis-swapfile-size-on-rasbian/ /docs/stable/", "date": "2016-09-20"},
{"website": "CockroachLabs", "title": "Why Can’t I Run a 100-Node CockroachDB Cluster?", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/cant-run-100-node-cockroachdb-cluster/", "abstract": "``` CockroachDB is designed to be a scalable, survivable, and strongly consistent SQL database. Building a distributed system with these capabilities is a big task. Beyond the required functionality, it must also be correct, performant, and stable, or it isn’t worth the bits used to copy the binary. ``` From the start, our approach was to focus first on correctness, as it always proves the most difficult to retrofit , next on performance for single nodes, and then on stability and performance for multi-node clusters. Since announcing our beta in March, we’ve made substantial progress on the first two points; over 2,000 commits to the GitHub repo have added an array of distributed SQL capabilities, made substantial performance improvements, and put significant work into distributed SQL execution. Unfortunately, what’s been lost in the fast-paced shuffle is sustained focus on stability. Our Q3 goal of a 10-node cluster running under continuous load for two solid weeks has been maddeningly elusive. Factors contributing to product instability include: Components are intertwined with enough complexity that surprising emergent behavior must be dealt with. Stability fixes are often under-designed, failing to address a problem adequately. Fixes are also sometimes over-designed, adding unnecessary complexity. Code churn (refactorings, new feature work, dependency upgrades) makes stability a moving target. No engineers are focused on stability full-time. A Focus on Stability Stability has become our most pressing concern. To effectively address it, we’re internally designating it as a “code yellow.” This is a Googlism, intended to accord extra importance to an effort of singular importance to the company. Without stability, we don’t have a working database , so the priority here is appropriate. During a code yellow, any requests made on its behalf take immediate precedence: as in, “drop everything else and focus on stability!” We’ve made several changes as part of the stability code yellow. First, we’ve split out a team to focus full-time; other team members will also be involved anywhere stability work overlaps their areas of expertise. Second, we’ve decided to stabilize the master branch in isolation; new feature development will continue in the develop branch . The develop branch will periodically merge stability fixes from master . Finally, merges to master will be tightly controlled by the stability team, who have responsibility for reviewing all changes, and for cherry-picking changes to develop which have stability overlap. What it Means for Contributors For contributors, working in develop instead of master may be disruptive, though that decision was made with intention. As long as this stability push is underway, we want contributors to consider the impact of their changes. In particular, avoid refactorings or dependency upgrades which affect the storage , rpc , or kv packages. The hope is we can minimize the complexity of downstream merging, as well as decrease the likelihood that those merges reintroduce instability. What it Means for Users Users of CockroachDB may have noticed our beta release schedule has slowed. We expect to resume weekly releases once we restore basic stability, likely next week. Future beta releases will primarily contain stability fixes, and will not include new features being added to the develop branch. To get access to new features in develop , you’ll need to build from source . The list of new features is small now, but it will continue to grow over the course of this code yellow. With a lot of hard work and a little luck, we’ll have CockroachDB clusters running smoothly. Stay tuned for my next blog post, which will be a post-mortem of this stability code yellow with lessons learned.", "date": "2016-08-25"},
{"website": "CockroachLabs", "title": "Consensus, Made Thrive", "author": ["Tobias Grieger"], "link": "https://www.cockroachlabs.com/blog/consensus-made-thrive/", "abstract": "When you write data to CockroachDB (for example, if you insert a row into a table through the SQL client), we take care of replication for you. To do this, we use a consensus protocol – an algorithm which makes sure that your data is safely stored on multiple machines, and that those machines agree on the current state even if some of them are temporarily disconnected. In this post, I will give an overview of common implementation concerns and how we address these concerns in CockroachDB. Then I will abandon these earthly constraints and explore how we could improve consensus algorithms. Specifically, what would it take to make them faster? Consensus Algorithms Applied Consensus algorithms are inherently distributed, and the problem they solve is fundamental to any piece of software which wants to keep a consistent state across multiple machines. After several decades, the body of research on them seemingly presents you with a variety of implementations options to choose from. However, as pointed out in Google’s “Paxos Made Live,” using consensus algorithms in the real world is not quite as simple: the things that matter most for real implementations are often mere side notes in their respective papers. A typical consensus algorithm accepts operations from a client, and puts them in an ordered log (which in turn is kept on each of the replicas), acknowledging an operation as successful to the client once it is known that the operation has been persisted in a majority of the replicas’ logs. Each of the replicas in turn execute operations from that log in order, advancing their state. This means that at a fixed point in time, the replicas may not be identical, but they are advancing through the same log (meaning that if you give them time to all catch up, they will be in the same state) – the best you can hope for in a distributed system. Typical Concerns When Implementing a Consensus Algorithm: Log Truncation : Having all of the operations in an ordered log is fine, but that log can’t grow forever! When all replicas have caught up, older log entries should be discarded. Snapshotting : Since the log can’t be kept forever, after an extended period of downtime of a replica, there must be an alternative way of catching it up. The only option is transferring a snapshot of the data and a log position from which to resume. Membership Changes : These are very tricky to get right. As we add a replica to the group, the size of a majority changes. A lot of decisions have to be made: which majority size is active while the membership changes? Does the new replica have any say in the group while it’s being added? When does it receive a snapshot? Can a snapshot be sent before the membership change is carried out, to minimize the impact of the change? Removal is similarly iffy, and the consensus group is typically more vulnerable while the process is ongoing. Replay Protection : commands proposed by a client may be executed multiple times (or never, depending on the implementation). While one client proposal ideally leads to exactly one executed command in almost all cases, general exactly-once delivery is impossible in a distributed system . In practice, this means keeping state about already executed commands, or even better, using only idempotent commands. Read Leases : when using a vanilla consensus protocol, all read operations of the replicated state have to go through that consensus protocol, or they may read stale data [1] , which is a consistency violation. In many applications, the vast majority of operations are reads, and going through consensus for those can be prohibitively expensive. These and many others (which aren’t as readily summarized) make it hard to work an instance of a consensus protocol into a real application, let alone thousands of them . “Raft Made Live” At CockroachDB, we have most of the above concerns sufficiently covered. The author of Raft , our consensus protocol of choice, did a pretty good job at providing comprehensive instructions for much of the above. We truncate our log appropriately, regardless of whether all replicas are up or not . We send snapshots when appropriate, and soon we will also send pre-emptive snapshots during membership changes. We implement replay protection using MVCC and a consensus-level component . And, last but not least, we have a stable leading replica which gets to serve reads locally. That’s all fine and well, but there are various areas of improvement. Let’s leave behind the realm of what’s been implemented (at least in CockroachDB, and probably almost everywhere else) and talk about what should be possible in an ideal world. Consensus is like caviar: too expensive to splurge on The most obvious problem with distributed consensus is that it’s inherently slow. A typical consensus operation goes as follows: CLIENT\n                                    |  ʌ\n                                (1) |  | (4)\n                                    v  |\n                                   LEADER\n                               [node1, Oregon]\n                                 /  ʌ    \\\n                          (2)  /  / (3)    \\ (5)\n                             /  /            \\\n                           v  /                v\n                        FOLLOWER            FOLLOWER\n                   [node2, California]  [node3, Virginia]\n                                         (responds later) A client sends a request to the leader. In turn, the leader must talk to a majority of nodes (including itself), i.e. in the picture it would have to wait for one of the followers (for simplicity we assume that node three is always last). In simple math, assuming that actual message processing takes no time, we get commit_latency = round_trip(client, leader) + round_trip(leader, follower) This internal coordination is expensive, and while it’s unavoidable, we can see that the price tag depends heavily on the location of the client. For example, with a client in Oregon, we have roughly zero latency from the client to the leader, and ~30ms round-trip between the leader and the follower in Virginia, for a total commit latency of about 30ms. That doesn’t sound so bad, but let’s look at a client on the east coast instead – it would presumably be close to our Virginia data center, but that doesn’t matter – the leader is in Oregon, and we pay perhaps an 80ms round trip to it, plus the same 30ms as before, adding up to a hefty 110ms. This goes to show that once you have consensus, you will do all you can to reduce the time you wait for those transcontinental (or even transmundial) TCP packets. For example, you could ask yourself why in that last example the client couldn’t talk directly to the node in Virginia. There is a relatively recent consensus protocol called EPaxos which allows this [2] , though we’ll save it for another blog post. Today, we’re going to deal with a more modest question: Can we make reads cheaper? Read operations may seem innocuous at first. They get served from the leader because that replica is the only one that can guarantee that it’s not reading stale data (since it decides when write operations commit), but read operations don’t have to go through consensus themselves. This means that for our example, we shave 30ms of the commit latency if we only read data. However, reads are still expensive when you’re far away from the leader. It seems silly that the client in Virginia can’t read from its local node; sure would be nice to do better, right? And you can! (At least in the literature.) The idea is simple: By letting the consensus group agree that commands must be committed by a special majority of the nodes as opposed to any majority, the nodes in that special majority can be sure to be informed about the latest state of the system. For example, in the above picture, we could set things up so that all nodes agree that nodes one and two are to be included in any majority when committing commands (regardless of who the leader is), and then these replicas could serve reads which are consistent with the consensus log. The resulting algorithm is investigated (in higher generality) in Paxos Quorum Leases: Fast Reads Without Sacrificing Writes . In a simpler world, modulo the usual implementation headaches, that could be the end of the story – but there’s an additional bit of complexity hidden here: the fact that CockroachDB is not a simple replicated log, but a full-fledged MVCC database. This means that the key-value pairs we store have a logical timestamp attached to them, and the one invariant that we must uphold is the following: If a value was ever read at some timestamp, it can only be mutated at higher timestamps. That makes perfect sense if you think about it – if you read a certain value at some timestamp, then I should not be able to perform a write that changes the value you already observed. On the leader, this is achieved by keeping an in-memory timestamp cache – a data structure which given a key and a timestamp will tell you whether the key was read at a higher timestamp previously. This structure must be consulted before proposing a write to consensus to guard us against the scenario described above – if there was such a read, we can’t perform the write. If local reads were served at another replica, naively the leader would have to be notified about that synchronously (in order to write to the timestamp cache) before returning the result of the read to the client – the very thing we wanted to avoid! Or, somewhat better, we could let reads remain cheap for the most part and shift complexity onto writes, requiring them to contact the special majority before proposing to confirm that writing at this timestamp is still possible, and prompting the special majority to not serve reads with conflicting timestamps (at least until they see our command pop out of the consensus protocol). Another (much more complicated) option is to incorporate that feature at the consensus level by allowing replicas to reject commands before the commit. In that scenario, roughly the following would occur: Follower 1 serves a local read at timestamp, say, ts=15 . A client asks the leader to write that same key at timestamp ts=9 . The leader proposes a corresponding command to consensus. The consensus algorithm on the leader tries to replicate this command to a majority of followers (including the special majority ). Each follower checks the command for timestamp cache violations. Some followers may acknowledge the proposal, but on Follower 1, it is rejected due to already having served a read for ts=15 prior to the write at ts=9 . The leader, upon receiving the rejection, informs the client and cancels replication of the command suitably (either by turning it into a no-op or by unregistering it completely, depending on what’s possible). To the best of my knowledge, such an addition has not been considered for any consensus protocol (and in particular not for the one of most interest to us, Raft). Allowing individual replicas to reject certain commands ad-hoc (i.e. basing their decision on auxiliary unreplicated state) must be considered very carefully and adds considerable complexity (in particular when leadership changes as these commands are in flight). Performing that work is likely a small research paper and a bunch of implementation, but in contrast to many other more complicated endeavor, it seems within reach (and with it, serving local reads from some replicas). Conclusion Our usage of consensus algorithms in CockroachDB is fairly standard and covers all the basic needs – but taking a step up is something we’ll be working on in the future. While the likely next step is serving reads from (some) followers, techniques which save round-trips on writes are also appealing, but those go extremely deep down the rabbit hole (and have new, much deeper challenges with respect to serving local reads). As usual, distributed consensus is hard. And if that’s just your cup of tea, you could have that tea every day . [1] Even if the client attempts to talk to the master node, the node it talks to may not be the actual master (though it may think it is), and so commands which have already committed and influenced the outcome of our read may not yet have been executed on the node we’re reading from yet – this violates linearizability on a single register. [2] Egalitarian Paxos ( There Is More Consensus In Egalitarian Parliaments )", "date": "2016-07-14"},
{"website": "CockroachLabs", "title": "Implementing Column Families in CockroachDB", "author": ["Daniel Harrison"], "link": "https://www.cockroachlabs.com/blog/sql-cockroachdb-column-families/", "abstract": "CockroachDB is a distributed SQL database built on top of a transactional key value store. We don’t (yet) expose the kv layer but it’s general purpose enough that we’ve used it to implement SQL without any special trickery. The particulars of how we represent data in a SQL table as well as the table metadata are internally called the “format version”. Our first format version was deliberately simple, causing some performance inefficiencies. We recently improved performance with a technique called column families, which pack multiple columns in one kv entry. Once implemented, column families produced dramatic improvements in our benchmarks . A table with more columns benefits more from this optimization, so we added a benchmark of INSERTs, UPDATEs, and DELETEs against a table with 20 INT columns and it ran 5 times faster . Press on, dear reader, and I’ll explain the details of how we did it and how they work. Format Version 1: CockroachDB Before Column Families CockroachDB requires every SQL table to have a primary index; one is generated if it was not provided by the user. Our first format version stored the table data as kv entries with keys prefixed by the columns in the primary index. The remaining columns were each encoded as the value in a kv entry. Additionally, a sentinel key with an empty value was always written and used to indicate the existence of a row. This resulted in N+1 entries for a table with N non-primary index columns. Secondary indexes work a little differently, but we don’t need them for today. This all results in something like: /<tableID>/<indexID>/<primaryKeyColumns...>/<columnID>  ->  <4 byte CRC><encoded value> And more concretely: CREATE TABLE users (id INT PRIMARY KEY, name STRING, email STRING);\nINSERT INTO users (11, \"Hal\", \"hal@cockroachlabs.com\");\nINSERT INTO users (13, \"Orin\", \"orin@cockroachlabs.com\");\n\n/<tableid>/0/11/0 -> <empty>\n/<tableid>/0/11/1 -> \"Hal\"\n/<tableid>/0/11/2 -> \"hal@cockroachlabs.com\"\n/<tableid>/0/13/0 -> <empty>\n/<tableid>/0/13/1 -> \"Orin\"\n/<tableid>/0/13/2 -> \"orin@cockroachlabs.com\" Note that columns never use ID 0 because it’s reserved for use as the sentinel. This is all described in much more detail in the original SQL in CockroachDB: Mapping Table Data to Key-Value Storage blog post. If you haven’t read it, I highly recommend you do. The Trouble with Format Version 1 Everything has to start somewhere, and while our first format version worked, it was a little inefficient. The encoded primary index data in the key was repetitive, and there is an MVCC timestamp and checksum for each entry, collectively wasting disk space and network bandwidth. Perhaps worse was that there was per-key overhead at the transaction level. Every key written within a transaction has a “ write intent ” associated with it. These intents need to be resolved when the transaction is committed, taxing performance. While our disk format avoids the key repetition with an incremental prefix encoding, the timestamp and the checksum still create ~12 bytes of overhead per key, not to mention the intents. Since the problem was using one kv entry per column in the table, the natural solution was to group multiple columns into one value. Several NoSQL databases use a similar technique and call each group a “column family”. Implementing Column Families in CockroachDB When we set out to implement column families, the first wrinkle was deciding whether to support get and set on individual columns in a family or to load and store an entire family to change one column. The former would allow us to make every table’s primary data one key value entry. Unfortunately, it would also require the kv layer to understand the encoding that packs multiple columns in one value. If we later decided to change the encoding, it would be much more difficult to migrate if it were baked into the key value layer. Plus, the tidy separation they’ve enjoyed so far has been a big help to testability and moving quickly. We felt this wasn’t a worthwhile tradeoff. As a result, we support multiple column families per table, so that setting a small field doesn’t necessitate roundtripping any large fields in the same table. Side note: A common question we get is whether we support use of the key value layer directly. We don’t right now, but by using one entry instead of two, we’ve gotten much closer to eliminating the overhead of using the CockroachDB key value store via a two column key and value SQL table. How Do Column Families in CockroachDB Work? Before column families, the value of an encoded table column was structured as: <crc><typetag><encodedvalue> With column families, this is now: <crc><columnid0><typetag0><encodedvalue0>...<columnidn><typetagn><encodedvaluen> or for our example above /<tableid>/0/11/0 -> <crc>/1/string/\"Hal\"/1/string/\"hal@cockroachlabs.com\"\n/<tableid>/0/13/0 -> <crc>/1/string/\"Orin\"/1/string/\"orin@cockroachlabs.com\" Notably, the column IDs in the keys have been replaced by family IDs. The first family ID is 0, doubling as the sentinel, and is always present. We use a variable length encoding for integers, including column IDs. This encoding is shorter for smaller numbers, so instead of storing the column ID directly, we store the difference to keep them smaller. NULLs are omitted to save space. A couple of the existing data encodings (DECIMAL and BYTES) didn’t self-delimit their length. It’s desirable if we can extract the data for some of the columns without decoding them all, so we added variants of these two encodings that are length prefixed. A constant concern of working in any system that persists data is how to read old data with new code. We made column families backward compatible by special casing a family that’s only ever had one column; it’s encoded exactly as it was before (with no column ID). This also happens to have the side benefit of being a nice space optimization. All this and more is detailed in the Column Families RFC if you’re interested. Using Column Families When a table is created, some simple heuristics are used to determine which columns get grouped together. You can see these assignments in the output of the `SHOW CREATE TABLE` command. CockroachDB can’t know the query patterns of a table when it’s created, but the way a table is queried has a big impact on the optimal column family mapping. So, we decided to allow a user to manually tune these assignments when necessary. A small extension (`FAMILY`) was added to our SQL dialect to allow for user tuning of the assignments. The various tradeoffs are detailed in our column families documentation . Building a SQL database after the rise of NoSQL means that CockroachDB gets to pick the best parts of both. In this case, we were able to use column families, an optimization commonly found in NoSQL databases, to speed up our SQL implementation. The resulting performance improvement moves us one step closer to our 1.0 release. Does jamming on distributed SQL put a spring in your step? Then good news — we're hiring! Check out our open positions here .", "date": "2016-09-29"},
{"website": "CockroachLabs", "title": "Memory Usage in CockroachDB", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/memory-usage-cockroachdb/", "abstract": "In this blog post, we provide some details on how CockroachDB uses system memory on each node, and what you can do to keep memory usage in CockroachDB under control. Overview To understand memory usage in CockroachDB, and specifically within a CockroachDB node, it is perhaps useful to imagine memory like a giant cake that is being split up in pieces and distributed to “eat” CockroachDB’s various components. There are three main cake eaters in CockroachDB; in approximately decreasing order of appetite: Cache memory. The biggest part of this is RocksDB’s cache. CockroachDB is based off RocksDB’s KV database , and RocksDB  allocates a very large chunk of system memory to serve as a shared cache between all KV operations performed by a node. Memory used at rest. This is composed of: Replica metadata . Each store holds in memory the metadata for the replicas available in that store. Metadata for other nodes . Each node maintains data about every other peer in the cluster. Memory used by activity. Mainly: Store processing structures . Each store maintains multiple operation queues of variable sizes; under load these contain data for ongoing transactions. SQL data structures . For every client, the node will instantiate many data structures in memory to support the execution of SQL statements. These consumers share in common that the amount of memory they need varies depending on run-time parameters . (There are other fixed-size usages of memory in CockroachDB, but it is negligible in comparison with those above.) RocksDB’s cache is pretty straightforward: it will consume up to the --cache-size parameter’s value worth of memory, by default 25% of total system memory, and no more. The node metadata, in comparison with the other consumers, is negligible, even for clusters with dozens of nodes. The other items – metadata for replicas and memory used for activity – need more attention and are the subject of this blog post. Memory usage by replica metadata On each node, store memory usage increases mainly with the number of stores times the number of replicas per store. How do database parameters and status influence per-store memory consumption? If your database grows larger, the number of ranges increases. Since CockroachDB tries to distribute data across nodes (assuming no further zone configuration here), the number of replicas at each nodes will also increase, roughly uniformly. If you change the replication factor, the number of replicas increases or decreases accordingly. When you change the number of stores, all other parameters kept equal, replicas and queue load will be spread across the available stores. So if you configure more stores, there will be more queues but the number of replicas and queue entries per store will decrease overall. So overall memory usage per node may change a little but not significantly. Therefore we can say that memory usage related to stores at rest evolves approximately with the following formula: k1 × (DataSize × ReplicationFactor) / NumNodes + other Where k1 is the metadata size per replica; and “other” is the rest of the memory usage per store, which is negligible in comparison. From a complexity theory perspective, we say that store memory usage is: O( (1 / NumNodes) × ( DataSize × ReplicationFactor) ). The main variables in this equation are available from CockroachDB’s admin UI: the number of nodes and replicas per node. Although the precise value of the parameter k1 is not directly available, an operator can estimate the overall memory incurred by stores. This can be done simply by first disabling SQL clients temporarily from one of the cluster nodes (or alternatively, by adding a new node without clients and letting the replicas migrate to the new node), ensure that distributed SQL queries are not run on the isolated node, then observe how much system memory is needed for the node, and subtract the RocksDB cache size from this. The result gives the overall cost of replica metadata. Another characteristic of the at-rest memory usage incurred by stores is that it evolves slowly and predictably : the overall database size is typically an application-level metric that closely follows business growth, and the number of nodes is is typically changed under operator supervision. Finally, note how how the formula above has the number of nodes as divisor: adding more nodes, keeping all parameters equal, will decrease memory usage per node. This property can be used advantageously when running off cloud resources, where memory is often not priced linearly: adding 10 new nodes with 2GB of RAM each is usually cheaper than switching five existing nodes from 2GB to 6GB each. Scale can lower your costs! Memory usage incurred by Store activity Under load, each node receives requests to update stores locally and across the network. These requests are queued until they are processed. Bursts of activity cause data to pile up in queues. How do database parameters and status influence this memory consumption? If you increase the load on the cluster (more active transactions overall) the number of in-flight operations stored in the queues increases, although of course the activity is also spread across all nodes. Memory usage in queues increases with activity but decreases as nodes process the queued operations. With faster CPU/disk speeds, the overall memory usage due to queued backlog become lower. When you change the number of stores, all other parameters kept equal, queue load will be spread across the available stores. So if you configure more stores, there will be more queues but the number of queue entries per store will decrease overall. So overall memory usage per node related to store activity may change a little but not significantly. Therefore we can say that memory usage related to store activity evolves approximately with the following formula: k2 × NumInFlightOperations / (NumNodes × NodeSpeed) + other Where k2 is the memory costs incurred per data accumulated in some queues per active transaction; and “other” is the rest of the memory usage due to activity load, which is negligible in comparison. From a complexity theory perspective, we say that memory usage related to store activity is: O( (1 / NumNodes) × ( NumInFlightOperations / NodeSpeed) ). Again the admin UI provides some insight by showing the number of SQL clients and queries executed, which in a production database is likely to correspond more or less with the load on store. This usage is less predictable than at-rest memory usage. Although the number of nodes is a known and relatively stable value, the number of in-flight operations can evolve unpredictable. For example, a sudden increase in contention (e.g. a single post going viral, leading to contention on transactions updating a hit counter) could cause large spikes in the number of in-flight operations. This said, notice how the processing speed of nodes is also the divisor in this formula. As long as the nodes running CockroachDB are fast enough (this includes both CPU and disk speeds), the stores will be able to absorb spikes in activity more effectively. In a way, hardware performance dampens the unpredictability of store activity . Memory usage incurred by SQL activity Each SQL client in CockroachDB that issues queries or statements will incur memory allocations on (at least) the node where the query was received. There are three overarching concepts that you need to understand before analyzing SQL memory usage further. The first is that SQL-related memory usage in CockroachDB is organized around the notion of a “ session ”. Sessions are not only the logical span of time between the point the SQL client connects to the point it disconnects; they also determine the lifetime of SQL-related memory allocations: most SQL allocations are scoped to the statement or transaction; only a small amount persists to the end of the session, and in any case no later than that. The second is that sessions are created both by external client applications connecting to CockroachDB, and by CockroachDB components internally . For example, when a node needs to log some error message, the error message is also saved to the database using SQL. To do this, the node creates an “internal session” and issues SQL statements to itself. Henceforth, we will say that there are multiple channels that open and maintain SQL sessions in a CockroachDB node; some externally visible (external clients, admin UI) and some invisible (internal components). Understanding which channels exist in CockroachDB helps answering the question “Where are sessions coming from?” and thus “How many sessions are open on this node?” Finally, CockroachDB’s architecture is layered. Approximately, clients connect to the SQL API, which is based on services from the KV API, which relies on the distributed store API, which itself relies on the store API. Each of these abstraction layers will allocate data structures at its level on behalf of the SQL session . Therefore, beyond first-order memory costs that are easily understandable by casual SQL users, like in-memory tables, there are also indirect costs to each session made by the layers below SQL. Understanding indirect costs helps answering the question “How much memory is allocated on behalf of a single session?” Once you grok these three concepts, you can understand the ground formula that determines SQL-related memory usage in CockroachDB: NumSessions × ( SQL-level memory usage + indirect costs) NumSessions depends on activity over the various channels. We detail these variables in the following sub-sections. Channels: where SQL sessions appear The most obvious channel through which SQL enters CockroachDB is client connections using the PostgreSQL wire protocol (“pgwire”) . In addition to this “main” channel, sessions are also created via: External channels: Uses of the Admin RPC over HTTP. This is mainly used by CockroachDB’s Admin UI. There is at most one active session per RPC call. Built-in control commands like cockroach user . Here too there is at most one active session per command running. Internal channels: Event logging : Up to one session is opened for each logged server event and per-replica maintenance operation. This may sound like a prohibitive overhead, since logging is usually a hotspot, but CockroachDB uses a lightweight form of session for this activity specifically. Backfilling : One session to support populating newly added columns or indexes. Each node can be running a single backfill per table. With N tables undergoing schema changes, you can see N backfills running simultaneously, with one session per backfill. Table lease updates : One internal session every time the node “takes a lease” on a table. Taking (or releasing) a lease means marking the table as being in use on that node; this is done by updating a lease table in the database using SQL. Leases are cached, so that lease-related SQL activity is amortized across multiple SQL transactions. Notice how table lease updates are not performed on behalf of client sessions, and are instead performed using an internal session. This is mainly because lease cache updates require a different set of database permissions than those available in client session. Per-session memory usage When a SQL session appears on a node, a couple of data structures of negligible size are allocated in memory, then the session starts accepting statement (or queries) by clients. The statements are organized in transactions: single-statement “implicit” transactions for statements issued outside of a BEGIN-COMMIT block, and “explicit” transactions where the BEGIN-COMMIT blocks is defined by the client. Memory allocations on behalf of a session can thus be further divided into: session-wide allocations , which can stay active until the end of a session, spanning possibly multiple transactions, transaction-specific allocations , which are guaranteed to be released when a transaction is committed or aborted, and session-lingering allocations , for results that stay in the session after a transaction completes and until the client processes the data. For now, the only session-wide allocations are for prepared statements and portals . The pgwire protocol allows clients to “prepare” statements once in the session, which can be reused multiple times after that without spelling out their SQL in full. After a statement has been prepared, its corresponding SQL code and data structures remain in the node’s memory until the session ends or the client deallocates the prepared statement. Portals add parameters to prepared statements in the node’s memory, and have a similar lifetime. Next to that, the only session-lingering allocations are result sets . After a query completes, and until the response of the query is fully transmitted back to the client, the results of that query are retained in the node’s memory. We call these packets of results “buffered result sets”. These result sets can become arbitrarily large depending on the SQL code being executed. (Note that we have plans to reduce the amount of buffering for results sets, but this is not implemented at the time of this writing.) The remainder of SQL memory usage is caused by transaction-specific allocations. Of these, the bulk of memory usage is caused by in-memory row storage : Sets of rows before sorting. When you use ORDER BY using non-indexed columns, the server sorts in-memory, so the rows need to fit there. Unique prefixes. When you use DISTINCT, the server keeps an in-memory hash table of the values already seen to filter out duplicates; this grows linearly with the number of unique values. Window partitions. When you use window functions, the server keeps partitioned rows in memory for window processing, too. Unique prefixes for UNION, INTERSECT and EXCEPT. Aggregate results during grouping. When you use GROUP BY, the server keeps one aggregate row in memory for each distinct group in the input data. Intermediate JOIN results. In our preliminary implementation, this grows linearly with the full size of one of the JOIN operands; then even with a better JOIN (in the future), you can expect memory usage to grow linearly with the size of the input tables, except in the very specific case where both JOIN operands are ordered optimally. Virtual tables. When you use a virtual table (like pg_catalog.schemata or information_schema.columns ), its contents are populated in memory for the duration of the transaction. The memory cost of these virtual tables is approximately proportional to the complexity of the database schema visible to the user issuing the query. We have some plans to support disk-based temporary storage for operations involving large tables , however regardless of further work on that front, CockroachDB will continue to support in-memory row storage so as to keep responding quickly to common queries. Beyond SQL-level row storage, there are two additional per-session indirect costs which can grow arbitrarily large depending on which SQL code is executed: KV batches . When a statement like INSERT or UPDATE is issued, the SQL layer constructs an object in memory called a “batch” where KV operations on the individual updated/inserted rows and index entries are queued, for later processing when the statement completes. Write intents . When a statement like INSERT or UPDATE completes, or when creating or modifying the database schema, markers called “write intents” are written to the database next to the new values. This is used by the MVCC transaction isolation so that transactions in other clients (sessions) can avoid reading data from non-committed transactions. These write intents are subsequently cleaned up when the transaction complete, but the cleanup process needs to know which intents have been created. The list of write intents for a transaction is kept in the node’s memory until clean-up occurs. Predictable SQL memory growth At this point we would like to bring your attention to the following additional observations: Each of the non-pgwire channels always delivers the same SQL queries and uses only simple parameters . We have tuned these queries so that the amount of memory they require stays small: they are written to use a small, fixed maximum number of rows in memory for intermediate SQL computations, and keep very small batches and few intents per transaction. During normal database operation, when the number of external client connections is large enough, we expect the number of non-pgwire sessions to be comparatively small . Therefore, we would like to posit that the SQL-related memory usage incurred by non-pgwire channels is comparatively negligible. You can perhaps invalidate this postulate by connecting many web browsers to the admin UI of a single node, but this is unlikely to happen, or even be allowed, in production databases. To enable operators to inspect SQL memory growth, we have recently added new metrics inside each CockroachDB node, which can also be viewed in the admin UI. These metrics separately track the approximate amount of memory used by pgwire clients, the admin RPC and “internal” sessions inside each node. You can now use these metrics to dimension system memory capacity as your database and application grow. Unpredictable SQL memory growth: when a node could blow up If you followed the previous analysis closely, you may have noticed that the amount of memory that can be allocated on behalf of pgwire-originating sessions is virtually unbounded! There are many SQL queries, even seemingly simple ones, that a client can send which can cause a very large amount of memory to be allocated by the node processing the query. So much memory can be allocated, in fact, that it’s possible to exceed the total system capacity and cause the operating system to terminate the server process abnormally. Here are a few examples: SELECT * FROM sometable when sometable contains many rows. This can blow up because the result set will be buffered in server memory. SELECT * FROM sometable ORDER BY somecolumn LIMIT 1 . Although LIMIT restricts the size of the result set, this can blow up if somecolumn is not indexed and the table contains many rows. SELECT * FROM sometable, someothertable LIMIT 1 . This syntax defines a cross join between sometable and someothertable . In the current implementation, sometable will be loaded first entirely in memory, and this can again blow up if it contains many rows. SELECT COUNT(DISTINCT *) FROM sometable . This requires the server to keep a copy of the unique rows in sometable , unless there is a unique index covering all columns in sometable`. UPDATE sometable SET x = x + 1 . This will lay as many write intents as there are rows in sometable , and keep track of all intents in the node’s memory until the enclosing transaction commits. When we started working on a SQL interface for CockroachDB, we initially thought the risk of memory blow-up could be sidestepped merely by thoroughly documenting how CockroachDB uses memory, including e.g. this blog post and appropriate documentation yet to be written. However, as time went by we also (re-)discovered an old truth: humans are fallible and developers make mistakes . When trying out SQL statements, whether as a CockroachDB or as an app developer, it is easy to forget about or misjudge the incurred memory costs and issue a query that will allocate a lot of memory in the node. This just happens! Seeing a node crash because of an accidental user mistake is frustrating. Therefore, in addition to appropriate documentation and monitoring in the admin UI, we now also provide a mechanism that provides a modicum of protection against accidental blow-ups. The total amount of SQL-related allocations performed by pgwire sessions (ie not by sessions coming via other channels) is now limited by a global setting --max-sql-memory , set to 25% of total system memory by default. This includes both session-wide (prepared statements, result sets) and transaction-specific (in-memory row storage) allocations. When a session causes allocations to reach the global maximum, it now fails with a client-side error, preventing the server’s effective memory usage from growing beyond the limit at which the operating system would kill the process. Caveats The analysis up to this point makes a few assumptions that are design goals for CockroachDB, but which are currently neither tested nor enforced: That the bulk of memory usage is indeed incurred by the RocksDB cache, store-related and SQL-related allocations; and that allocations for node peer metadata are comparatively negligible. That SQL activity caused by internal channels overall has a negligible memory footprint. That the admin UI, when loaded from the same node with a small number of web browsers, has a negligible memory footprint on the node. That the tracking mechanism we have implemented for SQL memory is inherently imprecise. Since Go is a managed language, there are more bytes allocated in memory for every byte usable and visible from CockroachDB’s code. It is hard to pinpoint the difference exactly. We are assuming so far it is a constant factor off from the tracked memory allocations, but we have not fully tested nor validated this assumption. We pledge to increase visibility on these overheads over time , however until then please contact us whenever you suspect that some significant memory usage has been left unaccounted for. In particular, because of the last caveat in the list above, do not set --max-sql-memory too high . If you bring it so high that, in combination with the other items discussed in this blog post, it reaches the total memory available to the CockroachDB process, you run the risk that the Go runtime’s invisible allocations are not accounted for and will cause the process to tip over the limit and crash (or become very slow by going to disk swap). If you wish to tune this setting, try different values and monitor in the admin UI how it influences your effective total memory usage under client load. You should probably avoid increasing the value further as soon the total memory usage under load grows beyond 80% of overall capacity available to the process. Also, at the time of this writing, pgwire-related memory usage is not fully tracked: KV batches and write intents are not accounted for yet. We plan to address this, too . Finally, we acknowledge that the information in this blog post is more qualitative than quantitative. We have introduced formulas for which some parameters are still largely unmeasured. It may be hard for the reader to step back and create a concrete picture in her mind: which is the largest memory consumer in practice? Store metadata at rest or in-flight store activity? Or SQL activity? To provide this intuition, we intend to run and publish some additional measurements in a later blog post; of course we welcome external validations of this as well. Summary: CockroachDB’s memory story This is our memory story: a CockroachDB node mainly consumes memory for the RocksDB cache, replica metadata, in-flight storage operations and data structures needed for SQL processing.  Most of this memory usage is relatively stable and/or predictable, following general principles that we have outlined above. We aim at improving our admin UI as the main interface available to operators to inspect, detail, and monitor memory usage. Additionally, to guard against unpredictable memory usage, which should be caused mostly by accident in external SQL client applications, CockroachDB now implements a simple but effective node-wide soft limit: runaway SQL sessions now simply fail with a client error before they incur a memory blow-up in the node.", "date": "2016-11-10"},
{"website": "CockroachLabs", "title": "How to Survive a Hackathon as a Sponsor", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/surviving-a-hackathon-as-a-sponsor/", "abstract": "Surviving a hackathon as a sponsor without becoming a complete zombie is no joke. To start, you’ll have plenty of practical, technical concerns that have to be sorted (How do you get people to try your product? Will the hackers be familiar with your application’s language?) . But once you’ve got the technical issues taken care of, you’ll likely have nearly as many questions about how to keep your team as sane and functional as possible––despite running on limited sleep, consuming excessive caffeine, and likely being completely out of their comfort zones. I’ve boiled down these concerns to what I call the “human factor” in hackathons. They become particularly glaring when you’re approaching hackathons before you’ve got a Developer Relations team in place, which is the current case at Cockroach Labs. Some of the “human factor” questions I had included: How and where do you sleep at a hackathon? What (and when) will we be eating at a hackathon? How do you mentally and emotionally prepare your developer volunteers? When I sought answers to these questions, I discovered that most of the resources – both online and offline – are tailored for participants ( Bring a sleeping bag! Drink all the red bulls!) . I thought it would be useful, therefore, to share my thoughts on how to survive hackathons as a sponsor. Surviving a Hackathon As a Sponsor Sleep. The importance of planning sleep for your team cannot be understated. Common schedules for university hackathons are Friday evening to Sunday afternoon – that’s 40+ hours straight that you’re expected to be bright-eyed and bushy-tailed. Set a sleep schedule. When possible, fit a sleep schedule to your team’s natural body clocks. I sent a survey asking when everyone preferred to sleep, and for those who didn’t respond, I intuited based on their daily behaviors in the office. If someone is at work at 9am, they’re probably a morning person and can tolerate an early wake-up. If someone arrives to work midway through lunch everyday, give them the night shift. Get a bed for every person. Our interns told me, “I don’t really sleep at hackathons,” and “I fall asleep wherever I can.” Ha! This is the hysterical talk of college students. Your team will need sleep, and they’ll need a proper place to do it. Dark, quiet, and with a bed. Hotels, house rentals, whatever you need to do: don’t be stingy when it comes to sleep. Food. These days, hackathons are practically synonymous with “all you can eat.” The hidden caveat is “all you can eat… if you’re a college kid.” Foods like pizza, french fries, cotton candy, and fried dough are widely available – delicious treats, but not adult food. Especially when you’ve got people running on less sleep than normal, eating healthful, normal food is a must. BYO Snacks. Hit the grocery store on your way to the hackathon, and pick up the kind of snacks people eat at the office. For us, that meant grocery bags full of nuts, granola, bananas, apples, string cheese, and jerky. Eat off campus. Encourage (coerce) your volunteers to leave the event for their meals. Not only will they be able to purchase the kind of food their bodies are craving, but it will also provide a very necessary mental break. Plan these meals into your budget. Awkwardness. Hackathons are awkward, especially for sponsors and their team of developers intending to engage with the participants. You’re talking to strangers; you’re pitching your product; you’re soliciting resumes. Developing a shared sense of purpose and proper preparation will help your team muscle their way through the whole event. Be clear about why you’re there. Explicit, realistic goal-setting does wonders for giving your team a sense of collective purpose. If you’re there to recruit, how many resumes should you collect? If you’re there to drive product adoption, what does “adoption” actually look like? When I sponsor hackathons, my event goal is to increase product adoption, with a target metric was “100 installations that show meaningful usage of CockroachDB.” For some products and teams, 100 installations is sandbagging, and you’ll want to aim higher. Write a script. When we went to our first hackathon, our intern Will drafted a script for us to use for approaching participants and enticing them to try CockroachDB. Run through the script with your team to identify where there are holes and where they still may be uncomfortable. These exercises were a relief to the volunteers and helped us navigate the wee hours of the morning. Surviving a hackathon as a sponsor without your whole team of volunteers zombifying – and by that I do mean eating each other and/or the participants – is something you can prepare for. Getting sleep, eating right, and setting goals are all tangible steps that will make your sponsorship successful and your team willing to volunteer for a hackathon again. I know this list isn’t exhaustive, so if you’ve got other tips, I’d love to hear them. Feel free to share them on twitter @cockroachdb .", "date": "2016-11-01"},
{"website": "CockroachLabs", "title": "Roaches on Open Water! CockroachDB on DigitalOcean", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/roaches-on-open-water-cockroachdb-on-digitalocean/", "abstract": "If you’re a fan of DigitalOcean and its powerful but simple platform to deploy cloud-based infrastructure, you’ll appreciate CockroachDB: it’s similarly simple to deploy and provides your stack a lot of power and flexibility. And while CockroachDB can be deployed anywhere, it’s a natural fit within DigitalOcean’s no-fuss framework: both are for developers who like easy-to-reason-about technology that lets them get work done quickly. Check out our office mascot Carl, who looks adorable in a costume clearly inspired by DigitalOcean’s Sammy the Shark. To show you the synergy between DigitalOcean and CockroachDB, this post is going to demo going from nothing to a distributed 3-node cluster in under 20 minutes. So you don’t have to budge from the command line, we’re going to show you how to stand everything up through DigitalOcean’s command-line tool, doctl . (If you’d rather not bother with doctl, we’ve got the standard instructions here .) After you’ve deployed everything, we’ll also check out some of CockroachDB’s core features (automatic data distribution and survivability) so you can get a better sense of how it makes your life easier. Because this is a quick demo, we’ll show you how to use an insecure cluster communicating over external IP addresses, neither of which we’d necessarily recommend for production. For a production-ready deployment guide, see Deploy CockroachDB on DigitalOcean . Install CockroachDB Create A Startup Script Because all of the servers we’re going to create need to install CockroachDB, we’re going to create a startup script that downloads the latest CockroachDB binary, and then makes it accessible from the command line. Create a file to store the startup script: $ sudo nano doctl.sh Enter the following contents into the file: #!/bin/bash\nwget https:// binaries.cockroachdb.com/cockroach-latest.linux-amd64.tgz\ntar -xf cockroach-latest.linux-amd64.tgz –strip=1 cockroach-latest.linux-amd64/cockroach\nsudo mv cockroach /usr/local/bin Write the file ( ^O ) and exit nano ( ^X ). Create Your Droplets With our startup script to install CockroachDB prepared, we can create our Droplets (which is just what DigitalOcean calls your virtual machines). Find your SSH key’s FingerPrint, which you’ll need when creating Droplets: $ doctl compute ssh-key list Create your Droplets with the following options: --size : We recommend at least 2GB --image : We’re using Ubuntu 16.10 but you should be able to use any contemporary Linux distro --user-data-file : Use the startup script we created, doctl.sh --ssh-keys : Use the SSH key’s FingerPrint you found in the last step # Create the first node\n$ doctl compute droplet create node1 \\\n--size 2gb \\\n--image ubuntu-16-10-x64 \\\n--region nyc1 \\\n--user-data-file doctl.sh \\\n--ssh-keys # Create the second and third nodes (only name differs)\n$ doctl compute droplet create node2 --size 2gb --image ubuntu-16-10-x64 --region nyc1 --user-data-file doctl.sh --ssh-keys $ doctl compute droplet create node3 --size 2gb --image ubuntu-16-10-x64 --region nyc1 --user-data-file doctl.sh --ssh-keys It can take a few minutes for your Droplets to finish being created. You can check on them using doctl compute droplet list . Get Your Droplets’ Details Once the Droplets have been created, we’ll need to get some information about them: $ doctl compute droplet list Copy down the following values from each Droplet: ID Name Public IPv4 Start Your Cluster At this point, we can start our cluster, which will contain all of our databases. SSH into node1 : $ doctl compute ssh <node1 ID> Enter yes to proceed connecting to the Droplet. Start your cluster: $ cockroach start --insecure --background --advertise-host=<node1 IP address> You’ll receive a message to stdout with your new cluster’s details, including its ID. Terminate the SSH connection to your first node ( CTRL+D ). Join Additional Nodes to the Cluster With the cluster up and running, you can now add nodes to the cluster. This is almost as simple as starting the cluster itself (just one more flag) and demonstrates how easy it is to horizontally scale with CockroachDB. SSH to the second node: $ doctl compute ssh <node2 ID> Enter yes to proceed connecting to the Droplet. Start a new node that joins the existing cluster using node1 ‘s IP address on port 26257 (CockroachDB’s default port): cockroach start --insecure --background   \\\n--advertise-host=<node2 IP address>  \\\n--join=<node1 IP address>:26257 Close the second node’s SSH connection and complete the same steps for your third Droplet, using its IP address instead. Make sure all 3 nodes are connected: $ cockroach node status The response to stdout should list all 3 nodes. Features in Action At this point, your cluster’s up and running, but we’re going to take you through some demos of CockroachDB’s features that work really well on a cloud provider like DigitalOcean. We’ve already seen how easy it is to scale a deployment by simply adding servers, but we’ll also cover: Data Distribution Survivability Data Distribution In a traditional RDBMS, you achieve scale by sharding your deployment, which splits a table into contiguous sets of rows and then stores those rows on separate servers. However, keeping a distribution of data in a sharded deployment is an enormous burden for your application, organization, and overworked DBA. Because we want to make operating a database as simple as possible, CockroachDB handles that kind of distribution for you without any kind of complicated configuration or additional settings. When you write data to any node in CockroachDB, it’s simply available to the rest of the nodes. Let’s demonstrate how that works by generating CockroachDB’s example data on one node and then viewing it from another node. From your first node, generate the example data (a database called startrek with two tables, episodes and quotes ): $ doctl compute ssh <node1 ID>\n$ cockroach gen example-data | cockroach sql --insecure Find out how much data was written to your node: $ cockroach sql --insecure --execute=\"SELECT COUNT(*) FROM startrek.episodes; \\\nSELECT COUNT(*) FROM startrek.quotes;\" You’ll see that episodes contains 79 rows and quotes contains 200. Terminate the connection with the first node ( CTRL + D ), and then connect to the second node: $ doctl compute ssh <node2 ID> Run the same SQL query to see how much data is stored in the two example tables: $ cockroach sql --insecure --execute=\"SELECT COUNT(*) FROM startrek.episodes; \\\nSELECT COUNT(*) FROM startrek.quotes;\" Again, 79 rows in episodes and 200 in quotes . Even though you generated the example data on another node, it’s been distributed and is accessible from all of your other servers. Best of all, you didn’t have to configure the sharding patterns or do much of anything for it to work. Survivability Now that you have a cluster up and running with data distributed between all of your nodes, what happens when one of the nodes dies? The TL;DR: Not a whole lot! By default, CockroachDB includes three-way replication, which means that even if one node goes down, your cluster still has two other copies of it. This allows your database to make forward progress and your application to remain blissfully unaware. To demonstrate this, we’ll remove a node from the cluster and show that all of the cluster’s data is still available. We’ll then rejoin the node to the cluster and see that it receives all updates that happened while it was offline. Assuming you’re still connected to node2, quit running CockroachDB: $ cockroach quit --insecure Now close this session ( CTRL+D ) and move to node3 : $ doctl compute ssh <node3 ID> Delete all of the quotes where the episode is greater than 50: $ cockroach sql --insecure --execute=\"DELETE FROM startrek.quotes WHERE episode > 50; \\\nSELECT COUNT(*) FROM startrek.quotes;\" You’ll see there are now 131 rows of data. Close the connection with node3 , and then move back to node2 : $ doctl compute ssh <node3 ID> Restart the node: cockroach start --insecure --background   \\\n--advertise-host=<node2 IP address>  \\\n--join=<node1 IP address>:26257 Count the number of rows available on the cluster: $ cockroach sql --insecure --execute=\"SELECT COUNT(*) FROM startrek.quotes;\" 131! So, despite being offline when the update happened, the node is updated as soon as it rejoins the cluster. If you’d like, you can now remove the example data: $ cockroach sql --insecure --execute=\"DROP TABLE quotes; DROP TABLE episodes; DROP DATABASE startrek;\" What’s next? Now that you’ve seen how easy it is to get CockroachDB up and running on DigitalOcean (and demoed the core features), you can kick it up a notch and try your hand at deploying CockroachDB on DigitalOcean with SSL encryption .", "date": "2016-11-30"},
{"website": "CockroachLabs", "title": "Testing Random, Valid SQL in CockroachDB", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/testing-random-valid-sql-in-cockroachdb/", "abstract": "Some months ago I started work on a way to test random SQL statements with CockroachDB. This is important to expose unintended behavior in our server. For example, we want to prevent valid SQL statements from unexpectedly crashing a server or using all of its CPU or memory. We have already performed some small-scale fuzz testing, but fuzz testing often produces un-parseable input since it modifies bytes (although some fuzzers like AFL do attempt to produce clean input). The goal here was to produce valid SQL statements that the parser would accept and the system would then execute. These statements would essentially attempt to try various combinations of valid SQL to panic or otherwise render the system unusable (like consuming all CPU). Generation There are a few ( sqllogictest , sqlsmith ) programs that generate valid random SQL. CockroachDB has used sqllogictest for many tests, but as our SQL semantics differ a bit from the other SQL implementations, the test results increasingly differ from our outputs, and so its usefulness has decreased. The sqlsmith program generates more random SQL, but CockroachDB is a young database and as such we don’t yet support some of the more obscure corners of SQL. In addition, we support some additional statements that aren’t generated by this tool. Modifying sqlsmith to produce CockroachDB grammar is possible, but would be an ongoing, manual process. CockroachDB’s SQL grammar is defined in a YACC grammar . YACC is a language that generates parsers from a grammar. A YACC grammar contains instructions based on input. Any unrecognized input is a parse error. Thus, a YACC grammar can enumerate all possible inputs. We already have a YACC parser . With that we were able to convert the YACC grammar into an AST , which allows for programmatic inspection of the YACC grammar itself. With the AST in memory [1] , a program was created that would take some top-level statement and choose one valid possibility for it to become, then repeat itself until there was nothing to change. This works by looking for non-terminals in the grammar and replacing them with any branch the non-terminal represents. For example, say we start with a grant statement . The non-terminals are privileges , privilege_target , and grantee_list . We can replace privileges with one of its possibilities : ALL , or privilege_list . One of those is randomly chosen, and the loop continues. Some tokens can loop back into themselves, so there is a restriction to prevent too many replacements before giving up. The statement is done when there are no tokens left, only literals (keywords, punctuation), identifiers (table or database names), or things like string or float constants. An example that’s a simplification of this might have this progression: stmt grant_stmt GRANT privileges ON privilege_target TO grantee_list GRANT ALL ON TABLE table_pattern_list TO name GRANT ALL ON TABLE table_pattern TO unreserved_keyword GRANT ALL ON TABLE * TO COVERING In the end we get a weird variety of statements: ALTER INDEX CHAR @ YEAR RENAME TO ident DEALLOCATE CHARACTERISTICS SAVEPOINT SAVEPOINT NEXT COPY ROLLUP ( ) FROM STDIN ALTER INDEX TIME @ TIMESTAMP RENAME TO SIMPLE START TRANSACTION PRIORITY HIGH , ISOLATION LEVEL READ COMMITTED GRANT DROP , DROP , DROP ON BYTES . REAL , * TO RENAME , ident , ident , ident REVOKE DELETE ON TABLE ZONE , SMALLINT . NULLS FROM TEXT ALTER TABLE ONLY ( EXTRACT_DURATION . COLLATION . * . * . * . * ) ALTER COLUMN COPY TRUNCATE TABLE ONLY ( NUMERIC ) , ONLY ( TIMESTAMP . TRIM ) , ONLY COALESCE CASCADE ALTER TABLE IF EXISTS BIT . * * RENAME CONSTRAINT UPSERT TO BYTES A separate function generates random valid function calls by inspecting our builtin function list and generating random arguments for them based on the type of argument. Results This technique found over a dozen unique panics , many of which were various uses of .* where a specific table name instead of a * was expected. The discovery of these panics led to a large refactor , where again, these tests found a small bug. By running many queries at once, a race condition was found where a lock was misused. While testing functions we were able to consume all memory or CPU by passing very high values into some math and string generation functions. Last week we added some new SQL syntax and another panic was found. These tests have found enough bugs that we now run them every night to proactively prevent new problems from happening. Since the tests are generated from our YACC grammar, as we continue to add new SQL syntax they will automatically incorporate the new features into the random testing. By continuing to run these tests nightly, our intent is to eliminate unexpected crashes and hangs in CockroachDB. Does building distributed SQL systems from the ground up put a spring in your step? Then good news — we're hiring! Check out our open positions here . [1] There are other approaches here that also would have worked to produce the same result. For example, we could use guru to find concrete types that implement the Statement interface and fill in its fields with a similar method. At the end we can produce a string with its String() method. This has an added benefit that we do not depend on YACC at all and could thus write a custom parser. Also, it avoids some annoying cases where an invalid statement is generated with the YACC method, for example when the parser validates that an interval is parseable . (Also posted at Matt Jibson's blog .)", "date": "2016-10-19"},
{"website": "CockroachLabs", "title": "How We're Building a Business to Last", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/how-were-building-a-business-to-last/", "abstract": "CockroachDB was inspired by frustration with the available open source databases and cloud DBaaS offerings. It was never conceived of as anything but open source software. In late 2014, with encouraging interest from the GitHub community and concomitant inquiries from some forward-looking venture capitalists, it was decision time: should we start a company to accelerate CockroachDB development? On the one hand, hiring a team of exceptional people would lead more quickly to a viable product. On the other hand, our goal would no longer be solely about building the next great open source database. It would necessarily expand to include concern for our employees and investors. We were faced with the difficult question of how to build a business around open source software. Building a Business Around Open Source Software There has been constant evolution of open source software business models since RedHat blazed the first trail. Few have succeeded using RedHat’s original model centered on support and services. In fact, most investors consider that early open source business model a losing proposition. The two common OSS business model alternatives are: Open Core. This typically involves a capable core product which is free and open source, often licensed with APL, MIT, or GPL. That’s the core. Around the core, a commercial entity provides a constellation of proprietary software, adding to or extending its capabilities. These proprietary add-ons are sold as commercial software, often bundled with support and services. Cloud-hosted services using open source software. Often, this also involves proprietary software (e.g. multi-tenancy, billing, service dashboards), but the end product is sold as a service instead of software. These two models are being successfully pursued by many companies. Cloudera, Elastic, and Confluent are three examples I like, all with different models, and at different stages of turning open source products into successful businesses. Cautionary Tales The landscape also contains cautionary examples. Some OSS companies set the bar too low for paid features, making the core OSS product feel “hobbled”. In 2017, any product whose core capabilities cannot scale without requiring a commercial license is probably setting the bar too low. There are also examples of companies which failed to provide enough proprietary value early on, while the open core was fast becoming a standard piece of infrastructure. Large corporations which saw value in the core product, and who in other circumstances would have been happy to pay for improvements had no choice but to build their own custom extensions. There are some excellent open source software companies which have ceased to develop their products for lack of revenue. Some quite recently, including RethinkDB . Companies which previously were more liberal with the capabilities that by default went into the open core have decided to be more discerning in the interests of viability (see Paul Dix’s InfluxDB post ). It’s a delicate balancing act. Building paid “enterprise” features for open source software can feel dirty. Paid features diminish the open source appeal and can lead to substantial community angst. On the other hand, it’s disheartening to see mammoth cloud service providers repackaging OSS for substantial gain without finding ways to foster the open source ecosystem, or hundred-billion-dollar multinationals foregoing support licenses from struggling OSS companies. If you’re serious about building a company around open source software, you must walk a narrow path: introduce paid features too soon, and risk curtailing adoption. Introduce paid features too late, and risk encouraging economic free riders. Stray too far in either direction, and your efforts will ultimately continue only as unpaid open source contributions. So, how will CockroachDB make money? I believe ultimately we’ll embrace both the cloud-hosted model and the open core model. Demand for DBaaS is evolving quickly, and only the first chapter has been written (spoiler alert: AWS is winning). But for the immediate future, our product is better aligned with companies which intend to run the database themselves, either in a public or private cloud. In other words, we’re pursuing the open core model, though with some interesting Cockroach Labs peculiarities. First, licensing. Many companies which have embraced the open core model implement their proprietary features as closed source extensions. Others ship two or more products, with enterprise versions containing closed source and distributed as compiled binaries. There are significant drawbacks to these models. They’re difficult to upgrade to, they often involve multiple development branches which are frustrating to manage, and they obviate benefits of open source where the new features are concerned: outside developers can’t debug or customize proprietary parts of the product. The CockroachDB Community License (CCL) We’re going to provide paid, enterprise features differently. Everything in our GitHub is currently licensed under the terms of the Apache License 2 (APL). Enterprise features we introduce will be contained in source files covered by a new license, called the CockroachDB Community License (CCL). The source code will still be available, but because it does not include the free redistribution right, it’s not open source by definition. Its intent is to ensure that commercial usage of enterprise features, beyond an evaluation period, is paid. These features will not be on by default, will be clearly marked in documentation, code, and help messages, and will be enabled only by operator or developer choice. The binary which we distribute will contain these features, but as a result, cannot be distributed under a FLOSS license. However, a “pure” FLOSS distribution will also be available, with enterprise features absent, for those that require it. Because the source code is available for all features covered by the CCL license, we expect others to learn from what we’re building, and one day to build better products. We expect our customers to customize the software to accommodate their own ambitions. How will we decide which features are covered by the CCL license? This is a difficult question, and ultimately the crux of the balancing act. We have distilled the choice to a litmus test: features necessary for a startup to succeed will be APL, and part of the open core; a feature which is primarily useful only to an already successful company will be CCL, and part of the enterprise product. Which license is chosen for a new feature will be determined by our intuition and community feedback. However, because such decisions are subjective, they will evolve over time. It is simple to move an enterprise feature from the CCL to APL, and we expect that to happen as a matter of course for any feature which turns out to be in high demand from startups. What does a startup need from a database in order to succeed in 2017? Every one of the features which motivated the design of CockroachDB: Cross-datacenter deployments and consistent replication to overcome failure disasters (e.g. downtime and lost or inconsistent data). Horizontal scalability and a cloud-native design to future-proof the data architecture. A SQL API with distributed ACID transactions and query execution for developer productivity. While some of the above features are considered enterprise in other databases, we believe they comprise a generationally-appropriate foundation for building products and services and they will remain free and available under the APL. These are, after all, the features which define CockroachDB. So what doesn’t a startup need to succeed, but an established company would consider an important requirement, or even a game-changing enabler of new use cases? We have two such offerings planned for 2017. The first is a fully-distributed, incremental capability for quickly and consistently backing up and restoring large databases using configurable storage sinks (e.g. S3 or GCS). The same functionality, but non-distributed , will be available for free to all users. The second is geo-partitioning, a mechanism for row-level control of how and where data is replicated. Geo-partitioning allows a single, logical database to provide low-latency access for geographically disparate customers, as well as enabling compliance with data sovereignty requirements. Building CockroachDB has been a two-plus year labor of love, and we’re now approaching our version 1.0 release. We recognize the challenge inherent in building a new database with these capabilities, and we’re trying to ensure we can continue developing CockroachDB, for as long as there’s a better product to release in the next version. Share your thoughts on the HackerNews thread.", "date": "2017-01-20"},
{"website": "CockroachLabs", "title": "[community post] Flowable and CockroachDB", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/community-post-running-flowable-cockroachdb/", "abstract": "Software engineer and tech blogger Joram Barrez discusses Flowable and CockroachDB, and how to best use the two together. Check out his tutorial below: CockroachDB is a project I’ve been keeping an eye on for a while. It’s a an open-source, Apache 2 licensed, database system( Github link ). At it’s core its a key-value store that scales horizontally. But what makes it really interesting for us though, is that 1) it supports SQL by using the Postgres wire protocol and 2) has full ACID semantics and distributed transactions. If you’re interested in how they achieve this, make sure to read the technical posts at the CockroachLabs blog (I admit, sometimes it’s not for the faint-of-heart ;-)). Do note that it is still a distributed system and thus follows the CAP theorem, more specifically it is a CP system. It’s still early days, as you’ll read in their FAQ, as many things are not optimized yet. However, now that they recently added basic support for joins , I figured I should give it a spin with the Flowable engine. In this post I’ll show how easy it is to run the Flowable v6 process engine on CockroachDB. (Sidenote: I love the name! For people that don’t understand it: cockroaches are one of the few creatures on earth that can survive something like a nuclear blast. Quite a resilient little animal … something you’d like for your data too) Read the full post here: Running Flowable on CockroachDB", "date": "2016-12-07"},
{"website": "CockroachLabs", "title": "Enriching Log Messages Using Go Contexts", "author": ["Radu Berinde"], "link": "https://www.cockroachlabs.com/blog/enriching-log-messages-using-go-contexts/", "abstract": "Building a complex system requires writing and debugging many tests. Developer tests that can be run without any infrastructure or configuration are crucial for allowing fast-paced development while avoiding regressions. Even for a distributed system like CockroachDB, tests that run on a single anode (e.g. as part of go test ) can cover many aspects of the system: one technique we use is that of creating a virtual test cluster with multiple CockroachDB nodes, all running within the same process. We typically use log messages to understand what is happening as part of a test and to identify problems. This blog post goes over how we ensure that the log messages contain enough information to be useful without encumbering the development of new code. Log tags Let’s take an example of a log message: log.Infof(ctx, \"streamed snapshot: kv pairs: %d, log entries: %d\", n, len(logEntries)) This message indicates that we transferred a streaming snapshot . By itself, the information in this message is unlikely to be useful during debugging: we can have many ranges in multiple stores, so we need to identify which range and store this message pertains to. The immediate solution is to simply add information about the node, store, and range to the message itself. However, this solution has several problems: It is tedious and repetitive: we have to include extra information in every message. It leads to inconsistent formatting for the same information – as an example, at some point all of the following were in use among our log messages: range=5, r5, RangeID=5, RangeID:5 . Every layer needs to have the information for all of the above layers (for example, the code issuing this message needs to know the store and the node). The determination of which layers to show information for is fixed. If we decide to add another layer – for example, imagine we want to run a test with multiple clusters and we want each log message to indicate which test cluster it pertains to – we need to go through all layers and manually add the extra information. To address these shortcomings, we decided to pass the information that we want to associate with log messages through a context.Context . Note that plumbing Contexts throughout our code is necessary regardless in order to be able to correlate operations spanning multiple nodes ( distributed tracing ). So the solution is to embed a chain of logging tags into Contexts (via an API that appends a log tag to a context) and to always pass a context when logging a message; the logging library inspects the tags embedded in the context and includes them in the message. Example: ctx := context.Background()\nlog.Infof(ctx, “test”)    // Message: “test”\nctx = log.WithLogTag(ctx, “n”, 1)\nlog.Infof(ctx, “test”)    // Message: “[n1] test”\nctx = log.WithLogTag(ctx, “s”, 2)\nlog.Infof(ctx, “test”)    // Message: “[n1,s2] test”\nctx = log.WithLogTag(ctx, “bootstrap”, nil)\nlog.Infof(ctx, “test”)    // Message: “[n1,s2,bootstrap] test” In some cases we want the value of a tag to be dynamic. For example, the node ID might not be known during early initialization time; or, the start and end keys that define a range might change over time. These cases are supported by passing an object as the tag value; the object is stringified every time we issue a message, allowing publishing of the up-to-date value via a String() function. Here is how our example log shows up with the proper log tags: I161119 08:37:36.515739 3313 storage/store.go:3134  [n1,s1,r8/1:/Table/51/1/{700-800}] streamed snapshot: kv pairs: 112, log entries: 9 We know that this event happened on node 1, store 1, range 8, replica 1 and the range holds keys between /Table/51/1/700 and /Table/51/1/800 . Note that the format of this information is intentionally concise because it appears in almost every log message. Initial solution: background contexts We want each layer to inherit the logging tags of the parent layer, and possibly expand them. Our initial solution was to equip each layer with a “background context”. The Node has a background context which contains the node ID log tag (in the example above, n1). When creating a Store (which conceptually is a “child” of a node), the node passes its context and the store adds a store log tag (s1) to create its background context. Similarly, the Replica background context is derived from the store context, with the addition of the range and replica log tag ( r8/1:/Table/51/1/{700-800} ). These contexts can be used directly for periodic background operations associated with each layer. But most operations – for example those that are part of a SQL statement – have a context.Context of their own. We want to use the operation context wherever possible (to support cancellation and tracing), but we want to also have the log tags of the proper layer. To do this, we have a WithLogTagsFromCtx function which can copy the log tags from a background context to an operation context (skipping any duplicate tags). Most layers settled on the pattern of providing a logContext wrapper and using it in functions receiving an outside context: // logContext adds the node, store and replica log tags to a context. Used to\n// personalize an operation context with this Replica's identity.\nfunc (r *Replica) logContext(ctx context.Context) context.Context {\n    // Copy the log tags from the base context. This allows us to opaquely\n    // set the log tags that were passed by the upper layers.\n    return log.WithLogTagsFromCtx(ctx, r.ctx)\n}\n\nfunc (s *Store) Send(ctx context.Context, ...) ... {\n    // Attach any log tags from the store to the context (which normally\n    // comes from gRPC).\n    ctx = s.logContext(ctx)\n    ... A cleaner solution: the AmbientContext The solution outlined above works, but it has some disadvantages that became nagging in time. A lot of code was switched to directly use long lived background contexts (like r.ctx ); code which was lacking correct plumbing of operation contexts used to be implicitly marked by the use of context.TODO() but now looked like it has a “legitimate” context. When reading code, it became hard to track whether the context passed to a function was coming from an operation context or from a background context. The proliferation of background contexts was also problematic for tracing: logging using a background context does not generate any tracing events since the context is not associated with any particular operation. This culminated in a proposal for a new solution. The idea was to standardize an interface similar to logContext and rely solely on that interface, avoiding storing and using long-lived background contexts directly. The implementation involves a new structure called AmbientContext which stores log tags and can be used to add its tags to a Context . Instead of storing a background context.Context in each layer, we now store an AmbientContext . Similar to before, each layer is created using the parent layer’s AmbientContext onto which new log tags are added. When annotating an operation context, AmbientContext.AnnotateCtx() can be used just like logContext above (note that AmbientContext is embedded into the Store structure): func (s *Store) Send(ctx context.Context, ...) ... {\n    // Attach any log tags from the store to the context (which normally\n    // comes from gRPC).\n    ctx = s.AnnotateCtx(ctx)\n    ... When a context is used in a function for which an operation context is not plumbed, we can use AnnotateCtx(context.TODO()) . This is effectively equivalent to the previous solution where we used the background context directly; however in the code we now have an explicit indication of the fact that we are not using a proper operation context. AmbientContext also has a variant which creates a tracing span , which is a container for a set of events related to an operation and allows correlation of events happening on multiple nodes (as part of that same operation). This is appropriate for periodic background operations: ctx, span := p.AnnotateCtxWithSpan(context.Background(), \"ts-poll\")\ndefer span.Finish()\nif err := p.db.StoreData(ctx, p.r, data); err != nil {\n    ... This code creates a tracing span with operation name “ts-poll” and an associated context. Events associated with the StoreData operation are recorded within this trace; in particular all log messages (like log.Infof in our example) used with ctx will be recorded as events in this trace. In most cases of background operations, creating the span is the right thing to do, as it enables visualisation of events in a tool like LightStep . This is the biggest advantage of AmbientContext : whereas in the old code we were content with directly using something like p.ctx, now we have to make an explicit decision to create a context and possibly a tracing span. Indeed, while refactoring code to use the new model, many internal operations were corrected to use tracing properly. Using enriched log messages We found that using enriched log messages has helped our debugging efforts, although the only metrics I have are in measures of sanity. If you ever struggled with log messages that lacked context, give this model a try in your project! Share your thoughts on the HackerNews thread .", "date": "2016-12-15"},
{"website": "CockroachLabs", "title": "CockroachDB Stability Post-Mortem: From 1 Node to 100 Nodes", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-stability-from-1-node-to-100-nodes/", "abstract": "In August, we published a blog post entitled “ Why Can’t I Run a 100-Node CockroachDB Cluster? ”. The post outlined difficulties we encountered stabilizing CockroachDB. CockroachDB stability (or the lack of) had become significant enough that we designated it a “code yellow” issue, a concept borrowed from Google that means a problem is so pressing that it merits promotion to a primary concern of the company. For us, the code yellow was more than warranted; a database program isn’t worth the bytes to store its binary if it lacks stability. In this post, I’ll set the stage with some background, then cover hypotheses for root causes of instability, our communication strategy, some interesting technical details, outcomes for stabilization efforts, and conclusions. It’s a long post, so bear with me! TL;DR: We achieved most of our stability goal. While we’re still working on some of the chaos scenarios, the system is easily stable at many more than 10 node clusters – we’ve tested it successfully at 100 nodes . Background To better set the stage: we announced the CockroachDB Beta release in April, after more than a year of development. Over the five months of progress on the beta, concerns over correctness, performance, and the general push for new features dominated our focus. We incorrectly assumed stability would be an emergent property of forward progress, just as long as everyone was paying some attention to, and fixing stability bugs whenever they were encountered. But by August, despite a team of 20 developers, we couldn’t stand up a 10-node cluster for two weeks without major performance and stability issues. Nothing could more effectively convey the gulf that had opened up between our stability expectations and reality than the increasingly frequent mentions of instability as a punchline in the office. Despite feeling that we were just one or two pull requests away from stability, the inevitable chuckle nevertheless morphed into a insidious critique. This blog post chronicles our journey to establish a baseline of stability for CockroachDB. Hypotheses on Root Causes What caused instability? Obviously there were technical oversights and unexpectedly complex interactions between system components. A better question is: what was preventing us from achieving stability? Perhaps surprisingly, our hypotheses came down to a mix of mostly process and management failures, not engineering. We identified three root causes: The rapid pace of development was obscuring, or contributing to, instability faster than solutions could be developed. Imagine a delicate surgery with an excessive amount of blood welling up in the incision. You must stop the bleeding first in order to operate. This analogy suggested we’d need to work on stability fixes in isolation from normal development. We accomplished this by splitting our master branch into two branches: the master branch would be dedicated to stability, freezing with the exception of pull requests targeting stability. All other development would continue in a develop branch. While many engineers were paying attention to stability, there was no focused team and no clear leader. Imagine many cooks in the kitchen working independently on the same dish, without anyone responsible for the result tasting right. To complicate matters, imagine many of the chefs making experimental contributions… Time to designate a head chef. We chose Peter Mattis, one of our co-founders. He leads engineering and is particularly good at diagnosing and fixing complex systems, and so was an obvious choice. Instead of his previously diffuse set of goals to develop myriad functionality and review significant amounts of code, we agreed that he would largely limit his focus to stability and become less available for other duties. The key objective here was to enable focus and establish accountability. Instability was localized in a core set of components, which were undergoing too many disparate changes for anyone to fully understand and review. Perhaps a smaller team could apply more scrutiny to fewer, careful changes and achieve what had eluded a larger team. We downsized the team working on core components (the transactional, distributed key-value store), composed of five engineers with the most familiarity with that part of the codebase. We even changed seating arrangements, which felt dangerous and counter-cultural, as normally we randomly distribute engineers so that project teams naturally resist balkanization. Communication The decision to do something about stability happened quickly. I’d come home from an August vacation blithely assuming instability a solved problem. Unfortunately, new and seemingly worse problems had cropped up. This finally provided enough perspective to galvanize us into action. Our engineering management team discussed the problem in earnest, considered likely causes, and laid out a course of action over the course of a weekend. To proceed, we had to communicate the decisions internally to the team at Cockroach Labs, and after some soul searching, externally to the community at large. One of our values at Cockroach Labs is transparency. Internally, we are open about our stability goals and our successes or failures to meet them. But just being transparent about a problem isn’t enough; where we fell down was in being honest with ourselves about the magnitude of the problem and what it meant for the company. Once decided, we drafted a detailed email announcing the code yellow to the team. Where we succeeded was in clearly defining the problem and risks, actions to be taken, and most importantly: code yellow exit criteria. Exit criteria must be measurable and achievable! We decided on “a 10-node cluster running for two weeks under chaos conditions without data loss or unexpected downtime.” Where we didn’t succeed was in how precipitous the decision and communication seemed to some members of the team. We received feedback that the decision lacked sufficient deliberation and the implementation felt “railroaded”. We didn’t decide immediately to communicate the code yellow externally, although consensus quickly formed around the necessity. For one thing, we’re building an open source project and we make an effort to use Gitter instead of Slack for engineering discussions, so the community at large can participate. It would be a step backwards to withhold this important change in focus. For another thing, the community surely was aware of our stability problems and this was an opportunity to clarify and set expectations. Nevertheless, the task of actually writing a blog post to announce the stability code yellow wasn’t easy and wasn’t free of misgivings. Raise your hand if you like airing your problems in public… Unsurprisingly, there was criticism from Hacker News commentators , but there were also supportive voices. In the end, maintaining community transparency was the right decision and we hope established trust. Technical Details With changes to process and team structure decided, and the necessary communication undertaken, we embarked on an intense drive to address the factors contributing to instability in priority order. We of course had no idea how long this would take. Anywhere from one to three months was the general consensus. In the end, we achieved our code yellow exit criteria in five weeks. What did we fix? Well, instability appeared in various guises, including clusters slowing precipitously or deadlocking, out-of-memory panics (OOMs), and data corruption (detected via periodic replica checksum comparisons). Rebalancing via Snapshots The generation and communication of replica snapshots, used to rebalance and repair data in a CockroachDB cluster, was our most persistent adversary in the battle for stability. Snapshots use significant disk and network IO, and mechanisms that limit their memory consumption and processing time while holding important locks were originally considered unnecessary for beta stability. Much of the work to tame snapshots occurred during the months leading up to the stability code yellow, which hints at their significance. Over the course of addressing snapshots, we reduced their memory usage with streaming RPCs, and made structural changes to avoid holding important locks during generation. However, the true cause of snapshot instability proved to be a trivial oversight, but it was simply not visible through the fog of cluster stabilization – at least, not until after we’d mostly eliminated the obvious symptoms of snapshot badness. Snapshots are used by nodes to replicate information to other nodes for repair (if a node is lost), or rebalancing (to spread load evenly between nodes in a cluster). Rebalancing is accomplished with a straightforward algorithm: 1. Nodes periodically advertise the number of range replicas they maintain. 2. Each node computes the mean replica count across all nodes, and decides: If a node is underfull compared to the mean, it does nothing If overfull, it rebalances via snapshot to an underfull node Our error was in making this judgement too literally, without applying enough of a threshold around the mean in order to avoid “thrashing”. See the animated diagram below which shows two scenarios. Exact Mean Threshold of Mean Simulation 1. In the left \"Exact Mean\" simulation, we rebalance to within a replica of the mean; this will never stop rebalancing. Notice that far more RPCs are sent and the simulation never reaches equilibrium. In the right \"Threshold of Mean\" simulation, we rebalance to within a threshold of the mean, which quickly reaches equilibrium. In practice, continuously rebalancing crowded out other, more salient, work being done in the cluster. Lock Refactoring Tracing tools were invaluable in diagnosing lock-contention as a cause of excessively slow or deadlocked clusters. Most of these symptoms were caused by holding common locks during processing steps which could sometimes take an order of magnitude longer than originally supposed. Pileups over common locks resulted in RPC traffic jams and excessive client latencies. The solution was lock refactoring. Locks held during Raft processing, in particular, proved problematic as commands for ranges were executed serially, holding a single lock per range. This limited parallelization and caused egregious contention for long-running commands, notably replica snapshot generation. Garbage collection of replica data after rebalancing was previously protected by a common lock in order to avoid tricky consistency issues. Replica GC work is time consuming and impractical to do while holding a per-node lock covering actions on all stores. In both cases, the expedient solution of coarse-grained locking proved inadequate and required refactoring. Tracing Tools Ironically, the same tracing tools used to diagnose degenerate locking behavior were themselves stability culprits. Our internal tracing tools were pedantically storing complete dumps of KV and Raft commands while those spans were held in a trace’s ring buffer. This was fine for small commands, but quickly caused Out-of-Memory (OOM) errors for larger commands, especially pre-streaming snapshots. A silver lining to our various OOM-related difficulties was development of fine-grained memory consumption metrics, tight integration with Go and C++ heap profiling tools, and integration with Lightstep , a distributed tracing system inspired by Google’s Dapper. Corruption! OOMs and deadlocks are often diagnosed and fixed through honest labor that pays an honest wage. What keeps us up at night are seemingly impossible corruption errors. Some of these occur between replicas (i.e. replicas don’t agree on a common checksum of their contents). Others are visible when system invariants are broken. These kinds of problems have been rare, though we found one during our stability code yellow. CockroachDB uses a bi-level index to access data in the system. The first level lives on a special bootstrap range, advertised via gossip to all nodes. It contains addressing information for the second level, which lives on an arbitrary number of subsequent ranges. The second level, finally, contains addressing information for the actual system data, which lives on the remaining ranges. Addressing records are updated when ranges split and are rebalanced or repaired. They are updated like any other data in the system, using distributed transactions, and should always be consistent. However, a second level index addressing record went unexpectedly missing. Luckily, Ben Darnell, our resident coding Sherlock Holmes, was able to theorize a gap in our model which could account for the problem, despite requiring an obscure and unlikely sequence of events, and perfect timing.  It’s amazing what a brilliant engineer can intuit from code inspection alone. Also, there ought to be a maxim that in a sufficiently large distributed system, anything that can happen, will happen. Raft Last, and certainly not least, we waged an epic struggle to tame Raft, our distributed consensus algorithm. In a resonant theme of these technical explanations, we had originally concluded that improvements to Raft that were on the drawing board could wait until after our general availability release. They were seen as necessary for much larger clusters, while the Raft algorithm’s impedance mismatch with CockroachDB’s architecture could simply be ignored for the time being. This proved a faulty assumption. Impedance mismatch? Yes, it turns out that Raft is a very busy protocol and typically suited to applications where only a small number of distinct instances, or “Raft groups”, are required. However, CockroachDB maintains a Raft group per range, and a large cluster will have hundreds of thousands or millions of ranges. Each Raft group elects a leader to coordinate updates, and the leader engages in periodic heartbeats to followers. If a heartbeat is missed, followers elect a new leader. For a large CockroachDB cluster, this meant a huge amount of heartbeat traffic, proportional to the total number of ranges in the system, not just ranges being actively read or written, and it was causing massive amounts of network traffic. This, in conjunction with lock contention and snapshots, would cause chain reactions. For example, too many heartbeats would fill network queues causing heartbeats to be missed, leading to reelection storms, thus bringing overall progress to a halt or causing node panics due to unconstrained memory usage. We had to fix this dynamic. We undertook two significant changes. The first was lazy initialization of Raft groups. Previously, we’d cycle through every replica contained on a node at startup time, causing each to participate in their respective Raft groups as followers. Being lazy dramatically eased communication load on node startup. However, being lazy isn’t free: Raft groups require more time to respond to the first read or write request if they’re still “cold”, leading to higher latency variance. Still, the benefits outweighed that cost. The success of lazy initialization led to a further insight: if Raft groups didn’t need to be active immediately after startup, why couldn’t they simply be decommissioned after use? We called this process “quiescence”, and applied it to Raft groups where all participants were fully replicated with no pending traffic remaining. The final heartbeat to the Raft group contains a special flag, telling participants to quiesce instead of being ready to campaign for a new leader if the leader fails further heartbeats. Naive Raft Quiescing Raft Simulation 2. In the left \"Naive Raft\" simulation, notice near constant sequence of heartbeats, denoted by the red RPCs between Raft groups. These are constant despite the slow trickle of writes from applications. In the right \"Quiescing Raft\" simulation, the Raft heartbeats occur only in order to quiesce after write traffic. In addition to other changes, such as Raft batching, we managed to meaningfully reduce background traffic. By doing so, we also directly contributed to another key product goal, to constrain network, disk, and CPU usage to be directly proportional to the amount of data being read or written , and never proportional to the total size of data stored in the cluster. Outcomes How did our process and management initiatives fare in addressing the three hypothesized root causes? Working With Two Branches Splitting the master branch was not without costs. It added significant overhead in near-daily merges from the master branch to develop in order to avoid conflicts and maintain compatibility with stability fixes. We effectively excluded changes to “core” packages from the develop branch in order to avoid a massive merge down the road. This held up some developer efforts, refactorings in particular, making it unpopular. In particular, Tamir Duberstein was a martyr for the stability cause, suffering the daily merge from master to develop at first quietly, and then with mounting frustration. Was the split branch necessary? A look at the data suggests not. There was significant churn in the develop branch, which counted 300 more commits during the split branch epoch. Despite that, there was no regression in stability when the branches were merged. We suspect that the successful merge is more the result of limits on changes to core components than to the split branches. While there is probably a psychological benefit to working in isolation on a stability branch, nobody is now arguing that was a crucial factor. The CockroachDB Stability Team Designating a team with stability as the specific focus, and putting a single person in charge, proved invaluable. In our case, we drafted very experienced engineers, which may have led to a productivity hit in other areas. Since this was temporary, it was easy to justify given the severity of the problem. Relocating team members for closer proximity felt like it meaningfully increased focus and productivity when we started. However, we ended up conducting a natural experiment on the efficacy of proximity. First two, and then three, out of the five stability team members ended up working remotely. Despite the increasing ratio of remote engineers, we did not notice an adverse impact on execution. What ended up being more important than proximity were daily “stability sync” stand ups. These served as the backbone for coordination, and required only 30 minutes each morning. The agenda is (and remains): 1) status of each test cluster; 2) who’s working on what; 3) group discussion on clearing any blocking issues. We also held a twice-weekly “stability war room” and pressed any and all interested engineers into the role of “production monkey” each week. A production monkey is an engineer dedicated to overseeing production deployments and monitoring. Many contributions came from beyond the stability team, and the war rooms were a central point of coordination for the larger engineering org. Everyone pitching in with production duties raised awareness and familiarized engineers with deployment and debugging tools. Fewer People, More Scrutiny A smaller team with a mandate for greater scrutiny was a crucial success factor. In a testament to that, the structure has become more or less permanent. An analogy for achieving stability and then maintaining it is to imagine swimming in the ocean at night with little sense of what’s below or in which direction the shoreline is. We were pretty sure we weren’t far from a spot we could put our feet down and stop swimming, but every time we tried, we couldn’t touch bottom. Now that we’ve finally found a stable place, we can proceed with confidence; if we step off into nothingness, we can swim back a pace to reassess from a position of safety. We now merge non-trivial changes to core components one-at-a-time by deploying the immediately-prior SHA, verifying it over the course of several hours of load, and then deploying the non-trivial change to verify expected behavior without regressions. This process works and has proven dramatically effective. The smaller stability team instituted obsessive review and gatekeeping for changes to core components. In effect, we went from a state of significant concurrency and decentralized review to a smaller number of clearly delineated efforts and centralized review. Somewhat counter-intuitively, the smaller team saw an increase per engineer in pull request activity (see stream chart below). Stream Chart 1. The stream chart shows pull request changed code lines (additions + deletions) for major CockroachDB components in 2016. The three blue areas at bottom contain the “core” components: gossip, kv, and storage . The vertical black lines show the start and end of the code yellow branch split. From the graph, you can see that stabilization efforts saw a significant tightening in pace of pull request changes near merge time. These effects can be traced to an increase in “embargoed” pull requests affecting core components, mostly refactorings and performance improvements which were considered too risky to merge to the increasingly stable master. Conclusions on CockroachDB Stability In hindsight (and in Hacker News commentary), it seems negligent to have allowed stability to become such a pressing concern. Shouldn’t we have realized earlier that the problem wasn’t going away without changing our approach? One explanation is the analogy of the frog in the slowly heating pot of water. Working so closely with the system, day in and day out, we failed to notice how stark the contrast had become between our stability expectations pre-beta and the reality in the months that followed. There were many distractions: rapid churn in the code base, new engineers starting to contribute, and no team with stability as its primary focus. In the end, we jumped out of the pot, but not before the water had gotten pretty damn hot. Many of us at Cockroach Labs had worked previously on complex systems which took their own sweet time to stabilize. Enough, that we hold a deep-seated belief that such problems are tractable. We posited that if we stopped all other work on the system, a small group of dedicated engineers could fix stability in a matter of weeks. I can’t stress enough how powerful belief in an achievable solution can be. Could we have avoided instability? Ah, the big question, and here I’m going to use “I” instead of “we”. Hacker News commentary on my previous blog post reveals differing viewpoints. What I’m going to say next is simply conjecture as I can’t assert the counterfactual is possible, and nobody can assert that it’s impossible. However, since I’m unaware of any complex, distributed system having avoided a period of instability, I’ll weakly assert that it’s quite unlikely. So, I’ll present an argument from experience, with the clear knowledge that it’s a fallacy. Enough of a disclaimer? I’ve worked on several systems in the same mold as CockroachDB and none required less than months to stabilize. Chalk one up for personal anecdote. While I didn’t work on Spanner at Google, my understanding is that it took a long time to stabilize. I’ve heard estimates as long as 18 months. Many popular non-distributed databases, both SQL and NoSQL, open source and commercial, took years to stabilize. Chalk several up for anecdotal hearsay. While proving distributed systems correct is possible, it likely wouldn’t apply to the kinds of stability problems which have plagued CockroachDB. After all, the system worked as designed in most cases; there was emergent behavior as a result of complex interactions. I’d like to conclude with several practical suggestions for mitigating instability in future efforts. Define a less ambitious minimally viable product (MVP) and hope to suffer less emergent complexity and a smaller period of instability. Proceed from there in an incremental fashion, preventing further instability with a careful process to catch regressions. When a system is functionally complete, proceed immediately to a laser focus on stability. Form a team with an experienced technical lead, and make stability its sole focus. Resist having everyone working on stability. Clearly define accountability and ownership. Systems like CockroachDB must be tested in a real world setting. However, there is significant overhead to debugging a cluster on AWS. The cycle to develop, deploy, and debug using the cloud is very slow . An incredibly helpful intermediate step is to deploy clusters locally as part of every engineer’s normal development cycle (use multiple processes on different ports). See the allocsim and zerosum tools. Does building a distributed SQL system and untangling all of its parts sound like your ideal Tuesday morning? If so, we're hiring! Check out our open positions here .", "date": "2016-11-16"},
{"website": "CockroachLabs", "title": "How We’re Fighting Unconscious Bias", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/fighting-unconscious-bias-cockroach-labs/", "abstract": "Before meeting a candidate, your brain has already started playing some unconscious games. You get excited if you see a candidate who went to the same school as you. You get irritated if you see a company you don't like. You have already started making assumptions simply based by what is on paper, showing the power unconscious bias can have in the hiring process. Unconscious biases are social stereotypes about certain groups of people that individuals form outside their own conscious awareness. Everyone holds unconscious beliefs about various social and identity groups, and these biases stem from one’s tendency to organize social worlds by categorizing. UCSF Office for Diversity and Outreach At Cockroach Labs, we wanted to fight our own unconscious bias. We all know that diversity in tech is dismal, and we are lucky that people like Joelle Emerson and her team at Paradigm are doing something about it. Last fall, our team completed an Unconscious Bias Workshop with Paradigm. Paradigm’s belief is that, “The benefit of diverse perspectives will lead our clients to design better products, deliver better services and build a better world”. The workshop resulted not only in ongoing discussions that raised awareness at Cockroach Labs but also in action – most notably in attacking unconscious bias in our hiring process. We instituted a number of changes to our hiring process, but what I’ll be discussing in this blog post is how we removed resumes from interviews. Our belief was that we could be missing out on strong candidates because of the unconscious biases of the interviewer and the ways bias influences the outcome of an interview. How Do We Remove Resumes from the Interview Process Full disclosure: At this point, it would be close to impossible for us to entirely remove the resume from the hiring process, although I am all ears on ideas. As such, the recruiter (me) still has access to the resume, which remains the first screen in the hiring process. I review the candidate’s work experience (current and past), standard HR questions, etc. If I proceed, I remove the resume and do not share the prescreen notes with the interview slate, as these may reveal the very details I am intending to mask. Instead of sharing the resume, I include the experience level of the candidate and in cases where the candidate has a PhD, I share the focus of their thesis so the interviewer can explore their work. If the candidate is an intern, I also include the candidate’s current year in school. What Kinds of Unconscious Bias Are We Fighting by Removing Resumes? Affinity Bias The tendency to warm up to people like ourselves. With this bias, we have a tendency to ignore negative traits of people we like and focus on the faults of those we don’t [1]. Example: It’s interview day at the plant! Homer is interviewing Barney, who also loves donuts. Barney seems a bit nervous and Homer can relate. Homer doesn’t like interviews. Homer kindly offers him a donut, which he accepts. They have a great discussion about the job and his skills, talk about the local bar, etc. Next on the rotation is Ned. Since the donuts helped Barney out, Homer offers Ned one. Ned kindly says, “I try to avoid sugar, it can make me jittery”. Homer is a bit shocked: “What person doesn’t like donuts?” He continues to interview Ned, but he has a tinge of discomfort with him. “Everyone likes donuts,” Homer’s brain repeats. At the end of the interview, Homer adds feedback that he really enjoyed the interview with Barney and thinks he’s a person he could grab a beer with. And he can’t put his finger on it, but there is something a bit off with Ned even though he has the skills… and honestly, he didn’t really open up in the interview. Halo/Horn Bias The “Halo” effect is the tendency to think everything about a person is good because you like that person. The opposite of this is the “Horns”, where we see one bad thing about a person and let it cloud our opinions of their other attributes [1]. Example: Smithers is reviewing resumes for the plant. There is a giant stack and he doesn’t know how to organize them. To move quickly, he decides that he will only move forward with resumes of candidates from Springfield College with a 3.5+ GPA. Everyone else is rejected. Similar to Me Bias Picking candidates based on personal characteristics that they share with the interviewer rather than job-related criteria [1]. Example: Lisa is running late for an interview. She grabs the candidate’s resume that is sitting on her desk and glances over it quickly. The candidate most recently interned at the EPA and had a stint as a personal Vegan chef. The candidate plays the baritone saxophone and was an active member of her town’s Free Tibet Group. Lisa starts to relax a little. She knows she won’t need the whole time to interview the candidate as it seems to be a no-brainer. “The candidate is exactly like me. They can totally do this job,” she thinks to herself. Lessons Learned Prepare the Candidate We discovered it was imperative for the recruiter to prepare the candidate for this type of experience. As candidates, we are surprised when interviewers don’t ask us questions that are relatable. Some of the initial feedback was that the interview felt cold and there wasn’t the opportunity for the interviewer or the candidate to truly engage with one another. Prepping the candidate that the focus of the interview is on skills required for the job, helps prepare them for this. Without a resume, we only ask candidates about work, not their history. Candidates can steer the conversation to their strengths, and answer in their own time. Prepare the Interviewer The biggest concerns interviewers had from the get-go were about how to break the ice and put the candidate at ease. When you ask them to avoid asking questions like: Which college were you at? How’s the weather over in … ? Oh, you worked with Homer S. Do you know Lenny at the plant? All of a sudden they don’t know where to start. They walk into the room, introduce themselves, and then what? This, in fact, inspired an entire rewrite on our interview process, modeling all of our interviews – even for non-technical roles – off of applied, skills-driven exercises. However, I will save that process for another day. The Results Well, we are still testing the process out! Success does not mean eliminating all bias or achieving absolute anonymity. Resumes are still a part of the initial recruiter screen (e.g., does the candidate have an appropriate amount of experience as required for the role, etc.) Instead, we are able to challenge preconceptions that are hiding in our unconscious bias, leading to a fairer process where better decisions are made. Credit: [1] SocialTalent Blog", "date": "2017-01-26"},
{"website": "CockroachLabs", "title": "Journey to Design for Enterprise", "author": ["Kuan Luo"], "link": "https://www.cockroachlabs.com/blog/journey-to-design-for-enterprise/", "abstract": "One would think that designing for creative consumers through a massively popular brand would be a dream come true for any young designer. After all, what more could you want than a great company, compelling product , and an engaged team? A bigger challenge, it turns out. Design for Enterprise Technology I started my design career working on printed materials and corporate identities at a Brooklyn studio . Even then, I was attracted to the endless possibilities of the digital interface and changed course to immerse myself in web and mobile design at the Washington Post. Soon after, I joined a startup as the second employee to distribute indie hardware to the masses. The startup was acquired , and I learned how to thrive at a large product design organization in a public tech company. But my desire for more risk and growth in my work outweighed the the comfort of a widely accessible audience. At this point, the startup life had imprinted itself into my mind. Regardless of all the great work I was able to do at Etsy, I had the itch to go back to startups and let the unknown fuel my growth. I poked friends, peers, and designers I admired to see what they were up to (and the companies they were into). Many coffee dates followed, informal interviews were held, and companies were pitched. After all those lattes, I homed in on enterprise technology startups – and found my new place at Cockroach Labs. A New Set of Challenges It all came down to wanting a new challenge, and the decision to delve into design for enterprise technologies gave me two. First, having been consumer-focused my whole career, the idea of designing for software developers was new territory. Would I need to learn and understand the technology and development process better? Could I even begin to understand this new audience? I had no idea – and I was thrilled about it. Secondly, I was curious to learn how designing for enterprise technology would be different from consumer-focused design. With different business goals to keep in mind, I wanted to know if I could adapt my skills rather than having to learn new ones. Using Cross-Team Collaboration To embed myself firmly in the center of developer-first design is to weave my work with the rest of the team. As a team of one, I rely on my colleagues to help me reach my design vision – achieve design excellence in the world of databases. So it’s crucial that my priorities are known amongst other team leads. When multiple departments share the same success metrics, especially at startups, ownership might be blurry at first. It’s thus important to remember that in this team sport , we’re winning or losing together. Furthermore, pushing my own limits and challenging the rest of the team to aim higher and think bolder are growth opportunities where my design skills are truly tested. How can one embrace ambiguity, navigate through different pools of opinions and data, and comes up with a solution for everyone? For me, that’s true innovation. Looking to the Future Five months of trials, errors, and reflections later, I realized that though business goals are different between enterprise and consumer-facing companies, the skills a designer needs to make the startup successful are largely the same. Regardless of the product or audience type, we ultimately design a better experience for users. There are many ways to do that successfully, as I will elaborate in future blog posts, but the crux of the matter is to understand who the audience is by learning what motivates and frustrates them. With vast space to innovate and grow, it has never been a better time to get into design for enterprise technology. If you ask me, the most exciting opportunity lies the beating heart of every business in the world – databases. If you are interested in joining the design effort at Cockroach Labs, get in touch .", "date": "2017-02-02"},
{"website": "CockroachLabs", "title": "On the Way to Better SQL Joins in CockroachDB", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/better-sql-joins-in-cockroachdb/", "abstract": "Six months ago, we reported our first implementation of SQL joins in CockroachDB . At that point in development, we merely provided a functional proof of concept that was severely limited performance-wise. This is changing, gradually. We are preparing to launch CockroachDB 1.0 later this spring, and SQL joins in CockroachDB 1.0 will be usable, if underpowered . The story in the infographic above is that we are working to make SQL joins perform better than the naive code we presented in July 2016. We still have a long way to go to land in the same ball park as the more mature, enterprise-grade SQL engines, but we will get there eventually. This blog post is meant as the next chapter of our story on joins in CockroachDB. In this chapter, you will learn how join query execution works now, what you can do now which you couldn’t do before, and what you can expect in v1.0. We’ll conclude with a peek at our roadmap on SQL optimizations. Six months further down the road In summary: We have replaced our default implementation of the join operator with a hash join . This changes the asymptotic time complexity for two input tables of sizes N and M to O(N+M) in most cases instead of O(N×M) previously — a linear speedup. Image inspiration: Time Complexity and Why It’s Important We have done some preliminary work on filter optimization. This causes many simple joins to be simplified to a constant-time operation instead of a costly quadratic lookup like previously. We have some preliminary working code to distribute and parallelize the execution of joins. This can enable speedups linear in the size of the cluster for some queries. Abstract query semantics CockroachDB interprets every SQL join as a binary operation. SELECT statements using multiple joins are transformed to a tree of binary join operations that follow the syntax of the query, assuming the join operator is left-associative. That is: SELECT * FROM a, b, c, WHERE ... Is interpreted as: SELECT * FROM ((a CROSS JOIN b) CROSS JOIN c) WHERE … Which internally is processed like this: Render *\n                           |\n                          Join\n                        /     \\\n                     Join      c\n                    /  \\\n                   a    b This overall interpretation has not changed yet: CockroachDB currently processes the joins in the logical order specified by the query, and the user has complete control over the join order. (The flip side of this control, naturally, is that if the user does not think about the join order the result can be sub-efficient. More about this later.) On top of this general structure for the data operands for a SELECT, CockroachDB must then filter out rows based on the WHERE clause. Until recently , a query of the form SELECT * FROM abc WHERE abc.x > 3 AND abc.y < 2 Would be handled as: Render *\n   |\nFilter on abc.x > 3 AND abc.y < 2\n   |\n  abc Which means that in combination with joins we had the following situation: a query with the form SELECT *\n  FROM a, b, c\n WHERE a.x = b.x AND b.y = c.y AND a.x+b.y+c.z > 0 Was handled like this: Render *\n                      |\n                Filter on a.x = b.x AND b.y = c.y\n                          AND a.x+b.y+c.z > 0\n                      |\n                     Join\n                   /     \\\n                Join      c\n               /  \\\n              a    b This meant that the entire cross-join result of a CROSS JOIN b CROSS JOIN c needed to be computed before we could decide which rows would be reported to the client. This could produce extremely large intermediate result sets in memory even for small input tables — e.g. a simple 5-way join between small tables with 100 rows each would produce 10 billion rows in memory before any result could be produced! This was silly, because what was really intended by the query above is more something like: Render *\n                      |\n                Filter on a.x+b.y+c.z > 0\n                      |\n                    Join on b.y = c.y\n                   /               \\\n             Join on a.x = b.x      c\n               /  \\\n              a    b Processing the query like this obviously reduces the amount of intermediate rows greatly. We needed to get here somehow, and the process to get there is what we describe in the upcoming sections. Optimizing the join operator The first thing that we changed was how the individual binary joins are processed. Previously we used this “nested loop”, a sequential algorithm which for every incoming row on one side looks at every row on the other side to decide whether there is a match. This works but always has quadratic complexity , i.e. nearly the worst one can imagine. So over the fall our beloved co-op and colleague Irfan Sharif provided an alternate implementation based on the classical hash join algorithm : the results of the first table are loaded in a hash table keyed with the join columns. The second table is then scanned, and at every row, the hash table is looked up to see if there is a match. The hash table lookup runs in amortized constant time, compared to the linear lookup of the initial code, and thus tremendous speedups were obtained. Because hash joins are conceptually only defined for equijoins (joins where the result exists when two input rows are pairwise equal on the join columns), this new algorithm was initially only activated for joins defined with USING or NATURAL, where the equality mandated by the query syntax. Soon afterwards, we were able to detect equality expressions when using the ON syntax and use hash joins for them as well. Then, an additional realization dawned: when a join is specified using a more complex expression, we can use the same hash join code by running the join “as if” there were no equality column: all the rows of the first input table then simply land in one bin of the hash table, and we must then scan the content of this table for every row of the second input table to decide join results. This is exactly equivalent in behavior to the “nested loop” code. (Not a big surprise in hindsight though, because the theory already tells us that hash joins degenerate to quadratic complexity when there are few equality classes in the input.) So we ended up removing the original code entirely, and replacing it with hash joins everywhere, letting the hash join devolve into the original behavior when it can’t do its magic. Side note: A common ritual among SQL database implementers is to experience trouble and discomfort while trying to make outer joins work properly. For example, PostgreSQL supported inner joins in its first release in 1996 but only started supporting outer joins in 2001 . We are experiencing our fair share of hassle too, and we had to fix our outer join implementation once again . Hopefully outer joins are functional in CockroachDB but they won’t receive as much attention for optimizations until the rest of our implementation matures a bit further. Optimizing query filters The second thing we looked at was to reduce the strength of join operations: how many rows are provided as input to the hash join algorithm. To do this, we observed that many queries specify a WHERE clause which narrows down the result rows, but that this WHERE clause could equally well be applied before the join instead of afterwards. That is, a query of the form: SELECT * FROM a,b WHERE a.x = b.x Is equivalent to: SELECT * FROM (a JOIN b USING(x)) And a query of the form: SELECT * FROM a, b WHERE a.y > 3 AND b.z < 4 AND a.x = b.x Is equivalent to: SELECT *\n   FROM (SELECT * FROM a WHERE a.y > 3) AS a\n        JOIN\n        (SELECT * FROM b WHERE b.z < 4) AS b\n        USING (x) To understand why this matters, imagine that tables a and b both contain 10 million rows, but only 100 rows in a have column y greater than 3 and 100 rows in b have column z smaller than 4. In the original case, the cross join would need to load the complete left table in memory (10 million rows), which would exhaust the server memory, before it could apply the filter. If we look at the equivalent query underneath, the two filters are applied first and the inner join then only has to match 100 rows on both sides to each other — a much lighter operation. Transforming the original query to the latter, equivalent query is an instance of a larger class of optimizations called “ selection propagation ”. The idea stems from relational algebra , where the various data processing stages of a SQL query are expressed as a formula using only basic building blocks like projection, selection, joins, etc. The theory explains that most queries that perform something (e.g., a join), then a selection, (a WHERE filter) can be always rewritten to do the selection first, then the rest — it is said that selection is commutative with most other relational operators — and that this transformation always reduces the strength of the other operators and is thus always desirable. So over the last quarter of 2016, we implemented this optimization in CockroachDB , and it is now activated for (but not limited to) inner joins. Once this was done, an immediate benefit was automatically activated too: as the WHERE filters “propagate down” the joins in the query, they become visible as constraints on the lookups from the table operands, and thus become visible for index selection and K/V lookup span generation . Thanks to this fortuitous combination of existing optimizations, joins are now also accelerated by the ability to use indexes and point KV lookups to access the individual table operands. For example, in SELECT cust.name, order.amount FROM cust JOIN order USING(cust_id) WHERE cust_id = 123 , just one K/V lookup will now be performed on cust and order , where previously all customers would be loaded in memory before the join could proceed. Elision of unused columns In addition to the work presented so far, which supports the bulk of the acceleration of queries compared to summer 2016, another optimization was introduced under the hood. Suppose you work with a “customer” table with columns “name”, “address”, ”id”, “company”, and you perform a query like SELECT customer.name FROM customer, order WHERE customer.id = order.cust_id . Internally, before the optimization above apply, this query is transformed to this: SELECT customer.name\n  FROM (SELECT name, address, id, company FROM customer)\n       JOIN\n       (SELECT … FROM order)\n WHERE customer.id = order.id That is, the input operand to the join “sees” all the columns in the customer table. This is necessary because the WHERE clause, which is specified “outside” of the join operator, can use any combination of columns from the input table, regardless of what the outer SELECT ends up using. The reason why this matters is that without additional care, this query could then be translated to use a table scan on the “customer” table that retrieves all the columns semantically listed at that level. In this example, that would mean all the columns in the table — even those columns that won’t be needed in the end . Then the join operation would accumulate all this data in memory , before the contents of the second table are read in to decide the final join results, and before the outer SELECT throws away all columns but one (“name”) to produce the query’s results. For tables that contain many columns but are used in queries that only access a few of them at a time, this behavior wastes a lot of time (loading and moving data around for nothing) and memory (to store these unneeded values) and network bandwidth. Acknowledging that cases where tables contain many columns but queries involving joins only use a few of them are rather common, we had to care about this seriously. So we implemented a small optimization that looks at the query from the outside and determines at each level which columns are strictly necessary to produce the final results. Every column that is not necessary is simply elided, i.e., it is not loaded nor further manipulated/communicated with during the query’s execution. Sneak peek — distributed query execution and parallelization As announced in the previous blog post on joins , we are also working on a distributed query execution engine. The present post is not the opportunity to detail this much — we will start a series of blog posts dedicated to distributed query execution — however here is a sneak peek of how it will help with joins. When CockroachDB distributes a join, it loads the data from both table operands simultaneously and in parallel on multiple nodes, performing multiple parts of the join on different nodes of the cluster, before merging the results towards the node where the query was received from the client. This way, the join can be theoretically sped up by a factor linear in the size of the cluster (in the ideal case). Again, we will report more on this topic separately! The update here is that this code is now available inside CockroachDB and can run some simple queries, even queries involving joins. This feature is not yet documented nor officially supported, but you can activate it on a per-session basis using the command SET DIST_SQL = ON . The takeaway for now is that distributed query execution will be one of the main instruments by which CockroachDB will accelerate the execution of joins further, in addition to traditional optimizations. SQL joins in CockroachDB today and in v1.0 Although our engineering efforts on query optimization thus far can be called “preliminary” — we have just started implementing simple, well-known classical algorithms — the use of hash joins and selection propagation by CockroachDB’s execution engine means that SQL client applications can now expect much better performance for simple queries using joins than was previously experienced. For example, whereas we would previously strongly discourage the use of the common, well-known syntax SELECT … FROM a,b,c WHERE … , because it implicitly defines a full cross join that has terrible time and space performance behavior, now CockroachDB can easily “see through it” and execute a smaller, much more efficient query plan if the WHERE clause makes this possible. This is the state which have reached now ( go try it out! *) and that you can expect in our v1.0 release. *User beware: Join-related documentation is still in the works. _Image Credit: Henrik Kniberg Plans for 2017 and beyond The optimizations we presented above are called “local” because they can be decided by just looking at the structure of a small part of the query, without considering the overall structure of the query, nor the database schema, nor statistics about the data stored in the database. We are planning a few more additional local optimizations, for example using merge joins when the operands are suitably ordered, but our attention will imminently grow to a larger scope. The state of the art with SQL query optimization is to re-order join operations based on which indices are currently available and the current cardinality of values — the number of different values in join columns — in the operand tables. This is where mature and enterprise SQL databases find their performance edge, and we intend to invest effort into doing the same. One particular source of inspiration we encountered recently is the article “How Good are Query Optimizers, Really?” by Victor Leis et al. published last year in the proceedings of the VLDB (Very Large Database) conference. This article not only highlights some metrics which we can use to fairly compare against our competition; it also points to the current fashionable and possibly useful benchmarks in the industry and interesting pitfalls in their application. In particular, thanks to their Join Order Benchmark , we now feel more adequately equipped to support and evaluate our ongoing efforts on join optimization. Until then, we would absolutely be thrilled to hear from you, if you have any opinion, preference, or even perhaps advice you would like to share about how to best make CockroachDB match your performance needs. Never forget, we are always reachable on Slack and our forum . And if building out distributed SQL engines puts a spring in your step, then check out our open positions here . Acknowledgments We would like to thank especially Irfan Sharif for his contributions in this area during his co-op with us.", "date": "2017-02-09"},
{"website": "CockroachLabs", "title": "CockroachDB Beta Passes Jepsen Testing", "author": ["Diana Hsieh"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-beta-passes-jepsen-testing/", "abstract": "Almost a year ago, we wrote about our use of Jepsen in testing CockroachDB. As we prepare for CockroachDB 1.0, we wanted to get independent verification of our findings, so last fall we hired Kyle Kingsbury, the author of Jepsen, to review our tests and add more of his own. Last week, Kyle published his results . Kyle’s testing found two new bugs: one in CockroachDB’s timestamp cache which could allow inconsistencies whenever two transactions are assigned the same timestamp, and one in which certain transactions could be applied twice due to internal retries. Both of these issues were fixed (the first in September and the second in October ), and the expanded Jepsen test suite is now a part of our nightly test runs. Now we can say with more confidence that CockroachDB meets its guarantee of serializability (the highest ANSI SQL isolation level) for all SQL transactions. The Tests CockroachDB’s original Jepsen test suite included four tests: register and bank , which were adapted from Jepsen tests for other databases (The bank test had previously been used to find serializability failures in MariaDB with Galera and Percona XtraDB Cluster), and monotonic and sets , which were developed specifically for CockroachDB. To these, Kyle added three more: G2 , sequential , and comments . The G2 test looks for (but did not find) a specific hypothesized anomaly called “anti-dependency cycles”. The sequential test shows that with an additional constraint (clients are “sticky” to a particular node), we provide sequential consistency (which is a stronger claim than our normal guarantee of serializability ). The comments test is different from the rest, in that it was expected to fail (it requires a database that is linearizable instead of merely serializable ). This helps verify that the testing methodology is able to detect the subtle differences between consistency levels. Kyle also added several new nemesis modes to inject adverse events into the test, such as clock changes or network failures. The Results The register , bank , G2 , and sequential tests passed under all tested configurations, verifying CockroachDB’s strong serializability claims, and the comments test failed as expected. The monotonic test (described in detail in section 2.6 of Kyle’s report) found a serializability violation when two transactions were assigned the same timestamp. This may seem unlikely since Cockroach uses timestamps with nanosecond precision, but our use of hybrid logical clocks for timekeeping can cause multiple servers to use exactly the same timestamps, especially when the system clocks are jumping around. The cause of this violation was a bug in the timestamp cache and was fixed in beta-20160915 . The sets test (described in section 2.7 ) revealed a bug when it was refactored to use a single auto-committed INSERT statement instead of a multi-statement BEGIN / INSERT / COMMIT transaction (using a table with no primary key). Single statements can be retried by the server after certain errors (multi-statement transactions generally cannot, because the results of previous statements have already been returned to the client). In this case, the statement was attempted once, timed out due to a network failure, then retried, and both attempts eventually succeeded (this was possible because the table had an auto-generated primary key, so the two insertion attempts were not using the same PK values). To fix this, we recognize certain types of errors as “ambiguous” since beta-20161027 and avoid retries when it might result in a statement executing twice. It is unfortunate that we must sometimes return this “result is ambiguous” error to the client, but ambiguity is inevitable in any protocol that does not include transaction ids or other mechanisms to query the outcome of a previous operation. This testing also confirmed that loose clock synchronization is necessary for CockroachDB to guarantee serializability. CockroachDB servers monitor the clock offset between them and will attempt to abort rather than return results that may be inconsistent, although this monitoring is imperfect and may not be able to react quickly enough to large clock jumps. All CockroachDB deployments should use NTP to synchronize their system clocks. The amount of clock offset that can be tolerated is configurable. We’ve found the default of 500ms (increased since Kyle’s testing, when it was 250ms) to be reasonable in most environments, including virtualized cloud platforms. Environments with very reliable clocks may be able to reduce this setting to improve performance in some cases. On Performance One of the most eye-catching lines in Kyle’s report concerns performance: “For instance, on a cluster of five m3.large nodes, an even mixture of processes performing single-row inserts and selects over a few hundred rows pushed \\~40 inserts and \\~20 reads per second (steady-state).” These, of course, are pitiful numbers for a database. However, it’s important to note that these numbers are not representative of what real-world applications can expect to see from CockroachDB. First, this is a test in which the Jepsen nemesis is continuously tweaking the network configuration to cause (and heal) partitions between different nodes. You don’t commonly see behavior like this in a real world deployment. Second, tests like these deliberately try to reach high levels of contention, in which many transactions are trying to operate on the same keys. In real-world situations contention tends to be much lower as each user is operating mainly on their own data. The performance of all transactional systems suffer under high contention (although CockroachDB’s optimistic concurrency model fares worse than most), and applications that anticipate high contention on part of their data (such as incrementing the “like” counter on a post that’s gone viral) should consider this in their schema and application design. Continued Work The development philosophy of Cockroach Labs has always been correctness and stability first, then performance. To that end, we’re focusing our attention now on performance, including both the kinds of high-contention transactions seen here and more typical low-contention scenarios , ahead of our 1.0 launch later this spring. We’ll have much more to say about performance in an upcoming blog post. If building out and proving the correctness of a distributed SQL system sounds like your ideal Wednesday, then good news — we’re hiring! Check out our open positions here .", "date": "2017-02-23"},
{"website": "CockroachLabs", "title": "Research, Reuse, Recycle", "author": ["Kuan Luo"], "link": "https://www.cockroachlabs.com/blog/research-reuse-recycle/", "abstract": "I recently came across an old piece on The Atlantic on design research . Author and educator Jon Freach wrote, “Design can exist without ‘the research.’ But if we don't study the world, we don't always know how or what to create.” His words resonated with me. Designers are innate problem solvers. Without “the research,” we wouldn’t know what problems to solve and for whom we create solutions. One may argue that people generally don’t know what they want, and it’s up to us creating something new to spark desire. Yet, that creation process isn’t sheer magic being pulled out of thin air. The creation process usually involves painstaking investigation of the world and deep inquiry on how we can make it better. In other words, the shiny new wonderful thing that everyone wants is just a reincarnation of a similar idea but thoroughly interrogated and researched. Smartphones existed before the first iPhone was introduced. Group chat was invented long before Slack was a company. Similarly, databases are not a new topic. When I started several months ago, I wanted to do “the research” to figure out how this new database with a funny name could spark desire. My first quest was to investigate pain points people have with their current database solutions, and how they first hear about, test and adopt alternative options. Having not been properly trained as a researcher, I hacked together a study with the help of GV’s extensive resources and recruited and interviewed a few developers and CTOs. Yet, the data I collected was scattered, with no clear pattern across the demographics. Given how crucial the questions were to understanding of our audience, I decided to reevaluate the research process, and redo the study with some tweaks. Here’s what I’ve learned from doing the same study twice, and why the second study was a lot more effective. Planning: Version 1 Version 2 I read up on available resources, wrote research objectives, and outlined questions. I sat down with our marketing department (the major stakeholder for this project) first, and tasked them with setting objectives, key questions, and recruitment criteria for the study. The interview guide was reviewed by the stakeholders, which was our marketing department. After they took a first stab at the interview guide, I commented and asked questions to ensure the study was focused with a smooth flow. I then wrote the recruitment criteria, and passed onto Marketing for review. For the second study, I experimented with letting the stakeholders set the goals. The result was delightful - not only did we save a lot of time up front, but everyone was also onboard with the research much earlier and was much more excited about the results. In the planning process, we also discussed our hypotheses on why the first study produced scattered data. And our conclusion was that the interviewees were too different from each other, so we weren’t able to get enough data on one archetype of the audience. For the first study, I recruited consultants, developers, and c-level decision makers at companies of all sizes. For the second study, therefore, we decided to focus solely on developers who aren’t decision-makers at their companies or organizations. Recruiting: Version 1 Version 2 I looked to interview both developers as well as C-level decision makers in enterprises for the study. We only recruited developers who aren’t decision-makers at work. Only Cockroach Labs swags were offered as incentives for participants’ time. We offered cash incentives in addition to our swags to broaden the appeal of participating. I emailed our existing user base and got roughly the same number of responses as we needed for the study. In addition to emailing our user base, we leveraged our engineering team’s network to broadcast our recruitment message on Facebook, Twitter, and LinkedIn. As a result, we were able to get a bigger pool of potential participants the second time around, and handpicked the most fitting profiles to interview. Unlike the first study, our data points were far less scattered and therefore provided a clear pattern of how non-decisioning-making developers shop for databases. Interviewing: Version 1 Version 2 I conducted all the interviews. An additional researcher conducted all the interviews. Half of the interview sessions were well attended by stakeholders and engineers. The rest were scheduled at inconvenient times for others to join and observe. We booked all five sessions in one day during work hours, and all stakeholders were in the same room (at our own office!) so we had minimal setup time. We were part of a coworking space, which made it difficult to secure a single room for all the interviews. We pre-drew grids on a whiteboard so we could easily identify patterns among participants. One person was responsible for noting on the whiteboard for each session. I used video calls. I used audio-only for all the interviews. For the second study, we were fortunate enough to have Michael Margolis, UX Research Partner at GV , conducting the interviews for us. His involvement freed us to attend more closely to the responses developers gave. Scheduling all the interviews in one day and having the entire group in the same room were keys in making the interview phase successful. By clearing our schedules and dedicating a full day to research, we saved ourselves time and energy from context switching throughout the day. The pressure of having just one day to research also motivated us to perk up our ears and devour as much information as we could. Having everyone in the same room allowed us to first-handedly hear the questions and pain points our developers had, and discuss the research results as they came in. Comments and ideas were bounced between departments and insights were immediately distributed across the board. During the interview, two changes were proven to be quite useful. First, using audio instead of video decreased the awkwardness that was experienced in the first round of interviews. Though the idea of seeing faces was well intended, it made both the interviewer and the interviewee more self-conscious and guarded. Second, the whiteboard was an excellent, hands-on replacement for a collaborative Google doc. By the end of the day, our research results were visually highlighted and annotated in front of us. Conclusion: It’s perhaps obvious why the second study was more successful: we involved our stakeholders early, spent time and incentives to recruit and observed the sessions as a group. In the end, we collected clear patterns, actionable insights and most importantly, better questions to answer in the future. And the fact that we ran the same study twice was a lesson of its own. Tracing our steps backward allowed us to identify opportunities for experiments and learn from mistakes. Though we’re far from perfecting our user research process, it’s the curiosity and rigor that motivates us to keep prototyping and learning about people who use our product. To Freach’s point, that’s how we know what to create. For more thoughts on design and building product, follow along as I ramble Back and Forth on Medium.", "date": "2017-03-07"},
{"website": "CockroachLabs", "title": "Implementing Unicode Collation in CockroachDB", "author": ["David Eisenstat"], "link": "https://www.cockroachlabs.com/blog/unicode-collation-in-cockroachdb/", "abstract": "CockroachDB recently gained support for Unicode collation, a standard for ordering strings in the different ways that our users around the world expect. This post describes the motivation for Unicode collation as well as the implementation challenges in providing collated strings as a first-class type. Collated strings are documented here . Note that CockroachDB doesn’t support every use of collation that PostgreSQL does, due in part to implementation deficiencies that we plan to address and in part because we believe that the bugs and performance problems caused by implicit type conversions outweigh their convenience. We’ve left the door open for full support, however. A curious sort Here's an excerpt from the Oxford 3000, a list of important English words in alphabetical order. ocean\no'clock\nOctober\nodd Let's see how CockroachDB orders these strings: CREATE TABLE words (word STRING PRIMARY KEY);\nINSERT INTO words VALUES ('ocean'), ('o''clock'), ('October'), ('odd');\nSELECT word FROM words ORDER BY word ASC;\n\n+---------+\n|  word   |\n+---------+\n| October |\n| o'clock |\n| ocean   |\n| odd     |\n+---------+\n(4 rows) Can you spot the difference? Like most software, CockroachDB defaults to ordering strings by their UTF-8 encoding, shown below in hexadecimal: October 4f63746f626572\no'clock 6f27636c6f636b\nocean   6f6365616e\nodd     6f6464 October is first because 4f (capital O ) is less than 6f (small o ). o'clock precedes ocean because 27 ( ' ) is less than 63 ( c ). By contrast, alphabetical order in English disregards capitalization and punctuation. New word orders Why doesn't CockroachDB default to alphabetical order? Performance considerations aside (more on these later), no single order would satisfy all users. In German, for example, öffnen precedes zumachen , whereas in Swedish, zon precedes öppna . There is also the small matter of what “alphabetical order” means in languages that don't have an alphabet . Over the years, many standards organizations have defined language-, culture-, and usage-specific orders on strings, culminating in Unicode Technical Standard #10 . #10 describes a generic algorithm for collation , which the Go project helpfully has implemented ( golang.org/x/text/collate ). Let's see how English collation works in CockroachDB: CREATE TABLE words (word STRING COLLATE en PRIMARY KEY);\nINSERT INTO words VALUES\n  ('ocean' COLLATE en),\n  ('o''clock' COLLATE en),\n  ('October' COLLATE en),\n  ('odd' COLLATE en);\nSELECT word FROM words ORDER BY word ASC;\n\n+---------+\n|  word   |\n+---------+\n| o'clock |\n| ocean   |\n| October |\n| odd     |\n+---------+\n(4 rows) October now sorts alphabetically, though o'clock doesn't. For true alphabetical order, the collator would have to ignore punctuation, and while #10 mentions this as an option, the Go library lacks support. The left operand of the COLLATE operator can be a string type or a string value. The right operand is the collation locale ( en for English). The result is a collated string with the same contents. Collated strings compare according to their shared collation locale. Let's revisit the collation difference between German ( de ) and Swedish ( sv ): SELECT ('öffnen' COLLATE de < 'zumachen' COLLATE de, 'zon' COLLATE sv < 'öppna' COLLATE sv);\n\n(true,true) True to type In CockroachDB, STRING , STRING COLLATE en , and STRING COLLATE de are three different types. PostgreSQL, by contrast, blurs the distinction. Both systems reject ('a' COLLATE en) < ('b' COLLATE de) , and rightly so – should the comparison use English rules or German? Only PostgreSQL, however, allows the insertion of an English-collated string into a German-collated column. Although we usually strive for compatibility with PostgreSQL, we felt that our design is easier to understand and implement, may prevent bugs in user SQL statements, and preserves more possibilities for our evolving type system. As a special case of 3, we can switch to the PostgreSQL design later without breaking backward compatibility. Keys all the way down Every column of a SQL table is either a (primary) key column or a value column. Storing collated strings in value columns is easy – just write out their UTF-8 encoding, as you would for ordinary strings. Let's examine why storing collated strings in key columns is more difficult. From the post introducing CockroachDB as a SQL system , you may recall that CockroachDB encodes SQL primary keys as key-value store keys (byte strings) in such a way that the former and the latter sort identically (more precisely, the encoding function is an order embedding ). Since CockroachDB uses UTF-8 order for ordinary strings, their key encoding is almost verbatim . The key encoding for collated strings, however, must reflect the collation locale. Fortunately for us, Unicode Technical Standard #10 defines collation in terms of an order embedding from (collated) strings to byte strings called collation keys . Let's pretend for the moment that this embedding capitalizes all letters. This is not the actual procedure! When the column type is STRING , the key-value pairs in the store look like this: Key                         Value\n--------------------------- -------\n/words/primary/'October'    Ø\n/words/primary/'o''clock'   Ø\n/words/primary/'ocean'      Ø\n/words/primary/'odd'        Ø When the column type is STRING COLLATE en_u_ks_level1 (English, ignoring case), the key-value pairs look like this: Key                         Value\n--------------------------- ------------\n/words/primary/'O''CLOCK'   'o''clock'\n/words/primary/'OCEAN'      'ocean'\n/words/primary/'OCTOBER'    'October'\n/words/primary/'ODD'        'odd' For each row, CockroachDB must store both the collation key and the original string because the former does not determine the latter (consider 'a' and 'A' ). We've adapted this procedure, which we call composite encoding , to floating-point and decimal numbers, the other types with nonidentical equal values (positive and negative zero, decimals with and without trailing zeros). To save space, only negative zero and decimals with trailing zeros have composite encoding. One wrinkle is that collation keys aren't stable across revisions of the tables accompanying #10. The aforementioned Go library hasn't been updated, but if that changes, we'll most likely vendor it and ponder our next move. Generic woes One rough edge of collation support is that most string functions and operators don't accept collated strings: SELECT 'a' COLLATE en || 'b' COLLATE en;\n\npq: unsupported binary operator: <collatedstring{en}> || <collatedstring{en}>\nError: pq: unsupported binary operator: <collatedstring{en}> || <collatedstring{en}>\nFailed running \"sql\" Our recommended workaround is casting to STRING : SELECT (('a' COLLATE en) :: STRING || ('b' COLLATE en) :: STRING) COLLATE en;\n\n1 row\n(('a' COLLATE en)::STRING || ('b' COLLATE en)::STRING) COLLATE en\nab We deferred the fix for this issue due to a limitation of our SQL type system, Summer, as well as the difficulty of writing high-performance generic code in Go. Type system (Summer) Summer has served us well, but its complex strategy for typing overloaded functions and operators has the unfortunate property that adding signatures can break backward compatibility. The present implementation, moreover, assumes that these signatures can be enumerated, whereas there are (in principle) infinitely many collation locales. Collated strings and other parametric types ( TIMESTAMP / TIMESTAMPTZ , arrays, fixed-width integers) are leading us to rethink Summer. Go CockroachDB provides many functions that should accept both ordinary strings and collated strings. For performance reasons, ordinary strings and collated strings have different underlying Go types – collated strings should cache their collation key without incurring bloat in ordinary strings. This means that we get to touch on everybody's favorite topic, writing generic code in Go. The usual suggestions are to use interfaces, duplicate code, or generate code. Interfaces require an extra allocation for each string value – not acceptable. We tried duplicating code for TIMESTAMP/TIMESTAMPTZ and found it to be tedious and error-prone on a smaller set of functions. We'll probably use a higher-order adapter function as a stopgap until we get around to generating code. See you collator As always, if you discover an issue with collated strings, please let us know on our GitHub . The implementation of collated strings required changes to a number of CockroachDB SQL subsystems. If you're interested in how these subsystems work, see our (forthcoming?) blog post on the documentation in https://github.com/cockroachdb/cockroach/tree/master/docs/stable/tech-notes . Alligator.", "date": "2017-04-13"},
{"website": "CockroachLabs", "title": "CockroachDB 1.0 is Production-Ready", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-1-0-release/", "abstract": "Today, we are pleased to announce the release of CockroachDB 1.0, the first open source, cloud-native SQL database. We’re also announcing a series B fundraise from investors who share our vision. The launch of 1.0 marks our graduation from beta to a production-ready database, designed to power business at any scale from the startup to the enterprise. A brief introduction is in order. While databases aren’t generally considered the most thrilling subject in technology news, ignoring them would be a mistake. Understanding the ongoing evolution of databases brings into focus a ruthless arms race between what businesses need to do with data, and what existing technologies struggle to provide. The most insistent pressures have forced databases alternatively to become faster, bigger, and more reliable. Then again: faster, bigger, and more reliable. And the cycle continues. We’re seeing just such an evolution in 2017. Some folks refer to it as “NewSQL,” we prefer to call it distributed SQL. It’s a combination of making general purpose relational database systems (RDBMSs) both bigger and more reliable. CockroachDB earns its NewSQL stripes by providing distributed SQL to accommodate ever-larger data sizes and multi-active availability, a new model for high availability (HA). These capabilities establish a fundamentally better standard for deploying global cloud services. With customers already in production, including Baidu and Heroic Labs, and more than 100,000 downloads since our beta release, we are happy to additionally announce our series B raise of $27M, led by Satish Dharmaraj of Redpoint Ventures, joined by Benchmark, FirstMark Capital, GV (formerly Google Ventures), Index Ventures, and Work-Bench. Satish will make a valuable addition to our investor and advisory team. Among many other things, he both founded and ran Zimbra, an open source company which successfully competed with an industry Goliath: Microsoft. This latest round provides the capitalization to match the size of both the challenge and the opportunity ahead. CockroachDB 1.0 CockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters. But what does “cloud-native” actually mean? We believe the term implies horizontal scalability, no single points of failure, survivability, automatable operations, and no platform-specific encumbrances. To realize these product goals, development over the past year has focused on three critical areas: distributed SQL to support small and large use cases alike and scale seamlessly between them; multi-active availability for always-consistent high availability; and flexible deployment for automatable operations in virtually any environment. Distributed SQL Relational databases offering SQL have traditionally been constrained to single-node deployments. The difficulty of scaling SQL workloads is clear from the meteoric rise of NoSQL databases over the past decade. NoSQL databases promised scale (and simplicity), and they delivered. However, adoption remains a small fraction of the total database market, and growth has slowed. One reason is that NoSQL solutions sacrifice guarantees, common to SQL, that make life simpler for programmers. No ACID transactions or read consistency, along with the requirement to learn a custom query and data modification language, can be significant barriers to adoption. CockroachDB provides scale without sacrificing SQL functionality. It offers fully-distributed ACID transactions, zero-downtime schema changes, and support for secondary indexes and foreign keys. It works out of the box with many popular ORM frameworks by supporting an industry standard SQL dialect. In addition, release 1.0 introduces a distributed query execution engine, enabling distributed JOINs to support analytics queries that speed up linearly as nodes are added to your cluster. We recently demonstrated a use case with Baidu processing 2 billion inserts a day for one of the top ten largest global internet companies. This rate of transactions continued regardless of artificially-induced “chaos” events, which caused significant concurrent re-replication and rebalancing across the cluster. Multi-Active Availability “High availability” (HA) is a crucial capability. If your database is not available, then the services that rely on it won’t be either. Until now, most database HA solutions have sacrificed consistency for performance. However, when networks and nodes fail in the real world, applications will read incorrect data, and write values that disappear into thin air. Speed makes for great benchmarks, but it must be a secondary consideration when working with mission-critical data. CockroachDB takes a different approach to HA, putting consistency first, where it belongs. We call it “multi-active availability”, because it’s an evolution in high availability from active-active replication. Instead of eventual consistency, it employs strongly-consistent consensus-based replication, which uses three or more active replicas, any of which can begin serving read/write client traffic. Unlike primary-secondary replication, which maintains 50% (or more) of all resources as idle replication targets, multi-active availability dynamically utilizes all available resources. Unlike active-active replication, multi-active availability will not read or write inconsistently and doesn’t require conflict resolution. One customer of ours, one of the largest online gaming companies in the world, was looking for a solution to serve consistent reads and writes across continents. With multi-active availability, nodes geographically closest to their users are able to serve queries, significantly reducing latency. Flexible Deployments Even the best technology is only as useful as its weakest operational link. If it can’t be easily deployed and operated, it will have trouble thriving in production. We made architectural and operational simplicity a priority from the start. That simplicity means CockroachDB can be managed both with popular datacenter orchestration technologies like Kubernetes and Docker Swarm, and even manually, meaning it can flexibly fit into any custom operational environment. CockroachDB can be run on premise or on any public cloud, deployed in hybrid cross-cloud configurations, and migrated between clouds with zero downtime. This gives companies a powerful degree of flexibility, distinctly absent from the vendor lock-in associated with using a proprietary database as a service. At the OpenStack Interop conference yesterday in Boston, CockroachDB was deployed live across 15 private clouds, demonstrating significant portability and flexibility. A video of the demo is below: Commercial Offering: CockroachDB Enterprise Working at enterprise-scale in production means that operational workloads must be run with minimal downtime and with multiple failsafes. In addition to support for zero-downtime, rolling upgrades and certificate rotation, we are happy to announce a feature available specifically as part of our CockroachDB Enterprise offering: distributed, incremental backup and restore. This feature is intended to serve customers with large data sets and exacting production requirements. Distributed backup / restore parallelizes backup and restore tasks across all nodes in the cluster. Incremental backups enable efficient periodic updates to full backups. Data is written to and restored from any configurable storage sink, such as blob stores offered by AWS, GCP, and Azure. Because periodic backup and restore is a best practice for anyone running a service on top of a database, a non-distributed option for backup / restore is available for free in the CockroachDB Core offering. The Future We’ve come to a significant milestone. Release 1.0 is the production-ready debut of the world’s first open source, cloud-native SQL database. But this is just table stakes. The most interesting future challenge is to tackle problems bedeviling the world’s fastest growing and largest companies, which are struggling to build global services. How do you build data architectures able to serve customers on multiple continents, while presenting your operators and application developers with what looks like a single, logical database? How do you both minimize latencies and provide the foundation for compliance with rapidly evolving data sovereignty laws? Our first significant step in this direction will be a geo-partitioning feature, which will enable row-level control of geographic replication, keeping your customers’ data next to your customers. While CockroachDB already supports flexible geographic replication at table and database granularities, geo-partitioning will allow any column or columns in a table to serve as partition keys. This makes it possible to relocate a customer from Sydney to London with a simple SQL UPDATE statement. Transactions and queries that span partitions will be handled transparently by CockroachDB. We expect a beta version of this capability in late 2017! Where Rubber Meets Road We’ve taken pains to test CockroachDB in many environments, with different workloads, and with various manufactured (and unmanufactured!) chaos events. We’ve also made significant progress on performance. However, to paraphrase Donald Rumsfeld, there are both known unknowns and worse, unknown unknowns in our immediate future. This is the point at which the rubber meets the road for CockroachDB, and we expect that the wider set of users, use cases, and environments will highlight deficiencies in our product. In accordance, we expect to devote significant resources to this “ruggedization” phase. While release 1.1 won’t be ready until October 2017, point releases along the way will further stabilize the 1.0 release. For release 1.1, expect additional SQL coverage, better diagnostic tools, and significant performance improvements. We look forward to welcoming new users to CockroachDB with our 1.0 release. And we’re committed to making you successful both now, and in the future, by continually expanding CockroachDB’s capabilities to keep your business well ahead of the curve. We will be in San Francisco on Thursday, May 11 and in NYC on Wednesday, May 17 for our CockroachDB user groups. Our CTO Ben Darnell will cover what went into the 1.0, so come give us your feedback.", "date": "2017-05-10"},
{"website": "CockroachLabs", "title": "apd: An Arbitrary-Precision Decimal Package for Go", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/apd-arbitrary-precision-decimal-package/", "abstract": "With the release of CockroachDB beta-20170223 , we’d like to announce a new arbitrary-precision decimal package for Go: apd . This package replaces the underlying implementation of the DECIMAL type in CockroachDB and is available for anyone to fork and use. Here we will describe why we made apd, some of its features, and how it improved CockroachDB. Background on Decimals The two standard numeric types in computing are integers and floats, which handle the two main problems in number representation: the need for exact integral numbers within a fixed, relatively small range (ints) and the need for numbers, within an arbitrarily large range, that can be approximate (floats). But SQL presents a third need: exact numbers within an arbitrarily large range. Any application that requires a high degree of precision in its calculations or that must guarantee exactness (that is, it is guaranteed to not round) can use neither int nor float. The SQL spec defines a decimal type to handle this kind of work that is able to perform computations either exactly or with a user-defined level of precision. The Go standard library includes a math/big package which extends the normal integer and float types to be able to operate on data with user-defined precision, which covers many use cases beyond the normal fixed-size integer and float types. If your problem domain requires non-integer computations, the big.Float type handles many use cases. However big.Float suffers from similar inexactness problems as the fixed-precision types, albeit at more of a user-defined precision. That is, the binary floating point types cannot exactly represent decimal fractions . To solve this problem, a type based on the decimal representation of numbers is needed. CockroachDB uses Go’s int64 and float64 types to support the corresponding integer and float SQL types. SQL also specifies support for decimal types. Go does not come with a decimal package , so we initially used an existing package, gopkg.in/inf.v0 . This served us well as a starting point, but over time we began to notice problems with it. Certain operations would produce incorrect results, panic, complete slowly, or fail to complete at all, fully using a CPU core. Many of these issues were fixed with checks inside CockroachDB to try to prevent runaway processes. Even with many of these problems triaged, there were still known inaccuracies and user-visible problems that were related to the design of the Go package, which were preventing us from meeting our own goals of: Panic-free operation Support for standard functions like sqrt , log , pow , etc Accurate and configurable precision Good performance We decided that fixing these issues would be about the same amount of work as starting from scratch. We also evaluated other Go decimal packages, but we decided that they would likely never meet our requirements due to the similar re-engineering required. We respect the work of those authors, but we had different goals. So in December 2016 we started development on apd. By February 2017, we ended up with a new decimal package that is really useful to us and that we hope is sufficient for reuse by the Go community. Features of apd apd implements much of the decimal specification from the General Decimal Arithmetic description. This is the same specification implemented by python’s decimal module and GCC’s decimal extension. This package has many features: Panic-free operation . The math/big types don’t return errors, and instead panic under some conditions that are documented. This requires users to validate the inputs before using them. Meanwhile, we’d like our decimal operations to have more failure modes and more input requirements than the math/big types, so using that API would be difficult. apd instead returns errors when needed. Support for standard functions . sqrt , ln , pow , etc. Accurate and configurable precision . Operations will use enough internal precision to produce a correct result at the requested precision. Precision is set by a “context” structure that accompanies the function arguments, as discussed in the next section. Good performance . Operations will either be fast enough or will produce an error if they will be slow. This prevents edge-case operations from consuming lots of CPU or memory. Numerous academic papers were consulted and are referenced (for example: sqrt , ln , exp ). These papers prove and describe algorithms that perform much better than their naive counterparts. Condition flags and traps . All operations will report whether their result is exact, is rounded, is over- or under-flowed, is subnormal , or is some other condition. apd supports traps which will trigger an error on any of these conditions. This makes it possible to guarantee exactness in computations, if needed. apd has two main types. The first is Decimal which holds the values of decimals. It is simple and uses a big.Int with an exponent to describe values. Most operations on Decimal s can’t produce errors as they work directly on the underlying big.Int . Notably, however, there are no arithmetic operations on Decimal s. The second main type is Context , which is where all arithmetic operations are defined. A Context describes the precision, range, and some other restrictions during operations. These operations can all produce failures, and so return errors. Context operations, in addition to errors, return a Condition , which is a bitfield of flags that occurred during an operation. These include overflow, underflow, inexact, rounded, and others. The Traps field of a Context can be set which will produce an error if the corresponding flag occurs. An example of this is given below. These features allow apd’s API to enable new kinds of operations that were previously difficult to perform in Go. Precision A Context ’s precision can be increased as desired. Flags and errors will change based on the operation being performed and whether or not it needs more precision to work. Inexact flags can be raised if there’s not enough precision to store the result. For example: twoHundred := apd.New(2, 2)\nfiveTwelve := apd.New(512, 0)\nd := new(apd.Decimal)\nc := apd.BaseContext\nfor i := uint32(0); i <= 7; i++ {\n    c.Precision = i\n    res, err := c.Quo(d, twoHundred, fiveTwelve)\n    fmt.Printf(\"%d: %s\", i, d)\n    if err != nil {\n        fmt.Printf(\", error: %s\", err)\n    }\n    if res != 0 {\n        fmt.Printf(\" (%s)\", res)\n    }\n    fmt.Println()\n} Output: 0: 0, error: Context may not have 0 Precision for this operation\n1: 0.4 (inexact, rounded)\n2: 0.39 (inexact, rounded)\n3: 0.391 (inexact, rounded)\n4: 0.3906 (inexact, rounded)\n5: 0.39063 (inexact, rounded)\n6: 0.390625\n7: 0.390625 Overflow and Underflow apd can detect or error on overflow and underflow conditions. We can define a context that has an exponent limit then perform operations with it until an overflow condition occurs. For example: // Create a context that will overflow at 1e3.\nc := apd.Context{\n    MaxExponent: 2,\n    Traps:       apd.Overflow,\n}\none := apd.New(1, 0)\nd := apd.New(997, 0)\nfor {\n    res, err := c.Add(d, d, one)\n    fmt.Printf(\"d: %4s, overflow: %5v, err: %v\\n\", d, res.Overflow(), err)\n    if err != nil {\n        return\n    }\n} Output: d:  998, overflow: false, err: <nil>\nd:  999, overflow: false, err: <nil>\nd: 1000, overflow:  true, err: overflow Exactness Some operations like division can sometimes produce exact ( 1/2 ) or inexact ( 1/3 ) outputs. The Inexact flag can be checked for inexact operations. The 1/3 example here will always be inexact. But there are other operations that will only be exact if the context has enough precision to fit the result. For example: d := apd.New(27, 0)\nthree := apd.New(3, 0)\nc := apd.BaseContext.WithPrecision(5)\nfor {\n    res, err := c.Quo(d, d, three)\n    fmt.Printf(\"d: %7s, inexact: %5v, err: %v\\n\", d, res.Inexact(), err)\n    if err != nil {\n        return\n    }\n    if res.Inexact() {\n        return\n    }\n} Output: d:       9, inexact: false, err: <nil>\nd:       3, inexact: false, err: <nil>\nd:       1, inexact: false, err: <nil>\nd: 0.33333, inexact:  true, err: <nil> ErrDecimal The ErrDecimal type wraps a context and and error. It will silently not perform operations after an error has occurred. This is useful to do many operations and then check for an error once at the end. For example: c := apd.BaseContext.WithPrecision(5)\ned := apd.MakeErrDecimal(c)\nd := apd.New(10, 0)\nfmt.Printf(\"%s, err: %v\\n\", d, ed.Err())\ned.Add(d, d, apd.New(2, 1)) // add 20\nfmt.Printf(\"%s, err: %v\\n\", d, ed.Err())\ned.Quo(d, d, apd.New(0, 0)) // divide by zero\nfmt.Printf(\"%s, err: %v\\n\", d, ed.Err())\ned.Sub(d, d, apd.New(1, 0)) // attempt to subtract 1\n// The subtraction doesn't occur and doesn't change the error.\nfmt.Printf(\"%s, err: %v\\n\", d, ed.Err()) Output: 10, err: <nil>\n30, err: <nil>\n30, err: division by zero\n30, err: division by zero TODO apd does not yet support: NaN Infinity These are planned for some time in the future. Update: NaN and Infinity are now supported. apd and Decimals in CockroachDB apd has allowed CockroachDB to have much greater parity with Postgres results, and at higher performance than before. Our tests have shown that CockroachDB now matches Postgres in nearly all operations. We hope to add more functionality to our SQL decimal implementation, like allowing users to define precision and rounding modes during SQL operations, although as stated above, the timing for that development isn’t yet clear. Conclusion The apd package is a well-tested and useful addition to CockroachDB. It is under the Apache license, so will hopefully be useful to the wider Go community. Although it has not been in use for a long time, it has shown to be better than an older package, and we had enough confidence in the tests to replace our implementation with it. As a disclaimer, we are not trained in numerical analysis. The papers we consulted were largely from the 1980s, so we hope there is more recent work that can be used to further improve performance. Contributions and API suggestions or complaints are welcome and encouraged by filing GitHub issues . (Also published on Matt Jibson's blog .)", "date": "2017-03-15"},
{"website": "CockroachLabs", "title": "[podcast] Unscripted Founders Q&A on CockroachDB 1.0", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/podcast-unscripted-founders-1dot0/", "abstract": "LISTEN ON : | SoundCloud | Join the Cockroach Labs founders for an unscripted conversation about the dirty details building 1.0 and in achieving consensus across three co-founders. What do they wish they could have made it into the 1.0 release? What are they most excited about in this production-ready release? And what happens when they disagree with each other? FULL TRANSCRIPT: Peter: Hi, I’m Peter Mattis and welcome to our first-ever Cockroach Labs podcast. Today I’m going to have an unscripted chat with my co-founders about CockroachDB 1.0, the production-ready release of Cockroach. For anyone new to us, Cockroach Labs is the company building CockroachDB, an open source, cloud-native SQL database. We’ve got in front of us a stack of questions about the release and about the team building it that we’ll be cherry-picking our way through, unscripted-style. Let’s get started started with everyone introducing themselves and telling our listeners which inventor they would like to meet, living or dead. Spencer: Hi. My name is Spencer Kimball. I'm the CEO and I'm also a co-founder at Cockroach Labs. If I had to choose an inventor, well, obviously Edison is a great one. I would say Nikola Tesla, just out of admiration for his ability to think amazing thoughts even though he was very misunderstood in his lifetime. Peter: Hi. My name's Peter Mattis. I'm a co-founder at Cockroach Labs and the VP of Engineering. For an intro question, which inventor, living or dead, would you most like to meet? I would choose Thomas Edison. I really believe that phrase, the quote he has about invention is 1% inspiration and 99% perspiration. Having the idea is only a very small part of invention. Actually changing, transforming that into a reality is where all the work is. Ben: My name is Ben Darnell. I'm a co-founder and also the CTO at Cockroach Labs, and Spencer and Peter took the obvious inventor choices. I'm going to go with an obscure one. I'm not sure how to pronounce his name exactly, but it's Vannevar Bush. He worked in the '40s and '50s and developed a lot of proposals for things that presage the modern internet and hypertext and that sort of thing. Peter: All right. How did we all meet? I met Spencer back in 1993 at Berkeley. He was actually my brother's roommate in college, and I remember, I think, coming out to visit them or something and seeing his roommate, my brother's roommate, Spencer, and I had seen him walk around in bare feet. That was my first memory of meeting Spencer, walking around in bare feet. Later on we actually had a number of classes, computer science classes together. The friendship blossomed from there. Spencer: I'm pretty sure, since we were at Berkeley, I wasn't in bare feet. I must've been in socks and Birkenstocks or something equally dorky. I remember my roommate James, who wasn't a computer science major, he was mechanical engineering, and he was talking about what a great programmer his little brother Pete was. I was like, \"Oh, yeah. Whatever. I'm sure he's not that great,\" and then when I met him, I was pretty blown away. Pete and I, actually the very first project we decided to work on was the GIMP, so that was back in '93, '94. I think we worked on it until '97. Ben: Yeah, so I met these guys a little bit later in 2002. I had just graduated college and started my first job at Google, and I got assigned to the same team as Spencer who was starting at about the same time, and we spent the next couple years working on a number of infrastructure projects at Google while Peter was also working there, a few cubicles over, working on Gmail. Peter: All right. What does 1.0 mean for CockroachDB beta users? Ben: The biggest difference for people who are already using our beta releases is that with 1.0, we're committing to a lot of production readiness features including things like zero downtime rolling upgrades, backup and restore, and that sort of thing. The difference between 1.0 and beta is really about being ready for real production use cases. Peter: There's also, we're committing to a release cycle with 1.0. we're still going to be producing unstable releases every week or every two weeks, but there, we're moving to a release cycle, an every six month release cycle where we release supported, stable releases. They get quite a bit more testing than those biweekly unstable releases get. Peter: All right. What are you excited for in releasing 1.0? Ben: I think the biggest thing is just that this product has been in development for so long. It's been nearly three years now, since the first commit, and it's been developed in isolation to a certain degree, which is strange because we've been so public about our development process, but this is where the rubber really meets the road and we get to see it in real world usage in starting to solve real problems. Spencer: I'm really excited to see how Cockroach is used, and this is also, I think there's another question in here which is about what's terrifying us about releasing 1.0. Maybe these two things are two sides of the same coin for me, but the, Cockroach has a very aspirational technological intent. It's inspired by what Google's done with their database technology, and it's meant to be both scalable and also very survivable, so highly available and there's a lot of complexity in there, and it will be amazing to see it fulfill that potential, but also somewhat terrifying to discover how many unforeseen circumstances and use cases that the technology's applied to. To be candid, I suspect many of those are going to run into trouble early on and that's really what the 1.0 release to the 1.1 release is chiefly concerned with, which is really the ruggedization of the product. Solving the problems that come up as people use Cockroach for many use cases beyond the things that we originally tested it for. Peter: For myself, I'm most excited about hearing success stories, people using Cockroach in production. I really want to hear that story about someone deploying a Cockroach cluster, realizing they're reaching capacity and easily adding a handful of more nodes and having the system just automatically rebalance. On the terrifying side, it is, you put something out there and it will have problems. We've done a huge amount of effort to try to get ahead of those problems, and yet they're bound to happen. The terrifying aspect is, what are those problems going to be that people report, and hopefully there are no disasters. Ben: Yeah. The terrifying part of this product is that as a highly available, survivable transactional database, we're making very strong claims about the consistency and durability of your data. We're asking businesses to trust us with their most critical data. That means that the stakes are very high and we think we've got something that's ready to handle those stakes. Peter: All right, fun question, which technology do you wish had never been invented? Spencer: That's a tough one. This is Spencer here. I'm usually such a techno optimist that it's hard for me to imagine a technology that I wouldn't want to see on the world stage despite the fact that technology obviously has a double edged sword sort of effect. If there was one that I think we could pretty much do without, it would probably have to be nuclear, atomic bombs. Ben: I'm thinking a little smaller here. I really hate talking on the phone, so I'm going to go with the voice telephone. Peter: Yeah. I'm going to go even smaller and say node.js. Who had the great idea of doing JavaScript on the server side? Spencer: All right, let's move on. This is really about the kinds of engineering communications that go on inside any company where someone proposes a design or submits a PR for review, and people begin to argue about it. The term of art here is bikeshedding, so I'm curious what you guys think have been some of our fiercest bikeshedding debates. Peter: I'm not sure about the fierceness of the debate, but we have gone back and forth about how to do logging and structured logging multiple times, and I feel that we still haven't arrived at something that is completely satisfactory to all parties. By far, that has been the area of the code that seemed the most churn. Ben: Yeah. Another related area is error handling. Go doesn't have as clearly established conventions for error handling as other languages, especially when you need to deal with structure in the errors and be able to get information back out of them. We've had a series of changes that have, in some cases, literally gone back and forth and reversed each other in the way that we handle errors in a way that maximizes our ability to extract structure from them later on when we need to. Spencer: Yeah, and somewhat related to that, we've had quite an odyssey over the two plus years of developing Cockroach around how we are able to stop a process in an orderly fashion, and Go itself has contributed to this debate with the addition of something called a context. We created our own mechanism called a stopper, and it has this sort of two phase shutdown process, and even that is very difficult to make work. I think that's a sort of unresolved and continuing conversation. Ben: All right. Here's another fun question. Which nonexistent technology do you wish existed today? Spencer: If I had to choose just one, what would be my favorite? I would have to say it would be great if we had brain uploading technology. Peter: I knew Spencer was going to say that because he talks about that all the time. I'd actually like almost the inverse of that. I would like to be able to download knowledge quickly into my brain a la The Matrix and be able to get kung fu skills in a day. Ben: Yeah. I would have to go with the Star Trek transporter. Spencer: Great. Let's move on to another founder question. This one actually looks pretty interesting. When have you disagreed with a founder who was right? Ben: I don't know. I only disagree with you guys when you're wrong. Spencer: Actually, I have the opposite feeling, which is I often disagree and I'm often the one that is wrong. I've actually learned over the years to censor my own immediate responses to things as much as I can and to dig a little bit deeper before disagreeing with these two guys because they're often right. Peter: Yeah. I can't remember a specific case, but very frequently happens that Spencer comes in. He'll have an idea to do something, and my initial reaction will be like, \"No, we can't do that. It's too hard,\" but I also try to temper that reaction and think about it for a while, and sometimes we come up with a way to actually accomplish that. Spencer: Okay. Well, that was fun. Let's get back to a question about our upcoming 1.0 release. All right. If you could fit one extra feature or capability into 1.0, what would it be? Maybe we should also think on the other side of this question. If there's one thing you could take out of the release, what would it be? Ben: My biggest disappointment in terms of things that didn't make it into 1.0 is the situation with handling certificates and security. I think there's, our security story is pretty strong as long as you're using TLS certificates properly, but we haven't provided the tools to make that as easy as I would like. It's still a fairly arcane process to generate your certificates and distribute and manage them in a secure way, and I was hoping we'd be able to make that process smoother for 1.0, but it looks like that's going to have to wait for the next release. Spencer: I would've loved to get some performance enhancements that I have in mind into 1.0, particular, the speed at which we can drop and truncate tables. Right now we do that in a very naïve way that maintains all of the past states so that you can still query historically, and that's difficult to do. There are clever ways to make that go very quickly, and they didn't make it into 1.0, which we'll have to correct in 1.1. Peter: For myself, it is the troubleshooting tools. We have a number of troubleshooting tools built into CockroachDB, but there's some standard ones that you expect to see in a SQL database. Listing running queries, canceling running queries, and then just some basic tools that apply more to CockroachDB than to other systems. The most frequent cause of having a problem that users encounter when starting up a new CockroachDB cluster is networking issues between their nodes that is, in some ways, outside of our purview. We can't really fix the problems, but we can provide a tool to much more easily diagnose what is going on in those situations. Spencer: Did you want to cover the thing that you're unhappy about making it into 1.0? Peter: I'm not sure if I'm unhappy, but we, over the past year there's been an enormous amount of effort put into stabilizing the core key value transactional store, and while that was happening, we kept on adding features into SQL. I'm not really sure I'm unhappy about this, but there might have been, we might've put too much into the SQL side and maybe should've just diverted additional resources to stabilizing and improving the performance of the transactional key value store, but then this always encounters the problem with just adding more engineers onto a given part of the code base. There wasn't really capacity to put more people onto the core of the system. Spencer: Too many cooks in the kitchen? Peter: Exactly. Spencer: Yeah. It's hard to answer that. There's always a tension in terms of what makes it in and what doesn't. I think we have a fairly good set of things in there. I think post-op we'll be able to say, \"Woo, maybe we shouldn't have put that thing in there because there may be some area of the system that ends up causing a lot of problems for people in the 1.0 release,\" but we're crossing our fingers there. Let's move on to a fun question. Do we have any fun questions? Ben: If you were developing a project using CockroachDB, what technologies are you most likely to use with it? What would that stack look like? Spencer: I don't know if that's a fun question. Well, one of the big reasons that I started working on CockroachDB in the first place was due to the last startup that Peter, Ben and I were all part of called Viewfinder, and we were building a very, what was meant to be a very large scale back end for a private photo sharing system. We built a pretty great one. It was based on DynamoDB so we didn't have transactions and we had to work around that in fairly clumsy ways, and ultimately when we imagined what the system would need to be if it became something as successful as, say, Snapchat, we realized that DynamoDB would have lots of problems that we'd have to work around. This is a, I think this would be a wonderful example of an application that could use something like CockroachDB successfully. Peter: What technologies would I use? Cockroach is primarily written in Go. If I was writing an application that used CockroachDB I would probably stick with Go because I'm extremely familiar with it now. If it had a web front end, I'd go crazy. I'd try to use GopherJS and get Go all the way in to the browser. If I needed an iOS or Android app, I would try to get Go down there as well and just get Go pushed everywhere so it wouldn't have to deal with JavaScript or TypeScript or Java on Android or, what is it, Objective-C or Swift on iOS. That'd be fun to do, just make Go the entire stack there. Ben: Yeah. I have a side project where I'm the maintainer of the Tornado web framework for Python, and so that would definitely be the central part of my stack. As for what I'd put around it, I'm not sure right now because Tornado is an asynchronous framework, and most of the database frameworks in Python are synchronous, and so I don't know. I may end up having to write that part of the stack myself. This always gets me with projects like this because I'm much more interested in building out infrastructure and frameworks and libraries than actually building the final application. Spencer: Yeah, and it actually does remind me, there's a long standing desire I've had since the Viewfinder days to build my own ORM. I think that I might get sidetracked before I actually ever built any kind of project using CockroachDB and I'd end up just building a, what I think is a better ORM. Ben: Didn't we already do that 15 years ago? Spencer: Yeah, and that wasn't exactly, in my opinion, a great ORM. Ben: I don't know. I still like the first version of it. The second version of it wasn't as good. Spencer: I wonder if that still exists at Google. Peter: Probably. Ben: Yeah. I don't think that's ever dying. Peter: I got a kind of fun question for you guys here, but it's a founder question. What would you do if you weren't an engineer? Spencer: Well, I think I might like to take up science fiction writing. Peter: You could always do that as a post career, hobby. Spencer: Yeah. That's something that may be in the cards for me. I certainly enjoy reading it, and sometimes I'm frustrated with the plot lines lacking any kind of likely, or fidelity with the likely future, put it that way. How about you, Pete? Peter: Yeah. I don't know. I have no idea what I would do if I wasn't an engineer. It just seemed I was completely drawn, especially to computer science. Little bit of trivia, I actually entered college as a mechanical engineer thinking I would follow in my father and my brother's footsteps, and I quickly determined that computer science was much more appropriate for me. Computer science was easy, mechanical engineering was super hard. I praise all mechanical engineers out there who can do the calculations and be good mechanical engineers. That wasn't in the cards for me, but being a computer engineer seemed like, I've just been drawn to computers my whole life. I was definitely born at the right time. Ben: Yeah, I'm similar to Peter. I was drawn to computer science very early on and even my main hobbies these days are also programming, so I don't know. I don't really have an alternative. Peter: We'd be useless. Spencer: You guys would be sitting at home, staring at the wall probably. It's a good thing computer science was open to you as a career path. All right. I'm going to read this question because I can't answer it, and this is about me and my dog named Carl, who is also a Cockroach Labs pseudo-employee. He doesn't get paid here but he comes into work often. Ben: He gets paid in snacks. Spencer: He does get paid in snacks and beef jerky. The question is, who snores louder, Spencer or Carl? Ben: I think the whole office can attest to how loudly Carl snores. I'm not sure if many of us know how loudly Spencer snores. Peter: Yeah, well, I actually had a chance to room with Spencer at a recent company ski trip, and yeah. It's a toss up there. Spencer was sawing logs there. The room was shaking like a train was going through it, which is interesting because I was also roommates with him back in college. Not just, we didn't have separate bedrooms, we actually shared a room. I don't remember any snoring back then. This is some kind of recent development. Spencer: Carl, by the way, is a Boston terrier, so they have some trouble breathing cleanly. Let's go on to a 1.0 question. Ben, you want to pick one? Ben: Yeah, so we've been talking about CockroachDB and how it provides multi-active availability, so what is multi-active availability and why does it need its own new term? Peter: Multi-active availability is a setup of a distributed system such that multiple nodes and multiple data centers are active and can process requests at the same time. The reason it needs a, there needs to be a new term for this is that existing terms are often completed in confusing ways. For example, if you just talk, there's standard master-slave or primary-secondary systems, but they don't really cover this setup. Then just saying active-active also is diminishing towards what we're actually accomplishing here. Spencer: Ben, you want to add to that? Ben: Yeah, I think the big difference, why we're using a new term, multi-active availability as opposed to the more traditional active-active availability is twofold. One is that active-active implies two replicas and CockroachDB uses at least three, and also in a traditional active-active setup, in most deployments that use this term today are inconsistently replicated. Each replica is serving traffic, but the differences between them are reconciled asynchronously. In CockroachDB, with its consensus based replication, everything stays consistent between all of the different active replicas. Spencer: Yeah. We also wanted it to be a, indicative of an evolution. Active-active is certainly one sort of predecessor to this idea of multi-active, and high availability is the term that you generally hear in the industry when you're talking about a system that provides better SLAs in terms of up time. We wanted to combine those two, multi-active because it's better than, it requires more than just two and availability is part of this new term because it's an evolution of high availability. Peter: All right. What was a surprising challenge in releasing 1.0? Spencer: Well, we had challenges on a number of fronts, but the one that was most surprising? Peter: I will actually kick off the answer for this. To a certain degree we know about these technologies out there. RocksDB, which we're using as the underlying local key value data store on Cockroach nodes. There's Raft and there's implementations of Raft, and to some degree it seems like, \"Ah, I can take Raft, I can take RocksDB, I can put some glue between and I'll have some kind of distributed, consistent key value store.\" I think the surprise is, and something that we should talk more about, is how hard it is to go from having these component technologies and putting it into an entire working system. Ben: Yeah. I built the first version of Raft that we used which covered about 80% of the Raft paper in a week. Then we've spent the last two years turning to Raft from a key value, from a basic Raft implementation into a full fledged transactional key value store. That's a lot harder than it looks on paper. I'd also say that a big surprise in the road to 1.0 is just how much breadth you have to cover in the SQL standard to be usable with existing libraries and frameworks. We thought that we would get the core part of the SQL standard that everybody uses and then we'd be able to support, hibernate and active record, and all of these kinds of frameworks, but it turns out that each of these frameworks goes pretty deep in terms of how much of the SQL supported by each database that they actually make use of. In order to be compatible with Postgres dialects of SQL, you have to really implement a very large portion of it exactly as Postgres does. Peter: Yeah. The interesting bit for me there is that ORMs, the SQL they generate for the application, it's reasonably standard and not too esoteric, but all these ORMs at startup time, they query the database and essentially you're doing reflection to find out the tables, the columns and whatnot. There is SQL standards information schema for providing that information, but the querying every little detail there, it feels like every tiny corner of what Postgres provides, some ORM utilizes. Spencer: There's also a very long uphill slog in terms of getting us to asymptotically approach a decent level of stability in the system, and this is something that we've written numerous blog posts about in the course of the last year. This was mostly, I'd say it's probably the largest single driving effort within the core part of CockroachDB. It must've spanned three quarters, four quarters, probably, where we were battling both the idiosyncrasies, the different public cloud providers. They have all kinds of problems with their clocks. They have very different strange layers in their storage devices that we found in some. We filed a number of different bugs for them, some of which have been fixed, some of which haven't, and so we've had to do lots of different workarounds. There's a huge amount of work to be done around the rate limiting on our side so that we don't run afoul of various limits that are imposed on the processes. That's been a long running battle, and I think this supports a lot of what Pete was saying, when talking about how much work it is actually to go from the conceptual idea of a distributed database to the actual practical reality of something that can run in lots of different environments. Ben: If you could pick a company to embody the ideal CockroachDB 1.0 use case, which would you choose and what would the project look like? Peter: I don't have a particular company or project in mind, just some attributes of what I would look for. You'd want a project that has a moderate scale. Scale isn't of primary importance, but you wouldn't want to choose something that the expected scale is always going to stick with on a single machine. You want something that the project might start, where it can stick on a, be run off a single machine, but has expected scale in the future where you have a reasonable concern that you would run into some issues there. Also, the project should have some tolerance for risk. This is a 1.0 version of a product. It's a pretty new storage system into any company. It should be approached with a modicum of caution, so there should be some risk tolerance there. Ben: Yeah, I think to Peter's point about scale, I don't think it's a bad idea to use Cockroach even if you don't expect to scale up. I think that the consistent replication of CockroachDB can be valuable even if you're only scaling to a single machine in each replica. I think that we're aiming to be a better database solution even for, at all levels of scale, even from small to large, compared to existing databases, but the ideal case is definitely something that has the potential to grow and need the scalability. Peter: Well that’s the last of our questions. Thank you for listening to our first ever Cockroach Labs podcast. For any developers out there, take our 1.0 for a spin. And if you're in San Francisco or New York City, come to our meetups! Ben will be giving an under the hood look at everything that went into making CockroachDB production ready. He will be in SF tonight May 11, and back in NYC on Wednesday May 17. You can RSVP on the cockroachDB user group pages on meetup.com. Thanks again everyone. Until next time.", "date": "2017-05-11"},
{"website": "CockroachLabs", "title": "Local and distributed query processing in CockroachDB", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/local-and-distributed-processing-in-cockroachdb/", "abstract": "When a CockroachDB node receives a SQL query, this is approximately what happens: The pgwire module handles the communication with the client application, and receives the query from the client. The SQL text is analyzed and transformed into an Abstract Syntax Tree (AST). This is then further analyzed and transformed into a logical query plan which is a tree of relational operators like filter, render (project), join. Incidentally, the logical plan tree is the data reported by the EXPLAIN statement . The logical plan is then handed up to a back-end layer in charge of executing the query and producing result rows to be sent back to the client. There are two such back-ends in CockroachDB: a local execution engine and a distributed execution engine . Local query processing The local execution engine is able to execute SQL statements directly on the node that a client app is connected to. It processes queries mostly locally , on one node: any data it requires is read on other nodes in the cluster and copied onto the processing node to build the query results there. The architecture of CockroachDB's local engine follows approximately that of the Volcano model , described by Goetz Graefe in 1993 ( PDF link ). From a software architect's perspective, each node in the logical query plan acts like a stateful iterator (e.g. what Python generators do): iterating over the root of the tree produces all the result rows, and at each node of the plan tree, one iteration will consume zero or more iterations of the nodes further in the tree. The leaves of the tree are table or index reader nodes, which issue KV lookup operations towards CockroachDB's distributed storage layer. Example logical plan for: SELECT cust_id, address FROM customer WHERE name LIKE 'Comp%' AND state = 'CA' Assuming primary key is cust_id and an index on customer(name) . From a code perspective, each relational operator is implemented as an iterator's \"next\" method; while a query runs, the tree of iterators is processed sequentially: each node's \"next\" method waits until the source nodes have completed their own \"next\" method call. From the outside, the query execution logic processes data and makes decisions (e.g. keep/remove a row, compute derived results) row by row. The processing is essentially sequential. The main characteristic of this engine is that it is relatively simple . The code for this engine can be reviewed and validated for correctness using only local reasoning; we (the CockroachDB developers) have come to trust it the most. Also, because the processing is performed locally, it can deliver results very fast if all the data it needs is available locally (on the same node), and/or when there are only few rows to process from the source tables/indices. Parallelized local processing for updates A common pattern in client apps is to issue multiple INSERT or UPDATE (or UPSERT, or DELETE) statements in a row inside a single session/transaction. Meanwhile, data updates in CockroachDB necessarily last longer than with most other SQL engines, because of the mandatory network traffic needed for consensus. We found ourselves wondering: could we accelerate the processing of data writes by executing them in parallel? This way, despite the higher latency of single data-modifying statements, the overall latency of multiple such statements could be reduced. This is, however, not trivial. The standard SQL language, when viewed as an API between a client app and a database server, has an inconvenient property: it does not permit concurrent processing of multiple queries in parallel. The designers of the SQL language, especially the dialect implemented by PostgreSQL and that CockroachDB has adopted, have specified that each SQL statement should operate \"as if the previous statement has entirely completed.\" A SELECT statement following an INSERT, for example, must observe the data that has just been inserted. Furthermore, the SQL \"API\" or \"protocol\" is conversational : each statement may have a result, and the client app can observe that results before it decides which statement will be run next. For example UPDATE has a result too: the number of rows affected. A client can run UPDATE to update a row, and decide to issue INSERT if UPDATE reports 0 rows affected (the row doesn't exist). These semantic properties of the SQL language are incredibly useful; they give a lot of control to client applications. However, the choice that was made to include these features in SQL has also, inadvertently, made automatic parallelization of SQL execution impossible. What would automatic parallelization look like? This is a classic problem in computer science! At a sufficiently high level, every solution looks the same: the processing engine that receives instructions/operations/queries from an app must find which operations are functionally independent from the operations before and after them . If a client app / program says to the processing engine \"Do A, then do B\", and the processing engine can ascertain that B does not need any result produced by A, and A would not be influenced if B were to complete before it does, it can start B before A completes (presumably, at the same time), so that A and B execute in parallel. And of course, the result of each operation reported back to the app/program must appear as if they had executed sequentially. With standard SQL, this is extremely hard to determine as soon as data-modifying statements are interleaved with SELECTs on the same table. In particular, it is hard to parallelize SELECT with anything else, because in order to determine which rows are touched by a specific INSERT/UPDATE, and whether these rows are involved in a SELECT close by, the analysis required would amount to running these statements, and thus defeat parallelism upfront. It is further impossible to parallelize multiple standard INSERT/DELETE/UPSERT/UPDATE statements, because each of these statements return the number of rows affected, or even data from these rows when a standard RETURNING clause is mentioned, and parallelization would cause these results to be influenced by parallel execution and break the semantic definition that the results must appear as if the statements execute in sequence. This is why there is not much CockroachDB can do to parallelize data updates using standard SQL syntax. However, when discussing this with some of our users who have a particular interest in write latencies, we found an agreement: we could extend our SQL dialect to provide CockroachDB-specific syntax extensions that enable parallel processing . This was found agreeable because the one-shot upfront cost of updating client code to add the necessary annotations was acceptable compared to ongoing business costs caused by higher-latency transactions. The detailed design can be found in our repository. To exploit this new feature, a client app can use the special clause RETURNING NOTHING to INSERT/DELETE/UPSERT/UPDATE. When two or more data-modifying statements are issued with RETURNING NOTHING, the local execution engine will start them concurrently and they can progress in parallel. Only when the transaction is committed does the engine wait until completion of every in-flight data update. Distributed query processing in CockroachDB Next to the local engine, CockroachDB also provides a distributed execution engine. What this does is to delegate parts of the processing required for a single SQL statement to multiple nodes in the cluster, so that the processing can occur in parallel on multiple nodes and hopefully finish faster. We can also expect this to consume less network traffic, for example when filters can be applied at the source. Why, and how? This blog post details the why and outlines the how. We will dedicate a couple separate articles to further explain how it works. Dispelling the false idols The usual motivation for a distributed processing engine is the observation that the data for a query is often spread over multiple nodes in the cluster. Instead of bringing the data onto a single processing node, intuition suggests we could ship the computation where the data is stored instead, and save processing time (= make queries faster). However, at a high level, this motivation is weak: if it were for only this motivation, there is a large range of possible solutions that we would have explored besides query distribution. For example, one could argue that there are already good non-distributed solutions to improve performance. A strong piece of wisdom crystallized over the past 30 years can be summarized thus: data workloads in production code have been historically observed to consist of either small bursty transactions that need up-to-date and consistent views of the data, but only touch very few rows ( OLTP workloads ), or long wide-spanning read-only transactions that touch a lot of rows, but don't usually need a very up-to-date and consistent view of the data (analytical workloads, online or not). From this observation, one can argue that OLTP workloads only need to talk to very few nodes in a distributed storage system (because primary and secondary indexes will narrow down the work to a few rows in storage), and analytical workloads can be run on materialized views that are maintained asynchronously in a separate system , optimized for fast throughput at the expense of consistency (as they do not need to update anything). In either case, distributed processing is not an obvious value-add. Another conventional motivation for distributed processing is a challenge to the aforementioned wisdom, acknowledging the rise of new workloads in internet services: it is now common to find OLTP workloads that need to read many rows before they update a few, and analytical workloads that benefit from reading from very up-to-date data. In both cases, distributed processing would seem to provide an effective technical solution to make these workloads faster. However, again this motivation is weak at a high level, because since these workloads have become commonplace, we have already seen seemingly simpler and effective technology and standards emerge to address precisely these use cases. For transactional workloads containing large reads before an update decision is taken, the common approach is to use suitable caching. Memcached and related technology are an instance of this. For analytical processing in want of up-to-date data, an extra replication stack that maintains consistent materialized views ensures that the analytics input is both up-to-date and fast to access. Good caches and transaction/event logging to maintain materialized views externally are well-known and effective technical means to achieve this, and the corresponding technology relatively easier to provide by vendors than general-purpose distributed processing engines. This back and forth between the expression of new computing needs and the design of specialized solutions that accelerate them is the staple diet of computer and software architects and, let's face it, the most recurring plot device in the history of computing. After all, it \"just works,\", right? It works, but it is so complicated! \"Complicated\", here, being an euphemism for expensive to use . Our motivation for distributed query processing in CockroachDB This is the point in the story where we reveal the second most recurring plot device in the history of computing: rejection of complexity . This is how it goes: \"As a programmer, I really do not want to learn about all these specialized things. I just want to get my app out!\" \"As a company owner, I do not want to have to deal with ten different technology providers to reach my performance numbers. Where's my swiss army knife?\" Without a general-purpose distributed computing engine, a developer or CTO working with data must memorize a gigantic decision tree: what kind of workload is my app throwing at the database? What secondary indices do I need to make my queries fast? Which 3rd party technology do I need to cache? Which stack to use to keep track of update events in my data warehouse? Which client libraries do I use to keep a consistent view of the data, to avoid costly long-latency queries on the server? The cognitive overhead required to design a working internet-scale application or service has become uncomfortably staggering, and the corresponding operational costs (software + human resources) correspondingly unacceptable. This is where we would like to aim distributed processing in CockroachDB: a multi-tool to perform arbitrary computations close to your data . Our goal in the longer term is to remove the burden of having to think about atomicity, consistency, isolation and durability of your arbitrarily complex operations, nor about the integration between tools from separate vendors. In short, we aim to enable correctness, operational simplicity and higher developer productivity at scalable performance , above all, and competitive performance in common cases as a supplemental benefit. This is a natural extension for building CockroachDB in the first place. After all, CockroachDB rides the distributed sql wave, fuelled by the renewed interest in SQL after a period of NoSQL craze: the software community has tried, and failed, to manage transactions and schemas client-side, and has come to acknowledge that delegating this responsibility to a database engine has both direct and indirect benefits in terms of correctness, simplicity and productivity. Our distributed processing engine extends this principle, by proposing to take over some of your more complex computing needs. This includes, to start, supporting the execution SQL queries, when maintaining a good combination of secondary indices and materialized views is either impractical or too detrimental to update performance . It also includes supporting some analytical workloads that regularly perform large aggregations where the results need up-to-date input data . Eventually, however, we wish to also cater to a larger class of workloads. We are very respectful of the vision that has produced Apache Samza , for example, and we encourage you to watch this presentation \"Turning the Database Inside Out\" by Martin Kleppmann to get an idea of the general direction where we're aimed. \"Batteries included!\" So we are set on implementing a distributed processing engine in CockroachDB. It is inspired by Sawzall and at a high-level works as follows: The request coming from the client app is translated to a distributed processing plan , akin to the blueprint of a dataflow processing network . The node that received the query then deploys this query plan onto one or more other nodes in the cluster. This deployment consists of creating \"virtual processors\" (like little compute engines) on every node remotely, as well as the data flows (like little dedicated network connections) between them. The distributed network of processors is launched to start the computation. Concurrently, the node handling the query collects results from the distributed network and forwards them to the client. The processing is considered complete when all the processors stop. We highlight again that this is a general-purpose approach: dataflow processing networks are a powerful model from theoretical computer science known to handle pretty much any type of computation. Eventually, we will want our users to become able to leverage this general-purpose tool in arbitrary ways! However, in an initial phase we will restrict its exploitation to a few common SQL patterns, so that we can focus on robustness and stability. In CockroachDB 1.0, for example, the distributed engine is leveraged automatically to handle SQL sorting, filtering, simple aggregations and some joins . In CockroachDB v1.1, it will take over more SQL aggregations and joins automatically. We will evaluate the reaction of our community to this first approach to decide where to extend the functionality further. Plans for the future in CockroachDB Lots remain to be done Truth be told, there are some complex theoretical questions we need to learn to answer before we can recommend distributed processing as a general tool. Some example questions we are working on: While data extraction processors can be intuitively launched on the nodes where the data lives, other processors like those that sort or aggregate data can be placed anywhere. How many of these should be launched? On which cluster nodes? How to ensure that computations stay close to the data while CockroachDB rebalances data automatically across nodes? Should virtual processors migrate together with the data ranges they are working on? What should users expect when a node fails while a distributed query is ongoing? Should the processing resume elsewhere and try to recover? Is partial data loss acceptable in some queries? How does distributed processing impact the performance of the cluster? When a node is running virtual processors on behalf of another node, how much throughput can it still provide to its own clients? How to ensure that a large query does not exhaust network or memory resources on many nodes? What to do if a client closes its connection during a distributed computation? We promise to share our progress on these aspects with you in subsequent blog posts. Summary: SQL processing in CockroachDB You now know that CockroachDB supports two modes of execution for SQL queries: local and distributed . In the local execution engine, data is pulled from where it is towards the one node that does the processing. This engine contains an optimization to accelerate multiple data updates , given some annotations in the SQL statements ( RETURNING NOTHING ), by parallelizing the updates locally, using multiple cores. In the distributed execution engine, the processing is shipped to run close to where the data is stored, usually on multiple nodes simultaneously. Under the hood, we are building a general-purpose distributed computing engine using dataflow networks as the fundamental abstraction. We plan to expose this functionality later to all our users to cater to various distributed processing workloads, but for the time being we just use it to accelerate some SQL queries that use filtering, joins, sorts and aggregations , in particular those that you may not be able to, or do not want to, optimize manually using classical techniques (e.g. indexes or asynchronous materialized views). Both engines can be active simultaneously. However, because we are working hard on distributed execution, we want users to experiment with it: we thus decided to make distributed execution the default , for those queries that can be distributed. You can override this default with SET , or you can use EXPLAIN(DISTSQL) to check whether a given query can be distributed. Subsequent blog posts will detail further how exactly this is achieved. And there is so, so, so much more we want to share about this technology. We'll write more. Stay tuned. But before we go... Does building distributed SQL engines put a spring in your step, then good news — we're hiring! Check out our open positions here .", "date": "2017-06-08"},
{"website": "CockroachLabs", "title": "The Path from Beta to 1.0", "author": ["Diana Hsieh"], "link": "https://www.cockroachlabs.com/blog/coming-soon-what-to-expect-in-cockroachdb-1-0/", "abstract": "A version of this blog post was originally published on May 1, 2017 and has been modified to provide the newest information available. With the recent 1.0 release, CockroachDB is now a production-ready database. 1.0 showcases the core capabilities of CockroachDB, while also offering users improved performance and stability with a cloud-native architecture that flexibly supports all manner of cloud deployments. It encompasses the core features that allow our users to run CockroachDB successfully in production. Now that the dust has settled on our 1.0 release, I wanted to share how we defined our target use case and dive into the actual product features that support running that use case in production. What We Knew Going into Beta Ask any one of our engineers, and they’ll say that CockroachDB is the database they wish they had before joining Cockroach Labs. Many of them are keenly familiar with the horror stories of sharding SQL databases or cleaning up after NoSQL databases that take haphazard approaches to consistency and integrity. The underlying issue is that existing databases are outdated. They can’t keep up with users who access data more frequently from different locations and across multiple platforms. They also require complex configuration and setup, making it difficult to take full advantage of the flexibility and power of the cloud. Instead of supporting business growth and innovation, databases often become a source of developer friction and technical debt. We knew we wanted to build a better database that addressed these issues, but we also wanted to make sure that we were addressing the needs of the larger community. That meant spending time getting to know our early users. Getting to Know Our Early Users We started out by conducting user interviews across a variety of seniority levels and industries to understand how users make database decisions. Given that databases support mission-critical applications, developers and operators prefer to work with familiar products even if they require manual workarounds and incur high operational costs. Further, the cost of switching databases is high - there is the immediate cost of reworking existing applications and the hidden cost of developing institutional knowledge to build on and maintain a new database. Being a better alternative to existing market solutions would not be enough. Instead, we needed to enable a new use case where the value proposition of CockroachDB was so compelling that developers and operators would build their most mission critical applications on top of it. Defining Our Target Use Case We homed in on distributed OLTP as our core use case. OLTP (Online Transaction Processing) covers workloads that are transactional in nature, including financial transaction processing, inventory management, e-commerce, and a wide variety of web applications. OLTP databases usually support mission critical applications that drive direct business success. Companies need their OLTP databases to grow as they grow while keeping their data safe and available at all times. To achieve this, companies end up “distributing” their databases to span more than one server, often sacrificing transactions in the process.Deployments that span multiple datacenters get even more complicated, with complicated asynchronous replication setups that require significant overhead to deploy and maintain. CockroachDB enables distributed OLTP by offering the strong consistency of SQL in combination with the scalability of NoSQL. This allows businesses to scale their OLTP databases without having to make application-level changes even as their database grows across multiple datacenters. However, the above discussion fails to cover a larger benefit that CockroachDB offers: the ability to build a cloud-native architecture that properly leverages the cloud. CockroachDB come with no single points of failure, the ability to survive disasters, automatable operations, horizontal scalability, and no vendor lock-in. With CockroachDB, users can add machines to their clusters and watch as CockroachDB automatically rebalances data across nodes. Replication also comes built-in, so that even if nodes fail, other nodes can pick up the slack, continuing to return correct and consistent data. CockroachDB also works across multiple datacenters, so customers can expect their data to stay safe even in the face of a datacenter-wide failure. Achieving Production-Readiness with CockroachDB 1.0 Having decided on our target use case, we needed to define what it meant to be production ready. For us, this meant building out the core differentiating features that would support a cloud-native distributed OLTP deployment, coupled with stability, performance, and usability. Our Core Features The two core features we settled on that were vital to distributed OLTP use cases were distributed SQL and multi-active availability. The SQL interface offers a powerful and familiar API that developers can use to define their transactional workloads and comes with a robust community and existing ecosystem of tools. CockroachDB supports a large fraction of the standard SQL footprint, including secondary indexes, foreign keys, JOINs, and aggregations. CockroachDB also supports distributed ACID transactions and query execution. To application developers, a CockroachDB cluster appears as a single logical database. To operators, a CockroachDB cluster can grow with data storage and throughput needs by simply adding symmetric CockroachDB nodes. In addition to supporting the SQL API, we also wanted to support a distributed deployment of CockroachDB in a manner that maintains strong consistency, fault tolerance, and scalability. That leads us into our second core feature, multi-active availability. Multi-active availability is an evolution from primary-secondary and active-active setups to a deployment which uses consensus-based replication to consistently replicate amongst three or more replicas, all actively serving R/W client traffic. Multi-active availability allows client traffic to be dynamically load balanced to utilize available replicas, instead of relying on fragile failover mechanisms. Because it employs strongly-consistent replication, multi-active availability avoids stale reads and the need for conflict resolution. Replication is strongly consistent at the range level with each range taking part in a Raft consensus group . A typical CockroachDB deployment across three datacenters can automatically scale and rebalance as capacity is added and survive datacenter level disasters. Stability With our 1.0 release, we wanted our users to not only be able to test and utilize our core features, but also feel comfortable running us in production supporting operational workloads with minimal downtime. We run multiple CockroachDB clusters across numerous cloud deployments under various chaos situations to ensure stability and availability. We also run rigorous testing suites before any code is committed and engaged Kyle Kingsbury to run his Jepsen tests against CockroachDB to guarantee data integrity. While running us in production, our users can also expect reduced downtime with support for no downtime rolling upgrades and certificate rotations. Our CockroachDB Enterprise backup and restore offering allows users to run periodic and incremental backups efficiently, while restoring in a distributed fashion. Performance As with any database utilizing consensus-based replication, CockroachDB queries incur network latency when achieving quorum on reads and writes. We have done significant work to improve query performance with read leases, parallelized SQL execution, and optimizations under high contention scenarios. For users looking to deploy us across multiple datacenters, load-based lease rebalancing allows ranges that receive the most traffic to serve faster reads. In order to enable distributed SQL, CockroachDB uses a sorted mechanism with data broken up into ranges instead of a hashing mechanism to distribute data. This allows us to do efficient scans and support typical SQL functionalities. We also built a distributed SQL layer that processes read queries in a distributed fashion to achieve a significant increase in query speeds. Usability In addition to stability and performance, we wanted to make it easier for developers and operators to adopt CockroachDB. We leverage the PostgreSQL wire protocol to tap into the existing and robust PostgreSQL ecosystem . Developers can use existing client drivers and ORMs to interface with CockroachDB, to the extent that they are compatible with CockroachDB. CockroachDB is also very flexible and configurable, making it easy for operators to meet modern availability requirements. Operators can specify zone configurations at a cluster, database, and table level. They can also deploy CockroachDB across clouds or with popular container orchestration tools. Being Successful with CockroachDB 1.0 CockroachDB is still a relatively young database and there are several caveats to keep in mind for 1.0. First of all, although CockroachDB uses the PostgreSQL wire protocol, it doesn’t support all of PostgreSQL’s functionality - particularly PostgreSQL-specific extensions. This means that not all tools that work with PostgreSQL will work with CockroachDB. Second, a distributed system comes with network latency. Certain data models and workloads come with a higher cost. Thirdly, there are still many optimizations to be made at the SQL layer for faster queries, so CockroachDB in its current form is a better fit for transactional workloads versus analytical workloads. As with any 1.0 release, it is important to first test specific workloads against CockroachDb before jumping into a production environment. We are big fans of the open source community and there are several channels available to users for help. Our Gitter channel, forum, and GitHub are actively monitored by our engineering team. Please engage with us with any issues and suggestions you have. Looking Forward CockroachDB 1.0 is just the first step towards building a cloud-native SQL database. We will continue improving, optimizing, and testing our database internally, and hope to hear from our community running us in production as well. Moving forward, we have several exciting new features that will only continue to make running databases at a global scale easier for everyone. This includes improved query performance, more integrations with deployment tooling, and more powerful deployment configurations. We expect our coverage of use cases to only expand with time. We will be sharing our upcoming 1.1 roadmap soon, and look forward to receiving your feedback.", "date": "2017-06-01"},
{"website": "CockroachLabs", "title": "CockroachDB + ActiveRecord (and Ruby on Rails!)", "author": ["Jordan Lewis"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-hearts-activerecord-ruby-on-rails/", "abstract": "``` Update on June 17, 2020: since initially publishing this post in 2017, we’ve now completed full support for Active Record . Also, CockroachDB does now fully support Common Table Expressions. ``` In our first blog post on ORM support , Cuong Do detailed how we provided support for Hibernate , a full-featured Java ORM, in CockroachDB. He also discussed our general motivation for providing support for popular ORMs in CockroachDB: to make it as easy as possible for developers to build applications with CockroachDB using a variety of languages and frameworks. Today, we’ll be discussing what went into providing support for another ORM: ActiveRecord! We have existing documentation on how to use ActiveRecord with CockroachDB , but we wanted to explain a little bit more about this support and go into some detail about how we built it. Why ActiveRecord? ActiveRecord is the ORM that powers Ruby on Rails , one of the most popular open-source frameworks out there for developing web applications, and one of the most popular open-source projects of all time. Rails has enjoyed wide popularity since its release in 2003, boasting status as the foundation for hundreds of thousands of web applications, including those of big-name companies like Twitch, Square and GitHub. Rails also has a great community that has inspired many programming newcomers to build their first apps, including global organizations like Rails Girls that are breaking down traditional barriers to learning to program in an era where anyone should be able to build an idea and put it on the web. Besides making CockroachDB robust, scalable and survivable, one of our guiding principles here is to make it easy to use. Operating a modern SQL database at scale should be easy - and it’s one of our goals to make CockroachDB a joy to develop for. So it’s only natural that we’d want to provide support for Ruby on Rails, a framework with the guiding principle of optimizing for developer happiness! The results from our developer surveys, market research, and informal questionnaires on our developer forum only confirmed what we already suspected - that we definitely needed to support Rails users who wanted to use CockroachDB. Background We made the decision early on to support the PostgreSQL wire protocol, so that client drivers that work with PostgreSQL would work with CockroachDB without any extra fuss. This has enabled support for the most popular PostgreSQL drivers for at least 9 languages without requiring any language-specific work. When you have a team as small as ours, any kind of multiplicative effect like this really goes a long way. However, there was one hazard of supporting PostgreSQL’s wire protocol that we didn’t foresee when we made the decision: PostgreSQL’s wide and full-featured SQL API. It turns out that when people implement language drivers for PostgreSQL, they tend to expose a lot of that API for use of higher-level components like ORMs. As we saw in the Hibernate blog post, ORMs tend to need to do a lot of really complicated introspection on the data and schemas stored in the database to get their job done, and when programmers are implementing ORM support for Postgres, it’s hard to resist using the fancier and more powerful bits of Postgres’s API surface. This creates a challenge for us at Cockroach Labs: we’ve already got our hands full trying to build a scalable, survivable and consistent database; we’d be foolish to presume that we could simultaneously rebuild 30 years of Postgres’s features into CockroachdB v1.x! On the other hand, we also want to avoid reinventing the wheel on the ORM side - implementing a CockroachDB adapter for every ORM out there is not something that we’re able to do with the resources that we have. So, how do we walk that fine line? For Hibernate, we chose to go the route of teaching CockroachDB to properly understand all of the queries that Hibernate issues to Postgres, since we thought that the features that enable them were popular enough that they’d get a good deal of use across many ORMs and client drivers. What about ActiveRecord? Common Table Expressions credit: hyperbole and a half We started off using the same approach for ActiveRecord that we did for Hibernate: implement all the (Postgres) things! We made a valiant attempt, adding support for at least 12 features that ActiveRecord uses in Postgres. In the end, however, our efforts were stymied by ActiveRecord’s use of a really cool PostgreSQL feature that we don’t support yet: Common Table Expressions . What are Common Table Expressions (CTEs), you ask? They’re a common SQL extension that permits breaking up large queries into smaller chunks, which, surprise, makes the lives of developers easier! They allow a query to be built up as a series of temporary views in an efficient way, which can clarify what the query is doing if it’s very long or complicated. ActiveRecord uses CTEs in the following statement, which is used to retrieve the primary keys for a table: WITH pk_constraint AS (\n  SELECT conrelid, unnest(conkey) AS connum FROM pg_constraint\n  WHERE contype = 'p'\n    AND conrelid = '\"table\"'::regclass\n), cons AS (\n  SELECT conrelid, connum, row_number() OVER() AS rownum FROM pk_constraint\n)\nSELECT attr.attname FROM pg_attribute attr\nINNER JOIN cons ON attr.attrelid = cons.conrelid AND attr.attnum = cons.connum\nORDER BY cons.rownum See that WITH pk_constraint AS statement at the beginning of the query, and the , cons AS half way down? That’s a CTE! The query creates pk_constraint as a temporary view that contains the result of the SELECT query within parentheses, uses that view to create another temporary view, cons , that runs a window function over pk_constraint , and then uses cons to produce the final query result at the end. Implementing CTEs correctly is tricky, and requires some backend infrastructure improvements that we aren’t quite ready to start working on. So, we decided to punt on implementing them until our SQL backend is good and ready. Stay tuned for more developments on that topic! In the meantime, though, we were stuck - we couldn’t proceed with our standard strategy of implementing all of the features that the Postgres adapter expects because we found one that was significantly more weighty in implementation cost than, say, adding a new table to our pg_catalog implementation. A Hybrid Solution: a lightweight adapter We decided to proceed by taking a hybrid approach between creating a complete ActiveRecord database adapter and implementing all of the features that the Postgres adapter uses in our backend. We created a lightweight ActiveRecord adapter gem that extends from the existing Postgres adapter, modifying just the bits that we couldn’t support, such as that query that uses a CTE. Unsurprisingly, it’s called activerecord-cockroachdb-adapter , and you can find it on GitHub . This gem, which is at present available only for Rails 5.x, allows Rails users to use CockroachDB as their backend database. All that’s necessary is changing a few configuration options in config/database.yml to instruct Rails to use our adapter, like so: default: &default\n  adapter: cockroachdb\n  port: 26257\n  host: your-host\n  user: your-user For a complete example of how to use CockroachDB with Ruby on Rails, check out our example repo . Summary: we did our homework, please share your love too! To all our friends using ActiveRecord and Ruby on Rails, we have some good news: we both delivered a bit more PostgreSQL compatibility and the bit of necessary glue in the shape of an ActiveRecord adapter gem! We consider this to be an important achievement in our ongoing quest to bring CockroachDB's scalability to a wider audience. The story is not finished yet, however: we need you, friendly Ruby developer, to help us improve it further. Please, try it out in your apps and give us your feedback! 💖💖💖", "date": "2017-06-15"},
{"website": "CockroachLabs", "title": "The Limits of the CAP Theorem", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/limits-of-the-cap-theorem/", "abstract": "The CAP theorem is a fundamental part of the theory of distributed systems. It states that in the presence of partitions (i.e. network failures), a system cannot be both consistent and available, and must choose one of the two. CockroachDB chooses consistency, and is therefore a CP system in the terminology of the CAP theorem (as opposed to an AP system, which favors availability over consistency). Both consistency and availability are crucial to any business, and you might wonder how you are expected to choose between such important goals. In this post I’ll explain how CAP-Consistent systems can still be highly available, how the CAP theorem applies to CockroachDB, and why consistency is the better choice for most databases. High Availability The CAP theorem defines availability in strict binary terms: a system is either CAP-Available or it is not. However, in the language of high availability and service level agreements, the term “availability” is also used, but it describes a continuum instead of a binary condition. A system might guarantee that it is available 99.99% of the time (“four nines”, allowing for less than an hour of downtime per year); 100% availability is generally regarded as unrealistic; engineering for high availability requires estimating the likelihood and severity of different kinds of outages, and balancing that against the cost of mitigating them. A CAP-Consistent system will sometimes be unavailable due to network partitions. But all systems, even CAP-Available ones, will sometimes be unavailable for all kinds of reasons. In a well-run network there is no reason to believe that the kind of partitions where CAP tradeoffs are relevant will be any more common than other kinds of outages. Dr. Brewer has gone so far as to say that partitions in Google’s network are so rare that the Spanner database is “technically CP” but “effectively CA” . This claim muddies the waters around this already-confusing subject, but it does illustrate that the loss of availability implied by the CAP theorem matters only within a narrow margin. Making the Right Tradeoffs Distributed systems engineering is full of tradeoffs , with tensions between a variety of concerns including consistency, availability, performance, and flexibility. The CAP theorem focuses on a single narrow tradeoff between consistency and availability, but this doesn’t cover all the causes of or solutions to unavailability. Outages can be caused by a variety of factors that the CAP theorem doesn’t consider, such as single-node hardware failure, application bugs, or operator error. And when the system is considered as a whole, even network partitions can be handled in ways that increase availability without sacrificing consistency. Choosing CAP-Availability buys very little in effective availability, but the loss of consistency pushes a significant amount of complexity into the application code and has a high price in engineering effort. For example, consider an application deployed across three datacenters, with client traffic load balanced across all three. If one of those datacenters is knocked offline by a network failure, client traffic directed to the offline datacenter would experience an outage whether the underlying database is CAP-Consistent or CAP-Available. When the load balancer is updated to direct traffic to the live datacenters (which can be based on automated health checks), service is restored, no matter how the underlying database handles the partition. The only time that a CAP-Available system would be available when a CAP-Consistent one would not is when one of the datacenters can’t talk to the other replicas, but can talk to clients, and the load balancer keeps sending it traffic. By considering the deployment as a whole, high availability can be achieved without the CAP theorem’s requirement of responses from a single partitioned node. If the increase in availability in a CAP-Available system is small, then why choose one over a CAP-Consistent one? One reason is write latency: consistent systems must coordinate between different nodes during writes to provide that consistency (and depending on the system, consistent reads may also incur higher coordination costs). Since inconsistent systems allow for the possibility of missing data, they can return responses more quickly. This may be an appropriate choice for applications where speed is more important than robustness. CAP in CockroachDB CockroachDB is a CAP-Consistent (CP) system: each piece of data lives on at least three replicas, and writes require that a majority of those replicas are able to communicate with each other. For reads, one of those replicas is granted a lease, or temporary ownership of a range of data, that allows it to serve reads without communicating with the others for a few seconds. In the event that the leaseholder is partitioned away from the other replicas, it will be allowed to continue to serve reads (but not writes) until its lease expires (leases currently last 9 seconds by default), and then one of the other two replicas will get a new lease (after waiting for the first replica’s lease to expire). This ensures that the system recovers quickly from outages, maximizing availability even though it does not satisfy the CAP theorem’s all-or-nothing definition of availability. CockroachDB’s foundation of strong consistency is what makes it possible to offer a distributed database with the expected guarantees of a traditional non- distributed SQL database while still being a highly available system. Without consistency, application developers would have to work around surprising behavior like secondary indexes that don’t have all the data (or that have pointers to records that haven’t been replicated yet), or deal with the potential loss of data when a node fails. For most applications, a CAP-Consistent database like CockroachDB is often the better choice, despite potentially longer latencies, because it offers a simple contract to the application developer: The most recent write is always visible to subsequent readers (single register linearizability). Other developers cannot compromise an app’s consistency with optional write settings. In the event of partitions, the system will block rather than return inconsistent data.", "date": "2017-06-27"},
{"website": "CockroachLabs", "title": "Hackathon Wins: My Experience With CockroachDB at NWHacks 2017", "author": ["Trevin Wong"], "link": "https://www.cockroachlabs.com/blog/hackathon-wins-nwhacks-2017/", "abstract": "After winning the Best Node.js App using CockroachDB prize at NWHacks 2017, University of British Columbia student Trevin Wong writes about his experience with the database at the March event. As I awoke to the noise of the SkyTrain, and the rays of sunlight streaming down on my face, I opened my eyes groggily to a beautiful mid-March day. Today was the start of nwHacks2017 , touted as the biggest hackathon to hit the Northwest. I hopped on the bus and arrived at the UBC Life Sciences Atrium, which was to host crowds of students and numerous software companies such as Google, Telus and Hootsuite. It was here that my group would then win the Best Node.js App using CockroachDB prize. Of course, I didn’t know it at the time, but let me take you through my experience of using it and the lessons that I learned. Choosing CockroachDB Originally, my friend Ryan and I had planned to use MySQL for the database of our geo-cached post feed app, known as GeoPost . However, after walking out of a strangely invigorating opening ceremony which encouraged us to take risks, we decided on using CockroachDB. Plus, there was the bonus of possibly winning the prize, as well as the allure of the nicely printed hoodies that all the members at the booth wore. However, the rest of our group wasn’t as enthusiastic as we were upon using it, and so the task of setting up the database was thrust upon me. Unfortunately, I hadn’t the slightest idea of how a database worked, or even how a server communicated with one. Regardless, I dove into the documentation of CockroachDB, and was immediately assaulted by terms such as “nodes” and “clusters”. Nevertheless, I set to work upon configuring the database on my computer. I created a node, then a cluster. Finally, I built a table. It worked, but then, for some strange reason, I thought that it wouldn’t be accessible to other people. So, I set about trying to configure it with Amazon Web Services . Ensue chaos and much confusion. Hosting Woes AWS has something called EC2, which essentially allows you to work on a virtual machine hosted by Amazon. So, I created an account, which needed my credit card information, something I found odd. After that, I was completely lost, and had to run over multiple times to the other hall to receive help from the kind CockroachDB people, who thankfully saw over my confused dealings with AWS, showing me how to connect to the VM using a key, and how to install CockroachDB. We ran into some problems with port forwarding , but as a temporary solution, we decided to simply re-direct all ports to the IP address, which is not recommended if you’re intending to deploy it to the public, since that has some major security issues. ...what I would recommend for anyone who would want to use CockroachDB in the future at a hackathon is to simply host it on your own local machine. But, for the sake of the hackathon, we decided that it was fine, and so we continued onwards. Then, I connected to my VM using the key that I created, and made a table on it. Hurray. However, what I would recommend for anyone who would want to use CockroachDB in the future at a hackathon, is to simply host it on your own local machine and forward your own ports, and deal with AWS later if you’re planning to release it. AWS wasn’t too difficult, but it was a hassle and it would have saved me a couple of hours if I simply hosted the database from my own machine. Trevin, The Database Guy Despite the hassle, however, once it was set up, I was pleasantly surprised how easy it was to manage. The SQL syntax was easy to use and understand, and within minutes, I could get tables up and running with data about our posts and comments sent in from our backend. Unfortunately, thanks to my lack of skill at Javascript and anything web-related, I became the database guy, sometimes editing columns and removing rows whenever needed. While not too exciting, it did make me thankful for CockroachDB’s ease of use. As the rest of our group trudged on, with two of my group-mates working on the Node.js backend, and the ever so talented Ryan continuing to polish the front-end with shiny buttons and logos, I sat around, drank coffee, and complained about how hard Javascript was to use. An excellent use of my time. Later, I did end up deploying the app using Heroku. Finally, one sleepless night later, we had finished the app. It was dreadfully tiring, and I swear I could see visions of localhost appearing now and then. Nevertheless, it was done, and we enjoyed some curry for breakfast, and went on to present our app. Thankfully, the CockroachDB guys seemed to really like it, and thankfully so, since I definitely spilt a lot of tears setting it up. When it came to the ending ceremony, the CockroachDB people called our name, bestowing upon us the title of the Best Node.js App using CockroachDB! It was a surreal experience. I was definitely stunned hearing our names. Why CockroachDB? Now, as I reflect on my experience at nwHacks2017, I’d like to share a little more about why I think CockroachDB would be a good choice to use at your next hackathon. Firstly, how easy it is to use. As a complete newbie to servers and databases, I found CockroachDB very easy to configure. Most of the difficulty I encountered was due in part to Amazon Web Services, who almost charged me $1200 for forgetting to turn off their instance. When setting up CockroachDB, I was calling “cockroach start” and making tables within the first hour. The SQL syntax is very easy to use. Secondly, the potential of scaling up into a legitimate app. If we wanted to, I’m sure we could have made GeoPost into something big. And to handle something big, we would have needed a database capable of handling thousands of posts and comments. By using CockroachDB, it would have been easy to do so by simply just adding more nodes and clusters. So, it’s a good idea to use CockroachDB just in case you land upon a good idea that you’d like to take further. While I can’t speak too much about the actual scalability and survivability since we didn’t take that extra step, I can see how useful it would be. Finally, just some recommendations I have if you decide to use CockroachDB. Install the database on your local machine, and simply forward the relevant ports. This is faster and easier than trying to deal with setting it up on AWS. You can always migrate to AWS later, as well. Concern yourself with only creating and manage 1 node. The documentation suggest that you create a cluster, but since this is a small-scale prototype, you only need one. Familiarize yourself with the drivers needed to communicate from your back-end to CockroachDB, and how to do so. That’s about all I have to say for CockroachDB, and if you ever get see CockroachDB at your local hackathon, I would encourage you to say hi to them and try using their technology. They were among the friendliest and most helpful people I’ve ever met, and I really enjoyed working with them and am definitely going to use CockroachDB again.", "date": "2017-07-13"},
{"website": "CockroachLabs", "title": "Navigating the Exercise-Based Interview", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/navigating-tech-writer/", "abstract": "Interviewing at Cockroach Labs comes with a twist. The interviewing process is unconventional by design---especially for non-engineering positions. The interviews are exercise-based, focused on practical, day-in-the-life style work tasks. Additionally, resumes are removed from the process to fight unconscious bias . The process is crafted thoughtfully and customized for each role being hired for, but it can be a bit daunting for candidates who are used to traditional interviews. As someone who recently experienced the process first-hand, I thought it would be beneficial for potential candidates to hear about my interviewing experience and gain insight into the process. For a little context, I joined Cockroach Labs in July as the new Technical Writer. In this post, I’ll be looking at the interviews from the perspective of a tech writer, but the process applies to all open roles across the company. The typical technical writer hiring process goes something like this: You apply for an online job posting with your resume and work samples. The HR representative conducts a phone interview. If selected, you are called for an in-person interview. The in-person interview generally entails a writing (read: grammar) exercise, followed by routine questions . The interview is usually conducted by two or three tech writers and the hiring manager. In my experience, the typical interview process relies on the candidate telling the hiring team what they can do, rather than showing what they can do. This does not necessarily translate into the candidate being a good fit for the position and the team. By contrast, the interview process at Cockroach Labs is cross-functional and exercise-based, which is unconventional, yet effective in the sense it not only helps the company decide if the candidate is a good match for the position, but also helps the candidate evaluate if the company is a good fit for them. At Cockroach Labs, Show beats Tell When I saw the AngelList job posting for a Senior Technical Writer for Cockroach Labs, I applied for it immediately. As a self-professed startup enthusiast, I love working with startups trying to solve an actual real-world problem. Cockroach Labs fit the bill perfectly. In a few short days, I received a phone call from Lindsay Grenawalt, Head of People Operations. We talked about my technical writing experience and education. The next step was an hour-long phone interview request with Jesse, the Head of Documentation. We talked about my previous work experience, how I would approach certain documentation projects, and my education in technical communication. I thoroughly enjoyed the discussion. As the next step, I was asked to complete a take-home exercise. I was asked to install CockroachDB, create a database and a table, and document a SQL statement. The fascinating thing about the exercise was that I was asked to work on a live document and not an arbitrary fictional exercise. Within a few days of submitting the exercise, I was asked to fly to New York for an in-person interview. Showtime The in-person interview was a writing marathon that I had never experienced in an interview process before. Since Lindsay had informed me about the process beforehand, however, I was prepared for it. In fact, Cockroach Labs will soon be open-sourcing its interview questions, which will be another fantastic way to prepare for the onsite interview in the future. The day started with an information architecture exercise with Sean (Senior Technical Writer). I had been asked to come prepared with an information architecture presentation about a topic of my choice. This served me well since it gave me the opportunity to start the day on a high note by demonstrating how I break down and structure information with an audience in mind. The next two interviews were technical competency interviews. The first one involved a technical discussion with Spencer (CEO) about the distributed transaction model, and the second was about online schema changes with David (Engineer). During these discussions, I got the opportunity to demonstrate how I learn complex topics and collaborate with technical colleagues by asking questions and verifying my understanding of the topic by recapping the concepts for them. After each discussion, I was asked to document the concepts, which gave me the chance to demonstrate core tech writing skills. I also had an informational lunch with Kuan (Design Lead) and a marketing blog editing exercise with Jessica (Director of Marketing). In my experience, cross-team interviews with engineering, marketing, and design are way outside the norms of interviewing for technical writing positions. These interviews gave me an insight into the collaborative work culture at the company. The final interview was with Jesse, and a wrap-up session with Lindsay. Earlier in the process, I was a bit concerned about the fact that none of the interviewers had looked at my resume. Lindsay had explained how Cockroach Labs combats unconscious bias by removing resumes from the process. I understood the motivation for not basing interviews on resumes, but it sure got me worried. After all, I was used to having my resume as tool to demonstrate my suitability for the position. But Lindsay explained that even though the interviewers won’t read through my resume beforehand, I was free to talk about and explain the relevant work experiences during the interviews. I took her advice and discussed my work experiences with the interviewers during the entire process. In my experience, with resume-based interviews, the discussion tends to be limited to the items in the resume. At Cockroach Labs, however, the discussions were not based on the resume items and I was able to convey my relevant work experiences more suitably in the context of the discussion with each interviewer. I was pleasantly surprised by the “show, don’t tell” aspect of the interviews. I wasn’t asked experiential questions like “How would you handle an interaction with a Subject Matter Expert”; instead I actually worked with an engineer to craft documentation on the spot. I wasn’t asked “Do you think you would be a good cultural fit?” Instead Kuan took me out to lunch and gave me a first-hand insight into the company culture. It was truly an unconventional, yet insightful interview process. By the end of the day, I felt like I had a day-in-the-life experience of working at Cockroach Labs. I had worked with engineers, created live documents, worked in cross-team collaborations, and had a fun lunch! My only qualm: Because the interviewing process was so unconventional, at the end of the day, I couldn’t decipher if I had fared well in the interview. The traditional interviewing metrics to judge my own performance did not apply here. I knew I had done my best, but I didn’t know if my best was good enough. That anxiety was alleviated when I heard back from Lindsay within a week with positive feedback. She informed me that the team was interviewing other candidates, but she touched base with me until the team made the hiring decision. I appreciated her going the extra mile to engage regularly with me and help me maintain interest in the position. Even before joining the organization, I had experienced how Roachers live and breathe the company values, which include respect, transparency, and balance. By the time I received the offer letter, I knew I would be a good fit and could contribute productively towards the company’s success. So this was my interviewing experience at Cockroach Labs. It was an enjoyable experience that I wholeheartedly encourage. If you want to work on a cutting-edge technical product in a fast-paced, value-based environment, get in touch .", "date": "2017-08-24"},
{"website": "CockroachLabs", "title": "Implementing Backup", "author": ["Daniel Harrison"], "link": "https://www.cockroachlabs.com/blog/implementing-backup/", "abstract": "Consistent, Distributed, Incremental: Pick Three Almost all widely used database systems include the ability to backup and restore a snapshot of their data. The replicated nature of CockroachDB's distributed architecture means that the cluster survives the loss of disks or nodes, and yet many users still want to make regular backups. This led us to develop distributed backup and restore, the first feature available in our CockroachDB Enterprise offering. When we set out to work on this feature, the first thing we did was figure out why customers wanted it. The reasons we discovered included a general sense of security, \"Oops I dropped a table\", finding a bug in new code only when it's deployed, legally required data archiving, and the \"extract\" phase of an ETL pipeline. So as it turns out, even in a system that was built to never lose your data, backup is still a critical feature for many of our customers. At the same time, we brainstormed whether CockroachDB's unique architecture allowed any improvements to the status quo. In the end, we felt it was important that both backup and restore be consistent across nodes (just like our SQL), distributed (so it scales as your data scales), and incremental (to avoid wasting resources). Additionally, we knew that backups need to keep only a single copy of each piece of data and should impact production traffic as little as possible. You can see the full list of goals and non-goals in the Backup & Restore RFC . In this post, we'll focus on backup and how we made it work. Step 0: Why We Reinvented the Wheel One strategy for implementing backup is to take a snapshot of the database's files, which is how a number of other systems work. CockroachDB uses RocksDB as its disk format and RocksDB already has a consistent backup feature , which would let us do consistent backups without any particular filesystem support for snapshots of files. Unfortunately, because CockroachDB does such a good job of balancing and replicating your data evenly across all nodes, there's not a good way to use RocksDB’s backup feature without saving multiple copies of every piece of data. Step 1: Make it Consistent Correctness is the foundation of everything we do here at Cockroach Labs. We believe that once you have correctness, then stability and performance will follow. With this in mind, when we began work on backup, we started with consistency. Broadly speaking, CockroachDB is a SQL database built on top of a consistent, distributed key-value store. Each table is assigned a unique integer id, which is used in the mapping from table data to key-values. The table schema (which we call a TableDescriptor ) is stored at key /DescriptorPrefix/<tableid> . Each row in the table is stored at key /<tableid>/<primarykey> . (This is a simplification; the real encoding is much more complicated and efficient than this. For full details see the Table Data blog post ). I'm a big fan of pre-RFC exploratory prototypes, so the first version of backup used the existing Scan primitive to fetch the table schema and to page through the table data (everything with a prefix of /<tableid> ). This was easy, quick, and it worked! It also meant the engineering work was now separable. The [SQL syntax for BACKUP ], the format of the backup files (described below), and RESTORE could now be divvied up among the team members. Unfortunately, the node sending all the Scan s was also responsible for writing the entire backup to disk. This was sloooowwww (less than 1 MB/s), and it didn’t scale as the cluster scaled. We built a database to handle petabytes, but this could barely handle gigabytes. With consistency in hand, the natural next step was to distribute the work. Step 2: Make it Distributed We decided early on that backups would output their files to the storage offered by cloud providers (Amazon, Google, Microsoft, private clouds, etc). So what we needed was a command that was like Scan , except instead of returning the data, it would write it to cloud storage. And so we created Export . [ Export is a new transactionally-consistent command] that iterates over a range of data and writes it to cloud storage. Because we break up a large table and its secondary indexes into multiple pieces (called \"ranges\"), the request that is sent gets split up by the kv layer and sent to many nodes. The exported files use LevelDB's SSTable as the format because it supports efficient seeking (in case we want to query the backup) and because it was already used elsewhere in CockroachDB. Along with the exported data, a serialized backup descriptor is written with metadata about the backup, a copy of the schema of each included SQL table, and the locations of the exported data files. Once we had a backup system that could scale to clusters with many nodes and lots of data, we had to make it more efficient. It was particularly wasteful (both cpu and storage) to export the full contents of tables that change infrequently. What we wanted was a way to write only what had changed since the last backup. Step 3: Make it Incremental CockroachDB uses MVCC . This means each of the keys I mentioned above actually has a timestamp suffix, something like /<tableid>/<primarykey>:<timestamp> . Mutations to a key don't overwrite the current version, they write the same key with a higher timestamp. Then the old versions of each key are cleaned up after 25 hours. To make an incremental version of our distributed backup, all we needed to do was leverage these MVCC versions. Each backup has an associated timestamp. An incremental backup simply saves any keys that have changed between its timestamp and the timestamp of the previous backup. We plumbed these time ranges to our new Export command and voilà! Incremental backup. One small wrinkle: if a given key (say /<customers>/<4> ) is deleted, then 25 hours later when the old MVCC versions are cleaned out of RocksDB, this deletion (called a tombstone) is also collected. This means incremental backup can't tell the difference between a key that's never existed and one that was deleted more than 25 hours ago. As a result, an incremental backup can only run if the most recent backup was fewer than 25 hours ago (though full backups can always be run). The 25 hour period is not right for every user, so it's configurable using replication zones . Go Forth and Backup Backup is run via a simple [ BACKUP SQL command], and with our work to make it consistent first, then distributed and incremental, it turned out blazing fast. We're getting about 30MB/s per node and there's still lots of low-hanging performance fruit. It's our first enterprise feature, so head on over to our license page to grab an evaluation license and try it out. While CockroachDB was built to survive failures and prevent data loss, we want to make sure every team, regardless of size, has the ability to survive any type of disaster. Backup and restore were built for large clusters that absolutely need to minimize downtime, but for smaller clusters, a simpler tool will work just fine. For this, [we've built cockroach dump ], which is available in CockroachDB Core. What's Next? We have plans for a number of future projects to build on this foundation: Change Feeds for point-in-time backup and restore, read-only SQL queries over backups, an admin ui page with progress and scheduling, pause/resume/cancel control of running backups, and more. Plus, BACKUP is worth far more with RESTORE (which turned out to be much harder and more technically interesting) and there's a lot more that didn't fit in this blog post, so stay tuned.", "date": "2017-08-09"},
{"website": "CockroachLabs", "title": "Exercise Based Interviewing at Cockroach Labs", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/exercise-based-interviewing/", "abstract": "When I first started at Cockroach Labs, the founders and I had a candid conversation about diversity. Studies have shown that diverse companies have a greater likelihood of success through higher employee performance and financial returns [ 2015 McKinsey report ]. We agreed that it was important for us to attract a diverse workforce and concluded that the best way to do so was through creating an inclusive environment. In addition to internal initiatives, we set out on a mission to remove bias from our interview process or more realistically, challenge the bias that our interviewers face when assessing candidates. Initially, we removed resumes from our interview process. Through removing resumes from the process, we challenge the interviewer not to assess a candidate based on associations that they have with the candidate through a piece of paper. Instead, we ask the interviewer to focus on the direct application of the candidate’s skills during the interview. We believe that by challenging our unconscious bias , we are creating a fairer process, leading to better hiring outcomes. When starting to hire for non-technical roles, our CEO, Spencer Kimball grew frustrated. As we were looking at the behavioral and situational questions for the position, he asked, “Why can’t we do something like our coding interviews for our non-technical candidates?” Our coding interviews at Cockroach Labs focus on questions that our engineers apply in their positions. Also, they provide the interviewee with an environment that is reflective of the real-world application of their skills. My answer to Spencer was, “we can.” And we have. Instead of focusing on behavioral questions based on past performance or situational questions based on hypothetical future situations, we focus on exercise based interviews with an emphasis on applied skills required for the role. Exercise based interviews can include case studies, group exercises involving role play or discussions, individual exercises, or presentations. They allow for our interviewers to witness the direct application of a candidate’s abilities to competencies required for the role. For example, our Technical Writers are asked to complete an information architecture exercise, which focuses on the candidate’s ability to break down and structure information with an audience in mind. Since each interview focuses on different areas, collectively, the interviews are mini snapshots of the position. Furthermore, we believe they lead to a stronger signal since they are likely to be strongly correlated with actual job performance. The added benefit? The candidate receives a clear understanding of what it would be like working in the position and with our team on a day-to-day basis. Furthermore, the interviews also require a high degree of engagement, which results in the interviews being more collaborative and overall more pleasant than traditional interviewing where they are responding to questions. Most importantly for us in this effort is that we strongly believe that exercise based interviewing reduces bias and subjectivity because the interviews are focused on providing candidates with real-world simulations where they can display their ability. The Nitty-Gritty: How to construct exercise based interviews? Once a candidate applies to roles at Cockroach Labs, we use the following process: Recruiter Phone Screen Take-home Test or Coding Interview (via CoderPad) In-Person Interview The Recruiter Phone Screen is the only stage that the candidate’s resume will be reviewed. Once the assessment portion of the interview process has concluded, the resume is hidden from the interview team. For non-engineering positions, candidates receive a take-home exercise, instead of a traditional phone interview with the hiring manager. These exercises typically take 1-2 hours to complete and allow for the hiring manager to review a candidate’s output of skills directly related to the position before proceeding to the onsite interview. The take-home exercise takes the place of one phone interview and one onsite interview, equaling two hours of engagement. For example, candidates for our recruiter position are asked to provide an overview of how they would kick-off a search for a position and provide three qualified candidates for the role. For engineering positions, candidates connect with a member of our engineering team and spend one hour on a coding exercise, leveraging coderpad.io . The coding exercises are directly related to the same concepts our engineers apply on a daily basis, covering areas like coding/debugging, algorithms, and data structures. During the in-person interview, candidates meet members from the team that they will work with. The slate includes members of the direct team or cross teams. As mentioned before, these interviews can include case studies, group exercises involving role play or discussions, individual exercises, or presentations. The interviews are structured as follows: Interviewer Presents the Exercise - The goal of the exercise is explained. Candidate Collects Data - The interviewer has a back and forth discussion with the candidate that allows the candidate to complete the exercise. Execution - The candidate works on the exercise. Discussion - The interviewer and candidate have a conversation about how the candidate arrived at the solution. The discussion involves a prompt of the initial questions, follow-ups that allow furthering the conversation, and probes, should clarifying questions be needed. Example Interview Onsite Exercise for Administrative Roles - Client Meetings Set-up: 15 minute warm-up/present problem/data collecting, 15 minute work time, 15 minute discussion. Goal: Schedule two one-hour client meetings for Co-Founder and VP of Engineering Peter Mattis. Peter has not met with these individuals before so please be sure to prepare a short bio on both clients by researching publicly available information. Please also be sure to include any information that may be useful for Peter to know (i.e. areas of overlap, current focus in roles, past experience, etc.). Client Meetings: Oscar Health - Alan Warren Goldman Sachs - R. Martin Chavez Considerations: Peter typically arrives at the office by 9am. He is traveling from his home in Connecticut via train and takes 4/5/6 from Grand Central. Peter prefers commuting via car, not public transportation for meetings. Peter likes to have 10 minutes of buffer time before the meetings so he does not feel like he is rushed. Peter’s LinkedIn: https://www.linkedin.com/in/peter-mattis-46549144 For the comprehensive exercise, click here . How does this influence the candidate experience? At the end of every onsite interview, we solicit feedback from our candidates to uncover what went well and what didn’t go well. From this survey, we can see if the candidate feels that exercise based interviewing provides the same benefits that we feel they do. Of 19 people who responded to the survey, 79% preferred the interview format being focused on exercise based interviewing. The remaining 21% felt neutral towards the interview format while there were no unfavorable responses in the data set. As Amruta Ranade discussed in her blog , she “was pleasantly surprised by the “show, don’t tell” aspect of the interviews,” which she felt “was truly an unconventional, yet insightful interview process.” As always, our interview process is living and breathing. Like everything we do, we have to iterate to improve. When we are kicking off a new requisition, we sit down after each interview to internally discuss what needs to be improved. Often, these improvements have been a direct result of feedback we get. For example, we have learned that for some of the exercises context is important. If there is information that we need to prepare the candidate with, we will send it to them before the interview. Another piece of feedback we received from a candidate is that they would have preferred to come prepared with information to apply to the exercises as they felt the time constraint of the interview didn’t allow for a strong representation of their work. If they could come prepared, they could have gathered more information needed for the exercise (e.g., read a blog post, etc.). Conclusion The positive candidate feedback that we have received regarding our process, along with our desire to challenge how companies approach recruiting has encouraged us to open source our interview process. In the upcoming month, we will be sharing all of our interview questions on Github. In doing so, we hope to encourage: Companies to think outside the box regarding their interview process. What else can we be doing to provide candidates with a process that is fair and inclusive? Candidates to come prepared! If candidates take the time to research the questions and comes onsite ready to have an engaged discussion, we will be thrilled. Collaboration. Let’s all do better than we have before. We can only do better through engaging with ideas from one another. We welcome feedback on our process and encourage candidates and recruiting professionals to contribute exercises as well. Look forward to future announcements of us open sourcing our interview process. Illustration by Lisk Feng", "date": "2017-09-07"},
{"website": "CockroachLabs", "title": "The Cross-Cloud Migration", "author": ["Jesse Seldess"], "link": "https://www.cockroachlabs.com/blog/cross-cloud-migration/", "abstract": "As a CockroachDB Tech Writer, when I document a new feature, generally, I first try to learn the business value behind it, then I test the feature thoroughly, and then I try to write up concise, informative guidance for users. Sometimes, the business value and usage aren’t unique to CockroachDB (it’s a SQL database, after all, and SQL has been around for a while). Other times, I get to document capabilities so novel and powerful that straight-up user documentation just doesn’t seem enough. The ability to span a CockroachDB cluster across multiple cloud platforms, and then completely migrate data from one cloud to another with a single command, was one such case. In the docs , I could explain how simple it is to take advantage of these capabilities, and I could walk users through a local simulation. But to really get the point across, I decided to do a little dogfooding: I recorded myself deploying a 6-node cluster across Digital Ocean and GCE and then using CockroachDB’s replication controls to migrate data off and on to each cloud, all while running the open-source YCSB load generator and HAProxy load balancer to simulate continuous client traffic. Have a look! And then read below on how it works and how to test this yourself. Also keep an eye out for upcoming posts explaining the mechanics of automated rebalancing and CockroachDB’s replication controls. Flexible Deployment As you saw in the video, the deployment process is simple. In essence, on each machine where you want a node, you download the binary and then execute the cockroach start command, using the --host flag to identify the address on which the node talks to other nodes and clients. For all but the first node, you also use the --join flag to specify the address of the first node. This is the way in which a new node knows it’s joining an existing cluster. That’s pretty much it! As long as the networking setup allows the nodes to talk to each other, it really doesn’t matter where the nodes reside. They can be on machines in a single datacenter, in multiple datacenters, in different public clouds (as shown in the video), or even in a mixture of public and private clouds. What’s behind this flexibility? It’s a combination of how the nodes self-organize via a Gossip protocol and how the cluster replicates data via the Raft consensus algorithm . Future posts and documentation will go into more technical detail, but in summary, the Gossip protocol ensures that every node has up-to-date details about every other node in the cluster, including the location of data and the storage capacity of each node. This means that every node can serve as a suitable SQL gateway for clients and that the cluster as a whole can work to continuously move data around to maintain balance. As for the Raft consensus algorithm, it’s the mechanism by which CockroachDB ensures that every \"range\" of data is replicated (3 times by default) and that replicas of the same \"range\" always remain consistent (a range is the basic unit of data for the purpose of replication). Flexible Migration The ease with which you can control the location of data is another key point in the video, and it ties into CockroachDB’s replication controls , specifically the notion of node locality. When you start a node, you can pass the --locality flag to describe its location. This can be anything from region to country to cloud to datacenter to rack, or any combination of these. If you set localities consistently on all nodes, CockroachDB automatically aims to balance replicas evenly across them. But you can also configure what we call \"replication zones\" to force data onto or off of nodes that match specific localities (as well as other node attributes). For example, in the video, I started the 3 nodes on Digital Ocean with --locality=cloud=do and the 3 nodes on GCE with --locality=cloud=gce , resulting in an even balance of data across all 6 nodes. Then with a single command, I was able to update the default replication zone for the cluster to require that all replicas reside on nodes with --locality=cloud=gce . The cluster then pretty quickly ensured that all replicas were on GCE nodes, leaving the Digital Ocean nodes empty: Once that migration completed, I then reversed the migration, again with a single command, by changing the constraint to require that all replicas reside on nodes with --locality=cloud=do . Again, the cluster enforced the constraint pretty quickly: Migration was very fast in my case because the cluster didn’t hold much data. Of course, the larger the cluster, the longer the migration. Nonetheless, the key point is that, with CockroachDB, you have the flexibility to deploy where you want and never feel locked into a specific vendor; migrating from one cloud to another is something CockroachDB handles for you behind-the-scenes, while the cluster continues to operate normally for clients and users, with no service interruption. It’s a powerful capability. Consider testing it yourself! Test It Yourself Create 3 VMs on GCE and 4 VMs on Digital Ocean (3 for cockroach, 1 for haproxy). Create GCE ingress/egress firewall rules allowing all traffic on ports 26257 (for inter-node communication) & 8080 (for accessing Admin UIs). Install and start NTP on the Digital Ocean VMs to ensure proper clock synchronization (running by default on GCE VMs): $ sudo apt-get install ntp Install the cockroach binary on all the VMs and on your local machine. Start CockroachDB nodes on the 3 Digital Ocean VMs: $ cockroach start --insecure --background --advertise-host=<node 1 external IP> --locality=cloud=do\n\n$ cockroach start --insecure --background --advertise-host=<node 2 external IP> --join=<node 1 external IP>:26257 --locality=cloud=do\n\n$ cockroach start --insecure --background --advertise-host=<node 3 external IP> --join=<node 1 external IP>:26257 --locality=cloud=do Start CockroachDB nodes on the 3 GCE VMs: $ cockroach start --insecure --background --advertise-host=<node 4 external IP> --join=<node 1 external IP>:26257 --locality=cloud=gce\n\n$ cockroach start --insecure --background --advertise-host=<node 5 external IP> --join=<node 1 external IP>:26257 --locality=cloud=gce\n\n$ cockroach start --insecure --background --advertise-host=<node 6 external IP> --join=<node 1 external IP>:26257 --locality=cloud=gce Open the Admin UI at http://<IP of any node>:8080 and check the Replicas per Node graph. You should see that replicas are evenly balanced across all 6 nodes. On the remaining Digital Ocean VM, install the HAProxy load balancer: $ apt-get install haproxy On the same VM, use the cockroach binary to generate an HAProxy config file , pointing to any node in the running cluster: $ cockroach gen haproxy --insecure --host=<external IP of any node> --port=26257 On the same VM, start HAProxy: $ haproxy -f haproxy.cfg On your local machine, install and build our version of the YCSB load generator , and then start it, pointing at the HAProxy IP: $ ycsb -duration 2h -splits 50 -tolerate-errors -concurrency 50 -initial-load 100 'postgresql://root@<HAProxy IP>:26257?sslmode=disable' Go back to the Admin UI and check out the effect of running 50 concurrent YCSB connections against the cluster. On your local machine, using the cockroach binary as a client, update the default zone config with a required constraint to force all data onto GCE: $ echo 'constraints: [+cloud=gce]' | cockroach zone set .default --insecure --host=<external IP of any node> -f - Go back to the Admin UI and again check the Replicas per Node graph. After a few minutes, you should see the replica count double on the GCE nodes and drop to 0 on the Digital Ocean nodes. On your local machine, using the cockroach binary as a client, change the default zone config’s required constraint to force all data onto Digital Ocean: $ echo 'constraints: [+cloud=do]' | cockroach zone set .default --insecure --host=<external IP of any node> -f - Go back to the Admin UI and check the Replicas per Node graph one more time. After a few minutes, you should see all replicas on the Digital Ocean nodes and 0 replicas on the GCE nodes.", "date": "2017-08-30"},
{"website": "CockroachLabs", "title": "Real Transactions are Serializable", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/acid-rain/", "abstract": "Most databases offer a choice of several transaction isolation levels, offering a tradeoff between correctness and performance. However, that performance comes at a price, as developers must study their transactional interactions carefully or risk introducing subtle bugs. CockroachDB provides strong (“ SERIALIZABLE ”) isolation by default to ensure that your application always sees the data it expects. In this post I'll explain what this means and how insufficient isolation impacts real-world applications. Isolation in the SQL Standard The SQL standard defines four isolation levels: SERIALIZABLE REPEATABLE READ READ COMMITTED READ UNCOMMITTED SERIALIZABLE transactions run as if only one transaction were running at a time; the other isolation levels allow what the SQL standard euphemistically calls \"the three phenomena\": dirty reads, non-repeatable reads, and phantom reads. Subsequent research has identified additional \"phenomena\" and isolation levels. In modern research, these \"phenomena\" are more commonly called \"anomalies\", or more bluntly, \"lies\" . When you use a non- SERIALIZABLE isolation level, you're giving the database permission to return an incorrect answer in the hope that it will be faster than producing the correct one. The SQL standard recognizes that this is dangerous and requires that SERIALIZABLE is the default isolation level. Weaker isolation levels are provided as a potential optimization for applications that can tolerate these anomalies. Isolation in Real Databases Most databases ignore the specification that SERIALIZABLE be the default, and instead prioritize performance over safety by defaulting to the weaker READ COMMITTED or REPEATABLE READ isolation levels. More worryingly, some databases (including Oracle, and PostgreSQL prior to version 9.1) do not provide a serializable transaction implementation at all. Oracle's implementation of the SERIALIZABLE isolation level is actually a weaker mode called \"snapshot isolation\". Snapshot isolation was developed after the initial standardization of the SQL language, but has been implemented in multiple database systems because it provides a good balance of performance and consistency. It is stronger than READ COMMITTED but weaker than SERIALIZABLE . It is similar to REPEATABLE READ but not exactly equivalent ( REPEATABLE READ permits phantom reads but prevents write skew, while the reverse is true of snapshot isolation). The databases that have implemented snapshot isolation have made different decisions about how to fit it into the four SQL standard levels. Oracle takes the most aggressive stance, calling their snapshot implementation SERIALIZABLE . CockroachDB and Microsoft SQL Server are conservative and treat SNAPSHOT as a separate fifth isolation level. PostgreSQL (since version 9.1) falls in between, using snapshot isolation in place of REPEATABLE READ . Because serializable mode is used less often in databases that default to weaker isolation, it is often less thoroughly tested or optimized. For example, PostgreSQL has a fixed-size memory pool that it uses to track conflicts between serializable transactions, which can be exhausted under heavy load. Most database vendors treat stronger transaction isolation as an exotic option to be enabled by applications with exceptional consistency needs. Most applications, however, are expected to work with the faster but unsafe weak isolation modes. This backwards approach to the problem exposes applications to a variety of subtle bugs. At Cockroach Labs, we like thinking about transactional anomalies so much that we named all our conference rooms after them, but I would have a hard time advising with confidence when it is both safe and beneficial to choose SNAPSHOT isolation instead of SERIALIZABLE . Our philosophy is that it's better to start with safety and work towards performance than the other way around. ACIDRain: Finding Transactional Bugs Recent research at Stanford has explored the degree to which weak isolation leads to real-world bugs. Todd Warszawski and Peter Bailis examined 12 eCommerce applications and found 22 bugs related to transactions, five of which would have been avoided by running at a higher isolation level. Many of these bugs were simple to exploit and had direct financial implications. For example, in five of the tested applications, adding an item to your cart while checking out in another browser tab could result in the item being added to the order for free. The researchers developed tools to identify these vulnerabilities in a semi-automated way, paving the way for similar attacks (which the researchers dubbed \"ACIDRain\") to become more prevalent. Most databases that default to weak transactional isolation provide workarounds, such as the (non-standard) FOR UPDATE and LOCK IN SHARE MODE modifiers for SELECT statements. When used correctly, these modifiers can make transactions safe even in weaker isolation levels. However, this is easy to get wrong, and even when used consistently these extensions introduce most of the downsides of SERIALIZABLE mode (in fact, overuse of SELECT FOR UPDATE in a READ COMMITTED transaction can perform worse than a SERIALIZABLE transaction, because it uses exclusive locks where serializability may only require shared locks). The ACIDRain research demonstrates the limitations of this technique: only one in three of the applications that attempted to use SELECT FOR UPDATE feature did so correctly; the others remained vulnerable. Conclusion Databases that encourage the use of weaker isolation levels have prioritized performance over the safety of your data, leaving you to study subtle interactions between your transactions and implement error-prone workarounds. CockroachDB provides SERIALIZABLE transactions by default to ensure that you always see the consistency that you expect from a transactional database. Are distributed transactions your jam? Our engineering team is hiring! Check out our open positions here . Illustration by Lisk Feng", "date": "2017-09-21"},
{"website": "CockroachLabs", "title": "Avoid Vendor Lock-in Risk with Multi-cloud Deployments", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/gs-response/", "abstract": "As businesses outsource their infrastructure to public cloud providers they are in turn taking on major risks. In a recent piece by Financial News (gated), senior executives at Goldman Sachs and Standard Chartered warned that an overreliance on a small band of cloud service providers could result in a major hack or outage wreaking havoc on the global banking system. Lock-in is a global issue: Bain’s Cloud Computing Survey noted that the share of respondents citing vendor lock-in as a “top three concern” grew from 7% to 23% from 2012 to 2015. Of course, cloud vendor lock-in issues extend beyond uptime risk; they also include the regulatory risk of changes in data sovereignty policies or the financial risk of having to endure price hikes without any negotiating power; Dropbox went so far as to migrate off of AWS and onto their own system to get control of their costs. Fortunately, CockroachDB can help eliminate these problems by enabling seamless multi-cloud deployments. Multi-cloud deployments hedge risk On stage at the 2017 OpenStack Summit , representatives from some of the top cloud providers including IBM, vmWare, and Red Hat set up a multi-cloud CockroachDB cluster. As servers joined the network, their new peers continually shared information about the cluster and collectively guaranteed clients would get the exact same results no matter which node was queried . A CockroachDB cluster operates as a self-healing, elastic data layer than can span private and public clouds alike, enabling services to survive a node, data center, or even an entire cloud provider going down without experiencing downtime or lost data. Using CockroachDB means shifting operational thinking from disaster recovery to disaster resilience, and completely side steps the risks associated with vendor lock-in. Retaining data control Helping companies of all sizes avoid vendor lock-in is top of mind for us; that’s one of the reasons why we built CockroachDB from the ground up to run on commodity hardware , keeping the choice of where and how data is managed in the hands of IT teams. In the Financial News article, a Standard Chartered executive pondered if other players would emerge to help eliminate this systematic risk of cloud vendor lock-in. We think CockroachDB answers the call. Photo by Alex Holyoake on Unsplash", "date": "2017-10-03"},
{"website": "CockroachLabs", "title": "Product Design is a Work in Progress", "author": ["Kuan Luo"], "link": "https://www.cockroachlabs.com/blog/product-design-wip/", "abstract": "The design team at Cockroach Labs recently doubled in size (Hooray!). During the hiring process, I got many questions about the company culture, specific projects that a new product designer would be working on, and of course, personal growth. The most common question, however, was surprisingly about what exactly a product designer does. Product design encompasses a wide range of responsibilities. Depending on the person and their company, the typical day of a product designer can vary quite a bit. Some write thorough code for their mockups, while others write clear, witty copy. Some craft pixel-perfect icons, while others excel at prototyping animations and user flows. If you're attracted to working at startups like me, then you're probably drawn to wearing many hats. Since resources are scarce, product designers sometimes manage projects, conduct interviews, create mockups, and take on prototyping. And if you're working on a product for which you aren't the primary the user (like a database), one skill that's crucial to success is the ability to leverage knowledge from your fellow coworkers so they can help you arrive at great solutions. A product designer at Cockroach Labs has three main responsibilities: research, translation and facilitation. Product Designer as a Researcher To deliver a great product, design skills don't necessarily come first. Listening to customers is undoubtedly the key to unlocking the product’s potential. Researchers help the rest of the team see the product through customers' eyes, prioritize problems to solve, and validate ideas before it's too costly to reverse course. Since we don't have researchers (yet!) at Cockroach Labs, product designers are picking up the slack. We run quantitative surveys as well as one-on-one interviews to create a direct line of feedback between users and the team. Just last week, to find out how to make a better monitoring interface for the database, our new designer Josué put on his researcher hat. He drafted a recruitment guide to make sure we spoke to the right people. He brainstormed activities to learn more about the participants during the interview and moderated all the sessions. As the team grows, product designers may no longer need to recruit and conduct usability studies. Still, it will always be our job to listen to customers and observe users. Having the empathy to ask questions and the persistence to dig deeper on why people do what they do is what makes product designers natural researchers. Product Designer as a Facilitator With insights gathered and features prioritized, the next step in the product development phase is to get everyone onboard with a solution. Facilitators get the right people in the same room, lead exercises for the group, and facilitate and document the conversations so by the time the meeting is over, everyone is on the same page. Having product designers as facilitators has two benefits. First, some database concepts are harder than others for product designers to understand. We need to hear about a feature from multiple perspectives, such as engineering and marketing, which helps us to comprehend the full scope of the project. One of the bigger initiatives on the admin UI side is to create some visualizations that would communicate the core benefits of CockroachDB. To kick off the project, I organized a few meetings to gather every stakeholder in the same space. We discussed why the project was important to us and what information needed to be there. Then, we brainstormed and sketched individually before coming back to the group to share our own ideas. Each meeting inched the project forward, until we were able to start designing how the visualization looked and felt. Making sure everyone aligns on the North Star every step along the way is crucial - not just for us with our still-growing team, but also for much larger companies. It's also important for larger organizations to execute on projects that grant the highest return on investment. Secondly, when we facilitate, we also educate the rest of the team on design thinking. Instead of talking about it, we apply it while guiding discussions. Classic tactics such as \"How might we...\" and ideation sketches are used to leverage the creativity of the rest of the team, and spark ideas unknown to us. In short, what's better? Fishing for the whole village, or teaching the whole village to fish? We tend to think the latter. Product Designer as a Translator Once scope is defined and everyone is onboard, product designers translate product requirements insights into wireframes, mockups and prototypes. Designer Michael Rock describes the goal of a translator in relation to design beautifully. He wrote, \"The intimate goal is the expression of a given content rendered in a form that reaches a new audience.\" Great translations capture not only the meaning of the content, but the spirit of the message with individuality of the translator. Whether it's the product page on our website or the graphics for our tech talk presentations, product designers most likely aren't the authors of the content. Instead, our goal is to transform the content into visually interesting layouts that help tell the story. When we launched our messaging, \"Build it right,\" on the homepage , I brainstormed many ways to translate it in subtle visuals to add depth without competing with the headline. The idea of the moving pixels on the grid in the background came to me as a way to illustrate how CockroachDB distributes replicas (small bits of data) flowing around the cluster to achieve high availability. The outcome reflects my aesthetic as a designer: grids, patterns, and contrast. No matter how big of a team we have, designing database systems will always require product designers to act as translators to grasp abstract concepts and to render them in a way that's understandable to the audience. Yet, what's more exciting is to have different aesthetics on the team to influence and mold each other, so our translations can be more alive and spirited as we grow. Next year's translation of our homepage will be radically different from this year's, for the better. Product Design is a Work in Progress Defining and refining the responsibilities of a product designer at Cockroach Labs into those three main roles has helped us tremendously in looking for the right candidate in the search. And we’re very lucky to have Josué aboard - he dances gracefully with his researcher, facilitator, and translator hats on. As Lindsay, our Head of People, always says, hiring doesn’t end the day new employees start. In order to continue to grow the team and strengthen design at Cockroach Labs, we have a lot more work to do. I recently peeked at what is the most thorough designer onboarding document ever to have existed, and one section is on team principles. In the next few months, I am working on defining some principles for our small team of two, and the responsibilities are the seeds where principles will likely blossom. How do you define product design in your organization? Which design team principles do you follow? What is one principle that you wish your team followed? I’d love to hear about them. Tweet @kuanluo or @cockroachdb .", "date": "2017-07-20"},
{"website": "CockroachLabs", "title": "CockroachDB on DC/OS: Resilient and Hassle-Free Operations for Global Services", "author": ["Alex Robinson"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-on-dcos/", "abstract": "CockroachDB makes data easier to manage by providing a strongly-consistent, highly-scalable, SQL interface that you can trust to be there when you need it. We’ve designed it to be a truly cloud-native, distributed SQL database that’s easy to operate in any environment you throw at it. One such computing environment that has grown in popularity over the previous few years is Mesosphere’s DC/OS, a datacenter operating system built on top of Apache Mesos. DC/OS is an orchestration system for deploying and managing distributed applications across a cluster of machines as if they were a single pool of resources. DC/OS has both an open source and an enterprise version that gives you the ability to elastically scale your infrastructure on prem or in the cloud. It provides scheduling, resource allocation, service discovery, automatic recovery from failure, load balancing, and more, all with the goal of making it easier to manage your applications. This makes CockroachDB and DC/OS a natural pair -- CockroachDB makes it easier to manage and scale mission-critical data, and DC/OS automates the management of CockroachDB, providing elastic scale, zero downtime, and hassle-free operations. In this post we’re going to take a look at how to combine the two by using the newly released framework for running CockroachDB on DC/OS . Inside the CockroachDB DC/OS framework The CockroachDB DC/OS framework is built using the dcos-commons SDK, which allows for creating incredibly resilient stateful frameworks without needing to write a whole bunch of new code. Instead, YAML and JSON configuration files are used to generate a framework that understands and can accommodate the needs of the underlying stateful application. The application (in this case, CockroachDB) can do its thing while the framework handles the hard problems around service discovery, storage management, scheduling, and deployment. As new improvements are added to the SDK, they’ll be automatically included in new releases of the CockroachDB framework. Getting started with the framework (once you have a DC/OS cluster running) is as easy as choosing CockroachDB in DC/OS’s web interface or running dcos package install cockroachdb from the command line. This process can be customized if you’d like to change one of the settings presets, such as the number of CockroachDB processes to create, which machines they’re allowed to run on, or how much disk to allocate for them. Once you’ve done this, you should shortly see a CockroachDB service in the cluster with a handful of running processes: The first three tasks you see here are the actual running CockroachDB processes. The next task in the list is responsible for scraping metrics from the CockroachDB tasks. The final task is the framework scheduler, which is responsible for scheduling and managing the others. While you might think that the scheduler isn’t doing much once the processes are up and running, in reality it’s actively ensuring that your database stays up and running. Health checks are constantly being run such that it will restart individual processes that are unhealthy. Individual machines are also being checked, and if one goes down, then the scheduler will move the CockroachDB task that was running on it to a healthy machine. And when you want to upgrade to a new version of CockroachDB, the scheduler will orchestrate the rolling upgrade process, restarting tasks one-by-one to avoid any user-visible downtime. You can connect to the database from within the DC/OS cluster by simply talking to the pg.cockroachdb.l4lb.thisdcos.directory network endpoint as if it was a single process, using the PostgreSQL wire protocol. The network will handle finding a running process to talk to, and CockroachDB’s multi-active availability model ensures that your queries will get consistent responses no matter which process they’re talking to. Beyond the normal operational capabilities of an orchestration system, the CockroachDB DC/OS framework has built-in support for backing up and restoring your database. Running a backup to AWS S3 is as simple as running dcos cockroachdb backup <database-name> <s3-bucket> , and restore works similarly. This makes it easy to run frequent regular backups to ensure that even in the case of a complete catastrophe, your data will be safe. While only S3 is supported for now, more backup destinations can be added as requested. Where to go from here While all the essentials are in place for running a production-ready deployment of CockroachDB on DC/OS, we’re sure there’s more that could be done to continue making it a better experience. We need your help to determine what’s most important. Try building an application using CockroachDB, running it on DC/OS, and providing feedback and patches to make the experience better for everyone! Illustration by Ana Hill", "date": "2017-09-28"},
{"website": "CockroachLabs", "title": "Survey of Rounding Implementations in Go", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/rounding-implementations-in-go/", "abstract": "Rounding in Go is hard to do correctly. That is, given a float64 , truncate the fractional part (anything right of the decimal point), and add one to the truncated value if the fractional part was >= 0.5. This problem doesn't come up often, but it does enough that as of this writing, the second hit on Google for golang round is a closed issue from the Go project, which declined to add a Round function to the math package. That issue also includes many community contributions about ways to round. In this blog post I'd like to examine those and other implementations of round and audit them for correctness. We will see that nearly all have bugs preventing use in production software. So to answer a question that appeared while I was writing this blog post: round seems obvious, but is not. Surveying the options for round There are multiple ways to round (away from zero, toward zero, half away from zero, half to even, etc.), depending on one's use case. Here we will discuss half away from zero, which rounds up if the fractional part is >= 0.5, and rounds down otherwise. The requirements of a correct round implementation are that it: rounds half away from zero for all finite inputs supports special values (NaN, Inf, -0) by returning them unchanged We will use the following test cases to verify correctness, where the second value is the desired result after rounding the first: tests := [][2]float64{\n\t{-0.49999999999999994, negZero}, // -0.5+epsilon\n\t{-0.5, -1},\n\t{-0.5000000000000001, -1}, // -0.5-epsilon\n\t{0, 0},\n\t{0.49999999999999994, 0}, // 0.5-epsilon\n\t{0.5, 1},\n\t{0.5000000000000001, 1},                         // 0.5+epsilon\n\t{1.390671161567e-309, 0},                        // denormal\n\t{2.2517998136852485e+15, 2.251799813685249e+15}, // 1 bit fraction\n\t{4.503599627370497e+15, 4.503599627370497e+15},  // large integer\n\t{math.Inf(-1), math.Inf(-1)},\n\t{math.Inf(1), math.Inf(1)},\n\t{math.NaN(), math.NaN()},\n\t{negZero, negZero},\n} These include all the special cases, some normal cases, and some edge cases that will prove difficult for many algorithms to handle. (Note that since floats aren't exact, using -0.49999999999999999 is the same as 0.5 . The value used here is the highest float < 0.5. Also, printing -0.49999999999999994 at very high precision returns -0.499999999999999944488848768742 .) The suggestions in the linked issue were often (obviously) untested, but assumed to work, even when suggested by very well known people. That they don't work is a testament to how difficult rounding is to do correctly for all inputs. int(f + 0.5) The first suggestion to the rounding issue comes from rsc: return int(f + 0.5) And fails with: round(-0.5): got: 0, want -1\nround(-0.5000000000000001): got: 0, want -1\nround(0.49999999999999994): got: 1, want 0\nround(4.503599627370497e+15): got: 4.503599627370498e+15, want 4.503599627370497e+15\nround(-Inf): got: -9.223372036854776e+18, want -Inf\nround(+Inf): got: -9.223372036854776e+18, want +Inf\nround(NaN): got: -9.223372036854776e+18, want NaN It doesn't work with special values, negative numbers, inputs > math.MaxInt64 or inputs close to 0.5. Floor + 0.5 The second suggestion in the linked issue is: if f < 0 {\n\treturn math.Ceil(f - 0.5)\n}\nreturn math.Floor(f + 0.5) And fails with: round(-0.49999999999999994): got: -1, want -0\nround(0.49999999999999994): got: 1, want 0\nround(4.503599627370497e+15): got: 4.503599627370498e+15, want 4.503599627370497e+15 What happened in the first two failures is the n-0.5 computation resulted in -1.0 , even though we expected something strictly > -1.0. If we look at the round implementation of Postgres we can see they explicitly avoid this problem: Subtracting 0.5 from a number very close to -0.5 can round to exactly -1.0, producing incorrect results... This is not an uncommon problem. Java up through version 6 was broken in this way , but has since improved their implementation. int + Copysign The third suggestion comes from minux, which is an attempt to fix the negative input problem: return int(f + math.Copysign(0.5, f)) And fails with: round(-0.49999999999999994): got: -1, want -0\nround(0.49999999999999994): got: 1, want 0\nround(4.503599627370497e+15): got: 4.503599627370498e+15, want 4.503599627370497e+15\nround(-Inf): got: -9.223372036854776e+18, want -Inf\nround(+Inf): got: -9.223372036854776e+18, want +Inf\nround(NaN): got: -9.223372036854776e+18, want NaN This fixed the negative input problem, but left the rest, and added a broken test that was working before ( -0.49999999999999994 ). Another user attempted to fix that problem with: if math.Abs(f) < 0.5 {\n        return 0\n}\nreturn int(f + math.Copysign(0.5, f)) This fails with: round(4.503599627370497e+15): got: 4.503599627370498e+15, want 4.503599627370497e+15\nround(-Inf): got: -9.223372036854776e+18, want -Inf\nround(+Inf): got: -9.223372036854776e+18, want +Inf\nround(NaN): got: -9.223372036854776e+18, want NaN Which is overall much better, but still doesn't handle specials or large inputs. Specials can easily be handled with some special cases, but the large inputs can't. The count now is 4 suggestions, and all 4 are broken. Now we will begin to look at some implementations of round in various packages. Kubernetes Kubernetes 1.7 has a round implementation : if a < 0 {\n\treturn int32(a - 0.5)\n}\nreturn int32(a + 0.5) And fails with: round(-0.49999999999999994): got: -1, want -0\nround(0.49999999999999994): got: 1, want 0\nround(4.503599627370497e+15): got: 4.503599627370498e+15, want 4.503599627370497e+15\nround(-Inf): got: -9.223372036854776e+18, want -Inf\nround(+Inf): got: -9.223372036854776e+18, want +Inf\nround(NaN): got: -9.223372036854776e+18, want NaN Since the return value of that function is an int32, we can assume they know their inputs may not ever be special or large, and ignore those inputs, but that still leaves the very close to 0.5 inputs that fail. Solutions that specify the rounding digit Many people would like something that, in addition to normal float -> int round, can round to N digits: round(12.345, 2) = 12.35 . strconv There was a long thread on golang-nuts about rounding, and one of the popular solutions was to use strconv: func RoundFloat(x float64, prec int) float64 {\n\tfrep := strconv.FormatFloat(x, 'g', prec, 64)\n\tf, _ := strconv.ParseFloat(frep, 64)\n\treturn f\n} This uses the 'g' format verb, which returns N digits total, not N digits after the decimal point. The test cases for that function were mostly less than 1, which is why it appeared to work, but it fails for general inputs. Multiply by 10^N Many other solutions for this problem use an implementation that multiplies the input by 10^N (where N is the desired number of digits), rounds, then returns that number divided by 10^N. These algorithms have two kinds of problems. First, if N is sufficiently large, then it can overflow during the multiplication to Infinity. Second, it still has to round correctly, and that's hard to do as seen above. For example, github.com/a-h/round does both: func AwayFromZero(v float64, decimals int) float64 {\n\tvar pow float64 = 1\n\tfor i := 0; i < decimals; i++ {\n\t\tpow *= 10\n\t}\n\tif v < 0 {\n\t\treturn float64(int((v*pow)-0.5)) / pow\n\t}\n\treturn float64(int((v*pow)+0.5)) / pow\n} Here it is trivial for v*pow to be greater than math.MaxInt64 (or MaxInt32 on 32-bit systems, as this converts to int ), and cause problems. Even if it doesn't do that, we've already seen above that the int(f - 0.5) solution doesn't work for various cases. Another example, github.com/gonum/floats does something slightly different: func Round(x float64, prec int) float64 {\n\tif x == 0 {\n\t\t// Make sure zero is returned\n\t\t// without the negative bit set.\n\t\treturn 0\n\t}\n\t// Fast path for positive precision on integers.\n\tif prec >= 0 && x == math.Trunc(x) {\n\t\treturn x\n\t}\n\tpow := math.Pow10(prec)\n\tintermed := x * pow\n\tif math.IsInf(intermed, 0) {\n\t\treturn x\n\t}\n\tif x < 0 {\n\t\tx = math.Ceil(intermed - 0.5)\n\t} else {\n\t\tx = math.Floor(intermed + 0.5)\n\t}\n\n\tif x == 0 {\n\t\treturn 0\n\t}\n\n\treturn x / pow\n} See the line with math.IsInf . This function detects when multiplying by 10^N will overflow, but it handles it by silently returning the input with no indication of error. Even when specifying 0 precision, it fails with: round(-0.49999999999999994): got: -1, want -0\nround(0.49999999999999994): got: 1, want 0 So still isn't usable. CockroachDB This is an old implementation from CockroachDB, before we used the Postgres algorithm. With comments removed: func round(x float64, n int) {\n\tpow := math.Pow(10, float64(n))\n\tif math.Abs(x*pow) > 1e17 {\n\t\treturn x\n\t}\n\tv, frac := math.Modf(x * pow)\n\tif x > 0.0 {\n\t\tif frac > 0.5 || (frac == 0.5 && uint64(v)%2 != 0) {\n\t\t\tv += 1.0\n\t\t}\n\t} else {\n\t\tif frac < -0.5 || (frac == -0.5 && uint64(v)%2 != 0) {\n\t\t\tv -= 1.0\n\t\t}\n\t}\n\treturn v / pow\n} This works correctly for banker's rounding (discussed below), but uses some undefined behavior of Go. The conversion of v (a float64) to uint64 is not well defined and works differently on amd64 and arm . While fixing the arm bug, CockroachDB decided to use a more tested algorithm, and consulted Postgres' approach. Working Implementations Below are some working implementations in Go. Postgres (adapted from C to Go by CockroachDB) The Postgres comment above is from a round implementation in C. CockroachDB adopted this to Go (shown here with comments removed). It implements banker's rounding (round to even), and excluding that difference, it passes all tests. func round(x float64) float64 {\n\tif math.IsNaN(x) {\n\t\treturn x\n\t}\n\tif x == 0.0 {\n\t\treturn x\n\t}\n\troundFn := math.Ceil\n\tif math.Signbit(x) {\n\t\troundFn = math.Floor\n\t}\n\txOrig := x\n\tx -= math.Copysign(0.5, x)\n\tif x == 0 || math.Signbit(x) != math.Signbit(xOrig) {\n\t\treturn math.Copysign(0.0, xOrig)\n\t}\n\tif x == xOrig-math.Copysign(1.0, x) {\n\t\treturn xOrig\n\t}\n\tr := roundFn(x)\n\tif r != x {\n\t\treturn r\n\t}\n\treturn roundFn(x*0.5) * 2.0\n} Let's analyze how this works. The first 6 lines handle some special cases. The next 4 set roundFn to Ceil or Floor depending on whether the input is negative. The following line stores the original input. Now it gets interesting: x -= math.Copysign(0.5, x) This moves x closer to zero by 0.5 . if x == 0 || math.Signbit(x) != math.Signbit(xOrig) {\n    return math.Copysign(0.0, xOrig)\n} Next it checks if x is equal to zero or went over zero to the other side (the sign change checking). If either of those happened, then the input was <= 0.5, so an appropriately signed zero is returned. if x == xOrig-math.Copysign(1.0, x) {\n    return xOrig\n} This tests for large inputs, for which x-0.5 == x-1.0 , and returns the input unchanged. r := roundFn(x)\nif r != x {\n    return r\n} Next the ceil or floor func is executed and returned if it mutated the input, which can only happen if the original value's fractional part was not exactly equal to 0.5 since subtracted 0.5 from the input earlier. return roundFn(x*0.5) * 2.0 Here the fractional part is equal to 0.5 so we need to round to nearest even (remember this isn't the same as away from zero like all the others; this is just how Postgres rounding works). The comment in the code describes it best: Dividing input+0.5 by 2, taking the floor and multiplying by 2 yields the closest even number. This part assumes that division by 2 is exact, which should be OK because underflow is impossible here: x is an integer. We could change this line to round away from zero with: return xOrig + math.Copysign(0.5, xOrig) Which makes this funtion work except when the input is exactly equal to 0.5 or -0.5 , because those cases are handled specially above. Notably, Postgres does not provide a round(x, n) function as it is likely really hard to do correctly, since it has two difficult problems in one, as we've seen above. (CockroachDB does have that function, but it cheats a bit by converting the float to an infinite-precision decimal, rounding there, and converting back.) github.com/montanaflynn/stats An implementation supporting precision selection at github.com/montanaflynn/stats works for the test inputs if specifying 0 precision. (Note that it doesn't detect the overflow if precision is high, but otherwise is a good implementation.) With comments and the precision code removed: func round(input float64) {\n\tif math.IsNaN(input) {\n\t\treturn math.NaN()\n\t}\n\tsign := 1.0\n\tif input < 0 {\n\t\tsign = -1\n\t\tinput *= -1\n\t}\n\t_, decimal := math.Modf(input)\n\tvar rounded float64\n\tif decimal >= 0.5 {\n\t\trounded = math.Ceil(input)\n\t} else {\n\t\trounded = math.Floor(input)\n\t}\n\treturn rounded * sign\n} The key difference between this algorithm and others is the use of math.Modf , which correctly splits out the fractional and integer part. math.Round in Go 1.10 Some months after the release of Go 1.8, someone re-requested the addition of math.Round . This discussion continued to post broken round implementations (bringing the number of broken implementations up to something above 8). But happily, the Go team has agreed to add math.Round in Go 1.10! Even more happily, someone has posted a working implementation . func Round(x float64) float64 {\n\tconst (\n\t\tmask     = 0x7FF\n\t\tshift    = 64 - 11 - 1\n\t\tbias     = 1023\n\n\t\tsignMask = 1 << 63\n\t\tfracMask = (1 << shift) - 1\n\t\thalfMask = 1 << (shift - 1)\n\t\tone      = bias << shift\n\t)\n\n\tbits := math.Float64bits(x)\n\te := uint(bits>>shift) & mask\n\tswitch {\n\tcase e < bias:\n\t\t// Round abs(x)<1 including denormals.\n\t\tbits &= signMask // +-0\n\t\tif e == bias-1 {\n\t\t\tbits |= one // +-1\n\t\t}\n\tcase e < bias+shift:\n\t\t// Round any abs(x)>=1 containing a fractional component [0,1).\n\t\te -= bias\n\t\tbits += halfMask >> e\n\t\tbits &^= fracMask >> e\n\t}\n\treturn math.Float64frombits(bits)\n} For those unfamiliar with how floats are implemented (I'm on that list), this function looks magical. Let's dig in and see how this works. Looking at the first two lines of code (skipping the constants): bits := math.Float64bits(x)\ne := uint(bits>>shift) & mask It looks like we get some bits, and are selecting some information out of them in the shift and mask. From IEEE 754 : The encoding scheme for these binary interchange formats is the same as that of IEEE 754-1985: a sign bit, followed by w exponent bits that describe the exponent offset by a bias, and p−1 bits that describe the significand. Looking at the consts above, the shift is 64 - 11 - 1 , which is 64 total bits less 11 for the exponent and 1 for the sign, or 52 bits for the mantissa (or significand). This means the shift is removing the 52 mantissa bits and the mask is removing the sign bit, leaving us with just the exponent. switch {\ncase e < bias: Exponents are offset by a bias, 1023 in this case, which means you have to subtract 1023 from the e computed above to get the actual exponent. Or, as written above, if e < bias , then we have a negative exponent, which means the absolute value of the float must be 0 < x < 1 . Indeed, the code reads: // Round abs(x)<1 including denormals.\nbits &= signMask // +-0\nif e == bias-1 {\n    bits |= one // +-1\n} Here bits is masked with the sign bit, so it will be 1<<63 if negative or 0 if positive. This is only used to preserve the correct sign: we can completely ignore the mantissa now. We can do that because what we actually care about is the exponent. Exponents in floats are in base 2, not 10. The representation is: (sign) (mantissa) * 2 ^ exponent . Since we are already in a e < bias block, we know that the smallest exponent we could have is -1. 2 ^ -1 is 0.5 . Furthermore, the mantissa has some value 1.X , where X are the bits of the mantissa in base 2. Thus, with exponent -1, the float must be in the range [0.5, 1). If the exponent were smaller at -2, then the float would have some value less than 0.5. So, if e == bias-1 , we are >= 0.5, and thus need to add one to the result. Phew. See Double-precision floating-point for the details about this. Now the second case: case e < bias+shift: What you think is going to be the condition in this case statement is case e > bias to cover all of the positive exponents. But instead we only get a subset of them. The use of shift here is especially interesting because it doesn't seem to be of a compatible unit with bias. One is the number of bits to move, the other is a numeric offset. But, since floats are represented as (1.mantissa) * 2 ^ X , if X is larger than the number of bits in the mantissa, we are guaranteed to have a value with no fractional part. That is, the exponent has moved the decimal point to the right enough that the mantissa is completely to its left. Thus, this case statement ignores float values that are already rounded. // Round any abs(x)>=1 containing a fractional component [0,1).\ne -= bias\nbits += halfMask >> e\nbits &^= fracMask >> e The first line here is easy: remove the bias out of e so we get the real exponent. The second line adds 0.5 to the value. This works because the highest bit of the mantissa contributes 0.5 to its final sum (see the representation linked in the wikipedia article above). In the case that this sum overflows the 52-bit bounds of the mantissa, the exponent will be increased by one. The exponent won't ever overflow to the sign bit because the exponent can't be higher than bias+shift from the case above. In either case, the fractional part is cleared. Thus, if the fractional part was >= 0.5, it will increase the value by 1, otherwise it will truncate it. Tricky, and not at all obvious until we looked deeper. Conclusion This post has mostly described away-from-zero rounding, but there are many others . Some applications may need others, and it is an exercise to the reader to figure those out. With the description of how correct rounding is performed in Go, though, it should now be more clear how to correctly write and evaluate rounding implementations. I think the Go team made the correct decision to reconsider the addition of the Round function in the standard library. Without that, we were stuck with lots of broken implementations. It is also no surprise that they chose to not add a function that accepted the number of digits to round, since that adds some additional complication that can quickly break things. The other insight here is that there are some very subtle issues with floats, and even experts can get them wrong. The \"just <one liner> \" copy pastes from issues are easy to come up with, but tricky to get correct. Finally, correctly rounding floating point numbers is ridiculously hard. It is no surprise that Java was broken for 6 major versions (15 years since the release of the Java 1.0 until Java 7). At least Go got there in less time than that. (Also published on Matt Jibson's blog .)", "date": "2017-07-06"},
{"website": "CockroachLabs", "title": "Distributed SQL (NewSQL) Made Easy: How CockroachDB Automates Operations", "author": ["Bram Gruneir"], "link": "https://www.cockroachlabs.com/blog/automated-rebalance-and-repair/", "abstract": "A modern distributed database should do more than just split data amongst a number of servers; it should correctly manage partitions (or shards). Moreso, it should automatically detect failures, fix itself without any operator intervention, and completely abstract this management from the end user. This post is the first in a series on how CockroachDB handles its data and discusses the mechanisms it uses to rebalance and repair. These systems make managing a CockroachDB cluster significantly easier than managing other databases. Data Storage Before we get into the details of automated rebalancing and repair, we’re going to have to start at the basics. So let’s take a quick look at how CockroachDB stores, partitions, and replicates data. Data is Stored as Key Value Pairs Partitioned into Ranges CockroachDB stores all user data and almost all system data in a giant, monolithic, sorted map of key value pairs. This keyspace is divided into what we call ranges, contiguous chunks of the keyspace, so that every key can always be found in a single range. A key may have a corresponding value, and if it does, this value is stored in that range. This figure depicts the full keyspace with each blue line intersecting with the horizontal keyspace denoting a key that’s been inserted. Each key has some random value associated with it. Note that the keys are in alphabetic order. This keyspace is then subdivided into ranges (subsets of the keyspace), shown here as sideways parentheses. No ranges ever overlap and every key, even ones that have not yet been written to, can be found in an existing range. Now there’s something missing here. This is clearly just the key-value layer and we’re a distributed SQL database . All SQL tables, rows, columns and index entries are mapped to keys behind the scenes. To see how this works, this post covers most of the details. Ranges are Distributed CockroachDB is a distributed database designed to be run across multiple nodes (servers or VMs) joined together into a single cluster. Given this design, it wouldn’t make sense for all ranges to reside on a single node, so CockroachDB spreads the ranges evenly across the nodes of the cluster: In this figure, each range is assigned to a node. What’s important here is that that ranges can be on any node and that more than one range can be a node. Ranges Split as They Grow As more data is added to the system, more keys will have values associated with them and the total size of the data stored in ranges will grow. CockroachDB aims to keep ranges relatively small (64 MiB by default). This size was chosen for two reasons. If the ranges are too large, it means that moving a range from one machine to another becomes a very slow task, which would slow down the cluster’s ability to repair itself (more on this in a bit). However, if the ranges are too small, then the overhead of indexing them all starts to become a problem. So when a range becomes too large, it’s split into two or more smaller ranges, and these new ranges represent slightly smaller chunks of the overall keyspace. When a split occurs, no data move around, only range metadata does, so the new range will be on the same node as the one it split from. In this figure, some new keys have been inserted and they all happened to fall into Range 3. As a result, Range 3 has become too large and needs to split. It splits into a smaller Range 3 and adds a new Range 6. Note that Range 6 will at first be on the same node as Range 3, since no data has moved during the split, just that some of Range 3’s keys have been assigned to the new range. Ranges are Replicated Organizing the keyspace into ranges makes it possible to store and distribute data efficiently across the cluster, but that alone doesn’t make the cluster robust against failure. For fault tolerance, it’s vital to have more than one copy of each key and value. To that end, CockroachDB replicates each range into what we call replicas (3 times by default), storing each of these replicas on a different node. The more replicas a range has, the lower the chance of data loss from failures, but of course, each change will have to be copied to a larger number of replicas. It’s important here to note that a write command only requires confirmation from a majority of replicas. This means that while latency does increase with the addition of more replicas, only a subset actually affect the latency of that write. To keep all the replica in sync, CockroachDB uses the Raft consensus algorithm. For a more in depth look at Raft, give this video a watch. It’s probably best to to clarify the terminology here. A range is actually an abstract concept, specifically owning a span of the keyspace. While a replica is a physical copy of all the keys and values within that span. So when a range splits, it happens on all replicas at the same time. The Raft consensus algorithm functions on the range’s level, keeping all of its replicas in sync. In this figure, we’re going to zoom out, and look at each node. Here 5 nodes are depicted, each with a collection of replicas. Each range from earlier is coloured the same way to make them easier to differentiate. Note that while in this diagram no two ranges have replicas on the same set of nodes, there is no system in place to prevent that. For a range to be considered available to serve reads and writes, a quorum (anything greater than 50%) of replicas must be present. With this in mind, when a single node fails (for whatever reason), the rest of the cluster can keep functioning. This ability to continue functioning during failures is one of the key advantages of a distributed system. But being distributed isn’t enough. If there is no automated repair policy, the next failure might cause some data to become unavailable. In this figure, we can see that in the case of a single node failure, Node 3, the cluster as a whole can still function, including the ranges that have replicas on the dropped node. A quorum (>50%) of replicas is required to maintain availability. All affected ranges clearly have 2 copies and are thus still available. If a 2nd node was affected, there may be some ranges that could become unavailable. Due to their small size (as discussed earlier), replicas can be moved quickly and freely between nodes. This is a key point on how CockroachDB automatically balances the cluster, and we’re going to get back to it in a bit. How We Know Where Range Replicas Live To keep track of on which nodes all of these replicas reside, we keep a two-tiered index into all ranges’ replicas. We call these the meta indexes. For each replica, a key in the first-level index points to a key in the second-level index, which in turn points to the actual locations of the data. These meta indexes must always be accurate in order for queries to find the keys they’re functioning on so they are stored just like all other data, as replicated ranges, but in a reserved portion of the keyspace. Whenever CockroachDB moves, adds, removes, or splits these ranges, it therefore does so with the same level of data integrity as for all other data in the system. This is a very significant benefit of CockroachDB, as it means applications never need to know anything about the location of data. They just query any node in the cluster, and CockroachDB handles data access behind-the-scenes. Automated Repair Now that we’ve mapped out how CockroachDB stores data as replicated ranges, what happens when somebody unplugs one of your servers, or a hard drive fails, or some other disaster inevitably occurs? Let’s return to our example above. When a single node fails, all the ranges that have replicas on that node are now missing a replica. Temporary Failure If this failure is temporary, each range will still have a quorum of replicas (66%, assuming each range starts with 3 replicas), so incoming requests will continue to be handled seamlessly. This figure shows that while Node 3 is still disconnected from the cluster, all replicas on that node can become stale. Range 4 had a number of new writes. Range 6 had some deletes. And Range 2 has split into Range 2 and Range 7. When the failed node comes back online and brings the missing replicas with it, it will quickly catch up restoring the specified level of fault tolerance. This figure demonstrates that when Node 3 reconnects, its stale replicas will catch up, including the split operation of Range 2 to Ranges 2 and 7. If there were further changes to range 7, it also will need to catch up. There are 3 ways in which a replica will catch up with a range once reattached to the cluster: If no changes were made to the range, the replica will simply rejoin the range. If there were only a few changes to the range, the replica will receive and process a list of commands in order to catch up. If the number of changes are large the replica will receive a snapshot of the current state of the range. This is the most expensive operation in terms of time, but since the limit for the size of a range is 64 MiB, it can be done quickly. If the range is active, even once that snapshot has been processed, there may still be some new commands that need to be applied. Permanent Failure But what about the case in which the node does not return? In this case, the loss of any other node could potentially result in a loss of a quorum of replicas for some ranges. This loss would in turn make those ranges’ data unavailable. This is a fragile state we don’t want the cluster in for very long, so once a node has been missing for five minutes, it is considered dead, and we automatically start the repair process. This five minute time is a customizable cluster wide setting server.time_until_store_dead which can be changed proactively if there is a longer than five minute downtime expected on a node. How We Repair Each CockroachDB node is continuously scanning all of its ranges. If any node notices that one of its range’s replicas is on a dead node, a new replica is added on a healthy node and the dead replica is removed. Since the ranges needing repair are distributed throughout the cluster, this repair process occurs across the system and the replicas being added will also be spread out. When a node is removed, there may be lot of ranges to repair, so these repairs are done at a slow yet steady pace designed to not impact latency. This process is based on the same heuristic we use for rebalancing data, which we’ll talk about next. In this figure, Node 3 did not reconnect, so the ranges with missing replicas, 2, 4, 6 and the newly created 7 all add new copies on other nodes. Automated Rebalancing One of the most important and novel aspects of CockroachDB is how it continuously rebalances data behind-the-scenes to fully utilize the whole cluster. Let’s start by considering some scenarios that would necessitate rebalancing: Scenario 1: You’re performing a large number of inserts on a 5-node cluster. The ranges affected are all on 3 of the 5 nodes, and those 3 nodes are starting to fill up while the other 3 nodes are pretty empty. In this figure, range 4 has grown significantly and has split into ranges 4, 7, 8 and 9. This has created an imbalance such that nodes 1, 2 and 3 now have significantly more replicas than the other nodes. This is remedied by rebalancing. A replica for Range 4 is moved from Node 1 to Node 5. A replica for Range 8 is moved from Node 1 to Node 4. And finally a replica for Range 7 is moved from node 3 to node 5. Now all nodes have either 5 or 6 replicas and the balance has been restored. Scenario 2: Your entire cluster is getting a little cramped, so you decide to add some new nodes to free up capacity and balance things out. This figure carries on after Node 3 was removed and the cluster repaired itself by making additional copies of the missing replicas. That node was replaced with a new node, Node 6. Once added, there is a clear imbalance. This was remedied by having Node 1 receive a number of replicas. It took a replica for Range 1 from Node 1, a replica for Range 3 from Node 5, and replicas of Ranges 5 and 6 from Node 4. Now all nodes have either 4 of 5 replicas and the cluster is again balanced. Scenario 3: Availability is important for a specific table that is queried frequently, so you create a table-specific replication zone to replicate the table’s data 5 times (instead of the default 3). In this figure, Range 1’s replication factor has been increased from 3 to 5. As two new copies are needed, new replicas of range 1 are added to nodes 2 and 5. In each of these scenarios, there’s a clear need to move replicas around, to less full nodes, to new nodes, or to nodes matching defined constraints. In classic sharded SQL databases, accomplishing this would require manually resharding the whole database, a painful process involving complex application-level logic and plenty of downtime (or at the very least a painful transition) that’s best avoided if possible. In most NoSQL databases, this process usually needs to be carefully planned and scheduled for performance reasons. In contrast, in CockroachDB, the rebalancer is always running and is designed to have minimal impact on performance. There is no need to tune or schedule it, and no need to make your application aware of any changes. Our Heuristic So how does CockroachDB decide which nodes should be rebalanced and which nodes are eligible to receive rebalanced ranges? After considering replication zone constraints and data localities, which will be the focus of an upcoming blog post, the primary factor in these decisions is the number of ranges on nodes: Mean number of ranges in the cluster: CockroachDB calculates the mean number of ranges across the cluster and considers it the ideal range count for each node. For example, in a five-node cluster, with four nodes containing 20 ranges and the fifth node containing 25 ranges, the mean and ideal count per node would be 21. Actual number of ranges on each node: If the number of ranges on a node is above the mean, the node moves enough ranges to other nodes to bring its range count closer to the mean. For example, continuing the scenario above, if 21 were the mean and ideal range count per node, the node with 25 ranges would move 1 range to each of the four other nodes, bringing each node to 21 ranges. Node storage capacity is also important. Once a node is at 95% capacity, it is no longer considered eligible to receive rebalanced ranges. If any node gets to this state, however, it is a good indication that additional nodes need to be added to the cluster -- a dead simple process. Once nodes have been added, the cluster quickly rebalances as described above. (To see this happen in real-time, try out our getting started tutorial on Scalability .) Preventing Thundering Herd When we first implemented rebalancing, adding a new node to a cluster caused a massive rush as all the other nodes immediately tried to move replicas onto it. To prevent this type of “thundering herd” scenario, we implemented a limit on the number of incoming rebalancing requests to a new node. Requests above this limit are rejected, and requesters are forced to either wait or find a different home for the replicas in need of rebalancing. Preventing Thrashing Another issue we encountered early on was what we called “thrashing”. Our original heuristic of moving replicas around based just on the mean caused some ranges to keep moving back and forth, ad nauseum. This meant that the cluster would never reach equilibrium such that a lot of replicas were continuously being moved back and forth between nodes. This continuous unneeded movement uses up resources and can add latency to the system. To combat this, a buffer was added to both the high and low sides of the mean (5%). So only once a store was above the mean plus this threshold, was it considered for rebalancing and we only consider a rebalancing if moving a replica actually pushed the cluster as a whole closer to the mean. Moving a replica The state of a cluster is always in flux; each node, store and range may be adding and removing data at any time. Replicas are being moved around and repairs may be taking place. In this type of chaotic environment, there is never any point in which any single part will know exactly the full state of the cluster and having a single node act as the coordinator for rebalancing would be both a single point of failure and as the system grew larger, a bottleneck that may slow down rebalancing. Thus the rebalancing problem is solved by allowing each range to determine its own fate by looking at what it thinks the current state of the cluster is. Information about other stores is shared using our gossip network. The details of how that works would be a little be too far afield for this post, but for our purposes, all we need to know is that it spreads the latest replica count, disk sizes, and free space about a store to all other stores in an extremely efficient manner. But this information spreading is not instantaneous and there will sometimes be small difference between these values and the actual ones. In order to perform any repairs or rebalances, each store is continuously looping through all of the replicas that it controls and checks if any of them require repairing (due to not having enough replicas) or rebalancing (when another store is a much more attractive location for one of its replicas). As previously discussed, if there is a need for a repair, it adds a new replica to a store that meets all constraints and has the most free space and removes the reference to the now missing replica. To rebalance the range, it first add the new replica on a new store and then removes the extra replica. Future work Clearly the rebalancing and repair mechanism, in its current state, is using only a very limited amount of data to make decisions. There are many promising directions we would like to take this in future releases. Right now, there’s an effort to rebalance based on the actual size of the replicas instead of just their count and to pinpoint hot nodes to spread their replicas around, improving their load and latencies overall. Also, for ranges that are experiencing very high loads, perhaps a way to spread that load around would be to split them, spreading out that load. For more details, take a look at this RFC . Final Thoughts These techniques discussed above allow Cockroach to continuously rebalance and spread ranges to fully utilize the whole cluster. By automating rebalancing, CockroachDB is able to eliminate the painful re-sharding procedure that is even still present in most modern NoSQL databases. Furthermore, by automating repairing and self-healing, CockroachDB is able to greatly minimize the number of emergencies due to failing machines. Since this repairing happens without having to schedule any downtime or run specific repair jobs during slow times it is a huge benefit to anyone trying to keep an important database up and serving load. Our goal from day one has been to Make Data Easy , and we feel that by automating a lot of these tasks, we’ve freed up those who manage databases from the tedium of dealing with these menial and potentially dangerous tasks to instead be able to think about expanding and improving the cluster as a whole. But there’s much more. We also provide controls, using database and table constraints, which allow you to be a lot more specific about where your data reside and the details of how these function will be covered in an upcoming blog post. Illustration by Rebekka Dunlap", "date": "2017-10-05"},
{"website": "CockroachLabs", "title": "Kindred Futures and Cockroach Labs Partner to Build Next Generation Global Online Gaming Platform", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-kindred-partnership/", "abstract": "We are excited to announce a partnership with Kindred Futures to build the next generation global online gaming platform for their parent company Kindred Group plc . Kindred is a rapidly growing, global business with strict data privacy and technical requirements. Their ambitious project to build a global online gaming platform with multiple active data centers that span continents is an exciting opportunity for our team. Furthermore, the close collaboration between our engineering teams is helping to shape the development of CockroachDB, starting with a design partnership for a geo-partitioning feature planned for the 1.2 release. We recently sat down with Kindred CTO Marcus Smedman to discuss the partnership and learn why he thought CockroachDB was the right solution for Kindred. How did you first meet the Cockroach Labs team? What made them stand out? Marcus - We first met Cockroach Labs a couple of years ago. They immediately stood out because they were trying to solve a problem that we were increasingly facing but for which no suitable solution existed in the marketplace. They are pretty much unique in their approach and capabilities. What does CockroachDB’s product or approach do for Kindred? What problem is it solving? Marcus - As Kindred continues to grow rapidly, gaining customers from across the globe, we need to ensure that we can still deliver the optimum experience – wherever our customers are in the world. The further they were from our data centers the more lag they would experience – which is not only frustrating but a deal breaker if trying to bet on fast moving in-play betting markets. By working with CockroachDB, we expect to be able to operate multiple active data centers – all perfectly synchronised with each other. This will allow us to offer the fastest possible performance of our products and services. How did the partnership come about? Marcus - It developed slowly actually. We took the time to learn about CockroachDB and their approach, and in parallel they learned about us and our particular challenges. As we both learnt more about each other it became apparent that there was the potential for considerable mutual benefit if we worked closely together, as partners. The strong foundation we built during that deliberately slow learning period is paying dividends through a really close fit – in terms of product direction and team dynamics. Why were you looking for a partner or new vendor? Why weren't current tech providers the right choice? Marcus – Outside of Google, there is no solution like this available anywhere. We had a well understood problem, and needed to find the right people to solve it with us. Cockroach Labs have repeatedly shown themselves to be that ideal partner. Why a partnership, rather than a standard operator/supplier relationship? Marcus – This is no off the shelf product, or it wasn't when we started working together. Through close collaboration we hope to help shape or influence the development of the solution, a least a little bit anyway, although we are quick to acknowledge that they are bringing all the expertise! This close collaboration simply would not happen in a standard operator / supplier relationship, where we would have to simply fit into a predetermined product roadmap that was not designed around our needs. With Cockroach Labs we have what feels like the opposite. Ultimately we would like to have a great solution to our particular challenge and at the same time, we would like to see Cockroach Labs go on to great things, becoming a hugely successful international company. If our partnership can contribute to their success in any way, then we will be really happy.. When you’re working with a start up, what specific benefits and/or challenges does that bring? Marcus - The two companies are of course different in many ways, with very different technical, cultural and structural set ups. Part of being partners is understanding each other's ways of working, and any particular priorities and constraints there might be. I’m sure at times our internal processes - pretty much inevitable for a medium size company - have meant we have moved a little slower than Cockroach Labs would have been able to, as a smaller more agile company but they understood that and were happy to move at our pace. Equally we had to understand they were working on a brand new product, learning as they went, growing fast. We are more used to working with established companies with well defined processes, but were happy to flex that approach to accommodate the startup journey they are working though. How do you see the partnership developing further? Marcus - CockroachDB is still in its early days with version 1.1 released. The future release plans are really exciting and are integral components of solving our global performance challenge. We hope to see this partnership continue to grow and strengthen, delivering ever increasing benefit to both parties.", "date": "2017-10-16"},
{"website": "CockroachLabs", "title": "Is CockroachDB Good for Hackathons?", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/hack-the-north-2017/", "abstract": "Hackathons are––for the uninitiated––a little insane, and HTN one of the craziest among them. 1,000 students from all across the globe (literally!) descend on Waterloo, Ontario for a 36-contiguous-hour event to build a team and create interesting software. While students might be motivated by prizes awarded in cash, pride, swag, and lucrative employment opportunities––it feels like most of all, students are eager to learn about new technologies and show what they can do in a day and a half. Last year, Hack the North was Cockroach Labs’s first-ever hackathon and we were thrilled by it. The energy and ingenuity were unparalleled, so when we were invited back to sponsor Hack the North 2017, we took up the opportunity immediately. And we’re pleased to announce that this year an exceptional group of hackers didn’t disappoint. Fame, Glory...and $1,000! While learning about technology is certainly rewarding, a cash prize does a lot to create a sense of competition. Because building a database-driven app (especially if you never have) can be a time-intensive process, we offered a $1,000 prize for the team who built the best app using CockroachDB––and their efforts were impressive. With the clever designs we saw throughout the weekend, we have to say that the answer to, “Is CockroachDB good for hackathons?” is a definite “yes.” With that, I’m pretty excited to announce the winners! SumNotes: Automatic Note Taker Powered by CockroachDB SumNotes solves a problem faced by a lot of students: taking notes while a professor lectures can be hard (especially at a school as demanding as Waterloo). Using their application, students can record lectures, have the entire lecture converted to text, and then automatically summarized. When we saw the demo, everyone on the Cockroach Labs team was really thrilled. It was a novel premise with a really elegant solution. And when we saw the team’s data model, we were stoked––it was not only the most complex of all the hackers, they also used CockroachDB in multiple ways. Not only was it used as an authentication system, but the transcription and summarization both used the database. And––on top of these other achievements––half of the team are high school students! A big congratulations to SumNotes for great work, and we hope they enjoy their prize. SumNotes on Devpost Honorable Mentions While SumNotes took home the prize, there were two other teams that we wish we could have awarded the prize! CockroachNest CockroachDB features a built-in AdminUI, but when these hackers saw it, they immediately thought of some interesting opportunities to help operators manage their deployments. Their hack ended up featuring a real-time cluster visualization tool that would provide you a quick glance of where your nodes were located and their current status. There were a few things about this that impressed us. First, the team actually set up an internationally-distributed cluster of 8 nodes across 3 continents. Secondly, CockroachNest shows a real solution to a problem that Cockroach Labs ourselves want to solve. Great work by this team! CockroachNest on Devpost Hyper Tickets Hyper Tickets leverages blockchain technology to eliminate ticket fraud––which is a real problem. In the UK alone, consumers lost £1.2 million to schemers selling phoney tickets. This team set up a distributed, load-balanced cluster on the Google Cloud Platform using our docs , and tracked ticket holder information through CockroachDB. It’s great to see the entrepreneurial spirit alive in this team: showing that some clever engineering can solve real world problems. Hyper Tickets on Devpost Want to show us what you’ve got? Whether you’re a student hacker, or a seasoned pro, we’d love to hear what you’re building now or dream of doing in the future. Find us at any of our upcoming events . Illustration by Lisk Feng", "date": "2017-10-10"},
{"website": "CockroachLabs", "title": "How to Work with Me", "author": ["Kuan Luo"], "link": "https://www.cockroachlabs.com/blog/how-to-work-with-me/", "abstract": "Some of my colleagues have children to tend to after work and many are night owls who are most productive during the wee hours while I’m drooling in my sleep. While I’m trying to figure out how to best work with them, they’re also trying to figure out how to work with me. Is it okay to send me Slack messages after 7pm? What is the best way to give me constructive feedback? Instead of tiptoeing around with our guesses, why not document our boundaries so others can access and learn about our work preferences in order to work better with us? I first heard about the idea of How to Work with Me from my friend Jen at Dropbox. When I brought it up with Lindsay, our Head of People, it was obvious how beneficial the document would be for new Roachers and veterans alike. How to Work With Me - Cockroach Labs Edition Our 1.0 version of How to Work with Me is a Google doc that asks everyone at the company, including the founders, to answer the following five questions: My work hours: How should you contact me? If I don’t respond immediately, try: How do you like to give feedback? How do you like to receive feedback? Things I’m more than happy to talk about: The questions are chosen based on our company values. Since we have flexible work hours, the first two questions set expectations for when and where one is expected to be mostly responsive. In order to give peer-to-peer feedback effectively to help each other grow, it’s also extremely useful to know how one might prefer to receive it. The last question serves as a conversation starter for 1:1 and coffee meetings to get to know each other. To make the activity more fun, we made filling out our own How to Work with Me doc into a mini team event. At around 4pm several Thursdays ago, we sat around the main conference room table with our laptops, wine and cheese. When we were done we’re done, our works of art looked like this: The success of How to Work with Me , according to staff engineer Raphael, is that it provides yet another way to foster and maintain team bonds. “By learning how to approach and interact with each other throughout the team, even when we don't have a direct reason to work with someone in particular,” he said, “we provide a better ‘feel’ to everyone about the dynamics of the entire team.” And for those who are new to the company, the document is now part of the onboarding process for new employees. Make it Yours Flexible hours and cultural freedom are the new norm at tech companies, and How to Work with Me is a very efficient way to bring transparency to teams and foster company culture. “Coming from the business world, CockroachDB was the first time I had to work directly with engineers.” said product manager Diana, “Through the exercise, I learned that engineers need to have blocks of undisturbed working time. They also prefer to be communicated with in as straightforward a way as possible - the less small talk on Slack, the better!” Interested in trying it out at your organization? Here are three tips to customize the exercise: Ask relevant questions. No two companies have the same culture. Picking a variety of the most relevant questions is going to yield the most success amongst the recipients. Start small. How to Work with Me is effective even amongst the smallest unit in the company. If it’s impossible to get 500 people to do the exercise at once, try pilot it with a smaller team, get some feedback and then spread the fire. Keep it short. We wanted to make the the exercise as burdenless as possible, so we started with 5 questions. Go wild , but be prepared for the participation rate and readership to drop consequently. Conclusion I’ve learned from the exercise that it’s okay to interrupt Chelsea from the Ops team when she has her headphones on, and staff engineer David reads code reviews at home with his fresh pot of coffee before coming to the office at 11am. It’s an accumulation of tiny, nuanced insights like these that make interacting with my colleagues and working in the office more dynamic and comfortable. Download the template and get your version of How to Work with Me . For the record, please slack or email me if my headphones are on. Illustration by Tsjisse Talsma", "date": "2017-10-19"},
{"website": "CockroachLabs", "title": "CockroachDB 1.1 Released: Production Made Easy", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-1dot1/", "abstract": "Today, we are thrilled to announce the release of CockroachDB 1.1. We’ve spent the last five months incorporating feedback from our customers and community, and making improvements that will help even more teams move to CockroachDB. We are also excited to share success stories from a few of our customers. Baidu, one the world’s largest internet companies, shares how they are using CockroachDB to automate operations for applications that process 50M inserts and 2 TB of data daily. Heroic Labs, a software startup, shares how they simplified deployment of their gaming platform-as-a-service by packaging CockroachDB inside each server. CockroachDB 1.1 focuses on three areas: seamless migration from legacy databases, simplified cluster management, and improved performance in real-world environments. Quickly migrate your data… and code. As we approached the 1.1 release, we wanted to understand the sticking points teams had when migrating from traditional RDBMS and NoSQL databases to CockroachDB. We identified issues around data transfer and application migration, and we’ve taken major steps to improve the experience for both. First let’s talk about your data. We launched CockroachDB with basic import functionality that mirrored what Postgres offered. This was fine for small data sizes but as we saw customers with terabytes and, in some cases, petabytes(!) of information looking to move to our database we had to deliver much, much faster facilities for supporting data import. We decided to take the scale-out approach used by our distributed backup and restore functionality and extend it to CSV imports. This feature can reduce the time it takes to migrate large data sets to CockroachDB from hours to minutes. This release also helps your team migrate both apps and expertise to CockroachDB by improving its SQL coverage and implementing postgreSQL features that enable us to better support ORMs like Hibernate and ActiveRecord . We’ve added an array type to give you more options for working with lists and improving the performance of certain queries by keeping related data together. Take control of your global clusters One of the reasons teams gravitate towards CockroachDB is it was built from the ground up to automate the heavy operational lifting like sharding, recovery, and rebalancing. However, operators still need visibility into what’s going on in their cluster and controls to prevent bad queries from degrading performance. With our 1.1 release, CockroachDB now gives operators real-time visibility and control of ongoing cluster activity. Let’s see how. Jobs Table in the Admin UI First we are introducing the jobs table, your hub for seeing all of the long-running work happening in your cluster. This includes things like schema changes, CSV imports, and CockroachDB Enterprise backups and restores. The jobs table lets you see what is happening, who triggered it, and estimated time remaining. You can now examine how new jobs are impacting the performance of the cluster and utilize new commands to cancel, pause, and resume backups and restores to keep your cluster healthy. SHOW QUERIES CockroachDB now provides greater visibility into running queries using the SHOW QUERIES command via the SQL CLI. Additionally, there are new SQL commands to cancel queries that match certain criteria in order to quickly check cluster capacity or prevent an errant query from doing too much damage. The combination of the jobs table and better query management provide the necessary tools to help you understand and manage your cluster. This is only the beginning; the suite of tools will grow with each subsequent CockroachDB release. Improved performance for cloud environments CockroachDB 1.1 has tightened latency and improved throughput across a variety of performance benchmarks. When tested against a high concurrency key-value workload, we saw our average latencies drop below 5ms (a 13% improvement, with 95-percentile latencies falling 11% to 17ms) and a modest throughput increase to 44k queries per second (a 14% improvement). For enterprise users, we’ve made tremendous improvements to our distributed backup and restore capability, which can now restore a database 17x faster than it did in the 1.0 release. Finally, we’ve laid the groundwork for a robust performance testing infrastructure, continued to focus on OLTP performance with an emphasis on TPCC workloads, and tested clusters of up to 128 nodes. We are improving performance in every release, and will share more about this topic in the months ahead. -- CockroachDB 1.1 is the next step in a long journey to transforming the way the world works with data. This post covered updates around migration, monitoring, and performance, but this is a small sample of what we delivered in the 1.1 release. Check out the release notes to see the full list. If you want an early look into how 1.2 is shaping up, checkout our public roadmap . You can download the latest version of CockroachDB here . Let us know what you think! We’re looking forward to hearing your feedback and success stories. Illustration by Rebekka Dunlap", "date": "2017-10-12"},
{"website": "CockroachLabs", "title": "What Does Clarity of Color Have to do with a Mission-Critical Database?", "author": ["Josué Rivera"], "link": "https://www.cockroachlabs.com/blog/admin-ui-clarity-of-color/", "abstract": "When the “worst case scenario” you expected your database to experience someday is unraveling before your eyes, you need performance monitoring software that’s built like a trusty sidekick, ready to help get your database back on track. That’s why, we’re rethinking our approach to designing monitoring software for mission-critical databases, to create experiences that are intuitive, empowering, and easy for you and your entire team. As the Senior Product Designer at Cockroach Labs, I’ve been thinking a lot about how to evolve and improve the user experience of our Admin UI , and in doing so, I dove head first into the intricate world of information visualization and the science of perception. What I’ve learned is that database performance monitoring software should help you do two things really well: Maintain a level of awareness that allows you to easily spot potential problems before they arise Provide you with the right insight to take the appropriate actions in preventing problems from happening But in the real world, performance monitoring software may at times create more noise than clarity, provide you with insufficient context to make informed decisions, and fail at giving you the right tools to take action. In this blog series, I’ll be summarizing some of the interesting things I’ve come across during my research and we’ll explore some ideas to help us design better experiences for mission-critical software. Each post is focused on a theme: Clarity of color Constructing meaning Encoding for action Let’s start with Clarity of color . Too much noise Take a look at Figure 1, it’s a screenshot of a real-time data visualization with a collection of time-series graphs. It was taken from the popular performance monitoring software Grafana: Figure 1 - Taken from the results of a Google search on “Grafana dashboards”. It appears to be a typical time-series performance dashboard created to keep track of a cluster. It’s pretty, but can you decipher, at a glance, what the most important metric we should be paying attention to is? ⌛Neither can I. We can dissect all the problems with this dashboard for hours but let’s focus on one aspect that’s creating a lot of noise: color. Color is irrelevant to much of our normal vision—it doesn't help us determine the layout of objects in space, how they are moving, or what their shapes are (Ware). But color does have a critical function: it helps us see distinction among similar objects, like spotting cherries in a bush: Figure 2 - Photo of an illustration from Colin Ware’s “Information Visualization”. During visual perception, a preconscious form of information processing is at work - it’s known as preattentive processing . Certain attributes of what we see are recognized during this processing at extremely high speed, which results in certain objects standing out, all without conscious thought (Ware). Color is a preattentive attribute that can be effectively used to highlight differences between objects in visualizations. Let’s see another example of how this works. Count how many instances of the number 5 are present: Okay, try it again: Did you notice the difference? Me too! Our brain uses the easiest and quickest method to filter out the bits of information that are most likely going to be useful in accomplishing tasks. When we design data visualizations with fast brain systems in mind, users can more easily identify what information they should focus their attention on. Let’s take a look Figure 5. The contrasting red squares stand out from the field of analogous blue/green ones, and capture your attention first. Figure 5: Image taken from “Expert Color Choices for Presenting Data” by Maureen Stone. Contrast and analogy may be used as design principles to define color. Contrasting color hues are different, analogous hues are similar. Contrast draws attention, analogy groups (Stone). When designing line graphs for use in time-series monitoring, we need to be careful about the ways in which color is used to encode data. Encoding data with color Let’s take a look at Figure 6, a screenshot of a dashboard for the real-time monitoring software Librato Agent. Figure 6: Image taken from the result of a Google search on “time-series monitoring graphs” Contrasting color hues can create unnecessary noise and result in a visualization that is taxing because it “has too many distinctions for the viewer to be able to sort quickly, thereby requiring careful, conscious, and thus slow examination on the part of the viewer” (IDD). There are also too many colors being used to represent each line for each time-series graph, such that you’d have to be very attentive to see the differences between them. Due to our limited working memory , qualitatively encoding too many items can make it even more difficult for users to spot the exceptional items that really need attention (Few). Precisely spotting hue variations of a particular color is not something that humans are very good at doing, let alone quickly (Ware). Hue How many different hues of green can you name? In an interesting study, Post and Green (1986) carried out an experiment on the naming of colors produced on a computer monitor. They generated 210 different colors and asked participants to name the colors they perceived displayed on the monitor. Interestingly, they found that only 8 colors plus white were consistently named with 75% probability: Figure 7: Outlined regions show the colors that were given the same name with bettern than 75% probability. Although an experiment, the results, and other similar studies , suggest that only a very small number of colors can be used effectively as labels in data visualizations (Ware). But if hue isn’t going to help users quickly differentiate between different data encodings, what will? Luminance Under normal vision, the human perceptual system is highly tuned to notice slight variations in luminance (Ware). Variation in luminance can be used to separate overlaid objects more clearly, where low contrast layers can sit behind high contrast ones without causing visual clutter (Stone). Figure 8 demonstrates this principle. (You’ll also notice that hue is not effective at creating the separation between objects.) Figure 8: Image taken from “Expert Color Choices for Presenting Data” by Maureen Stone. In terms of color, this means that instead of using variations in hue we can use variations in intensity of a single hue to greater effect (Few). Even better, variations in the intensity of a single hue are seen as different even by people who are color blind. Keep in mind though, when we vary lines in time-series graphs by color intensity, we’re limited to about five different intensities, any more and it becomes difficult for our visual system to distinguish each line (IDD): Arbitrary Encoding So what about showing an example that doesn’t look like someone threw up a rainbow? Okay, how about this one from another popular performance monitoring software Datadog: Figure 9: Image was taken from the results of a Google Search on “Datadog dashboard” and shows a typical performance dashboard tracking various database performance metrics In this example, the color system looks somewhat less haphazard but we’re running into similar problems. The color, in each panel on the dashboard, is arbitrarily encoded from each other panel. For example, the color blue is used throughout the dashboard to encode different data sets, meaning that in one panel it’s used to encode the frequency for Queries Per Second of a particular machine, and in another panel, it’s used to encode the density of Database Latency values in a heat map. And on top of that, blue is also used for background colors and icon color as well. ¯_(ツ)_/¯ Arbitrarily encoding data with color requires the viewer to focus their attention panel by panel in order to learn the color encoding for each time-series graph. Overtime, for at a glance viewing, the user may learn to discard the color encoding and instead use other visual cues while scanning for things that need attention. As Moritz Stefaner said “color is difficult,” thus, unfortunately, arbitrary color encoding has become somewhat of a common practice in performance monitoring software. We’re also doing it here at Cockroach Labs : We can do better, we must do better, to help users maintain the level of awareness needed to spot potential problems before they arise. Design explorations As we learned, color has intrinsic qualities that make it good at certain things and bad at others. If we design with our perceptual system in mind, we can create visual color systems that empower users to kick ass at what they do. Let’s explore some improvement ideas! What if, most of the color system were grayscale and we used a primary color hue to popout things that require the most attention: Or we can use the primary color with a simple intensity scale to encode how urgent the alerting is: Or what if, the data is encoded with with a primary analogous color family of varying intensities, and we use a contrasting secondary color to popout important information: These are very rough explorations—not answers. But you can see where I’m going with this. There are better ways to create clarity using color than what we currently default to as designers. And by designing with our perceptual system in mind, we can better help our users maintain the level of awareness needed to easily spot potential problems before they occur. Summary Let’s summarize what we learned: If it’s critical that people pay attention to certain information, create a strong signal to make the information easily stand out against the noise Our brains are great at thinking faster than we’re conscious of, so use our perceptual system to your advantage when designing If you’re using color, pay attention to the context in which it’ll be used, and as Edward Tufte said “do no harm” In the next blog post, let’s chat about how we can help users create meaning out of their database performance metrics, and in the meantime, here’s some great material to check out: Sources Tufte, Edward (1990). “Envisioning Information.” Graphics Press, Cheshire, CT Ware, Colin (2013). “Information Visualization: Perception for design.” Morgan Kaufmann, Amsterdam Few, Stephen (2013). “Information Dashboard Design: Displaying data for at-a-glance monitoring.” Analytics Press, Burlingame, California Few, Stephen (2007). “Dashboard Design for Real-Time Situation Awareness.” Perceptual Edge Weinschenk, Susan M. (2011). “100 Things Every Designer Needs To Know About People.” New Riders, Berkeley, California Stone, Maureen. “Expert Color Choices for Presenting Data.” http://www.stonesc.com/pubs/Expert%20Color%20Choices.pdf Illustration by Tiago Galo", "date": "2017-11-02"},
{"website": "CockroachLabs", "title": "Data Migration Made Easy: Bulk Ingest from CSV", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/bulk-ingest-from-csv/", "abstract": "We think CockroachDB is a great database for many people, and want them to try us out. Not just for new applications, but for existing, large applications as well. The first problem that users with an existing database will hit when trying us out for the first time is getting their data into CockroachDB. For the 1.1 release, we built a new feature that performs high-speed, bulk data import. It works by transforming CSV files into our backup/restore format , then is able to quickly ingest the results. CSV was chosen because it is so common, and most databases have ways to export to it (other formats can quickly be added in the future, given enough user need). Although this feature uses our enterprise backup and restore code, it does not (and will not) require a paid license. There is a doc describing all options and usage of CSV importing. Because CSV import uses our enterprise restore feature, it is significantly faster than executing INSERTs or using the Postgres COPY protocol. In addition, the IMPORT statement supports (and encourages!) defining secondary indexes during CSV conversion. The indexes are created right alongside the primary index data, allowing all data to be ingested quickly. Predefining the indexes is much faster than importing and then adding indexes later. Technical Details Let’s go over the story again: a user wants to import a lot of data, and the fastest way to ingest data is with our RESTORE feature. Additionally, CSV is a common export format. Thus, we have made a new feature, IMPORT, that converts CSV to the format expected by RESTORE. This format, as detailed in the above link, relies on contiguous, non-overlapping SST files. So, the job of IMPORT is to convert CSVs into SSTs. The basic idea for an implementation of this is: Read a table descriptor (a CREATE TABLE statement). For each line of a CSV file: Parse and convert it to SQL datums. Run the normal INSERT code that converts these datums to one or more KV pairs. There will be more than one pair if there are secondary indexes, for example. Sort the resulting KVs. Split them into 32MB SST files. First implementation: local, no cluster The initial implementation of this feature was done as a CLI subcommand that didn’t require a cluster at all, it just needed the input CSV files and table structure, and wrote the output to a directory. It was implemented in two phases. Phase one converted the CSVs into KVs and added them to a RocksDB instance for sorting. Although RocksDB does store its data as SST files, those files were not directly used here. We used RocksDB only because it provides efficient sorting and iterating of KV pairs. Since RocksDB was doing the sorting, we were free to insert into it in any order. This allowed us to make the input parsing highly parallelizable and concurrent. We used separate goroutines to read the CSV, convert it into KVs, and write the KVs to RocksDB. The errgroup pipeline pattern was used to coordinate between the various goroutines in case any produced an error, and allowed much faster ingestion than using a naive, single-threaded solution. Phase two was to sort the KVs, then iterate through them in order, producing the desired 32MB files. Overall this implementation performs well, is able to determine the 32MB split points as-needed, but requires 3x disk space (one copy for the CSVs, one for the intermediate RocksDB data, one for the final SSTs). For CockroachDB 1.1, we connected this implementation to our SQL layer for easy use as the IMPORT statement. This allowed us to fetch and store data remotely, reducing the disk requirements to just the middle RocksDB temp data. The entire conversion will still happen on a single node, though. Only when restoring the created SSTs will CockroachDB’s distributed features be used. Second implementation: distributed Although the above single-node implementation works, we thought we would be able to process larger amounts of data faster if we could do it with distributed processing. This second implementation of CSV conversion used the DistSQL framework. DistSQL is meant to move SQL data processing (like WHERE clauses) down closer to where the data is on a disk and allow for concurrent operations across nodes, hopefully producing faster queries. The IMPORT statement’s use of DistSQL is a bit strange since there’s no SQL data on disk, but we were able to adapt it to be useful for this kind of mapreduce workflow. The basic idea is similar to the single node implementation: Assign each CSV file to a node in the cluster. Convert CSVs to KVs. Route KVs by range each to a DistSQL processor. Each processor would collect KVs, sort them, and write the final SST file. All of the steps here were similar to those above, except for the third step: routing. The hard part here is figuring out which processor to route each KV. This is difficult because we don’t know the key distribution before we’ve read the CSVs. To solve this, we read through the CSVs twice. The first read samples the KVs at a certain rate. These samples are sent back to the coordinator, which can use them to estimate where the 32MB split points would be. The second read is then done, but now with the estimated split points, which are used to instruct the readers where to send the KVs. A side effect of this is that the actual sizes of the final SST files are just close to 32MB, with some standard deviation, since sampling doesn’t provide a perfect picture of the data. We are still performing testing and benchmarking of this implementation (and as a result, it is not enabled by default in CockroachDB 1.1). Our goal is that our next major release will have the ability to choose between these implementations based on the size and number of input files so that whichever will be faster for the incoming data will be used. Illustration by Quentin Vijoux", "date": "2017-10-26"},
{"website": "CockroachLabs", "title": "CockroachDB for Windows Users", "author": ["Nikhil Benesch"], "link": "https://www.cockroachlabs.com/blog/windows-binary/", "abstract": "For the first two years of CockroachDB’s existence, the Windows installation instructions read like this: Install Docker. Get the latest CockroachDB Docker image: docker pull cockroachdb/cockroach The instructions were deceptively short, considering that they amounted to downloading a supported operating system (Linux), booting it in a hypervisor, and running CockroachDB inside of that virtual machine. The layers of indirection in this scheme caused no shortage of problems . Docker containers excel at isolating a process from the outside world, but a distributed database like CockroachDB needs just the opposite: each node needs to communicate over the network with other nodes in the cluster and write persistent data to the filesystem. Fat-finger the docker run flags and you’d wind up with a CockroachDB node that writes its data to an ephemeral directory that would be deleted. Or perhaps you’d forget to expose the necessary ports and create three isolated one-node clusters instead of one three-node cluster. Even properly-configured clusters would occasionally run into what could only be explained as bugs in Docker itself, or at least details of the Windows filesystem leaking through the Docker environment. Suffice it to say that CockroachDB on Windows was not a seamless experience. We viewed Docker on Windows as a last-resort solution, and were were hopeful that most prospective users of CockroachDB ran macOS or Linux instead of Windows. After all, we weren’t seeing that many bug reports from Windows users. Or so we thought until earlier this year. In mid-March, we sent a contingent from Cockroach Labs to present at nwHacks , and they reported back some surprising news: the majority of the students they met at the hackathon were running Windows! Unsurprisingly, most of these students got stuck trying to install CockroachDB. We had failed to consider that for every user who reported an issue with installation, potentially dozens more had encountered the same issue and given up silently. So we decided to make the Windows development experience a company priority and invested significant engineer time in producing a binary that could run natively on Windows, cutting Docker out of the process entirely. By May, we’d succeeded: every release of CockroachDB since v1.0-rc.1 has shipped with a precompiled binary for Windows 8[^1] and later. CockroachDB on Windows is now as simple as downloading an executable and running it. Full disclosure: Windows binaries are not rigorously stress-tested like the macOS and Linux binaries, and are provided as a convenience for local development and experimentation. Production deployments of CockroachDB on Windows are strongly discouraged! Getting to this point, however, was anything but simple. Nearly every layer of CockroachDB made some assumptions that were invalid on Windows machines. First, some background. CockroachDB is written in Go, and one of the best things about the Go programming language is its first-class support for Windows. For many Go programs, Windows support is as simple as running go build on a Windows machine. Indeed, many of the Go packages that make up CockroachDB immediately compiled cleanly on Windows. For example, the heart of our SQL execution engine, pkg/sql, worked on Windows on the first try. Why? The SQL engine makes no assumptions about the machine: it simply accepts SQL strings as input, and instructs the underlying key–value store to fetch the necessary data. Unfortunately, the SQL engine is entirely unusable without the layers beneath it in the stack—like the key–value store—and the further you descend in the stack, the more you interface with the operating system. At the lowest layer, we link in a C++ storage engine, RocksDB, which employs all manner of tricks to squeeze out performance. (It has its own memory allocator and file I/O library, for example.) RocksDB made available an experimental Windows port in 2015 , so the hard work of abstracting away the differences between Linux and Windows APIs was already complete. The work was funded by Microsoft, however, and the Windows port only supported using Microsoft’s proprietary compiler, Microsoft Visual C++ (MSVC). We’re committed to open source at Cockroach, so we wanted to make sure that our users could build from source on Windows using free, open source technologies. The MSYS2 distribution platform makes this possible by providing Windows versions of many open-source Linux tools, like GCC, Make, and Bash.[^2] In theory, this means a build system that works on Linux will “just work” on Windows. Where you would have to adapt compiler flags for Microsoft’s compiler, you simply use MSYS2’s GCC. Where you’d have to rewrite a Bash script in PowerShell or Batch, you can simply use MSYS2’s Bash. Of course, reality is never so simple, and MSYS2’s packages don’t perfectly emulate their Linux counterparts. We had to submit several patches upstream to RocksDB to make their build system compatible with MSYS2.[^3] One particularly amusing case was discovered while we were trying to figure out why a test took nearly five times longer to run on Windows than Linux. The answer? Building RocksDB in an MSYS2 environment did not turn on compiler optimizations . Whoops! Impressively, our three other C and C++ dependencies—the Snappy compression library , the jemalloc memory allocator , and Protocol Buffers —all compiled without a hitch in an MSYS2 environment. We owe those project maintainers a debt of gratitude for their cross-platform compatibility efforts, as well as the RocksDB maintainers for testing and accepting our patches. With our dependencies in line, it was time to turn our focus to our own product and remove all the Unix assumptions we’d baked into CockroachDB over the years. The broad classes of fixes we made are listed below, along with some examples. Most of the incompatibilities were textbook examples of platform-incompatible code, but are useful empirical evidence of where it’s easy or tempting to subvert Go’s platform abstractions. Avoiding hardcoded file paths. The null device, which silently discards all data written to it, is spelled /dev/null on Unix but NUL on Windows. The temporary directory is $TMPDIR on Unix, but %TMP% or %TEMP% on Windows. Go conveniently provides os.DevNull and os.TempDir(), which always point to the right places ( ae3e74c , bee33e3 ). Avoiding symlinks. Windows, by default, requires administrator permission to create symlinks. It’s usually not worth asking users to change the system configuration or to run the binary with administrator permissions when symlinks are easily avoided ( 4aeef50 , 34f8629 ). Using raw signals and syscalls carefully. Go allows you to make raw system calls, but nearly everywhere we did so needed to be wrapped in an if !windows check ( 0011737 , 2555400 , 4b91617 ). Baking in some Windows assumptions. It’s was worth baking in some Windows-specific tweaks to keep our users happy. Windows machines don’t ship with a way to decompress tarballs, so we taught our build system to package CockroachDB in a Zip archive instead ( bb60bab ). Also, most Windows users also expect executables to end in .exe, even though it’s technically possible to execute an unsuffixed executable file, so we special-cased the suffix in the build system ( dcd183e ). In the process, we even bumped up against a few shortcomings in Go’s support of Windows, including a broken os.Stat ( 2253a66 ) and a missing API ( 1e4778a ). Both of those issues are happily fixed in the latest release of Go. With all those fixes in place, it was now possible to build Cockroach on Windows! Unfortunately, even a simplified version of the installation process read like this: Download MSYS2 and click through the installer Install GCC, Bash, Make, and Git via MSYS’s pacman Install Go 1.9 Build a RocksDB static library from source Build a jemalloc static library from source Build a Snappy static library from source Build a Protocol Buffers static library from source Move the static libraries into a hardcoded location in your $GOPATH go get -u github.com/cockroachdb/cockroach cd github.com/cockroachdb/cockroach && make build Far too complicated! On other platforms, we would automatically build the right versions of our C and C++ dependencies from source; on Windows, you had to figure out how to build them yourself. This was annoying for our users and, more pressingly, meant that we couldn’t automatically build Windows binaries for our releases. The underlying problem was that go build could not use the MSYS2-related fixes we’d made to RocksDB’s build system. RocksDB ships nearly 1000 lines of CMake to determine the proper compiler and linker flags, with support for many edge cases that have been discovered over the years. go build, however, has no support for integrating with other build systems, or even dynamically determining compilation flags. We’d had to throw out RocksDB’s build system and had hardcoded one set of compiler flags per platform. We’d survived with a set of hardcoded compilation flags that worked on most Linux machines and another set that worked on most macOS machines, but adding a third set that worked on most Windows machine was taking it too far. So we did as many large Go projects do: we abandoned go build in favor of a tried-and-true tool, GNU Make ( #14840 ). To build Cockroach, you must instead run make build. Our Makefile checks to see whether you have an up-to-date build of RocksDB (and the other C dependencies); if you do not, we simply invoke RocksDB’s native build system, which works out how to build on your platform. Only after all C dependencies are up-to-date does Make invoke go build. The downside is that you can no longer run go get github.com/cockroachdb/cockroach to build Cockroach or vendor it in your project. The upside, of course, is that we now have Windows binaries.[^4] So please, if you use Windows, download the binary, take CockroachDB for spin, and let us know how it goes! [^1]: Versions of Windows before Windows 8 did not provide access to the precise timekeeping that CockroachDB needs. [^2]: MSYS2 is a lighter-weight version of cygwin and the successor to the seemingly-defunct MinGW project . [^3]: The patches were, in no particular order: #1910, #2051, #2097, #2107, #2161, #2315. [^4]: The build system overhaul additionally fixed most, if not all, compilation errors on FreeBSD, OpenBSD and IllumOS. These platforms are not supported by Cockroach Labs, but producing a working CockroachDB binary on any of the three should require only a straightforward translation of the Linux “Build from Source” installation instructions. Illustration by Ayesha Rana", "date": "2017-11-21"},
{"website": "CockroachLabs", "title": "What is RPO & How to Avoid Downtime", "author": ["Diana Hsieh", "Robert Lee"], "link": "https://www.cockroachlabs.com/blog/demand-zero-rpo/", "abstract": "Businesses accept higher customer and economic risk when they live with disaster recovery plans that don’t meet a zero recovery point objective. According to 2020 research from Gartner the average cost of IT downtime is $5,600 per minute. With new advances in technology and products like CockroachDB, businesses no longer need to choose higher risk in order to manage complexity. Why RPO and RTO are so Important to Business Success Businesses care deeply about ensuring the resiliency and uptime of applications. Any downtime could directly impact top line revenues, hurt brand perception, and divert valuable resource hours to failure recovery processes. As a result, CEOs, CIOs, and top level technology executives are focused on meeting application uptime goals and minimizing the cost of infrastructure-level failures. For DBA teams, this means defining and meeting recovery point objectives (RPOs) and recovery time objectives (RTOs) for different tiers of applications. What is RPO? What is RTO? A Recovery Point Objective (RPO) marks how much data can be lost when a failure occurs. A non-zero RPO means that any committed transactions that occurred between the RPO and failure time could be lost. A Recovery Time Objective (RTO) defines how much time it should take to recover from a failure. A non-zero RTO results directly in application downtime. On an ecommerce website, for example, this could mean losing minutes (if not hours) of customer transactions resulting in lost revenue. It’s important to note that RPO and RTO work in tandem. In the case of a fast recovery with non-zero RPO, businesses have to either try to manually reconcile their accounts or live with data loss. On the other hand, a zero RPO solution that recovers slowly would result in significant application downtime. An optimal disaster recovery plan needs to take into account the required RPO and RTO for a given application. For mission critical applications, businesses need to get as close to zero RPO and RTO as possible to minimize the overall risk to both the business and their customers. An application that handles financial transactions with a non-zero RPO could lose deposits or transactions. A reservation system could lose customer reservations. Even worse, losing patient data in real-time healthcare systems could directly impact patient safety. Businesses Settle for Non-Zero RPO and RTO Meeting zero RPO and RTO is incredibly complicated. Several architectural layers contribute to RPO and RTO including database systems, clustering technology, data replication solutions, and storage replication. Each layer is a separate product that must be integrated, configured, and set up by the customer. This means that each layer needs a team of experts to set up, manage, and maintain the system. Often, the combination of products and the way the products are configured become unique to a customer. This entire discussion assumes that existing databases can actually achieve zero RPO and low RTO, but in many cases, this is not true. Active-active setups are supposed to continue serving traffic without data loss in the case of datacenter level failures, but in practice, messages can be lost in transit between datacenters. Further, they rely on a timely detection of the failure to trigger recovery (view figure 1). Standby setups have a similar problem with lost messages during detection of failure and recovery. In contrast to active-active and standby setups, NoSQL solutions can run on more than two servers, providing higher availability and scalability. NoSQL has built-in replication, which means that businesses don’t need a separate solution to support replication or clustering. However, NoSQL comes with its own hidden cost. Although it can survive failures, eventual consistency means that stale data and split brain situations could occur leaving DBAs with inconsistent data that they have to reconcile. Even though downtime is reduced, the data contained in the database is either stale or incorrect. Further, recovery time from disaster scenarios in NoSQL can sometimes take days, since data needs to bootstrapped and repaired to ensure that the data is usable and up to date. CockroachDB Provides Zero RPO CockroachDB makes meeting required SLA targets viable and cost-effective by wrapping the complexity of building a highly resilient infrastructure into a single product. It reduces the component complexity of IT resilience by 75% by eliminating the need for separate replication, clustering, and storage solutions in order to achieve fault-tolerance. Instead, everything comes built into the database system software , reducing the cost and complexity associated with purchasing, deploying, and managing multiple solutions from multiple vendors. Unlike NoSQL, it provides ACID guarantees through consensus-based replication, so that data is always consistent and committed transactions are guaranteed to persist. CockroachDB can also be deployed on commodity hardware, since it has built-in resiliency for storage-level failures at the software layer. More detailed description of each of the layers is available here . Underneath the covers, CockroachDB intelligently replicates data across the cluster, spreading copies out across different availability zones to provide the highest level of fault tolerance based on the available infrastructure. This means that for any hardware failure ranging from disk to datacenter-level disasters, CockroachDB can continue to serve client traffic while recovery takes place. CockroachDB is also architected to support an average of 4.5 seconds RTO. It is important to note that this includes both the time it takes to detect a failure, as well as the time it takes to recover from it. No other database vendor can provide these guarantees along with the ease of use and operational simplicity of CockroachDB. Business Should Stop Accepting Anything Less than Zero RPO IT leaders tasked with the difficult mission of shipping products faster while managing cost and risk have historically had to make trade-offs between protecting their data and the cost of doing so. With new database technologies like CockroachDB, IT leaders are empowered to make zero RPO a baseline requirement for all core business applications given the high cost and risk associated with data loss. Finally, IT leaders can reduce the complexity of their data architectures while reducing risk, freeing them up to build reliable and innovative products quickly. If you want to plan for survival instead of failure the modern, distributed SQL databases are the best option. CockroachDB has a free managed database offering that is excellent for experimenting with the database if distributed SQL is unfamiliar territory.", "date": "2017-11-16"},
{"website": "CockroachLabs", "title": "The Guide to Secure Deployments in CockroachDB", "author": ["Marc Berhault"], "link": "https://www.cockroachlabs.com/blog/secure-deployments/", "abstract": "Production deployments are a world apart from development and testing environments. They come with their own best practices and recommendations, usually customized for each piece of your software stack. In this post, we’ll examine some of the more critical decisions to be made when deploying CockroachDB in production. Security CockroachDB provides two diametrically-opposed security modes governed by the flag --insecure . Let’s see what happens when we use it: $ cockroach start --insecure\n*\n* WARNING: RUNNING IN INSECURE MODE!\n* \n* - Your cluster is open for any client that can access <all your IP addresses>.\n* - Any user, even root, can log in without providing a password.\n* - Any user, connecting as root, can read or write any data in your cluster.\n* - There is no network encryption nor authentication, and thus no confidentiality.\n* \n* Check out how to secure your cluster: https://www.cockroachlabs.com/docs/stable/secure-a-cluster.html\n* This is meant to scare you and I hope it worked as intended. It does highlight the difference between our two modes: Mode Data Transfer Authentication Server Identity Check Insecure plaintext none none Secure TLS 1.2 certificate or password certificate based Encryption Enabling secure mode changes all network communication to be done through TLS. All traffic between CockroachDB nodes as well as client-server communications end up encrypted. You may start off thinking you have no need for in-flight encryption because your datacenter is secure and you have strict controls in place for which clients can even access your CockroachDB nodes. But before long you find yourself needing to add firewall rules for ad-hoc services running outside your carefully-controlled environment, or you decide to expand your CockroachDB cluster to another datacenter, communicating over untrusted network links. Without encryption, you now find yourself sending plaintext data over networks your are not in full control of. In this day and age we probably don’t need to elaborate on the need for encrypted communication. Authentication Authentication is the act of verifying the identity of the other party in a communication. In a secure CockroachDB deployment, we perform authentication in a few places: CockroachDB nodes have the right client certificate All nodes should have a client certificate for the special user node . This is to restrict access to the node-to-node protocol. Node addresses are in the server certificate The address (IP address or DNS name) used to reach a node, either directly or through a load balancer, should be listed in the server certificate presented by the node. This is needed to make sure we are indeed talking to a CockroachDB node, and not a man-in-the-middle. Client is who it says it is We need to verify that a client does have the right to act as the requested SQL user. This is done either through certificates (by checking against the Common Name ), or through password authentication. These checks allow CockroachDB to restrict access to authorized clients and make sure that nodes cannot be impersonated. You may again find yourself in a firewalled and strictly-controlled environment and think you may not need authentication. You trust your users to connect using the correct SQL user, and you are not concerned with node impersonation. However, you will soon find that your (trusted and honest) internal users start connecting as root on an ad-hoc basis. From there, it’s only a matter of time until an unfortunate typo drops one of your more critical tables. Without authentication you will be at the mercy any attacker getting through into your controller environment, and you will be at the mercy of carelessness on the part of your own users. Monitoring & Alerting Another critical aspect of running anything in production (be it your database, monolithic server, or micro service) is monitoring. You should not be waiting for user complaints to know that your system is experiencing problems. CockroachDB comes with a built-in Admin UI showing you high-level metrics about various aspects of the system. While these are helpful in examining the current workload on your cluster, there is one thing it cannot do: alerting. The reason for this is quite simple: the worst of cases is when most or all of the cluster is down. Because the Admin UI is built into CockroachDB, it could not possibly alert you if your cluster were down. To solve this problem, we provide metrics in a format understood by Prometheus . Besides recording all metrics, Prometheus allows you to write rules for alerts to send to the AlertManager. In turn, AlertManager can notify you in any number of ways (email, slack, pagerduty, etc…) and make sure you are promptly notified of cockroach issues. We’ve hopefully convinced you of the need to run CockroachDB in secure mode, as well as setup monitoring. While a lot more goes into a good production deployment (provisioning, tooling, backups, etc.), security and monitoring are two of the early decisions you should not skip. Illustration by Jared Oriel", "date": "2017-11-09"},
{"website": "CockroachLabs", "title": "Using Tunable Controls for Low Latency in CockroachDB", "author": ["Andy Woods", "Alex Robinson"], "link": "https://www.cockroachlabs.com/blog/tunable-controls-for-latency-survivability/", "abstract": "Geographically distributed databases like CockroachDB offer a number of benefits including reliability, cost-effective deployments, and more . Critics often counter that distributed databases increase latency. What if a database could offer all of the benefits of distribution, but also provide low-latency? With this challenge in mind, we set out to minimize latency in CockroachDB, all the while providing exceptional reliability for mission-critical workloads. We built “follow-the-workload” to be a key feature to improve performance and provide additional control to database administrators (DBAs). This feature will make use of geo-partitioning to allow DBAs to specify zone configurations in order to create data access patterns for improved latency. The blog post is the first of a two-part series exploring the use of “follow-the-workload” and replication zone configurations to beat the latency-survivability tradeoff Latency-Survivability Tradeoff Before diving into how “follow the workload” can be used to improve performance, let’s first give a quick overview of how CockroachDB distributes data. As an advanced distributed database, unless instructed otherwise (via Configuring Replication Zones ), CockroachDB will automatically shard data and spread it evenly across all nodes in order to increase data survivability and availability as described here . To read an introduction on how CockroachDB breaks data up into pieces and automatically distributes these pieces please consult Distributed SQL (NewSQL) Made Easy: How CockroachDB Automates Operations . We designed CockroachDB to automatically trade an increase in latency for strong consistency and improved survivability. By breaking data up and spreading it across multiple nodes we increase survivability. However, as a result of this very design, we increase the distance between data nodes, thereby creating additional latency as the copies of data communicate with each other from afar. We recognize that, given this tradeoff, we need to provide the most performant system possible. As such, we set two main goals as we continued to evolve CockroachDB: Increase user control of the latency-survivability tradeoff Optimize performance based on DBA choices in the latency-survivability tradeoff Low Latency with Improved User Controls As discussed above, latency increases in parallel with the distance between data and requests. While CockroachDB can automatically make this tradeoff for customers, many users express a greater desire to influence this decision based on their business needs. CockroachDB provides users the controls to set data location and thereby reduce latency. For example, table/database level replication zone configurations can be used to specify exactly in which Localities the data in a given table or database should or should not be placed. Localities are key-value labels associated with each running CockroachDB process via a command-line flag. Users must turn on Localities through the locality flag to take advantage of replication zones configurations and “follow-the-workload.” Replication zone configuration constraints work by hooking into the rebalancing decision-making process, as explained in Distributed SQL (NewSQL) SQL Made Easy: How CockroachDB Automates Operations . CockroachDB prioritizes these constraints before all other factors when ranking possible nodes for data distribution. Only after resolving these constraints will CockroachDB address other considerations, such as diversity or balance. Replication zone configurations also allow users to constrain a table’s location for legal and compliance reasons. For example, a table that contains European financial transactions, can be restricted to European datacenters. Replication zone configurations only work on entire tables or databases (e.g., non-split tables). We plan to address this limitation through geo-partitioning, an upcoming enterprise feature set for release in 2.0. Future Work Geo-Partitioning for Low Latency Geo-partitioning will allow DBAs to specify replication zone configurations on subsets of a table. This can be used to specify exactly in which localities individual rows of a table should be placed based on the value of some column(s) in the row. We believe that this feature will be useful when table/database level replication zone configurations need more specificity for customer business purposes (e.g., onerous table-splitting). “Follow-the-workload” will automatically take advantage of the propensity of geo-partitioning to create similar data access patterns (e.g., by location) to improve latency. CockroachDB is built to be a high-performance distributed database. We hope that by using features like replication zone configurations we will continue to “make data easy” for our users.", "date": "2017-11-30"},
{"website": "CockroachLabs", "title": "Open Sourcing the Interview Process to Reduce Unconscious Bias", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/open-sourcing-the-interview/", "abstract": "Since the U.S. Equal Employment Opportunity Commission (EEOC) opened their doors for business on July 2, 1965, a year after Title VII’s enactment into law, the mission to eliminate unlawful employment discrimination has made progress in our society. No one can deny that we are in a significantly better place than we were just over 50 years ago. The act has benefited many and has served as a model for future anti-discrimination measures passed by Congress, including the Pregnancy Discrimination Act and the Americans with Disabilities. However, the passing of this legislation has by no means solved the problem of discrimination in the workplace. Discrimination is not as overt, but is rather found in grey areas, lurking beneath the surface and often rooted deep into an individual’s unconscious bias. I know what you are thinking: “I know I have biases, but I don’t discriminate against people who are different from me!” It feels shameful. Your biases are shaped by your experiences, culture, and your lifestyle. They frame your perspective and the way you behave in the world. It’s not shameful. However, we believe that recognizing your bias and how it affects your decision-making processes will allow you to understand if and how those biases are impeding your organization's ability to create a balanced and inclusive workforce. At Cockroach Labs, we have put processes in place for our employees to be aware of the role their unconscious bias plays in hiring decisions they are making. We have removed the dependency on resumes and focused on exercise based interviewing . Our end goal is to create practices that lead to a more inclusive culture. It’s proven that by providing candidates with an understanding of what to expect throughout the process, as well as insight into what to expect, helps them perceive the process as being fair [ Klehe, König, Richter, Kleinmann, Melchers ]. In addition to creating fairness, we want to see if full transparency can also lead to better hiring outcomes. With this in mind, we have made the decision to open source our interview process. Benefits for candidates Before we open sourced the interview process, a candidate for the Senior Technical Writer position showed up to the interview with a book of notes that she had taken before coming onsite. She had read up on database solutions in the marketplace and spent some time on our blog to prepare for the interview. As she got settled for her first interviewer, she asked me if it was okay for her to use her notebook during the interview process. I replied, “Of course!” Leaving the room, I asked myself “Why wouldn’t we want all candidates to come prepared for their interviews?” I recently wrote about how Cockroach Labs relies on exercise-based interviewing for our evaluations of candidates. As discussed in the blog post , exercise-based interviews allow for our interviewers to witness the direct application of a candidate’s abilities to competencies required for the role. While we believe that these types of interviews lead to a stronger signal of a candidate’s success in the position, there are also several advantages for the candidate. Since the interviews are snapshots of the position, candidates are able to show their skills. However, our interviews can still feel taunting and strenuous, as most interviews do. Another benefit that we would like to extend to our candidates is access to the types of exercises they should expect during their interviews. Advantages of open-sourcing the interview process: It puts the candidate at ease. It allows the interviewer to see how the candidate would engage in their daily work. It provides more clarity and removes confusion about the interview process, allowing for the candidate and interviewer to engage as they would if they were working on a problem in their work environment. By leveraging our open-sourced interview repo , candidates have a direct insight into the interview process as well as the types of exercises they should expect during the interview. We encourage candidates to use the repo as a tool to prepare for their interview process. Next Steps for recruiting professionals and other companies We are all responsible for building an industry that is inclusive. By working together, we will only get further. Step one is acknowledging that we all suffer from unconscious bias. We can not challenge that bias unless we are aware of the role it plays in decisions we are making, especially within the hiring process. We ask that recruiting professionals partake in improving the current hiring processes we implement as an industry. As we have done, we encourage you to share strategies that allow your companies to challenge bias and hope you add exercises to our Github repository so we can all learn how to better fairly assess skills required for positions for which we all are hiring. Conclusion Both candidates and those responsible for the hiring process should head over to Open Sourced Interview Process on github.com/cockroachlabs/ . Let’s pledge to do better than we have before by engaging with ideas from one another. We welcome feedback on our process from both candidates and recruiting professionals. Illustration by Quentin Vijoux", "date": "2017-11-28"},
{"website": "CockroachLabs", "title": "How to Improve IoT Application Performance with Multi-Row DML", "author": ["Robert Lee", "Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/multi-row-dml/", "abstract": "Internet of Things (IoT) and microservices-style applications need a database that can handle requirements such as fluctuating number of client connections, unpredictable workloads, and bursty throughputs. Traditional single-node databases handle these requirements by reducing latency to improve throughput. However, for modern distributed databases such as CockroachDB, the optimal approach to handle these requirements is to use multi-row SQL Data Manipulation Language (DML) and parallel processing. Multi-row DMLs provide an order-of-magnitude improvement in throughput performance as compared with equivalent single-row DMLs, which is why databases such as Oracle, MySQL, and Postgres widely support multi-row DMLs. CockroachDB has supported multi-row DMLs since the 1.0. This blog post discusses how to use multi-row DMLs, the performance benefits of multi-row DMLs over single-row DMLs, and the effects of compounding database and application parallelism in single-node vs. distributed databases. When to Use Multi-Row DML Syntax The following table shows examples of single-row and multi-row DMLs for INSERT , UPSERT , and DELETE statements. The number of rows specified in the DML is called the “batch size”. Single-row DML always has batch size of 1. Example: Tabulation Service Using multi-row DMLs improves CockroachDB’s throughput dramatically. To understand why, let’s consider the example of a tabulation service, say for financial transactions, voting mechanisms, or ecommerce apps, and see how the service works with CockroachDB. A tabulation service processes events generated by a fluctuating number of clients and persists the events (that is, saves the events) into a database. Because the rate of event generation is higher than the rate at which the events can be persisted to the database, a mismatch occurs between event generation and persistence. This mismatch can create throughput impedance mismatch (or to put it simply, it can cause throughput bottleneck). In order to compensate for the mismatch, the events are placed in a queue, as shown in Figure 1. Figure 1 The client retrieves the event from the queue, processes it, and persists it to the database using the basic INSERT DML. Insert into x values (1,’A’); The client then connects to CockroachDB through a gateway process and sends the INSERT DML. If you have followed CockroachDB over the years, you may be familiar with our approach of mapping SQL onto a KV keyspace . The gateway sends the KV request to the node that contains the KV range. CockroachDB Single-row DML Now let’s assume a heavy impedance mismatch that caused 9 events to be queued up. The client retrieves and processes one event at a time from the queue. The single-row DML inserts nine separate statements as follows: Insert into x values (1,’A’);\nInsert into x values (2,’B’);\nInsert into x values (3,’C’);\nInsert into x values (4,’D’);\nInsert into x values (5,’E’);\nInsert into x values (6,’F’);\nInsert into x values (7,’G’);\nInsert into x values (8,’H’);\nInsert into x values (9,’I’); Assume there are 3 CockroachDB nodes, where the KV node with ranges 1 to 3 contains keys 1 through 3, the KV node with ranges 4 to 5 contains keys 4 through 5, and the KV node with range 7 to 9 contains keys 7 through 9. CockroachDB receives a series of DMLs and processes them one at a time. The gateway receives the statement, which is then sent to the KV node. The KV node receives and executes the single statement with one row. When a statement touches a single range, CockroachDB performs fast-path commit directly on the KV node. Figure 2 shows the sequence diagram for the operation, which clearly shows that executing each single-row statement causes other events to wait in the queue. Figure 2 CockroachDB Multi-row DML Now let’s consider how the same scenario works with multi-row DML. With multi-row DML, The client retrieves all events from the queue, and inserts the nine values in a single statement as shown below. Insert into x values (1,’A’),(2,’B’),(3,’C’),(4,’D’),(5,’E’),(6,’F’),(7,’G’),(8,’H’),(9,’I’) The sequence diagram in Figure 3 shows that the initial client-to-gateway communication is the same as the single-row DML. The gateway creates three execution paths based on the three KV nodes. In parallel, each of the KV nodes receives and executes the single statement with three rows. Once the KV nodes complete their operation, the gateway performs the CockroachDB equivalent of a two-phase commit by writing COMMIT intent in the transaction record in the first KV range. As evident from Figures 2 and 3, the multi-row DML shows considerable reduction in execution steps over multiple single-row DMLs. Figure 3 Potential Throughput Improvement Let’s compare the number of occurrences for single-row and multi-row execution steps required to insert the 9 events into CockroachDB. As seen from the table, multi-row DML requires fewer network round-trips and statement parses that should result in better throughput with less resource consumption. However, the impact of multi-row DML’s Commit Intent is unknown. Let’s figure out the impact of the Commit Intent on performance. Actual Throughput Improvement To measure the throughput improvement, we used an open-source load generator called Rand to execute INSERT DMLs into TPC-H lineitem table on a 4-node CockroachDB cluster. The following graph plots relative performance improvement by varying the batch size on the x-axis. The baseline measurement uses a batch size of 1 (single row) with concurrency of 2 -- that is two separate threads running independently. As the graph shows, increasing the batch size increased the throughput as expected. A batch size of 10 yielded 2x the throughput, and batch size of 50 yielded close to 7x throughput respectively. However, the graph also shows an inflection, a point of diminishing return, at a batch size of 100. This inflection point depends on the number of KV nodes, network latency, and the workload. Generally, the throughput improvement will be higher with more servers, higher network latency, and bigger batch size. Database Parallelism and Application Parallelism A common practice to improve performance even further is to compound parallelism from application and database levels. But with single-instance databases, the resources are limited, so over-parallelizing can actually decrease performance. A system can context switch among many parallel workloads, which is normal for regular workloads. But when the system resources are fixed (as is the case with single-node databases) and parallelism is increased beyond the limits, the system will spend more time on the context switch rather than running the workload. The excessive context switch leads to decrease in throughput. Therefore, increasing the application concurrency with database parallelism should move the inflection point to the left on the graph. To understand the impact of over-parallelizing in single-node databases, let’s use CockroachDB to artificially create the limitations that single-instance databases face with limited resources. In the previous section, we saw that CockroachDB multi-row automatically parallelized execution to the same degree as the number of KV nodes. Our example test also used an application concurrency of 2. The following graph shows a second test with an application concurrency of 4. Doubling the application concurrency doubles the throughput at batch size of 10. However, as the batch size is increased to 50, we encounter the point of diminishing returns. At batch size of 100, the throughput is actually below that of batch size 50. In this example, we deliberately limited the number of nodes to 4 to artificially create the limitations that single instance databases face. But CockroachDB being a distributed database, we could easily scale the number of nodes, which would help us overcome these limitations. The second test shows that if your database is not scalable, then you need careful upfront planning to avoid over-parallelization. This planning, often called sizing and capacity planning, is complex and time consuming. CockroachDB, on the other hand, is a scalable database that handles the complexity so you don’t have to. It enables your IoT applications and microservices to safely handle unpredictability concurrency. Summary Multi-row DML improves throughput and reduces resource consumption. CockroachDB automatically parallelizes the multi-row DML processing, and the empirical data that the Rand tool test shows that increasing the batch size results in better throughput. We also saw how databases that are not scalable can face excessive context-switching when aggressive application parallelism is used in combination multirow DML, but because CockroachDB is scalable database, the multi-row DMLs can safely be used to improve throughput. Future blog posts will discuss scalability and its benefits in further detail. Illustration by Lea Heinrich", "date": "2017-12-07"},
{"website": "CockroachLabs", "title": "Kicking the tires: Automated CockroachDB Test Cluster Deployment in AWS", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/cloud-formation-test-cluster-deployment/", "abstract": "Today, we’re providing an automated way to setup multi-node CockroachDB clusters so developers can easily try out the latest stable and pre-release functionality. To automate test cluster deployment, we combine AWS CloudFormation (Amazon’s infrastructure automation product) with Kubernetes to let users spin up self-healing, horizontally scaling test clusters with just a couple clicks. Automated stack creation AWS CloudFormation uses templates, which are instructions that their service can follow to reliably provision services or applications they call “stacks.” When you launch our template , you’ll be sent to a form where you can specify settings like CockroachDB version and cluster size. After you click “create”, AWS will slowly (the whole process takes about 15 minutes) but surely create your environment for test cluster deployment, which will include an Amazon Virtual Private Cloud (VPC), Auto Scaling group, and your Kubernetes-orchestrated CockroachDB cluster. Here’s a rough outline of the architecture: When your stack is complete, you will have: access to the connection string for your database, an SSH command to access the K8s master node a link to the web UI. That’s all you need to get started! Why Kubernetes? The CockroachDB architecture has no single points of failure, as each node communicates via a peer-to-peer network to intelligently rebalance, repair, and replicate while maintaining data integrity at all times. However, for this stack, we introduce an additional master node to host Kubernetes. This way, you can take advantage of automatic process management, load balancing, and other orchestration goodness that Kubernetes provides . How to use this template We specifically designed this template for testing rather than production use. The stack is deployed in a single Availability Zone so this configuration would not survive a zone outage (though this stack will easily survive node failures if you deploy at least 3 nodes). Furthermore, because traffic is not encrypted , other processes living within the VPC can see plaintext data flowing between nodes. Some things you can use these clusters for: Determining what would be involved in migrating an existing application to CockroachDB Creating a sandbox to evaluate pre-release functionality from our roadmap Getting a sense for the general operational aspects of distributed clusters If you need data to play with, we have some load generators to get you started! Need more help? Check out these detailed instructions in docs . Please note that AWS charges you for all the resources created by CloudFormation templates, so you’ll want to delete stacks when you are finished. -- Our CloudFormation template and scripts are available on GitHub and were initially forked from Heptio’s AWS Quick Start for Kubernetes . This template was created as part of our Free Fridays initiative. Illustration by Lisk Feng", "date": "2018-01-11"},
{"website": "CockroachLabs", "title": "Thank You for a Fantastic 2017! | Cockroach Labs", "author": ["Swati Kumar"], "link": "https://www.cockroachlabs.com/blog/end-of-year/", "abstract": "``` As we wrap up 2017, we would like to thank everyone who has contributed to and used CockroachDB and who has supported and encouraged all of us here at Cockroach Labs. This has been a great year for the product's - and company’s - evolution as we launched 1.0 then our 1.1, and are now well on our way to CockroachDB 2.0! ``` Along with our 1.0 release back in May, we announced our $27M Series B raise, led by Satish Dharmaraj of Redpoint Ventures and joined by Benchmark, Firstmark Capital, GV, Index Ventures, and Work-Bench. Around the same time, at the OpenStack Interop conference in Boston, CockroachDB was deployed live across 15 private clouds, demonstrating significant portability and flexibility. Fast forward to October during our 1.1 launch , we excitedly announced our first public customers: Baidu , Kindred Futures , and Heroic Labs . The two releases saw an evolution in CockroachDB with: Significantly improved stability in the core database Faster performance through optimizations ranging from leaseholder rebalancing to transaction queues to faster JOINs Easier to use with workflows that allowing users to run distributed backup / restore quickly and safely, regardless of node failures. Making such advances would never be possible without a fantastic team, of course. We grew 50%, from 30 to 45 employees this year, along with five co-ops. Both our Design team and Product teams grew from one-person shows to teams of three. We established a brand new Sales team, growing from zero to three within the year. Our newly minted Head of Product (Nate Stewart), Head of Sales (Armen Kopoyan), and Head of Sales Engineering (Robert Lee) dove in head-first to bring about the advancements we can now show off proudly. If that wasn't enough, we even opened our first satellite office in Boston! Moving into 2018, we expect CockroachDB 2.0 to exceed expectations and make its own mark. No team can ever get this far without the support of you, our users and supporters. Thank you for a great year and Happy Holidays!", "date": "2017-12-21"},
{"website": "CockroachLabs", "title": "Using “Follow-the-Workload” to Beat the Latency-Survivability Tradeoff in CockroachDB", "author": ["Andy Woods", "Alex Robinson"], "link": "https://www.cockroachlabs.com/blog/follow-the-workload/", "abstract": "Geographically distributed databases like CockroachDB offer a number of benefits including reliability, security, cost-effective deployments, and more . Critics often counter that distributed databases increase latency. What if a database could offer all of the benefits of distribution, but also provide low-latency? With this challenge in mind, we set out to minimize latency in CockroachDB, all the while providing exceptional reliability for mission-critical workloads. We built “follow-the-workload” to be a key feature to improve performance and provide additional control to database administrators (DBAs). This blog post is the second of a two-part series. The first part defined the latency-survivability tradeoff and explained the use of zone configurations, while this post moves forward to expand on that and discuss “follow-the workload” as a feature. Optimizing Performance with “Follow-the-Workload” “Follow-the-workload” is a new feature released in 1.0 designed to improve performance. “Follow-the-workload” allows CockroachDB to automatically change the location of the leaseholder (you can think of a “leaseholder” as a read-holder) for a given piece of data (provided one enables localities ) to reduce latency. For each range of data in the cluster, CockroachDB maintains a stable leaseholder replica that serves all requests for the data contained within it. In Consensus, Made Thrive , we explained the reasons for electing a leaseholder for each range that serves all reads and writes on that range. “Follow-the-workload” optimizes the placement of these leaseholders to be closer to the applications that use them. Every query, whether a read or a write, requires interaction with the leaseholder for the appropriate range. As the distance between the leaseholder and the request increases, so does latency. For example, if a leaseholder located in the U.S. receives a request from a node located in Australia, the request has to travel halfway around the world (and back) before processing, adding 150-200ms of network latency. On the other hand, if the leaseholder was located in Australia, processing this same request would result in single digit milliseconds, if not microseconds, of latency. Without a feature like “follow-the-workload,” it is more likely that users would encounter service latency. As we’ve illustrated, CockroachDB can reduce latency and drastically improve user experience simply by transferring the lease to a range closer to the origin of the majority of its requests without moving data. We designed leaseholders to track the number of requests from each locality and automatically move the lease if a large number of requests originate from distant locations on the network (as measured in round-trip service time). The more imbalanced the request load and the further away the source of the requests, the faster CockroachDB will move the lease. If all the load on the system results from one locality, then all the ranges in the system will have their lease placed in that locality. This results in faster reads and writes than if the leases were distributed automatically around the world. This optimization has come to be known as “follow-the-workload” leaseholder rebalancing after the original “follow-the-sun” customer use case that inspired it. Our customer, a globally operating business, deployed CockroachDB to handle worldwide transactions. After deployment, we noticed that, at any given time, the majority of requests originated from daytime locations. As the day proceeded, the workload would move around the world synchronously with daylight hours, essentially “following the sun”. We recognized the value in re-configuring our leaseholder program to mimic “follow-the-sun”-style workloads. Further, we understood that “follow-the-sun” represents only one special type of workload that would require a leaseholder re-balancing via “follow-the-workload”. “Follow-the-Workload” Use Cases We expect that “follow-the-workload” will be heavily used by customers who need widely distributed data (e.g. for fault tolerance), and also by those who experience traffic patterns that can be segmented by location. With “follow-the-workload”, segmentation can be accomplished by both time, as described in “follow-the-sun”, and by primary key. Primary keys can be established based upon a user's location (e.g. state or country) and further used to pre-split tables. If DBAs decide to do this, CockroachDB will employ “follow-the-workload” to ensure that the lease for a given range will be located nearest the state that its users originate requests from. “Follow-the-workload” will rarely take effect for workloads that do not vary based on user location. We took “follow-the-workload” one step further as we designed it to run automatically. In fact, it may end up finding and optimizing patterns in workloads that DBAs would never have noticed! CockroachDB smartly executes these automatic optimizations for DBAs thereby helping to fulfill our mission to “make data easy.” Future Work & Final Thoughts Replica “Follow-the-Workload” We currently only employ “follow-the-workload” for lease behavior. A logical next step would be to expand “follow-the-workload” to data replicas. Moving data replicas costs more than moving leases but may offer additional future latency improvements. For example, if a range currently in localities A, B, and C receives a majority of its requests from locality D (geographically distant from A, B, and C), CockroachDB could create a replica for that range in D (provided approval from relevant replication zone configurations). We’d love to learn more about your interest in this feature as we continue to evolve our offerings. CockroachDB is built to be a high-performance distributed database. We hope that by using features like “follow-the-workload” we will continue to “make data easy” for our users. To learn more and try it out yourself, click here .", "date": "2017-12-05"},
{"website": "CockroachLabs", "title": "Multi-cloud Deployment with CockroachDB", "author": ["Nikhil Benesch"], "link": "https://www.cockroachlabs.com/blog/multi-cloud-deployment/", "abstract": "Why is a multi-cloud database important? Suppose you’re operating an application that receives the bulk of its traffic from the east coast of the United States. You want to minimize latency by locating your database on the eastern seaboard, but it’s important that availability doesn’t suffer as a result. Even with a distributed database like CockroachDB, if you locate your database nodes in just one or two data centers, you’re vulnerable to an outage. You’ll need at least a majority of replicas (i.e., two replicas, under the default three-way replication) online to serve traffic, so you’ll need nodes in at least three data centers to survive a one data center failure. How often do data centers fail? Exact statistics are hard to come by, but examples of major outages abound: last year, many services in an entire Amazon Web Services region were unavailable for several hours , and an entire Google Compute Engine region was offline for an hour . Unfortunately, most cloud providers offer only one data center on the east coast; those that offer two data centers in the “eastern United States” tend to either colocate them in the same state (see Azure), putting you at risk in the case of a major power outage or natural disaster, or put the second data center in the midwest (see Amazon Web Services), meaning your latency would suffer. But with CockroachDB, you aren’t bound by the data centers of just one cloud provider! Creating a multi-cloud deployment is a breeze. Here’s how. For the sake of this example, we’ll use the following data centers[^1]: Digital Ocean, region nyc1 Amazon Web Services, region us-east-1 Google Compute Engine, zone us-east1 That gives us one data center in New York, one data center in Virginia, and one data center in South Carolina, respectively. Not bad for creating a cluster out of stock parts. Note: This is not an endorsement of these particular cloud platforms. Choosing a cloud provider requires evaluating hardware performance, price, and geographic location for your particular workload. Get started by launching one VM in each of the three data centers listed above. Then launch an additional VM in one of the data centers, for a total of four VMs. Why the extra VM? Running a CockroachDB cluster with less than three nodes will trigger warnings about “underreplication,” and later we’ll be taking one of the four VMs offline to simulate a data center failure. You might find the following guides helpful: How to Create Your First DigitalOcean Droplet Launching an Amazon EC2 Instance Creating and Starting a [Google Compute Engine] VM Instance CockroachDB works with any recent x64 Linux distribution. If you don’t have a preference, we recommend using the latest long-term support (LTS) release of Ubuntu. Be sure to configure networking to allow inbound and output TCP connections on both port 26257, for inter-node communication and SQL access, and port 8080, for the web admin UI. We have more detailed walkthroughs in our deployment guides: Deploy CockroachDB on Digital Ocean Deploy CockroachDB on AWS EC2 Deploy CockroachDB on Google Cloud Platform GCE Once you have VMs on each cloud provider up and running, it’s time to install and boot Cockroach. You can follow our comprehensive installation instructions , or, as a quickstart, you can run the following commands on each VM, replacing PUBLIC-IP and DATA-CENTER appropriately: $ wget -qO- https://binaries.cockroachdb.com/cockroach-v1.1.3.linux-amd64.tgz | sudo tar -xvz -C /usr/local/bin --strip=1\n\n$ cockroach start --insecure --background \\\n    --advertise-host PUBLIC-IP-SELF \\\n    --join PUBLIC-IP-1,PUBLIC-IP-2,PUBLIC-IP-3,PUBLIC-IP-4 \\\n    --locality data-center=DATA-CENTER Remember: Real production deployments must never use --insecure! Attackers can freely read and write data on insecure cross-cloud deployments. You can choose any value for DATA-CENTER, provided that nodes in the same data center use the same value. For example, you might launch two Digital Ocean nodes, each with --locality data-center=digital-ocean . CockroachDB uses this locality information to increase “replica diversity,” that is, preferring to store copies of data on machines in different localities rather than machines in the same locality. From your local machine, initialize the cluster. You can use any of the nodes’ public IPs in this command. $ cockroach init --host=PUBLIC-IP --insecure That’s it! You’ve created a cross-cloud CockroachDB cluster. If you take a peek at the admin UI, served at port 8080 on any of the nodes, you should see that four nodes are connected: Now, let’s insert some data into the cluster. CockroachDB ships with some example data: $ cockroach gen example-data | cockroach sql --host=PUBLIC-IP --insecure Here’s a quick tour of this data: $ cockroach sql --host=PUBLIC-IP --insecure\nroot@52.91.188.221:26257/> SHOW DATABASES;\n+--------------------+\n|      Database      |\n+--------------------+\n| crdb_internal      |\n| information_schema |\n| pg_catalog         |\n| startrek           |\n| system             |\n+--------------------+\n\n> SHOW TABLES FROM startrek;\n+----------+\n|  Table   |\n+----------+\n| episodes |\n| quotes   |\n+----------+\n\n> SELECT * FROM startrek.episodes ORDER BY random() LIMIT 5;\n+----+--------+-----+----------------------+----------+\n| id | season | num |        title         | stardate |\n+----+--------+-----+----------------------+----------+\n| 33 |      2 |   4 | Mirror, Mirror       | NULL     |\n| 56 |      3 |   1 | Spock's Brain        |   5431.4 |\n| 35 |      2 |   6 | The Doomsday Machine |   4202.9 |\n| 72 |      3 |  17 | That Which Survives  | NULL     |\n| 55 |      2 |  26 | Assignment: Earth    | NULL     |\n+----+--------+-----+----------------------+----------+ Looks like our CockroachDB cluster is a Star Trek whiz. We can verify that our episode table is safely stored on at least three nodes: > SHOW TESTING_RANGES FROM TABLE startrek.episodes;\n+-----------+---------+----------+--------------+\n| Start Key | End Key | Replicas | Lease Holder |\n+-----------+---------+----------+--------------+\n| NULL      | NULL    | {1,2,3}  |            1 |\n+-----------+---------+----------+--------------+\n(1 row) CockroachDB automatically splits data within a table into “ranges,” which are then copied to three “replicas” for redundancy in case of node failure. In this case, the entire table fits into one range, as indicated by the Start Key and End Key columns. The Replicas column indicates this range is replicated onto node 1, node 2, and node 3[^2]. Note: If your Replicas column only lists one node, your nodes likely can’t communicate over the network. Remember, port 26257 needs to allow both inbound and outbound connections. Consult our Cluster Setup Troubleshooting guide , or ask for help in our Gitter channel or on our forum . In the cluster shown here, nodes 1 and 4 are in the same data center. Since the table is stored on nodes 1, 2, and 3, that means we have one copy of the table in each of the data centers, just as we’d hoped! Fair warning: node numbers are not assigned deterministically and will likely differ in your cluster. Now, let’s simulate a full data center failure. Power off the VM in one of the data centers with only one VM. In this example, we’ll power off node 3. Make sure your SQL session is connected to a node besides the node you’re taking offline. (In a real deployment, to avoid this point of failure, you’d use a load balancer to automatically reroute traffic to a live node.) First, notice that even in the moment after node 3 goes offline, querying the episodes table still succeeds! > SELECT * FROM startrek.episodes ORDER BY random() LIMIT 5;\n+----+--------+-----+-----------------------------+----------+\n| id | season | num |            title            | stardate |\n+----+--------+-----+-----------------------------+----------+\n| 27 |      1 |  27 | The Alternative Factor      |   3087.6 |\n| 59 |      3 |   4 | And the Children Shall Lead |   5029.5 |\n| 18 |      1 |  18 | Arena                       |   3045.6 |\n| 13 |      1 |  13 | The Conscience of the King  |   2817.6 |\n| 21 |      1 |  21 | The Return of the Archons   |   3156.2 |\n+----+--------+-----+-----------------------------+----------+ When a node dies, CockroachDB immediately and automatically reroutes traffic to one of the remaining replicas. After five minutes, by default, the cluster will mark a down node as permanently “dead” and move all of its replicas to other nodes. In this example, node 3 is no longer in the replica set: > SHOW TESTING_RANGES FROM TABLE startrek.episodes;\n+-----------+---------+----------+--------------+\n| Start Key | End Key | Replicas | Lease Holder |\n+-----------+---------+----------+--------------+\n| NULL      | NULL    | {1,2,4}  |            2 |\n+-----------+---------+----------+--------------+ Note that the replica set now includes two nodes in the same data center, node 1 and node 4. With only two data centers, CockroachDB will reluctantly put two copies of the data in the same data center, as a total of three copies is better than only two copies. If the third data center does come back online, or if another data center is connected to the cluster, CockroachDB will again balance replicas across data centers. That’s all there is to it! With very little configuration, we have a multi-cloud deployment of CockroachDB that checks all the boxes: nodes in three data centers along the east coast to minimize latency without compromising availability, even in the face of total data center failure. If you’re not yet ready to commit to a permanent multi-cloud deployment, CockroachDB’s multi-cloud support can still be instrumental in providing operational flexibility. Check out how CockroachDB unlocks zero-downtime migrations from one cloud to another . Illustration by Zoë van Dijk [^1]: Most cloud providers charge an egress bandwidth fee for network traffic that leaves the data center. Be sure to factor this additional cost into account before launching a multi-cloud cluster. [^2]: Technically, the Replicas column lists store IDs, not node IDs. In the default configuration we’ve described in this tutorial, however, every node has exactly one store, so store IDs correspond one-to-one to node IDs.", "date": "2018-01-18"},
{"website": "CockroachLabs", "title": "Database Scaling Strategies: A Practical Approach", "author": ["Sean Loiselle", "Alex Robinson"], "link": "https://www.cockroachlabs.com/blog/scaling-distributed-database/", "abstract": "In tech, we hear the importance of “scale” all the time. People plan for it, try to work around not having it, and build companies to help others achieve it. But when it comes time to scale something yourself or integrate a scalable solution with your app, it’s difficult to find practical guides to help you understand what it takes. Why’s that? Well, it’s kind of hard. Actually scaling a database beyond a single availability zone takes considerable planning and engineering investment––but that being said, it’s an incredibly powerful tool to delight your users with low latencies and high availability. To help you understand some high-level considerations, this post will cover: Understanding your database in the context of scale Determining both performance & geographic strategies If this is interesting to you, we also have a guide that covers more detail of the points above, as well as: Deployment strategies for multi-region database deployments Networking to handle complicated use cases Interested? Download our guide, Scaling Databases with Multi-Region Deployments . Using Your Distributed Database of Choice Understanding Your Database in a Multi-Region Context The best place to begin is in understanding how the database you want to use actually works once you’ve scaled to multi-region context––including any workarounds you might need to develop along the way. Active-Active/Multi-Master In an ideal world, your database would be able to handle both reads and writes everywhere in the world. For many databases, though, “active active” deployment poses a lot of difficulties, largely due to conflicts that inevitably occur when multiple nodes accept writes (for a great primer, checkout pg. 168 of Martin Kleppmann’s book Designing Data Intensive Applications ). CockroachDB is a notable exception to this paradigm. Because of its consensus replication, CockroachDB lets you read and write from all nodes in the cluster without generating conflicts (which we call “ multi-active availability ”). This represents the easiest way to have your database span multiple regions. Replication Impact When your database is deployed to multiple regions, it has to replicate data between nodes in your deployment. One of the first things to understand is how scaled out––especially multi-region––deployments impact your particular database. For example, if you use a NoSQL database with eventual consistency, how “eventual” does the consistency become when replicas span continents? It’s important to substantiate this not to dissuade a team from scaling out to multiple regions, but to ensure your SLAs and engineering efforts account for it. You'll also need to account for how your application deals with this replication delay, and optionally whether it could lead to conflicting writes overwriting each other. Another consideration is how replication impacts your ability to comply with regulations like General Data Protection Regulation (or GDPR ). If you’re using Amazon RDS cross-region read replicas , for instance, you have to ensure that the data is not being replicated to locations outside of the EU (for customers who disallow you to move their data outside of the EU). Understand Data Domiciling It’s crucial when you interact with user data to err on the side of caution. Failure to comply with GDPR––by processing user data outside of the EU without consent––results in crippling fines. So, even though many EU residents will allow for their data to be stored and processed outside the EU, you will do better to account for those that insist on strict data domiciling. In short, this means that your database must have the ability to partition data based on some row-level key, or your application must contain routing/gateway logic to ensure that writes reach the correct table in your database. CockroachDB offers replication zone configuration to provide table-level control for distributing data, and our Enterprise version will offer row-level partitioning in 2.0, which can radically simplify GDPR compliance. Geographic Strategy The most important part of a multi-region deployment is the regions themselves. This is ultimately the factor that’s going to provide the bang for your buck. By placing data close to users, they'll get a better, faster experience because their requests are traveling shorter distances. Know Your Audience First things first: you need to know where your user base is. This can be as simple as polling your user database and finding the most granular piece of geographic data you can. If we were to imagine this as a SQL query, it would look like: SELECT region, COUNT(*) FROM users GROUP BY region; From there, you have an idea of where you need to serve data from (and which regions to invest most in). Understand Regulations +When analyzing your customer base, you might identify users in the EU or China whose experience you’re concerned with. Before committing to deploy your app to these regions, it’s crucial to have a clear understanding of the implications of what deployments in these places entail (lest you fail to comply with the regulation and incur a crippling penalty). While we’re not lawyers, the general guidance here is that you must domicile user’s data from these regions in these regions. For example, Chinese users’ sensitive data must be kept in China . When dealing with the EU, the regulations can become even more stringent because you also cannot necessarily process user data outside of the EU without a user’s explicit consent. If you did your due diligence in assessing how your database works when it’s scaled across regions, you should have a clear sense of your technological capabilities to work within a region. Performance Strategy Every undertaking benefits from metrics to understand its success, and setting those out beforehand will help you and your team substantiate the impact of multi-region deployments on your application. Multi-region deployment will provide you two major upsides: speed and availability. To make sure you can substantiate your work’s impact, it’s important to develop clear strategies and benchmarks for both. Latency Goals & Strategy By moving data close to users, you’re removing network latency between them and your application. You can do some very rough back-of-the-envelope calculations by standing up a VM in a region you’re considering a deployment to and pinging it. You will be able to shave approximately that much time off of the requests of users that are near the zone in the region where you deploy. This is a very rough approximation, though, and is useful only in establishing a notion of what's possible. Of course, your actual requests will still take a few milliseconds (2-10ms assuming high-speed internet is available) within the region. With an understanding of the performance gains you can make, you should examine the services you’re connecting and determine the largest area of impact you can make by deploying those across regions. It’s also important to note that services that you don’t deploy to a region will begin incurring latency equal to the gains you’re making elsewhere; that doesn’t mean you need to replicate every service in every region, but is an important factor in determining your SLAs. Another factor to consider is the trade-off between consistency and latency. If consistency doesn’t matter and your application can tolerate potentially losing data, you can often speed up the request by making it asynchronous. If that’s not the case, synchronous requests can be somewhat slower, but can provide ACID (or ACID-like) guarantees. Availability Goals & Strategy By distributing a service among two machines with 90% availability (which is pretty lousy, equaling 3 days of downtime per month ), you can achieve 99% availability, also known as “two nines.” If you’re dealing with more robust services, it’s easy to start achieving three or four nines. These calculations, though, assume that the services’ availability is independent of one another, though––but if they’re in the same datacenter and the entire datacenter goes down, so does your application. However, to ensure this, you also have to account for things well beyond your control––which might very well be why you’re considering a multi-region deployment in the first place. So it’s important to have a contingency plan in place assuming the entire region will go offline. How to accomplish this and what it means largely depends on the database you’re using and the level of consistency your application requires. Because CockroachDB automatically repairs and rebalances itself, you don’t need special strategies in place to handle failovers. By simply balancing load to other nodes, your application will continue serving requests. Low Hanging Fruit: Keep Your Deployment Up To ensure your services remain available, you also need to account for trivial and common problems: tedious things like VMs going down. Fortunately, these are relatively simple problems to solve by leveraging configuration management tools like Chef , Puppet , or Ansible . Using these (coupled with monitoring), you can automatically spin up new replicas. With a distributed database, this can dramatically improve a service's uptime (assuming the provisioning process is tuned well). ...and what else? Of course, there are many more considerations when you begin the work of scaling your database. To get more detail––including potential architectures for most common distributed databases––download our guide Scale Databases with Multi-Region Deployments . Illustration by Rebekka Dunlap", "date": "2018-02-08"},
{"website": "CockroachLabs", "title": "Geo-Partitioning: What Global Data Actually Looks Like", "author": ["Andy Woods", "Daniel Harrison"], "link": "https://www.cockroachlabs.com/blog/geo-partitioning-one/", "abstract": "As we’ve written about previously , geographically distributed databases like CockroachDB offer a number of benefits including reliability, security, and cost-effective deployments. You shouldn’t have to sacrifice these upsides to realize impressive throughput and low latencies. Distribution, by definition, creates some latency as data must travel from one node to another. We built CockroachDB to make it easy to minimize the latency created in geo-distributed clusters. We can minimize latency by minimizing the distance between where SQL queries are issued and where the data to satisfy those queries resides. While this makes intuitive sense, the simplicity of this basic concept can mask the complexities involved in setting up a global deployment. Luckily, CockroachDB makes this easy with geo-partitioning. This blog post will present a detailed walkthrough of a new enterprise feature named geo-partitioning aimed at improving performance by reducing latency. You can try geo-partitioning before it’s available in our 2.0 release (coming in April) by downloading the latest 2.0 beta and signing up for a free 30-day enterprise trial . Geo-partitioning defined Geo-partitioning grants developers row-level replication control. By default CockroachDB lets you control which tables are replicated to which nodes. But with geo-partitioning, you can control which nodes house data with row-level granularity. This allows you to keep customer data close to the user, which reduces the distance it needs to travel, thereby reducing latency and improving user experience. To geo-partition a table: Define location-based partitions while creating a table. Create location-specific zone configurations. Apply the zone configurations to the corresponding partitions. US Asia-Pacific role-play Imagine yourself as the proud developer of the multinational (and fictional) Roachmart online storefront with many users in both the United States and Asia-Pacific. As a customer-oriented developer, you want to provide the best experience for your users by minimizing latencies. But, you don’t want this low-latency to come at the cost of increased organizational complexity or to require you to run multiple databases. With this goal in mind, you decide to deploy a single, global CockroachDB cluster that uses geo-partitioning to keep the data for your United States users in the United States and the data for your Asia-Pacific users in Asia. First, create a users table that is partitioned by zones. CREATE TABLE users (zone STRING, id SERIAL, name STRING, …)\nPARTITION BY LIST (zone) (\n    PARTITION asia VALUES IN ('JP', 'TW', …),\n    PARTITION united_states VALUES IN ('US')\n) Tables with data about a user, like orders or posts, that should be subject to the same partitioning rules can be easily “ interleaved ” into the users table. CREATE TABLE orders (…) INTERLEAVE IN PARENT users (usr_zone, usr_id) Now you might wonder, what exactly are zones? Regions and Availability Zones Amazon EC2 and Google Compute Engine have parallel concepts called regions and zones. Each region can be imagined as a completely independent and separate data center site. Each region in turn includes multiple zones (called 'availability zones' by Amazon), which connect to each other with high-speed networking, but are otherwise kept as isolated as possible. While it is unlikely for a single event to cause unavailability in two different zones, it does happen . Note, it is even less likely that a single event causes unavailability in two different regions. Replicas and Leaseholders For fault tolerance, CockroachDB keeps multiple copies, called replicas , of each range of data distributed on different nodes. In this post, we assume Roachmart uses CockroachDB to keep 3 replicas, but you can configure this as needed. CockroachDB balances replicas across nodes automatically to adapt to traffic patterns and machine failures. Writes require contacting a majority of replicas, but with the \" leaseholder \" optimization, reads need to consult only one replica which holds the lease. We cover this in more detail in previous posts such as Consensus, Made Thrive . Local reads and writes Back to your uber successful international Roachmart application. A Korean user refreshes a page in the application, making an API call which Amazon or Google's load balancers route to the nearest API server, sitting in Taiwan. This API server then makes its SQL requests to a CockroachDB node in the same availability zone (asia-east1-b). If this SQL query is a read, then CockroachDB can respond using the nearby replica leaseholder in asia-northeast1-a. Image 1: Local Reads: only one local replica needed If the query is a write, then CockroachDB can respond without any cross-pacific network hops when a majority (in this case 2/3) of the replicas (e.g., asia-northeast1-a, asia-east1-a) are in Asia-Pacific. The leaseholder (asia-northeast1-a) is one of these two. The remaining minority of replicas (in this case, just the one replica in us-west1-a) will update asynchronously without any compromise to consistency. Image 2: Local Writes: two local replicas needed Resilience to region unavailability As discussed above, we must ensure that two replicas for each of the ranges in the Asia-Pacific partition live in an Asia-Pacific datacenter to complete local reads and writes. You can configure replicas using replication zones as outlined below: $ ./cockroach start --locality \\\n    continent=asia,region=asia-east-1,zone=asia-east1-a\n\n$ echo \"constraints: {'+continent=asia': 2}\" |\n    ./cockroach zone set --insecure roachmart.users.asia -f - CockroachDB configures each node at startup with hierarchical information about locality. For Roachmart, the gateway node’s continent is Asia, the region is asia-east-1 and the zone is asia-east1-a (public clouds don't expose rack information, but a private datacenter would likely include this in the locality flag) while the leaseholder replica is located in the region asia-northeast1 and zone asia-northeast1-a . These localities are used as targets of replication zone configurations, but they're also used by the system to keep diversity as high as possible. Roachmart requires that two replicas remain in Asia to ensure low latency reads and writes. CockroachDB will automatically attempt to keep the third (and final) replica in a different region (in this case us-west) to ensure maximum diversity. This replica (located in us-west1-a) will only return to Asia in the event of a region specific failure such as unavailable or overloaded nodes. CockroachDB will keep the two replicas within Asia in as diverse geographic regions as possible to increase survivability. For example, if there are nodes in two regions, CockroachDB will keep one of these two replicas in each. If there is only one region (either by design or due to temporary region unavailability), CockroachDB will automatically spread replicas across the zones in it to maximize survivability. Consider the unlikely event of unavailability of the entire asia-northeast1 region, which means we've lost one replica of the range and are left with two. The replica in asia-east1-a becomes the leaseholder. Reads are still served without crossing the Pacific, but writes now temporarily require the second remaining replica (us-west1-a) to reach consensus. Image 3: Data center failure: temporarily under replicated Eventually, CockroachDB will replace the missing replica. This uses network bandwidth and other resources, so it waits to do this for five minutes (by default). If the outage ends before the five minutes elapses, the original replica is reused. Otherwise, a replacement replica must be created and as always, it is placed to ensure maximum diversity. In this case, a different availability zone is the best we can do (asia-east1-a instead of the already existing gateway node in asia-east1-b). Image 4: Data center failure: fully replicated in extended region outage One region and multiple cloud provider deployments You may wonder, what happens if my geographic area has only one region? At the time of writing, both Amazon and Google each offer a single Australian region (though as we’ve seen, many other continents have more than one). Australian users can run CockroachDB across different cloud providers because it is not only cloud-agnostic but also supports multi-cloud deployments. You can assign some nodes to the Amazon Australia region and some to the Google Australia region. You could also stay within one cloud provider but expand to a nearby region that's not in Australia, for example, the previously mentioned Asia-Pacific region. Final Thoughts Ultimately, the distance inherent in global deployments means developers must always make a tradeoff between availability and latency. In typical applications, read queries are far more common than writes. This may allow for some applications to configure reads as Asia-Pacific local (only one replica located in Asia-Pacific) but writes with one cross-Pacific hop (as the majority of replicas are not in Asia-Pacific). No matter the unique details of your application, CockroachDB offers you the tools to make the best tradeoff for your application. You can try geo-partitioning before it’s available in GA by downloading the latest 2.0 beta and signing up for a free 30-day enterprise trial . Illustration by Lea Heinrich", "date": "2018-03-15"},
{"website": "CockroachLabs", "title": "Be Flexible & Consistent: JSON Comes to CockroachDB", "author": ["Justin Jaffray", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/json-coming-to-cockroach/", "abstract": "We are excited to announce support for JSON in our 2.0 release (coming in April) and available now via our most recent 2.0 Beta release . Now you can use both structured and semi-structured data within the same database. No longer will you need to sacrifice ACID guarantees, accuracy, or the ability to scale in order to use multiple data models within the same database. This post will explain how we implemented JSON and give you a few examples of how JSON can be used to model your data. How can I use JSON in CockroachDB? Imagine that you run a service that allows users to keep track of their insect collection. Some pieces of structured data might be obvious to you as a developer, but many may not. Using a JSONB column gives you the flexibility to store whatever data you want in a way that's still queryable by the database. Consider the case of modeling insect collections. First, you might create a table that allows for storing JSON data. CREATE TABLE insects (\n  id UUID PRIMARY KEY,\n  user_id UUID REFERENCES users,\n  species STRING REFERENCES insect_species,\n  acquisition_date TIMESTAMP,\n  name STRING,\n  metadata JSONB,\n  INDEX (species)\n); Now, a simple insert allows you to populate the insects table: INSERT INTO insects VALUES\n  (..., ..., ‘cockroach’, now(), 'Craig', '{\"favorite_food\": \"crumbs\", \"nickname\": \"Craiggy\", \"life_history\": [...], \"best_friend\": \"Cynthia\"}'),\n  (..., ..., ‘cockroach’, now(), 'Cynthia', '{\"favorite_food\": \"hamburger\", \"life_history\": [...], \"best_friend\": \"Craig\"}'); As you can see from this data, only Craig has a nickname (e.g., “Craiggy”). As a developer (and not your application's end user), you might not have considered the need for nicknames as not every customer gives a nickname to their insects. For a full demo of JSONB, please consult our documentation . Now that we’ve covered one example in which you might want to use JSONB within a relational database, let’s dive deeper into the implementation details. Postgres design After reviewing the above example, you may notice that we implemented JSONB within CockroachDB based on the Postgres design. We chose to do this so that our users are not forced to learn another new syntax to support JSON. This means that you can use JSON in CockroachDB via the vibrant third-party Postgres ecosystem. Further, applications developed initially for Postgres will largely work out-of-the-box with JSONB in CockroachDB. What is JSONB? CockroachDB implemented JSONB rather than JSON. Postgres introduced JSONB as an optimized version of JSON strings. This means that it is in a binary format that doesn’t require parsing the entire document. We chose to support JSONB over JSON because our research showed that JSONB was far more popular. Further, storing the data in an optimized binary format (JSONB) rather than text (JSON) allows for quicker operations. To illustrate the difference, suppose someone hands you the JSON metadata describing a insect: {“favorite_food”: “crumbs”, “life_history”: [...], “best_friend”: “Cynthia”} A common query run on this could be: who is this cockroach's best friend? If the document was stored as plain JSON, you’d have to read through it field by field until you get to the “best friend” field—a slow endeavor when the field you’re interested in is stored behind pages of life history. If the metadata is instead stored as JSONB, a header at the beginning of the document tells you how many bytes to skip so that you can jump directly to the best friend field. The difference is negligible for one document, but substantial when scanning through potentially millions of documents. On-disk encoding We evaluated encoding based on the speed at which we could perform operations on the data and the number of bytes a given datum took to store. As opposed to storing semi-structured data as an opaque sequence of bytes, JSONB can be manipulated from within the database. Because we expect our users to perform queries upon JSONB, we designed commonly used operations to avoid costly speed delays (e.g., decoding the entire document). For example, JSONB allows this query to efficiently extract a nickname field (metadata>’nickname’) from a JSON document for a particular known event: SELECT metadata->’nickname’ FROM insects WHERE species = ‘cockroach’ The query above can extract the “nickname” field from the metadata without decoding the entire JSON document thanks to the efficient JSONB encoding. Unsurprisingly, we encoded JSONB to take up as few bytes as possible. Because certain byte patterns can be more or less amenable to compression, any solution for JSONB encoding also needed to make good use of the compression algorithms already present in CockroachDB. We used an approach adopted from Postgres in order to improve the compressibility of our encoding, which you can read more about in our RFC . Unpredictably shaped data Suppose you overheard your friend talking about their favorite insect, Craiggy. You know that your friend previously mentioned Craiggy’s species, but you can’t remember it now. Recalling the different fields present in JSONB, you decided to construct the following query to jog your memory: SELECT species FROM insects WHERE metadata->’nickname’ = ‘Craiggy’ This query looks similar to the one above, but notice that this query now filters by JSON column. When filtering by species, the database can perform a quick lookup by using the index on species, but filtering by metadata->’nickname’ requires a full table scan because there is no index on that field. Further complicating matters, the ‘nickname’ field is not guaranteed to exist in the metadata JSON at all. Many developers would prefer to index on the important fields at the time of development, however, this can be challenging to predict because the JSON structure in a given column is often heterogeneous. What if you later need to filter by metadata->’best_friend’? We expect customers to consistently need solutions for unpredictable data. This is why we decided to implement inverted indices (known as a GIN index in Postgres) alongside JSONB. Because inverted indices do not require you to know, or specify ahead of time, indexed fields, users don’t need to pre-specify the shape of their data. This allows you the flexibility to update your application as your business needs to change, so that you can always efficiently query your data. CREATE INVERTED INDEX ON insects (metadata) This index allows us to efficiently query any field in the metadata JSON, no matter what information our customers want to store. You could work around this lack of knowledge about your customers desired data storage by indexing a cross section of all fields with comparatively cheap storage. However, at this point you’d have a large amount of data indexed that does not provide additional value. Ideally, you would wish to index JSON columns on precisely the fields they each contain. Luckily, this is exactly how inverted indexing with JSONB works in CockroachDB. Implementation of Inverted Indices You can view a JSONB document as a set of paths from the root to the leaves. For example, the following JSON document has four paths from the root to a leaf: {\n    \"a\": {\"b\": 1}\n    \"c\": {\"d\": 2, \"e\": 3},\n    \"f\": 4\n}\n\n\na/b/1\nc/d/2\nc/e/3\nf/4 An inverted index creates one index entry for each of these, allowing efficient querying for any kind of JSONB document. For example, a user might ask for all documents x for which x.a.b equals 7 . These queries are done using the JSONB “containment” operator: SELECT x FROM some_table WHERE x @> ‘{“a”: {“b”: 7}} Before we introduced JSONB and inverted indices, CockroachDB required you to know which fields need indexing in advance. Now, a single inverted index can support the containment queries you know about today as well as lay the foundation to support any future containment queries you might need. If you’re curious to peek behind the curtain a bit more, we explored encoding, inverted indices, and more within our public RFC process. When should I use structured data instead of JSONB? JSONB performs best for frequently read and infrequently written data. Due to CockroachDB’s use of MVCC , a modification to a piece of data requires copying and retaining the old version of that piece of data. Further, since we replicate data three times to increase survivability, rewriting large pieces of data many times can hurt performance due to write amplification. We know that many developers find JSON to be an intuitive and useful tool. However, using JSON will be less effective when a traditional relational schema will suffice. For example, every JSON document inefficiently stores all of its keys when compared to relational data (though this is alleviated somewhat due to our compression). Therefore, you should use JSON to efficiently manage rows with different types of values and hierarchical data. You can also use JSON to conduct rapid prototyping. How do we work with our open-source community? CockroachDB loves open-source contributions! Since JSON was our most requested feature on GitHub (coming in with more than 80 thumbs up, smileys, party hats, and hearts), we knew we’d like to get the community involved in its implementation. But how could we get you involved? We opened a GitHub issue soliciting help from our community to implement the long tail of functions that operate on JSONB columns. While many people contributed to our final product, we’d like to offer a special thank you to Yihong He , who submitted PRs that covered multiple built-in functions. As always, we welcome the community’s involvement in CockroachDB and look forward to more open-source contributions. Try JSONB! In the future, we will dive deeper into JSONB use cases to further explain how you can use CockroachDB and JSONB to make data easy. You can try JSONB before it’s available in GA by downloading the latest 2.0 beta . Illustration by Wenting Li", "date": "2018-03-22"},
{"website": "CockroachLabs", "title": "Your business will evolve. Your database shouldn't have to", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/database-evolution/", "abstract": "Best on Desktop! This article features some really interesting simulations that you should check out on your desktop. Cockroaches first evolved more than 300M years ago, and yet the O.G. is still recognizable. “Modern” cockroaches are about 200M years old; that they’re still with us, largely unchanged, is quite impressive from an evolutionary perspective. Meanwhile, everything seems to be evolving at lightspeed in our industry. The growth of the public cloud has prompted technology investments, resulting in a new wave of advanced orchestration capabilities (e.g. Cloud Foundry, Mesosphere, Docker, Kubernetes, etc). That these capabilities bring Google-like power to operations is no accident—they were largely inspired by or even created at Google. Their aim is simple: realize the efficiencies available through the public cloud by coherently managing the lifecycles of hundreds or thousands of VMs to deploy microservices. The public cloud provides the extremely low-friction resources; the orchestration layer provides the control surface. To date, Kubernetes has made running instances of stateless application logic simple. It essentially boils down to packaging the application logic into a Docker image and instructing Kubernetes to schedule N instances behind a load balancer (I’m going to use Kubernetes generically from here on out). It sounds simple, but the combination of technologies are accomplishing a number of tasks which used to require truly significant fixed R&D and variable opex costs. Unless your next project is an online calculator, there is no point in building a stateless application. The gaping hole in the Kubernetes story has been—and remains—state. Companies looking to re-architect and re-deploy with Kubernetes can stride into the future with one foot (application microservices), while the other (storage systems) remains stuck in the past. Existing monolithic database technologies like Oracle, MySQL and Postgres can certainly be scheduled using Kubernetes, but that misses the point. They’re unable to take advantage of the benefits inherent in the very idea of “the cloud”. It’s like casting pearls before swine, no offense to MySQL or Postgres. What do I mean by using the inherent benefits of the cloud? It’s simple really: cloud-native databases can leverage the ability to quickly schedule resources within a facility, but also across facilities, cloud service providers, and even continents. This can allow them to provide scale, unmatched resilience, low-latency global operation, and data sovereignty compliance. Monolithic databases remain useful pieces of technology, but because they require scaling up just one master node, they are evolutionary dead ends. They are the products of a smaller, less-connected era, and their shortcomings risk becoming liabilities for your business as it evolves. CockroachDB takes advantage of cloud resources to let your business evolve gracefully as it grows, without requiring a re-architecture or significant migration. It scales elastically using commodity resources, has integral high availability, can geographically replicate data for global access, provides an industry-standard SQL API, and deploys with Kubernetes as easily as your stateless microservices. These are vital capabilities, and I’m going to try to make them more concrete in the remainder of this post. I’ve created a D3 simulation (scroll down) which illustrates how CockroachDB can be deployed as the central OLTP component of your company’s data architecture, no matter which stage you’re at. There are five deployments shown here, and in most cases, the simulation illustrates a transition from one stage to the next. Stage 1: Build. You've got to start somewhere. And that's actually a significant problem with some cloud DBaaS offerings. CockroachDB is open source and runs on macOS, Linux, and Windows, so development on a laptop is simple and expedient. You can easily setup local clusters too, using the roachdemo tool . If you're developing microservices using Go, you can spin up ephemeral CockroachDB clusters in unittests using testserver . Stage 2: Stand up a resilient service Listen to the explanation on YouTube >> The first time CockroachDB is deployed to the cloud, it might be started as a single node. However, a crucial benefit of CockroachDB is that it's inherently highly available (HA), requiring no complex configuration or third-party failover mechanism to replicate and remain available in the event of node failure. It just needs additional nodes to join the cluster. But a highly available database with a symmetric, shared-nothing architecture isn't just for resilience in the face of unplanned failures. It's a crucial enabler for automating prosaic administrative tasks like zero-downtime upgrades and VM rescheduling. CockroachDB uses the Raft consensus protocol to consistently replicate data between nodes. Table data is split into segments of contiguous key space (ordered by primary key), internally called “ranges”. Each range runs its own Raft algorithm to replicate and repair the data it contains. If you'd like a more sophisticated explanation, there's more detail available here . In the simulations below, each range is visually depicted by a vertical stack of three replicas (replicas are visually depicted as boxes). Before we get started, you might be wondering about the figures in the simulation diagrams on this page. Here's a quick legend. Each of the circular figures represent either a single node or a collection of nodes. If labeled as an internal IP address (e.g. “10.10.1.1”), they are a single node. Otherwise, they represent a collection of nodes, either as a facility (e.g. “New York City”) or even multiple facilities within a region (e.g. “United States”). Facilities and regions may be clicked to expand the facilities or nodes they contain. Use the escape key or the browser's back button to zoom back out. Hovering over the outside of the capacity gauge expands it, showing a pie-chart with the breakdown of space used between four database tables: Registration, Products, Orders, and Details. Hover over the direct center to see network links (note that this only works if there's more than one node shown). Stage 3: Achieve significant scale Listen to the explanation on YouTube >> You can put a lot of data on a server these days, but big and monolithic is only the way people are used to running databases. You wouldn't deploy your application logic on a solitary, scaled-up server because you'd want to avoid a single point of failure, and you'd want the option to scale beyond even the largest monolithic server. You'd also want to minimize any disruption to client load in the event of node loss. The same principles apply to your database, only more so. A typical disruption to a monolithic database is total (as experienced by connected clients), and can have long recovery time objectives, even with sophisticated failover mechanisms. Worse, monolithic architectures, even when configured with active/passive or active/active replication, can have a non-zero recovery point objective, meaning there could be data loss. When a CockroachDB node experiences failure, the entire aggregate bandwidth of the cluster is used to up-replicate the missing data. This same mechanism is used to rebalance data as new nodes are added to a cluster. In the simulation below , the original three node cluster is scaled by adding five additional nodes. Note that the capacity of each node in this example has been reduced to more clearly illustrate relative fullness and iterative rebalancing. Stage 4: Provide enterprise SLAs Listen to the explanation on YouTube >> You have a fast-growing business and CockroachDB has allowed you to scale within your primary datacenter (in this example, it's located in New York City). Whether your business is B2C and you've reached critical mass, or B2B and you've landed some big enterprise customers, at some point the pressures on your data architecture will again expand. This time, with more stringent requirements around service level agreements. In other words, you really can't allow the system to go down because of a facility outage. To accomplish this, data must be replicated not just within a facility, but across facilities. You need some level of geo-replication. There is a cost to geo-replication, especially when done with quorum-based replication (like Raft). The cost you pay is latency, because for a write to become permanent, a majority of replication sites must acknowledge it. This means that writes have a minimum latency equal to the second slowest communication link between replication sites (in the case of three replicas). In practice, you want to choose facilities which are relatively close: within 30ms of each other, but probably not across the globe. However, you also want to balance proximity with geo-diversity, such that you minimize correlated failures (i.e. avoid doubling up on power sources or fiber backbones). Stage 5: Service global customers Listen to the explanation on YouTube >> Your business has grown to the point where you must service customers internationally. These days, this situation can just as easily apply to a fast-growing startup company as a multi-national enterprise. How do you solve the thorny issues around latency and data sovereignty? The old way of doing things was to run a primary facility on the East Coast of the United States, with a secondary facility ready as a hot standby. But customers, whether they're individual consumers of your online game, or other companies using your SaaS offering, are becoming less satisfied with the status quo. The two big challenges which need to be solved are service latency and customer data domiciling preferences. With the EU's GDPR regulations coming into effect in May of 2018, and many other countries following suit, personal data privacy is an issue whose time has come. In particular, companies must get a very explicit consent from a customer when personal data will leave their jurisdiction for processing or storage. Companies that fail to provide for local data domiciling can expect hefty fines, the loss of their customers, or both. One solution is to break up your global service into individual regional services, but this is expensive operationally and greatly compounds complexity for your application developers. Your customers likely still expect you to be providing a global service. They move, they interact with other customers across regions. These are difficult problems to solve at the application layer. Geo-Partitioning Enter geo-partioning (to be released in CockroachDB 2.0 later this spring). Database partioning isn't a new concept. RDBMSs like Oracle, SQLServer, and Postgres allow you to partition tables, mostly in order to manage the size of active data so that it can be quickly restored. CockroachDB has from the first version been able to replicate different databases or tables to different replication sites within a cluster. Geo-partitioning allows row-level control of replication. So, for example, a table might be partitioned based on its “region” column, containing values like “us-ca”, “eu-de”, “eu-fr”, and “cn-bj”. Any rows with region=“eu-de” might be replicated within a single facility in Germany, or across three facilities in Germany, whereas rows with region=“cn-bj” might be replicated to three facilities near Beijing, or even across China. Now that you’ve seen what it can do, install CockroachDB yourself . Illustration by Zach Meyer", "date": "2018-02-22"},
{"website": "CockroachLabs", "title": "CockroachDB is 10x more scalable than Amazon Aurora for OLTP workloads", "author": ["Arjun Narayan", "Jordan Lewis", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/performance-part-two/", "abstract": "[For CockroachDB's most up-to-date performance benchmarks, please read our Performance Overview page ] The three design principles of CockroachDB are correctness, stability, and performance. Having achieved our correctness and stability goals with CockroachDB 1.0 and 1.1, we focused heavily on performance with CockroachDB 2.0. For more information on which benchmarks matter, or to see a comparison between CockroachDB 1.1 and 2.0, you can read CockroachDB 2.0 Makes Significant Strides . Today we are releasing a comprehensive whitepaper that demonstrates how CockroachDB achieves high OLTP performance of over 128,000 tpmC on a TPC-C dataset over 2 terabytes in size. This OLTP performance is over 10x more TPC-C throughput than Amazon Aurora , in a 3x replicated deployment with single-digit seconds recovery time and zero-downtime migrations and upgrades. This far surpasses a typical active-passive database deployment with manual failure recovery. CockroachDB achieves this in serializable isolation, unlike competing databases that sacrifice isolation for performance. We’re very excited to talk about performance, and have taken care to make this more than just an announcement with vague numbers. In keeping with our open source philosophy, our whitepaper contains a step-by-step reproduction of instructions to verify all our performance claims, as well as context on our benchmarking philosophy and practices. We don’t just want you to take our word for our performance numbers, we’d like to arm you with all the tools you need to check it out for yourself! In this post, we’d like to cover some brief highlights, but do check out the whitepaper for more details and a fully reproducible test script. Benchmarking Scenario We’ve decided to start with an unofficial TPC-C benchmark as our first benchmark. The TPC-C is a specification for a set of load that simulates the backend data processing requirements of a large retailer. Unlike NoSQL benchmarks such as YCSB, which involve a single logical table of key-values, no transactions, and only two queries, TPC-C exercises a richer set of SQL semantics with foreign keys, multiple indexes, transaction rollbacks, and joins. It also stresses the underlying storage capabilities of the database. DISCLAIMER: this benchmark script was not validated and certified by the Transaction Processing Council. The results obtained are not official TPC-C results, and the results are not comparable with any official TPC-C results. Instead, we only compare our results to other unofficial TPC-C results by competing vendors such as Amazon Aurora. We have also made one modification to the TPC-C specification: we run every transaction in serializable mode as opposed to degrading the isolation guarantees selectively. As we show in this benchmark, one can achieve high performance without compromising safety, maintainability, and correctness. Benchmarking Setup We benchmarked two scenarios: a 3 node cluster on the TPC-C 80GB (1,000 warehouse) dataset, and a 30 node cluster on the TPC-C 800GB (10,000 warehouse) dataset. We compare our unofficial TPC-C results to Amazon Aurora RDS unofficial TPC-C results from AWS re:Invent 2017. We also used Aurora’s SIGMOD 2017 paper for additional information as to their test setup and load generator. Small clusters We first set up a 3 node CockroachDB cluster on Google Compute Engine. Three nodes is the minimum for proper fault tolerance in CockroachDB, and a good initial setup. We use 3 n1-highcpu-16 VMs and loaded it with 1,000 warehouses of data. This translates to 80GB of data replicated 3 ways, which results in 200GB of data stored across the cluster after compression. Large Clusters We then set up a 30 node CockroachDB cluster on Google Compute Engine, using the same n1-highcpu-16 VMs, loaded 10,000 warehouses of data (a 2TB replicated dataset). Database CRDB 2.0 Amazon Aurora RDS MySQL Throughput (1,000 warehouses) 12,819 tpmC 12,582 tpm Median latency (1,000) 88.1ms Not reported 95th percentile latency (1,000) 151.0ms Not reported Hardware (1,000) 3x Google Compute Engine n1-highcpu-16 with attached Local SSDs 2x r3.8XL (One read replica with automatic failover) Database CRDB 2.0 Amazon Aurora RDS MySQL Throughput (10,000 warehouses) 128,587 tpmC 9406 tpmC Median latency (10,000) 88ms Not reported 95th percentile latency (10,000) 176ms Not reported Hardware (10,000) 30x Google Compute Engine n1-highcpu-16 with attached Local SSDs r3.8XL (MySQL RDS) Database CRDB 2.0 Amazon Aurora RDS MySQL Availability Can tolerate failure of any node. Can tolerate failure of a master and automatically promote read replicas to master. RPO Zero (data is replicated 3x across the cluster) Zero (underlying elastic block storage is 6x replicated) Default Transaction Isolation Level Serializability Repeatable read Takeaways 10x more throughput than Amazon Aurora: Our highest reported tpmC of 128,587 is 10x that of Aurora’s 12,582 tpmC. Linear Scalability: CockroachDB scales linearly in performance: our 10,000 warehouse uses 10x the nodes as our 1,000 warehouse cluster, providing 10x the throughput. Combined with our zero-downtime cluster migrations and node additions, your operational costs will scale with your business. Performance under strong isolation We achieve this performance in serializable mode, the strongest isolation mode in the SQL standard. Unlike Aurora and other databases that selectively degrade isolation for performance, we show that this is not a choice you have to make. CockroachDB gives you both correctness and performance, without sacrificing one or the other. Don’t just take our word for it, reproduce these numbers yourself! We aren’t just publishing these numbers today. Complete step-by-step reproduction instructions are in our whitepaper . Our database and all our tooling is open-source, so you can run these benchmarks yourself, and then scrutinize the code to ensure we haven’t missed anything. And if we have, we’d greatly appreciate your bug report or your pull request! Illustration by Dalbert B. Vilarino", "date": "2018-04-18"},
{"website": "CockroachLabs", "title": "How to Leverage Geo-partitioning", "author": ["Andy Woods", "Daniel Harrison"], "link": "https://www.cockroachlabs.com/blog/geo-partitioning-two/", "abstract": "Introduction As we’ve written about previously , geographically distributed databases like CockroachDB offer a number of benefits including reliability, security, and cost-effective deployments. We believe you shouldn’t have to sacrifice these upsides to realize impressive throughput and low latencies. That’s why we created geo-partitioning. This blog post defines two new features, geo-partitioning and archival-partitioning, as well as explains when you might want to leverage these features. We previously provided a sneak-peak walkthrough of geo-partitioning that can be found here . Geo-partitioning & Archival Partitioning Defined Geo-partitioning allows you to keep user data close to the user, which reduces the distance that the data needs to travel, thereby reducing latency and improving user experience. To geo-partition a table, you should define location-based partitions while creating a table, create location-specific zone configurations, and apply the zone configurations to the corresponding partitions. Archival-partitioning allows you to store infrequently-accessed data on slower and cheaper storage. To archival-partition a table, you should define frequency-based partitions while creating a table, creating frequency-specific zone configurations with appropriate storage devices constraints, and applying the zone configurations to the corresponding partitions. For a more detailed description of both geo-partitioning and archival-partitioning please consult our documentation . Why should you use geo-partitioning or archival partitioning? Reduce latency Companies increasingly insist on low latencies to minimize delays which cause a direct decline in revenue. For example, 100ms of latency costs Amazon 1% in sales and an extra 500 ms in search page generation time dropped Google’s traffic by 20% . With geo-partitioning, developers can use what they know about a user’s location to keep data close to them while taking advantage of CockroachDB’s durability guarantees of consistency in the face of machine (or even datacenter) failure. Keeping data close to users reduces the distance data needs to travel, thereby reducing latency and improving end-user experience. Follow data domiciling & GDPR regulations Governments continue to introduce new and increasingly tough regulations that put significant financial and organizational pressure on companies. Regulations like the EU GDPR and China’s Cyber Security Law impose restrictions on where data can reside, which undermine some of the core operational and economic benefits of the cloud. Geo-partitioning restricts the location of data to specific regions (i.e., data-domicile), such as EU countries. Regulations, in some cases (e.g., China), directly require data domiciling. In other cases (e.g., GDPR) restrictions on data location aim to provide data privacy as a fundamental right and allowing companies greater control for managing user data. Save money with colder storage Keeping all corporate data in the same storage class is costly and inefficient. This is especially pernicious when the benefits of a solid state drives are not needed for data that’s infrequently accessed or only being preserved for compliance purposes. By using archival-partitioning data based on storage time, developers can use CockroachDB to move certain types of data to colder, slower, and cheaper storage. How do other databases use partitioning? At this point, you may be wondering, hasn’t some form of partitioning always existed in other databases? How is CRDB different? CockroachDB automatically shards data We built CockroachDB from the ground up as a distributed database. This means that some of the partitioning use cases you may be familiar with in other DBMSs are solved by superior features within CockroachDB. CockroachDB keeps your data in 64MB ranges that the system constantly keeps balanced in response to machine downtime and changes in load. Instead of manually sharding your data into partitions when it's too big or too high traffic to fit on one machine, we transparently (and automatically) shard all data (while allowing you to retain control over where it lives to maintain optimum performance). In the same vein, a simple replication zone metadata change moves data from one machine to another (such as when moving data from high-end storage tier to low-cost storage tier). CRDB’s SQL planner automatically tunes performance Other databases often use partitioning for performance tuning. \"Partition pruning\" will avoid sending requests to machines containing data that couldn't be in a partition (SELECT … FROM table WHERE date > '2018-01-01'). Our SQL planner does this for every query, whether the tables are partitioned or not, only contacting the relevant 64MB ranges . We only contact the shards that are relevant for each query, addressing the smallest amount of potentially relevant data. Other databases use partitioning to exclude ranges of data that are not germane to the query in question; if a partition is a-b and the query >c, it will avoid the a-b. Management/maintenance of tables Partitioning can also be used to complete rolling upgrades or schema changes. By contrast, CockroachDB doesn’t need to use partitioning for this feature because we already have online schema changes . Online schema changes allow DBAs to run database changes without interrupting mission critical workloads. Bulk load with import CSV Finally, some databases use partitioning for bulk loading to avoid interrupting business as usual. CockroachDB doesn’t need to use partitioning for bulk import because CRDB’s architecture and automatic replication make it easy to bulk import data using the Import statement . Geo-partitioning demo To get a better understanding of how geo-partitioning works, you should check out the \"Geo Replication\" section of this blog post (though the whole thing is worth a read). Future work + enhancements We don’t yet support using partitioning to import data to or bulk delete data from an existing table. We view these as logical enhancements of our current work and we plan to support them in the future. We also aspire to provide a set of features aimed at making regulatory compliance even easier for our users to follow. This includes allowing you to query only from partitioned tables based upon the regulatory conditions faced by their business. We built CockroachDB as a high-performance distributed database. We hope that by using features like geo-partitioning and archival-partitioning we will continue to “make data easy” for you. To learn more and try it out yourself, click here . Illustration by Lea Heinrich", "date": "2018-04-12"},
{"website": "CockroachLabs", "title": "Kubernetes: The State of Stateful Apps", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/kubernetes-state-of-stateful-apps/", "abstract": "Over the past year, Kubernetes––also known as K8s––has become a dominant topic of conversation in the infrastructure world. Given its pedigree of literally working at Google-scale, it makes sense that people want to bring that kind of power to their DevOps stories; container orchestration turns many tedious and complex tasks into something as simple as a declarative config file. The rise of orchestration is predicated on a few things, though. First, organizations have moved toward breaking up monolithic applications into microservices. However, the resulting environments have hundreds (or thousands) of these services that need to be managed. Second, infrastructure has become cheap and disposable––if a machine fails, it’s dramatically cheaper to replace it than triage the problems. So, to solve the first issue, orchestration relies on the boon of the second; it manages services by simply letting new machines, running the exact same containers, take the place of failed ones, which keeps a service running without any manual interference. However, the software most amenable to being orchestrated are ones that can easily spin up new interchangeable instances without requiring coordination across zones. Why Orchestrating Databases is Difficult The above description of an orchestration-native service should sound like the opposite of a database, though. Database replicas are not interchangeable; they each have a unique state. This means you cannot trivially bring them up and down at a moment’s notice. Deploying a database replica requires coordination with other nodes running the same application to ensure things like schema changes and version upgrades are visible everywhere. In short: managing state in Kubernetes is difficult because the system’s dynamism is too chaotic for most databases to handle––especially SQL databases that offer strong consistency. Running a Database with a Kubernetes App So, what’s a team to do? Well, you have a lot of options. Run Your Database Outside Kubernetes Instead of running your entire stack inside K8s, one approach is to continue to run the database outside Kubernetes. The main challenge with this, though, is that you must continue running an entire stack of infrastructure management tools for a single service. This means that even though Kubernetes has a high-quality, automated version of each of the following, you'll wind up duplicating effort: Process monitoring (monit, etc.) Configuration management (Chef, Puppet, Ansible, etc.) In-datacenter load balancing (HAProxy) Service discovery (Consul, Zookeeper, etc.) Monitoring and logging That’s 5 technologies you’re on the hook for maintaining, each of which is duplicative of a service already integrated into Kubernetes. Cloud Services Rather than deal with the database at all, you can farm out the work to a database-as-a-service (DBaaS) provider. However, this still means that you’re running a single service outside of Kubernetes. While this is less of a burden, it is still an additional layer of complexity that could be instead rolled into your teams’ existing infrastructure. For teams that are hosting Kubernetes themselves, it’s also strange to choose a DBaaS provider. These teams have put themselves in a situation where they could easily avoid vendor lock-in and maintain complete control of their stack. DBaaS offerings also have their own shortcomings, though. The databases that underpin them are either built on dated technology that doesn’t scale horizontally, or require forgoing consistency entirely by relying on a NoSQL database. Run Your Database in K8s––StatefulSets & DaemonSets Kubernetes does have two integrated solutions that make it possible to run your database in Kubernetes: StatefulSets By far the most common way to run a database, StatefulSets is a feature fully supported as of the Kubernetes 1.9 release. Using it, each of your pods is guaranteed the same network identity and disk across restarts, even if it's rescheduled to a different physical machine. DaemonSets DaemonSets let you specify that a group of nodes should always run a specific pod. In this way, you can set aside a set of machines and then run your database on them––and only your database, if you choose. This still leverages many of Kubernetes’ benefits like declarative infrastructure, but it forgoes the flexibility of a feature like StatefulSets that can dynamically schedule pods. StatefulSets: In-Depth StatefulSets were designed specifically to solve the problem of running stateful, replicated services inside Kubernetes. As we discussed at the beginning of this post, databases have more requirements than stateless services, and StatefulSets go a long way to providing that. The primary feature that enables StatefulSets to run a replicated database within Kubernetes is providing each pod a unique ID that persists, even as the pod is rescheduled to other machines. The persistence of this ID then lets you attach a particular volume to the pod, retaining its state even as Kubernetes shifts it around your datacenter. However, because you’ll be detaching and attaching the same disk to multiple machines, you need to use a remote persistent disk, something like EBS in AWS parlance. These disks are located––as you might guess––remotely from any of the machines and are typically large block devices used for persistent storage. One of the benefits of using these disks is that the provider handles some degree of replication for you, making them more immune to typical disk failures, though this benefits databases without built-in replication. As of this post’s publication, StatefulSets support for local disks is in beta . Once this feature moves to general availability, we’ll update our recommendations accordingly. Performance Implications Because Kubernetes itself runs on the machines that are running your databases, it will consume some resources and will slightly impact performance. In our testing, we found an approximately 5% dip in throughput on a simple key-value workload. Because StatefulSets still let your database pods to be rescheduled onto other nodes, it’s possible that the stateful service will still have to contend with others for the machine’s physical resources. However, you can take steps to alleviate this issue by managing the resources that the database container requests. DaemonSets: In-Depth DaemonSets let you specify that all nodes that match a specific criteria run a particular pod. This means you can designate a specific set of nodes to run your database, and Kubernetes ensures that the service stays available on these nodes without being subject to rescheduling––and optionally without running anything else on those nodes, which is perfect for stateful services. DaemonSets can also use a machine’s local disk more reliably because you don’t have to be concerned with your database pods getting rescheduled and losing their disks. However, local disks are unlikely to have any kind of replication or redundancy and are therefore more susceptible to failure, although this is less of a concern for services like CockroachDB which already replicate data across machines. Performance Implications While some K8s processes still run on these machines, DaemonSets can limit the amount of contention between your database and other applications by simply cordoning off entire Kubernetes nodes. StatefulSets vs. DaemonSets Kubernetes StatefulSets behave like all other Kubernetes pods, which means they can be rescheduled as needed. Because other types of pods can also be rescheduled onto the same machines, you’ll also need to set appropriate limits to ensure your database pods always have adequate resources allocated to them. StatefulSets’ reliance on remote network devices also means there is a potential performance implication, though in our testing, this hasn’t been the case. DaemonSets on the other hand, are dramatically different. They represent a more natural abstraction for cordoning your database off onto dedicated nodes and let you easily use local disks ––for StatefulSets, local disk support is still in beta. The biggest tradeoff for DaemonSets is that you're limiting Kubernetes' ability to help your cluster recover from failures. For example, if you were running CockroachDB and a node were to fail, it can't create new pods to replace pods on nodes that fail because it's already running a CockroachDB pod on all the matching nodes. This matches the behavior of running CockroachDB directly on a set of physical machines that are only manually replaced by human operators. Up Next In our next blog post, we continue talking about stateful applications on Kubernetes, with details about how you can can (and should) orchestrate CockroachDB in Kubernetes leveraging StatefulSets . If you're eager to get something started, though, you should check out our Kubernetes tutorial . And if building and automating distributed systems puts a spring in your step, we're hiring! Check out our open positions here . Illustration by Zoë van Dijk", "date": "2018-05-01"},
{"website": "CockroachLabs", "title": "Cluster Visualization: Getting Started with a Globally Distributed Database", "author": ["Diana Hsieh"], "link": "https://www.cockroachlabs.com/blog/cluster-visualization/", "abstract": "CockroachDB makes it possible to support a global customer base while remaining compliant with data privacy regulations. Operators interact with a single logical control plane that they can use to define how they want CockroachDB to store their row-level data. Meanwhile, developers continue to interact with our PostgreSQL-compatible API that transparently handles distributing queries across a global cluster. With our 2.0 release, we introduced a new cluster visualization in our web UI to help operators monitor a global cluster. In this blog, we’ll use a fictional startup - MovR - to outline how CockroachDB 2.0 makes it easier for operators to manage infrastructure for global user bases. \"MovR\" (first introduced in our 2.0 meetup ) is a P2P vehicle sharing app looking to take over the global market for vehicle sharing. The majority of their customers are in New York, where they started, and they have recently expanded to San Francisco and Amsterdam. MovR’s users want to book the vehicles they need whenever they need them without having to stare at a loading circle. Running a single datacenter in New York is not an option, as this setup would be prone to service outages and forces users in San Francisco and Amsterdam to experience the network latency that comes with having to access data that lives in New York. Low Operational Cost MovR doesn’t have the resources to invest in huge operations teams to build and maintain complex infrastructure. They want to focus on doing what they do best - building features for P2P vehicle sharing. With CockroachDB, operators define how they want CockroachDB to store data, and the database handles the placement, replication, and consistency of the data transparently to the user. Operators only have to monitor a single cluster compared to multiple disparate stacks installed in different countries. Visualizing Global Deployments Single logical control plane for operators and developers With CockroachDB, operators can view their entire cluster as a pool of resources that can smartly support distributed work loads. Maintenance operations like backups, restores, and inputs can utilize all resources available. The cluster exposes metrics that can be fed into monitoring, and applications talking to New York nodes can still access data stored on San Francisco nodes. Our new cluster visualization helps developers and operators visualize this unified resource pool, offering a global view of the cluster while bucketing resources into geographic regions. Aligning resources to customer hubs for better performance and latency characteristics With a view of the global state of their cluster, MovR is also able to dive in deeper to view the state of just New York nodes, as shown below. These views help operators identify areas in which they may need to dig in further to troubleshoot issues. For example, in the below view, Node 1 rejoined the cluster a couple hours ago and it appears to have more CPU and disk utilization than Node 2, which has been up for the last 13 days. This could indicate a hot range on Node 1 that ought to be split in order to more evenly distribute the load across the nodes. Scaling up to grow with your business The distributed and scalable nature of CockroachDB allows companies to allocate resources to regions that need it the most. If MovR were to notice a huge spike in usage in San Francisco, they would want to add an additional node to San Francisco. They can do this quite simply by bringing up another node and having it join the cluster. Now, we can click into the San Francisco and view the two nodes that make up that region’s resources. This is just the beginning CockroachDB 2.0 is a peek into a new way of deploying global infrastructure. We are planning on investing more time making it easier and faster to run global databases, and invite you to join us in our journey. Currently, both the cluster visualization is an enterprise feature, but you can easily sign up for a free 30 day trial to check it out. We have startup pricing for businesses just getting started, and you can always upgrade to an enterprise license from the core (free) version. We would love to get your feedback - please engage with us through GitHub or StackOverflow . We’d like to thank our entire community who have followed us so far, and hope to bring CockroachDB to many more users in the future.", "date": "2018-05-15"},
{"website": "CockroachLabs", "title": "What Write Skew Looks Like", "author": ["Justin Jaffray"], "link": "https://www.cockroachlabs.com/blog/what-write-skew-looks-like/", "abstract": "Syndication from What Does Write Skew Look Like by Justin Jaffray This post is about gaining intuition for Write Skew, and, by extension, Snapshot Isolation. Snapshot Isolation is billed as a transaction isolation level that offers a good mix between performance and correctness, but the precise meaning of \"correctness\" here is often vague. In this post I want to break down and capture exactly when the thing called \"write skew\" can happen. A quick primer on transactions The unit of execution in a database is a transaction. A transaction is a collection of work that either completes in its entirety or doesn't run at all. There are no half-run transactions. There's a number of guarantees typically provided around transactions, but we're going to focus on isolation. Isolation is what allows users of a database to not be concerned with concurrency and it determines the extent to which a transaction appears as if it's running alone in the database. For example, if transaction A reads x = 5 , then transaction B overwrites that to x = 8 , if A reads again and sees x = 8 , A 's isolation has been violated: it no longer can maintain the illusion that it's the only process running in the database. You can think of a transaction as a process which performs reads and writes in a database. Because of this, we often talk of the read set and write set of any given transaction. You can think of the read set as the set of memory locations read by the transaction, and the write set as the set of memory locations written to by the transaction. Isolation levels in a database are concerned with which executions (also called \"histories\") are allowed by the database and which are not. A lower isolation level will reject fewer histories, while a higher isolation level will reject more histories. For instance, a history in which a transaction reads a write from an aborted transaction is probably no good and should be blocked (somehow) by the database. The classification of various possible isolation levels is an interesting topic , but here we're going to focus on the relationship between just two of them: SERIALIZABLE and \"Anomaly SERIALIZABLE\" (sometimes known as SNAPSHOT). SERIALIZABLE In some sense, serializability is \"perfect\" isolation. This is what we get when it appears as if every transaction actually is run in the database all by itself, even though that's probably not what's going on under the hood (my server has lots of cores, I want to make use of them). If we were running every transaction all by itself, this would just be called \"serialized\". Since it's just equivalent to running them individually, it's called serializable. The clean definition of a serializable execution is, \"one which is equivalent to some serial execution,\" a serial execution being one in which we just run our transactions one at a time with no interleaving of operations due to concurrency. We can think of serializability as a scheme in which every transaction logically does all of its work at a single, unique timestamp. Since every transaction, conceptually at least, happens instantaneously, there is no troublesome concurrency with which the user should be concerned. As an aside, it's worth noting that \"the SERIALIZABLE isolation level\" and \"a serializable execution\" are two distinct concepts. Most discussions I've seen conflate them, despite this leading to confusing questions such as \"how does a transaction which is denoted 'serializable' interact with one which is not\"? Anomaly SERIALIZABLE (but actually SNAPSHOT) Serializability is strange, being one of the few concepts in transactional theory that has a very simple and unambiguous definition and yet still somehow managed to have two completely different mainstream meanings. Back in the day, when ANSI defined the SERIALIZABLE isolation level, their definition, while correct, could be interpreted to not preclude a lower isolation level now called \"Snapshot Isolation\", and a handful of enterprising database vendors took advantage of this fact. The most well-known example of this is that if you ask Oracle for SERIALIZABLE, what you get is actually SNAPSHOT. This entire post is in some sense about understanding what is the deal with Snapshot Isolation, so if you don't get intuition for it immediately, don't fret. There are two main properties that characterize Snapshot Isolation: A transaction in Snapshot Isolation has two significant timestamps: the one at which it performs its reads, and the one at which it performs its writes. The read timestamp defines the \"consistent snapshot\" of the database the transaction sees. If someone else commits writes at a point after a transaction T's read timestamp, T will not see those changes (this is generally enforced using MVCC . We'll refer to a transaction named \"x\" as T x and its read and write timestamps as R x and W x , respectively. Two transactions are concurrent if the intervals during which they are executing overlap (R 1 < W 2 and W 1 > R 2 ). In Snapshot Isolation, the database enforces that two committed transactions which are concurrent have disjoint write sets (meaning they don't write to any of the same memory locations). Any transaction whose commit would cause this restriction to be violated is forced to abort and be retried. It's not obvious why, or even if, SNAPSHOT is distinct from SERIALIZABLE . It took a while before anyone figured out that it was . The anomaly that occurs in Snapshot Isolation was termed \"write skew\" (depending on who you ask, there is another variety , but I'm considering it a kind of write skew for simplicity), and the prototypical example of it looks like this: Consider two transactions, P and Q . P copies the value in a register x to y , and Q copies the value in a register y to x . There are only two serial executions of these two, P , Q or Q , P . In either, the end result is that x = y . However, Snapshot Isolation allows for another outcome: Transaction P reads x Transaction Q reads y Transaction P writes the value it read to y Transaction Q writes the value it read to x This is valid in Snapshot Isolation: each transaction maintained a consistent view of the database and its write set didn't overlap with any concurrent transaction's write set. Despite this, x and y have been swapped, an outcome not possible in either serial execution. Who cares? It's fair to ask why anyone should care if we're just a little bit nonserializable. Snapshot feels...pretty close to serializable, right? What is lost by our transactions not being serializable? Ben Darnell has a good outline for real-world problems that can arise due to a lack of serializability in Real Transactions are Serializable . He references Warszawski and Bailis who explore security vulnerabilities in real-world applications due to a lack of serializability. I guess I'm not a pragmatic person; I find this empirical evidence convincing, but not particularly satisfying. Is there a more fundamental reason serializability should be considered the one true way? Why is serializable the meaningful level of isolation? After all, the classic isolation \"anomalies\" are only \"anomalous\" when viewed from the perspective of serializability-as-default. If your default mode of thinking is SNAPSHOT, write skew is just normal behaviour and SERIALIZABLE is throwing away perfectly good histories. I think there are two main answers to this question. The first is simple: SERIALIZABLE is the only isolation level that \"makes sense\". By \"makes sense\", I mean \"was conceived in a principled, meaningful way\". Consider the loss of formalism and generality when going from \"every execution is equivalent to some serial execution\" to \"absence of these very specific phenomena whose definitions are biased towards the operations of SQL\". This lack of formalism was eventually rectified by Atul Adya in his thesis \"Weak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions\" , but the definitions given for lower levels are still significantly more contrived and less elegant than the definition for serializable. A more concrete reason than \"nothing else makes sense\" is a question of local vs. global reasoning. If a set of transactions must maintain some kind of invariant within the database (for instance, the sum of some set of fields is always greater than zero). In a database that guarantees serializability, it's sufficient to verify that every individual transaction maintains this invariant on its own. With anything less than serializability, including Snapshot, one must consider the interactions between every transaction to ensure said invariants are upheld. This is a significant increase in the amount of work that must be done (though in reality, I think the situation is that people simply don't do it), a point made by Alan Fekete in this talk on isolation. When is Snapshot not Serializable? This brings us to our central question: in a database that provides Snapshot Isolation, can we characterize all the nonserializable behaviour? What can we say about anomalies in Snapshot Isolation in general? What must be done to eliminate them? Fekete, Liarokapis, O'Neil, O'Neil, and Shasha (FLOOS) provide a precise answer in Making Snapshot Isolation Serializable . I'm going to attempt to outline their analysis with a focus on building up intuition for Snapshot Isolation. It's interesting that this paper comes at this problem from a strange angle: it's aimed at a DBA with access to all of the transactions being run against a database, and provides a procedure by which they can (by hand) statically analyze these transactions to verify that they will always execute serializably. I would be extraordinarily surprised if anyone besides the authors ever actually did this, but this work led the way for the Serializable Snapshot Isolation algorithm a couple years later. To proceed, we need a little bit of formalism. We need to figure out how to talk about dependencies between transactions. If T a and T b are transactions, we say T a happens before T b (written T a → T b ) if one of the following is true: T a writes a value which T b reads (a wr dependency), T a writes a value which T b overwrites (a ww dependency), or T a reads a value which T b overwrites (a rw dependency or anti-dependency). To remember what's going on here, just think that in the \"wr\", \"ww\", \"rw\" shorthands, the letter on the left matches the transaction on the left of the →, and same with the right, so if T a → T b is brought about by an \"rw\" dependency, T a reads and then T b writes. Here's how you should think about each of these cases: In a wr dependency, T a writes a value and then T b reads it, so T b can't come first, since the value it reads doesn't exist until T a runs. In a ww dependency, since T b overwrote T a 's value, T b has to come last, since its is the value that replaced T a 's value. In a rw dependency, T a has to come before T b because after T b , the value T a reads no longer exists. It is a very important observation that an rw dependency is the only dependency that can occur between concurrent transactions in Snapshot Isolation. This is because If a ww dependency occurred, it means the two transactions had intersecting write sets, which is not possible between concurrent Snapshot transactions. If a wr dependency occurred, for the later transaction to view the former's writes, it can't have begun before the former had committed. We can represent these relationships as a serialization graph. The vertices in this graph are transactions, and there's an arc from T x to T y when T x → T y . Say we had the following set of dependencies: T a → T b (wr dependency) T b → T c (rw dependency) T b → T d (ww dependency) From this we get this serialization graph: A dashed line indicates an anti- or rw-dependency. Here's a set of dependencies that isn't serializable: T a → T b (wr dependency) T b → T c (rw dependency) T c → T a (rw dependency) With the following serialization graph: Here we have a cycle: a comes before b comes before c comes before a . A history is serializable if and only if its serialization graph has no cycles. With this framework, it is \"easy\", then, to guarantee serializable execution: when a transaction goes to commit, check if its commit would create a cycle in the serialization graph. If the answer is yes, the transaction must not be allowed to commit. This technique is called \"Serialization Graph Testing\". Finding and killing all cycles is not a perfect solution if we care about performance. Detecting cycles in a graph is expensive, especially when the graph is large. Worse, because transactions are constantly committing, the graph is constantly changing. This leads us then to another question: if we know the graph was produced in a database providing snapshot isolation (as many do), is there anything special we can say about the structure of a cycle in said graph that would allow us to detect it more easily? The answer is yes. From Theorem 2.1 of FLOOS: Suppose H is a multiversion history produced under Snapshot Isolation that is not serializable. Then there is at least one cycle in the serialization graph DSG(H), and we claim that in every cycle there are three consecutive transactions T 1 , T 2 , T 3 (where it is possible that T 1 and T 3 are the same transaction) such that T 1 and T 2 are concurrent with an edge T 1 → T 2 , and T 2 and T 3 are concurrent with an edge T 2 → T 3 . Remember that the only dependency that can occur between concurrent transactions in Snapshot Isolation is an rw dependency. Thus, what FLOOS is saying here is that if there is a cycle in the serialization graph of a Snapshot history, in that cycle, there are always two consecutive dashed (rw) arcs somewhere. Theorem 2.1 is the basis of the \"Serializable Snapshot Isolation\" algorithm used today in Postgres: Running in Snapshot Isolation , every transaction tracks whether it is involved in a rw-dependency on either side, and if it is on both ends of an rw dependency, it (or its successor, or its predecessor) gets aborted. This conservatively aborts some transactions which are not involved in cycles, but definitely prevents all cycles. This approach is outlined in Serializable Isolation for Snapshot Databases . It turns out that even in Snapshot Isolation, there are so few ways two transactions can have a dependency, we can list them out explicitly. In each of these diagrams, time flows from left to right. We fix T 1 as the transaction which has the earlier write timestamp. A line denotes a transaction, with its left tip denoting the point at which it performs its reads and its right tip denoting the point at which it performs its writes. Case 1: T 2 reads a write of T 1 (wr) Case 2: T 2 overwrites a write of T 1 (ww) Case 3: T 2 overwrites a read of T 1 (rw) Case 4: T 2 overwrites a read of T 1 , T 1 and T 2 concurrent (rw) Case 5: T 1 overwrites a read of T 2 , T 1 and T 2 concurrent (rw) There's also secret cases 4b and 5b where R 2 comes before R 1 , but the differences are immaterial for our purposes, so we won't consider them separately. Here's the proof of the theorem (recall the theorem states that in any cycle there are two consecutive rw arcs somewhere): Say we have a cycle in a Snapshot history. Consider the transaction in the cycle with the earliest write timestamp. Call this T 3 , its predecessor T 2 , and T 2 's predecessor T 1 . Since T 3 has the earliest write timestamp, the only way a transaction can come before it in the cycle is via case 5. So T 2 is the predecessor of T 3 and they are connected as in case 5. Next, the read timestamp of T 1 must come before the write timestamp of T 2 or else there's no way T 1 could precede T 2 , so R 1 < W 2 . Further, the write timestamp of T 1 must come after the write timestamp of T 3 , since T 3 by definition has the earliest such timestamp. So W 1 > W 3 > R 2 . So now we know R 1 < W 2 and W 1 > R 2 , which means T 1 is concurrent with T 2 . Since concurrent transactions can only have an rw dependency, we're done. Here's what this looks like: T 2 is something of a \"pivot\" transaction that bridges the gap between the last transaction and the first. And that's it! It's not too complex of an argument, but we've completely characterized how anomalies can occur in Snapshot Isolation. As a fun exercise, try to draw the diagrams for how this fits into the example posed at the beginning of this post (with x and y ) and the read only anomaly . Conclusion If you're running a mainstream database besides CockroachDB, odds are good that the isolation level you're running at is SNAPSHOT, or even lower. Almost no other mainstream databases default to SERIALIZABLE, and many don't provide it at all. Given the prevalence of SNAPSHOT, I think it's important to understand what exactly that means. As I hope you've been convinced, the commonly understood example of \"write skew\" is insufficient. If you'd like to learn more about this topic, I can't speak highly enough of: Arjun Narayan's \"A History of Transaction Histories\" Making Snapshot Isolation Serializable A Critique of Snapshot Isolation Atul Adya's Thesis Thanks to Arjun Narayan, Ben Darnell, Sean Loiselle, Ilia Chtcherbakov, and Forte Shinko for reading this post and providing feedback. Illustration by Jamie Jacob", "date": "2018-05-24"},
{"website": "CockroachLabs", "title": "CockroachDB 2.0 Has Arrived!", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-2-0-release/", "abstract": "CockroachDB debuted as the open source database that made it possible to build massive, reliable cloud applications without giving up SQL. Forward-thinking companies adopted it to hide the complexity of dealing with distributed scale, resilience, and consistency problems in the database layer. The promise was simple: keep your apps simple and your pagers silent. Over the last six months, we’ve welcomed Mesosphere as a customer and helped companies like Kindred and Baidu continue to migrate internet-scale workloads onto CockroachDB. We’ve also watched our distributed SQL database enable exciting new use cases, from a blockchain solution for certifying document authenticity to a system of record for tracking simulations that help optimize oil and gas exploration. Since our 1.0 release , we’ve heard from our users that changes in everything from the countries where they do business to their users’ ever-growing requirements are causing them to rethink how they build and deploy applications. Developers have figured out how to design scalable and adaptable stateless services. However, many still rely on monolithic relational databases for application state. Whether deployed via a DBaaS or on-prem, these databases are stalling growth as applications ultimately depend on a hard-to-update, single point of failure that can’t take advantage of the nearly unlimited, on-demand resources available in the cloud. In this post, we highlight how CockroachDB 2.0 enables your data layer to evolve with your business: JSON enables rapid iteration in response to changing customer requirements; major throughput and scalability improvements help you handle huge increases in user request volumes; and a groundbreaking toolkit for managing multi-regional workloads lets you deliver low-latency applications to customers anywhere in the world. Adapting to changing requirements First, we’ll cover CockroachDB 2.0’s native support for semi-structured data. Early in your company or project’s life, you need to quickly adjust to frequently changing customer requirements, tweaking your data model and prototyping new features to iterate your way to success. As your project grows, you’ll need to strike a balance between the need for updates and the need to minimize downtime. CockroachDB helps shift that balance back towards shipping by adding support for JSON . Querying JSON data using Postgres syntax. JSON data types helped NoSQL databases become the top choice among many fast-moving teams that needed to avoid elaborate design planning and expensive schema migrations in order to adopt a more agile development approach. CockroachDB 2.0’s Postgres-compatible JSON implementation includes a slew of operators for doing in-place transformations and, importantly, inverted indices to speed up queries on giant volumes of data without having to map out the types of requests you want to process ahead of time. When you combine JSON support with its capability for making zero-downtime schema changes , CockroachDB 2.0 becomes a powerful tool for supporting swift development and rapid prototyping, even for mission-critical systems. Adapting to fast growth Next, we review CockroachDB 2.0’s huge performance and scalability improvements. As a developer, fast growth can be a mixed blessing. On one hand, you’ve built something people genuinely want; on the other, this new level of demand is often accompanied by sleepless nights as the increased volume highlights the different ways your previously stable system breaks down at scale. Many cloud-native relational databases can scale to support certain types of queries by adding special servers that exclusively handle reads, while restricting all writes to a single, master node; if you ever need to scale writes, you must bring down your database while scaling up the master instance to a more powerful machine. Since its 1.0 release, CockroachDB does something much different: to scale out, developers just add more nodes to the cluster. CockroachDB nodes self-organize to balance reads and writes while simultaneously guaranteeing the highest level of data integrity in the SQL standard . With our 2.0 release, we believe CockroachDB offers the best scalability and correctness tradeoffs available in any database. In a previous post , we showed how 2.0 increased our throughput and lowered latency on TPC-C, the industry standard for testing real-world transactional throughput. Today, we’re sharing the results of our first competitive benchmark. TPC-C models a retailer with active warehouses that deliver orders, process payments, and monitor stock levels. This benchmark helps determine just how big a company’s database can get while maintaining max throughput before the system buckles (and fictional customers head to competitors). Take Amazon Aurora, for example : they maintain max throughput to 1,000 warehouses, but by the time they get to 10,000 warehouses, their reported throughput hits a wall, falling below even their 1,000 warehouse levels. CockroachDB maintains max throughput through 10,000 warehouses; that’s ten times Amazon Aurora’s highest reported limit! CockroachDB outperforms Amazon Aurora by a wide margin on an industry-standard benchmark. The TPC-C example shows how CockroachDB 2.0 helps you adapt to growth without altering your architecture. Because CockroachDB enables no-downtime scaling out to support huge increases in read and write traffic, you can start thinking about capacity planning as a just-in-time activity, rather than something that must be forecasted months or years in advance. You can read more about CockroachDB’s performance characteristics and our benchmarking methodology in our performance whitepaper to be released next week. Adapting to global user interest Finally, let’s explore CockroachDB 2.0’s expanded capabilities for working with multi-regional data. Services are often launched with a relational database deployed to a single, high-powered VM located in a public cloud provider’s regional data center. However, as the breadth of the customer base grows, maybe from a US-only audience to a global one, operators experience a growing chorus of complaints about remote customers’ long wait times, as requests must travel from the customer all the way to their data, which may be an ocean away. Lowering end-to-end response times by liberating data from being locked into a single region requires resources and expertise that are prohibitively expensive for most companies. However, with CockroachDB 2.0 Enterprise, we’ve added two significant features that make it easy for teams of all sizes to adapt to multi-regional demand. New visualization for geographically distributed clusters. First, we’ve added support for new ways to visualize globally distributed clusters. As you move data from a monolithic server to distributed servers closer to your users, new types of operational considerations emerge. One example is the role physical distance between nodes plays in determining how quickly writes become durable via our distributed consensus algorithm . Our new cluster dashboard helps you quickly answer these questions, letting you drill down from regional summaries to individual nodes, getting hints on performance bottlenecks and stability problem areas along the way. This dashboard is an easier way to resolve production problems in global clusters, keeping your multi-regional cluster running smoothly, and your customer response times fast. We’ve also added Geo-partitioning , an incredibly powerful feature that lets you control where your data lives at the individual record level. With geo-partitioning, you create policies that, for instance, pin a customer’s data in the closest datacenter to keep end-to-end latencies low. Another great use case for geo-partitioning is keeping all data associated with customers from a particular country (or countries) in data centers in that same region. This is a powerful building block that’s already helping Kindred build architectures that simplify EU GDPR compliance. Below is a simulation highlighting how multi-regional data can self-organize with Geo-partitioning. You’ll find an interactive version of this demo here . Geo-partitioning allows developers to control which records live in which regions. CockroachDB 2.0 is the only database that provides fine grained control over, and visibility into, distributed, relational data without incurring downtime or re-architecture as your business grows. It’s the only relational database designed to serve a global customer base while remaining nimble enough to adapt to shifts in data domiciling requirements as they inevitably arise. A more adaptive relational database We built on CockroachDB 1.0’s promise of providing a distributed SQL database with stability and correctness as its core value proposition and made tremendous scalability and flexibility enhancements. CockroachDB 2.0 has the performance and functionality to help your team adapt to a changing customer base at every stage of your company’s growth. CockroachDB 2.0 is an exciting leap forward in our mission to make data easy. It gives you the capabilities described above while working equally well in private, public, or hybrid cloud environments. CockroachDB’s shared-nothing, symmetric architecture makes it simple enough to deploy manually, and also a natural fit for orchestrated environments. We hope you’ll build the next great application with the time saved in architecture meetings, downtime planning, and middleware management – or maybe get just a little more sleep! -- These are just a few highlights from our 2.0 release. You can the full list of updates in the release notes . -- Illustration by Rebekka Dunlap .", "date": "2018-04-04"},
{"website": "CockroachLabs", "title": "How Cockroach Labs Explores Brand Voice Through Diverse Illustrations", "author": ["Kuan Luo"], "link": "https://www.cockroachlabs.com/blog/brand-voice-through-diverse-illustrations/", "abstract": "Last year on the blog, we shared our point of view on product design and research , two of the three main responsibilities of the Design team at Cockroach Labs. The one that we haven’t discussed yet is brand. Despite having a distinct and memorable name like Cockroach Labs, nailing our brand hasn’t been simple and is still a work-in-progress. From day one, our mission has been to make data easy. But what does a brand that makes data easy look like? That’s the question that keeps the design team up at night. Eight months ago, we attempted to answer this question in a true startup fashion: pick one aspect of the visual system that’s representative of the brand, launch experiments, and iterate until we find what sticks. Brand? Brand voice? Before we dive into the details of our experiment, let’s step back and talk about what brand is and why finding a brand voice matters. A brand is a lot more than just the logo. It’s a collection of the channels through which a company communicates with you: customer service, price, product quality, experience, advertising, press articles and more. And regardless of channels, a brand needs a voice: a consistent, purposeful way of expressing itself through words and visuals. The voice creates personality, and it makes the brand alive, and more importantly, opinionated and lovable. Creating a voice that’s just right for any brand is a daunting challenge, but it’s worthwhile because the voice is the origin of all design inspirations. Start with blog illustrations Looking at the various aspect of the visual system, such as typography, color, illustration, iconography, blog illustrations stood out as an ideal playground to explore and evolve our brand voice for a few reasons. First, the blog posts were by far the most visited pages of the site. The high readership meant we could expect a lot of exposure and feedback from the community. Second, we were using generic, stock photos (if at all) to accompany the content, so the blog posts looked extra-dense and difficult to digest. Lastly, our blog is a diverse gathering place for stories from how we use our Free Fridays to how to use Kubernetes with CockroachDB . Different illustration styles would fit naturally on the blog accompanying a wide variety of writing styles and stories. We knew our brand was simply more exciting on its face if supported by strong illustrations. Our hypothesis was that attractive illustrations would lead to increased social media click-through rate, longer engagement on the content, and positive anecdotal feedback from the community. The results Eight months since we commissioned the first editorial, we’ve worked with over 25 international illustrators on over 40 illustrations for the blog. And we’ve seen our mission “Make Data Easy” translated through diverse styles. The process was incredibly fun, and we saw some great success. Looking back at our initial hypothesis, we found that social media posts were read and shared significantly more often when they’re combined with a strong illustration, and our community and candidates have sent encouraging feedback. More importantly, we’ve learned a lot about what connects with the audience, and what works for our brand: literal execution with a clever, thoughtful and humble tone. Literal representations with a clear focus Since most of our blog posts are highly technical, successful illustrations balance the content with a literal representation of the key concepts. Take the multi-cloud deployment piece as an example. Brooklyn-based illustrator Zoë Van Dijk used dramatic lighting on the ship, cleverly named Cloud Line, moving from one cloud world to the other, representing the cloud migration in the most literal sense. Clever ways to tell a story We often tell stories of our product through public announcements, and the stories aren’t necessarily just about one thing. There are many points, angles to cover. And successful illustrations are able to paint the depths of the story. In the story of how our Beta passed the Jepsen test , we surely ran into some hurdles and struggled, and Paris-based illustrator Quentin Vijoux did a stellar job distilling the journey. ![image 2](/uploads/2018/05/5-Jepsen by Quentin Vijoux.jpg) Thoughtful use of metaphors In our culture-related posts, the illustrations can be more playful to tell stories of the people behind the product. And we’re a considerate and respectful bunch, and we want the illustrations to show that via thoughtful and bold use of color and metaphors. In the story of how we’ve updated our engineering interviews to be more fair and reflective of the real world, Toronto-based illustrator Wenting Li linked human arms as bridges to show that we want our candidates to be successful in interviews so we can make better hiring decisions. Learnings and looking forward Developing a brand voice is difficult, and what we’ve learned is that sometimes the best approach is not to double down on one concept, but to expand our horizon and be willing to try many things at once. Only through exploration, were we able to understand, adapt and grow our brand. The 40 pieces of diverse illustrations have given us a library of inspirations to point to when we discuss brand voice and design directions. What we learned with illustration has paved the way forward to tackle the other aspects of the brand puzzle: visual design, typography and color. What is an honest, humble typeface for Cockroach Labs? How can we use colors and metaphors effectively throughout the visual system? In the latest 2.0 updates, we prototyped a way of using open shapes as part of the visual identity for the website. The open shapes are metaphors for open source and friendliness, and the colors we chose are much brighter and bolder, comparing to the dark blue that’s signature to our brand. And we’re very agile in ditching dated approaches in favor of new ideas, and we’re okay with the fact that we haven’t completely nailed our brand yet, but we know we will, and that makes the journey all the more exciting. Finally, we’d like to thank all the illustrators we’ve worked with, and honor a few pieces that generated the most discussions and helped push our brand and our design efforts forward. Best Use of Metaphor by Rebekka Dunlap Best User of Non-Digital Media by Sophy Hollington Most Inspired by Burning Man by Dalbert Vilarino Least Gross Use of a Cockroach by Zach Meyer Best Use of Technical Detail by Lea Heinrich", "date": "2018-05-31"},
{"website": "CockroachLabs", "title": "DBAs at Baidu Grow the CockroachDB Global Community", "author": ["Diana Hsieh"], "link": "https://www.cockroachlabs.com/blog/baidu-dba-community/", "abstract": "The Baidu DBA team has been an active part of our community for quite some time. Starting in 2015 as open source contributors, they became internal evangelists of CockroachDB, ultimately building several production applications . Late last year, the team approached us with an exciting proposition. They wanted to take part in leading, building, and growing the CockroachDB open source community. Given the rapid growth and excitement surrounding CockroachDB in China, Baidu and Cockroach Labs jointly decided to hold a conference that would bring top-tier DBAs and developers together to discuss CockroachDB. China boasts a vibrant open source community eager to leverage the horizontal scaling capabilities and low operational complexity of CockroachDB to support rapid business growth. The CockroachDB China Community Conference included several talks delivered by guest speakers from Baidu, Alibaba, Tencent, HUAWEI, Intel, and New Oriental. Since our CEO, Spencer Kimball, was unable to attend the conference in person, we recorded a video specifically for the conference to introduce CockroachDB to the Chinese community. The Baidu DBA team then presented several talks covering the CockroachDB technical architecture, use cases, and our upcoming roadmap. Baidu also shared with the community the tools they’ve built internally to support CockroachDB in production, including MySQL protocol and syntax compatibility and a real-time synchronization tool between CockroachDB and other databases for migrating data. Other guest speakers shared their thoughts on topics ranging from optimizing database performance with hardware to building big data cloud services. You can view the entire conference delivered in Chinese here . We are excited and encouraged by how successful our first China Community Conference was and are thankful for the support and leadership from the Baidu DBA team. We hope to continue seeing our community grow both in China and across the globe as we seek to build the best SQL database in the market to make data easy. Illustration by Kati Szilágyi", "date": "2018-01-25"},
{"website": "CockroachLabs", "title": "What does GDPR compliance mean for my database?", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/gdpr-compliance-for-my-database/", "abstract": "Many inky, black pixels have been rendered over GDPR. It dramatically shifts the landscape for businesses with any EU users, so there are a lot of questions about what it means in general, as well as what it takes to actually comply with it. In this post, we’ll cover how Cockroach Labs conceives of GDPR’s major tenants (known as Data Subject Rights, which translates to “things you must do for your users”), as well as some considerations as to what it actually means for your company’s database. What are the Data Subject Rights? Well, there are a lot of them , but let’s focus on those that are most obvious and impactful. We’ve gathered them into a few major groups: Data Location & Consent: To make sure that data stays within the EU, controllers must acquire explicit consent from users to transfer user’s personal data outside the EU. Right to access: On request, controllers must provide data subjects confirmation as to whether or not they are processing any of the subject’s data, where and for what purpose. The controller must also freely provide a copy of any personal data in an electronic format. Data portability: Data subjects can transmit personal data given to them by a controller to another controller. Right to rectification: Data subjects can correct any erroneous personal data controllers store. Right to be forgotten: Data subjects can have controllers erase all of their personal data, cease distributing it, and potentially have any processors from continuing to use it. This not only applies to an explicit request from the subject, but also when the data is no longer relevant to the original purposes of processing. Privacy by design: Controllers and processors must design their services considering the “state of the art” to protect their customer’s data. Additionally, controllers and processors must notify users of any data compromises. Now, let’s break down two of the most complex data subject rights mean for your database: Data Location & Consent One simple misgiving with this regulation is easy to dispel: this right doesn’t mean you can’t send EU user data outside of the EU. Instead, it means you must let users know what data you’re sending and how it’s to be used. What this means for your business depends on the strategy you adopt to comply with GDPR. User notifications could range from minimal (e.g. you’re only temporarily reading data in the US for business analytics), to very upfront (e.g. primary copies of their data will be stored and read outside the EU). The problem with the latter statement is not a technological one, but one of sentiment. For a lot of users who are understandably leery of businesses sharing and storing their data, this puts your business at a disadvantage against competitors who are more privacy-focused. Solving the Problem with CockroachDB To improve teams’ solutions to this problem, CockroachDB Enterprise offers a geo-partitioning feature , which lets you control the physical layout of your table’s ranges (also known as shards or partitions) using row values from the table. For instance, if you had a user base that spanned the EU and the US, you could simply create two partitions based on your user’s table country column. This has two substantial benefits: You can confidently tell users that primary copies of their data are stored in the EU, which gives you a competitive advantage over those who store their users’ data outside the EU. By keeping data close to the user to which it belongs, you’re able to provide users low latency responses. Privacy by design This right has broad, sweeping implications that touch many facets of your database, but we’ll cover those that we’re aware of being most impactful: Connections to your database must be encrypted via SSL. You should limit access to your data in ways that minimizes the number of services accessing them. If possible, the your data and its backups should be stored using encryption-at-rest. Note that if you read the regulation itself refers to encryption as “pseudonymization”, but they’re the same thing. Solving the Problem with CockroachDB Designing a secure application requires focus on many facets of your application––some of which are outside the purview of your database. That being said, CockroachDB is able to contribute to your efforts. CockroachDB makes securing your cluster simple through a built-in SSL tool, which makes it simple to create certificate authority, as well as node and user certificates. You can also integrate it with another tool like OpenSSL. You can control who has access to data through CockroachDB Enterprise’s role feature , which lets you create groups of users and control their permissions with table-level granularity. And a whole lot more… There are actually so many angles to consider with GDPR, we created an entire guide around bringing your app into the EU. If this post was helpful to you, there are more in-depth strategies and tactics in the guide. Interested? Check out Scaling Your App with GDPR Compliance in Mind . Illustration by Christina Chung .", "date": "2018-07-10"},
{"website": "CockroachLabs", "title": "CockroachDB 2.0 Performance Makes Significant Strides", "author": ["Andy Woods", "Arjun Narayan", "Jordan Lewis"], "link": "https://www.cockroachlabs.com/blog/2-dot-0-perf-strides/", "abstract": "[For CockroachDB's most up-to-date performance benchmarks, please read our Performance Overview page ] Correctness, stability, and performance are the foundations of CockroachDB. We've invested tremendous resources into correctness and stability . Today, performance takes the spotlight as we will be publishing benchmarked metrics that demonstrate that you can achieve correctness, stability, and excellent performance within the same database. So how does CockroachDB define performance? On which type of queries does CockroachDB perform well? CockroachDB expressly prioritizes online transaction processing ( OLTP ) performance. We define OLTP queries as any queries you might expect a row-oriented database to support. Following our 1.1 release, we spent the past six months working on performance in all areas of CockroachDB, resulting in reduced latencies as well as improved throughput and scalability. While we are excited to share the results of this effort, we do see additional opportunities for improvement and will continue to invest in performance in future releases. A three-node, fully-replicated, and multi-active CockroachDB 2.0 cluster achieves a maximum throughput of 16,150 tpmC on a TPC-C dataset. This is a 62% improvement over our 1.1 release. Additionally, the latencies on 2.0 dropped by up to 81% compared to 1.1, using the same workload parameters. That means that our response time improved by 544%. CockroachDB 2.0 will be released in GA in the coming weeks. Read on to learn more about which performance metrics we think matter most, why you should demand benchmark TPC-C results under serializable isolation, and how CockroachDB 2.0’s performance compares to 1.1. Which performance metrics matter? For OLTP workloads, performance can be broken down into throughput (often thought of as processing speed) and latency (time to achieve something). Throughput We measure throughput as transactions per unit of time. In the case of the TPC-C results discussed below, we discuss transactions per minute (tpmC). Some benchmarks measure results as queries per second (QPS). This is a generic concept, and usually means a simple, non-transactional read or write command is sent to the server. In the real world, applications generate transactional workloads which consist of a combination of reads and writes, possibly with concurrency and likely without all data being loaded into memory. If you see benchmark results quoted in QPS, take them with a grain of salt, because anything as simple as a \"query\" is unlikely to be representative of the workload you need to run in practice. Latency Latency is traditionally measured in milliseconds (ms) and will usually be distinguished as either read or write latencies depending upon the transaction. It’s important to note that it is not sufficient to evaluate median latency . We must also understand the distribution, including tail performance, because these latencies occur frequently in production applications. This means that we must look critically at 95th percentile (p95) and 99th percentile (p99) latencies. For example, a p95 latency states that 95% of transactions offer a latency of X ms or lower. High throughput, low latency Good performance means high throughput and low latency. Keep in mind that throughput and latency are not directly correlated. For example, you can increase throughput with more clients/nodes without realizing corresponding improvements to latency. Why not using TPC-C to compare database performance is a mistake Left to their own devices, database vendors often select workloads that produce favorable numbers. For example, CockroachDB can serve 220,000 queries per second in a 95% read focused key-value workload. These numbers can be useful, especially when comparing across versions of the same database, as in the following chart which shows that CockroachDB 2.0 exhibits 37% more throughput than CockroachDB 1.1 on 12 node x 16 vcpu/node configurations. But you shouldn’t have to take our word for it. If every database vendor chose their own snowflake metrics and workloads, then making apples to apples comparisons of databases would be an exercise in futility. And because the composition of a workload has a huge impact on its performance, self-created workloads are highly susceptible to tampering. This is why the Transaction Processing Council (TPC) benchmarks were developed. TPC, as industry consortium, created a benchmark specifically for OLTP workloads called TPC-C. TPC-C is old, but it has withstood the test of time. Despite being created in 1992, it’s still the most mature and relevant industry standard measure for OLTP workloads. In its own words , TPC-C: “...involves a mix of five concurrent transactions of different types and complexity either executed on-line or queued for deferred execution. The database is comprised of nine types of tables with a wide range of record and population sizes. While the benchmark portrays the activity of a wholesale supplier, TPC-C is not limited to the activity of any particular business segment, but, rather represents any industry that must manage, sell, or distribute a product or service.” As a result, TPC-C includes create, read, update, and delete (e.g., CRUD) queries, basic joins, and other SQL statements used to administer mission critical transactional workloads. It includes detailed specifications for concurrency and workload contention. TPC-C is the only objective comparison for evaluating OLTP performance. You should demand that all database companies share TPC-C performance benchmarks before starting any project. What is good TPC-C performance? TPC-C measures the throughput and latency for processing sales through a customer warehouse using a “business throughput” metric called tpmC that measures the number of order transactions performed per minute throughout the system. In fact, tpmC is a considerably more realistic metric than TPS or QPS alone because it summarizes multiple transactions per order and accounts for failed transactions. TPC-C also has several latency requirements that apply to median, p90, and max latencies. Finally, TPC-C specifies restrictions on the maximum throughput achievable per warehouse. This is done to ensure that as a system becomes progressively more capable of throughput, it must also deal with progressively more data. This is how things work in the real world, and it makes little sense to say that your database can process a bazillion transactions per second if it’s processing the same data over and over again. Because TPC-C is constrained to a maximum amount of throughput per warehouse, we often discuss TPC-C performance as the maximum number of warehouses for which a database can maintain the maximum throughput. For a full description of the benchmark, please consult the official documentation . CRDB 2.0 vs. 1.1 TPC-C Benchmarks In 2.0, CockroachDB achieves max throughput on 1,300 warehouses on a three-node, fully-replicated multi-active cluster (16 vcpus/node) running TPC-C [^1]. This is an improvement of 62% compared to the maximum number of warehouses supported in the 1.1 release, 850 warehouses. Metric TPC-C Results CRDB 2.0 3 Node Max Throughput 16,150 CRDB 1.1 3 Node Max Throughput 9,983 Max Throughput % Increase 62% In fact, due to CockroachDB’s built-in scalability, we can add more nodes and continue to scale linearly with TPC-C to many more warehouses. More on this to come in our upcoming 2.0 release post. We compared the p50, p95, and p99 latencies for these same workloads under three nodes. In CockroachDB 1.1, our latencies increased dramatically after 850 warehouses. Similar to throughput improvements, CockroachDB also managed to maintain low latency measures through 1300 warehouses in 2.0. These results were achieved at the same isolation level (i.e., serializable) and number of nodes (i.e. 3) Again, with additional nodes, CockroachDB 2.0 can continue to maintain low latency and scale linearly with TPC-C across many more warehouses. Our 2.0 release blog post will cover this scaling in more detail. Next, we examined an apples to apples comparison of latency under the same 850 warehouse workload between CockroachDB 2.0 and CockroachDB 1.1 with a three node (16 vcpus/node) setup: Metric CRDB 1.1 CRDB 2.0 % Improvement Average Latency (p50) 201 67 67% 95% Latency (p95) 671 151 77% 99% Latency (p99) 1,140 210 82% These results were achieved at the same isolation level (i.e., serializable), number of nodes (i.e. 3), number of warehouses (i.e., 850). For the same load, CockroachDB 2.0 reduces latency by as much as 82% when compared to CockroachDB 1.1. Viewed another way, CockroachDB 2.0 improves response time by 544% (CockroachDB 1.1 p99/CockroachDB 2.0 p99) when compared to 1.1. Isolation Levels Most databases present a choice of several transaction isolation levels, offering a tradeoff between correctness and performance. CockroachDB provides strong (“ SERIALIZABLE ”) isolation by default to ensure that your application always sees the data it expects. In an earlier blog post, Real Transactions are Serializable , we explained recent research at Stanford that explored the degree to which weak isolation leads to real-world bugs in 12 eCommerce applications. In five of the twelve tested applications, low isolation levels left companies vulnerable to serious financial loss. Many competitors will have you believe that serializable isolation isn’t feasible in a production application. In fact, Oracle’s documentation states: “While [serializable] isolation between transactions is generally desirable, running many applications in this mode can seriously compromise application throughput. Complete isolation of concurrently running transactions could mean that one transaction cannot perform an insert into a table being queried by another transaction. In short, real-world considerations usually require a compromise between perfect transaction isolation and performance.” While some databases offer serializable isolation (or a weaker approximation of it, like Oracle and SAP Hana ), they rarely publish benchmarks with that level of safety because their performance degrades significantly. CockroachDB's serializable isolation provides uncompromising safety without impacting performance. In CockroachDB 2.0, improvements in our serializable isolation implementation make it feasible to use for all transactions. We’ve deprecated snapshot isolation because it provides little or no performance benefit and comes with significant safety concerns. Note that while it is possible to engineer an application to be safe at weaker isolation levels, doing so is onerous for application developers and creates a maintenance time bomb. CockroachDB benchmarks TPC-C using the highest level of isolation recognized under the SQL standard. How to Survive Failure You might have noticed that we reported the above numbers under a multi-node (e.g., 3 node) deployment. We did this to demonstrate that CRDB can be highly performant while offering superior survivability for a single data center, single region, or full global deployment. To ensure that this survivability does not come at the cost of performance, CRDB employs a number of features like “ follow-the-workload ” to minimize distribution’s effect on throughput and latency. Because CockroachDB is built to be inherently survivable and always correct, we report all TPC-C results with full consensus replication and serializable isolation. That's not something we've seen from any other database vendor on the market. What’s next for CockroachDB We stand behind all of the numbers in this document and plan to publish a more detailed look at performance including a step by step guide to reproduce our findings in a forthcoming performance report. CRDB employs an excellent combination of correctness, survivability, and performance through our use of serializable isolation, distributed database, and evidenced by our TPC-C throughput and latency. No longer are you forced to trade correctness and survivability to meet your customers’ performance demands. Why not take this opportunity to test CockroachDB 2.0 (still in beta) for yourself? P.S. Stay tuned for our upcoming 2.0 release announcement in which we will share our TPC-C 10,000 warehouse performance benchmarked against a leading cloud competitor! [^1]: Note: We have not filed for official certification of our TPC-C results. However, we will post full reproduction steps in a forthcoming whitepaper. Illustration by Dalbert B. Vilarino", "date": "2018-03-29"},
{"website": "CockroachLabs", "title": "Practical Applications of JSON: Why and Where You Should Use It", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/why-where-json/", "abstract": "CockroachDB provides scale without sacrificing SQL functionality. It offers fully-distributed ACID transactions, zero-downtime schema changes, and support for secondary indexes and foreign keys. But what about some of the additional benefits NoSQL databases provide such as the ability to use semi-structured data? CockroachDB listened to our customers and realized that we needed to provide options for managing rapid development, data without a clear schema or whose schema you do not control, and impedance mismatches. That’s why we are excited to support JSON in 2.0! Previously we’ve shown you how we implemented JSON and inverted indexes as well as a few examples. This blog post will explain when you might want to leverage these features. Why use CockroachDB +JSONB? Rapid Development Most successful new products (or new companies) use rapid prototyping to quickly adjust to frequently changing customer requirements. JSONB makes it easy to tweak your data model and prototype new features to iterate your way to customer success. As your product grows, you’ll need to strike a delicate balance between the need for data model updates and the need to minimize downtime. CockroachDB helps shift that balance back towards shipping product by adding rich support for JSON. Imagine you launch a new blog platform that includes a table in which we use JSONB to store metadata about a user's blog post: CREATE TABLE posts (\n  user_id UUID REFERENCES users,\n  title STRING NOT NULL,\n  content STRING NOT NULL,\n  metadata JSONB\n); Within the metadata field, we might store something like: {\n  \"est-reading-time\": \"15m\",\n  \"topics\": [\"consensus algorithms\", \"research\"]\n} We continue to add metadata fields that represent our best hypothesis about the kinds of informations our users will want to store. After our initial launch, we checked in with a few of our users and learned that we have a large number of travel bloggers. These bloggers would like to store the location they wrote the blog post from. Since we are using JSON, we do not need to complete a schema change. Instead, we can insert new data fields: {\n  \"est-reading-time\": \"15m\",\n  \"topics\": [\"consensus algorithms\", \"research\"],\n  \"writing-location\": \"Manhattan\"\n} We are now able to quickly modify our application as we learn more about our users. Data without a clear schema or whose schema you do not control Many applications cannot be modeled ahead of time (e.g., game elements, inventory items). These models are frequently stored as opaque bytes that can be difficult to update. Without JSONB, any data model changes would result in the need for individual data to be updated, extracted, modified, and stored back within a transaction. Imagine a table events which stores system-wide events, with the caveat that not every event has the same metadata associated with it. For example, if a user likes a post, we have to store different information than if, say, some group appoints a new administrator. Having JSONB columns available lets us model this scenario without resorting to a complex table structure with many foreign keys or redundant columns. CREATE TABLE events (\n  id UUID DEFAULT uuid_v4()::UUID,\n  ts TIMESTAMP DEFAULT now(),\n  data JSONB,\nINDEX (ts)\n); Now inserting data is as simple as: INSERT INTO events (data) VALUES\n  ('{\"event_type\": \"liked_post\",\n     \"liking_user\": \"Carl\",\n     \"liked_user\": \"Princess\",\n     \"post_contents\": \"i love dog treats\"}'),\n  ('{\"event_type\": \"new_admin\",\n     \"admin_user\": \"Lola\",\n     \"group_name\": \"Cockroach Dogs\"}'); We do not need to do any schema changes to record new event types. Instead, we can insert the new data from our application and everything will automatically be stored correctly. This allows us to run interesting queries such as, for example, the most recent 10 \"liked post\" events: SELECT * FROM EVENTS WHERE data->'event_type' = '\"liked_post\"'::JSONB ORDER BY ts DESC LIMIT 10; Without inverted indexes, the above query would be very slow. For more information on how inverted indices work please consult Be Flexible & Consistent: JSON Comes to CockroachDB . Impedance mismatch Many developers don’t like modeling the world in a fully normalized way and want databases that can more intuitively map to the object-oriented programming languages powering their applications. Both startups and enterprises need a method for storing increasingly large amounts of unstructured data such as storing sensor data. Or say I want don't want complex data at all - I just want a plain old document store with no restrictions at all on what I put in it, but I still want all the transactional guarantees provided by CockroachDB. Let's say that we require all of our documents to have an id field. We can make a simple document store like this: CREATE TABLE documents (\n  id STRING AS data->'id' STORED PRIMARY KEY,\n  data JSONB\n); Now, we can insert JSON documents and they will be appropriately indexed: INSERT INTO documents (data) VALUES\n\t  ('{\"id\": \"fox\", \"name\": \"Fox\", \"tier\": 1}'),\n\t  ('{\"id\": \"falco\", \"name\": \"Falco\", \"tier\": 2}'),\n\t  ('{\"id\": \"marth\", \"name\": \"Marth\", \"tier\": 3}'); Since we added an inverted index on the data column, CockroachDB will add an index entry for every path through the JSON tree of every inserted value. This makes it efficient to look up containment relationships in JSON data, because it means that you can do a single point lookup into the inverted index to find out whether there's a match. Why not use a NoSQL database? NoSQL databases have garnered a large amount of public support and press about their easy to setup capabilities. Things like the “MEAN stack” became not just a cleverly marketed name but almost a requirement to prove your developer credentials. Given the press around these databases, you might be asking yourself, why don’t I just use a NoSQL database? Aren’t they designed to take advantage of semi-structured data? Whether you are an early-stage startup or a fully vetted enterprise, you might be tempted to use NoSQL for their promised scale and simplicity. However this could cost your business money. Most NoSQL databases fail to offer performant, general ACID transactions (e.g. Serializable isolation). We believe that businesses have not placed enough weight upon correctness when making these decisions. In Real Transactions are Serializable we explained recent research at Stanford that explored the degree to which weak isolation leads to real-world bugs in 12 eCommerce applications. In five of the tested applications, adding an item to your cart while checking out in another browser tab could result in the item being added to the order for free. NoSQL databases cannot even maintain weak isolation levels as they often fail to administer transactions correctly, a simpler construct than isolation levels. Weak isolation doesn’t just leave your application vulnerable to attack, it might also prevent you from answering fundamental questions about your business such as what is my revenue? Luckily for you, this doesn’t have to be true for your business. CockroachDB provides you with easy to use JSON with the same ACID transactions and serializable isolation levels you have come to expect from a world-class SQL database. Now you can have your cake and eat it too. Final thoughts CockroachDB provides the flexibility of JSON with strict guarantees of ACID transactions and serializable isolation. We are excited to provide you with the tools to make your company successful no matter what your combination of structured or semi-structured data turns out to be. Illustration by Wenting Li", "date": "2018-06-21"},
{"website": "CockroachLabs", "title": "Updating Your Engineering Interview to Make Better Hiring Decisions", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/updating-eng-interview/", "abstract": "For many engineers, thinking about spending countless hours of studying for coding interviews leaves them exhausted before they even get started. At Cockroach Labs, we have open sourced the interview process on Github to create familiarity for candidates and removed resumes and utilized exercise-based interviews to reduce bias. Beginning in mid-2017, we engaged with our engineering team on how we can make our engineering interview process less intimidating and more fair, resulting in better hiring decisions. Traditional Engineering Interviews Traditional engineering interviews focus heavily on real-time coding questions. At Cockroach Labs, our past interview process included five coding questions and one system design question for each engineering candidate. Our interviewers assessed the candidate’s skill with coding and debugging, algorithms, data structures, and overall code structure and design. These coding interviews were done in a “live coding” format either over Google Hangouts or in-person using CoderPad or the candidate’s preferred editor/IDE. Each interviewer asked their own coding question, and a handful of our engineers were prepared to ask a systems design question. While all of the coding questions focused on technical concepts that our engineers work on within their roles, only the engineer owning that question was versed on the rubric for the assessment. For our one design interview, the assessments that resulted in ‘strong yes’ or ‘strong no’ led to the most confidence in the signal to hire or not. Our interviewer assessments that were on the fence were often disregarded because we could not find meaning in the signal on how to proceed. When this occurred, the only assessments that were meaningful were the interviews that focused on coding and debugging, algorithms, and data structures. To make better hiring decisions, we wanted to make the engineering interview process more reflective of real-world engineering, focusing on assessments past coding and debugging, algorithms, and data structures and creating stronger hiring signals for a candidate’s experience with large-scale system design and design decisions. Where to Start Iterating on an interview process that our engineers have been using throughout their careers was extremely challenging. We anticipated that we would be able to roll out a new process in 1 quarter. However, the reality of structuring a new process and running betas to test the quality of assessments from interviews is a massive time investment, which we have worked on for the past 3 quarters. Before we got started on changing the process, we first had to define our goals for our effort of iterating the engineering interview process. For Cockroach Labs: Increase fairness of interviews by creating familiarity for the candidates. Get stronger signals from our interviews, and thus make better hiring decisions. For Candidates: Build a more fair interview process by offering familiarity and interviews that reflect their day-to-day roles. Make sure that all of our questions apply to problems we solve in our daily work. For Interviewers: Create consistency and better calibration by using a small repository of questions that every engineer can ask, solve, and grade. Create stronger signals from our interviews, resulting in more confident hiring decisions. After discussion, we determined that we wanted to make a few changes to the interview process, specifically around coding and our design portion of the interview. To address both these areas, we added the following: Take-home Exercise Reduced Question Repository Choose Your Own Design Take-home Exercise We decided that we wanted to let candidates get started at their own pace, in an environment that they choose, which mirrors more closely to what they do in their day-to-day work. Engineers can do the exercise whenever works for their schedule. While we let candidates know that the exercise usually takes one to two hours to complete, we ask candidates to let us know if that’s not the case for them so that we can make sure our expectations are reasonable. We have simplified the process by asking the candidate to use a unique CoderPad link that contains the coding question, instructions, tips, and how to start test cases. Once the candidate has completed the question, they submit the link via our applicant tracking system for a member of our engineering team to review. The intent behind the take-home exercise is to make it as close to an on-the-job simulation as possible to create a familiar environment in which engineers can do their best work. After reviewing a take-home exercise, assuming the submitted solution looks good to the reviewer, we schedule a follow-up with the candidate where they work with the interviewer, usually over a Google Hangout and CoderPad session, to extend their submission with additional or changed functionality. This we think is more representative of our day to day work -- where we modify existing code to add features or handle changed requirements, rather than starting with an blank slate every day. Additionally, lets the candidate demonstrate their ability while working with they wrote, which hopefully gives them a familiar and more comfortable starting point jumping into an interactive interview that could otherwise be more intimidating. Reduced Question Repository We want to see how candidates apply their skills to technical challenges that our engineers solve in their daily work and give candidates the opportunity to see what type of work they would be doing at Cockroach Labs. Previously, we had our interviewers ask coding questions that they defined and that only they understood the rubric for the assessment. These questions varied, with some being more or less challenging with regards to different skill areas. For example, some questions pushed on data structures, while other pushed on algorithms. The areas that each interviewer assessed was left up to who the recruiter decided to put on the interview slate. By reducing the number of coding questions, we can ensure that every candidate sees similar questions, regardless of who is interviewing them. Also, every engineer on our team can ask, solve, and grade both take home challenges and onsite interviews. We believe that our interviewers should use the same, objective standards to assess a candidate’s skill set. This improves consistency in the questions and grading and increases the overall fairness of our interview process. A New Systems Design Question One area in which we knew we were lacking strong hiring signals was in our systems design assessments. In addition to an interview focused on large-scale system design, we created a “Choose Your Own” Systems Design Question. This question is a discussion between the candidate and interviewer about a system that the candidate has worked on in the past in any capacity. The recruiter prepares the candidate for this portion of the interview before coming onsite. By speaking about something the candidate already knows in their first interview, they build some confidence moving into the rest of the interviews. It also allows for the candidate to share their subject matter expertise. While we are still very early on with this format, we have seen a notably greater confidence in the hiring team’s assessment of a candidate’s design skills since we now we have two systems design interviews - one focused on the candidate’s experience and one related to large-scale systems. Continuing to Iterate Ultimately, we made these updates to our engineering interview process to create a candidate experience that is less intimidating and fairer. We also felt that the changes with the new interview structure and reduced question repository will lead to more confidence in our hiring assessments. We are still in initial stages. The engineering team is going through ongoing training with the question repository. Once we have a significant enough data set from candidate’s going through the hiring process, we will look at the data to determine if the goals that we set are being accomplished. From there, we will use the data and feedback from both our team and candidates to continue to iterate. The engineer hiring process is one that continues to be iterated across technology companies. We encourage more companies to share their stories so we can all learn from each other. Illustration by Wenting Li", "date": "2018-05-17"},
{"website": "CockroachLabs", "title": "Happy GDPR Day", "author": ["Swati Kumar"], "link": "https://www.cockroachlabs.com/blog/gdpr-day/", "abstract": "May 25, 2018 has loomed over businesses for two years as the day the General Data Protection Regulation ( GDPR ) takes effect––and that day has finally arrived. If the number of \"Updates to Our Privacy Policy\" emails I've received in the last week are any indication, people are still hard at work trying to make GDPR compliance happen. However, unlike the cleanly cut deadline of filing your taxes, meeting GDPR's regulations isn't something most companies will complete today. According to Crowd Search Partners' GDPR report , as many as 60% of companies, as of today, fail to comply. What that means is that if you're not already there, there's still time. In that spirit, Cockroach Labs is planning to release more GDPR-related content in the coming weeks. We've already talked about what globe-spanning data looks like , but we'll also have information about what it means for databases to comply with GDPR (and why it's a hard thing for most to do), along with practical guidance for making GDPR compliance a reality for your apps. In the Meantime... Because it's not too late to start working toward a better GDPR solution, we think it's worth mentioning that CockroachDB can help teams achieve their compliance goals with our geo-partitioning feature , which lets you control the physical layout of your table data. To get a sense of how, check out this small segment from Kindred 's architect Kai Niemi (who oversees one of the world's largest online gaming and gambling platforms with millions of players in the EU and Australia): If you're interested in learning more, check out our documentation on geo-partitioning and play around with the feature by starting a 30-day trial of CockroachDB Enterprise .", "date": "2018-05-25"},
{"website": "CockroachLabs", "title": "Flex Fridays: Sean Loiselle Works Toward a Graduate Degree in Computer Science", "author": ["Chelsea Lee", "Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/free-fridays-sean-loiselle/", "abstract": "Over the next few months, we plan to share with you what some of our Roachers are working on for their Flex Friday projects . Our Fridays are given to employees as a day to self manage, where they can decide to come into the office or WFH and to focus on work priorities or a side project. We hope this helps explain the bigger picture of one of our company values: Establish Balance. -Chelsea Lee, Culture and Office Experience Manager Sean Loiselle, Technical Content Marketing Manager What is your role at Cockroach Labs? At Cockroach Labs, I’m the Technical Content Marketing Manager, which is a fancy way of saying I write and manage technical blog posts and other miscellaneous hypertext deliverables. I started at Cockroach Labs as a Senior Technical Writer and that’s where my background lies. What do you do day to day at Cockroach Labs? I spend a lot of time researching the content for our current or upcoming campaigns. This means reading a lot of other company’s documentation, watching talks on YouTube, and figuring out how to integrate Cockroach with other technologies. Then there’s actually writing, editing, and publishing the content which takes an incredible amount of time––not only do I have to keep the internals of CockroachDB in mind, but also how it interfaces with the rest of the world. I also tinker around on creating scripts for doing marketing analysis, though it’s been a side-side project and I’d like to spend more time on that in the future. What does Flex Friday mean to you? Flex Friday is my chance to grow. Cockroach Labs really prizes the fact that it lets employees determine how best to use their time on Fridays, and I try really hard to push myself to understand things that were totally foreign to me before. What is your current Flex Friday project? I’ve been building out a way to simplify how CockroachDB users can access the database’s introspective metrics––things like storage space, currently running goroutines, and aborted transactions––by building a catalog of charts. The nice thing about the way I’ve done this is that developers can easily add new charts to this catalog with a few lines of code. So, not only do I want it to be easy for users to consume these metrics, but I also want to make it easy for our team to be able to expand the number of them we track and make them useful. Up next, I’m working on building more autonomous ways to detect cases where the metrics are out-of-whack and determine when that means that there’s something awry in a CockroachDB cluster. What else have you done on a Flex Friday? I built a classroom/learning management system in Node.js for an education startup. I got pretty far along with the project attempting to dogfood CockroachDB, but I ended up moving it over to PostgreSQL because I needed some data manipulation functions we didn’t support at the time. Describe a typical Flex Friday. I sleep in until 10am, and then start in on class work (I’m enrolled in a grad program in CS which was paid for by Learning is Good, a program that gives employees an annual stipend to support the enhancement of knowledge and skills). Once that’s done––typically around noon or 1pm––I pick up where I left off the week before on one of my other engineering projects. I’ll work on this until 5pm or 6pm, and then grab dinner out in the city. How does Flex Friday support your long term goals? I would like to move into software development and Flex Friday has been a way forward with that. At a higher level, Flex Fridays creates space to engage in the work I find most meaningful and exciting. There’s something really positive and reaffirming in the fact that the only person I’m truly accountable to for how I spend this time is myself. You have to reckon with what you really want to do, and Flex Friday has been a great catalyst for that. If I wasn’t willing to invest time in it, even though I am literally being paid to do it, is it really that important to me? Please share any other topics that may be related to Flex Friday or that you want to share. Flex Friday is the benefit I care about most at Cockroach. If keeping it meant that I had to give back 20% of my pay, I would in a heartbeat. Illustration by Quentin Vijoux", "date": "2018-07-20"},
{"website": "CockroachLabs", "title": "Amazon Aurora Costs 10x More than CockroachDB for OLTP Workloads", "author": ["Sean Loiselle", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/aurora-price-vs-cockroachdb/", "abstract": "[For CockroachDB's most up-to-date performance benchmarks, please read our Performance Overview page ] Back in April of 2018, we ran extensive tests comparing CockroachDB to Amazon Aurora in terms of TPC-C throughput—the industry standard for testing real-world transactional throughput (you can check out those findings here ). No matter how crucial a component’s performance is, though, teams must also consider the price of those numbers––an app that consumes more financial resources than it generates is difficult to maintain. To better understand the financial impact when choosing between software, we decided to also determine the price of running TPC-C against both CockroachDB and Aurora. As the headline says, we found it costs between 5-10 times more to choose Aurora over CockroachDB. A Quick Refresher on TPC-C We have much more detail about TPC-C in our performance guide if you’re interested, but here’s a rundown of what you need to know to understand the rest of this post. TPC-C is the industry standard for establishing a consistent point of comparison among different databases’ performance in an online transaction processing (OLTP) workload. The benchmark heavily relies on a robust set of SQL features such as foreign keys, multiple indexes, transaction rollbacks, and joins. Because of its volume, TPC-C also stresses the underlying storage capabilities of the database, which is crucial to demonstrating how it performs under real load. Data Layout TPC-C models nine tables that one might create to manage inventory for an online store, tracking elements such as SKUs, inventory, customers, and orders. TPC-C also relies heavily on the notion of a “warehouse,” which is meant to simulate a distinct physical warehouse where the retailer would store goods. Warehouses, though, are not explicitly defined in the TPC-C schema. They are instead represented through a column in the stock table. Scaling To scale the benchmark, TPC-C lets you create more warehouses, each with 80MB of data (e.g. 100 warehouses would be 8GB). The benchmark then also linearly scales all other variables with the number of warehouses. An example is 12.8 transactions per minute per warehouse. In the example of 100 warehouses, you cannot exceed 1280 transactions/min. Comparing Price While benchmarks primarily focus on reporting throughput, it’s also crucial to understand the financial assets that are required to achieve those numbers. If it’s only possible to reproduce someone’s benchmark results on a fleet of 32-core machines, that’s important to know because it impacts a project’s feasibility. Further, because TPC-C places an upper bound on the number of transactions any service can complete on a given number of warehouses, a cost-based metric lets you meaningfully differentiate between software with similar TPC-C throughput. To actually compare the price, then, simply divide the total cost of the test by the benchmark’s tpmC number, giving you a “Dollars per tpmC” metric. CockroachDB vs. Amazon Aurora RDS In our blog post detailing the results of CockroachDB vs. Aurora’s TPC-C throughput on 1,000 TPC-C warehouses, the two databases achieved similar results. Given that, we thought it would be interesting to compare the price of the two services. To make the evaluation mirror reality as closely as possible, we chose to price both services using three-year contracts with their respective partners. With this length of commitment, infrastructure providers offer aggressive discounts which, in our experience, are attractive enough that many users are eager take them. Database CockroachDB 2.0 Amazon Aurora RDS MySQL Throughput 12,475 tpmC 12,582 tpmC 3-Year Cost $49,670 $260,670 – $540,6051 * $/tpmC $3.98 $20.72 – $42.97 * Hardware 3x Google Compute Engine n1- highcpu-16 with attached Local SSDs 2x r3.8XL (One read replica with automatic failover) * Aurora pricing is shown as a range because Amazon does not reveal the number of I/O per second (IOPS) Aurora requires to achieve their TPC-C results, so we made two estimates. We set an upper-bound at 30k IOPS, which is the amount pre-provisioned for MySQL on a similar workload. Knowing that Aurora can be more efficient than MySQL, we then estimated a lower-bound of 10k IOPS. Comparison CockroachDB matches over 99% of Aurora’s throughput, and has only 10 to 20% of the price. We think it’s safe to say that on the TPC-C 1k warehouse benchmark, CockroachDB is a much more attractive option. CockroachDB vs. Aurora tpmC CockroachDB vs. Aurora Dollars per tpmC Details & Differentiators Because Aurora is a managed service and CockroachDB is open source software, our total costs account for a DBA with a $200k/yr salary to spend 7 hours per month managing a CockroachDB cluster. CockroachDB was tested on Google Cloud because we found their hardware to be more price efficient. As mentioned above, we had to estimate that Aurora achieved their tpmC number using somewhere between 10k-30k IOPS. More Detail? If you want more detail about these numbers, as well as how to reproduce CockroachDB’s TPC-C throughput, download our guide, CockroachDB vs. Amazon Aurora Cost Comparison . If you’re interested in how much CockroachDB would cost for your team’s specific scenario, get in touch with us . Illustration by Zoë van Dijk", "date": "2018-07-25"},
{"website": "CockroachLabs", "title": "Flex Fridays: Jordan Lewis Builds His Own Experiments into CockroachDB", "author": ["Chelsea Lee", "Jordan Lewis"], "link": "https://www.cockroachlabs.com/blog/free-fridays-jordan-lewis/", "abstract": "Over the next few months, we plan to share with you what some of our Roachers are working on for their Flex Friday projects . Our Fridays are given to employees as a day to self manage, where they can decide to come into the office or WFH and to focus on work priorities or a side project. We hope this helps explain the bigger picture of one of our company values: Establish Balance. -Chelsea Lee, Culture and Office Experience Manager Jordan Lewis, Engineering Manager What is your role at Cockroach Labs? I’m an engineering manager. I manage the SQL Execution team, which is concerned with the implementation of the engine that runs a SQL query plan by talking to CockroachDB’s underlying key value store API. I also find time to be an engineer, mostly working on improving our SQL execution performance and semantics. What do you do day to day at Cockroach Labs? My time is split about 50-50 between being a manager and being an engineer. That means I balance programming, doing code reviews, and communicating about technical topics with having one-on-ones with my team members, working on project planning, creating roadmaps with product managers, and trying my best to make sure my team is unblocked and productive. I usually get in sometime between 9am and 10am, and leave around 6pm. I like the flexibility that Cockroach Labs provides for working from home, and I take advantage of that from time to time, but I feel that I perform better in my role if I’m in the office most days. What does Flex Friday mean to you? Flex Friday is a great way to concentrate on working on engineering experiments or ideas that I’ve always wanted to add to CockroachDB, but don’t quite fit into the roadmap. It’s a time to stretch my wings and work on features or parts of the product that I don’t normally get to, because of time constraints or product priorities. Finally, it’s a time to focus: there are no meetings, and the office is generally very quiet and conducive to engineering productivity. What is your current Flex Friday project? Last Friday, I worked on an experiment to add query memory monitoring to SHOW SESSIONS, which is a SQL command that’s used to show the currently active connections to the database. Several users have been confused about which queries are using their memory up before, and we already have the infrastructure to track query memory, so I figured it might be an interesting idea to hook the two concepts up. I showed the idea to the team, who thought it was a good direction with a couple of changes. The work to finish this up is now represented in our upcoming milestone because it aligns well with some other initiatives to understand query performance. This coming Friday, I’m planning to work on an experiment to add per-query-fingerprint latency graphs to our Web UI’s time series data. This will likely be difficult to productionize, as it would take up a lot of space on disk to store latencies for all query fingerprints in an active database, but it’s an interesting idea that a few people have been talking about so I’ll make a quick prototype to see whether or not it’s useful. What else have you done on a Flex Friday? Mostly engineering experiments or random tasks that I felt like picking off. Once, I wrote a command-line code review tool that a few people use at the company called re: https://github.com/jordanlewis/re . I’ve worked on an experiment for a columnar execution engine for SQL queries, added some missing SQL builtin functions for Postgres compatibility, worked on performance issues, and fixed bugs. Recently I started work on a component diagram of CockroachDB that uses D3 to render itself, but I haven’t gotten very far with that yet. Describe a typical Flex Friday. It’s very common that I’ll think of an idea during the week that might be a good one, but requires confirmation to move forward in that direction. Flex Fridays are a perfect time to validate ideas like that. Sometimes these ideas end up being small, quick wins that I can knock out on Friday and get merged the next week, and sometimes they end up being more complex than what I can handle in a day. In that case, I’ll show them to the team and see what they think we should do with the ideas. I’m one of the rare few that actually likes to come into the office on Flex Friday. I probably come in about 80% of the time. I enjoy the quiet and lack of meetings, and can use the time to really focus on engineering - something that I don’t usually get to do in large chunks of contiguous time during the week, due to my role as a manager. How does Flex Friday support your long term goals? Flex Friday lets me keep some time reserved to focus on improving my engineering skills, learning about databases, and working on neglected corners of the product. Illustration by Quentin Vijoux", "date": "2018-07-27"},
{"website": "CockroachLabs", "title": "Flex Fridays: Rebecca Taft Builds a Neural Network Query Optimizer", "author": ["Chelsea Lee", "Rebecca Taft"], "link": "https://www.cockroachlabs.com/blog/free-fridays-rebecca-taft/", "abstract": "From the beginning, Flex Fridays have been a part of the Cockroach Labs culture. It’s a day where Roachers can make their own schedules: Should I come into the office or WFH? Should I send out these PRs or focus on my side project? Or should I just close my laptop and go on a weekend getaway? Quite frankly, it’s up to them. Now, three and half years later, Flex Fridays are still going strong and are a benefit that we can brag about. Over the next few weeks, we’ll be sharing how some of our employees spend their Flex Fridays. I’m excited for you to see what my colleagues are working on in and out of the office. While every Flex Friday can be different, I hope this highlights the range of ways our Roachers spend this time and the value that it has within our company culture. -Chelsea Lee, Culture and Office Experience Manager Rebecca Taft Software Engineer What is your role at Cockroach Labs? I am a software engineer and am currently a member of the SQL Query Optimizer team, which is building a state-of-the-art cost-based query optimizer. Since the query optimizer is a new feature that has not yet released, we have the luxury of designing and building it from the ground up without worrying about production issues or backwards compatibility. In order to ensure the optimizer will have correct behavior and improve CockroachDB’s performance, we are constantly learning about the subtleties of SQL scoping and semantics, reading papers about logically equivalent query transformations, and researching how to use statistics to correctly estimate the cost of query execution. As a member of the larger engineering team, I am also part of the on-call customer support rotation, so around every six months, I spend a week triaging production issues and answering questions from the community. All engineers, including myself, are also responsible for occasionally conducting technical interviews in order to share the load of Cockroach Labs’ hiring process. What do you do day to day at Cockroach Labs? Most of my time is spent writing code and reviewing the code written by other engineers on the SQL Query Optimizer team. I have very few meetings each week; I usually spend no more than two to three hours per week in meetings. When I’m not working on the query optimizer or in a meeting, I may be catching up on emails or slack messages, or taking advantage of the many opportunities for learning and development at Cockroach Labs such as lunch & learns and career development workshops. What does Flex Friday mean to you? As a recent PhD graduate, I often use Flex Fridays to maintain my connection to the academic world by reviewing papers, going to talks, or working on my own papers. Flex Friday is also a day to catch up on the items on my to-do list that I didn’t have time to do earlier in the week. It’s a day to sleep in, meet friends for lunch, and catch up on personal errands such as visits to the dentist or laundry. What is your current Flex Friday project? My partner, Raul, is a postdoc at MIT and also studies database management systems (we met when I was at MIT for grad school). We are working on a project together to build a query optimizer based on neural networks, or “deep learning”. Since traditional query optimizers (including the one we are building at Cockroach Labs) rely on many heuristics as well as statistics that are often stale and out of date, it is possible that a neural network could do a better job of choosing a query plan. It is too early to tell if our idea will work, but if it does, it would be a game-changing addition to Cockroach Labs’ query optimizer, and a major boost for Raul’s academic career. What else have you done on a Flex Friday? Although I would like to spend most Fridays working on the neural network query optimizer, in practice, other tasks tend to take precedence. I am a member of the program committee for the three major academic database conferences this year (VLDB, SIGMOD, and ICDE), so I often need to spend my Flex Fridays reviewing papers. Reviewing a single paper usually takes at least 3-4 hours (sometimes 6-8 if I’m not familiar with the topic), so often that is all I have time for. I also spent several Flex Fridays last fall working to get my own paper published with the last bit of work from my PhD thesis. Finally, I often spend at least some time on Fridays doing personal errands, answering personal emails, or doing my normal day-to-day work of writing and reviewing code for the (non-neural-network) query optimizer. Describe a typical Flex Friday. On Fridays I typically sleep in until 8 or 9, and have a leisurely breakfast at home. I might start a load of laundry and then answer emails and catch up on news for an hour-or-so. By 10 I’m ready to start working, but my task for the day is different almost every Friday. This Friday, I am working on the blog entry you are now reading, but on any typical Friday I could be reviewing a paper or doing any of the other academic, personal, and professional projects mentioned above. Sometimes I meet a friend for lunch or coffee in the afternoon, and return home to work for a few hours until around 6 pm. How does Flex Friday support your long term goals? Although I have chosen a career in industry rather than academia, I hope that I can continue to stay connected to the academic community. By using Flex Fridays to review papers, I am staying informed about the latest ideas in the research community and helping ensure that papers published in the major database conferences are relevant to industry. As time goes on, I hope to spend more Flex Fridays pursuing my own research as well, perhaps in collaboration with other members of the engineering team. I’d eventually like to help the team write a paper about the existing Cockroach database, since I think there are many aspects of the system that the research community would find interesting. Publishing in academic conferences such as VLDB and SIGMOD will give us additional credibility in the research community and help attract top talent to the company. Illustration by Quentin Vijoux", "date": "2018-07-13"},
{"website": "CockroachLabs", "title": "Flex Fridays: Swati Kumar Consults for Nonprofits", "author": ["Chelsea Lee", "Swati Kumar"], "link": "https://www.cockroachlabs.com/blog/free-fridays-swati-kumar/", "abstract": "Over the next few months, we plan to share with you what some of our Roachers are working on for their Flex Friday projects . Our Fridays are given to employees as a day to self manage, where they can decide to come into the office or WFH and to focus on work priorities or a side project. We hope this helps explain the bigger picture of one of our company values: Establish Balance. -Chelsea Lee, Culture and Office Experience Manager Swati Kumar, Marketing Manager What is your role at Cockroach Labs? I work as a Marketing Manager at Cockroach Labs. Working on a small team, my role spans a wide variety of tasks and requires that I continuously learn new ways to get things done quickly, effectively, and often creatively. What do you do day to day at Cockroach Labs? My work ranges from sourcing and guiding content for the company blog to marketing campaign strategy to email marketing, website copywriting, and more. On a day to day basis, I spend the morning a home addressing emails and setting a priority list for the day. I then come in for meetings scattered throughout the day and work on blog post edits and campaign management. Any writing I need to do generally happens after I go home for the day between 6pm and 7pm to avoid distractions. What does Flex Friday mean to you? For me, Flex Fridays are a way to use my skills in marketing in a new context. Generally, this manifests as consulting projects for nonprofits, individuals, and small businesses who can’t afford a professional marketing consultant. Many small ventures fail simply due to a lack of understanding when it comes to scaling barriers to market. I try to make this avoidable issue irrelevant to a particular organization's success. What is your current Flex Friday project? I hold marketing office hours on Fridays (and Sundays) for nonprofits and small business owners. I also use some of this time to work on brainstorming for work projects. This is particularly difficult to do amidst meetings during the work week. What else have you done on a Flex Friday? I’ve worked on larger consulting projects in an advisory role for college students and friends starting their own business, worked on my personal brand, and worked on side projects like my own fiction writing. How does Flex Friday support your long term goals? Flex Fridays allows me to develop my skills in a different context from my day-to-day life. This ensures that my expertise remains flexible enough to be applied to multiple scenarios rather than becoming fixed into a certain formula that works just for specific types of companies and organizations. Illustration by Quentin Vijoux", "date": "2018-08-10"},
{"website": "CockroachLabs", "title": "High Availability Without Giving Up Consistency", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/high-availability-with-consistency/", "abstract": "If you’re reading this, you’re surely familiar with the arguments for high availability: services are only useful when they’re online. Unavailable services not only lose money, but also deteriorate your credibility in customers’ eyes. This could lead to immeasurable costs to your company in the future. Given that CockroachDB got its name because of its ability to survive failures, we thought we would cover some architectural considerations when building high availability services on top of Cockroach. Why Choose CockroachDB at All Apps and web services have become deeply intertwined in our lives, so it’s natural that our expectations of them have dramatically increased. First and foremost, we always want them to be on––always. But that’s basically just become a requirement. Secondly, we expect everything to “ just work ”. For an end user, this is a simple (albeit vague) requirement: everything should remain consistent. There are some places where this isn’t as important (the number of likes on a social media post), but there are others that are crucial for your users and their experiences. Their shopping carts shouldn’t lose items. Their reservations should be set in stone. Accomplishing this, though, can be elusive for an infrastructure team. Ensuring that customers have services that are always on, while also guaranteeing that they behave exactly as users expect is something that historically posed a lot of challenges. With CockroachDB, though, you’re able to develop with an incredibly reliable foundation. Using our Multi-Active Availability model, your cluster is guaranteed to always be totally consistent, while still tolerating failures. Multi-Active Availability: The tl;dr Version CockroachDB starts with the premise that you’re using a deployment across multiple machines (probably in a cloud environment)––this is the premise of all high availability models. From there, you’ll need to run at least 3 machines. Why three? To replicate data between nodes, CockroachDB relies on a the Raft consensus protocol––using it, we guarantee that data remains consistent by requiring a majority (or consensus) of replicas agree on the data’s current state. So, the smallest number of nodes you can have which can achieve a consensus is three––and it turns out this third node is powerful. It not only powers consensus (and therefore consistency), but it also means that you can easily lose a node entirely without forcing the cluster to go down. To tolerate more failures, you simply need to increase the number of replicas (as well as the number of machines their on). CockroachDB’s architecture also lets any node serve data for the entire cluster, including data it doesn’t store. For those details, check out our Distribution Layer’s documentation. The CockroachDB High Availability Recipe With an understanding that CockroachDB must have a majority of nodes online to remain consistent and available, let’s look at what that means in practice. Where is it OK to fail? It’s important to first identify how large of a failure you want to tolerate. For example, you likely want to gracefully handle single machines failing––but what about an entire availability zone? Or an entire data center? For some teams, the likelihood of an entire datacenter failing is low enough that they’re OK with their service going offline in that case. So, the largest element whose failure they want to handle is simply an availability zone. Building a Robust Deployment To survive the failure of the element you identified in the last section, you’ll need your CockroachDB cluster to be deployed across 3 availability zones. This way, if one AZ goes down, you still have 2 that are operational and your cluster remains active. To make sure that your data gets evenly distributed across these 3 availability zones, though, you’ll need to use CockorachDB’s --locality flag to identify which node is in which availability zone. Here’s a quick example: # Start your node in Us-East-1\ncockroach start --locality=az=us-east-1 ...\n\n\n# Start your node in Us-East-2\ncockroach start --locality=az=us-east-2 ... Once these nodes are started, CockroachDB automatically ensures that data is evenly distributed across availability zones, maximizing your ability to survive a failure. ...And Other Considerations Unsurprisingly, there are actually many other considerations you need to make when creating a high availability service. While it’s easier with CockroachDB than with other databases, it’s still a long list of elements to take into consideration. To make the task less daunting, we’ve created a guide: Building Highly Available & Consistent Services with CockroachDB. In it, we cover availability models, as well as tactical guidance to ensure your deployments can survive outages of any size––keeping your customers happy. You can check out the guide here . If building a distributed SQL sytem from the ground up puts a spring in your step, then good news — we're hiring! Check out our open positions here . Illustration by Christina Chung", "date": "2018-08-23"},
{"website": "CockroachLabs", "title": "Flex Fridays: Matt Jibson Problem-Solves by Building a sqlfmt", "author": ["Chelsea Lee", "Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/free-fridays-matt-jibson/", "abstract": "Over the next few months, we plan to share with you what some of our Roachers are working on for their Flex Friday projects . Our Fridays are given to employees as a day to self manage, where they can decide to come into the office or WFH and to focus on work priorities or a side project. We hope this helps explain the bigger picture of one of our company values: Establish Balance. -Chelsea Lee, Culture and Office Experience Manager Matt Jibson, Member of the Technical Staff What is your role at Cockroach Labs? I am an engineer working on the Bulk IO team. I work mostly on improving IMPORT, which is a high-speed way to get data into CockroachDB. What do you do day to day at Cockroach Labs? Most days, I spend the afternoon in the office doing a mix of code reviews, email, in-person collaboration, and some engineering work. I go home around 5:30pm to be with my family, including a toddler. Late evenings are when I get most of my actual work done, since I can work alone when everyone else is asleep. What does Flex Friday mean to you? With a child and nanny at home, Friday afternoons turn out to often be a time my wife and I go out together. We have gone to a spa, out for drinks or a slow lunch. I do often also use them for side projects, but having a free day to make an odd schedule is really valuable. What is your current Flex Friday project? Currently I’m working on sqlfmt , a SQL formatter. All of the code in my life is auto-formatted except for SQL, and this has been a thing I’ve wanted for years. Many of our team members and external users have requested this feature. I used a few Fridays to read some literature about this problem and work on a solution. It has been great to learn some new algorithms, solve an actual problem I had, and also find and fix some bugs along the way. What else have you done on a Flex Friday? My previous project was hots.dog, a data analysis tool for a video game I play. I wanted to build a real life application that used CockroachDB. This allowed me to get some practical experience using and running our product, as well as make a site that I still use today. I’ve spent many Flex Fridays working on it so far. It has been fun to get back into web programming and write real SQL queries again. I learned a lot about our product and began fixing some of the definencies I encountered when using it as a normal user. It was a chance to solve a problem I had, learn something, and have some fun. Describe a typical Flex Friday. I usually don’t go into the office on Fridays. My schedule is the same, except I work from home. During the afternoons when I go out with my wife instead of working, I meet her in the city somewhere How does Flex Friday support your long term goals? I like having dedicated time to work on CockroachDB-adjacent projects or use the time to go out (or even get a babysitter while staying in). I really like working on side projects when I think of a fun one. With a new family, I no longer have time to do so on evenings or weekends. Using Fridays as designated time to pursue related interests is great. It also allows me to remember to focus the other days of the week on customer-related work. Illustration by Quentin Vijoux", "date": "2018-08-03"},
{"website": "CockroachLabs", "title": "Flex Fridays: Josué Rivera Evolves CockroachDB’s User Experience", "author": ["Chelsea Lee"], "link": "https://www.cockroachlabs.com/blog/free-fridays-josue-rivera/", "abstract": "Over the next few months, we plan to share with you what some of our Roachers are working on for their Flex Friday projects . Our Fridays are given to employees as a day to self manage, where they can decide to come into the office or WFH and to focus on work priorities or a side project. We hope this helps explain the bigger picture of one of our company values: Establish Balance. -Chelsea Lee, Culture and Office Experience Manager Josué Rivera, Senior Product Designer What is your role at Cockroach Labs? At Cockroach Labs, I am the Senior Product Designer for the Web UI team. By closely collaborating with three incredibly talented teammates (two software engineers and one PM) I'm influencing the evolution of web-based software that enables our users to monitor and troubleshoot their CockroachDB cluster. Every day at work is an invaluable learning experience that I truly cherish. Not coming from a database or engineering background, it’s humbling and exciting taking such a complex topic and creating simple, intuitive, and impactful user experiences. What do you do day to day at Cockroach Labs? I split my days between the time I use to research, brainstorm, create, and manage projects in our product roadmap. Each day is time blocked to allow ample time for each of the processes I mentioned and to help me focus on particular tasks without much context switching. My work is highly collaborative, so I try to structure my day to be flexible and include enough time for spontaneous discussions with my teammates, as I've found these conversations spark incredible ideas and strengthen my design output. What does Flex Friday mean to you? To me, Flex Friday means being trusted by the people with whom I work to utilise my time how I find most useful and will create the most impact for my self, my team, and the company as a whole. It’s a lot like a ”choose your own adventure game” where I can change up my daily routine and let go of the issues that occupy my mind Monday through Thursday. It's also a great time to experiment with new ideas to help improve our product, internal collaboration, processes, and much more. What is your current Flex Friday project? At the moment I’m working on restructuring the information architecture for the Web UI and the overall vision for how we'd like to evolve the user experience. It’s a project I’ve wanted to get started for some time, and it’s exciting seeing it progress so quickly. Our UI team meets up for a few hours on Fridays to discuss critical problems our users face and brainstorm creative ways to empower and supercharge their workflows. What else have you done on a Flex Friday? Some Fridays I like to learn something new or reboot and spend time on personal things that help reorient my emotional well-being. I’ve taken classes in prototyping, public speaking, improv, pottery, flower arranging, and free diving while also dedicating time for creative work like painting, sculpture or music. Fridays are also a great time to relax with my wife by going for walks, biking, or having brunch. Describe a typical Flex Friday. My Friday mornings are calm. After meditating and stretching, I'll recap my week over coffee and finish any small tasks that need my attention before the weekend. The rest of the day is very flexible. On most Fridays, I dedicated the early afternoons to my Flex Friday project or any project related work that I wasn't able to complete throughout the week. I tend to finish my day early to spend the later part of the afternoon on my personal growth. How does Flex Friday support your long-term goals? With Flex Fridays, I'm enabled to push the boundaries of my day to day responsibilities by having the time to explore new ideas, experiment or tune out and enjoy life. Having Flex Fridays is crucial in developing new skills for my career growth and in creating the right balance between work and life. Illustration by Quentin Vijoux", "date": "2018-09-14"},
{"website": "CockroachLabs", "title": "sqlfmt: A SQL formatter for writing prettier SQL", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/sql-fmt-online-sql-formatter/", "abstract": "sqlfmt is an online SQL formatter. It is pronounced sequel fumpt. Its purpose is to beautifully format SQL statements. I built sqlfmt with my Cockroach Labs colleague Raphael “knz” Poss. Here I will describe how to use it and its features. In addition, I will argue for its need in light of the existing SQL formatters and describe its somewhat interesting implementation. As we are dealing with code formatting here, there is much opinion, and here I will discuss mine. If you do not heartily ascribe to automated, opinionated (i.e., few or no options) choices in your code formatters, sqlfmt is not for you. sqlfmt is for those who think it is better to have no choice in the SQL formatting than it is to format it by hand. Justification A search for sql formatter uncovers lots of online and offline formatters. I tested six of the formatters from the first page of results with the query SELECT 1, 2 , and all six of them formatted it onto two or three lines. While I understand why they do that, I was not satisfied with using them to automatically format my SQL. I wanted something that I could hook up to my text editor on save and it would always produce beautiful results. I believe that using available space is a requirement for that. Those other formatters undoubtedly use a simpler algorithm for their formatting. This is fine and I'm happy they exist and have provided value. It makes sense that they have easy rules like \"each term on its own line\". However I've recently been spoiled in my JavaScript (well, JSX) editing with prettier . It has been a complete game changer for me in terms of how I write fancy JS. As my code gets more complicated or deeplier nested, I just love that it slowly adds newlines to break up functional hierarchy into visual blocks. I wanted a SQL formatter that acted in the same way. Not just something that puts newlines between all terms, but one that adapts to the functional depth of the statement by grouping blocks together. Features sqlfmt's goal is to beautifully format SQL statements. A beautifully formatted SQL statement is one in which the operation of the statement is understandable visually. It was designed to: Understand all CockroachDB (mostly anything Postgres) syntax. Attempt to use available horizontal space in the best way possible. Always maintain visual alignment regardless of your editor configuration to use spaces or tabs at any tab width. To demonstrate, let’s take, for example, a query SELECT a FROM t . Here are some possible renderings of this query: SELECT a FROM t SELECT a\nFROM t SELECT\n\ta\nFROM\n\tt Which one is the most understandable visually? For me, it is the first. This statement contains a mere four words that are instantly parseable by the mind. For me, the second and third renderings detract by using whitespace needlessly. Let us assume we had some 80 characters of width in which to work (an archaic, standard, if not small, editor size). The four lines used by the third rendering have wasted a maximum amount of screen space for zero additional benefit in terms of visual understanding. This feels like YAGNI applied to text formatting. Why use four lines when one will do? But let us now slowly increase the complexity of our query. What if we named many columns in it, many tables in the FROM, added filters and sorts, with subqueries and joins of varying depth? As a query gains complexity, it deserves more space. Indeed what we would like is for the query to use more lines of space and highlight subqueries and other blocks by clearly indenting them. Let's take the first demonstration query from the website at a target width of 50: SELECT\n\tcount(*) AS count,\n\twinner,\n\tcounter * 60 * 5 AS counter\nFROM\n\t(\n\t\tSELECT\n\t\t\twinner,\n\t\t\tround(length / (60 * 5)) AS counter\n\t\tFROM\n\t\t\tplayers\n\t\tWHERE\n\t\t\tbuild = $1\n\t\t\tAND (hero = $2 OR region = $3)\n\t)\nGROUP BY\n\twinner, counter What is clear about this query? Immediately we see that we are getting back three result columns, as they are written each on their own line. We compare that to the two GROUP BY columns. Those are not written on their own line, nor need they be, since it's just two words. In each case we maintain a high degree of visual understanding based on the available versus needed space. Continuing, the subselect in the FROM is clearly the only data source, and its inner query is clear to read. The WHERE clause nicely shows how its boolean operators will be processed. Try this yourself. Start with a tiny query like SELECT a and slowly add and watch the output change and adapt as more complexity is added. Adjust the slider to see what things look like at different available widths. (Is there any other online SQL formatter that is as pleasant to use?) Usage sqlfmt is useable from its website at sqlfum.pt . There is a box in which to paste or type SQL statements. Multiple statements are supported by separating them with a semicolon ( ; ). The slider below the box controls the desired maximum line width in characters. Various options on the side control tab/indentation width, the use of spaces or tabs, simplification, and alignment modes. Simplification causes the formatter to remove unneeded parentheses and words when the meaning will be the same without them. There are four alignment modes. The default, no , uses left alignment. SELECT\n    a\nFROM\n    t\nWHERE\n    c\n    AND b\n    OR d partial right aligns keywords at the width of the longest keyword at the beginning of all lines immediately below. SELECT a\n  FROM t\n WHERE c\n       AND b\n       OR d full is the same as partial but the keywords AND and OR are deindented, in a style similar to the sqlite tests. SELECT a\n  FROM t\n WHERE c\n   AND b\n    OR d other is like partial but instead of deindenting AND and OR , their arguments are instead indented. SELECT a\n  FROM t\n WHERE        c\n          AND b\n       OR d In addition to the website, the latest Cockroach 2.1 beta release has a new sqlfmt subcommand that can be used on your local computer. This can be made into a macro for whatever editor you are using. Run cockroach sqlfmt --help to see its options. Implementation sqlfmt is based on a paper that describes an algorithm to efficiently layout documents with multiple possible layouts. It includes a working implementation in Haskell, a functional language with a deep type system. The algorithm is a fairly small amount of code and is easy to understand. The most difficult part of it is the parsing (converting the original text file into an in-memory data structure), and then converting the parsed representation into one that describes the possible document outputs (this means things like \"could be either a space or a newline here, whichever fits\"). Since sqlfmt needed to parse SQL (CockroachDB SQL specifically), I thought it would be easier to use a language with an existing parser and port the pretty printing algorithm to it than porting the SQL parser to Haskell. The CockroachDB SQL parser is written in Go, so the algorithm had to be written in Go. Haskell and Go are not related very much at all. They approach many concepts differently and have unique sets of optimized features. For example, Haskell is lazy-evaluated (things are evaluated only when needed and the result is cached). The algorithm took advantage of many of these features in Haskell in its implementation, and these needed to be converted into a Go equivalent. For example, let's look at how lazy evaluation with a cached result is done, using the iDoc method . It takes a few comparable parameters that can be used as a key in a Go map. If that key exists, its result is returned (which is safe because we guarantee that the same inputs always result in the same outputs). Otherwise a new instance is created. This allows for very fast recomputation of results during the algorithm that is trying to choose the best line layout based on many possible placements for a newline. After the initial port was complete, a significant amount of additional work was done to add new features to the underlying algorithm. The paper describes how to beautify a document based on a few operators. These are things like a line of text, a new line, indentation, and combinations of those. There are some guarantees that these operators must meet in order to be used, and proofs describing why they each of them are safe. We wanted to add some other functionality (for example the alignment modes) that required new operators. These had to be constructed in a way that still met the guarantees required by the beautification algorithm. We were able to borrow work from others and do some ourselves that allowed us to implement new formatting rules. There are still more of these we’d like to do, so we expect sqlfmt to improve over time. Conclusion sqlfmt is an opinionated SQL formatter. It’s not designed to be completely customizable, and we hope to remove options over time as we decide what works best. Use it and stop thinking about how to format SQL. Does tinkering on distributed SQL engines put a spring in your step? Then good news — we're hiring! Check out our open positions here . (Also published on Matt Jibson's blog .)", "date": "2018-09-27"},
{"website": "CockroachLabs", "title": "An Introduction to Join Ordering", "author": ["Justin Jaffray"], "link": "https://www.cockroachlabs.com/blog/join-ordering-pt1/", "abstract": "The development of the relational model heralded a big step forward for the world of databases. A few years later, SQL introduced a rich vocabulary for data manipulation: filters, projections, and—most importantly—the mighty join. Joins meant that analysts could construct new reports without having to interact with those eggheads in engineering, but more importantly, the existence of complex join queries meant that theoreticians had an interesting new NP-hard problem to fawn over for the next five decades. Ever since, the join has been the fundamental operation by which complex queries are constructed out of simpler \"relations\". The declarative nature of SQL means that users do not generally specify how their query is to be executed—it’s the job of a separate component of the database called the optimizer to figure that out. Since joins are so prevalent in such queries, the optimizer must take special care to handle them intelligently. As we'll see, this isn't a trivial task. In this post, we'll look at why join ordering is so important and develop a sense of how to think of the problem space. And then, in upcoming posts, we'll begin discussing ways to implement a fast, reliable algorithm to produce good join orderings. A Refresher on SQL and Joins Let’s do a quick refresher in case you don’t work with SQL databases on a regular basis. A relation or table is basically a spreadsheet. Say we have the following relations describing a simple retailer: customers products orders The customers, or C relation looks something like this: customer_id customer_name customer_location 1 Joseph Norwalk, CA, USA 2 Adam Gothenburg, Sweden 3 William Stockholm, Sweden 4 Kevin Raleigh, NC, USA the products , or P relation looks like this: product_id product_location 123 Norwalk, CA, USA 789 Stockholm, Sweden 135 Toronto, ON, Canada The orders, or O relation looks like this: order_id order_product_id order_customer_id order_active 1 123 3 false 2 789 1 true 3 135 2 true The cross product of the two relations, written P \\times O , is a new relation which contains every pair of rows from the two input relations. Here’s what P \\times O looks like: product_id product_location order_id order_product_id order_customer_id order_active 123 Norwalk, CA, USA 1 123 3 false 123 Norwalk, CA, USA 2 789 1 true 123 Norwalk, CA, USA 3 135 2 true 789 Stockholm, Sweden 1 123 3 false 789 Stockholm, Sweden 2 789 1 true 789 Stockholm, Sweden 3 135 2 true 135 Toronto, ON, Canada 1 123 3 false 135 Toronto, ON, Canada 2 789 1 true 135 Toronto, ON, Canada 3 135 2 true However, for most applications, this doesn’t have much meaning, which is where joins come into play. A join is when we have a filter (or predicate) applied to the cross product of two relations. If we filter the above table to the rows where product_id = order_product_id , we say we’re \"joining P and O on product_id = order_product_id \". The result looks like this: product_id product_location order_id order_product_id order_customer_id order_active 123 Norwalk, CA, USA 1 123 3 false 789 Stockholm, Sweden 2 789 1 true 135 Toronto, ON, Canada 3 135 2 true Here we can see all of the orders that contained a given product. We can then remove some of the columns from the output (this is called projection ): product_id order_customer_id 123 3 789 1 135 2 This ends up with a relation describing the products various users ordered. Through pretty basic operations, we built up some non-trivial meaning. This is why joins are such a major part of most query languages (primarily SQL): they’re very conceptually simple (a predicate applied to the cross product) but can express fairly complex operations. You might have observed that even though the size of the cross product was quite large ( |P| \\times |O| ), the final output was pretty small. Databases will exploit this fact to perform joins much more efficiently than by producing the entire cross product and then filtering it. This is part of why it’s often useful to think of a join as a single unit, rather than two composed operations. Some Vocabulary To make things easier to write, we’re going to introduce a little bit of notation. We already saw that the cross product of A and B is written A \\times B . Filtering a relation R on a predicate p is written \\sigma_p( R ) . That is, \\sigma_p( R ) is the relation with every row of R for which p is true, for example, the rows where product_id = order_product_id . Thus a join of A and B on p could be written \\sigma_p(A \\times B) . Since we often like to think of joins as single cohesive units, we can also write this as A \\Join_p B . The columns in a relation don’t need to have any particular order (we only care about their names), so we can take the cross product in any order. A \\times B = B \\times A , and further, A \\Join_p B = B \\Join_p A . You might know this as the commutative property. Joins are commutative. We can \"pull up\" a filter through a cross product: \\sigma_p(A) \\times B = \\sigma_p(A \\times B) . It doesn’t matter if we do the filtering before or after the product is taken. Because of this, it sometimes makes sense to think of a sequence of joins as a sequence of cross products which we filter at the very end: (A \\Join_p B) \\Join_q C = \\sigma_q( \\sigma_p( A \\times B ) \\times C) = \\sigma_{p\\wedge q}(A \\times B \\times C) Something that becomes clear when written in this form is that we can join A with B and then join the result of that with C , or we can join B with C and then join the result of that with A . The order in which we apply those joins doesn’t matter, as long as all the necessary filtering happens at some point. You might recognize this as the associative property. Joins are associative (with the asterisk that we need to pull up predicates where appropriate). Optimizing Joins So we can perform our joins in any order we please. This raises a question: is there some order that’s more preferable than another? Yes. It turns out that the order in which we perform our joins can result in dramatically different amounts of work required. Consider a fairly natural query on the above relations, where we want to get a list of all customers’ names along with the location of each product they’ve ordered. In SQL we could write such a query like this: SELECT customer_name, product_location FROM\n  orders\n  JOIN customers ON customer_id = order_customer_id\n  JOIN products ON product_id = order_product_id We have two predicates: customer_id = order_customer_id product_id = order_product_id Say we first join products and customers . Since neither of the two predicates above relate products with customers, we have no choice but to form the entire cross products between them. This cross product might be very large (the number of customers times the number of products) and we have to compute the entire thing. What if we instead first compute the join between orders and customers ? The sub-join of orders joined with customers only has an entry for every order placed by a customer - probably much smaller than every pair of customer and product. Since we have a predicate between these two, we can compute the much smaller result of joining them and filtering directly (there are many algorithms to do this efficiently, the three most common being the hash join, merge join, and nested-loop/lookup join). Visualizing the Problem To better understand the structure of a join query, we can look at its query graph. The query graph of a query has a vertex for each relation being joined and an edge between any two relations for which there is a predicate. Since a predicate filters the result of the cross product, predicates can be given a numeric value that describes how much they filter said result. This value is called their selectivity. The selectivity of a predicate p on A and B is defined as: sel(p) = \\dfrac{|A \\Join_p B|}{|A \\times B|} = \\dfrac{|A \\Join_p B|}{|A||B|} In practice, we tend to think about this the other way around; we assume that we can estimate the selectivity of a predicate and use that to estimate the size of a join: |A \\Join_p B| = sel(p)|A||B| So a predicate which filters out half of the rows has selectivity 0.5 and a predicate which only allows one row out of every hundred has selectivity 0.01 . Since predicates which are more selective reduce the cardinality of their output more aggressively, a decent general principle is that we want to perform joins over predicates which are very selective first. It’s often assumed for convenience that all the selectivities are independent, that is, |A \\Join_p B \\Join_q C| = sel(p)sel(q)|A \\times B \\times C| Which, while indeed convenient, is rarely an accurate assumption in practice. Check out \"How Good Are Query Optimizers, Really?\" by Leis et al. for a detailed discussion of the problems with this assumption. It turns out that the shape of a query graph plays a large part in how difficult it is to optimize a query. There are a handful of canonical archetypes of query graph \"shapes\", all with different optimization characteristics. A “chain” query graph A “star” query graph A “clique” query graph A “cycle” query graph Note that these shapes aren’t necessarily representative of many real queries, but they represent extremes which exhibit interesting behaviour and which permit interesting analysis. Query Plans To visualize a particular join ordering, we can look at its query plan diagram. Since most join execution algorithms only perform joins on one pair of relations at a time, these are generally binary trees. The query plan we ended up with for the above query has a diagram that looks something like this: There are also two main canonical query plan shapes, the less general \"left-deep plan\": Where every relation is joined in sequence. The more general form is the \"bushy plan\": In a left-deep plan, one of the two relations being joined must always be a concrete table, rather than the output of a join. In a bushy plan, such composite inners are permitted. Is this actually \"hard\"? In the examples we’ve seen, there were only a handful of options, but as the number of tables being joined grows, the number of potential query plans grows extremely fast—and in fact, finding the optimal order in which to join a set of tables is NP-hard. This means that when faced with large join ordering problems, databases are generally forced to resort to a collection of heuristics to attempt to find a good execution plan (unless they want to spend more time optimizing than executing!). I think it’s important to first answer the question of why we need to do this at all. Even if some join orderings are orders of magnitude better than others, why can’t we just find a good order once and then use that in the future? Why does a piece of software like a database that’s concerned with going fast need to solve an NP-hard problem every time it receives a query? It’s a fair question, and there’s probably interesting research to be done in sharing optimization work across queries. The main answer, though, is that you’re going to want different join strategies for a query involving Justin Bieber’s twitter followers versus mine. The scale of various relations being joined will vary dramatically depending on the query parameters and the fact is that we just don’t know the problem we’re solving until we receive the query from the user, at which point the query optimizer will need to consult its statistics to make informed guesses about what join strategies will be good. Since these statistics will be very different for a query over Bieber’s followers, the decisions the optimizer ends up making will be different and we probably won’t be able to reuse a result from before. Once you accept that you have to solve the problem, how do you do it? A common characteristic of NP-hard problems is that they’re strikingly non-local. Any type of local reasoning or optimization you attempt to apply to them will generally break down and doom you to look at the entire problem holistically. In the case of join ordering, what this means is that in most cases it’s difficult or impossible to make conclusive statements about how any given pair of relations should be joined - the answer can differ drastically depending on all the tables you don’t happen to be thinking about at this moment. In our example with customers, orders, and products, it might look like our first plan was bad only because we first performed a join for which we had no predicate (such intermediate joins are just referred to as cross products ), but in fact, there are joins for which the ordering that gives the smallest overall cost involves a cross product (exercise for the reader: find one). Despite the fact that optimal plans can contain cross products, it’s very common for query optimizers to assume their inclusion won’t improve the quality of query plans that much, since disallowing them makes the space of query plans much smaller and can make finding decent plans much quicker. This assumption is sometimes called the connectivity heuristic (because it only considers joining relations which are connected in the query graph). This post has mostly been about the vocabulary with which to speak and think about the problem of ordering joins, and hasn’t really touched on any concrete algorithms with which to find good query plans. Join ordering is, generally, quite resistant to simplification. In the general case—and in fact, almost every case in practice—the problem of finding the optimal order in which to perform a join query is NP-hard. However, if we sufficiently restrict the set of queries we look at, and restrict ourselves to certain resulting query plans, there are some useful situations in which we can find an optimal solution. Those details, though, will come in a follow-up post. Thanks to Andy Kimball for his technical review of this post. If you like this post you can go even deeper on Join Ordering with Join Ordering Part II: The SQL Does building distributed SQL engines put a spring in your step? If so, we're hiring! Check out our open positions here .", "date": "2018-10-23"},
{"website": "CockroachLabs", "title": "Announcing Managed CockroachDB: The Geo-Distributed Database as a Service", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/launching-managed-cockroachdb/", "abstract": "This week we’re pleased to announce the availability of Managed CockroachDB, the fully hosted and fully managed service created and run by Cockroach Labs that makes deploying, scaling, and managing CockroachDB effortless. Managed CockroachDB is cloud agnostic and available at launch on both AWS and GCP. The goal is simple: allow your development team to focus on building highly scalable applications without worrying about infrastructure operations. Sign up to get started here: Get CockroachDB Our mission at Cockroach Labs is to make data easy. CockroachDB’s design makes that mission a reality by providing an industry-leading model for resilience, horizontal scalability to accommodate fast-growing businesses, and the ability to move data close to your customers, wherever they reside across the globe. We’ve also devoted significant effort making it seamless to get started on CockroachDB – it takes only a single binary download to your VMs, and you can have a scalable distributed SQL database in minutes. Nevertheless, when we talked to our customers – both big and small – we learned that while they appreciate how easy it is to get started on CockroachDB, not all of them prefer to manage the day to day operation of a distributed system themselves. Relieving our customers of infrastructure operations is the clear next step in making data truly easy. Always-on Service Managed CockroachDB is the always-on service for your mission critical applications. We automatically replicate your data across three availability zones for single region deployments. As a globally scalable distributed SQL database, we also support geo-partitioned clusters at whatever scale your business demands. Since CockroachDB is cloud agnostic, we can migrate from one cloud service provider to another at peak load with zero downtime, should your business needs change over time. Operational Excellence Cockroach Labs takes care of hardware provisioning, setup, and configuration for your managed clusters so they are optimized for performance. We ensure automatic upgrades to the latest releases, and hourly incremental backups of your data. We provide 24x7 monitoring, and enterprise grade security for all our customers. Distributed SQL Experts As the makers of CockroachDB, we have amassed years of experience running CockroachDB clusters in a wide variety of configurations, and are pleased to be able to provide this experience in service of our customers’ mission-critical workloads. We want to share our battle-tested reference architectures and in-house site reliability engineering expertise with you, so that you can go to market faster with the unprecedented capabilities CockroachDB provides for building ultra-resilient, high-scale, global applications. Three years ago we introduced you to the database we wanted to use when starting our next company. We had a simple but bold mission: to Make Data Easy. Today, I am pleased to announce another major step towards realizing our mission with the availability of Managed CockroachDB, the Geo-Distributed Database as a Service. Sign up for early access to Managed CockroachDB here: Get CockroachDB", "date": "2018-10-30"},
{"website": "CockroachLabs", "title": "A Brief History of High Availability", "author": ["Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/brief-history-high-availability/", "abstract": "I once went to a website that had “hours of operation,” and was only “open” when its brick and mortar counterpart had its lights on. I felt perplexed and a little frustrated; computers are capable of running all day every day, so why shouldn’t they? I’d been habituated to the internet’s incredible availability guarantees. However, before the internet, 24/7 availability wasn’t “a thing.” Availability was desirable, but not something to which we felt fundamentally entitled. We used computers only when we needed them; they weren’t waiting idly by on the off-chance a request came by. As the internet grew, those previously uncommon requests at 3am local time became prime business hours partway across the globe, and making sure that a computer could facilitate the request was important. Many systems, though, relied on only one computer to facilitate these requests––which we all know is a story that doesn’t end well. To keep things up and running, we needed to distribute the load among multiple computers that could fulfill our needs. However, distributed computation, for all its well-known upsides, has sharp edges: in particular, synchronization and tolerating partial failures within a system. Each generation of engineers has iterated on these solutions to fit the needs of their time. How distribution came to databases is of particular interest because it’s a difficult problem that’s been much slower to develop than other areas of computer science. Certainly, software tracked the results of some distributed computation in a local database, but the state of the database itself was kept on a single machine. Why? Replicating state across machines is hard. In this post, we want to take a look at how distributed databases have historically handled partial failures within a system and understand––at a high level––what high availability looks like. Working with What We Have: Active-Passive In the days of yore, databases ran on single machines. There was only one node and it handled all reads and all writes. There was no such thing as a “partial failure”; the database was either up or down. Total failure of a single database was a two-fold problem for the internet; first, computers were being accessed around the clock, so downtime was more likely to directly impact users; second, by placing computers under constant demand, they were more likely to fail. The obvious solution to this problem is to have more than one computer that can handle the request, and this is where the story of distributed databases truly begins. Living in a single-node world, the most natural solution was to continue letting a single node serve reads and writes and simply sync its state onto a secondary, passive machine––and thus, Active-Passive replication was born. Active-Passive improved availability by having an up-to-date backup in cases where the active node failed––you could simply start directing traffic to the passive node, thereby promoting it to being active. Whenever you could, you’d replaced the downed server with a new passive machine (and hope the active one didn’t fail in the interim). At first, replication from the active to the passive node was a synchronous procedure, i.e., transformations weren’t committed until the Passive node acknowledged them. However, it was unclear what to do if the passive node went down. It certainly didn’t make sense for the entire system to go down if the backup system wasn’t available––but with synchronous replication, that’s what would happen. To further improve availability, data could instead be replicated asynchronously. While its architecture looks the same, it was capable of handling either the active or the passive node going down without impacting the database’s availability. While asynchronous Active-Passive was another step forward, there were still significant downsides: When the active node died, any data that wasn’t yet replicated to the passive node could be lost––despite the fact that the client was led to believe the data was fully committed. By relying on a single machine to handle traffic, you were still bound to the maximum available resources of a single machine. Chasing Five 9s: Scale to Many Machines As the Internet proliferated, business’ needs grew in scale and complexity. For databases this meant that they needed the ability to handle more traffic than any single node could handle, and that providing “always on” high availability became a mandate. Given that swaths of engineers now had experience working on other distributed technologies, it was clear that databases could move beyond single-node Active-Passive setups and distribute a database across many machines. Sharding Again, the easiest place to start is adapting what you currently have, so engineers adapted Active-Passive replication into something more scalable by developing sharding. In this scheme, you split up a cluster’s data by some value (such as a number of rows or unique values in a primary key) and distributed those segments among a number of sites, each of which has an Active-Passive pair. You then add some kind of routing technology in front of the cluster to direct clients to the correct site for their requests. Sharding lets you distribute your workload among many machines, improving throughput, as well as creating even greater resilience by tolerating a greater number of partial failures. Despite these upsides, sharding a system was complex and posed a substantial operational burden on teams. The deliberate accounting of shards could grow so onerous that the routing ended up creeping into an application’s business logic. And worse, if you needed to modify the way a system was sharded (such as a schema change), it often posed a significant (or even monumental) amount of engineering to achieve. Single-node Active-Passive systems had also provided transactional support (even if not strong consistency). However, the difficulty of coordinating transactions across shards was so knotted and complex, many sharded systems decided to forgo them completely. Active-Active Given that sharded databases were difficult to manage and not fully featured, engineers began developing systems that would at least solve one of the problems. What emerged were systems that still didn’t support transactions, but were dramatically easier to manage. With the increased demand on applications’ uptime, it was a sensible decision to help teams meet their SLAs. The motivating idea behind these systems was that each site could contain some (or all) of a cluster’s data and serve reads and writes for it. Whenever a node received a write it would propagate the change to all other nodes that would need a copy of it. To handle situations where two nodes received writes for the same key, other nodes’ transformations were fed into a conflict resolution algorithm before committing. Given that each site was “active”, it was dubbed Active-Active. Because each server could handle reads and writes for all of its data, sharding was easier to accomplish algorithmically and made deployments easier to manage. In terms of availability, Active-Active was excellent. If a node failed, clients just needed to be redirected to another node that did contain the data. As long as a single replica of the data was live, you could serve both reads and writes for it. While this scheme is fantastic for availability, its design is fundamentally at odds with consistency. Because each site can handle writes for a key (and would in a failover scenario), it’s incredibly difficult to keep data totally synchronized as it is being processed. Instead, the approach is generally to mediate conflicts between sites through the conflict resolution algorithm that makes coarse-grained decisions about how to “smooth out” inconsistencies. Because that resolution is done post hoc, after a client has already received an answer about a procedure––and has theoretically executed other business logic based on the response––it’s easy for active-active replication to generate anomalies in your data. Given the premium on uptime, though, the cost of downtime was deemed greater than the cost of potential anomalies, so Active-Active became the dominant replication type. Consistency at Scale: Consensus & Multi-Active Availability While Active-Active seemed like it addressed the major problem facing infrastructure––availability––it had only done so by forgoing transactions, which left systems that needed strong consistency without a compelling choice. For example, Google used a massive and complex sharded MySQL system for its advertising business, which heavily relied on SQL’s expressiveness to arbitrarily query the database. Because these queries often relied on secondary indexes to improve performance, they had to be kept totally consistent with the data they were derived from. Eventually, the system grew large enough in size that it began causing problems for sharded MySQL, so their engineers began imagining how they could solve the problem of having both a massively scalable system that could also offer the strong consistency their business required. Active-Active’s lack of transactional support meant it wasn’t an option, so they had to design something new. What they ended up with was a system based around consensus replication, which would guarantee consistency, but would also provide high availability. Using consensus replication, writes are proposed to a node, and are then replicated to some number of other nodes. Once a majority of the nodes have acknowledge the write, it can be committed. Consensus & High Availability The lynch-pin notion here is that consensus replication lies in a sweet spot between synchronous and asynchronous replication: you need some arbitrary number of nodes to behave synchronously, but it doesn’t matter which nodes those are. This means the cluster can tolerate a minority of nodes going down without impacting the system’s availability. (Caveats made for handling the downed machines’ traffic, etc.) The cost of consensus, though, is that it requires nodes to communicate with others to perform writes. While there are steps you can take to reduce the latency incurred between nodes, such as placing them in the same availability zone, this runs into trade-offs with availability. For example, if all of the nodes are in the same datacenter, it’s fast for them to communicate with one another, but you cannot survive an entire datacenter going offline. Spreading your nodes out to multiple datacenters can increase the latency required for writes, but can improve your availability by letting an entire datacenter going offline without bringing down your application. Multi-Active Availability CockroachDB implements much of the learnings from the Google Spanner paper (though, notably, without requiring atomic clocks), including those features beyond consensus replication that make availability much simpler. To describe how this works and differentiate it from Active-Active, we’ve coined the term Multi-Active Availability. Active-Active vs. Multi-Active Active-Active achieves availability by letting any node in your cluster serve reads and writes for its keys, but propagates any changes it accepts to other nodes only after committing writes. Multi-Active Availability, on the other hand, lets any node serve reads and writes, but ensures that a majority of replicas are kept in sync on writes ( docs ), and only serves reads from replicas of the latest version ( docs ). In terms of availability, Active-Active only requires a single replica to be available to serve both reads of writes, while Multi-Active requires a majority of replicas to be online to achieve consensus (which still allows for partial failures within the system). Downstream of these databases' availability, though, is a difference of consistency. Active-Active databases work hard to accept writes in most situations, but then don't make guarantees about the ability for a client to then read that data now or in the future. On the other hand, Multi-Active databases accept writes only when it can guarantee that the data can later be read in a way that's consistent. Yesterday, Today, Tomorrow Over the last 30 years, database replication and availability have taken major strides and now supports globe-spanning deployments that feel like they never go down. The field’s first forays laid important groundwork through Active-Passive replication but eventually, we needed better availability and greater scale. From there, the industry has developed two predominant paradigms of databases: Active-Active for applications whose primary concern is accepting writes quickly, and Multi-Active for those that require consistency. May we all look forward to the day when we can harness quantum entanglement and move to the next paradigm in managing distributed state. If defining the next phase of database replication and availability is your coffee-break daydream, then check out our open positions here . Illustration by Christina Chung", "date": "2018-10-04"},
{"website": "CockroachLabs", "title": "CockroachDB 2.1 — Easier Migrations and a 5x Scalability Improvement", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-2dot1-release/", "abstract": "CockroachDB was built to help teams scale their applications across the globe without sacrificing SQL’s convenience, power, and data-integrity guarantees. In CockroachDB 2.1, we’ve made it easier than ever to migrate from MySQL and Postgres, improved our scalability on transactional workloads by 5x, and launched a managed offering to help teams deploy low-latency, multi-region clusters with minimal operator overhead. What we’ve learned from our last release Building a database for an unpredictable world was a major goal for us in CockroachDB 2.0; that release provided the functionality required to run global systems that respond to changes in customer geography and data-sovereignty regulations while supporting just-in-time horizontal scaling to 10x the scale of Amazon Aurora . Since our last release, we’ve helped Comcast provide its users with great customer support by enabling a hybrid deployment that spans the width of the United States. As a result, their customers’ data is always available, and always close to where it’s needed. Additionally, CockroachDB is now the foundation of Bose’s data strategy for their cloud-connected products initiative. In the last six months alone, we’ve helped a multi-billion dollar global retailer and a Fortune 500 financial institution modernize their stacks. Developers we’ve met with have expressed their need for more support and tooling for moving data and workloads into CockroachDB from MySQL and other systems of record. Continuing to push beyond distributed, ultra-high-availability SQL, our customers have demanded ever higher levels of scalability. They’ve also helped us identify the areas in which they need guidance for running widely distributed systems, which may span cloud regions or even cloud providers. In this release, we’ve continued to improve our support for the responsive global data architectures we’ve enabled with CockroachDB 2.0. Below, we’ll cover major performance updates, migration and data export improvements, and our new managed service offering. Migrate to CockroachDB faster with new tooling Native support for importing from MySQL and Postgres Our collaboration with Baidu to replace their sharded MySQL instances with CockroachDB (which now serves 40TB of data at over 100k QPS) was the first of many major MySQL migrations we would support. While MySQL is the world’s most popular open-source database, as developers tire of working around its resilience and scalability issues, they increasingly look to CockroachDB to support their next-generation applications. With this release, it’s never been easier to make the switch to CockroachDB. We’ve taken our learnings from supporting MySQL migrations and baked them into the product -- developers can now import full mysqldump files with a single command. We’ve also enhanced our IMPORT feature to support Postgres’ dump file format. Support for more Postgres tooling and ORMs out of the box CockroachDB 2.1 is the next step in our journey toward implementing the SQL standard . We’ve paid close attention to the types of workloads our customers want to support and used that knowledge to prioritize the features that will make more apps and Postgres ORMs work out of the box. This release, we’ve also worked with partners to support more Postgres UIs; CockroachDB now works with DBeaver to give developers another fully functional administrative tool for managing their databases in a familiar way. Stream changes from CockroachDB into your existing stack CockroachDB is the system of record for some of the largest and fastest-growing businesses in the world. However, while CockroachDB is a general-purpose database that can run both transactional and analytical queries, OLAP-style workloads often require teams to offload data onto dedicated data warehouses. To support these users, this release introduces beta support for Change Data Capture (CDC) for enterprise customers. This feature streams changes to Apache Kafka where they can be added to other systems in real time, allowing teams to minimize the time between when data is recorded and when it’s actionable. Support for Kafka also means teams can use CDC to trigger events in real time without having to rely on polling. Our CDC implementation is always transactionally consistent and allows teams to determine the proper ordering of events, even in distributed environments. Note that unlike most major 2.1 features, we will continue to make incremental updates to CDC in minor releases as we collect feedback. We’d love to hear how you use streaming data in your own applications. CockroachDB 2.1 is 5x more scalable than v2.0 5x scalability improvement for transactional workloads CockroachDB gives developers NoSQL levels of scalability without requiring NoSQL concessions (i.e., lack of SQL and strong consistency). One way we track our progress here is with TPC-C , the industry-standard benchmark that uses a mix of concurrent transactions to model the operations of any business that must \"manage, sell, or distribute a product or service.\" CockroachDB 2.1 achieves ~50x the throughput of Amazon Aurora’s best-reported TPC-C results and 5x the throughput of our 2.0 release. What this means for users is that they can trust that CockroachDB can handle growing and complex transactional workloads by adding more commodity machines rather than buying expensive, specialized hardware. CockroachDB achieves these results without sacrificing consistency (CockroachDB runs exclusively at the highest isolation level guaranteed by the SQL standard ), so teams don’t have to worry about complicated workarounds for managing eventual consistency and can avoid increasingly exploitable consistency-related security vulnerabilities . Quickly diagnose and fix performance bottlenecks While CockroachDB can rebalance and relocate data to improve performance problems behind the scenes, developers need deep visibility into what’s happening in their clusters in order to make manual tweaks. With CockroachDB 2.1, we’ve introduced a statements page and hardware-metrics dashboards so developers and operators can see how their cluster is performing and drill down to identify the root cause for slow queries or performance bottlenecks quickly. You can watch a demo of the new statements page here . We’ve also added debugging pages that allow teams to examine the internals of CockroachDB, like details on data distribution and replication, in order to make more sophisticated provisioning and configuration decisions. Simplify deployment of multi-regional clusters with Managed CockroachDB Finally, we are pleased to announce a faster way to deploy geo-distributed clusters: Managed CockroachDB . We’ve partnered with a handful of global businesses to run multi-region clusters in which Cockroach Labs handles the networking, provisioning, and load balancing, which allows our customers to focus on app development. This is a fast way to get up and running with an enterprise CockroachDB install and to leverage our expertise for deploying always-on mission-critical systems. Managed CockroachDB is currently in limited-availability, but it will be widely available in future releases. Recap CockroachDB 2.1 is an important step forward for teams that want to run mission-critical cloud-native data architectures. We’ve made huge performance improvements from our 5x scalability bump to new tools for visualizing bottlenecks. We’ve added more migration support from MySQL and Postgres so it’s easier to port existing apps, while CDC lets you use CockroachDB to power your data warehouses with realtime updates. Finally, with Managed CockroachDB , you can deploy clusters to the furthest reaches of the world with minimal operator overhead. We’re hoping you use CockroachDB 2.1 to make your global data seem local to your users. This post is only a brief summary of what you can expect from our new release. For the full update, check out our release notes .", "date": "2018-11-01"},
{"website": "CockroachLabs", "title": "How We Built a Cost-Based SQL Optimizer", "author": ["Andy Kimball"], "link": "https://www.cockroachlabs.com/blog/building-cost-based-sql-optimizer/", "abstract": "Here at Cockroach Labs, we’ve had a continual focus on improving performance and scalability. To that end, our 2.1 release includes a brand-new, built-from-scratch, cost-based SQL optimizer. Besides enabling SQL features like correlated subqueries for the first time, it provides us with a flexible optimization framework that will yield significant performance improvements in upcoming releases, especially in more complex reporting queries. If you have queries that you think should be faster, send them our way! We’re building up libraries of queries that we use to tune the performance of the optimizer and prioritize future work. While as an engineer, I’m eager to dive right into the details of how our new optimizer works (TL;DR - it’s very cool stuff), I need to first set the stage. I’ll start by explaining what a cost-based SQL optimizer is, and then tell you the story of how we decided we really, really needed one of those. Enough that we took 4 engineers, shut them into a windowless Slack room, and gave them carte blanche to rewrite a major component of CockroachDB. After story time, I’ll move onto the really interesting stuff, giving you a peek “under the hood” of the new optimizer. A peek will have to suffice, though, as digging deeper will require more words than one blog entry can provide. But do not despair; future articles will delve further into optimizer internals, so stay tuned. What is a SQL optimizer anyway? A SQL optimizer analyzes a SQL query and chooses the most efficient way to execute it. While the simplest queries might have only one way to execute, more complex queries can have thousands, or even millions , of ways. The better the optimizer, the closer it gets to choosing the optimal execution plan , which is the most efficient way to execute a query. Here’s a query that looks deceptively simple: SELECT * FROM customers c, orders o WHERE c . id = o . cust_id AND c . name < ’John Doe’ I won’t bore you (or me) with the exhaustive list of questions that the optimizer must answer about this query, but here are a few to make my point: Should we evaluate the name filter before the join or after the join? Should we use a hash join, merge join, or a nested loop join with an index (called a “lookup join” in CockroachDB)? If a lookup or hash join, should we enumerate customers and then lookup orders? Or enumerate orders and then lookup customers? If there’s a secondary index on “name”, should we use that to find matching names, or is it better to use the primary index to find matching ids? Furthermore, it’s not enough for the optimizer to answer each of these questions in isolation. To find the best plan, it needs to look at combinations of answers. Maybe it’s best to use the secondary index when choosing the lookup join. But, if a merge join is used instead, the primary index may be better. The optimal execution plan depends on row counts, relative performance of the various physical operators, the location and frequency of data values, and … a lot of other stuff. Heuristic vs. cost-based So how do optimizers choose among so many possible execution plans? People have been thinking and writing about that longer than I’ve been alive, so any answer’s going to be inadequate. But, it’s still valuable to discuss two common approaches to the problem. The first approach is what everyone who builds an optimizer for the first time takes. They come up with preset heuristic rules based on general principles. For example, there might be a heuristic rule to always use a hash join instead of a nested loop join if an equality condition is present. In most situations, that will result in a better execution plan, and so it’s a good heuristic. An optimizer based on rules like this is called a heuristic optimizer . However, static heuristic rules have a downside. They work well in most cases, but they can fail to find the best plan in other cases. For example, a lookup join loops over rows from an outer relation and looks for inner rows that match by repeatedly probing into an index over the inner relation. This works well when the number of outer rows is small, but degrades as that number rises and the overhead of probing for every row begins to dominate execution time. At some cross-over point, a hash or merge join would have been better. But it’s difficult to devise heuristics that capture these subtleties. Enter the cost-based optimizer . A cost-based optimizer will enumerate possible execution plans and assign a cost to each plan, which is an estimate of the time and resources required to execute that plan. Once the possibilities have been enumerated, the optimizer picks the lowest cost plan and hands it off for execution. While a cost model is typically designed to maximize throughput (i.e. queries per second), it can be designed to favor other desirable query behavior, such as minimizing latency (i.e. time to retrieve first row) or minimizing memory usage. At this point, you may be thinking, “but what if that cost model turns out to be wrong?”. That’s a good question, and it’s true that a cost-based optimizer is only as good as the costs it assigns. Furthermore, it turns out that the accuracy of the costs are highly dependent on the accuracy of the row count estimates made by the optimizer. These are exactly what they sound like: the optimizer estimates how many rows will be returned by each stage of the query plan. This brings us to another subject of decades of research: database statistics . The goal of gathering database statistics is to provide information to the optimizer so that it can make more accurate row count estimates. Useful statistics include row counts for tables, distinct and null value counts for columns, and histograms for understanding the distribution of values. This information feeds into the cost model and helps decide questions of join type, join ordering, index selection, and more. (Re)birth of an optimizer CockroachDB started with a simple heuristic optimizer that grew more complicated over time, as optimizers tend to do. By our 2.0 release, we had started running into limitations of the heuristic design that we could not easily circumvent. Carefully-tuned heuristic rules were beginning to conflict with one another, with no clear way to decide between them. A simple heuristic like: “use hash join when an equality condition is present” became: “use hash join when an equality condition is present, unless both inputs are sorted on the join key, in which case use a merge join” near the end, we contemplated heuristics like: “use hash join when an equality condition is present, unless both inputs are sorted on the join key, in which case use a merge join; that is, except if one join input has a small number of rows and there’s an index available for the other input, in which case use a lookup join” Every new heuristic rule we added had to be examined with respect to every heuristic rule already in place, to make sure that they played nicely with one another. And while even cost-based optimizers sometimes behave like a delicately balanced Jenga tower, heuristic optimizers fall over at much lower heights. By the last half of 2017, momentum was growing within Cockroach Labs to replace the heuristic optimizer that was showing its age. Peter Mattis, one of our co-founders, arranged to have an outside expert on database optimizers run a months-long bootcamp, with the goal of teaching our developers how state-of-the-art optimizers work, complete with homework assignments to read seminal papers on the subject. In order to kickstart discussion and momentum, Peter created a cost-based optimizer prototype called “opttoy” , that demonstrated some of the important concepts, and informed the production work that followed. By the time I joined the company in early 2018, the company had come to the collective decision that it was now time to take the next step forward. Given my background and interest in the subject, I was tasked with leading a small (but highly motivated) team to build a cost-based optimizer from scratch. After 9 months of intense effort, our team is releasing the first version of the new optimizer as part of the CockroachDB 2.1 release. While there’s still much more we can (and will) do, this first release represents an important step forward. Here are a couple of important new capabilities that the 2.1 cost-based optimizer supports: Correlated subqueries - these are queries that contain an inner subquery that references a column from an outer query, such as in this example: SELECT *\nFROM customers c\nWHERE EXISTS (\n     SELECT *\n     FROM orders o\n     WHERE o.cust_id = c.id\n) Optimizing correlated subqueries is another blog post all on its own, which I hope to cover in the future. Automatic planning of lookup joins : When deciding how to execute a join, the optimizer now considers lookup joins, in addition to merge and hash joins. Lookup joins are important for fast execution of equality joins, where one input has a small number of rows and the other has an index on the equality condition: SELECT COUNT(*)\nFROM customers c, orders o\nWHERE c.id=o.cust_id AND c.zip='12345' AND c.name='John Doe' Here, the optimizer would consider a plan that first finds customers named “John Doe” who live in zip code “12345” (likely to be a small number of rows), and then probes into the orders table to count rows. Under the Hood As promised, I want to give you a quick peek under the hood of the new optimizer. To start, it’s useful to think of a query plan as a tree, with each node in the tree representing a step in the execution plan. In fact, that’s how the SQL EXPLAIN statement shows an execution plan: This output shows how the unoptimized plan would execute: first compute a full cross-product of the customers and orders tables, then filter the resulting rows based on the WHERE conditions, and finally compute the count_rows aggregate. But that would be a terrible plan! If there were 10,000 customers and 100,000 orders, then the cross-product would generate 1 billion rows, almost all of which would simply be filtered away. What a waste. Here’s where the optimizer proves its worth. Its job is to transform that starting plan tree into a series of logically equivalent plan trees, and then pick the tree that has the lowest cost. So what does “logically equivalent” mean? Two plan trees are logically equivalent if they both return the same data when executed (though rows can be ordered differently if there is no ORDER BY clause). In other words, from a correctness point of view, it doesn’t matter which plan the optimizer picks; its choice is purely about maximizing performance. Transformations The optimizer does not generate the complete set of equivalent plan trees in one step. Instead, it starts with the initial tree and performs a series of incremental transformations to generate alternative trees. Each individual transformation tends to be relatively simple on its own; it is the combination of many such transformations that can solve complex optimization challenges. Watching an optimizer in action can be magical; even if you understand each individual transformation it uses, often it will find surprising combinations that yield unexpected plans. Even for the relatively simple query shown above, the optimizer applies 12 transformations to reach the final plan. Below is a diagram showing 4 of the key transformations. You can see that the filter conditions get “pushed down” into the join and then become part of the scan operator for maximum performance. During the final transformation, the optimizer decides to use a lookup join with a secondary index to satisfy the query. As of this writing, the cost-based optimizer implements over 160 different transformations, and we expect to add many more in future releases. And because transformations lie at the heart of the new optimizer, we spent a lot of time making them as easy as possible to define, understand, and maintain. To that end, we created a domain specific language (DSL) called Optgen to express the structure of transformations, along with a tool that generates production Go code from that DSL. Here is an example of a transformation expressed in the Optgen language: [MergeSelectInnerJoin, Normalize]\n( Select $input:(InnerJoin $left: * $right: * $ on : * )\n\t$filter: * ) => (InnerJoin\n\t$left\n\t$right\n\t(ConcatFilters $ on $filter)\n) This transformation merges conditions from a WHERE clause with conditions from the ON clause of an INNER JOIN . It generates ~25 lines of Go code, including code to ensure that transitively matching transformations are applied. A future blog post will delve into more Optgen specifics, as there’s a lot there to cover. If you can’t wait for that, take a look at the documentation for Optgen . You might also take a look at some of our transformation definition files . If you’re especially ambitious, try your hand at crafting a new transformation that we’re missing; we always welcome community contributions. The Memo I’ve explained how the optimizer generates many equivalent plans and uses a cost estimate to choose from among them. That sounds good in theory, but what about the practical side of things? Doesn’t it take a potentially exponential amount of memory to store all those plans? The answer involves an ingenious data structure called a memo . A memo is designed to efficiently store a forest of plan trees for a given query by exploiting the significant redundancies across the plans. For example, a join query could have several logically equivalent plans that are identical in all ways, except that one plan uses a hash join, another uses a merge join, and the third uses a lookup join. Furthermore, each of those plans could in turn have several variants: in one variant, the left join input uses the primary index to scan rows, and in another variant it uses a secondary index to do the equivalent work. Encoded naively, the resulting exponential explosion of plans would require exponential memory to store. The memo tackles this problem by defining a set of equivalence classes called memo groups, where each group contains a set of logically equivalent expressions. Here is an illustration: To build a plan, pick any operator from group #1, then pick its left input from group #2 and its right input from group #3. No matter which you pick, you have a legal plan, since operators in the same group are guaranteed to be logically equivalent. Simple arithmetic reveals that there are 12 possible plans (3 * 2 * 2) encoded in this memo. Now imagine a complex reporting query with a 6-way join, complex aggregation, and lots of filter conditions. The number of plans might number in the thousands, and yet would be encoded in much less space than you would expect if you weren’t aware of the memo structure. Of course the optimizer does not just randomly pick one of the possible plan trees from the memo. Instead, it tracks the lowest cost expression in each memo group, and then recursively constructs the final plan from those expressions. Indeed, the memo is a beautiful data structure, reminding me of the illuminati diamond ambigram from Dan Brown’s novel “Angels and Demons”. Both encode more information than seems possible. Conclusion Our team plans to make future blog posts on the internals of the cost-based optimizer in CockroachDB. I’ve only scratched the surface in this post. Let us know if you’d like us to cover specific topics of interest to you. Or even better, come join Cockroach Labs , and help us build a planet-scale ACID database. And if building a distributed SQL engine is your jam, then good news — we're hiring! Check out our open positions here .", "date": "2018-11-08"},
{"website": "CockroachLabs", "title": "Flex Fridays: Lakshmi Kannan discovers what it takes to run a 99.999 system", "author": ["Lakshmi Kannan"], "link": "https://www.cockroachlabs.com/blog/free-fridays-lakshmi-kannan/", "abstract": "What is your role at Cockroach Labs? At Cockroach Labs, I am one of four product managers on the Product team. At a high level, I work on increasing the market share of CockroachDB by making it easy to access and run CockroachDB. More specifically, I work on building out a CockroachDB Cloud offering that takes the day to day operations of running a distributed database out of the hands of our users, to truly “make data easy.” What do you do day to day at Cockroach Labs? I usually try to do a few hours of heads down, focused work (like reading, writing, or research into a new technical topic or feature we are building) first thing in the morning between 7am and 10am. The actual day varies a lot with meetings spread throughout the day. As a PM, I sit on many customer calls where I try to understand common use cases, feature requests, what issues users are facing with the product, and generally collect as much feedback as possible. I use this data to make decisions and evaluate trade-offs on what we build and when. I also work closely with our design colleagues, as we think through the ideal user experience for a database-as-a-service, and also collaborate with sales and marketing on pricing and go-to-market for the offering. And finally, I measure how our features and clusters are doing, where users fall off in their process of using CockroachDB and try to improve their experience to get wide adoption. What does Flex Friday mean to you? When I first started at Cockroach Labs, I used Flex Fridays to go deep on various topics in distributed systems and databases. For example, I spent the first few Flex Fridays reading and reviewing concepts from this great book that Nate, our VP of Product, recommends all PMs read. It is very important to me that Fridays remain a no-meetings day. This allows me to go deep on whatever task I am working on without interruptions or distractions. I highly value the space and time to do this kind of deep work that is very hard during the week. What is your current Flex Friday project? My current Flex Friday project entails being an AWS expert. Building a database-as-a-service product requires understanding cloud infrastructure well, so I am taking an AWS solutions architect exam at the end of 2018 which is really just a motivation to get my hands dirty with cloud provider offerings. It’s been a great way to further my skill sets for my current role, but also build expertise and professional development for the long term. What else have you done on a Flex Friday? My partner lives in DC, and I work in the NYC office Monday through Thursday which means I don’t see her during the week. So sometimes, we’ll take a half day to go hiking or travel elsewhere for the weekend. Describe a typical Flex Friday. In the past, I have tried running a secure CockroachDB cluster in AWS and GCP to understand what it takes to run a 99.999 system. Since we were in the early stages of building a cloud offering, getting my hands dirty with the ins and and outs of load balancers, networking, key rotation, encryption, security and certificates etc. in each cloud provider was very valuable for my “day” job. How does Flex Friday support your long term goals? Flex Fridays are integral to my long term career goals of being a great PM. I use the time for professional development, and tinkering with things that I wouldn’t otherwise have time to during the week. Secondly and perhaps even more importantly, the flexibility of not having to come into the office on Fridays means that I spend more than half the week in DC which is important for my overall personal life and happiness. Please share any other topics that may be related to Flex Friday or that you want to share. I view Flex Fridays for my career development as going to the gym regularly or getting enough sleep for my health. While in the short/medium term, you might not see the difference, in the long term, it makes you a healthier, and happier person. Similarly, Flex Fridays make me a better PM and employee in the long run.", "date": "2018-11-20"},
{"website": "CockroachLabs", "title": "CockroachDB 2.1 is now 50x more scalable than Amazon Aurora", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-2dot1-performance/", "abstract": "[For CockroachDB's most up-to-date performance benchmarks, please read our Performance Overview page ] Correctness, stability, and performance are the foundations of CockroachDB. Today, we will demonstrate our rapid progress in performance and scalability with CockroachDB 2.1. CockroachDB is now 50x more scalable than Amazon Aurora at less than 2% of the price per tpmC. And unlike Aurora and other databases that selectively degrade isolation levels for performance, CockroachDB can achieve massive scale while maintaining serializable isolation , protecting your data from fraud and data loss. Read on to see benchmarked metrics that demonstrate that CockroachDB can provide customers an ultra-resilient and highly available database at massive scale. TPC-C: Benchmarking for OLTP Databases Cockroach Labs measures performance through many diverse tests, including the industry standard OLTP benchmark TPC-C , which simulates an ecommerce or retail company. In April we introduced our readers to TPC-C , publishing our first TPC-C benchmark metrics detailing our throughput performance as measured by transactions per minute (tpmC). We expanded upon this blog by publishing TPC-C results at scale . Finally, scaling is important, but at what cost? We published a follow-on cost comparison that showed that CockroachDB not only scales better than Amazon Aurora, but it does so at a cheaper price. In the following post we will expand upon these three foundational dimensions and show that CockroachDB 2.1 is now 50x more scalable than Amazon Aurora. Updated Metrics Benchmarking Scale and Transactional Throughput with TPC-C CockroachDB 2.1 can hit an incredible 631K tpmC at TPC-C 50K! In fact, we suspect we could easily push CockroachDB 2.1 even further as we achieved these results at 98% of the max possible efficiency for TPC-C 50k. We compared our unofficial TPC-C results to Amazon Aurora RDS unofficial TPC-C results from AWS re:Invent 2017. We also used Aurora’s SIGMOD 2017 paper for additional information as to their test setup and load generator. As such, based upon their last published metrics, CockroachDB is now 50 times more scalable than Amazon Aurora (a 5x increase from our CockroachDB 2.0 ), supporting 25 billion rows and more than 4 terabytes of frequently accessed data. CockroachDB 2.1 Amazon Aurora Max Throughput 631851 tpmC 12582 tpmC Max Warehouses with Max Efficiency 50000 Warehouses 1000 Warehouses Max Number of Rows 24.9B 0.499B Max Unreplicated Data 4TB 0.08TB Machine type c5d.4xlarge r3.8xl Unlike Amazon Aurora, CockroachDB achieves this performance in serializable mode, the strongest isolation mode in the SQL standard. Like many other databases, Aurora selectively degrades isolation levels for performance, leaving your business susceptible to fraud and data loss . KV: Another Way to Benchmark Scale Another way to measure scale is to compare what happens to throughput and latency as we increase the number of nodes. We ran a simple benchmark named KV (95% point reads, 5% point writes, all uniformly distributed) on an increasing number of nodes to demonstrate that adding nodes increases throughput linearly while holding p50 and p99 latency constant. We used KV in addition to TPC-C because it’s easier to demonstrate performance as nodes increase. TPC-C is instead designed to increase performance as warehouses increase, which is a related but orthogonal concept to adding nodes to a cluster. Improving Overall Efficiency: 3-Node Performance While we have many customers pushing CockroachDB to scale (see Baidu ), we recognize that many deployments don’t require global scale. This is why it’s important for us to push our performance on a 3-node cluster to efficiently provide value to all customers. Gains in efficiency directly translate to cost-savings as fewer resources (e.g. nodes) are needed to support the same throughput. Any efficiency gains we make on a 3-node cluster affect every workload--not just those customers already operating at global scale. We’ve improved our 3-node performance by increasing max supported warehouses from 1,300 to 2,300 warehouses in 2.1. This translates to an increase of 76.9% in TPC-C warehouses. CockroachDB 2.1 saves you money by efficiently providing more throughput for the same number of resources. Cost: CockroachDB 2.1 costs only 1.8% of Amazon Aurora We’ve improved our cost metrics as well. Because of efficiency improvements, CockroachDB 2.1 only needs 18 nodes to run TPC-C 10K, down from requiring 30 nodes in CockroachDB 2.0. This translates to a cost savings of 66%! CockroachDB 2.1 costs only 1.8% of Amazon Aurora per tpmC TPC-C 10K* CockroachDB 2.1 Amazon Aurora Max Throughput (tpmC) 124,262 12,582 Estimated Cost $131,032 $266,227 - $546,163 Price per tpmC $1.05 $28.30 -$58.07** * Amazon Aurora has not attempted TPC-C 50K so we kept our comparisons to 10K (and provided their highest observed tpmC when, in fact, they only achieved 9,406 tpmC) ** Amazon Aurora did not publish their iOPS number, a key lever in their cost model. As such we provided a range of costs based on iOPS ranging from 10k - 30k). Another way to conceptualize these cost savings is that CockroachDB 2.1 can now achieve TPC-C 2,200 on a 3-node cluster (as opposed to only 1,300 in 2.0). This means that we’ve improved our price per performance by the same 66% (from $3.08 to $1.85) on a 3-node cluster. Summary CockroachDB can achieve massive scale while protecting your data from fraud and data loss. In fact, CockroachDB 2.1 uses a more sophisticated fraud protection (Serializability) than Amazon Aurora and still out-scaled Aurora by 50x at less than 2% of the price per tpmC. Click here to learn more about how CockroachDB can provide you a managed cluster that can provide a hassle-free way to achieve these benefits and more. Note: We recently transitioned from collecting CockroachDB performance numbers in GCE to AWS. In a forthcoming blog post we will dive into the reasons behind this decision--stay tuned!", "date": "2018-11-28"},
{"website": "CockroachLabs", "title": "AWS Outperforms GCP in the 2018 Cloud Report", "author": ["Masha Schneider", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/2018_cloud_report/", "abstract": "[THE 2021 CLOUD REPORT IS AVAILABLE. READ IT HERE] Note: As of December 20, 2018, we have updated two sections in this report: Network Throughput and I/O Experiment. The updates do not change the relative performance of either cloud — however, it does narrow the gap. Our customers rely on us to help them navigate the complexities of the increasingly competitive cloud wars. Should they use Amazon Web Services (AWS)? Google Cloud Platform (GCP)? Microsoft Azure? How should they tune their workload for different offerings? Which is more reliable? We are committed to building a cloud neutral product, and we run test clusters on all three leading US cloud providers. As we were testing features for our 2.1 release, we noticed something interesting: AWS offered 40% greater throughput than GCP. We were curious as to why AWS offered such a stark difference in throughput, and set out to test the performance of GCP and AWS in more detail. Ultimately, we compared the two platforms on TPC-C performance (e.g., throughput and latency), CPU, network, I/O, and cost. This inspired what has become the 2018 Cloud Report . Our conclusion? AWS outperforms GCP on nearly every criteria we tested — including cost. Note: We did not test Microsoft Azure due to limits on internal resources, but we plan to do so in the future. Machine Types Tested in the 2018 Cloud Report GCP has a variety of instance types (including standard and high CPU) but we focused on the n1-standard-16 machine with Intel Xeon Scalable Processor (Skylake) in the us-east region . We were familiar with this instance type as we used it to conduct our previous performance benchmarking. A similar configuration isn’t quite as trivial as it sounds for AWS. AWS has more flavors of instances than GCP. It has the standard high CPU and general instances. We chose the latest compute-optimized AWS instance type, c5d.4xlarge instances, to match n1-standard-16 , because they both have 16 cpus and SSDs (although AWS only offers 32 GB of RAM as compared to 60 GB of RAM on GCP) within the us-east-2 region. For those readers not familiar with AWS, the first letter c corresponds to machine type; the number 5 is the generation; d corresponds to SSD; and the 4xlarge corresponds to CPU. Each of the most popular AWS machine types tested here varies the machine type, the generation, SSD or EBS, but not the CPU when evaluating AWS on TPC-C performance. Experiments Run in the 2018 Cloud Report We designed our experiment to first tease out whether or not AWS and GCP performance differed on a simulated customer workload. We started with a customer workload (and not micro-benchmarks) because it most directly simulates real-world customer behavior. It was only after observing differences in applied workloads that we moved onto micro-benchmarks like CPU, network, and I/O performance. Differences in micro-benchmarks matter more when informed by the knowledge that the overall customer workload performance of the platforms differ. CPU, network, and I/O all represent separate hypothesis for why performance might vary between GCP and AWS. TPC-C Performance Note: these results were collected using nobarrier. See the \"Why nobarrier Matters\" section below for a detailed explanation. We chose to test workload performance by using TPC-C , a popular OLTP benchmark tool that simulates an e-commerce business, given our familiarity with this workload . CockroachDB 2.1 achieves 40% more throughput (tpmC) on TPC-C when tested on AWS using c5d.4xlarge than on GCP via n1-standard-16 . We were shocked that AWS offered such superior performance. Previously, our internal testing suggested more equitable outcomes between AWS and GCP. We decided to expand beyond the c5 series to test TPC-C against some of the most popular AWS instance types. At first blush, it appears that SSDs offered by c5d and m5d outperform EBS. Unfortunately, it’s a bit more complicated than that as AWS offers EBS out of the box with gp2 volume types rather than the higher performing io1 volume type . To isolate this change, we focused on the higher performing c5 series with SSDs, EBS-gp2, and EBS-io1 volume types: Clearly, EBS volumes can offer effective performance if tuned to the io1 volume type and provided with sufficient iOPS. So if the difference in TPC-C performance observed among various AWS instance types isn’t explained by SSD vs. EBS, what else might explain it? AWS recently introduced their new Nitro System present in c5 and m5 series. The AWS Nitro System offers approximately the same or superior performance when compared to a similar GCP instance. The results were clear: AWS wins on TPC-C benchmark performance. But what causes such large performance differentials? We set out to learn more by testing a series of micro-benchmarks on CPU, network, and I/O. CPU Experiment We began our testing like any aspirational scientists by seeking to disprove our main hypothesis: that cloud platforms' different provisioning policies might affect CPU performance. We focused on a CPU performance microbenchmark first as it can have a large impact on performance. To test CPU performance, we needed a 3rd party easy to use benchmark. The two most frequently used benchmark test suites in the market today are sysbench and stress-ng . We chose stress-ng because it offered more benchmarks and provided more flexible configurations than sysbench. We ran the following Stress-ng command five times on both AWS and GCP: stress-ng --metrics-brief --cpu 16 -t 1m AWS offered 28% more throughput (~2,900 bogo ops/s) on average on stress-ng than GCP. This is a credit to the investments made by both platforms as unpredictability can have a material cost for business paid in the over-provisioning of virtual machines. Note that the Skylake series (used in this experiment) offers a marginal 4% improvement over standard hardware on GCPs n1-standard-16. Now that we observed an initial difference in both CPU performance on GCP and AWS, we couldn’t help ourselves from continuing to investigate other potential differences. Was the entirety of the TPC-C difference generated from the advantage in CPU performance? Network Experiment Next, we tested network throughput and latency. To test the network, we measured throughput using a popular tool called iPerf and latency via another popular tool ping . iPerf’s configurations include a buffer data size (128KB), a protocol, a server and and a client. iPerf attempts to connect the client and the server with the data from buffer size via the protocol. We set up iPerf similarly to this blog post . This test provides a throughout for the network which allows for us to compare the performance of the network on AWS and GCP. We ran the test four times each for AWS and GCP and aggregated the results of all four tests in histograms (each 1 sec run is stacked to form this chart): Note: An earlier version of this test showed GCP network throughput using an n1-standard-4 machine type from early November rather than the n1-standard-16 used throughout the rest of the report. The original data from n1-standard-4 offered much lower throughput (coupled with a greater variance) as compared to the n1-standard-16 . GCP shows a skewed left normal distribution of network throughput centered at ~8.6 Gb/sec. In addition to the raw network throughput, we also care about the variance of the network throughput so that we can have consistent expectations for the network performance. Throughput ranges from 7.62 Gb/sec to 8.91 Gb/sec — a somewhat unpredictable spread of network performance, reinforced by the observed average variance for GCP of 0.17 Gb/sec. AWS, on the other hand, offers higher throughput, centered on 9.6 Gb/sec, while providing a tighter spread between 9.60 Gb/sec and 9.63 Gb/sec when compared to GCP. On AWS, iPerf transferred a total network throughput of 2,296 Gb in 60 seconds across all four runs. This is an increase of 11% over GCP. On average this is more than 1 Gb/sec increase in throughput. What about network throughput standard deviation? On AWS, the standard deviation is only 0.006 Gb/sec. This means that the GCP network standard deviation of 0.17 Gb/sec is ~27x more than on AWS. We tested network latency, in addition to the throughput and variance. Without testing for latency we can miss significant delays in service that may be masked by overall performance. We used the industry standard tool ping to measure latency. Like network throughput, AWS has a tighter network latency than GCP. Looking at the data closely, we can see that there are several outliers such that the max latency, 1.25 ms, is more than 5 times the average! Similarly, to network throughput, AWS offers a stark difference to GCP. AWS’s values are centered on an average latency, 0.057 ms. In fact the spread is so tight it can’t be visualized on the same scale as GCP. The max latency is only 0.077 — a difference of only .02 ms (or 35%) from the average! AWS offers significantly better network throughput and latency with none of the variability present in GCP. Further, it looks like Amazon may be racing further ahead in network performance with the introduction of the c5n machine type that offers significantly higher network performance across all instance sizes as compared to the rest of the c series. I/O Experiment Note: Many readers expressed interest in further understanding our observations with and without nobarrier . We have adapted this original post to expand upon the published numbers without nobarrier to include the with nobarrier numbers. In this section, we investigate the maximum I/O performance attainable when the application is able to tolerate unreliable writes to its disk (e.g., when VMs are ephemeral). We tested I/O with the same machine types as above (using c5d.4xlarge and n1-standard-16 ) using a configuration of sysbench that simulates small writes with frequent syncs for both write and read performance. This test measures throughput and latency based on a fixed set of threads, or the number of items concurrently writing to disk. We ran this experiment with nobarrier and without nobarrier (don’t you just love double negatives?). Why nobarrier Matters As a refresher, nobarrier is a method of writing directly to disk without guaranteeing that writes will be persisted in the same order in which they were performed. In many cases, nobarrier can offer superior performance, but at the risk of data loss. In the event of a power failure, data on disk can be corrupted causing you to lose meaningful data. In traditional deployments, nobarrier is used with battery-backed write caches. In cloud environments it is difficult to tell exactly what hardware is used, but it's also unnecessary: local SSDs do not survive a reboot of the host machine, so it is safe to set nobarrier on a cloud local SSD. We conduct most of our performance tests using nobarrier to demonstrate the best possible performance but understand that not all use cases can support this option. As a result, we tested I/O with and without nobarrier on GCP and AWS. Note, nobarrier has no impact on CPU or network testing and was not used to conduct those experiments. First, we tested write performance: AWS consistently offers more write throughput across all thread variance from 1 thread up to 64 both with nobarrier and without. In fact, it can be as high as 67x difference in throughput on 1 thread throughput without nobarrier . It’s also easy to observe that GCP benefits from nobarrier to a much greater degree than AWS. It’s unclear why this difference between cloud providers exists on nobarrier , and we will avoid speculating but do hope to learn more. AWS also offers better average and 95th percentile write latency across all thread tests with and without nobarrier . AWS held clear advantages in write throughput and latency. What about read throughput and latency? AWS provides more read throughput from 1 to 8 threads with and without nobarrier . nobarrier begins to make a difference at 16 threads tipping the advantage to GCP and marginally increases the advantage GCP has at higher thread count. Similarly to read throughput, AWS wins the read latency battle up to 16 threads. At 32 and 64 threads GCP and AWS split the results with and without nobarrier . Overall, AWS wins for write performance at all threads and read performance up to 16 threads. GCP offers a marginally better performance with similar latency to AWS for read performance at 32 threads and up which can be improved upon by using the nobarrier option. Note: We evaluated the I/O performance of storage using locally-attached SSD devices. Results for durable network storage devices are likely to be different and should be considered independent of these findings. Cost On applied benchmarks (e.g., TPC-C) and the more descriptive micro-benchmark — CPU, network, and I/O — AWS outperformed GCP. But at what cost? Do you pay for this increased performance on AWS? Let’s circle back to the TPC-C setup discussed at the beginning. For TPC-C, we used n1-standard-16 on GCP with local SSD and c5d.4xlarge on AWS. For both clouds we assumed the most generous discounts available: On GCP we assumed a three-year committed use price discount with local SSD in the central region. On AWS we assumed a three-year standard contract paid up front. Not only is GCP more expensive than AWS, but it also achieves worse performance. This is doubly reflected in the price per performance (below), which shows GCP costing ~2 times more than AWS per tpmC ( the primary metric of throughput in TPC-C )! Conclusions of the 2018 Cloud Report AWS outperformed GCP on applied performance (e.g., TPC-C) and a variety of micro-benchmarks (e.g, CPU, network, and I/O) as well as cost. CockroachDB remains committed to our stance as a cloud-agnostic database. We will continue to use GCP, AWS, Microsoft Azure, and others for internal stability and performance testing. We also expect that these results will change over time as all three companies continue to invest in the modern infrastructure ecosystem. Note, the 2018 Cloud Report focused on evaluating AWS and GCP because they are the most popular cloud platforms among our customers. In future editions, we plan to expand upon our testing with Microsoft Azure, Digital Ocean, and other cloud platforms. [THE 2020 CLOUD REPORT IS AVAILABLE. READ IT HERE]", "date": "2018-12-13"},
{"website": "CockroachLabs", "title": "Why we built CockroachDB on top of RocksDB", "author": ["Arjun Narayan", "Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/", "abstract": "If, on a final exam at a database class, you asked students whether to build a database on a log-structured merge tree (LSM) or a BTree -based storage engine, 90% of your students would probably respond that the decision hinges on your workload. \"LSMs are for write-heavy workloads and BTrees are for read-heavy workloads\", the conscientious ones would write. If you surveyed most NewSQL (or distributed SQL) databases today, most of them are built on top of an LSM, namely, RocksDB. You might thus conclude that this is because modern applications have shifted to more write-heavy workloads. You would be incorrect. The main motivation behind RocksDB adoption has nothing to do with its choice of LSM data structure. In our case, the compelling drivers were its rich feature set which turns out to be necessary for a complex product like a distributed database. At Cockroach Labs we use RocksDB as our storage engine and depend on a lot of features that are not available in other storage engines, regardless of their underlying data structure, be it LSM or BTree based. What is a storage engine? A storage engine's job is to write things to disk on a single node. For many databases — which may be single node databases themselves — this constitutes a large part of the engineering effort, and so is pretty intricately tied to the engineering effort of the database building itself. However, in a distributed setting, the distribution, replication, and transaction coordination parts are an added engineering complexity, that at Cockroach Labs we began to look to a mature storage engine product we could build on, rather than building one from scratch. So what exactly is a storage engine? A simple first answer, considering the big asks for any database - Atomicity, Consistency, Isolation, and Durability, or ACID - is that the storage engine's responsibility is A tomicity and D urability. This frees up the higher layers of the database to focus on the distributed coordination required to get strong I solation guarantees like serializability , and providing the C onsistency primitives needed to ensure data integrity . Beyond atomicity and durability though, a storage engine has another big job: performance. The performance characteristics of a storage engine determine to a large extent the ceiling on the performance of the entire database. For example: almost every storage engine has a write-ahead log for quickly making a write durable, but which will later get moved to the main indexed data structure. This is a performance optimization that is pretty vital, but also tricky to get right, and involves delicate performance tradeoffs, as committing writes to the write-ahead-log while maintaining high concurrency is tricky. Storage engines also have to provide a defined isolation model (e.g. that your reads will reflect writes that are still in the write-ahead log and will be as of some single point-in-time snapshot), while supporting many concurrent operations. While that isolation model might be simpler than the overall isolation provided by the database (for instance, RocksDB provides snapshot isolation, while CockroachDB does extra bookkeeping to provide serializability), maintaining it at high performance is still complex. If this sounds like a lot of the work that goes into building a database, it is! For instance, Postgres doesn't really have a defined \"storage engine\" - it's all one monolithic system. But for a distributed database, a single node's storage engine is a smaller part of the larger distributed system, so let's take a deeper dive into understanding how we use RocksDB in CockroachDB, paying attention to the lesser-known features that we use. A RocksDB primer RocksDB is a single-node key-value storage engine. The design is based on log-structured merge trees (LSMs). RocksDB is a fork of an earlier Google project called LevelDB, which was an embedded key-value store inspired by the low-level storage engine used by BigTable. RocksDB has since gone on to become a much more robust and feature complete storage engine, but the basic structure is the same as LevelDB and many other LSM-based storage engines. In RocksDB, keys and values are stored as sorted strings in files called SSTables . These SSTables are arranged in several levels . Within a single level, SSTables are non-overlapping: one SSTable might contain keys covering the range [a,b) , the next [b,d) , and so on. The key-space does overlap between levels: if you have two levels, the first might have two SSTables (covering the ranges above), but the second level might have a single SSTable over the keyspace [a,e) . Looking for the key aardvark requires looking in two SSTables: the [a,b) SSTable in Level 1, and the [a,e) SSTable in Level 2. Each SSTable is internally sorted (as in the name), so lookups within an SSTable take log(n) time. SSTables store their keys in blocks , and have an internal index , so even though a single SSTable may be very large (gigabytes in size), only the index and the relevant block needs to be loaded into memory. The levels are structured roughly so that each level is in total 10x as large as the level above it. New keys arrive at the highest layer, and as that level gets larger and larger and hits a threshold, some SSTables at that level get compacted into fewer (but larger) SSTables one level lower. The precise details of when to compact (and how) greatly affect performance; Leveled Compaction is one compaction strategy, but for an exhaustive resource, Mark Callaghan's blog has more descriptions of the various compaction algorithms and the tradeoffs between them (see here , here , and here ). Above the on-disk levels is an in-memory memtable . The memtable is a sorted in-memory data structure (we use a concurrent skiplist, although RocksDB has several options), which makes reads cheap, but is persisted as an unsorted Write-Ahead-Log (WAL). If a node crashes, on startup, the durable WAL is read back and the memtable is reconstructed. As this data-structure grows with more writes, it needs to be flushed to disk. This eviction can be a critical bottleneck during sustained write throughput. In order to make memtable flushes cheap, L0 is special: its SSTables are allowed to overlap. This, however, makes L0 a critical bottleneck for compactions and read performance — it is usually compacted in large chunks into L1, and every table in L0 increases read amplification. Thus, as you can see, writes create deferred write amplification, in the form of eventual compactions that will eventually push the keys down the hierarchy of levels. Bursty write workloads can accommodate a lot of writes. Sustained writes will require dedicating some portion of IO bandwidth to performing some compactions concurrently (a decent amount of the motivation for the initial RocksDB project was in using multicore concurrency to solve this problem more efficiently than in LevelDB). Translating higher level SQL operations into K and V operations CockroachDB is a distributed SQL database. These SQL operations are translated down into key and value operations over a single logical keyspace. This logical keyspace is sharded into physical 'ranges' of keys, and each range is replicated across three (or more) Cockroach nodes. Given this structure, a given SQL operation gets turned into a set of key value operations, which are spread across multiple machines. At a given machine, these KV operations thus need to be performed on the underlying storage engine. This API is relatively simple, since RocksDB also provides a key-value interface. But what exactly do we mean by 'key-value interface'? This lack of standardization hides a lot of subtle detail: this interface is more than just put , get , and delete operations on the keys. RocksDB also supports scans over a range [start, end) . Also consider other important operations: delete a range over [start,end) , and bulk ingest a set of keys and values. Having your storage engine do these operations in a performant fashion is critical, as otherwise some SQL operations can become very slow. Let's cover why these are so critical to a distributed database like CockroachDB. Fast scans One surprising part of engineering CockroachDB is the realization that scans are more frequent than you would think! Many academic papers use put/get operations for testing storage engine performance, such as by using YCSB as a storage engine benchmark. However, in CockroachDB, put/scan are the two most dominant operations because of the higher level guarantees we provide as a serializable SQL database. Consider multi-version concurrency control ( MVCC ) - Cockroach stores multiple values of a given key, and also stores the timestamp at which each key was written. Transactions happen at a particular timestamp, and are guaranteed to read the latest value as of that timestamp. Thus, what may appear to be a GET operation at the database level (e.g. SELECT * from tablename WHERE primarykeycol = 123 translates into a GET for the value of the key that stores that row), turns into a SCAN at the storage engine level ( SCAN for the newest value of that key). Thus each CockroachDB key is annotated with a timestamp to create the equivalent RocksDB key. And updates to a key create additional RocksDB keys. The use of MVCC causes additional complications. Many key-value storage engines have fast GET operations, but slower SCAN operations. Consider any log-structured merge tree (LSM) implementation: a particular key can be in any of the levels of the LSM. This means that every GET operation has a read amplification factor (RAF) of the number of levels (One logical read = RAF disk reads). In order to alleviate this log(n) multiple, storage engines typically use bloom filters on SSTs. Bloom filters are probabilistic data structures that are small enough to keep in memory. A bloom filter answers the question: \"is a given key present in this level?\" with either a \"no\" or \"maybe\" (if it's a \"maybe\", you have to perform the read to find out the answer, but a \"no\" means you can skip reading). Unfortunately, bloom filters are per-key. A SCAN involves a potentially infinite keyspace between two endpoints, so the bloom filter cannot be used to rule out levels to scan. That is unless you can pre-process keys when constructing your the bloom filter. RocksDB has a feature that elegantly does this - prefix bloom filters , which allows the bloom filter to be constructed on a prefix of a key. This means that a scan that is over that prefix can benefit from using the bloom filter. Without this, a storage engine will have to scan every level on every logical GET operation, a huge performance hit. Prefix bloom filters, like RocksDB provides, are thus table stakes for an MVCC database like CockroachDB. RocksDB Snapshots Cockroach's distributed replication means that occasionally, a new node needs to be brought up to speed with a copy of some data. This requires that a large chunk of the keyspace be scanned, and sent over the network to the new node. Depending on the size of the data and the speed of the network, this operation can take a decent amount of time (from a few seconds to tens of seconds). If it takes a while, Cockroach has two options: do the scan over a long period of time, or do the entire scan and hold the data separately until the transmission is complete. Most storage engines, including RocksDB, provide the feature that any given scan operation is done over a consistent 'snapshot' of the database — writes done after the scan has started will not be reflected in the scan operation. This isolation guarantee at the storage level is a useful building block for the database to build on top of. It also comes in use in sending these snapshots for replication, but the challenge comes in providing the snapshot functionality without using too many resources. In particular, reading a snapshot pins memtables during the read operation, which slows down incoming writes. Naively pinning memtables can be expensive, as is the alternative of reading out the snapshot to free up the storage engine, only to then have to hold those keys in memory at a higher level until the transmission completes. This problem is exacerbated by the fact that CockroachDB shards the keyspace into many ranges — each up to 64mb. This means that when a new node comes up, it is likely to cause many snapshots to be created and transmitted across the entire cluster before it fills up to parity with the other nodes. And all of this while the cluster itself has to continue normal operation. RocksDB provides an alternate middle ground -- the explicit snapshot. Explicit snapshots do not pin memtables. Instead, they are a sort of placeholder that informs the rest of the storage engine not to perform compactions that would compact across the time boundary of that snapshot. Holding an explicit snapshot, even for hours, does not consume additional resources (apart from preventing some compactions that might make your storage layout more efficient). When you're ready to iterate over the data, you create an implicit snapshot (which might have to pin memtables for the duration of the iterator), but this way, you don't have to hold on to an expensive implicit snapshot for a long time. Blitzing through more RocksDB features We use a lot of RocksDB features in CockroachDB, and covering all of them exhaustively would take a lot more pages! So here's a short preview into many RocksDB features that we use: SSTable ingestion During restoration of a backup, we want to ingest files that contain keys that span a portion of the keyspace which we can guarantee is empty when we begin the ingestion. Using the normal write path is wasteful --- the normal write path involves writing to high levels of the LSM, and then compacting down, which involves a lot of write amplification. If we know this keyspace is empty, we can simply pre-construct SSTables at a low level that are guaranteed to not overlap the other SSTables. This greatly improves restore throughput. This is a fairly critical feature for CockroachDB, and thus ingesting of SSTables into RocksDB is a key feature. Custom key comparators The MVCC timestamp suffix that we tack onto keys could be encoded in such a way that lexicographical comparison (byte for byte) is equivalent to logical comparison. But such an encoding is slower to encode and decode than one which does not compare lexicographically. RocksDB comes to the rescue again by allowing a custom comparator to be defined for the key. This enables an encoding of the MVCC timestamp that is quick to encode and decode, while also allowing the correct ordering to be maintained (timestamps are sorted in descending order since we want the latest version of the key to sort first so that scans scan stop at the first match). Efficient custom comparators between keys are thus a feature we need from RocksDB. Range Deletion Tombstones Efficiently deleting an entire range of keys is required, otherwise operations like DROP TABLEs or simply moving a chunk of data between nodes can block for really long periods of time. (Non-engineers: in a computer, a move is always implemented as a copy followed by a delete). RocksDB achieves this by performing delete range operations by writing a range deletion tombstone . When reading a key, if a range tombstone that covers that key is read at a higher level than a concrete value for that key, then the key is considered deleted. The actual deletions in the SSTables are performed during compaction. Backwards iteration Backwards iteration makes queries like SELECT * FROM TABLE ORDER BY key DESC LIMIT 100 efficient, even if the index on key is ordered ascending. Some storage engines do not provide the ability to iterate backwards, which makes those queries inefficient. It's worth noting that backwards iteration is always going to be more expensive than forward iteration without changes to the underlying data layout. RocksDB abstracts away the complexities from the higher levels of the system. It is worth noting that there are opportunities for improved performance in this area. Indexed Batches Batches are the unit of atomic write operations (containing set, delete, or delete range operations). The basic batch is write-only. RocksDB also supports an \"indexed batch\" that allows reads from the batch. In a distributed database, some writes can take longer, as CockroachDB waits for remote operations to commit. However, other operations in the same transaction need to be able to read the pending writes. To support this, we rely on the storage engine to provide a mechanism for batching a set of updates that can be applied atomically, while also providing a means to read a merged view of those updates layered on top of the full database. RocksDB's support for indexed batches makes this much easier. Encryption Support For an upcoming feature -- encryption at rest -- we rely on RocksDB's modular support for encrypting SSTables . This does a lot of the heavy lifting of keeping the data encrypted, so that we can focus on key management, rotation, and the user-facing parts of supporting encryption at rest. RocksDB at CockroachDB today Today RocksDB is deeply embedded in Cockroach's architecture. Other storage engines that don't have the above features would require significant re-engineering on our part in order to adopt them and even then, would probably result in performance degradation. Promised performance increases on raw key/value access speed are quite likely to disappear once all these considerations are taken into account. That said, RocksDB is not all roses. After all, we've only gotten this performance after expending considerable effort in engineering RocksDB for our needs! There are also downsides to having a performance critical part of our codebase in C++ while the rest of the system is in Go. Crossing the CGo barrier is delicate - and involves a 70ns overhead per call. That sounds fast (a nanosecond is a billionth of a second), but we perform a lot of RocksDB calls. Minimizing the number of CGo crossings has a measurable impact! We now construct our entire batch of RocksDB operations (e.g. a set of PUTs) in Go, and then transfer them in a single CGo call for efficiency. We also incur performance penalties in having to copy values from C allocated memory into Go allocated memory. A Go-native storage engine could provide us many performance benefits, as well as streamlining our codebase, though simply using an existing Go-native storage engine is a bit of a non-starter given the requirements above (we're not aware of any which provide all of the functionality we need). Given the choice of implementing all of these delicate performance-critical features or engineering around minimizing CGo overhead, we've found the latter manageable so far, but we're keeping an eye on when this calculation changes. And finally, as surprising as it may seem, RocksDB includes still more features that we do not (yet?) use in CockroachDB, such as column families, FIFO compaction, backups and checkpoints, persistent caches, and transactions… Perhaps we'll find performance reasons to include them in upcoming versions of CockroachDB! If building distributed systems is your jam (and if you made it to the end of this post, it probably is), we've got good news for you: Cockroach Labs is hiring! Check out our open positions here .", "date": "2019-01-17"},
{"website": "CockroachLabs", "title": "CockroachDB's Consistency Model", "author": ["Andrei Matei"], "link": "https://www.cockroachlabs.com/blog/consistency-model/", "abstract": "A few days ago, prompted by a Hacker News post, my friend Ivo texted me saying \"Does your head ever explode when you're thinking about databases and consistency semantics and whatever models? It just sounds like pointless taxonomy stuff. We are <N, K>-serializable whereas QuinoaDB is only ü-serializable\". The answer is yes --- my head does explode. I don't think it's pointless, though, although I agree that the discussions are generally not exactly productive. Separately, the other day a colleague told a user that \"CockroachDB implements serializability, not linearizability\". While we say this statement often, and it is the best kind of correct, I don't like it much because I think it doesn't do us justice and it's also not particularly helpful for the users --- it doesn't teach them very much about CockroachDB. In this post, I'm attempting to present the guarantees that CockroachDB gives and the ones it doesn't, and offer my preferred marketing slogan summarizing it all. The first section provides background and some terminology for consistency models to support the following, CockroachDB-specific section. It's not formal, rigorous or exhaustive (I link to better sources, though) so readers who are familiar with these things might want to skip it and head straight to the section on CockroachDB's consistency model. Database consistency models recap First of all, a brief introduction to what we're talking about. Databases let many \"clients\" access data concurrently, and so they need to define the semantics of these concurrent accesses: for example, what happens if two clients read and write the same data \"at the same time\". Moreover, distributed and replicated databases generally store multiple copies of the data, usually over a network of machines, and so they need to define what complications can arise from the fact that different machines are involved in serving reads and writes to the same data: e.g. if I tell machine A to write a key, and then immediately after I ask machine B to read it, will machine B return the data that had been just written? Informally speaking, what we'd ideally want from our database is to hide the data distribution and replication from us and to behave as if all transactions were being run one at a time by a single machine. A database that provides this kind of execution is said to implement the \"strict serializability\" consistency model - that's the holy grail. But, of course, we also want our database to be resilient to machine failure, and we want the transactions to execute fast, and we want many transactions to execute at the same time, and we want data for European customers to be served from European servers and not cross an ocean network link. All these requirements generally come in conflict with strict serializability. So then databases start relaxing the strict serializability guarantees, basically compromising on that front to get execution speed and other benefits. These compromises need precise language for explaining them. For example, consider a replicated database and a write operation executed by one of the replicas followed quickly by a read operation served by another one. What are admissible results for this read? Under strict serializability, the answer is clear --- only the value of the preceding write is acceptable. Under more relaxed models, more values are allowed in addition to this one. But which values exactly? Is a random value coming out of thin air acceptable? Generally, no. Is the value of some other relatively recent write acceptable? Perhaps. To define things precisely, we need specialized vocabulary that's used by well studied sets of rules (called \"consistency models\"). Historically, both the distributed systems community and the databases community have evolved their own terminology and models for consistency. In more recent years, the communities have joined, driven by the advent of \"distributed databases\", and the vocabularies have combined. Things are tricky though, plus different databases try to market themselves the best way they can, and so I think it's fair to say that there's a lot of confusion on the topic. I've been thinking about these things for a couple of years now in the context of CockroachDB, and I still always struggle to make unequivocal and clear statements on the subject. Additionally, I'll argue that none of the standard lexicon describes CockroachDB very well. For a more systematic treaty on the different meanings of consistency, see The many faces of consistency and Jepsen's treatment of the topic . Transaction isolation levels and serializability The databases community has been describing behavior in terms of _transactions_ , which are composite operations (made up of SQL queries). Transactions are subject to the ACID properties ( A tomicity, C onsistency, I solation, D urability). This community was primarily interested in the behavior of concurrent transactions on a single server, not so much in the interactions with data replication --- it was thus initially not concerned by the historical issues around distributed consistency. For our discussion, the I solation property is the relevant one: we have multiple transactions accessing the same data concurrently and we need them to be isolated from each other. Each one needs to behave, to the greatest extent possible, as if no other transaction was interfering with it. Ironically, the C onsistency in ACID refers to a concept that's only tangentially related to what we're talking about here --- the fact that the database will keep indexes up to date automatically and will enforce foreign key constraints and such. To describe the possible degrees of transaction isolation, the literature and the ANSI standard enumerates a list of possible \"anomalies\" (examples of imperfect isolation), and, based on those, defines a couple of standard \"isolation levels\": Read Uncommitted, Read Committed, Repeatable Read, Serializable. To give a flavor of what these are about, for example the Repeatable Read isolation level says that once a transaction has read some data, reading it again within the same transaction yields the same results. So, concurrent transactions modifying that data have to somehow not affect the execution of our reading transaction. However, this isolation level allows the Phantom Read anomaly. Basically, if a transaction performs a query asking for rows matching a condition twice, the second execution might return more rows than the first. For example, something like select * from orders where value > 1000 might return orders (a, b, c) the first time and (a, b, c, d) the second time (which is ironic given Repeatable Read's name since one might call what just happened a non-repeatable read). Frankly, the definitions of the ANSI isolation levels are terrible (also see A Critique of ANSI SQL Isolation Levels ), arguably with the exception of the Serializable one. They have been defined narrow-mindedly with a couple of database implementations in mind and have not stood the test of time. The Serializable isolation level, which, as far as the SQL standard is concerned, is the gold standard, doesn't allow any of the defined anomalies. In plain terms, it states that the database needs to ensure that transactions need to behave as if the transactions executed sequentially, one by one. The definition allows that database to choose the order of transactions in an equivalent sequential execution. This is less than ideal because it allows for the following scenario: HN1: We consider three transactions. The first one is insert into hacker_news_comments (id, parent_id, text) values (1, NULL, 'a root comment') . The second one is insert into hacker_news_comments (id, parent_id, text) values (2, 1, 'OP is wrong') . The third one is select id, text from comments . I run transaction one. I yell across the room to my friend Tobi who's just waiting to reply to my threads. Tobi runs transaction 2. We then tell our friend Nathan to stop what he's doing and read our thread. He runs transaction 3 and gets a single result: (2, 'OP is wrong') . So, Nathan is seeing the response, but not the original post. That's not good. And yet, it is allowed by the Serializable isolation level and, in fact, likely to occur in many distributed databases (spoiler alert: not in CRDB), assuming the actors were quick to yell at each other and run their transactions. The serial order in which the transactions appear to have executed is 2, 3, 1. What has happened here is that the actors synchronized with each other outside of the database and expected the database's ordering of transactions to respect \"real time\", but the isolation levels don't talk about \"real time\" at all. This seems to not have been a concern for the SQL standardization committee at the time, probably since this kind of thing simply wouldn't happen if the database software runs entirely on one machine (however many database researchers were thinking about the issues of distributed databases as early as the 70s--for example, see Papadimitriou paper on serializability . Distributed systems and linearizability While database people were concerned with transaction isolation, researchers in distributed and parallel systems were concerned with the effects of having multiple copies of data on the system's operations. In particular, they were concerned with the semantics of \"read\" and \"write\" operations on this replicated data. So, the literature evolved a set of operation \"consistency levels\", with names like \"read your own writes\", \"monotonic reads\", \"bounded staleness\", \"causal consistency\", and \"linearizable\" which all give guidance about what values a read operation can return under different circumstances. The original two problems in need of solutions were how to resolve concurrent writes to the same logical address from two writers at separate physical locations using local replicas (CPUs on their local cache, NFS clients on their local copy), and when/how a stale copy should be updated (cache invalidation). The spectrum of possible solutions has been explored in different ways by the original communities: designers of memory caches were constrained by much tighter demands of programmers on consistency, whereas networked filesystems were constrained by unreliable networks to err on the side of more availability. Generally speaking, this evolutionary branch of consistency models doesn't talk about transactions. Instead, systems are modeled as collections of objects, with each object defining a set of operations it supports. For example, assuming we have a key-value store that provides the operations read(k) and write(k,v), the system obeys the \"monotonic reads\" model if, once a process reads the value of a key k, any successive read operation on k by that process will always return that same value or a more recent value. In other words, reads by any one process don't \"go backwards\". There's two things to note about this model's definition: first of all, it talks about a \"process\", so the system has a notion of different threads of control. Understanding this is a burden; the serializable isolation level we discussed in the databases context did not need such a concept[^1] --- the user of a system did not need to think about what process is performing what operations. Second, this model is quite relaxed in comparison to others. If one process performs a write(\"a\", 1) and later another process performs read(\"a\") (and there's no intervening writes to \"a\"), then the read might not return 1. The monotonic reads model describes various distributed systems where data is replicated asynchronously and multiple replicas can all serve reads. The gold standard among these models is linearizability. It was formalized by Herlihy and Wing in a delightful paper . This model aims to describe systems with properties pretty similar to the ones guaranteed for database transactions by the Serializable isolation level. Informally, it says that operations will behave as if they were executed one at a time, and an operation that finished before another one began (according to \"real time\") has to execute before the second one. This model, assuming systems can actually implement it efficiently, sounds really good. Let's definite it more formally. Usually, linearizability is defined at the level of a single, relatively simple \"object\" and then expanded to the level of a system comprised of many such objects. So, we have an object that affords a couple of operations, and we want to devise a set of rules for how these operations behave. An operation is modeled as an \"invocation\" (from a client to the object) followed by a \"response\" (from the object to the client). We're talking in a concurrent setting, where many clients are interacting with a single object concurrently. We define a \"history\" to be a set of invocations and responses. For example, say our object is a FIFO queue (providing the enqueue/dequeue operations). Then a history might be something like: H1: client 1: enqueue \"foo\" client 1: ok client 1: dequeue client 1: ok (\"foo\") client 1: enqueue \"bar\" client 2: enqueue \"baz\" client 1: ok client 2: ok client 1: dequeue client 1: ok (\"baz\") The first event in this history is an invocation by client 1, the second one is the corresponding response from the queue object. Responses for dequeue operations are annotated with the element they return. We say that a given history is \"sequential\" if every invocation is immediately followed by a response. H1 is not sequential since it contains, for example, this interleaving of operations: client 1: enqueue \"bar\" client 2: enqueue \"baz\" Sequential histories are easy to reason about and check for validity (e.g. whether or not our FIFO queue is indeed FIFO). Since H1 is not sequential, it's a bit hard to say whether the last response client 1 got is copacetic. Here's where we use linearizability: we say that a history H is linearizable if it is equivalent to some valid sequential history H', where H' contains the same events, possibly reorderdered under the constraint that, if a response op1 appears before an invocation op2 in H, then this order is preserved in H'. In other words, a history is linearizable if all the responses are valid according to a sequential reordering that preserves the order of non-overlapping responses. For example, H1 is in fact linearizable because it's equivalent to the following sequential history: client 1: enqueue \"foo\" client 1: ok client 1: dequeue client 1: ok (\"foo\") client 2: enqueue \"baz\" client 2: ok client 1: enqueue \"bar\" client 1: ok client 1: dequeue client 1: ok (\"baz\") Now, an object is said to be linearizable if all the histories it produces are linearizable. In other words, no matter how the clients bombard our queue with requests concurrently, the results need to look as if the requests came one by one. If the queue is to claim linearizability, the implementation should use internal locking, or whatever it needs to do, to make this guarantee. Note that this model does not explicitly talk about replication, but the cases where it is of value are primarily systems with replicated state. If our queue is replicated across many machines, and clients talk to all of them for performing operations, \"using internal locking\" is not trivial but has to somehow be done if we want linearizability. To raise the level of abstraction, a whole system is said to be linearizable if it can be modeled as a set of linearizable objects. Linearizability has this nice \"local\" property: it can be composed like that. So, for example, a key-value store that offers point reads and point writes can be modeled as a collection of registers, with each register offering a read and write operation. If the registers individually provide linearizability, then the store as a whole also does. Two things are of note about the linearizable consistency model: First, there is a notion of \"real time\" used implicitly. Everybody is able to look at one clock on the wall so that it can be judged which operation finishes before another operation begins. The order of operations in our linearizable histories has a relation with the time indicated by this mythical clock. Second, concurrent operations are allowed to execute in any order. For example, in our history H1, the last event might have been client 1: ok (\"bar\") because a serial history where enqueuing baz finishes before enqueuing baz begins would also have been acceptable. It's worth reminding ourselves that linearizability does not talk about transactions, so this model by itself is not well suited to be used by SQL databases. I guess one could shoehorn it by saying that the whole database is one object which provides one transaction operation, but then a definition needs to be provided for the functional specifications of this operation. We're getting back to the ACID properties and the transaction isolation levels, and I'm not sure how the formalism would work exactly. What the literature does for advancing a database model to incorporate this relationship that linearizability has with time is to incorporate its ideas into the serializable transaction isolation level. A note on clocks The mentioning of \"real time\" and the use of a global clock governing a distributed system are fighting words for some of my colleagues. It's understandable since, on the one hand, Einstein realized that time itself is relative (different observers can perceive events to take place in different orders relative to each other) and, on the other hand, even if we are to ignore relativistic effects for practical purposes, this one true, shared clock doesn't quite exist in the context of a distributed system. I'm not qualified to discuss relativistic effects beyond acknowledging that there is such a thing as _relativistic linearizability_ . I believe the casual database user can ignore them, but I'll start blabbering if you ask me exactly why. The fact that there is no shared clock according to which we can decide ordering is a problem all too real for implementers of distributed systems like CockroachDB. The closest we've come is a system called TrueTime built by Google , which provides tightly synchronized clocks and bounded errors brought front and center. As far as the linearizability model is concerned (which assumes that a shared clock exists), the way I think about it is that the model tells us what to expect if such a clock were to exist. Given that it doesn't quite exist, then clients of the system can't actually use it to record their histories perfectly: one can't simply ask all the clients, or all the CockroachDB replicas, to log their operation invocations and responses and timestamp them using the local clocks, and then centralize all the logs and construct a history out of that. This means that verifying a system that claims to be linearizable isn't trivial. In other words, Herlihy talks about histories but doesn't describe how one might actually produce these histories in practice. But that doesn't mean the model is not useful. What a verifier can do is record certain facts like \"I know that this invocation happened after this other invocation, because there was a causal relationship between them\". For certain operations for which there was not a causal relationship, the client might not have accurate enough timestamps to put in the history and so such pairs of events can't be used to verify whether a history is linearlizable or not. Alternatively, another thing a verifier might do is relay all its operations through a singular \"timestamp oracle\", whose recording would then be used to produce and validate a history. Whether such a construct is practical is debatable, though, since the mere act of sequencing all operations would probably introduce enough latency in them as to hide imperfections of the system under test. Bringing the worlds together: strict serializability As I was saying, the ANSI SQL standard defines the serializable transaction isolation as the highest level, but its definition doesn't consider phenomena present in distributed databases. It admits transaction behavior that is surprising and undesirable because it doesn't say anything about how some transactions need to be ordered with respect to the time at which the client executed them. To cover these gaps, the term \"strict serializability\" has been introduced for describing (distributed) databases that don't suffer from these undesirable behaviors. Strict serializability says that transaction behavior is equivalent to some serial execution, and the serial order of transactions corresponds to real time (i.e. a transaction started after another one finished will be ordered after it). Note that strict serializability (like linearizability) still doesn't say anything about the relative ordering of concurrent transactions (but, of course, those transaction still need to appear to be \"isolated\" from each other). We'll come back to this point in the next sections. Under strict serializability, the system behavior outlined in the Hacker News posts example from the Serializability section is not permitted. Databases described by the strict serializability model must ensure that the final read, Nathan's, must return both the root comment and the response. Additionally, the system must ensure that a query like select * from hacker_news_comments never returns the child comment without the parent, regardless of the the time when the query is executed (i.e. depending on the time when it's executed, it can return an empty set, the root, or both the root and the child). We'll come back to this point when discussing CRDB's guarantees. Google's Spanner uses the term \"external consistency\" instead of \"strict serializability\". I like that term because it emphasizes the difference between a system that provides \"consistency\" for transactions known to the database to be causally related and systems that don't try to infer causality and offer stronger guarantees (or, at least, that's how me and my buddies interpret the term). For example, remembering the Hacker News example, there are systems that allow Tobi to explicitly tell the database that his transaction has been \"caused\" by my transaction, and then the system guarantees that the ordering of the two transaction will respect this. Usually this is done through some sort of \"causality tokens\" that the actors pass around between them. In contrast, Spanner doesn't require such cooperation from the client in order to prevent the bad outcome previously described: even if the clients coordinated \"externally\" to the database (e.g, by yelling across the room), they'll still get the consistency level they expect. Peter Bailis has more words on Linearizability, Serializability and Strict Serializability . CockroachDB's consistency model: more than serializable, less than strict serializability Now that we've discussed some general concepts, let's talk about how they apply to CockroachDB. CRDB is an open-source, transactional, SQL database and it's also a distributed system. In my opinion, it comes pretty close to being the Holy Grail of databases: it offers a high degree of \"consistency\", it's very resilient to machine and network failures, it scales well and it performs well. This combination of features already makes it unique enough; the system goes beyond that and brings new concepts that are quite game-changing --- good, principled control over data placement and read and write latency versus availability tradeoffs in geographically-distributed clusters. All without ever sacrificing things we informally refer to as \"consistency\" and \"correctness\" in common parlance. Also it's improving every day at a remarkable pace. I'm telling you --- you need to try this thing! But back to the subject at hand --- the consistency story. CRDB is a complex piece of software; understanding how it all works in detail is not tractable for most users, and indeed it will not even be a good proposition for all the engineers working on it. We therefore need to model it and present a simplified version of reality. The model needs to be as simple as possible and as useful as possible to users, without being misleading (e.g. suggesting that outcomes that one might think are undesirable are not possible when in fact they are). Luckily, because CockroachDB was always developed under a \"correctness first\" mantra, coming up with such a model is not too hard, as I'll argue. There's a standard disclosure that comes with our software: the system assumes that the clocks on the Cockroach nodes are somewhat synchronized with each other. The clocks are allowed to drift away from each other up to a configured \"maximum clock offset\" (by default 500ms). Operators need to run NTP or other clock synchronization mechanism on their machines. The system detects when the drift approaches the maximum allowed limit and shuts down some nodes, alerting an operator[^2]. Theoretically, I think more arbitrary failures modes are possible if clocks get unsynchronized quickly. More on the topic in Spencer's post \"Living Without Atomic Clocks.\" Back to the consistency. For one, CockroachDB implements the serializable isolation level for transactions, as specified by the SQL standard. In contrast to most other databases which don't offer this level of isolation as the default ( or at all, for crying out loud! ), this is the only isolation level we offer; users can't opt for a lesser one. We, the CockroachDB authors, collectively think that any lower level is just asking for pain. It's fair to say that it's generally extremely hard to reason about the other levels and the consequences of using them in an application (see the ACIDRain paper for what can go wrong when using lower isolation levels). I'm not trying to be condescending; up until the 2.1 version we used to offer another relatively high level of isolation as an option (Snapshot Isolation), but it turned out that it (or, at least, our implementation of it) had complex, subtle consequences that even we hadn't fully realized for the longest time. Thus, we ripped it out and instead improved the performance of the our implementation ensuring serializability as much as possible. Below serializability be dragons. But simply saying that we're serializable is selling our system short. We offer more than that. We do not allow the bad outcome in the Hacker News commenting scenario. CockroachDB doesn't quite offer strict serializability, but we're fairly close to it. I'll spend the rest of the section explaining how exactly we fail strict serializability, what our guarantees actually are, and some gotchas. No stale reads If there's one canned response I wish we'd give users that pop into our chat channels asking about the consistency model, I think it should be \"CockroachDB doesn't allow stale reads\". This should be the start of all further conversations, and in fact I think it will probably preempt many conversations. Stating this addresses a large swath of anomalies that people wonder about (in relation to distributed systems). \"No stale reads\" means that, once a write transaction committed, every read transaction starting afterwards[^3] will see it. Internalizing this is important and useful. It does not come by chance; the system works hard for it and so have we, the builders. In the Hacker News comments example, once I have committed my root comment, a new transaction by Nathan is guaranteed to see it. Yes, our system is distributed and data is replicated. Yes, Nathan might be talking to a different node than I was, maybe a node with a clock that's trailing behind. In fact, the node I was talking to might have even crashed in the meantime. Doesn't matter. If Nathan is able to read the respective table, he will be able to read my write. Beyond serializability, saying \"no stale reads\" smells like linearizability (and, thus, strict serializability) since \"staleness\" is related to the passing of time. In fact, when people come around asking for linearizability, I conjecture that most will be satisfied by this answer. I think this is what I'd be asking for if I hadn't educated myself specifically on the topic. Relatedly, this is also what the C(onsistency) in the famous CAP theorem is asking for. And we have it. So why exactly don't we claim strict serializability? CockroachDB does not offer strict serializability Even though CRDB guarantees (say it with me) \"no stale reads\", it still can produce transaction histories that are not linearizable. Consider the history HN2 (assume every statement is its own transaction, for simplicity): Nathan runs select * from hacker_news_comments . Doesn't get a response yet. I run insert into hacker_news_comments (id, parent_id, text) values (1, NULL, 'a root comment') and commit. Tobi runs insert into hacker_news_comments (id, parent_id, text) values (2, 1, 'OP is wrong') and commits. Nathan's query returns and he gets Tobi's row but not mine. This is the \"anomaly\" described in Section 2.5 of Jepsen's analysis of CRDB from back in the day. So what happened? From Nathan's perspective, Tobi's transaction appears to have executed before mine. That contradicts strict serializability since, according to \"real time\", Tobi ran his transaction after me. This is how CRDB fails strict serializability; we call this anomaly \"causal reverse\". Before freaking out, let's analyze the circumstances of the anomaly a bit. Then I'll explain more technically, for the curious, how such a thing can happen in CRDB. First of all, let's restate our motto: if Nathan had have started his transaction after Tobi committed (in particular, if Nathan would have started his transaction because Tobi committed his), he would have seen both rows and things would have been good. An element that's at play, and in fact is key here, is that Nathan's transaction was concurrent with both mine and Tobi's. According to the definition of strict serializability, Nathan's transaction can be ordered in a bunch of ways with respect to the other two: it can be ordered before both of them, after both of them, or after mine but before Tobi's. The only thing that's required is that my transaction is ordered before Tobi's. The violation of strict serializability that we detected here is not that Nathan's transaction was mis-ordered, but that mine and Tobi's (which are not concurrent) appear to have been reordered. Non-strict serializability allows this just fine. My opinion is that this anomaly is not particularly bad because Nathan was not particularly expecting to see either of the two writes. But if this was my only argument, I'd probably stay silent. There's another important thing to explain: both my and Tobi's transactions are, apart from their timing, unrelated: the sets of data they read and write do not overlap. If they were overlapping (e.g. if Tobi read my comment from the DB before inserting his), then serializability would not allow them to be reordered at all (and so CRDB wouldn't do it and the anomaly goes away). In this particular example, if the schema of the hacker news comments table would contain a self-referencing foreign key constraint (asking the database to ensure that child comments reference an existing parent), then the \"reading\" part would have been ensured by the system. So, for this anomaly to occur, you need three transactions to play. Two of them need to appear to be independent of each other (but not really be, or otherwise we probably wouldn't have noticed the anomaly) and the third needs to overlap both of them. I'll let everybody judge for themselves how big of a deal this is. For what it's worth, I don't remember hearing a CRDB user complaining about it. Beyond the theory, there are technical considerations that make producing this anomaly even more unlikely: given CRDB's implementation, the anomaly is avoided not only if the read/write sets of my and Tobi's transactions overlap, but also if the leadership of any of the ranges of data containing hacker news comments rows 1 and 2 happens to be on the same node when these transactions occur, or if Nathan's database client is talking to the same CRDB node as Tobi's, and also in various other situations. Also, the more synchronized the clocks on the three nodes are, the less likely it is. Overall, this anomaly is pretty hard to produce even if you try explicitly. As you might have guessed, I personally am not particularly concerned about this anomaly. Besides everything I've said, I'll add a whataboutist argument and take the discussion back to friendly territory: consider this anomaly in contrast to the \"stale reads\" family of anomalies present in many other competing products. All these things are commonly bucketed under strict serializability / linearizability violations, but don't be fooled into thinking that they're all just as bad. Our anomaly needs three transactions doing a specific dance resulting in an outcome that, frankly, is not even that bad. A stale read anomaly can be seen much easier in a product that allows it. Examples are many; a colleague gave a compelling one recently: if your bank was using a database that allows stale reads, someone might deposit a check for you, at which point your bank would text you about it, and you'd go online to see your balance. You might see the non-updated balance and freak out. Banks should be using CRDB. Other CockroachDB gotchas I've discussed the CRDB guarantees and violations of strict serializability. Our discussion used, laxly, the SQL language to illustrate things but the discussion used language and concepts from more theoretical literature. We bridged the gap by implying that SQL statements are really reads and writes used by some models. This section discusses some uses of CockroachDB/SQL that fall a bit outside the models we've used, but are surprising nevertheless. I think these examples will not fall nicely into the models used for the strict serializability definition, at least not without some effort into expanding the model. The SQL now() function Consider the following two transactions: 1. insert into foo (id, time) values (1, now())\n2. insert into foo (id, time) values (2, now()) Assuming these two transactions execute in this order, it is possible (and surprising) to read the rows back and see that the time value for row 2 is lower that the one for row one. Perhaps it's realistic to think that this happens in other systems too, even single-node systems, if the system clock jumps backwards (as it sometimes does), so perhaps there's nothing new here. as of system time queries and backups CockroachDB supports the (newer) standard SQL system-versioned tables; CRDB lets one \"time travel\" and query the old state of the database with a query like select * from foo as of system time now()-10s . This is a fantastic, really powerful feature. But it also provides another way to observe a \"causal reverse\" anomaly. Say one ran these two distinct transactions, in this order: 1. insert into  hacker_news_comments (id, parent_id, text) values (1, NULL, 'a root comment')\n2. insert into hacker_news_comments (id, parent_id, text) values (2, 1, 'OP is wrong') It's possible for an as of system time query to be executed later and, if it's unlucky in its choice of a \"system time\", to see the second row and not the first. Again, if the second transaction were to read the data written by the first (e.g. implicitly through a foreign key check), the anomaly would not be possible. Relatedly, a backup, taken through the backup database command, is using as of system time queries under the hood, and so a particular backup might contain row 2 but not row 1. CockroachDB implementation details The architecture of CockroachDB is based on a separation between multiple layers (a SQL layer on top down to a storage layer at the bottom). For the subject at hand, the interesting layer is the Transaction Layer , which is in charge of making sure that a transaction doesn't miss writes that it's supposed to be seeing. Each transaction has a timestamp, assigned by the \"gateway node\" --- the node that a client happens to be talking to --- when the transaction starts (through a SQL BEGIN statement). As the transaction talks to different other nodes that might be responsible for _ranges_ of data it wants to read, this timestamp is used to decide what values are visible (because they've been written by transactions \"in the past\") and which values aren't visible because they've been written \"in the future\". CockroachDB uses multi-version concurrency control (MVCC), which means that the history of each row is available for transactions to look through. The difficulties, with respect to consistency guarantees, stem from the fact that the timestamp recording into MVCC are taken from the clock of the gateway node that wrote it, which generally is not the same one as the gateways assigning transaction timestamp for a reader, and we assume that the clock can be desynchronized up to a limit (we call the phenomenon \"clock skew\"). So, given transaction timestamp t and value timestamp t' , how does one decide whether the value in question should be visible or not? The rules are that, if t' <= t , then the transaction will see the respective value (and so we'll essentially order our transaction after that writer). The reasoning is that either our transaction really started after the other one committed, or, if not, the two were concurrent and so we can order things either way. If t’ > t , then it gets tricky. Did the writer really start and commit before the reader began its transaction, or did it commit earlier than that but t’ was assigned by a clock that’s ahead of ours? What CRDB does is define an \"uncertainty interval\": if the values are close enough so that t' could be explained by a trailing clock, we say that we're unsure about whether the value needs to be visible or not, and our transaction needs to change its timestamp (which, unless we can avoid it, means the transaction might have to restart. Which, unless we can further avoid it, means the client might get a retriable error). This is what allows CockroachDB to guarantee no stale reads. In the Hacker News example, if Nathan starts his transaction after me and Tobi committed ours, the worst that could happen is that he gets a timestamp that's slightly in the past and has to consider some of our other writes uncertain, at which point he'll restart at a higher timestamp. We work quite hard to minimize the effects of this uncertainty interval. For one, transactions keep track of what timestamps they've observed at each node and uncertainty is tracked between nodes pair-wise. This, coupled with the fact that a node's clock is bumped up when someone tries to write on it with a higher timestamp, allows a transaction to not have to restart more than once because of an uncertain value seen on a particular node. Also, overall, once the maximum admissible clock skew elapses since a transaction started, a transaction no longer has any uncertainty. Separately, when a transaction's timestamp does need to be bumped, we try to be smart about it. If either the transaction hasn't read anything before encountering the uncertain value, or if we can verify that there's been no writes on the data its already read before encountering the uncertainty, then the transaction can be bumped with no fuss. If we can't verify that, then the transaction needs to restart so it can perform its writes again. If it does have to restart, we don't necessarily tell the client about it. If we haven't yet returned any results for the transaction to the client (which is common if the client can send parts of a transaction's statements as a batch), then we can re-execute all the transaction's statements on the server-side and the client is none the wiser. Conclusion CockroachDB provides a high level of \"consistency\", second only to Spanner among distributed databases as far as I know (but then CRDB is a more flexible and easy to migrate to database --- think ORM support --- so I'll take it over Spanner any day). We offer a relatively easy to understand programming model, although the literature doesn't give us a good name for it. It stronger than serializability, but somewhat weaker than strict serializability (and than linearizability, although using that term in the context of a transactional system is an abuse of the language). It's probably easiest to qualify it by understanding the anomaly that it allows --- \"causal reverse\" --- and the limited set of circumstances under which it can occur. In the majority of cases where one might be wondering about semantics of reads and writes in CRDB, the slogan \"no stale reads\" should settle most discussions. If data consistency is your thing, we're hiring. Check out our careers page . Notes [^1]: Although I think the definition of the Serializable isolation level would have benefitted from introducing some notion of different clients. As phrased by the SQL standard, I believe it technically allows empty results to be produced for any read-only transaction with the justification that those transactions are simply ordered before any other transaction. Implementing that would be egregious, though. [^2]: We're thinking of ways to make CRDB resilient to more arbitrarily unsynchronized clocks. [^3]: As discussed in the \"A note on clocks\" section, figuring out what \"afterwards\" means is not always trivial when the clients involved are not on the same machine. But still, sometimes (in the cases that matter most), a transaction is known to happen after another one, usually through a causal relationship between the two.", "date": "2019-01-24"},
{"website": "CockroachLabs", "title": "Gotchas & Solutions Running a Distributed System Across Kubernetes Clusters", "author": ["Alex Robinson"], "link": "https://www.cockroachlabs.com/blog/experience-report-running-across-multiple-kubernetes-clusters/", "abstract": "``` I recently gave a talk at KubeCon North America -- “Experience Report: Running a Distributed System Across Kubernetes Clusters”. Below is a blog based on that talk for those who prefer to read rather than listen. For anyone interested in viewing the talk, it is available here . ``` If you have run Kubernetes clusters recently, you've probably found that it's a nice way to run your distributed applications. It makes it easy to run even pretty complicated applications like a distributed system. And importantly, it's been drastically improving over the years. New features like dynamic volume provisioning, StatefulSets, and multi-zone clusters have made it much easier to run reliable stateful services. Community innovations like Helm charts have been great for people like me who want to make it easy for other people to run an application they develop on Kubernetes. And for end users, the increasing number of managed Kubernetes services these days make it so that you don't have to run your own cluster. However, the situation hasn't really improved if you want your service to span across multiple regions or multiple Kubernetes clusters. There have been early efforts, such as the Ubernetes project, and the recent Federation v2 project is still ongoing, but nothing has yet solved the problem of running a distributed system that spans multiple clusters. It's still a very hard experience that isn't really documented. Why run a multi-region cluster? You might ask why you would want to run a system across multiple regions, and that maybe there's a reason that this work hasn't been done by the community. I'd argue there are a few big reasons: User latency . The further away your service is from your users, the longer it takes for them to receive a response. This is especially critical if you're running a global service that has users all over the world. If you don't have any instances of your service near them, they're going to have to wait hundreds of milliseconds to get responses back from your service. This is a very strong reason for wanting to run across multiple regions. Fault tolerance also becomes very important . Your application should continue running even if an entire data center or an entire region of data centers goes offline for some terrible reason. In this case, if the data center over in California goes down, the users on the West Coast might have to wait a little longer for their responses, but they'll still be able to use your service because of the other replicas that are stood up on the other parts of the country. Bureaucratic reasons . There are laws in many countries now around where you're allowed to store users' data without their permission. You might have to store the data from users in China in China, and you might need to store the data from users in Russia in Russia, and having a system that spans multiple regions enables you to do that. If you only have a data center in the US, then you're going to have a tough time. In this blog, I want to talk about the practical experience of running a distributed system across multiple Kubernetes clusters and what it's really like. I’m going to cover: Why it's hard What kinds of detailed information about Kubernetes or about your distributed system you should know before you get started What solutions are available, and which solutions we've put into production and seen work well for us. Why it’s hard The question of why it's hard is something that many people don't even think to ask. I've had a number of conversations with folks where they'll assume that since CockroachDB runs really well across regions and because Kubernetes makes it really easy to run CockroachDB, that it's just naturally going to be easy to run CockroachDB across multiple Kubernetes clusters spanning regions. In fact it's not easy at all. You might have noticed that so far I've been using the terms \"multi-region\" and \"multi-cluster\" essentially interchangeably. Kubernetes is not designed to support a single cluster that spans multiple regions on the wide area network. For quite a while, it wasn't even recommended to have a single cluster span multiple availability zones within a region. The community fought for that capability, and now it is a recommended configuration called a \"multi-zone cluster\". But running a single Kubernetes cluster that spans regions is definitely done at your own risk. I don't know anyone who would recommend it. So I'm going to keep using these terms - “multi-cluster” and “multi-region” - mostly interchangeably. If you want to run something like CockroachDB across multiple regions, you are necessarily going to have multiple Kubernetes clusters, at least one in each region. To better understand the problem of setting up a system like CockroachDB across multiple regions, it helps to understand: What does a system like CockroachDB, or any other stateful service, need to run reliably in the first place? For this blog post, I will be using CockroachDB as the example because it’s the system that I'm most familiar with running in Kubernetes. Cockroach requires roughly three things in order to run reliably. Each Cockroach node must have persistent storage that will survive across restarts. Each node can communicate directly with each other node. Each node must have a network address that it can advertise to the other nodes so that they can talk to each other reliably. That address ideally should survive restarts so that you're not always changing addresses. (We can be flexible with this one. CockroachDB will work if your nodes are changing network addresses, but this is something that most stateful systems need, and so I will leave it for the sake of completeness.) Many stateful systems need these three things, and Kubernetes provides these all very well within the context of a single cluster. The controller, known as a StatefulSet, was designed to provide exactly these three things because they're so commonly needed. The Kubernetes StatefulSet gives each node its own disk or multiple disks as well as its own address. Kubernetes itself guarantees that all pods can communicate directly with each other. But across multiple Kubernetes clusters, we lose requirement #3 above: there's no cross-cluster addressing system. We often lose requirement #2, as well: It's often not set up such that a pod in one cluster can talk directly to a pod in another cluster. I call this “pod-to-pod connectivity” later in this post. So the core problems of running a multi-region system all come down to the networking . You might lose pod-to-pod communication across clusters. You might have to communicate across separate private networks in order to talk from one cluster to another. For example, if you're on AWS, each Kubernetes cluster in a different region is going to be in a different VPC, so you will have to connect those with some sort of VPC peering or a VPN. This is going to be even more true if you're spanning multiple cloud providers . Finally, we also need a persistent address that works both within and between the clusters. One option is to have two separate addresses -- one that works within the local cluster and one that works between them. This works as long as your system can handle having separate addresses. While this works for CockroachDB, it wouldn't work quite so well for other distributed systems that assume just one address per instance. Why is networking the problem? Kubernetes specifies a list of requirements from the network that it runs on. Each pod running in a cluster has to be given its own IP address. The IP address that the pod itself sees inside its container must be the same IP address that other pods in the network see it as. So a pod running ifconfig should see the same IP address as any other pod that it connects to. All this communication should happen directly, without any form of NAT. This poses a real problem because most users want to run a lot of pods on each machine, but traditional networks only allocate one or two IP addresses to each host in a cluster. So extra work is needed to allocate new routable IP addresses for each of the pods on these machines. As Kubernetes has grown in popularity, dozens of solutions have been built over the last few years to satisfy these requirements. Each has details that differ from each other, making it difficult to rely on any particular implementation details to be true for any given Kubernetes cluster. In all of this, the multi-cluster scenario was left completely unspecified, making all those little implementation details of the different networking solutions really critical because most solutions aren't trying to provide any sort of guarantee about how multi-cluster works, so the way that multi-cluster works is completely left up to the unintentional implementation details. This makes using multi-cluster a bit of a minefield because you won't know exactly how the network underneath you works unless you know exactly which network solution the cluster is using. There are a lot of networking solutions out there — more than 20 are listed in the Kubernetes networking docs alone. They all work differently, so there's only so much in common that you can depend on, and that means that the way you make multi-cluster networking work will have to depend on your networking provider. This is really frustrating from my perspective where I'm trying to make configurations that everyone can use without needing to be Kubernetes experts. We can group the numerous networking solutions into three high-level categories: Those that enable direct pod-to-pod communication across clusters out of the box. (Includes Google Kubernetes Engine and Amazon's Elastic Kubernetes Service) Those that make it easy to enable. (Includes Azure Kubernetes Service \\[“advanced networking mode”] and Cilium \\[\"cluster mesh\"]) Those that still make it quite difficult (I won’t name names). Solutions: What can you do? This is a tough problem. With so many networking providers being used under the hood, it's unlikely that there's one single solution, but I will explore a few of them, and the problems they may still present. Note: I did most of my work on this in the first half of the year, so it's possible some development has happened here that I haven't heard about that gets rid of some of these cons. I've been keeping up and trying to play with things every once in a while, and I haven't seen a great solution that's fully mature yet. Work is being done on it, though, which we'll see later. Some of the solutions: Don't work in certain environments. Break very easily if the operator of the clusters isn't very careful about what they're doing. Use a slower data path, giving you a less performance system. Don't work well with TLS certificates, so you might have to run your cluster in what we at Cockroach call \"insecure mode\", where you aren't relying on certificates to secure all communication between nodes. Rely on an immature or very complex system that isn't quite ready to be used in production yet, even if it's on the right track. Note: For most of these solutions, I'm assuming that in each Kubernetes cluster, you're just going to run a StatefulSet. I'm then focusing on how to connect the pods from each cluster together. Each StatefulSet can be managed independently, meaning that if you wanted to scale up the nodes that you have running in one Kubernetes cluster, you would just scale the StatefulSet that's running there. Solution #1: Static pod placement + HostNetwork One solution is to pretend that you're not really using Kubernetes at all. Kubernetes allows you to statically place pods on specific nodes, or you can use a DaemonSet to put one pod on every node. You can then use the HostNetwork option in the Kubernetes configuration file to tell the pod to use the host machine's network directly. The pod will then be able to use the host machine's IP address or hostname for all of its communication. This means that your pod will be using the ports directly on the host's network, so you can't have more than one Cockroach pod on that machine because you're going to run into port conflicts if you do. However, you can use that host's IP address to communicate between nodes in different clusters. As long as you've set up your network such that your VMs or your physical machines in the different clusters can directly talk to each other, then Cockroach pods on those machines will be able to directly talk to each other. In order to avoid changing addresses all the time and potentially getting some sort of split brain situation, you're probably going to want to statically assign these pods to machines. Your options are to say directly in the config file which host you want each pod to run on, or to use the DaemonSet such that you're just always running one Cockroach instance on each machine, keeping their IPs the same. This is great because it works even if you don't have that pod-to-pod IP address communication between clusters. You're relying on the machines in each cluster having direct IP routability, and this has no moving parts. No need for any cross-cluster naming service. You'll just have to set up your join flags when you start nodes to include some of the machine's IP addresses. Another perk of this configuration is that it makes it very natural to use each machine's local disk for storage, so you don't have to deal with creating and managing remotely attached persistent volumes. One big caveat with the above is that it depends on your host's IP addresses not changing. This is something to be especially careful about if you’re running on a public cloud. If you're using a hosted solution like GKE or EKS, they do node software upgrades VM by just tearing down each VM and recreating it. When it gets recreated, it's very likely to get a different IP address. Because join flags rely on those IP addresses staying the same, this could lead to a new cluster or nodes being started up that are not able to connect back to the cluster after being restarted during these upgrades. Note: You want to be really careful on GKE, which can sometimes do automatic node upgrades if you don't configure it not to. A workaround is to orchestrate your own custom upgrade process, or to modify the IP addresses in the join flags while the upgrades are happening. I have run this in test environments, and it works well because you get high performance because the nodes are just talking directly to each other. Once you set it up, you don't have to think much about it, other than during those upgrade scenarios. It requires a bit of manual work to edit the configuration files to put in the join IP addresses, and, assuming you want to run in secure mode, to create the certificates for each IP address. But overall, you may get some extra reliability by doing it this way because any sort of dynamic movement that Kubernetes would otherwise do to move your pods around and have to detach and reattach disks becomes a non-issue because none of that is happening here. For those who would argue that they don't trust Kubernetes to run their stateful services, they must trust something like this because Kubernetes is prevented from getting in the way of things. Solution #2: Use an external load balancer for each pod This requires running a StatefulSet in each cluster, and then creating a public load balancer for each CockroachDB pod. It is really easy to do in all of the major cloud providers because they all have plug-ins for Kubernetes that make it easy to expose a service from within a cluster, giving a separate load balanced address to each of the Cockroach nodes. For example: If we have three Cockroach nodes in our cluster, we'll get a separate load-balanced address for cockroachdb-0 , cockroachdb-1 , and cockroachdb-2 . We take those addresses from the load balancers and have all the pods connect to each other using those addresses. We then create certificates for all the nodes using those addresses. Since those load balancer addresses are stable, the underlying CockroachDB pods can be deleted and recreated, moved from one node to another, and so on, and the load balanced addresses will always route to the correct Cockroach pod. Solution #2: The Pros We have had customers using this for many months. It works even without pod-to-pod communication between clusters, as in the previous solution, because all communication between pods that are in different clusters is going through that load balanced IP address. It continues working even as Kubernetes hosts churn in and out of the cluster. It avoids the problems of the previous solution because even if nodes are being deleted and recreated all the time in the cloud, the pods are still using the same load balanced IP addresses that never change. It doesn't require you to add any extra moving parts to your system, other than the load balanced IPs. You don't have to run some service that allows the Cockroach pods in one cluster to be able to find the Cockroach pods in another cluster because you just have those load balancers that are presumably being run by your cloud provider. Solution #2: The Cons However, there are some problems with this option. Most notably, it is hard to configure and can become expensive. It can be a little slower than the previous solution, because on all but the most sophisticated load balancers, when you send a packet to that load balanced IP address, the load balancer will forward it to any random host in the other Kubernetes cluster. Then that other host has to look up where the pod is actually running and forward the packet to that host. Note: Some providers are smarter. I believe the load balancer integration of Google Cloud is smart enough now that the load balancer will send the packet directly to where the pod is running. However, this is not going to be true everywhere, so you might see slightly worse performance going this route. It can be a pain to have to create a new load balancer every time you want to add a new node to your cluster. If you forget, which can be easy to do because you might just run the kubectl scale command, you'll see that your new pod isn't working because it doesn't have an IP address already configured for it. So you have to remember to do that . It can be expensive to run so many load balancers. This will again depend on the cloud, but creating this many load balancers on the major cloud providers could be quite costly. You might also not be able to do this on-prem if you don't have good load balancer integration between your Kubernetes cluster and some sort of manually-run load balancer like HAProxy. It requires a lot of manual config file editing and certificate creating in order to create the complex configuration needed. Once you've set it up, it's going to be very reliable. I haven't heard of any outages caused by this sort of configuration. Just some hiccups while adding a new node to the cluster, but that didn't affect the currently-running nodes, thankfully. Solution #3: Use pod IPs directly If you have direct communication between pods in different clusters, you can let the pods advertise their internal pod IP addresses to each other directly. As pods get deleted and recreated on a new node, they might come back up with a different IP address. Some systems, for instance CockroachDB, are able to actually handle this, as long as the disk is moving around with the pod. You'll also need to set up some sort of load balancer that can be used for the join flag. You need the join flag to have some sort of address that will always be there, but luckily that join flag doesn't get used on the data path, only during initialization. This makes the solution quite fast, with the pods communicating directly with each other across the different clusters. Solution #3: The Pros This is very fast. There's no overhead on the data path at all, other than the normal Docker overhead of bringing packets into a container and back out of it again. This is very resilient to changes. You can add or move hosts as much as you want. It requires very little work to configure and maintain. Solution #3: The Cons The real caveat with this setup is that it can be difficult to do in secure mode. Because IP addresses can change across pod deletions / recreations, creating TLS certificates that can stand up to hostname verification is tricky. We have only run it as a proof of concept in insecure mode. You would need some sort of dynamic certificate creating and signing solution in order to create new certificates any time a pod is given a different IP address than it had before. So while this has worked really well for insecure deployments, and it's really fast and easy to maintain, setting it up for secure deployments is tough. 5 Solution #4: DNS chaining This is what we have documented in our docs , and it's what I would call \"DNS chaining\". Basically we're taking the previous solution where pods are talking directly to each other on their IP addresses, which is great for performance, and adds persistent addresses to that solution. Specifically, we want to add persistent network addresses that will work even as the underlying pods' IP addresses change, which will allow us to put those persistent addresses into the certificates for each pod, and then to run in secure mode. To do this, we want to configure the built-in DNS server within each Kubernetes cluster to serve DNS entries from the other clusters. The question, of course, is how, because kube-dns is set up to only create DNS names for the services in its own cluster. DNS chaining: Use CoreDNS instead of the default kube-dns service One way to do this is to use CoreDNS instead of the default kube-dns. CoreDNS is a much more fully featured DNS server that allows for more customization. It has a full plug-in architecture. There's even an existing plug-in called kubernetai , which was written to allow one CoreDNS server to watch the API server of multiple Kubernetes clusters. With kubernetai, we could set up CoreDNS in each cluster to watch the services in all of our Kubernetes clusters, and then kubernetai would automatically create service names for the services in the other clusters. However, in practice swapping CoreDNS in for kube-dns on any of the managed Kubernetes offerings is very difficult. It's also difficult to modify each cluster's DNS domain. You'll notice if you look at the full service addresses of Kubernetes services, they all end with .svc.cluster.local (unless you've already customized it yourself). Ideally you would want each cluster to have a separate DNS domain so that it would be easier to distinguish between a service running in one cluster and a service running in another cluster so you don't run into conflicts between them. Earlier this year, I tried for a couple days to replace kube-dns or to set up a different cluster DNS domain on GKE and had no luck. With CoreDNS becoming the standard DNS server as of 1.13, this may get more feasible. Setting up a custom DNS domain might still be very difficult, so there would still be that roadblock. The kubernetai plug-in that we would want to use also doesn't come in the default CoreDNS Docker image that gets used, so there would still be some difficulties in distributing the plugin, but this is becoming more feasible as CoreDNS becomes the standard. DNS chaining: Use CoreDNS alongside the default kube-dns service Another alternative which I haven't fully explored is to run CoreDNS alongside the default kube-dns service. We could configure kube-dns to defer certain look-ups to CoreDNS, and then have our own CoreDNS that's configured exactly the way that we want it running in each cluster watching the other Kubernetes clusters. We could then use some of CoreDNS's plug-ins or the rewrite rules to make the cross-cluster look-ups work out. I haven't fully tried this, and I haven't heard of anyone else that's actually doing it. So if you're going to try this out, you'll be in uncharted territory, but it should be feasible. DNS chaining: Chain DNS servers together using “stub domains” What we do for systems like GKE and DKS is use what are called \"stub domains\". Stub domains are a kube-dns config feature that allow you to defer DNS look-ups for certain domains to the name server of your choice. It allows us to configure a suffix that enables the kube-dns in one cluster to redirect look-ups for anything that ends in that suffix to a different cluster's DNS service. For example, if you have a cluster in us-west-1b and you do a lookup for a DNS address that ends in us-east-1b.svc.cluster.local , then kube-dns will know to send that along to the DNS server that's running in that us-east-1b cluster. I've put together some scripts for this, and it’s documented on our website for people to try. “Stub domains”: The Pros There's no overhead on the data path other than the normal Docker overhead, which is tough to avoid. It's very resilient to hosts being edited or removed. There's no need to configure cross-cluster naming service. You're just using the built-in DNS, and there's actually no manual work needed to get this running. We have a script in the CockroachDB GitHub repository that requires you to fill in a few parameters around where your clusters are running and what their kubeconfig names are. You can just run the script and in a couple minutes you'll have a functional multi-region Cockroach cluster in Kubernetes. You need to have pod-to-pod communication across clusters for this to work, and you need to set up some way for the DNS server in one cluster to reliably reach the DNS cluster in another server. So you can do this using an internal load balancer on the various clouds, or you could use an external load balancer, or you could come up with your own solution for that. It's not great. On GCE today you have to use an external load balancer because their internal load balancers don't work across regions. I believe they're working on fixing that, but I don't know when a solution will be around, and so that would be exposing your DNS server to the public internet, which may have security repercussions depending on your cluster setup. You could put firewalls in place to keep that from being as much of an issue, but it's something to definitely be concerned about. You might end up needing to run your own load balancer to enable these DNS servers to find each other on the network, and at that point, maybe you should just use that load balancer to handle the pod-to-pod communication. Solution #5: Istio Istio is a project that has become popular over the last year or two. They've been working on a multi-cluster mode to handle the problem of addressing across clusters for the last half year or so. Note: This description reflects the state of multi-cluster support in Istio version 1.0, which was released in July 2018. The 1.1 release that should be coming out in early 2019 takes some massive steps forward toward solving these problems. Istio explicitly punts on the problem of enabling pod-to-pod communication. All it does right now is naming. You install the Istio control plane in one primary Kubernetes cluster, and then you have to install Istio’s remote component in all of your other clusters. It has a very small overhead because packets go through the envoy proxy — an efficient, low-level proxy written in C++ that's in use at a number of companies. Like past solutions, it's very resilient to hosts being edited or removed. The cons, however, are still pretty heavy. The entire control planning runs in a single Kubernetes cluster, so if that region goes down, you've lost your connectivity between clusters. This leaves a single point of failure, and that hasn't been resolved yet. You also have to solve the problem of the Istio components' IP addresses potentially changing as they're restarted, which is the same problem we're trying to solve for Cockroach in the first place. While Istio is super promising and hopefully will solve the problem in the next year, as far as I can tell, it's pretty far from being suitable for production use. It's under active development and it looks like it will improve quite a lot very soon, though. Conclusion Multi-cluster networking is pretty tricky. It's much more complicated than I would have expected or than most people would expect, and unfortunately not a whole lot has been done about it for years. This past year is the first I've seen a lot of good movement in the right direction. And even after you nail pod-to-pod connectivity, you still have to solve the problem of providing stable names that work across clusters. I am really excited about the many different projects like Cilium , Istio, and Crossplane that have spun up that are starting to care about it. Although it's hard to recommend a single answer for everyone, out of these few solutions there's definitely at least one of them which would be very reliable for you if you're willing to spend some time upfront on setup. If you want to learn more about Kubernetes, visit here , where we’ve aggregated tutorials, videos, and guides. And if building and automating distributed systems puts a spring in your step, we're hiring! Check out our open positions here .", "date": "2018-12-20"},
{"website": "CockroachLabs", "title": "Introducing the High Availability Architecture Guide (CockroachDB vs. Oracle)", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/oracle-vs-cockroachdb/", "abstract": "Which is worse...? One of your users goes to check her bank balance in your app, and the service is down, or, One of your users goes to check her bank balance in your app and there's a data inconsistency. Engineers are frequently faced with this false tradeoff: do you place a higher premium on data correctness, or high availability? This problem only becomes more complicated when you begin dealing with users distributed across broad geographies. When IT experts consider high availability infrastructure for mission-critical services, their minds often leap to Oracle as the preeminent service provider. But Oracle's database was designed in a pre-cloud world, and the means by which it achieves high availability on geo-distributed workloads are complex. Oracle requires a staggering number of technologies that must be implemented, and still, their solutions can allow potentially costly anomalies into your data. As a cloud native database, CockroachDB introduces a new way of providing always-on availability, strong data consistency, and distributed performance. Today, we're releasing a side-by-side comparison of CockroachDB and Oracle to help you get a better understanding of the architecture (and cost) of setting up a highly available distributed service. Overview of high availability features Here's a quick side-by-side comparison of the tools you need to create a high-availability architecture using Oracle vs. CockroachDB. We chose to focus on these features because they represent the majority of what teams need to guarantee high availability. First, you need to make sure you can replicate data within a single site to survive individual machine failures. Next, to survive an entire site going down, you need to scale and replicate data across regions. In widely distributed deployments, you might want to optimize your application's performance by controlling your data's physical layout. And lastly, every team needs a good disaster recovery plan--just in case. HA Feature Oracle CockroachDB Scale & HA at one site RAC, disk management ($$) Included in database binary Intra-site networking Private Interconnect ($$) Included in database binary Intra-site replication Distributed Lock Storage ($$), CacheFusion ($$) Included in database binary Scale across-sites GoldenGate ($$) Included in database binary HA & Inter-site routing Global Data Service ($$) Edge tool (OSS options available) Control data's physical layout Configurable via GoldenGate ($$) Table-level: Included in database binary Row-level: Geo-partitioning ($$) Disaster Recovery Non-Distributed: RMAN Non-Distributed: Included in database binary Distributed: Enterprise ($$)BACKUP/RESTORE ( $$ denotes features you must obtain a paid license to use in a production environment. ) As you'll read in our guide, Oracle requires a separate tool for almost every HA feature––and all but one of them (RMAN) requires its own enterprise license. CockroachDB, on the other hand, has options to be run in production with no licensing costs. For those with more complex needs, we offer more robust enterprise-grade tools with a license. While this overview can help contextualize some of the bewildering complexity Oracle introduces into your infrastructure, we have a more comprehensive guide to high availability architecture available here for you to check out.", "date": "2019-02-12"},
{"website": "CockroachLabs", "title": "Pipelining Consensus Writes to Speed Up Distributed SQL Transactions", "author": ["Nathan VanBenschoten"], "link": "https://www.cockroachlabs.com/blog/transaction-pipelining/", "abstract": "CockroachDB supports ACID transactions across arbitrary data in a distributed database. A discussion on how this works was first published on our blog three years ago. Since then, a lot has changed. Perhaps most notably, CockroachDB has transitioned from a key-value store to a full SQL database that can be plugged in as a scalable, highly-available replacement for PostgreSQL . It did so by introducing a SQL execution engine which maps SQL tables onto its distributed key-value architecture . However, over this period of time, the fundamentals of the distributed, atomic transaction protocol at the core of CockroachDB have remained untouched [^1]. For the most part, this hasn't been an issue. The transaction protocol in CockroachDB was built to scale out to tremendously large clusters with arbitrary data access patterns. It does so efficiently while permitting serializable multi-key reads and writes. These properties have been paramount in allowing CockroachDB to evolve from a key-value store to a SQL database. However, CockroachDB has had to pay a price for this consistency in terms of transaction latency. When compared to other consensus systems offering weaker transaction semantics, CockroachDB often needed to perform more synchronous consensus rounds to navigate a transaction. However, we realized that we could improve transaction latency by introducing concurrency between these rounds of consensus. This post will focus on an extension to the CockroachDB transaction protocol called Transactional Pipelining , which was introduced in CockroachDB's recent 2.1 release. The optimization promises to dramatically speed up distributed transactions, reducing their time complexity from O(n) to O(1) , where n is the number of DML SQL statements executed in the transaction and the analysis is expressed with respect to the latency cost of distributed consensus. The post will give a recap of core CockroachDB concepts before using them to derive a performance model for approximating transaction latency in CockroachDB. It will then dive into the extension itself, demonstrating its impact on the performance model and providing experimental results showing its effects on real workloads. The post will wrap up with a preview of how we intend to extend this optimization further in upcoming releases to continue speeding up transactions. Distributed Transactions: A Recap CockroachDB allows transactions to span an entire cluster, providing ACID guarantees across arbitrary numbers of machines, data centers, and geographical regions. This is all exposed through SQL — meaning that you can BEGIN a transaction, issue any number of read and write statements, and COMMIT the transaction, all without worrying about inconsistencies or loss of durability. In fact, CockroachDB provides the strongest level of isolation , SERIALIZABLE , so that the integrity of your data is always preserved. There are a few competing ideas which combine to make this all possible, each of which is important to understand. Below is a brief introduction to each. For those interested in exploring further, more detail can be found in our architecture documentation . Storage At its most fundamental level, the goal of a durable database is to persist committed data such that it will survive permanently. This is traditionally performed by a storage engine, which writes bytes to a non-volatile storage medium. CockroachDB uses RocksDB , an embedded key-value database maintained by Facebook, as its storage engine. RocksDB builds upon its pedigree ( LevelDB and more generally the Log-structured merge-tree (LSM tree) data structure ) to strike a balance between high write throughput, low space amplification, and acceptable read performance. This makes it a good choice for CockroachDB, which runs a separate instance of RocksDB on each individual node in a cluster. Even with software improvements like improved indexing structures and hardware improvements like the emergence of SSDs, persistence is still expensive both in terms of the latency it imposes on each individual write and in terms of the bounds it places on write throughput. For the remainder of this post, we'll refer to the first cost here as \"storage latency\" . Replication Replicating data across nodes allows CockroachDB to provide high-availability in the face of the chaotic nature of distributed systems. By default, every piece of data in CockroachDB is replicated across three nodes in a cluster (though this is configurable)—we refer to these as “replicas”, and each node contains many replicas. Each individual node takes responsibility for persisting its own replica data. This ensures that even if nodes lose power or lose connectivity with one another, as long as a majority of the replicas are available, the data will stay available to read and write. Like other modern distributed systems, CockroachDB uses the Raft consensus protocol to manage coordination between replicas and to achieve fault-tolerant consensus, upon which this state replication is built. We've published about this topic before. Of course, the benefits of replication come at the cost of coordination latency. Whenever a replica wants to make a change to a particular piece of its replicated data, it \"proposes\" that change to the other replicas and multiple nodes must come to an agreement about what to change and when to change it. To maintain strong consistency during this coordination, Raft (and other consensus protocols like it) require at least a majority of replicas (e.g. a quorum of 2 nodes for a replication group of 3 nodes) to agree on the details of the change. In its steady-state, Raft allows the proposing replica to achieve this agreement with just a single network call to each other replica in its replication group. The proposing replica must then wait for a majority of replicas to respond positively to its proposal. This can be done in parallel for every member in the group, meaning that at a minimum, consensus incurs the cost of a single round-trip network call to the median slowest member of the replication group. For the remained of this post, we'll refer to this as \"replication latency\". Distribution Replicating data across nodes improves resilience, but it doesn't allow data to scale indefinitely. For that, CockroachDB needs to distribute different data across the nodes in a cluster, storing only a subset of the total data on each individual node. To do this, CockroachDB breaks data into 64MB chunks, called Ranges. These Ranges operate independently and each manage its own N-way replication. The Ranges automatically split, merge, and move around a cluster to hold a suitable amount of data and to stay healthy (i.e. fully-replicated) if nodes crash or become unreachable. A Range is made up of Replicas, which are members of the Range who hold a copy of its state and live on different nodes. Each Range has a single \"leaseholder\" Replica who both coordinates writes for the Range, as well as serves reads from its local RocksDB store. The leaseholder Replica is defined as the Replica at any given time who holds a time-based \"range lease\". This lease can be moved between the Replicas as they see fit. For the purpose of this post, we'll always assume that the leaseholder Replica is collocated with (in the same data center as) the node serving SQL traffic. This is not always the case, but automated processes like Follow-the-Workload do their best to enforce this collocation, and lease preferences make it possible to manually control leaseholder placement. With a distribution policy built on top of consistent replication, a CockroachDB cluster is able to scale to an arbitrary number of Ranges and to move Replicas in these Ranges around to ensure resilience and localized access . However, as is becoming the trend in this post, this also comes at a cost. Because distribution forces data to be split across multiple replication groups (i.e. multiple Ranges), we lose the ability to trivially order operations if they happen in different replication groups. This loss of linearizable ordering across Ranges is what necessitates the distributed transaction protocol that the rest of this post will focus on. Transactions CockroachDB's transactional protocol implements ACID transactions on top of the scalable, fault-tolerant foundation that its storage, replication, and distribution layers combine to provide. It does so while allowing transactions to span an arbitrary number of Ranges and as many participating nodes as necessary. The protocol was inspired in part by Google Percolator , and it follows a similar pattern of breaking distributed transactions into three distinct phases: 1. Preparation A transaction begins when a SQL BEGIN statement is issued. At that time, the transaction determines the timestamp at which it will operate and prepares to execute SQL statements. From this point on, the transaction will perform all reads and writes at its pre-determined timestamp. Those with prior knowledge of CockroachDB may remember that its storage layer implements multi-value version control , meaning that transactional reads are straightforward even if other transactions modify the same data at later timestamps. When the transaction executes statements that mutate data ( DML statements ), it doesn't write committed values immediately. Instead, it creates two things that help it manage its progress: The first write of the transaction creates a transaction record which includes the transaction's current status. This transaction record acts as the transaction's \"switch\". It begins in the \"pending\" state and is eventually switched to \"committed\" to signify that the transaction has committed. The transaction creates write intents for each of the key-value data mutations it intends to make. The intents represent provisional, uncommitted state which lives on the same Ranges as their corresponding data records. As such, a transaction can end up spreading intents across Ranges and across an entire cluster as it performs writes during the preparation phase. Write intents point at their transaction's record and indicate to readers that they must check the status of the transaction record before treating the intent's value as the source of truth or before ignoring it entirely. 2. Commit When a SQL transaction has finished issuing read and write statements, it executes a COMMIT statement. What happens next is simple - the transaction visits its transaction record, checks if it has been aborted, and if not, it flips its switch from \"pending\" to \"committed\". The transaction is now committed and the client can be informed of the success. 3. Cleanup After the transaction has been resolved and the client has been acknowledged, an asynchronous process is launched to replace all provisional write intents with committed values. This reduces the chance that future readers will observe the intents and need to check in with the intents' transaction record to determine its disposition. This can be important for performance because checking the status of another transaction by visiting its transaction record can be expensive. However, this cleanup process is strictly an optimization and not a matter of correctness. That high-level overview of the transaction protocol in CockroachDB should be sufficient for the rest of this post, but those who are interested can learn more in our docs . The Cost of Distributed Transactions in CockroachDB With an understanding of the three phases of a distributed transaction in CockroachDB and an understanding of the abstractions upon which they are built, we can begin to construct a performance model that captures the cost of distributed transactions. Specifically, our model will approximate the latency that a given transaction will incur when run through CockroachDB 2.0 and earlier. Model Assumptions To begin, we'll establish a few simplifying assumptions that will make our latency model easier to work with and visualize. The first assumption we'll make is that the two dominant latency costs in distributed transactions are storage latency and replication latency . That is, the cost to replicate data between replicas in a Range and the cost to persist it to disk on each replica will dominate all other latencies in the transaction such that everything else can safely be ignored in our model. To safely make this approximation, we must assume that Range leaseholders are collocated with the CockroachDB nodes serving SQL traffic. This allows us to ignore any network latency between SQL gateways and Range leaseholders when performing KV reads and writes. As we discussed earlier , this is a safe and realistic assumption to make. Likewise, we must also assume that the network latency between the client application issuing SQL statements and the SQL gateway node executing them is sufficiently negligible. If all client applications talk to CockroachDB nodes within their local data centers/zones, this is also a safe assumption. The second assumption we'll make is that the transactional workload being run is sufficiently uncontended such that any additional latency due to queuing for lock and latch acquisition is negligible. This holds true for most workloads, but will not always be the case in workloads that create large write hotspots, like YCSB in its zipfian distribution mode. It's our belief that a crucial property of successful schema design is the avoidance of write hotspots, so we think this is a safe assumption to make. Finally, the third assumption we'll make is that the CockroachDB cluster is operating under a steady-state that does not include chaos events. CockroachDB was built to survive catastrophic failures across a cluster, but failure events can still induce latencies on the order of a few seconds to live traffic as Ranges recover, Range leases change hands, and data is migrated in response to the unreachable nodes. These events are a statistical given in a large-scale distributed system, but they shouldn’t represent the cluster’s typical behavior –– so, for the sake of this performance model, it's safe to assume they are absent. The model will not be broken if any of these assumptions are incorrect, but it will need to be adapted to account for changes in latency characteristics. Latency Model First, let's define exactly what we mean by \"latency\". Because we're most interested in the latency observed by applications, we define transactional latency as \"the delay between when a client application first issues its BEGIN statement and when it gets an acknowledgement that its COMMIT statement succeeded.\" Remember that SQL is “conversational”, meaning that clients typically issue a statement and wait for its response before issuing the next one. We then take this definition and apply it to CockroachDB's transaction protocol . The first thing we see is that because we defined latency from the client's perspective, the asynchronous third phase of cleaning up write intents can be ignored. This reduces the protocol down to just two client-visible phases: everything before the COMMIT statement is issued by the client and everything after. Let's call these two component latencies L_prep and L_commit , respectively. Together, they combine to a total transitional latency L_txn = L_prep + L_commit . The goal of our model is then to characterize L_prep and L_commit in terms of the two dominant latency costs of the transaction so that we can define L_txn as a function of this cost. It just so happens that these two dominant latency costs are always paid as a pair, so we can define this unit latency as L_c , which can be read as \"the latency of distributed consensus\". This latency is a function of both the replication layer and the storage layer. It can be expressed to a first-order approximation as the latency of a single round-trip network call to, plus a synchronous disk write on, the median slowest member of a replication group (i.e. Range). This value is highly dependent on a cluster's network topology and on its storage hardware, but is typically on the order of single or double digit milliseconds. To define L_prep in terms of the unit latency L_c , we first need to enumerate everything a transaction can do before a COMMIT is issued. For the sake of this model, we'll say that a transaction can issue R read statements (e.g. SELECT * FROM t LIMIT 1 ) and W write statements (e.g. INSERT INTO t VALUES (1) ). If we define the latency of a read statement as L_r and the latency of a write statement as L_w , then the total latency of L_prep = R * L_r + W * L_w . So far, so good. It turns out that because leaseholders in CockroachDB can serve KV reads locally without coordination (the committed value already achieved consensus), and because we assumed that the leaseholders were all collocated with the SQL gateways, L_r approaches 0 and the model simplifies to L_prep = W * L_w . Of course, this isn't actually true; reads aren't free. In some sense, this shows a limitation of our model, but given the constraints we've placed on it and the assumptions we've made, it's reasonable to assume that sufficiently small, OLTP-like reads have a negligible cost on the latency of a transaction. With L_prep reduced to L_prep = W * L_w , we now just need to characterize the cost of L_w in terms of L_c . This is where details about CockroachDB's transaction protocol implementation come into play. To begin, we know that the transaction protocol creates a transaction record during the first phase. We also know that the transaction protocol creates a write intent for every modified key-pair during this phase. Both the transaction record and the write intents are replicated and persisted in order to maintain consistency. This means that naively L_prep would incur a single L_c cost when creating the transaction record and an L_c cost for every key-value pair modified across all writing statements. However, this isn't actually the cost of L_prep for two reasons: The transaction record is not created immediately after the transaction begins. Instead, it is collocated with and written in the same batch as the first write intent, meaning that the latency cost to create the transaction record is completely hidden and therefore can be ignored. Every provisional write intent for a SQL statement is created in parallel, meaning that regardless of how many key-value pairs a SQL statement modifies, it only incurs a single L_c cost. A SQL statement may touch multiple key-value pairs if it touches a single row with a secondary index or if it touches multiple distinct rows. This explains why using multi-row DML statements can lead to such dramatic performance improvements. Together this means that L_w , the latency cost of a single DML SQL statement, is equivalent to L_c . This is true even for the first writing statement which has the important role of creating the transaction record. With this substitution, we can then define L_prep = W * L_c Defining L_commit in terms of the unit latency L_c is a lot more straightforward. When the COMMIT statement is issued, the switch on the transaction's record is flipped with a single round of distributed consensus. This means that L_commit = L_c . We can then combine these two components to complete our pre-2.1 latency model: L_txn = (W + 1) * L_c We can read this as saying that a transaction pays the cost of distributed consensus once for every DML statement it executes plus once to commit. For instance, if our cluster can perform consensus in 7ms and our transaction performs 3 UPDATE statements, a back-of-the-envelope calculation for how long it should take gives us 28ms . The \"1-Phase Transaction\" Fast-Path CockroachDB contains an important optimization in its transaction protocol that has existed since its inception called the \"one-phase commit\" fast-path. This optimization prevents a transaction that performs all writes on the same Range and commits immediately from needing a transaction record at all. This allows the transaction to complete with just a single round of consensus ( L_txn = 1*L_c ). An important property of this optimization is that the transaction needs to commit immediately. This means that typically the fast-path is only accessible by implicit SQL transactions (i.e. single statements outside of a BEGIN; ... COMMIT; block). Because of this limitation, we'll ignore this optimization for the remainder of this post and focus on explicit transactions. Transactional Pipelining The latency model we built reveals an interesting property of transactions in CockroachDB — their latency scales linearly with respect to the number of DML statements they contain. This behavior isn't unreasonable, but its effects are clearly noticeable when measuring the performance of large transactions. Further, its effects are especially noticeable in geo-distributed clusters with very high replication latencies. This isn't great for a database specializing in distributed operation. Transactional Pipelining is a extension to the CockroachDB transaction protocol which was introduced in v2.1 and aims to improve performance for distributed transactions. Its stated goal is to avoid the linear scaling of transaction latency with respect to DML statement count. At a high level, it achieves this by performing distributed consensus for intent writes across SQL statements concurrently. In doing so, it achieves its goal of reducing transaction latency to a constant multiple of consensus latency. Prior Art (a.k.a. the curse of SQL) Before taking a look at how transactional pipelining works, let's quickly take a step back and explore how CockroachDB has attempted to address this problem in the past. CockroachDB first attempted to solve this issue in its v1.0 release through parallel statement execution . Parallel statement execution worked as advertised - it allowed clients to specify that they wanted statements in their SQL transaction to run in parallel. A client would do so by suffixing DML statements with the RETURNING NOTHING specifier. Upon the receipt of a statement with this specifier, CockroachDB would begin executing the statement in the background and would immediately return a fake return value to the client. Returning to the client immediately allowed parallel statement execution to get around the constraints of SQL's conversational API within session transactions and enabled multiple statements to run in parallel. There were two major problems with this. First, clients had to change their SQL statements in order to take advantage of parallel statement execution. This seems minor, but it was a big issue for ORMs or other tools which abstract the SQL away from developers. Second, the fake return value was a lie. In the happy case where a parallel statement succeeded, the correct number of rows affected would be lost. In the unhappy case where a parallel statement failed, the error would be returned, but only later in the transaction . This was true whether the error was in the SQL domain, like a foreign key violation, or in the operational domain, like a failure to write to disk. Ultimately, parallel statement execution broke SQL semantics to allow statements to run in parallel. We thought we could do better, which is why we started looking at the problem again from a new angle. We wanted to retain the benefits of parallel statement execution without breaking SQL semantics. This in turn would allow us to speed up all transactions, not just those that were written with parallel statement execution in mind. Buffering Writes Until Commit We understood from working with a number of other transaction systems that a valid alternative would be to buffer all write operations at the transaction coordinator node until the transaction was ready to commit. This would allow us to flush all write intents at once and pay the \"preparation\" cost of all writes, even across SQL statements, in parallel. This would also bring our distributed transaction protocol more closely in line with a traditional presumed abort 2-phase commit protocol. The idea was sound and we ended up creating a prototype that did just this. However, in the end we decided against the approach. In addition to the complication of buffering large amounts of data on transaction coordinator nodes and having to impose conservative transaction size limits to accommodate doing so, we realized that the approach would have a negative effect on transaction contention in CockroachDB. If you squint, write intents serve a similar role to row locks in a traditional SQL database. By \"acquiring\" these locks later into the lifecycle of a transaction and allowing reads from other transactions to create read-write conflicts in the interim period, we observed a large uptick in transaction aborts when running workloads like TPC-C . It turns out that performing all writes (i.e. acquiring all locks) at the end of a transaction works out with weaker isolation levels like snapshot isolation because such isolation levels allow a transaction's read timestamp and its write timestamp to drift apart. However, at a serializable isolation level, a transaction must read and write at the same timestamp to prevent anomalies like write skew from corrupting data. With this restriction, writing intents as early as possible serves an important role in CockroachDB of sequencing conflicting operations across transactions and avoiding the kinds of conflicts that result in transaction aborts. As such, doing so ends up being a large performance win even for workloads with just a small amount of contention. Creating significantly more transaction aborts would have been a serious issue, so we began looking for other ways that we could speed up transactions without acquiring all locks at commit time. We'll soon see that transactional pipelining allows us to achieve these same latency properties while still eagerly acquiring locks and discovering contention points within a transaction long before they would cause a transaction to abort. A Key Insight The breakthrough came when we realized that we could separate SQL errors from operational errors. We recognized that in order to satisfy the contract for SQL writes, we only need to synchronously perform SQL-domain constraint validation to determine whether a write should return an error, and if not, determine what the effect of the write should be (i.e. rows affected). Notably, we realized that we could begin writing intents immediately but don't actually need to wait for them to finish before returning a result to the client. Instead, we just need to make sure that the write succeeds sometime before the transaction is allowed to commit. The interesting part about this is that a Range's leaseholder has all the information necessary to perform constraint validation and determine the effect of a SQL write, and it can do this all without any coordination with other Replicas. The only time that it needs to coordinate with its peers is when replicating changes, and this doesn't need to happen before we return to a client who issued a DML statement. Effectively, this means that we can push the entire consensus step out of the synchronous stage of statement execution. We can turn a write into a read and do all the hard work later. In doing so, we can perform the time-consuming operation of distributed consensus concurrently across all statements in a transaction! Asynchronous Consensus In order to make this all fit together, we had to make a few changes to CockroachDB's key-value API and client. The KV API was extended with the concept of \"asynchronous consensus\". Traditionally, a KV operation like a Put would acquire latches on the corresponding Range's leaseholder, determine the Put s effect by evaluating against the local state of the leaseholder (i.e. creating a new write intent), replicate this effect by proposing it through consensus, and wait until consensus succeeds before finally returning to the client. Asynchronous consensus instructs KV operations to skip this last step and return immediately after proposing the change to Raft. Using this option, CockroachDB's SQL layer can avoid waiting for consensus during each DML statement within a transaction––this means we no longer need to wait W *L_c during a transaction’s preparation phase. Proving Intent Writes The other half of the puzzle is that transactions now need to wait for all in-flight consensus writes to complete before committing a transaction. We call this job of waiting for an in-flight consensus write \"proving\" the intent. To prove an intent, the transaction client, which lives on the the SQL gateway node performing a SQL transaction, talks to the leaseholder of the Range which the intent lives on and checks whether it has been successfully replicated and persisted. If the in-flight consensus operation succeeded, the intent is successfully proven. If it failed, the intent is not proven and the transaction returns an error. If consensus operation is still in-flight, the client waits until it finishes. To use this new mechanism, the transaction client was modified to track all unproven intents. It was then given the critical job of proving all intent writes before allowing a transaction to commit. The effect of this is that provisional writes in a transaction never wait for distributed consensus anymore. Instead, a transaction waits for all of its intents to be replicated through consensus in parallel, immediately before it commits. Once all intent writes succeed, the transaction can flip the switch on its transaction record from PENDING to COMMITTED. Read-Your-Writes There is an interesting edge case here. When a transaction writes a value, it should be able to read that same value later on as if it had already been committed. This property is sometimes called \"read-your-writes\". CockroachDB's transaction protocol has traditionally made this property trivial to enforce. Before asynchronous consensus, each DML statement in a transaction would synchronously result in intents that would necessarily be visible to all later statements in the transaction. Later statements would notice these intents when they went to perform operations on the same rows and would simply treat them as the source of truth since they were part of the same transaction. With asynchronous consensus, this guarantee isn't quite as strong. Now that we're responding to SQL statements before they have been replicated or persisted, it is possible for a later statement in a transaction to try to access the same data that an earlier statement modified, before the earlier statement's consensus has resulted in an intent. To prevent this from causing the client to miss its writes, we create a pipeline dependency between statements in a transaction that touch the same rows. Effectively, this means that the second statement will wait for the first to complete before running itself. In doing so, the second intent write first proves the success of the first intent write before starting asynchronous consensus itself. This results in what is known as a \"pipeline stall\", because the pipeline within the transaction must slow down to prevent reordering and ensure that dependent statements see their predecessor's results. It is worth noting that the degenerate case where all statements depend on one-other and each results in a pipeline stall is exactly the case we had before - all statements are serialized with no intermediate concurrency. This mix of asynchronous consensus, proving intent writes, and the strong ordering enforced between dependent statements that touch the same rows combine to create transactional pipelining. Latency Model: Revisited Transactional pipelining dramatically changes our latency model. It affects both the preparation phase and the commit phase of a transaction and forces us to rederive L_prep and L_commit . To do so, we need to remember two things. First, with transactional pipelining, intent writes no longer synchronously pay the cost of distributed consensus. Second, before committing, a transaction must prove all of its intents before changing the status on its transaction record. We hinted at the effect of this change on L_prep earlier - writing statements are now just as cheap as reading statements. This means that L_prep approaches 0 and the model simplifies to L_txn = L_commit . However, L_commit is now more expensive because it has to do two things: prove all intents and write to its transaction record, and it must do these operations in order. The cost of the first step is of particular interest. The transaction client is able to prove all intents in parallel. The effect of this is that the latency cost of proving intent writes at the end of a transaction is simply the latency cost of the slowest intent write, or L_c . The latency cost of the second step, writing to the transaction's record to flip its switch does not change. By adding these together we arrive at our new transaction latency model: L_txn = L_commit = 2 * L_c We can read this as saying that a transaction whose writes cross multiple Ranges pays the cost of distributed consensus twice, regardless of the reads or the writes it performs. For instance, if our cluster can perform consensus in 7ms and our transaction performs 3 UPDATE statements, a back-of-the-envelope calculation for how long it should take gives us 14ms . If we add a fourth UPDATE statement to the transaction, we don't expect it to pay an additional consensus round trip - the estimated cost is constant regardless of what else the transaction does. Benchmark Results Simplified performance models are great for understanding, but they are only as useful as they are accurate. Before releasing v2.1, we ran a number of experiments with and without transactional pipelining enabled to see what its performance impact is in practice. Inter-Node Latency vs. Txn Latency The first experiment we ran was looking at TPC-C's New-Order transaction . TPC-C is an industry standard benchmark that we have spent significant time looking at in the past . The backbone of the benchmark is its New-Order business transaction, which is a mid-weight, read-write transaction with a high frequency of execution and stringent response times. With a careful attention to multi-row statements, the transaction can be composed of as few as 4 read-only statements and 5 read-write statements. Given this detail, we can use the latency models we derived early to estimate how long we expect the transaction to take with and without transactional pipelining. With transactional pipelining, we expect the transaction to take two times the distributed consensus latency. Without it, we expect the transaction to take six times the distributed consensus latency . We put these models to the test on a three-node cluster within the same GCE zone. We then used the tc linux utility to manually adjust inter-node round trip latency. The client-perceived latency of a New-Order transaction was measured as the synthetic latency grew from 0ms up to 90ms . The experiment was run twice, first with the kv.transaction.write_pipelining_enabled cluster setting set to true and next with it set to false. The graph demonstrates that our latency model was spot on. The line without transactional pipelining enabled has a slope of 6, demonstrating that every extra millisecond of inter-node latency results in six extra milliseconds of transactional latency. Meanwhile, the line with transactional pipelining enabled has a slope of 2, demonstrating that every extra millisecond of inter-node latency results in two extra milliseconds of transactional latency. This indicates that without transactional pipelining, transactional latency is dependent on the number of DML statements in the txn, which in this case is 5. With transactional pipelining, transactional latency is not dependent on the number of DML statements in the txn . DML Statement Count vs. Txn Latency To dig into this further, we performed a second experiment. This time, we fixed the inter-node round trip latency at 10ms and instead adjusted the number of DML statements performed within a transaction. We started with a single DML statement per transaction and increased that up through 256. Finally, we plotted the client-perceived transactional latency with and without transactional pipelining enabled. The trial with transactional pipelining performs so much better than trial without that it’s hard to see what’s going on. Let’s use a logarithmic scale for the y-axis so we can dig in perform a real comparison. With a logarithmic y-axis, the graph reveals a few interesting points. To begin, it shows that when a transaction only performs a single DML statement, pipelining makes no difference and the transaction latency is twice the consensus latency. This lines up with our intuition and our performance model. More critically, it demonstrates the difference in how transaction latency scales with respect to DML statement count with and without transactional pipelining. With transactional pipelining disabled, transaction latency scales linearly with respect to the DML statement count. With transactional pipelining enabled, transaction latency scales sub-linearly with respect to the DML statement count. This is a clear demonstration of the effectiveness of transactional pipelining and shows what a major improvement it can provide. It is also in line with what our latency models would predict. However, even in the second line, the scaling is not completely constant. In some senses, this reveals a failing of our new latency model, which assumes that the synchronous stage of each DML statement is free with transactional pipelining. Of course, in reality, each additional DML statement still does have some latency cost. For instance, in this setup, each additional DML statement looks to cost a little under half a millisecond. This cost is negligible compared to the dominant cost of distributed consensus with single and double digit DML statement counts, but starts to make a difference towards the upper end of the graph. This demonstrates both the utility of performance models and the importance of putting them to the test against real-world experimentation. These experiments help us to see when secondary costs can be discounted and under which conditions they start to matter. Other Benchmarks These two tests demonstrate how effectively transactional pipelining achieves its goals. Those interested can explore even more tests that we ran during the development of transactional pipelining in the associated Github pull request The testing revealed other interesting behavior, like that even when replication latencies were negligible, transactional pipelining improved latency and throughput of benchmarks because it permits increased levels of concurrency through the storage layer. We've found these tests to be extremely valuable in justifying the creation of transactional pipelining, but it would have been impossible to test everything. We'd love to hear from others about their experience with the optimization and whether it provides a speedup in their unique workloads. We encourage anyone interested to open a Github issue to discuss their results. This will help us continue to optimize CockroachDB going forward. What's Next? Transactional pipelining is part of CockroachDB v2.1. Use of the feature is controlled by the kv.transaction.write_pipelining_enabled cluster setting. This setting defaults to true , which means that when you upgrade to version 2.1, you'll immediately begin taking advantage of the optimization. You can expect a speedup in your transactions across the board. However, we're not done trying to reduce transactional latency. The latency model we left off with scales independently with respect to the number of DML statements in the transaction, but it still incurs the latency cost of two rounds of distributed consensus. We think it's possible to cut this in half. In fact, we have a concrete proposal for how to do it called \"parallel commits\". The optimization deserves a separate blog post of its own, but we can summarize it here as follows: Parallel commits allows the two operations that occur when a transaction commits, proving provisional intent writes and flipping the switch on the transaction record, to run in parallel. Or in more abstract terms — it parallelizes the prepare and the commit phases of the 2-phase commit protocol. Effectively, this reduces the cost of L_commit in our new latency model to just a single L_c . The optimization is still in development, and we're actively exploring how best to introduce it into future releases of CockroachDB. If distributed transactions are your jam, check out our open positions here . Footnote [^1]: One exception to this is that CockroachDB's concurrency protocol has moved from optimistic to pessimistic. This deserves an entire blog post of its own, but what it means in practice is that contending transactions rarely restart each other anymore. Instead, contending transactions queue up to make modifications, meaning that transaction intents act a lot more like traditional locks. This approach is more complicated because it requires a distributed deadlock detection algorithm, but we justified making the switch after seeing how much better it performed for highly-contended workloads. More detail about this can be found in our docs .", "date": "2019-01-10"},
{"website": "CockroachLabs", "title": "40x faster hash joiner with vectorized execution", "author": ["Angela Chang"], "link": "https://www.cockroachlabs.com/blog/vectorized-hash-joiner/", "abstract": "For the past four months, I've been working with the incredible SQL Execution team at Cockroach Labs as a backend engineering intern to develop the first prototype of a batched, column-at-a-time execution engine. During this time, I implemented a column-at-a-time hash join operator that outperformed CockroachDB's existing row-at-a-time hash join by 40x. In this blog post, I'll be going over the philosophy, challenges, and motivation behind implementing a column-at-a-time SQL operator in general, as well as some specifics about hash join itself. In CockroachDB, we use the term \"vectorized execution\" as a short hand for the batched, column-at-a-time data processing that is discussed throughout this post. What is vectorized execution? In most SQL engines, including CockroachDB's current engine, data is processed a row at a time: each component of the plan (e.g. a join or a distinct) asks its input for the next row, does a little bit of work, and prepares a new row for output. This model is called the \"Volcano\" model, based off of a paper by Goetz Graefe. By contrast, in the vectorized execution model, each component of the plan processes an entire batch of columnar data at once, instead of just a single row. This idea is written about in great detail in the excellent paper MonetDB/X100: Hyper-Pipelining Query Execution , and it's what we've chosen to use in our new execution engine. Also, as you might have guessed from the word \"vectorized\", organizing data in this batched, columnar fashion is the primary prerequisite for using SIMD CPU instructions, which operate on a vector of data at a time. Using vectorized CPU instructions is an eventual goal for our execution engine, but since it's not that easy to produce SIMD instructions from native Go, we haven't added that capability just yet. Before we go further, a quick note: even though we've said that this vectorized model is about column-at-a-time processing, our work here isn't related to \"column stores\", databases that organize their on-disk data in a columnar fashion. The execution engine our team is working on is only dealing with SQL execution itself, and nothing in the storage layer. Why is vectorized execution so much more efficient? The title of this blog post made a bold claim: vectorized execution improved the performance of CockroachDB's hash join operator by 40x. How can this be true? What makes this vectorized model so much more efficient than the existing row-at-a-time model? The core insight is essentially that computers are very, very fast at doing simple tasks. The less work the computer has to do to complete a task, the faster the task can be executed. The vectorized execution model aims at this simple truth by replacing the fully-general, interpreter-like SQL expression evaluator with a series of specific compiled loops, specialized on datatype and operation, so that the computer can do many more simple tasks in a row before having to step back and make a decision about what to do next. Was that abstract enough for you? We'll get much more concrete next. To illustrate the difference between the Volcano model and the vectorized model, consider a People table with three columns: Id , Name , and Age . In the Volcano model, each data row is processed one at a time by each operator - a row-by-row execution approach. By contrast, in the vectorized execution engine, we pass in a limited-size batch of column-oriented data at a time to be processed. Instead of using a tuple array data structure, we use a set of columns, where each column is an array of a specific data type. In this example, the batch would consist of an array of integers for the Id , an array of bytes for the Name , and an array of integers for the Age . The following pictures show the difference between the data layout in the two models. Volcano model rows Vectorized execution batch Now, let's examine what happens when we execute the SQL query SELECT Name, (Age - 30) * 50 AS Bonus FROM People WHERE Age > 30 . In the Volcano model, the top level user requests a row from the Project operator, and this request gets propagated to the bottom layer Scan operator. Scan retrieves a row from the key-value store and passes it up to Select , which checks to see if the row passes the predicate that Age > 30 . If the row passes the check, it's returned to the Project operator to compute Bonus = (Age - 30) * 50 as the final output. Volcano model, row-at-a-time processing If we zoom into the Select operator, the code is exactly as you would expect. The Select operator (named filterNode in our codebase) asks for the next row from its child operator and checks if it passes the filter condition. If it does, then the row is returned to its parent operator. Otherwise, the row is discarded and the process is repeated. Eventually, a row or EOF is returned. func (f *filterNode) Next(params runParams) (bool, error) {\n    for {\n        if next, err := f.source.plan.Next(params); !next {\n            return false, err\n        }\n\n        passesFilter, err := sqlbase.RunFilter(f.filter, params.EvalContext())\n        if err != nil {\n            return false, err\n        }\n\n        if passesFilter {\n            return true, nil\n        }\n        // Row was filtered out; grab the next row.\n    }\n} Note carefully what's going on here: for every row, we're invoking a filter that is a fully-general scalar expression! The expression could be anything - a multiplication, a division, an equality check, or a builtin function, and it could even be a long tree of such things. Because of this generality, the computer has a lot of work to do on every single row - it's got to check what the expression is before it can even do any work. This is the same trouble that interpreted languages have, compared to compiled languages. In the vectorized execution model, we take a rather different approach. The philosophy behind each vectorized operator is to permit no degrees of freedom, or run-time choices, during execution. This means that for any combination of tasks, data types, and properties, there should be a single, dedicated operator responsible for the work. For the example query, the user requests a batch from the chain of operators. Each operator requests a batch from its children, performs its specific task, and then returns a batch to its parent. To visualize this, consider a People batch that gets processed by SelectIntGreaterThanInt . This operator would select all Age values that are greater than 30. This new sel_age batch then gets passed to the ProjectSubIntInt operator which performs a simple subtraction to produce the tmp batch. Finally, this tmp batch is passed to the ProjectMultIntInt operator which calculates the final Bonus = (Age - 30) * 50 values. In order to actually implement one of these vectorized operators, we want to break apart the process into tight for loops over a single column. The following code snippet implements the SelectIntGreaterThanInt operator. The function retrieves a batch from its children, and loops over every element of the column while marking the values that are greater than 30 as selected. The batch along with its selection vector are then returned to the parent operator for further processing. func (p selGTInt64Int64ConstOp) Next() ColBatch {\n    for {\n        batch := p.input.Next()\n\n        col := batch.ColVec(p.colIdx).Int64()[:ColBatchSize]\n        var idx uint16\n        n := batch.Length()\n        sel := batch.Selection()\n\n        for i := uint16(0); i < n; i++ {\n            var cmp bool\n            cmp = col[i] > p.constArg\n            if cmp {\n                sel[idx] = i\n                idx++\n            }\n        }\n    }\n} Note that this code is obviously extremely efficient when you look at it, just based on how simple it is! Look at the for loop: it's not 100% clear without context, but it's iterating over a Go-native slice of int64 s, comparing each to another constant int64 , and storing the result in another Go-native slice of uint16s . This is just about as simple and fast of a loop as you can program in a language like Go. The traditional hash joiner Before we dive into the world of vectorized hash joiners, I wanted to first introduce some hash joiner jargon. Your typical equi-join query probably looks something like SELECT customers.name, orders.age FROM orders JOIN customers ON orders.id = customers.order_id AND orders.person_id = customers.person_id . Upon dissection of this query, we can extract the important information. Left input Right input Table orders customers Equality key (id, person_id) (order_id, person_id) Output columns [age] [name] A hash joiner is a physical implementation of a relational equi-join operator. Here is a simple algorithm for solving this problem: Choose the smaller table to be the build table. Build phase: construct a hash table indexed on the equality key of the build table. Probe phase: For every probe table row, look up its equality key in the hash table. If the key is found, construct a resulting row using the output columns of the probe table row and its matching build table row. The vectorized hash joiner Now that we have a taste of what vectorized execution and hash joiners are, let's take it one step further and combine the two concepts. The challenge is to break down the hash join algorithm into a series of simple loops over a single column, with as few run-time decisions, if statements and jumps as possible. Marcin Zukowski described one such algorithm to implement a many-to-one inner hash join in his paper \" Balancing Vectorized Query Execution with Bandwidth-Optimized Storage \". This paper laid invaluable groundwork for our vectorized hash join operator. Many-to-one inner hash join The algorithm described in Zukowski's paper separates the hash join into two phases; a build phase and a probe phase. The following pseudocode was provided for the build phase. // Input: build relation with N attributes and K keys\n// 1. Compute the bucket number for each tuple, store in bucketV\nfor (i = 0; i < K; i++)\n    hash[i](hashValueV, build.keys[i], n); // type-specific hash() / rehash()\nmodulo(bucketV, hashValueV, numBuckets, n);\n// 2. Prepare hash table organization, compute each tuple position in groupIdV\nhashTableInsert(groupIdV, hashTable, bucketV, n)\n// 3. Insert all the attributes\nfor (i = 0; i < N; i++)\n    spread[i](hashTable.values[i], groupIdV, build.values[i], n); Let's try our hand at understanding this part of the algorithm. The ultimate objective of the build phase is to insert every row of the build table into a hash table indexed on its equality key. The hash table we are building is a bucket chaining hash table. Consider the query SELECT customers.name, orders.age FROM orders JOIN customers ON orders.id = customers.order_id AND orders.person_id = customers.person_id and the following sample tables. First, we compute the hash value of each build table row by looping over each equality key column and performing a hash. This hash function is unique in that it utilizes the old hash value in its calculation. Next, we perform a modulo over the hashValueV array in order to determine the bucket values of each build table row. In this example, our hash table only has 3 buckets. In the second step, we actually insert the build table rows into the hash table in their corresponding bucketV values. The hashTableInsert function is a simple for loop that inserts each new row to the beginning of the bucket chain in the hash table. It looks like this: hashTableInsert(groupIdV, hashTable, bucketV, n) {\n    for (i = 0; i < n; i++) {\n        groupIdV[i] = hashTable.count++;\n        hashTable.next[groupIdV[i]] = hashTable.first[bucketV[i]];\n        hashTable.first[bucketV[i]] = groupIdV[i];\n    }\n} groupIdV is an array that corresponds to the unique row ID of each build table row. The following diagram captures a snapshot of the system state upon insertion of the final row of the build table. Finally, the last step of the build phase is to perform a spread on every build table output column. This copies and stores all the build table output column values in memory. Now, that we've stored all the build table rows into a hash table, it's time for the probe phase. The following pseudocode was presented in the paper for the probe phase. // Input: probe relation with M attributes and K keys, hash-table containing \n//     N build attributes\n// 1. Compute the bucket number for each probe tuple.\n// ... Construct bucketV in the same way as in the build phase ...\n// 2. Find the positions in the hash table\n// 2a. First, find the first element in the linked list for every tuple, \n//      put it in groupIdV, and also initialize toCheckV with the full \n//      sequence of input indices (0..n-1).\nlookupInitial(groupIdV, toCheckV, bucketV, n);\nm = n;\nwhile (m > 0) {\n    // 2b. At this stage, toCheckV contains m positions of the input tuples \n    //     for which the key comparison needs to be performed. For each tuple \n    //     groupIdV contains the currently analyzed offset in the hash table. \n    //     We perform a multi-column value check using type-specific \n    //     check() / recheck() primitives, producing differsV.\n    for (i = 0; i < K; i++)\n        check[i](differsV, toCheckV, groupIdV, hashTable.values[i], probe.keys[i], m);\n    //  2c. Now, differsV contains 1 for tuples that differ on at least one key,\n    //      select these out as these need to be further processed\n    m = selectMisses(toCheckV, differV, m);\n    // 2d. For the differing tuples, find the next offset in the hash table,\n    //     put it in groupIdV\n    findNext(toCheckV, hashTable.next, groupIdV, m);\n}\n// 3. Now, groupIdV for every probe tuple contains the offset of the matching\n//    tuple in the hash table. Use it to project attributes from the hash table.\n//    (the probe attributes are just propagated)\nfor (i = 0; i < N; i++)\n    gather[i] (result.values[M + i], groupIdV, hashTable.values[i], n); The objective of the probe phase is to lookup the equality key of every row in the probe table, and if the key is found, construct the resulting output row with the matching rows. To do this, we need to calculate the bucket values of each probe table row, and follow their corresponding bucket chain in the hash table until their is a match or we've reached the end. However, to do this in a vectorized fashion is a little more complex because we need to break up this process into several, simple loops over a single column. The lookupInitial function computes the bucketV values for each probe table key using the same approach as the build phase. It then populates groupIdV with the first value in the hash table in each bucket. It is also necessary to verify that every row is indeed a match, since different equality keys can reside in the same hash table bucket (due to collision). Thus, we also populate the toCheckV array with the indices (0, …, n) . The updated state of the hash joiner is as follows. The next part of the probe phase is to verify that the equality key of each probe table row whose index resides in toCheckV is equal to the build table row that the corresponding groupIdV value represents. The toCheckV array is rebuilt with all the probe table row indices that did not match, and the groupIdV values are updated with the next value in the same bucket chain. After one iteration of this process, the state of the hash joiner is as follows. This process can continue until the longest of all bucket chains have been traversed. At the end, the groupIdV array contains the matching build table row ID for each probe table row. If there were no matches, then the groupIdV value would be 0. The final step is to gather together the matching rows by copying the probe table and matching build table output columns column at a time into a resulting batch. Why did we go through all this trouble? As we expected, running a series of simple for loops over a single column is extremely fast! The row-at-a-time Volcano model is, by contrast, extremely complex and unfriendly to CPUs, by virtue of the fact that it must essentially behave as a per-row, fully-type-general expression interpreter. Some other major advantages of using vectorized execution include: Algorithms are CPU cache friendly since we are often operating on sequential memory Operators only operate on a single data type - this eliminates the need for casting and reduces conditional branching Late materialization - values are not stitched together until necessary at the end. Check out the impressive benchmark comparison between Cockroach's pre-existing Volcano model hash joiner and the vectorized hash joiner. The numbers below represent something close to a 40x improvement in throughput. Not bad! Volcano model hash joiner in CockroachDB today: BenchmarkHashJoiner/rows=16-8       13.46 MB/s\nBenchmarkHashJoiner/rows=256-8      18.69 MB/s\nBenchmarkHashJoiner/rows=4096-8     18.97 MB/s\nBenchmarkHashJoiner/rows=65536-8    15.30 MB/s Vectorized hash joiner: BenchmarkHashJoiner/rows=2048-8     611.55 MB/s\nBenchmarkHashJoiner/rows=262144-8   1386.88 MB/s\nBenchmarkHashJoiner/rows=4194304-8  680.00 MB/s I hope you enjoyed learning how our vectorized hash joiner works underneath the hood. Getting to work on such a rewarding project has been an invaluable experience. It gave me the opportunity to really consider things like CPU architecture and compiler optimizations - because at this level, one simple line of code can have non-trivial performance impacts. I'm super grateful for my manager Jordan and Roachmate Alfonso for all their help throughout this internship. Thank you Cockroach Labs, it has been a blast! P.S. Cockroach Labs is hiring engineers ! If you enjoyed reading about our SQL execution engine, and you think you might be interested in working on a SQL execution engine (or other parts of a cutting edge, distributed database) check out our positions open in New York, Boston, SF, and Seattle.", "date": "2019-01-31"},
{"website": "CockroachLabs", "title": "Lessons Learned from 2+ Years of Nightly Jepsen Tests", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/jepsen-tests-lessons/", "abstract": "Since the pre-1.0 betas of CockroachDB, we've been using Jepsen to test the correctness of the database in the presence of failures. We have re-run these tests every night as a part of our nightly test suite. Last fall, these tests found their first post-release bug . This blog post is a more digestible walkthrough of that discovery (many of the links here point to specific comments in that issue's thread to highlight the most important moments). Two years of Jepsen testing Running Jepsen tests in an automated fashion has been somewhat challenging. The tests run in a complex environment with network dependencies, spawn multiple cloud VMs, and have many potential points of failure, so they turn out to be rather flaky. If you search our issue tracker for Jepsen failures you'll see a lot of issues, but before the bug we're discussing here they were all benign - failures of our automation setup, not an inconsistency or bug in CockroachDB itself. By now, though, we've worked out the kinks and have the tests running reliably. Our Jepsen test suite includes 7 different test workloads, combined with a number of different nemesis processes to simulate failures. Every night we start up a 5-node cluster and run each test+nemesis combination for 6 minutes each. The test that found the bug was the register test with the split nemesis. The register workload is perhaps the simplest possible one for consistency tests: it performs simple single-key reads and writes, and verifies that reads see the last value written. The split nemesis uses the CockroachDB SPLIT AT command to cause a series of splits in the underlying KV store. Finding a real bug On Sep 29 (two years to the day after the beta release that Aphyr tested), the nightly test run reported a failure in the jepsen/register/split test. The logs included Jepsen's signature table-flipping error message Analysis invalid! (ﾉಥ益ಥ）ﾉ ┻━┻ , indicating that this is a real inconsistency, not an automation failure. The inconsistency involved a single key, 167. At 11:54:47, we read a value of 1 for this key. At 11:54:48, we read a value of 4. In between, there were attempts to write a value of 4 for this key, but they all failed. The split nemesis also activated during this time, suggesting that the split process may be involved in the bug, but it's not conclusive. Since we later read the value 4, one of those failed writes must have in fact succeeded. In CockroachDB (as in any distributed system), failures are sometimes ambiguous in the event of a server being killed - it is impossible to tell whether the operation succeeded or failed. Jepsen understands this, and when it encounters an ambiguous error it considers both possibilities. This gave us our first theory of the bug: an ambiguous failure was getting incorrectly marked as unambiguous. Searching for the cause Armed with this theory, we started looking for possible causes. What kinds of errors was this test encountering that might be incorrectly classified? Fortunately we save packet captures for failed test runs, so we were able to dig in to the network traces and see. Initial analysis with wireshark pointed to the recently-added transactional pipelining feature. This feature altered error handling (in ways that sometimes made it harder to tell what exactly was going on), but we couldn't see any ways in which it could turn ambiguous failures unambiguous. We also tried to reproduce the problem to see if we could get more information about it. Attempts to write a targeted test based on the theory that the split nemesis was responsible were fruitless . On Oct 24, nearly a month after the first failure, the nightly tests failed again . This time it wasn't the split nemesis but the majority-ring+subcritical-skews nemesis. This tells us a couple of things: our suspicion of the split process was likely misplaced, and the bug is pretty rare - it could easily have been lying dormant for a month or more before it was first observed. With our split theory invalidated, we went back to the Jepsen tests as our best shot at reproducing the problem. To get more than one reproduction a month, Andrei built a test harness that ran 50 copies of the tests in parallel. This made it feasible for us to add additional logging and eventually get more details out of the failing tests. The breakthrough After a couple weeks of this (now two months since the first report), Tobi smelled a side-effect leak . In one of the traces he noticed that a waiting transaction was allowed to proceed at the same time that the transaction it was waiting on was going through a second iteration of the reproposal loop. Pulling on this thread revealed that when a transaction failed to commit on its first proposal, it would still signal waiting transactions as if it had. The bug involves pipelined writes, a new feature in CockroachDB 2.1 described in detail in this blog post . In short, pipelined writes are performed asynchronously. We start the process of writing, but instead of waiting for the write to finish we move on to the next step in the transaction, and then before the transaction commits we verify that the write completed successfully. Now we can walk through what happens when the bug occurs: Transaction A begins its (pipelined) write: UPDATE test SET val=4 WHERE id=167 . This leaves a write intent on key 167. Transaction B tries to read key 167. It sees the pending write intent and goes into the transaction wait queue . Transaction A tries to commit. With pipelined writes, when transactions commit they must also verify that the intents they wrote earlier are actually present. The first commit attempt passes this check, but while it is in progress a leadership transfer occurs. The old leader aborts its work in progress and the commit will be automatically retried on the new leader. Now we hit the bug: Transaction A erroneously reports to the transaction wait queue that it has committed. Transaction B wakes up and springs into action. Since it has been informed that transaction A committed, it proceeds to resolve the write intent on key 167. This means that from the perspective of an outside observer, key 167 now has the value 4 even though transaction A hasn't completed yet. Transaction A makes a second attempt to commit (on the new leader). This time the commit fails because the pipelined write intent is no longer there - it was resolved by transaction B. Now both transactions are confused. Transaction A believes that it has failed and reports an (unambiguous) error to the client. Transaction B believes that transaction A succeeded, and resolved its intent to make this effectively true. If these had been multi-key transactions, the situation could have been even worse: transaction B could have committed some intents while transaction A aborted others. This would mean that transactions that should be atomic could be broken up, violating one of the pillars of ACID. The bug is more severe than it first appeared, when we thought it was just about misclassifying ambiguous errors (although we never saw an in-the-wild instance of this more severe failure mode). The fix After all this, the fix was almost embarrassingly simple: the addition of a single if statement (in a patch that also included some other changes for testing). Unfortunately, the two months of this investigation overlapped with the Oct 30 release of CockroachDB 2.1 , so the bug was present in that release and fixed a few weeks later in 2.1.1 . On further investigation, the bug turned out to be quite old - it had been present in the codebase since version 2.0 (and was fixed for that branch in version 2.0.7). Version 2.0 introduced the transaction wait queue but did not yet have write pipelining. That's enough to trigger some version of the bug in theory, although in practice write pipelining makes unexpected outcomes much more likely. What we learned This issue demonstrates both the power and limitations of Jepsen as a testing tool. It's one of the best tools around for finding inconsistencies, and it found a bug that nothing else did. However, it's only as good as the workloads in the test suite, and the workloads we had were not very sensitive to this particular error. It took a long time to hit the error, and even when we encountered it, it wasn't very clear what had gone wrong. Developing new test workloads that are more sensitive to this error (and more importantly, other bugs that may be introduced in the future) remain an open development project. If building out and proving the correctness of a distributed SQL system is your jam then good news — we’re hiring! Check out our open positions here.", "date": "2019-02-21"},
{"website": "CockroachLabs", "title": "Reproduction Steps Now Available for the 2018 Cloud Report", "author": ["Andy Woods", "Nathan VanBenschoten"], "link": "https://www.cockroachlabs.com/blog/2018-cloud-report-update/", "abstract": "[THE 2021 CLOUD REPORT IS AVAILABLE. READ IT HERE] CockroachDB is a cloud-neutral database, which means it eliminates dependencies on a particular cloud environment and gives you the flexibility and choice to run it anywhere you like. We are committed to this principle and in order to deliver on this promise, we systematically deploy and test CockroachDB clusters on the three leading US cloud providers: Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure. While executing these tests as part of our 2.1 release a few months back, we noticed something curious: the throughput on AWS test clusters was 40% higher than GCP test clusters. This surprising initial finding led to further internal investigation and experiments on cloud provider performance. We shared those findings in what became the 2018 Cloud Report . Much like our product, the 2018 Cloud Report has taken additional shape and direction from the incredible input from cloud providers (GCP & AWS), the broad open source community and from our Cockroach community. We had a lot of great conversations and with this input, we made updates to the report to reflect some of the most common questions and concerns raised within the community. A huge thank you to everyone who has contributed to the conversation, and who is helping us improve our benchmark. Today, we're releasing an update to the 2018 Cloud Report based on some of the conversations we've had with you. One of the most common questions/requests was for more information on how we configured our experiments to get these numbers. While we used open source benchmarks to test CPU, network, and I/O, there are other factors that impact analyses like this--many of which are difficult to isolate and control. Getting to a similar configuration (for a benchmark) between cloud vendors isn't quite as trivial as it sounds. Today's updated report includes the reproduction steps to the 2018 Cloud Report findings . These steps allow for copy-paste reproduction of our process to empower you to check our work. Have additional questions? We would love for you to join our webinar on February 14 where we'll walk through why we decided to benchmark, what we learned from the exercise, and then outline the gotchas we encountered along the way. Reserve your spot now. CockroachDB remains committed to our stance as a cloud-neutral database. We'll continue to benchmark GCP and AWS for internal stability and performance testing, but since we expect these results to change as companies continue to invest in future infrastructure, our benchmarks will continue to evolve. In future editions, we will expand our test efforts to Microsoft Azure, Digital Ocean, and other cloud platforms. [THE 2020 CLOUD REPORT IS AVAILABLE. READ IT HERE]", "date": "2019-02-07"},
{"website": "CockroachLabs", "title": "Why we're switching to calendar versioning", "author": ["Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/calendar-versioning/", "abstract": "One small step for Cockroach Labs, one giant leap for our release numbering. Since our initial launch, Cockroach Labs has used semantic versioning in our release cycle guidelines. Two years, one major release, and n-patch fixes later, we're making the switch to Calendar Versioning. This means subscribers to our release notes will see quite the jump in today's version numbering, from last week's 2.1.5 to today's 19.1 beta . Why the switch? While working on our upcoming April release, we kept butting up against some problems with our semantic versioning system. We knew the next release would be significant for our community and user base. Among other things, it includes a cost-based optimizer that can complete the TPC-H benchmark, CDC, and two major enterprise security features. But was this enough of a value jump to merit the leap to 3.0? Or was this a 2.2? Strictly following the semantic versioning rules would specify 2.2, as there are no backwards incompatible API changes. But with that logic guiding the major version bump, we could potentially remain on 2.x forever. We wanted to find a solution that would minimize frustration (and time-consuming meetings) internally, while setting the correct expectations with users around quality and stability. Switching to calendar versioning side-steps versioning discussions now and in the future, and helps us (and our users) avoid the confusion around the significance of a release. It also eliminates the mental gymnastics needed to figure out how old a release is or how much time has elapsed between two releases. Another significant advantage of this approach is that naming guidelines are essentially baked in. Since we release on a 6 month cadence, we're starting to name releases by season and year, which means our next release is now CockroachDB Spring '19. Keep your eyes peeled. Notes on numbering A couple details on the semantics of our numbering system: we're using a two-digit year for the major component and release number within the year for the minor one. We chose release number rather than two-digit month to reduce the chance users inadvertently violate our constraint that we don't support skipping versions for upgrades. For patch releases, we'll use the third, \"micro\" number in the versioning scheme to indicate the patch number, omitting the micro number on the first release number for external representations of the version number. Our Spring '19 (formerly 2.2) release will now be 19.1. Our Fall '19 release will be 19.2, the first patch version in Fall will be 19.2.1. For more information about today's beta and our upcoming release, check out today's release notes . New to Cockroach? Click here to install CockroachDB.", "date": "2019-02-25"},
{"website": "CockroachLabs", "title": "Where is Data Regulatory Compliance Worth the Cost?", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/data-localization-compliance/", "abstract": "In 2016, LinkedIn chose not to comply with Russia's requirement for data to be stored locally. As a result, they were kindly blocked from doing business in the country. Facebook and Twitter, on the other hand, both decided that compliance in Russia is worth the effort. Neither has fully met Russia's requirements but they have shown enough progress to avoid being blocked. This is an example of the kind of decision that data localization laws force companies to make. Should we spend the time and money to be compliant or should we do an about-face? Giving up on substantial markets is not an easy thing to do in capitalist economies, where the money you lose by slowing down often seems more important than the money you already made. Right now roughly 69% of the world's population lives within borders governed by data privacy regulations. That percentage is likely to grow, and the existing restrictions are likely to strengthen their grip in the future. The sweeping freedom of the world-wide-web has been tamed and turned into a collection of disparate country-regulated-webs. LinkedIn is just one of many companies deciding to simply walk away from a market. Digital businesses with global reach or global ambitions are in a difficult position. Many are not equipped with a database that scales horizontally . Even if you're using a distributed database there are costs of compliance that might outweigh the financial opportunities. The consequence of this reality is that companies are delaying their expansion into some countries, and abandoning others altogether. To Comply Or Not To Comply? The decision to pursue or abandon compliance involves mathematical gymnastics unique to each business. Without knowing the details of your database architecture and your product roadmap we can't accurately calculate the cost/benefit of becoming compliant. That being said, there are two important factors that will inform the compliance conversation no matter what business you're in: ' compliance difficulty ' & ' market opportunity .' We've taken both of those factors and built a simple ratio to help you visualize the compliance decision. Cost of Compliance Ratio & Heat Map Market Opportunity ÷ Compliance Difficulty = X In our Cost of Compliance Ratio 'market opportunity' is assigned a number based on the Gross Domestic Product of each country. 'Compliance difficulty' is assigned a number based on the broadness and strictness of the regulations in each country. 'Market opportunity' is our numerator and 'compliance difficulty' is our denominator. We realize that this is not Dijkstra's Algorithm or Newton's Method. Our goal is simply to give you a bird's eye view of what the compliance challenges look like across the globe. In the infographic below you'll find our ratio, as it applies to 15 countries with substantial GDP's. Below the chart, you'll find our Compliance R.O.I Heat Map in which we took the ratio data and applied it to a map. The substance for each of these infographics was derived from this white paper about the current state of data localization regulations . These infographics are not hearty enough on their own to enable you to make a decision about compliance. Our hope is that these images and the corresponding white paper help you to have an informed discussion about what your compliance strategy will be. We would love to be a resource while you go through the process of deciding where and how to comply with data localization regulations. If you're interested in learning more you can start with this blog about the Future of Data Protection Law or this blog about Compliance Strategies .", "date": "2019-03-19"},
{"website": "CockroachLabs", "title": "The Future of Data Protection Law", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/data-protection-law/", "abstract": "GDPR went into effect less than a year ago. And still, the era of conducting global business with limited legislative obstructions already feels like some free-spirited, far away past. Right now the global landscape of data protection law is littered with obstacles and exceptions. GDPR has been the loudest but there are plenty of other regions and countries with regulations in place. Even within the E.U., countries like Germany and Switzerland have their own unique protection regulations. Russia and China have very draconian laws, and they're changing quickly. There are around 120 countries now with data protection laws in place. The point is that today's data protection laws offer plenty of challenges, and they're just the beginning . In five years these data protection laws will seem as quaint as VHS tapes. The stakes for global businesses will increase proportionally as regulations become stricter and enforcement becomes ubiquitous (and not just aimed at Google). I'm going to read the tea leaves now, and offer some conjecture about how data protection laws will evolve in the future and what companies can do to prepare. GDPR To Become More Draconian Something I've recently realized (and perhaps the EU has also realized), is that people will trade their privacy away without batting an eye. Without a second thought, they'll trade their privacy away for something as small as a single Instagram follower. Privacy means so little to most people because it's an often complex abstraction. But it can become a very concrete problem just as quickly as you can trade it away. Privacy is worth a heck of a lot more than people routinely value it. But you can't convince someone of that until it's been violated. GDPR has huge enforcement potential (super steep fines), but the actual regulations don't really grapple with the fact that the people they're trying to protect are much too prone to casually sacrificing their personal data to any interested company offering a marginal incentive. GDPR tries to balance protection with informed user consent. I may be a cynic, but acquiring explicit, informed consent is difficult when the burden falls on the user to become informed. What sleazy or incautious companies end up with is explicit, uninformed consent, but it's indistinguishable from the \"informed\" variety, so long as the legal boilerplate is observed. This is where GDPR will likely be amended in the future. If the goal is to protect user privacy, policymakers cannot escape a coming showdown with the inconvenient fact that people will eagerly trade their privacy for a chance to look at \"10 Celebrities Who Didn't Age Well.\" I know the direction of this logic is headed towards a 'nanny state.' I don't think the E.U. will ever look like what we see in China or Russia (a juicy topic for another day). But the existing GDPR regulations are short-circuited so easily by this user consent problem. I just don't know what solution there is, except to become more draconian. Is The United States Next? The United States, to this point, has not meaningfully entered the fray. But it feels like change is imminent. Governor Andrew Cuomo began steering New York towards data protection laws with his cybersecurity regulations back in 2017. California's data privacy law will go into effect in 2020. If the U.S. is going to move forward with disparate state-by-state data protection regulations, the challenge of compliance would be massive. Today, companies are being severely taxed worrying about compliance with regulations in countries and regions. To imagine requiring compliance with different US state sovereignty regulations makes the head spin. The cost to businesses could be astronomical. In January of this year, the Government Accountability Office published a report recommending that the United States Congress consider the implementation of data privacy legislation. This kind of legislation will likely take years to coalesce. But if you're not worrying about it to some extent, you probably should be. It's coming. In the meantime, more states will continue constructing their own data protection laws. It's possible that the whole country could be covered in this strange, unbalanced scaffolding of state-specific data regulations. Data Protection Stress & Silver Lining The only certainty for the future of data protection laws is that they will proliferate. To what degree is uncertain. But you can hang your hat on the fact that at some point your business will need an architecture that can adapt to evolving regulations, wherever you do business. I read this really interesting report recently, by Accenture , in which they showed that CTO's and CIO's are, in large part, worried about the future of global business because of data protection regulations. The upshot is that many companies are planning to punt on doing business in –or expanding to– certain countries because of the laws. CIO's, CTO's, Chief Architects, and other business decision makers are asking themselves questions like: 'Is it worth it to become compliant in India, China or the E.U….?' \"Can we even do that technically? Can we adhere to this new set of regulations in such a way that we are in compliance?\" Those are the right questions to be asking. If you're the Chief Architect and your job is to make the decisions about how to move forward you don't have a lot of options. Most data sovereignty laws are specific about the requirement that data is stored locally. With any monolithic database architecture (this includes virtually every mainstream, traditional vendor from Oracle and SQLServer to MySQL and Postgres), this means you'd need to run a version of your business or service in each region with strict data domiciling regulations. This is an expensive cost structure, as your operating costs cannot achieve economies of scale. Technically, this often requires application-level balancing of database connections, loss of transactionality across siloed services, and complexity which accrues cumulatively. You'll have to weigh that cost with the market opportunity and make a decision. Or, consider updating your architecture. To comply with regulations and to flexibly adjust to future regulations, you need a truly global data architecture. The world is more connected than ever; customers move, customers travel, and customers interact across regional boundaries. The right data architecture should reflect that reality. The foundation for building a global data architecture is a database capable of partitioning data geographically according to domiciling constraints. This infrastructure allows services to be deployed once and expanded as necessary to meet demand, even as new regions have differing data sovereignty requirements. This enables economies of scale for your operational costs. It also manages complexity by pushing difficult problems (like transactional guarantees and queries across regions) from the application layer into the database's domain. While this blog entry has had a bit of a gloomy tone, we can end on a lighter note. Rapid advances in available tech have set into motion an industry-wide migration to the global cloud, and with it a transition to cloud-native, distributed architectures. And as with every historical technological shift, new database architectures have arisen to tackle expanding business requirements and opportunities by exploiting new resources and capabilities. With a globally spanning CockroachDB cluster, for example, you can pin user data to a particular country/region using geo-partitioning . Pinning the data will have the added (and not insignificant!) virtue of decreasing the user-experienced latency (think of this as the revenge of the Aussies). Additionally, CockroachDB is cloud-neutral. Which means that you can avoid being locked into one specific cloud provider. The promise of the global cloud encompasses more than trading your current deployment environment for the AWS monoculture. The reality for most businesses is to build a bridge to a cloud-first future via a hybrid embrace of the public cloud. Once there, flexibility to use a combination of providers will be the key to agile adjustments required for the data protection laws of the future.", "date": "2019-02-26"},
{"website": "CockroachLabs", "title": "One-Step MySQL Import in CockroachDB", "author": ["Roland Crosby"], "link": "https://www.cockroachlabs.com/blog/one-step-mysql-import-in-cockroachdb/", "abstract": "Update 3/4/19: This feature is out of beta! Want to learn more? Check out our webinar on migrating MySQL data to CockroachDB. We want to make it easy for users of existing database systems to get started with CockroachDB. To that end, we’re proud to announce that CockroachDB now has beta-level support for importing MySQL database dump files: you can now import your data from the most popular open-source database to our modern, globally- distributed SQL database with a single command. We had already done some work to enable this -- since version 1.1, CockroachDB has supported importing data from tabular formats like CSV and TSV -- but there were some obvious limitations to this existing functionality. Because CSV files don’t contain any schema information, users could only import one table at a time, and the target CockroachDB schema needed to be specified explicitly. In addition to being verbose, this process also steepened the learning curve for getting started with CockroachDB, since the schema-specification step required users to be familiar with the intricacies of CockroachDB data types before loading their data. Could there be a more direct way to migrate data to CockroachDB in bulk? Many popular database systems offer functionality to produce SQL “dump” files, which consist of long lists of SQL statements that define the schemas and populate the data for multiple tables. (CockroachDB itself can also generate files like this, via the cockroach dump command.) While dump files are convenient for ad-hoc backup and restore operations within a single database system, the plethora of existing SQL implementations makes these dump files inherently proprietary. They often rely on vendor-specific SQL statements and extensions: CockroachDB or Postgres can’t directly execute the SQL commands in a dump file produced by MySQL, and vice versa. And even if all dump files used mutually-intelligible SQL syntax, transactionally running the INSERT statements in a dump file would be much slower than our existing bulk import operations. Despite these limitations, the convenience and ubiquity of SQL dump files convinced us that it was worth the engineering effort to natively support importing them. We’ve seen a large volume of customer interest in migrating from MySQL — for example, the Chinese internet giant Baidu moved critical application data for a billion users from MySQL to CockroachDB, and now stores 40 TB of data in CockroachDB — so we prioritized building MySQL import for CockroachDB 2.1. Using MySQL Import Here’s what a real-world migration from MySQL to CockroachDB looks like, in which the data from a Drupal demo site running on MySQL is migrated to CockroachDB with a single SQL statement: Export your data from MySQL to a dump file using the mysqldump utility Execute an IMPORT MYSQLDUMP statement in CockroachDB Verify that the imported tables from the MySQL dump are now present in CockroachDB Beneath this simple process is a great deal of complex engineering: First, we need to parse the SQL statements in the file. Because CockroachDB’s SQL dialect adapts the syntax and semantics of Postgres , our parser doesn’t natively understand the MySQL dialect. Fortunately, the Vitess project provides an open-source MySQL parser that we were able to adapt to meet our needs. Once the input file is parsed, CockroachDB needs to translate the input table definitions and data types to its own data types. We researched the existing data and sequence types supported by MySQL and mapped them to the closest appropriate CockroachDB data types. After each table’s data types are determined, the actual table data needs to be written to CockroachDB. We adapted our existing CSV import system to add entire tables at once without the performance penalty of executing multiple logical INSERT statements. CockroachDB first samples the input SQL dump file to determine the approximate distribution of rows across the keyspace. It then generates the appropriate key-value pairs to store the data, and distributes them to the database nodes that will store them. Gotchas with MySQL Import Importing data from SQL dump files is currently in beta — while we’re generally confident in our implementation of this functionality, the complexity of MySQL’s grammar means that there are going to be some issues. Known issues that we intend to address prior to the general-availability release of this functionality include: CockroachDB can’t import MySQL tables that define fulltext indexes ( #29625 ). Workaround: Remove the fulltext index definition from the schema in the dump file. CockroachDB can’t import certain invalid date values supported by MySQL ( #29298 ). Workaround: Update MySQL records with date fields containing zeros to use NULL or a different sentinel value; change column definitions that specify a default zero date to be nullable. SQL dump import performance is lower than CSV import performance, because reading a single large input file isn’t yet parallelized across cores or nodes. These issues aside, we’re confident that MySQL dump import will greatly simplify the process of getting started with CockroachDB. Follow the instructions in our MySQL migration guide to try it yourself — and please file a bug report if you run into any issues importing your data.", "date": "2018-11-15"},
{"website": "CockroachLabs", "title": "How to Improve Performance using Geo-Partitioning", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/geo-partitioning/", "abstract": "One of the most exciting features of distributed SQL is the ability to tie data to a specific location and CockroachDB delivers this capability via our geo-partitioning feature. If you aren’t already familiar with this yet we’ve recently published a few items that help explain how it works, including a video demo and a tutorial . But first, here’s an explanation of geo-partitioning: Geo-partitioning is the ability to attach data (at the row-level) to a location. This enables you to control data locality in the database, as opposed to requiring manual schema changes and complex, brittle application logic. Geo-partitioning is also distinctly different than ‘partitioning’ because it combines the values of the data with the physical implementation of the database itself. In a distributed SQL database , each node runs on a server and that server has a location. CockroachDB is able to use this information to allow for geo-partitioning of data. It is the only database that offers geo-partitioning. Often, geo-partitioning is discussed within the context of Data Localization (which is suddenly a very portentous topic in the wake of the Mark Zuckerburg & Yuval Harari conversation ). The ability to pin customer data to a specific location can help make your business compliant in countries that require data to be domiciled there. What gets lost in the conversation about compliance is the fact that geo-partitioning enhances performance. Geo-Partition For Low Latency In this video demo of geo-partitioning, you can watch how the implementation of geo-partitioning improves application performance by reducing data access latencies. The video features a 9-node deployment across 3 US regions on GCE. Before geo-partitioning is added a max latency of 99% of the queries is in the 100’s of milliseconds . After geo-partitioning, 99% of all queries are now 4 milliseconds or less , and 90% of all SQL queries execute in less than 2 milliseconds . In some cases, latency is even sub-millisecond . We think this kind of performance improvement is really exciting and our esteemed docs team built out a tutorial so you can get your hands in the geo-partitioning soil. We hope that you’ll use the tutorial to engage with geo-partitioning. And we look forward to your questions about how you can leverage this feature to enhance your performance. If you’d like to dig deeper into geo-partitioning you can reference these resources to learn more: • Blog Post : What Global Data Actually Looks Like • Webinar : The Power of Data Locality in Distributed SQL Please feel free to connect with us in the CockroachDB Forum to share your feedback.", "date": "2019-05-02"},
{"website": "CockroachLabs", "title": "Automatic Table Statistics in CockroachDB", "author": ["Rebecca Taft"], "link": "https://www.cockroachlabs.com/blog/automatic-sql-statistics/", "abstract": "Last year, we rebuilt our cost-based optimizer from scratch for CockroachDB’s 2.1 release. We’ve been continuing to improve the optimizer since then, and we’ve added a number of new features for the CockroachDB 19.1 release . One of the new features is automatic collection of table statistics. Automatic statistics enables the optimizer to make better decisions when choosing query plans. This post explains why statistics are important for the optimizer and describes some of the challenges we overcame when implementing automatic collection. A Tale of Two Queries Consider the following two SQL queries, used by an imaginary kitchen supplies company to count the number of toaster ovens purchased by customers in New York City, grouped by the date of purchase: SELECT count(*), o.purchased\n    FROM products AS p,\n         orders AS o,\n         customers AS c\n    WHERE c.id = o.cust_id\n         AND p.id = o.prod_id\n         AND c.city = 'New York'\n         AND p.type = 'toaster oven'\nGROUP BY o.purchased; Query A SELECT count(*), o.purchased\n    FROM customers AS c,\n         orders AS o,\n         products AS p\n    WHERE c.id = o.cust_id\n         AND p.id = o.prod_id\n         AND c.city = 'New York'\n         AND p.type = 'toaster oven'\nGROUP BY o.purchased; Query B With minimal optimization, these queries correspond to the following logical query plans: Query A Query B These queries compute the same result, but one query is vastly more expensive than the other. Can you tell which one? (scroll down to see the answer…) . . . . . . . . . Spoiler alert: it’s not possible to determine which query is faster without more information. So suppose I give you the following statistics: Table 1 Now can you tell? Before I give the answer, let’s discuss what makes a query expensive in the first place, and why you should care. The “cost” of a query roughly translates to the total amount of computing resources required to execute the query plan, including CPU, I/O and network. Lower-cost plans tend to execute faster (i.e., they have lower latency), and also enable the system to maintain a higher throughput, since more resources are available for executing other queries simultaneously. That means less money spent on extra hardware, and less time waiting for results! How do you estimate the cost of a query? We can estimate the cost of a particular query plan by estimating the cost of each operation in the plan and adding them together. For example, consider Query Plan A. You can think of data as flowing through the plan from bottom to top. First we scan table customers and apply a filter, then we scan table orders and join the two together, etc. Each stage in the plan costs a certain amount depending on the type of operation and the amount of data that must be processed. Operations involving I/O are generally more expensive than operations involving only CPU, so reading 10,000 rows from disk (e.g., as part of a scan) is much more expensive than applying a filter to those rows. The actual cost formula varies for each operator and is beyond the scope of this blog entry, but the important thing to know is that the cost of each operation is directly proportional to the number of rows processed . So how do we estimate the number of rows processed by each operator? With statistics, of course! Similar to how data propagates from the bottom to the top of a query plan, we can also propagate statistics from the bottom to the top to estimate the number of rows at each step. Query Plan A with Statistics Let’s look at the example of query plan A. Given the stats from Table 1, we know that customers has 100,000 rows. Since in this simple example we don’t have any indexes on customers , the scan must process all of the rows. Similarly, the filter operator must process all rows produced by the scan to test the filter predicate city='New York' on each row. But this is where it gets interesting. To estimate the number of rows that match the filter predicate and pass on to the join operator, we need to calculate the selectivity of the filter, or the percentage of rows that are likely to match. For this calculation, we make the simplifying assumption that all values are uniformly distributed in each column. For example, we assume that a column with 30 rows and 3 distinct values will have 10 of each value. In our running example, we can see from Table 1 that column city has only two distinct values (it turns out our imaginary kitchen supply company only has two locations). After we apply the predicate city='New York' , there is at most one distinct value, the value 'New York' . Utilizing the assumption of uniformity, we can estimate the selectivity of the predicate as 1 ⁄ 2 . With 100,000 input rows, we estimate that after the predicate is applied there will be 100,000 * 1 ⁄ 2 = 50,000 rows. Note that this result of 50,000 rows is just an estimate , since the assumption of uniformity is not always correct. It’s possible that the data is skewed and nearly all of all rows match city='New York' . It’s also possible that almost none of the rows match city='New York' . In the next release we will incorporate histograms into our estimates to handle such cases. But in practice, the uniformity assumption works reasonably well for query optimization. The cost-based optimizer does not need to know the exact cost of a query plan; it just needs a good enough estimate that it can compare two plans and select the relatively cheaper plan. Given that we estimate 50,000 rows from the filter, how do we propagate the statistics up the tree? We can’t directly apply the 0.5 filter selectivity to the distinct count, since that would be statistically incorrect. Suppose that there is another column in customers that tracks the customer’s gender . Assuming that gender is independent of city (which is true, last time I checked…), it is likely that gender will still have (at least) two distinct values after half of the rows are removed that don’t match city='New York' . If we were to blindly apply the selectivity to distinct counts (similar to how we applied it to the row count), we would estimate that gender had only one distinct value, which is clearly incorrect given the assumption of independence. Instead, we use the following formula: d' = d-d*(1-selectivity)^{n/d} Where n is the number of input rows, d is the distinct count before the selectivity is applied, and d’ is the distinct count after. It can be derived as follows: If each distinct value appears n/d times, and the probability of a row being filtered out is (1-selectivity) , the probability that all n/d rows are filtered out is (1-selectivity)^{n/d} . So the expected number of values that are filtered out is d*(1-selectivity)^{n/d} . This formula has the nice property that it returns d*selectivity when d=n but it’s closer to d when d << n . Similar to the uniformity assumption, the independence assumption is not always correct, but it makes calculations significantly simpler. We’ll likely relax the independence assumption in future releases of the optimizer. Moving further up the query plan, we must next calculate the number of rows produced by the join. This is a notoriously difficult problem, and remains an open area of research 1 . For equi-joins such as the ones in queries A and B, we make the simplifying assumption that we have a primary key-foreign key join, so the output cardinality will be equal to the larger of the two inputs multiplied by the selectivity of any filters that have been applied. Distinct counts are not affected significantly by joins. As the last step in both query plans, we apply a GROUP BY operation which calculates the number of matching orders grouped by purchased date. Luckily, calculating the number of output rows of a GROUP BY operator is easy: it’s equal to the number of distinct values of the grouping column(s). Although this example only covers a subset of all SQL operators, it gives a good flavor for how we use statistics to estimate the cost of different query plans. Hopefully now you can see that query A is significantly more expensive than query B. The CockroachDB optimizer will always transform query A so that it uses the same query plan as B. And for good reason: when we execute the two plans, we see that query A takes 1.9 seconds while query B takes 16 ms: over a 100X difference! How CockroachDB Collects Table Statistics So how do we get statistics such as the ones shown in Table 1 that are used by the optimizer to calculate the cost of candidate query plans? In CockroachDB, we collect statistics using the command CREATE STATISTICS . CREATE STATISTICS performs a distributed, full table scan of the specified table, and calculates statistics in parallel on each node before a final merge (see Figure 1). After collection, statistics are stored in the system table system.table_statistics , and are cached on each node for fast access by the optimizer. Figure 1: The distributed SQL plan of a CREATE STATISTICS statement on 5 nodes The statistics we currently support are: Row Count This is an exact row count as of the time of the table scan. Although this is the simplest statistic, it’s also the most important. As you learned in the previous section, row count is the primary input to our cost model for determining the cost of each candidate query plan. Distinct Count For each index column and a subset of other columns in each table, we estimate the number of distinct values. As described above, distinct counts are useful for estimating the selectivity of filter predicates and estimating the number of rows output by GROUP BY and other relational SQL operators. We don’t calculate exact distinct counts, because for columns with many distinct values, the calculation could require nearly as much memory and disk space as the entire table itself! Instead, we use a variant of the well-known HyperLogLog algorithm called HLL-TailCut+ 2 and the excellent golang implementation by Axiom, Inc 3 . HyperLogLog estimates distinct counts with high accuracy and very low memory overhead. It can also be calculated in parallel across several nodes and efficiently merged to produce a distinct count estimate for the full table. Null Count We calculate null counts explicitly, as NULL is a value that can appear quite often in some workloads. We can get better cardinality estimates for certain queries by tracking this count separately. In the future, we plan to support more statistics, including multi-column distinct counts and histograms. Changes to Table Statistics in CockroachDB 19.1 The CREATE STATISTICS command existed in the 2.1 release, but we found that most customers were not taking advantage of it, and the few that knew about it were not using it effectively. There were a few reasons for this: We did not advertise the feature, so many customers were not aware of it. We did this purposefully, since as described below, CREATE STATISTICS is difficult to use correctly. Even if they discovered the command on their own, they might have run it once and forgotten to run it again. This is a problem because statistics can quickly become stale for tables that are modified frequently. Running CREATE STATISTICS once can be worse than never running it at all, since we have some reasonable defaults in the optimizer if there are no stats. But we’ll always use whatever stats are available, so suppose you create a table, add one row, and then run CREATE STATISTICS . And then you add 1 million rows but forget to refresh the stats. The optimizer will choose a plan optimized for the case of one row, even though that may be an awful plan for the case of one million rows. A savvy user might have realized that they needed to refresh stats periodically, but then it was not clear how often to perform a refresh. The optimal frequency may be different for every table, since some tables are updated frequently and others are updated less frequently. Furthermore, some tables are large, and even a lot of updates don’t change the stats much, while some tables are small and a few updates can drastically change the stats. Even if a user managed to overcome all of these hurdles, we were not satisfied with the status quo. Our mission at Cockroach Labs is to “make data easy”, and forcing users to perform their own periodic stats refreshes did not fit with this mission. The Solution: Automatic Statistics Collection The solution, of course, is for CockroachDB to perform statistics collection automatically. Our idea at the beginning of the release was to detect when some significant portion of the data in a table had changed, and then automatically trigger statistics collection by having the system call CREATE STATISTICS . This introduced two key challenges: We want to detect when a significant amount of data has changed, but there should be negligible impact on the performance of INSERT , UPDATE and DELETE transactions. Once we’ve decided to trigger a statistics refresh, the collection of statistics should have negligible impact on the performance of concurrent transactions. Let’s examine each of these challenges: Deciding to Trigger a Refresh In order to decide when to trigger a refresh, we want to know when a “significant” portion of a table has changed. To make this concrete, let’s say that we want to update stats after approximately 20% of the rows in a table have been inserted, updated or deleted. The advantage of using a percentage instead of an absolute row count is that it allows the frequency of refreshes to scale inversely with the size of the table. So the challenge is to determine when 20% of rows have changed. Perhaps you’re thinking, “this doesn’t seem very hard…”. At first glance, it does seem straightforward; we know how many rows are affected by each mutation operation on each node. But the problem is that we can have many mutation operations happening simultaneously to the same table on different nodes, and we’d like to avoid having a central counter somewhere that could be a source of contention. The only way a geo-distributed system like CockroachDB scales is if nodes are largely independent. Luckily, the decision to refresh a table after some percentage of rows have changed does not need to be exact. There is a tradeoff between more frequent refreshes (which add overhead to the cluster), and less frequent refreshes (which could result in suboptimal query plans). We have found that refreshing with 20% of rows “stale” provides a good balance, but 10% or 30% would also be perfectly fine in most cases. Given this flexibility, we decided to solve the problem by using the following statistical approach: after every mutation operation (e.g., INSERT , UPDATE or DELETE ), there is a small chance of statistics getting refreshed. In particular, the probability of a refresh is: P(refresh) = \\dfrac{number  of  rows  updated,  inserted,  deleted}{number  of  rows   in   table * 0.20} To implement this statistical approach, we essentially use the following simple algorithm (although there are some complexities discussed in the next section): after each mutation, generate a random number between 0 and 1, and if it falls below this probability value, kick off a CREATE STATISTICS run. What this means is that over time, stats for each table are refreshed after approximately 20% of rows have changed. We also have some guard rails in place in case there are statistical outliers. In particular, we always refresh stats for a table if it has no stats yet or if it has been a long time since the last refresh. Running the Refresh The second challenge was ensuring that running a statistics refresh did not significantly impact running workloads. It’s one thing if there is high overhead when a user knowingly runs CREATE STATISTICS on the command line, but it’s another thing if we trigger it without their knowledge and all of a sudden the command is using half the resources in the cluster. Since a statistics refresh can happen at any time, it must have minimal overhead. This requirement forced us to rethink the solution described in the last section for triggering a refresh after 20% of rows changed. In particular, the simple algorithm of possibly triggering a refresh after each mutation was problematic. There can be many updates per second, but each CREATE STATISTICS run can take minutes. As a result, we could have multiple threads scanning the same table to collect statistics at the same time. This is a big problem, since a single full table scan can impact performance. Many table scans at once can actually bring down the cluster. The solution we came up with was to have one background “refresher” thread running on each node. Mutation operations such as INSERT , UPDATE and DELETE send messages to that thread with the table and number of rows affected, and the refresher aggregates the counts of rows updated for each table on that node. Periodically, the refresher thread starts up a separate thread that uses the latest counts to possibly kick off a statistics refresh for each table. The background refresher thread ensures that at most one statistics refresh is triggered at once per node, but it does not provide any coordination between nodes. To ensure that at most one automatic statistics refresh is running globally on the cluster, we took advantage of the existing “jobs” infrastructure used to run commands like IMPORT , BACKUP and RESTORE . In the last release, CREATE STATISTICS was a normal SQL statement, but we changed it during this release cycle to run as a job. Now, every time CREATE STATISTICS is called, it creates an entry in the system.jobs table. If a node wants to perform a refresh, it first checks the system.jobs table to be sure that there are no other statistics jobs currently running. In this way, we ensure that only one statistics refresh is active at once. The jobs infrastructure ensures that we always make progress, since if a node dies, another node will adopt the job. Even with all of this infrastructure in place, we found that the overhead of a single CREATE STATISTICS job was too high for some workloads. Workloads with heavy utilization of resources saw throughput drops and latency spikes each time a statistics refresh started. This observation led us to the final requirement: we needed to limit the overhead of each individual CREATE STATISTICS job. The solution we used was to throttle the execution of statistics collection. Specifically, we insert some idle time (i.e., sleep) after every 10,000 rows are processed. The amount of idle time is adaptive, and depends on the current CPU utilization of the cluster. If utilization is high, we sleep more, and if utilization is low, we turn off throttling altogether. The Practical Stuff By this point you may be wondering, “How do I actually use this feature?” Consistent with our mission to “make data easy”, you don’t need to do anything other than upgrade to CockroachDB 19.1 . Automatic statistics are enabled by default in 19.1, so unless you explicitly disable the feature, the system will trigger statistics refreshes as needed. Although we’ve made every effort to minimize the impact of automatic statistics collection on performance of the system, there will always be overhead due to the cost of performing a full table scan for each refresh. For most workloads, the amortized impact on performance is less than 1%, but we’ve seen some cases with larger performance degradation. If your workload is negatively affected, our documentation on the automatic statistics in CockroachDB describes how you can adjust the frequency of refreshes or disable automatic collection altogether. It’s still possible to run CREATE STATISTICS manually if you want full control over when stats are refreshed. Conclusion We hope this post has convinced you that statistics are essential for the optimizer to produce good query plans for a wide variety of queries, and we hope it has allowed you to peek inside the optimizer to learn a bit about how it works. CockroachDB 19.1 provides tools to collect statistics both manually and automatically, with the goal of keeping statistics fresh for the optimizer to use with minimal performance impact. Happy querying! Notes See, for example, A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned cardinalities: Estimating correlated joins with deep learning. CoRR, abs/1809.00677, 2018. [return] Q. Xiao, Y. Zhou and S. Chen, “Better with fewer bits: Improving the performance of cardinality estimation of large data streams,” IEEE INFOCOM 2017 - IEEE Conference on Computer Communications , Atlanta, GA, 2017, pp. 1-9. [return] https://github.com/axiomhq/hyperloglog [return]", "date": "2019-05-09"},
{"website": "CockroachLabs", "title": "Introducing CockroachDB 19.1", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/cockroachdb-19dot1-release/", "abstract": "It’s been a little over four years since we started our mission to deliver an enterprise-ready distributed SQL database . Today, we’re excited to release CockroachDB 19.1. With this release, we enhanced distributed SQL capabilities, and expanded upon enterprise-grade features for security and data integrations. 19.1 continues to solve the challenge of complex, distributed SQL while meeting all the “enterprise” requirements that are expected of a database . Here’s Nate Stewart, our VP of Product, with a quick intro on what you can expect in CockroachDB 19.1. And for a deeper tutorial with Nate, register for our CockroachDB 19.1 webinar . For a full list of features, check out the feature appendix below, or our release notes . 19.1: SQL features, but distributed A lot of work in our 19.1 release serves our mission of building a distributed SQL database that will allow you to scale more easily while also meeting the needs of even the most demanding applications. This includes general performance improvements and a lot of small changes to our new cost-based optimizer , which we built from scratch and introduced this past fall. Join hints : The optimizer now supports hint syntax to control the use of a join algorithm. In most cases, the optimizer is smart enough to pick the right join algorithm based upon cost inputs such as cardinality. Occasionally, the optimizer can pick a plan that is less efficient than other available options. Hints put over-ride control into the hands of users to allow users to force certain join algorithms when the optimizer missed an opportunity to operate at peak efficiency. Reverse index scans : Specifying scan direction in an index hint is that weird screwdriver sitting in the bottom of your toolbox. It’s not used that often, but it’s really handy when nothing else fits. Forcing a reverse can be really helpful, for example, during performance tuning . Optimizer index locality constraints : CBO + index locality = magical speed for read-only queries of reference tables in geo-distributed clusters (e.g. postal codes). In a geo-distributed cluster, this can lead to large reductions in latency due to improved data locality and reduced network traffic. Automatic statistics : Table statistics have been available for a while now, but we wanted to ease the friction in running them. Automatic statistics does the work for you. With automatic statistics enabled, the optimizer is able to make better selections on how to optimize queries. Two additional highlighted SQL features we introduced in this release include: Logical Query Plans in the Web UI : Users can now view the sampled execution plan for their queries in the UI, giving them greater visibility into how it will be executed and helping to identify bottlenecks. Follower reads: We’ve added functionality for running historical queries against nearby replicas rather than leaseholders to dramatically reduce latency in geo-distributed clusters. Queries using AS OF SYSTEM TIME with a sufficiently old timestamp will be automatically directed to the closest replicas rather than the leaseholders. For users who want to use the most recent timestamp that qualifies a query to be read by a replica, we’ve added a new built-in function (experimental_follower_read_timestamp) to automatically generate an acceptable timestamp. 19.1: Meeting the demands of the enterprise Our enterprise customers have a discrete set of requirements around security and integration that we have pushed forward in 19.1. With this release, we extend and deliver on some of these core capabilities Change data capture for downstream processing While CockroachDB is an excellent system of record, it also needs to coexist with other systems. For example, you might want to keep your data mirrored in full-text indexes, analytics engines, or big data pipelines. To that end, we’ve improved our change data capture (CDC) capabilities to allow data to flow more easily to backend warehouses, event-driven microservices, streaming analytics, search indexing, and “big data” applications. With CDC, CockroachDB 19.1 assures that writes to watched tables are efficiently and consistently emitted to configurable data sinks. CDC captures data that has changed from a source database as a result of create, update, or delete operations. Change data capture provides efficient, distributed, row-level change feeds into Apache Kafka (including compatibility with the Confluent stack) for downstream processing such as reporting, caching, or full-text indexing. Enterprise-grade security CockroachDB now integrates with existing directory services within an organization, simplifying the management of user accounts through single sign-on and bringing them in line with corporate standards. CockroachDB 19.1 also allows organizations to define policies to encrypt data at rest , securing data both at rest and in flight to provide end-to-end protection for the sensitive information typically found in transactional workloads. Learn more about 19.1 and use it today! If you are interested in checking out the complete list of features in this release, we added a summary below and you can also reference our release notes . We’ve also got a 19.1 webinar coming up on May 9 to introduce some more features of the release. And if you have any questions, please join in the conversation over at forum.cockroachlabs.com. Finally, you can download and use 19.1 today ! Thanks! (from all of us here at Cockroach Labs) Appendix: Complete list of new features in CockroachDB 19.1 CockroachDB Core Features Load-based splitting CockroachDB will automatically split a range based on load so that users experience fewer hotspots and maximize their resources Read from Follower with Timestamp Bound We’ve added functionality for running historical queries against nearby replicas rather than leaseholders to dramatically reduce latency in some geo-distributed clusters. Queries using AS OF SYSTEM TIME with a sufficiently old timestamp will be automatically directed to the closest replicas rather than the leaseholders. For users who want to use the most recent timestamp that qualifies a query to be read by a replica, we’ve added a new built-in function (experimental_follower_read_timestamp) to automatically generate an acceptable timestamp. Achieve TPC-C 10k using partitioning with 15 nodes We’ve improved our efficiency and successfully hit this benchmark! CockroachDB SQL Features Schema Change Performance Improvements In this release, we altered how schema changes are implemented so that we use a bulk ingest to speed up the process by 10x. Schema Change Jobs CockroachDB now allows users to see many schema changes as a ‘job’ in the web UI, giving them greater visibility into when schema changes are complete. Schema Changes in Transactions We have improved upon CockroachDB’s ability to modify schema elements within a transaction to include created columns and indexes. Also support for created tables. Inventory SQL Errors Error messages are now more descriptive, and include links to our documentation or Github where appropriate. Parallel Range Scans CockroachDB increased performance by allowing range scans to be conducted in parallel. Improved Vectorized Execution (Prototype) We have demonstrated that a vectorized execution prototype in CockroachDB can dramatically improve performance (up to 3x) for read queries. Increased Migration and Integration Support Fixed some compatibility issues to improve CockroachDB’s overall integrations (Hibernate, Spring ORM, etc) and customer migrations from Postgres. Examples: SERIAL and INTEGER 32-bit implementation To improve the customer migration experience, CockroachDB now supports a flag to modify our SERIAL and INTEGER types to run as 32-bit integers, so Postgres customers can use these types without making changes to their data structure. Customized Retry Savepoint Names CockroachDB now offers a connection setting that lets users name their transaction retry savepoints. This enables us to better integrate with ORMs like Spring and will allow users to take advantage of client-side retries. Cost Based Optimizer Correlated Subqueries (Part 2) Building on the last release, we now support nearly all correlated subqueries in the optimizer. Correlated subqueries are nested queries where the inner query relies on values from the outer query – a structure that often leads to inefficiencies. In 2.1, we optimized these inefficient subqueries by de-correlating them, and now in 19.1 we’ve added support for correlated subqueries that can’t be de-correlated by adding an apply operator that executes a sub-plan for every row in its input. Query Plan Cache The optimizer will now cache optimized query plans. This will improve performance, as CockroachDB will not recalculate the optimized plan for the same queries repeatedly and can now leverage existing plans to quickly create the optimized plan for any query. Join Reordering This functionality allows the optimizer to reorder joins automatically to investigate multiple functionally equivalent plans to pick the best plan for performance. It is a configurable setting (defaulting to 4) such that picking 0 will default to the order in which the query was written. Locality Preference The cost based optimizer can now take locality into account when calculating optimal queries, preferring indexes in the same locality to improve overall performance of geo-distributed clusters.This feature requires the use of per-index zones, an enterprise feature, to work, but it is not an enterprise feature. Query Optimizer Hints In conjunction with improvements to our cost-based optimizer, CockroachDB now provides support for manually editing query plans generated by the optimizer where the generated plan does not maximize efficiency for a particular workload. Automatic Statistics Collection CockroachDB will automatically collect statistics for the cost based optimizer to improve the performance of queries. Previously, we assumed all tables had 1,000 rows and all columns had 10% distinct values, which served as a reasonable guide, but having real statistics will ensure the CBO actually optimizes for a specific scenario. Deprecated Heuristics Based Optimizer In this release, we’ve improved the functionality of features previously released to meet customer expectations and can now serve almost all queries through the cost based optimizer. CockroachDB Visibility & Troubleshooting Logical Query Plans in the Web UI Users can now view the execution plan for their query in the UI, giving them greater control over how it will be executed and helping to identify bottlenecks. CockroachDB Ops & Tools GSSAPI (Kerberos) Authentication for LDAP/Active Directory CockroachDB now supports integration with Kerberos, a common enterprise-level authentication protocol. Users can use their corporate credentials to access CockroachDB according to their pre-configured access level. Encryption at Rest CockroachDB now fully supports the ability to encrypt data while it is stored on disk without requiring changes to client applications for enterprise users. Change Data Capture (CDC) Enterprise Implementation (Iteration 2) In this release, we’ve expanded CDC to be production-ready for enterprise customers. This included adding support for new output formats and improving push latencies. Core implementation An experimental Postgres protocol-based change data capture implementation is now available for both core and enterprise users. This enables CockroachDB users to consume insert, update, and delete events for watched tables without needing to use our Kafka or Cloud Storage data sinks. Cloud Storage Sink CockroachDB now has built-in CDC functionality to deliver changefeed data to a cloud storage sink for ingest into OLAP or big data systems, without requiring transport via Kafka.", "date": "2019-04-30"},
{"website": "CockroachLabs", "title": "Why We're Relicensing CockroachDB", "author": ["Peter Mattis", "Ben Darnell", "Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/oss-relicensing-cockroachdb/", "abstract": "CockroachDB was conceived of as open source software. In the years since it first appeared on GitHub, we’ve tread a relatively typical path in balancing open source with creating a viable business. We’ve kept our core code under the Apache License version 2 (APL), launched a managed service, and gated some features for established companies under an enterprise license. But our past outlook on the right business model relied on a crucial norm in the OSS world: that companies could build a business around a strong open source core product without a much larger technology platform company coming along and offering the same product as a service. That norm no longer holds. Competitors have always been legally allowed to offer another company’s OSS product as a service. Now, we’re finally seeing it take place. We’re witnessing the rise of highly-integrated providers take advantage of their unique position to offer “as-a-service” versions of OSS products, and offer a superior user experience as a consequence of their integrations. We’ve most recently seen it happen with Amazon’s forked version of ElasticSearch, which Salil Deshpande neatly described in TechCrunch as both “self-interested and rational.” To respond to this breed of competitor, we’re introducing a change to our software licensing terms. The full details of the change are below and on Github , but the short version is this: Today, we’re adopting an extremely permissive version of the Business Source License (BSL). CockroachDB users can scale CockroachDB to any number of nodes. They can use CockroachDB or embed it in their applications (whether they ship those applications to customers or run them as a service). They can even run it as a service internally. The one and only thing that you cannot do is offer a commercial version of CockroachDB as a service without buying a license. In order to continue building a strong open source core, this restriction has a rolling time limit: three years after each release, the license converts to the standard Apache 2.0 license. Our goal in relicensing with a time restriction is two-pronged: to simultaneously create a competitive database as a service (DBaaS) while also providing a guarantee that the core product will become pure open source. Evaluating Open Source License Options A number of companies have licensed their products with this same dual goal. When evaluating existing options, we found that they fit into two general camps: the “copyleft” model, and the tiered model, neither of which fit what we wanted to get out of a license change. The “copyleft” model The “copyleft” tradition was started by the GNU GPL and aims to prevent proprietary forks by requiring source code to be published in some cases. The Affero General Public License (AGPL) and the related Server-Side Public License (SSPL) both fall under this camp. The SSPL in particular is intended to address competitive hosted services. However, we believe that these licenses do both too much and too little. Too much, in that the details are complex and the scope of the copyleft requirement is not always clear. Many would-be users are scared off by the publication requirements of these licenses which are potentially very broad. Too little, in that competitors are both willing and able to publish enough code to enable their own services, without providing any commensurate benefit to the authors of the core technology. Amazon did this with Open Distro for Elasticsearch even though they weren’t compelled to do this by a copyleft license. We needed something simpler, and stronger. The tiered model Another option we considered was adopting a three-tier model: open source core, enterprise components, and a middle ground of features that are not open source but available at no cost. This is a popular model, and more or less the model we have had up until today. However, this creates bad incentives for our company. It creates pressure to avoid creating new features in the core and do as much work as possible in the non-open-source components. Basically, we would be tempted to sell our core users short by permanently putting our most exciting features behind an enterprise license. If you squint towards the horizon, this model does not bode well for the open source ecosystem. The BSL solution We started exploring licenses with a time-limited restriction, and found that we didn’t have to create a new license from scratch. MariaDB’s Business Source License (BSL) 1.1 has the provisions we want and has already been endorsed by OSI founder Bruce Perens. The BSL is a parameterized license, so our use of it is not exactly the same as MariaDB’s. The key difference is in the “Additional Use Grant”: MariaDB’s MaxScale product, for example, allows you to use MaxScale with up to three server instances . CockroachDB’s Additional Use Grant allows you to use CockroachDB with as many nodes as you want as long as you are not offering it as a commercial DBaaS. Our BSL protects CockroachDB’s current code from being used as a DBaaS without an enterprise license for a period of three years. After 3 years this restriction lapses and the code becomes open source (per our current Apache license) and is free to use for any purpose. We’re applying this license to the core edition of CockroachDB (i.e., the code that is currently under the Apache 2.0 license). This means that CockroachDB core is no longer Open Source (according to OSI’s Open Source Definition), although the complete source code is still available, and any commercial usage is allowed with the one exception of building a DBaaS. We believe this is the best way to balance the needs of the business with our commitment to Open Source. The new license still permits the vast majority of users to use, redistribute, and modify CockroachDB freely, and will become no-strings-attached Open Source after three years. Nuts and Bolts: How the BSL Works We are relicensing CockroachDB beginning with 19.2, adding the restriction that it may not be used in a commercial database-as-a-service (DBaaS) without a license agreement with Cockroach Labs. The three-year restriction is applied on a rolling basis for each release. CockroachDB’s enterprise features will continue to use the Cockroach Community License (CCL). Use of enterprise features will always require a license agreement with Cockroach Labs; this license does not convert to open source after three years. To put this in concrete terms, CockroachDB 19.2 (tentatively scheduled for October 2019) will be the first release to use this new license scheme. It will include code under both the BSL and CCL. In October 2022 (three years after its release), the portions of CockroachDB 19.2 under the BSL will convert to the APL. Patch releases don’t change the conversion date: all 19.2.x releases will convert in October 2022, even though only the base 19.2.0 release is three years old at that point. CockroachDB 20.1 (planned for April 2020) will become open source in April 2023, and so on. Older versions are not affected by this license change: CockroachDB 19.1 is still using the Apache license, and all present and future patch releases in the 19.1.x series will also use the APL. We're committed to building a powerful core product that is open source. Even though the BSL isn’t an open source license, this compromise felt closest to the spirit of open source, while still protecting our business. And three years after each release, our commitment to Open Source automatically completes. We believe that the path we’ve chosen is how to build an open core company and still maintain a strong open source legacy in this new reality.", "date": "2019-06-04"},
{"website": "CockroachLabs", "title": "4 Challenges When Migrating to a Cloud-Native RDBMS", "author": ["Keith McClellan"], "link": "https://www.cockroachlabs.com/blog/cloud-native-relational-database-management/", "abstract": "As organizations migrate to the cloud, they need a cloud-native, relational database to help them move all their applications to this new environment. Over the last ten years, the infrastructure that runs our applications has fundamentally changed. As we move to the cloud, we now have to think about managing workloads in an environment where we don’t have tight control over the infrastructure that hosts our applications. Things like “how can I recover my data?” and “what happens if my instance fails?” are very different when you host applications in the cloud. And we’re doing it at a scale that was once only considered by a handful of companies worldwide. Largely, these changes have been for the positive. Launching a new technology no longer requires months of planning, procuring, and commissioning hardware. Our individual apps have become a combination of micro-services that work together and can be set up to scale dynamically as traffic demands. We can try out something new without having to convince management that it’s worth a capital investment. However, when it comes to data, it hasn’t been easy to make the transition. RDBMS migrations to the cloud are difficult. The large companies that were first to move to the cloud made some difficult decisions to sacrifice functionality to create resilient, scalable databases. NoSQL databases like Apache Cassandra(™) are an example of this - extremely performant, scalable, and resilient to failure. But these NoSQL databases are limited to very simple query operations and a model that can’t guarantee consistency in all but the simplest of cases to make that possible. So to work around that lack of functionality, those “micro”-services tend to grow in scope and complexity because they have to implement data management logic in the service layer. If you’re rewriting your app from the ground up, and you have an army of developers that are familiar with complex data management principles, that might be okay, but could also prove costly. And worse yet, data has gravity - it’s hard to move, so you need to have a solution that allows you to transition to and from an on-premise environment, as well as between clouds. If you wait until you have to move to think about this, it may be too heavy to lift. Implementing RDBMS in the cloud: 4 Challenges So, what exactly do you need to bring the next evolution of the transactional database into the cloud? Number one is full SQL support - it’s the lingua franca of databases everywhere, and the way we’ve all been trained to ask questions of our data. From there, we need to guarantee that data will always be a reliable and correct state, no matter the circumstance - that means full ACID compliance. Beyond that, we need to be able to scale the database without adding operational complexity. We also need to keep in mind where that data should live - for example, European data needs to stay in the EU, or information about someone who lives in NYC should primarily be located on the east coast for easy access. And since we can have data spread across multiple regions (and even all over the globe) geo-locating data to where it needs to be accessed or written is also critical for optimal performance. All while staying prepared for nodes, availability zones, or even entire regions from a cloud provider going down at times for reasons outside of our control. 1. Full SQL Support Full SQL support is the first thing that we lost as we shifted data management to the cloud. To make it easier for people who were developing these first cloud-native data management systems, we reverted to earlier design patterns that are easier to scale - namely, what we now call key-value stores. This pattern uses a “key” to file a piece of data away and then retrieve it using that same key in some predictable fashion. This makes it easy to scale because based on the key I can predict which machine I squirreled the data away on, but it makes it difficult to answer higher-order questions with that data. That logic has to get applied at the application level. Which means every time I need to answer a new question, I have to write the code that can do it - something I haven’t had to do since the late 1970s. Databases like Apache Cassandra(™) try to bridge the gap by bringing some SQL-like syntax to the mix, but you run into a problem. It “feels” like SQL, but it doesn’t act like SQL. And SQL is what you need to take an application that includes business logic, but no data management logic, and migrate it to the cloud. Without that SQL layer, you’re engaging in a costly rewrite of your application that in many cases isn’t worth the investment. 2. Consistency and Isolated Transactions Over the past 40 years, we’ve loosened the ACID standard to get better performance on single-node relational databases. Largely, we’ve eased up on the consistency (different copies of the same data are always the same) and isolation (multiple queries aren’t allowed to mutate or leverage the same data at the same time) requirements to allow databases to process more queries at once. We could get away with this in most cases because individual queries run extremely quickly, and local replication of data is blisteringly fast on a single node (or in some cases in a single datacenter). But as we try to move those same systems to the cloud and distribute them across environments, the time windows where a system is vulnerable to those worst case scenarios increases from microseconds to seconds (or even in some cases, minutes), all because you might have multiple servers trying to field the same queries. NoSQL databases handle this by simply not guaranteeing that you’ll get consistent results - leaving your application to handle cases of dirty reads and conflicting writes. Once again, this is logic you now have to write in your application, complicating your migration. A true RDBMS handles these cases for you with full, ACID-compliant transactions. This allows you to have the confidence in your data that you need to run your business - financial records and all. 3. Elastic Scale & Strong Resilience A true, cloud-native solution should take advantage of the inherent advantages of moving to the cloud. Namely, the ability to scale; not just in failure scenarios but under different load scenarios as well. If you’ve ever tried to scale a traditional RDBMS, you know it’s a complicated endeavor. Your best case scenarios allow you to spend an enormous amount of time and effort to manually shard your data, or some form of a read-ahead cache that won’t be guaranteed to be consistent with some master database. And while the best NoSQL solutions like MongoDB(™) and Apache Cassandra(™) can definitely be considered scale-out systems, both are difficult to scale back if you decide you no longer need some of that capacity. On top of that, when your data isn’t guaranteed to be consistent like it is with NoSQL solutions, you don’t know if the node being removed from your cluster is the only one with the latest version of a record. That not only impacts scale, but it also makes your writes less durable in cases of failure. Losing a single database node should never cause data to be lost. A well designed cloud-native environment should never lose data under any failure scenario. But it’s easy to imagine a case where that could happen with a NoSQL database because we simply don’t know the state of the local data. 4. Geo-Partitioning and tying data to a location Moving your data to the cloud doesn’t mean it can live just anywhere. Not only do regulations like GDPR come into effect when talking about customer data, but from a purely practical standpoint, it’s important to keep it close to where it’s being accessed. That means being able to logically define rules for where a record should and shouldn’t live. And not just at an object level, the database should be able to tie data to a location while still being able to treat the entire corpus as a single table to keep that complexity from our applications. CockroachDB addresses these challenges to deliver a cloud-native distributed SQL database. Luckily, there’ve been some technological advancements over the past decade that finally allow you to bring that rich set of RDBMS functionality to this cloud-native environment. CockroachDB is the culmination of that research - a transactional, scalable, cloud-native relational database with full SQL support and a consistency model that guarantees you will always get the correct answer. CockroachDB is the last piece that was necessary to help the next wave of applications to transition to the cloud. If you’re interested in learning more about our distributed SQL , cloud-native database you can consult our docs or reach out directly to set up a call.", "date": "2019-05-16"},
{"website": "CockroachLabs", "title": "CockroachDB Change Data Capture: Transactionally and Horizontally Scalable", "author": ["Daniel Harrison"], "link": "https://www.cockroachlabs.com/blog/change-data-capture/", "abstract": "CockroachDB is an excellent system of record, but no technology exists in a vacuum. Some of our users would like to keep their data mirrored in full-text indexes to power natural language search. Others want to use analytics engines and big data pipelines to run huge queries without impacting production traffic. Still others want to send mobile push notifications in response to data changes without doing the bookkeeping themselves. CockroachDB aims to Make Data Easy and sometimes this means playing well with others. The industry standard solution for this is Change Data Capture (commonly abbreviated as CDC). Each database's take is a little different, but it generally looks like a stream of messages, each one containing information about a data change. We call ours a changefeed. A CockroachDB `CHANGEFEED` is a realtime stream of the changes happening in a table or tables. As SQL statements execute and alter the stored data, messages are emitted to an external system which we call a \"sink\". Executing INSERT INTO users (1, \"Carl\"), (2, \"Petee\") might send {\"id\": 1, \"name\": \"Carl\"}` and `{\"id\": 2, \"name\": \"Petee\"} . We could support emitting directly to everything we want to work with, and that may eventually happen, but this involves a client driver and performance tuning for each one. Instead, we emit to a \"message broker\", which is designed to be an intermediary for exactly this sort of thing. User feedback led us to select Kafka as the first to support. An Early Challenge The biggest challenge in building changefeeds for CockroachDB was clear right from the start. We wanted our changefeeds to scale out horizontally, but we also wanted them to keep our strong transaction semantics. In a single node database, this is conceptually easy. Every database I know of uses a Write Ahead Log (WAL) to handle durability (the D in ACID) in the face of disk failure or power loss. The WAL itself is simply an ordered log on disk of every change, so the work of building a changefeed mostly becomes exposing this log in a sensible way. In fact, Postgres has a plugin system for tailing the WAL and the various changefeed implementations for Postgres are implemented as these plugins. Other databases work similarly. CockroachDB, however, has a unique distributed architecture. The data it stores is broken up into \"ranges\" of about 64MB. These ranges are each duplicated into N \"replicas\" for survivability. A CockroachDB transaction can involve any or all of these ranges, which means it can span any or all of the nodes in the cluster. This is in contrast to the \"sharded\" setup used when horizontally scaling other SQL databases, where each shard is a totally independent replicated unit and transactions cannot cross shards. A changefeed over a sharded SQL cluster is then simply a changefeed per shard, typically run by the leader of the shard. Since each transaction happens entirely in a single shard, there's little benefit to the changefeed worrying about the relative ordering of transactions between shards. It also means the individual shard feeds are perfectly parallelize-able (one Kafka topic or partition per shard is typical). Figure 1: Transactions in a sharded SQL database can't cross shards. This means it's easy to form independent streams of ordered transactions; each shard leader's WAL is already exactly this. Since a CockroachDB transaction can use any set of ranges in the cluster (think cross-shard transactions), the transaction ordering is much more complicated. In particular, it's not always possible to partition the transactions into independent streams. The easy answer here is to put every transaction into a single stream, but we weren't happy with that. CockroachDB is designed to scale horizontally to large numbers of nodes, so of course we want our changefeeds to scale horizontally as well. Figure 2: Transactions in CockroachDB can cross nodes. The `(A->6,C->7)` and `(B->8,D->9)` transactions are not possible in a sharded SQL database. This means the only way to have a fully ordered stream of transactions is to have a single stream, which would limit horizontal scalability. Note that the `(A->3,B->4)` and `(C->5)` transactions are independent, but both overlap the other transactions, so even these can't be partitioned. A Lightbulb Moment A SQL table in CockroachDB can span many ranges, but each row in that table is always contained by a single range. (The row can move when a range gets big and the system splits it into two as well as when the range gets small and the system merges it into a neighboring range, but these can be handled separately.) Further, each range is a single raft consensus group and thus has its own WAL that we can tail. This means that we can produce an ordered stream of changes to each SQL row. To power this, we developed an internal mechanism to push these changes out directly from our raft consensus, instead of polling for them. It's called RangeFeed , but it's a big enough topic for a blog post of its own, so I won't go into detail. Each of the row streams are independent, which means we can horizontally scale them. Using our distributed SQL framework , we colocate the processor that emits row changes next to the data being watched, which eliminates unnecessary network hops. It also avoids the single point of failure we'd have if one node did all the watching and emitting. Figure 3: CockroachDB range leaders each emit changes directly to Kafka (or other sink). For many changefeed uses, this is enough; a mobile push notification can be triggered by each message and some datastores don't support transactions. Ordered row streams work great for both of these. For other uses, it's not enough; a mirror of data into an analytics database certainly doesn't want to apply partial transactions. Every CockroachDB transaction already commits each row change with the same HLC timestamp . Exposing this timestamp in each message for a changed row is enough to get transaction information back (group rows by timestamp) [1] as well as a total ordering (sort rows by timestamp). Building on top of our existing transaction timestamps means that our changefeeds have the same serializability guarantees as everything else in CockroachDB. The final piece is knowing when to do this group or sort. If a changed row is emitted with time hlc1 from one CockroachDB node, how long do you have to wait to make sure none of the other nodes have changes at hlc1 before acting on it? We solve this with a concept we call a \"resolved\" timestamp message. This is a promise that no new row changes will be emitted with a timestamp less or equal to the one in the resolved timestamp message. This means the above user can act on hlc1 after receiving from each node [2] a resolved timestamp >= hlc1 . Figure 4: One possible ordering of the first few messages emitted for the transactions in Figure 3. In figure 4, imagine that two independent streams have each been read through the X . hlc1 has been resolved on one stream, but not the other, so nothing is resolved yet. Now imagine that at some later point, messages have been read through Y . Both streams have resolved hlc1 , so we know that we have received all changes that have happened up to and including hlc1 . If we group the messages by timestamp, we can get the transactions back. In this case, only (B->1,C->2) , which committed at hlc1 . This transaction could now be sent to an analytics database. Note that the (A->3) change happened at hlc2 and so is not yet resolved. This means the changefeed user would need to continue buffering it. We can also reconstruct the state of the database at any time up to and including hlc1 by keeping the latest value seen for each row. This even works across ranges and nodes. In this case, at time hlc1 the database was B=1,C=2 . Finally, imagine some later time when all messages up through Z have been read. Going through the same two processes again gets us the transactions and state of the database. In this case, transaction (A->3,B->4) committed at hlc2 and (C->5) committed at hlc3 . At hlc3 the database contained A=3,B=1,C=5 . Note that we can also reconstruct the database at hlc2 if necessary. Whenever I explain this all now, it seems so obvious, but it's one of those ideas that is only obvious in retrospect. (At least to us.) In fact, the aha! moment came from a discussion with a fellow engineer about a really interesting paper on distributed incremental computation which included the idea of adding data to a system with a certain timestamp and periodically \"closing\" that timestamp (a promise that you won't later introduce new data with a timestamp <= the closed timestamp). This allows the incremental computation to finish out everything up to that timestamp and there was no reason we couldn't use the same idea in CockroachDB. As an aside, one of our engineers was excited enough about this paper that he recently left to co-found materialize.io and build a company around it with one of the paper's authors. Sinks and More Sinks So far we've talked about the Kafka message broker, but this is not the only sink we support. Many popular analytics databases, including Google BigQuery, Snowflake, and Amazon Redshift support loading data from cloud storage. If this the only thing a user needs CDC for, there's no reason they should need to run Kafka in the middle, especially if they weren't already using it elsewhere. There are many message broker options beyond Kafka. We'll add first-class support for them as demand dictates, but in the meantime, we also support HTTP as a sink. HTTP plus JSON (our default format) is the lingua franca of the internet, so this makes it easy to glue CockroachDB changefeeds to anything you can imagine. Some of the ideas we've had include message brokers we don't support yet as well as \"serverless\" computing, but we're even more excited about the ones our users will think up. Finally, for users of CockroachDB Core, we've provided a new ` CHANGEFEED FOR` statement that streams messages back over a SQL connection. This is similar in spirit to RethinkDB's changefeeds (which are much beloved by their community). We don't (yet) let you use them as a source of data in queries like RethinkDB does, but there's no reason we couldn't add this in the future. These three sinks are initially being exposed in our 19.1.0 release as experimental features so we can make sure we get the APIs just right before committing to them. New Trails CockroachDB's SQL language is one that has decades of history. This means that for most features, we already have strong precedent for what the user-facing external surface area should look like. But occasionally our unique distributed architecture means that we get to blaze a bit of new trail and our changefeeds are one example of that. We think our approach makes the easy things easy (ordered row updates for push notifications and non-transactional datastores) while making hard things possible (horizontal scalability with strong multi-node transaction and ordering guarantees). We hope you agree. We're actively looking for feedback, so try them out and let us know what you think! Footnotes 1: Well almost. Two transactions that don't overlap can commit with the same timestamp, but they have nanosecond precision, so this is rare in practice. We also haven't found anyone that needs more granularity than this, but if someone does we could expose our internal unique transaction IDs. 2: As always, the reality is slightly more complicated than this. A changefeed user doesn't typically receive data directly from CockroachDB, instead receiving it from a message broker such as Kafka. This means resolved timestamps are actually scoped to Kafka partitions, not CockroachDB nodes.", "date": "2019-06-06"},
{"website": "CockroachLabs", "title": "Query Plan Caching in CockroachDB", "author": ["Radu Berinde"], "link": "https://www.cockroachlabs.com/blog/query-plan-caching-in-cockroachdb/", "abstract": "Since the 2.1 release, CockroachDB has had a cost-based optimizer . Rewriting a big component of an existing system is always challenging. In particular, we really wanted to avoid regressing on any workloads, including simple transactional queries where the existing planner was already generating the best plan. A cost-based optimizer inherently does more work and thus involves longer planning times. To mitigate this, we worked on caching and reusing optimizer state across multiple instances of the same query. For now, we chose a conservative path: only cache state from which we can still generate the best plan (the one we would have generated without caching), making caching invisible to the user. We’ll start with an overview of the stages of the query planning process; then we’ll go over the methods clients can use to issue queries against CockroachDB, and finally we’ll discuss the caching work we have done to speed up query planning. Optimization stages The query planning process encompasses everything that happens from the point where CockroachDB receives a query, to the point where we have specific instructions for the execution layers describing how the query is to be executed. Planning a query using the optimizer is comprised of 5 stages: 1. Parse: Parsing is the very first thing thing that happens to a SQL query: the SQL string is syntactically validated and converted to an abstract syntax tree (AST). There is no semantic analysis of the query at this stage. 2. OptBuild: The next step to planning a query is to analyze the AST semantically and convert it to a relational expression tree. This is the part of the code that “understands” SQL, and includes resolving database object names as well as type checking and type inference. 3. Normalize: Optimizers have a large collection of “rewrite\" transformations (we call them normalization rules). These rules convert a relational expression to another equivalent expression that is likely to result in a better plan. These transformations are assumed to be “always good” and do not look at the estimated cost of plans. Major classes of normalization rules are: constant folding, pushing down filters, decorrelation, and elision or simplification of unnecessary operators. In our implementation, the Normalize and OptBuild steps are not separate processes: they are intertwined, with normalization rules running as soon as possible on subtrees generated by the OptBuild process. This is done for efficiency: it avoids creating temporary expressions that are then discarded; for example, if we have a filter that we determine is always true, the filtering operator is eliminated before creating an expression for it. Despite this detail, the Normalize and OptBuild steps can still be considered conceptually separate - and we can even measure their running time separately, by recording the build time with and without normalization rules enabled. 4. Explore: Exploration rules are transformations that generate new relational expressions that are equivalent to a starting expression and which might or might not result in faster execution. The exploration step runs all exploration rules, generating many possible expressions. For each expression we estimate a cost, and at the end of the process we pick the cheapest expression. To efficiently store and process these expressions, we use a Memo data which organizes expressions into equivalency groups (more details about the Memo can be found in the previous blog post ). 5. ExecBuild: The final step is to convert the final optimized expression to a data structure that is understood by the execution engine. To get an idea of how long these stages take, we look at a sample set of queries : kv-read : a single-row scan from a simple key-value table; tpcc-new-order : a single-row scan on a more complicated table (with an index); tpcc-delivery : an ordered scan with limit; tpcc-stock-level : a join followed by an aggregation. The chart below shows the planning time for these queries, broken down into stages. We can see that the first stages consume a significant part of the planning time, so caching the results of these stages would be a worthwhile optimization. Query issue methods To understand when and how these stages run, we have to be cognizant of how queries are issued by clients. This involves some details of the PostgreSQL’s network protocol called pgwire , which is used between the database client and CRDB. pgwire defines two ways of issuing queries: 1. Simple protocol: can only be used when the query has no arguments; everything must be embedded in the SQL string, which is sent to the server in a single request. For example (using Go’s database/sql library): db.QueryRow(“SELECT v FROM kv WHERE k = 10”) 2. Extended protocol: must be used when the query has arguments. In this case the SQL string has “placeholders” ($1, $2, etc.) which refer to arguments that are serialized and sent separately. For example: db.QueryRow(“SELECT v FROM kv WHERE k = $1”, 10) Conceptually there are two main steps: Prepare: a request is sent with just the SQL string. The server parses the query and makes sure it is valid and semantically sound. Execute: a Bind request is sent with the values for the query arguments (in this example, the integer 10), followed by an Execute request. The extended protocol is the recommended way to pass arguments to the server, preferable to embedding the values in the SQL string because it eliminates the need to correctly escape values (and thus prevents SQL injection attacks). Unfortunately, it is also inefficient because it requires the server to process multiple requests (and with many client drivers it involves two roundtrips). But it is possible to explicitly prepare a statement once, and then keep reusing the prepared statement, only repeating the Execute step: stmt, err := db.Prepare(“SELECT v FROM kv WHERE k = $1”) stmt.Query(10) stmt.Query(11) … Reusing a prepared statement allows the server to do some of the work once and then reuse it for every execution. There are thus three possible methods to issue queries, which we will refer back to: simple : simple protocol (no arguments) prepare-each : extended protocol, each statement is prepared and executed. This is the least efficient method. prepare-once : extended protocol, reusing explicit prepare. This is the most efficient method. Each of these methods requires different techniques for caching planning state. For our 2.1 release, we focused on optimizing the most efficient method prepare-once . In the subsequent release, we extended caching to cover the other methods as well. Query Caching in CockroachDB 2.1 Going back to the optimization stages, what stages do we need to run and when? For the simple protocol, we run through all the stages. What about preparing a statement? To check that the statement is semantically sound, we need to run the Parse and OptBuild/Normalize stages. To execute the statement later, we would of course like to pick up where the prepare step left off and continue with the rest of the stages. Unfortunately, we can’t do that in general. If the statement was prepared some time ago, the meaning of the query could be different now. A few examples of problematic things that could happen in-between the prepare and the execution step: A table used by the query is dropped and another table with the same name is created instead. This table resolves to a different internal table ID and could have different column names or types. A schema change removes a column used by the query. The current database could have been changed, affecting resolution of unqualified table names. In all these cases, the result of the OptBuild stage changes. There is another wrinkle here: in the preparation stage, the query contains placeholders (like $1, $2 ) which don’t yet have values. When we pick things off in the execution step, we must replace these placeholders with their values. This might trigger additional normalization rules. For example, a condition like $1 = ‘foo’ can be normalized to true or false when we replace $1 with a definite value. We refer to this step as AssignPlaceholders and it involves processing the optimizer’s relational tree to replace placeholders with their values and running any new normalization rules that can be applied. Placeholders without values also greatly affect the results of the Explore stage, in large part because we need definite values to constrain scans to specific parts of a table or index. There is thus little value in running this stage during the prepare step. The work we did on planning state caching in the 2.1 release revolved around addressing these difficulties and usually reusing the state generated during prepare and continuing with the AssignPlaceholders and Explore step. This involved detecting all the conditions where reusing the state could lead to different results, and building infrastructure to replace the placeholders with their values. This was important for getting good performance with the prepare-once method (which should be the method of choice for performance-sensitive workloads). This caching also speeds up the execution step of prepare-each , but this method is still relatively inefficient because of the prepare step (which runs for every query instance). Query Caching in CockroachDB 19.1 For our 19.1 release , we wanted to build upon the previous caching work to speed up the other two issue methods. We implemented a server-wide query cache, which remembers recent SQL queries. For queries with no arguments, the cache holds the final expression (the result of the Explore step). Thus, repeating a query frequently using the simple method is much cheaper. Note however, that with this method, the query string contains all the values of the query. If we run a similar query but with different values, it would be considered a different query. For queries with arguments, the cache holds the expression generated by the Normalize step. This state can be reused if the same query is prepared frequently, as is the case with the prepare-each method. Unfortunately, for historical reasons the current code modifies the abstract syntax tree (generated in the Parse step) in place during OptBuild . This means that we cannot reuse the AST across queries; so even if a query is in the cache, we still need to parse it. This is something we are planning to fix in the next major release. Results Recall the benchmark that breaks down query planning time by phases: If we use the prepare-once method and execute a previously prepared statement, we store the result of the Parse/OptBuild/Normalize stages with the prepared statement and on each execution we can proceed with the AssignPlaceholders/Explore/ExecBuild stages: This is a significant (more than 50%) reduction in planning time. Of course, in most cases the actual execution will involve more work than planning, so the improvement in the overall performance will be more subtle. To illustrate the end-to-end throughput (which includes execution time), we look at a benchmark on a KV workload (simple key-value reads and writes): Note that the keys in this benchmark are uniformly random, which explains why we don’t get any benefit with the simple query mode: each query instance will have a different SQL query string (because it contains a random key). Skewed versions of the workload in which some keys are accessed much more frequently than others do show some benefit. Future Work The caching we implemented so far is intended to be invisible to the user. It is merely an internal mechanism to speed things up, and the resulting execution plans are identical to the plans we would get without caching. However, in many cases, we could get a lot more benefit if we relaxed this and allowed reusing a plan that was generated using different values. For example, in the key-value workload above, each instance of the query gets the same plan regardless of the values. This would allow us to elide most of the planning work. To implement this, we need to extend our internal infrastructure to allow expression and costing of a fully optimized plan which still has placeholders. In addition, if the optimizer is going to make a decision that can affect the query plan, we would have to give the user visibility into this process and provide relevant controls. This blends into related future work for plan management which aims to give users more control over the query plans. Another improvement direction would be to detect cases where a query is frequently run with the exact same argument values, and cache the final plan (again, allowing elision of most of the planning work). Finally, an idea for improving the simple method would be to automatically parameterize queries (effectively converting them internally to something like prepared queries with arguments). Conclusion We’ve touched upon a few areas of our query planning process, as part of an ongoing series of posts on the optimizer. Building a modern query optimizer is a huge challenge; Andy Pavlo once mentioned a saying: if you try to do query optimization and fail, you can always fall back to rocket science. It will take a number of releases to extend our capabilities, as well as refine and improve our approach. We are, however, making progress at a fast pace and we hope you will follow along with us as our optimizer matures.", "date": "2019-06-20"},
{"website": "CockroachLabs", "title": "SQLsmith: Randomized SQL Testing in CockroachDB", "author": ["Matt Jibson"], "link": "https://www.cockroachlabs.com/blog/sqlsmith-randomized-sql-testing/", "abstract": "Randomized testing is a way for programmers to automate the discovery of interesting test cases that would be difficult or overly time consuming to come up with by hand. CockroachDB uses randomized testing in many parts of its code. I previously wrote about generating random, valid SQL . Since then we’ve added an improved SQL generator to our suite called SQLsmith, inspired by a C compiler tester called Csmith . It improves on the previous tool by generating type and column-aware SQL that usually passes semantic checking and tests the execution logic of the database. It has found over 40 new bugs in just a few months that the previous tool was unable to produce. Here I’ll discuss the evolution of our randomized SQL testing, how the new SQLsmith tool works, and some thoughts on the future of targeted randomized testing. History of randomized testing at Cockroach Labs The first randomized SQL testing in CockroachDB (about four years ago) came from totally normal Go fuzzing . We generated some random bytes and sent them off to our parser. Most of the time they didn’t even make it past the scanner/lexer, because most byte sequences aren’t valid SQL tokens. This approach found about 6 bugs. We don’t run this fuzzer much anymore, certainly not in our CI. In order to make it past the lexer and actually into the parser itself, we followed up the next year with a targeted fuzzer that was SQL-aware . This works by parsing our YACC file (a file that describes how to build a parser) then randomly crawling it and generating statements from valid branches. This allowed us to produce SQL tokens in some order that the parser would accept. They often wouldn’t actually execute since the types or column or table names weren’t correct, but at least they’d parse. It could, for example, produce queries like SELECT foo FROM bar , where maybe bar wasn’t a table or if it was, foo wasn’t a column in it. This approach has found over 70 panics, race conditions, and other bugs in the three years it’s been on. It's been running nightly for a while in our CI. Consider the path now. We started with random bytes that didn’t make it past the lexer. Then we had SQL tokens in the correct order which passed the lexer and parser, but not the semantic (type) checker. In order to cross that hurdle, we would need something that was aware of types, tables, columns, and their availability in different scopes. This is somewhat revisionist history, since at the time we didn’t realize this. But looking backward the progression is clear. Compilers have similar architectures. In the below diagram, this would be like being able to make it past all of the analyzer stages and into code generation. Phases of a compiler (source) SQLsmith: a domain-aware fuzz tester A while ago we found a program called SQLsmith , a domain-aware fuzz tester for SQL. It is written in C++ and can target Postgres while generating random queries. It begins by introspecting the database for available operators, functions, types, tables, and columns. When it generates queries it keeps track of available columns and tables and uses them. This program found over 70 bugs in Postgres. This original SQLsmith uses a few Postgres features that CockroachDB doesn’t yet support, so it didn’t quite work out-of-the-box for us. (CockroachDB is compatible with Postgres , but sometimes we don’t fully support everything or have slightly different semantics.) We made some changes that allowed it to work with CockroachDB and it found a bunch of bugs. But since it was written in C++ and CockroachDB is written in Go, it was not immediately obvious how to integrate it into our normal testing frameworks. Another colleague later spent their Flex Friday successfully porting SQLsmith from C to Go. From there it was refactored to be more integrated with CockroachDB (allowing us to test non-Postgres and other features easily). Having a Go version has lots of benefits for us: everyone here knows Go, we could hook into our existing nightly fuzz testing, and it allowed us to use our existing SQL type system and use things like randDatum which produces random constants. From there our version of SQLsmith has found over 40 panics, data races, and other bugs in CockroachDB. This kind of fuzz testing is not able to deduce correctness. Today, its goal is limited to finding panics, which it’s quite good at. Correctness is difficult because we don’t have any oracle of truth, which would require a known working SQL engine, which is exactly the thing we’re trying to break. We are unable to use Postgres as an oracle because CockroachDB has slightly different semantics and SQL support, and generating queries that execute identically on both is tricky and doesn’t allow us to use the full CockroachDB grammar. What we can do, though, is to try to make it expert at testing various combinations of features together for which we may not have already written tests. SQL is a large language. CRUD apps exercise a tiny portion of what SQL can do. I’ve been working here for over three years and still learn new facets of SQL. These features sometimes interact in unexpected ways (well, they can cause panics). For example, the C++ SQLSmith found a bug that manifested during a set operation like UNION or INTERSECT where there was a SELECT that selected two identical constants ( SELECT 1, 1 ). It’s impossible to hand write tests for all of the possible combinations. Hence, a domain-aware fuzzer is a useful tool to explore this space in an automated way. How SQLsmith works SQLsmith produces single SQL statements in a loop. The statement type (SELECT, UPDATE, INSERT, etc.) is randomly selected and its parts are randomly filled in (the FROM, LIMIT, WHERE, ORDER BY, etc.). This part is pretty simple and straightforward. Things get interesting when an expression is needed. An expression is something like a column reference, table, function, result of some unary or binary operator. Take the binary operator + that performs addition when its arguments are numbers. Each of those arguments, then, also needs to be an expression, which can continue to pick from the same list of options above, and recurse until some point. Expression generation is where SQLsmith starts to generate interesting queries that break things. Let’s take arguments to the FROM clause of a SELECT . A FROM expects a list of table expressions. Most usually, these are just named tables ( SELECT * FROM t ). But lots of other things are valid table expressions also: a VALUES clause ( SELECT * FROM (VALUES (1), (2)); ), a subquery ( SELECT * FROM (SELECT * FROM t WHERE t.a = 1)), a join ( SELECT * FROM a INNER JOIN b ON some_bool_expr ). In CockroachDB you can use [ … ] to turn something into a table source ( SELECT * FROM [DELETE FROM t RETURNING t.a] ). Each of these choices allows SQLsmith to fetch from something on disk or hard code data into a query. The final example there actually performed a mutation as part of a SELECT . There are many examples of this kind of random recursion in SQLsmith. One immediate problem that occurs in this class of testing is query runtime. SQLsmith can recursively generate queries that each have many joins, leading to a possibly huge number of result rows and a long execution time. While these kinds of queries can produce bugs, in general we want queries to be fast so we can test a higher number of them. To avoid this problem we needed a way to tell expression generators to stop recursion and instead produce some simple output. This was done by adding a complexity budget (an integer) and a function to ask if recursion can be used. The budget is decreased each request and starts returning false after it is exhausted. This method can probably be improved but it is configurable and allows us to tune how large the queries are. Take my favorite example : ALTER TABLE defaultdb.public.t ADD COLUMN col_40 STRING AS ( ( SELECT tab_44.col_18 AS col_41 FROM defaultdb.public.t AS tab_44 CROSS JOIN defaultdb.public.t AS tab_45, [ DELETE FROM defaultdb.public.t AS tab_46 RETURNING col_18 AS ret_1 ] LIMIT 1:::INT8 ) ) STORED NOT NULL; The statement here is adding a computed column to an existing table. The value of the computed column is a subquery that does a three-way join between one on-disk table joined with itself and the result of a DELETE. The bug here is that we didn’t run the same validity checks during ALTER as we do during CREATE, so this unsupported query was allowed and then panic’d when used. SQLsmith sniffed this out because the argument to the computed column is just an expression that returns anything of type STRING. SQLsmith made a random choice to use a rather funny subquery here with a DELETE inside of it and bam, a panic. These kinds of interactions are exactly what tools like SQLsmith are good at finding. Analysis So far SQLsmith has found over 40 panics, data races, and other bugs in CockroachDB, in the three months since it’s been running: 12 missing edge case checks (a specific kind of logic error where a certain case wasn’t handled) 9 other kinds of logic errors 9 type propagation errors (like an untyped NULL) 2 data races 5 others (like incorrect text marshaling) We have had some internal discussions trying to figure out if SQLsmith is just finding a bunch of long tail bugs that have little business value (i.e., the cost of fixing is higher than the loss of income from leaving the bug there) that would essentially be a time loss for us. Looking at the kinds of bugs it is producing, however, I do not think that these bugs are just long tail rarities that never occur in customer workloads. The original Csmith paper, whose ideas influenced the creation of the original SQLsmith, says this about the C compiler bugs they found: \"We claim that Csmith is an effective bug-finding tool in part because it generates tests that explore atypical combinations of C language features. Atypical code is not unimportant code, however; it is simply underrepresented in fixed compiler test suites. Developers who stray outside the well-tested paths that represent a compiler’s “comfort zone”—for example by writing kernel code or embedded systems code, using esoteric compiler options, or automatically generating code—can encounter bugs quite frequently. This is a significant problem for complex systems.\" I believe the same is true about the usefulness of bugs that SQLsmith finds. It is nearly impossible to write tests that can come up with all possible combinations in a language as rich as SQL without a program to help automate exploring the search space. Future Work The most immediate future work is to run our tool against Postgres and attempt to compare the results to CockroachDB. This is difficult because, while CockroachDB and Postgres support the same wire protocol, our SQL support is only mostly the same. All the little stuff that’s not exactly the same needs to be removed from the queries. As some small examples: the type INT is 4 bytes in Postgres but 8 in CockroachDB, Postgres doesn’t have some CockroachDB-specific SQL functions, etc. Beyond language differences, one of the biggest difficulties is that output is rarely deterministic. Unless a SELECT specifies an ORDER BY , the rows can be returned in any order (and even then, rows with matching values for the ORDER BY can be returned in any order). This means a SELECT even run on the same system multiple times can return rows in a different order. This matters when there are nested SELECT statements where some of them have LIMIT clauses. If rows can legally be returned in any order then the LIMIT clause will take some random subset of them, and pass those on up to some parent SELECT , thus significantly changing the output. Producing actually deterministic output in SQL is hard, making comparison testing more tricky than expected. But these challenges can be overcome, and different results may suggest a bug present in one of the systems. Beyond that, we will continue adding features by teaching SQLsmith about new parts of SQL (views, sequences, CockroachDB-specific features like interleaved tables, etc.). I’m also interested in applying these same ideas and principles to other parts of CockroachDB. Our Core layer is what the SQL layer uses when fetching and writing data to disk. This Core layer handles these requests while partitioning data, moving data around, handling leader elections, and other events. A domain-aware fuzzer could be taught about all of these background knobs, turn some, and then verify that the requests still function. Conclusion SQLsmith has added a new level of confidence when we ship new features or do refactors. It nearly always finds bugs when it is taught about more syntax or when we do large changes. We have added assertions that are often tripped by SQLsmith, helping us to find and fix bugs early on in the release cycle. This kind of domain-aware fuzzing can be many times more useful than byte-level fuzzing. I have found that putting an hour into teaching SQLsmith about new things results in more bugs than I could find manually trying to find bugs in something. In terms of human-hours of input to number of bugs found, targeted fuzzers have been much higher than hand-written tests, and I will continue to invest time in it until it stops producing bugs. Is building a randomized SQL fuzzer your cup of tea? Join the Cockroach Labs team ! (Also published on Matt Jibson’s blog .)", "date": "2019-06-27"},
{"website": "CockroachLabs", "title": "Why are my Go executable files so large?", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/go-file-size/", "abstract": "This blog post was originally published on the author's personal blog . Overview I built some tooling to extract details about the contents of a Go executable file, and a small D3 application to visualize this information interactively as zoomable tree maps . Here’s a static screenshot of how the app illustrates the size of the compiled code, in this example for a group modules in CockroachDB: The reason why I built this was to help me discover and learn what makes Go executable programs larger than I would expect. Thanks to this tool, I made several discoveries about how Go builds executable files. AnD YoU wOnT BeLiEVe WhAt I fOuND InSiDE! (Read more below.) The source code is public on GitHub: https://github.com/knz/go-binsize-viz Background and motivation My co-workers and I are busy preparing the latest release of CockroachDB, version 19.1. CockroachDB is released as a single program executable file containing all functionality. Today, the latest build is 123MB large, 88MB stripped. This is a 94% (resp. 125%) increase since CockroachDB v1.0 was released, a bit more than two years ago. What happened? This is especially puzzling given that: there is about 70MB of source code currently in CockroachDB 19.1, and there was 50MB of source code in CockroachDB v1.0. The increase in source was just ~40%. How come did the binary size increase by a larger factor? typically, compiled code is smaller than the source code it is compiled from. There is 70MB of source, a lot of which is just comments. Yet the binary is 88MB. What makes the executable size larger than the source code? These questions alone made me curious. Meanwhile, I also personally care about program size for practical reasons: smaller binaries cause less cache thrashing . They are easier to distribute and deploy. They make container orchestration more nimble. For these additional reasons, I would prefer if CockroachDB release binaries could become smaller. Figuring out what they contain might suggest how to achieve that. Building the visualization Method My goal was to find some clarity into 123MB of inscrutable executable data. I started without knowing exactly how to achieve that. I knew about the standard Unix utility nm which can display the size of individual entries in an excecutable file, and I knew that Go provides its own half-baked re-implementation ( go tool nm ). However, even a small program in Go will contain dozens of entries, and there were tens of thousands in the particular files I was looking at. So I needed an overview. I also knew about tree maps , ever since the movie Jurassic Park featured the fsn 3D browser in 1993. This visualization represents sized hierarchical entries—for example files on disk, and in my case also like entries inside an executable binary—using visual elements whose size on the screen is proportional to their size in bytes on disk. I thus decided to connect the two: visualize Go binaries using tree maps. Furthermore, reminding myself that I have potentially tens of thousands of entries to visualize, I decided upfront that it would do me no good to attempt to represent them all simultaneously on screen. I thus went looking for zoomable tree maps. Finally, I already had learned some D3 basics and wanted to learn more, so I decided I would use D3 for this exercise too. So I went to search for “zoomable d3 tree map” on my favorite search engine and discovered that D3 has native supports for tree maps, provided some input data in a suitable format. I initially started to tinker with Mike Bostok’s zoomable treemaps but then quickly ran into some issue where some D3 feature I wanted to use was not available: Mike’s code uses D3 V3, “modern” D3 runs at V5, and there were major API changes between V3 and V4. Converting the V3 example to V4 (or even V5) seemed non-trivial. Instead I set out to find some examples built for V4+. I then discovered this example from Jahnichen Jacques , itself inspired from Mike Bostok’s, and finally this simpler example from Guglielmo Celata inspired from both but with a simpler implementation. All these examples worked using D3 hierarchical data sets loaded from CSV or JSON with a particular schema. The main thinking exercise was thus to massage the output of nm into a format suitable to load into D3. The rest of the work was simpler, to adapt D3 examples I found online into something that made sense for the type of data I was working with. Extracting executable entries A Go executable binaries contains, as per go tool nm -size , two types of entries: entries compiled from Go. These look like this: 10ddac0         17 t github.com/cockroachdb/cockroach/pkg/base.(*ClusterIDContainer).Unlock entries compiled from C/C++ via cgo , or linked into the Go program from external C/C++ libraries. These look like this (modulo filtering using c++filt ): 28404a0         44 T rocksdb::PosixDirectory::~PosixDirectory() The first column is the address, and is of no interest here. The second column is the size. The third column is the entry type, and of no interest here either. The last part is the symbol for the entry. To build a tree visualization we thus need to decompose each symbol into name components that group the symbols into a hierarchy. Decomposing Go symbols Intuitively, a Go symbol contains a hierarchical package path (e.g. github.com/lib/pq ) and some package-local name. The package-name name is either global in the package (e.g. main ), or a method name with some receiver type prefix (e.g. (*File).Write ). This model accurately describes a large majority of symbols, and can be readily decomposed using a simple regular expression. However, we quickly come across exotic names that do not fit this model: 5250978         13 d crypto/tls..gobytes.1\n3823b40         48 r go.itab.*compress/flate.byLiteral,sort.Interface\n aa3740        113 t go.(*struct { io.Reader; io.Closer }).Close\n e79cb0         10 t database/sql.glob..func1\n 8ce580        123 t encoding/json.floatEncoder.encode-fm\n 73aed0         82 t runtime.gcMarkDone.func1.1 I thus iterated to expand a simple regular expression to properly decompose the variety of names found in Go binaries. The result was a bit gnarly can be found here . For the examples above, my program produces the following: Size Path Name 13 ['crypto/', 'tls.', '.gobytes.'] 1 48 ['compress/', 'flate.'] go.itab.*byLiteral,sort.Interface 113 ['go.', '(*struct { io.Reader; io.Closer }).'] Close 10 ['database/', 'sql.', 'glob..'] func1 123 ['encoding/', 'json.', 'floatEncoder.'] encode-fm 82 ['runtime/', 'gcMarkDone.', 'func1.'] 1 Decomposing C/C++ symbols Intuitively, a C/C++ symbol contains a hierarchical namespace path (e.g. std:: or google::protobuf:: ) and then a variable or function name. Again, looking at real-world binaries, we easily find many items that don’t fit the simple model: 37ee1d0          8 r $f64.c05eb8bf2d05ba25\n26abe20        100 T void rocksdb::JSONWriter::AddValue<int>(int const&)\n28388f0         71 t rocksdb::(anonymous namespace)::PosixEnv::NowNanos()\n2821ae0        231 T rocksdb::NumberToString[abi:cxx11](unsigned long)\n 5b8c5c         34 t rocksdb::PosixRandomRWFile::Sync() [clone .cold.88]\n265a740        211 T google::protobuf::internal::LogMessage::operator<<(long) Using the same technique, I iterated from a simple regular expression to decompose the variety of symbols encountered. I even went one step further and chose to decompose identifiers at underscore boundaries. The result regular expressions are again rather complex and can be found here . For the examples above, my program produces the following: Size Path Name 8 ['$f64.'] c05eb8bf2d05ba25 100 ['rocksdb::', 'JSONWriter::'] void AddValue<int>(int const&) 71 ['rocksdb::', '(anonymous namespace)::', 'PosixEnv::'] NowNanos() 231 ['rocksdb::', 'PosixRandomRWFile::'] Sync() [clone .cold.88] 211 ['google::', 'protobuf::', 'internal::', 'LogMessage::'] operator<<(long) Some difficulty arises from the fact that C++ symbols can contain an arbitrarily amount of nested template parameters or parentheses in types, and regular expressions cannot match recursively. My current implementation is thus restricted to only supports 6 levels of nesting. This appears to be insufficient to capture all symbols in my program of interest (where some symbols contain 10+ levels of nesting!) but I chose to exclude a few symbols to keep my regular expression simple®. In my target analysis, the size of these excluded symbols is negligible anyway. Organizing the data as a tree After decomposing the path components of each symbol, my program creates nested Python dictionaries using a simple recursive function . However, the result of this strategy for the path a,b,c is something like this: {'children':{\n    'a':{'children':{\n        'b':{'children':{\n          'c': ...\n          }}\n     }}\n}} Whereas the D3 visualization code really wants this: {'children':[\n    {'name':'a', 'children':[\n        {'name':'b', 'children':[\n           {'name':'c', ... }\n          ]}\n     ]}\n]} For this, I built a separate simplification program that turns the former format into the latter. The reason why I separated the code into two programs is that the decomposition of symbols is rather expensive, and once I was satisfied with the decomposition I wanted the ability to iterate quickly on the tree transform without having to decompose over and over again. Additionally, the simplification program collapses (“flattens”) multiple hierarchy levels with a single child into just one level with a combined name. For example, the hierarchy a/ → b/ → c/ → x,y becomes a/b/c/ → x,y . Visualization using D3 The original D3 tree map example as initially designed by Mike Bostok, and modified by Jahnichen Jacques and Guglielmo Celata operates thus: it prepares a SVG canvas in a named HTML “chart” entity; it defines a display function which, given a computed D3 tree map layout, creates a 3-level (grandparent-parent-child) visualization in the SVG; the display function internally defines transition logic to zoom in and out when clicking on the canvas; it loads the data from JSON , attaches it to a D3 tree map layout , and renders it using the aforementioned facilities. On top of this logic by the previous authors, I added the following: displaying sizes using both absolute values and as percentages ; a stable color map ; the ability to switch betwen visualization of sizes and visualization of counts ; the ability to view multiple data sets inside the same web page . Example visualization for a simple program Example program We’ll use the following Go code: package main\n\nimport \"fmt\"\n\nvar x = struct { x [10000]int }{}\n\nfunc main() {\n    fmt.Println(\"hello world\", x)\n} I choose to use a large struct for variable x so that the main package becomes significant in size compared to the imported runtime objects. The program can be compiled as usual: $ go build hello.go Transformation process I then use the following commands: $ go tool nm -size hello            >hello.symtab\n$ python3 tab2pydic.py hello.symtab >hellodic.py\n$ python3 simplify.py hellodic.py   >hello.js Interactive visualization The following HTML code is sufficient: <html>\n  <head>\n   <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n   <link rel=\"stylesheet\" type=\"text/css\" href=\"treemap.css\">\n  </head>\n  <body>\n   <p class=\"chart\" id=\"chart\"></p>\n   <script src=\"js/d3.v4.min.js\" type=\"text/javascript\"></script>\n   <script src=\"js/d3-color.v1.min.js\"></script>\n   <script src=\"js/d3-interpolate.v1.min.js\"></script>\n   <script src=\"js/d3-scale-chromatic.v1.min.js\"></script>\n   <script src=\"app3.js\" type=\"text/javascript\"></script>\n   <script type=\"text/javascript\">\n     viewTree(\"chart\", \"example-data/hello.js\");\n   </script>\n  </body>\n</html> Which renders as follows (ensure Javascript is enabled): Although this simple executable file appears to only contain Go symbols, it actually does contain C/C++ symbols too. However, their size is negligible and they initially appear as a mere line on the right side of the tree map. By clicking on that line, you may be able to zoom into them and obtain this: Initial impressions The small program above contains 6 lines of source code and compiles to a 1.3MB binary. The breakdown of sizes is as follows: Package Size runtime 900K / 71% main 80K / 6.3% unicode 77K / 6.1% reflect 72K / 5.7% fmt 38K / 3.0% strconv 31K / 2.5% sync 10K / 0.8% internal 9K / 0.7% syscall 6K / 0.5% (others) (remainder) In addition to code compiled from source, 24K (1.9% of size) are compiler-generated type equality and hashing functions. These are accumulated in the box TYPEINFO in the tree map. The following becomes clear quickly: the Go standard library is not well modularized; importing just one function ( fmt.Println ) pulls in about 300KB of code. even accounting for fmt.Println and its dependencies and the 80K of predictable code from the main program, we are still left to wonder about 900K (71%) of code from the runtime package. Zooming into there, we see that 450K (35%) are taken by just one single entry with name runtime.pclntab . This is more than the code for the main program and all the supporting code for fmt.Println combined. We’ll come back to this below. What’s inside a CockroachDB executable binary? Visual exploration Applying the tool on a recent CockroachDB build, one can find the following: Analysis By exploring this visualization, we discover: 71M of total entries; 8.4M (12% of size) in C/C++ entries, including 3M (4.3%) from RocksDB; 62M (88%) in Go entries, including: 32M (45%) compiled directly from CockroachDB source code or its dependencies; 26M (36%) from the runtime package. … wait, what? We established above, in the simple example, that runtime was about 900K in size. Now it has become 26M—a 28X increase! What is going on? Zooming in, the mystery is revealed: all of the increase went into that single object runtime.pclntab . The other entries in package runtime do not appear to differ between programs. We will come back to this pclntab object later below. Comparison between CockroachDB versions The visualization above was for a pre-release build of CockroachDB v19.1. For reference, here is the data for CockroachDB v1.0: This has: 32M of total entries; 2.5M (7.8%) in C/C++ entries, including 1.5M (4.7%) from RocksDB; 30M (92%) in Go entries, including: 16M (50%) compiled from CockroachDB sources and dependencies; 7.9M (25%) from the runtime package, of which 7.3M (23%) comes directly from pclntab . Recalling the initial problem statement, CockroachDB increased about 40% in source code size between v1.0 and v19.1. Thanks to the visualization, we observe that the compiled code that directly originates from the CockroachDB sources increased from 16M to 32MB, which is about a 100% increase. Meanwhile, runtime.pclntab increased from 7.9M to 26M—a 230% increase! What’s this runtime.pclntab anyway? It is not too well documented however this comment from the Go source code suggests its purpose: // A LineTable is a data structure mapping program counters to line numbers. The purpose of this data structure is to enable the Go runtime system to produce descriptive stack traces upon a crash or upon internal requests via the runtime.GetStack API. So it seems useful. But why is it so large? The URL https://golang.org/s/go12symtab hidden in the aforelinked source file redirects to a document that explains what happened between Go 1.0 and 1.2. To paraphrase: prior to 1.2, the Go linker was emitting a compressed line table, and the program would decompress it upon initialization at run-time. in Go 1.2, a decision was made to pre-expand the line table in the executable file into its final format suitable for direct use at run-time, without an additional decompression step. In other words, the Go team decided to make executable files larger to save up on initialization time. Also, looking at the data structure, it appears that its overall size in compiled binaries is super-linear in the number of functions in the program, in addition to how large each function is. We will come back to this below. Of size/performance trade-offs and use cases The change in design between Go pre-1.2 and go 1.2 is a classic trade-off between performance (time) and memory (space). When is this trade-off warranted? if a program is executed often (e.g. microservice, system tools), then it is worth accelerating its start-up time at the cost of an increase in size.If, moreover, the program is small , with relatively few functions, (a reasonable assumption for programs executed often, like microservices or system tools), the increase in size incurred by runtime.pclntab will be negligible and thus have no significant impact on usability: file transfers, deployments, orchestration, etc.In that case, the Go 1.2 design is sound and warranted. if, in contrast, a program is executed rarely (e.g. a system service that runs continuously in the background, like, say … a database server), then the start-up time can be amortized throughout the lifetime of the process. The performance benefit of accelerating start-up is then not so clear.If, moreover, the program is large , with tens of thousands of of functions (a reasonable assumption for complex, feature-driven enterprise software like … database servers), the increase in size incurred by runtime.pclntab becomes inconveniently significant.This makes the Go 1.2 design … not so advantageous. In the case of CockroachDB, runtime.pclntab could soon exceed the entirety of code compiled from sources, even with a conservative assumption of linear growth in compiled code size: Year 2017 2019 2021 (projected) 2023 Total size > 32M > 71M > 157M > 350M CockroachDB code > 16M (50%) > 32M (45%) > 64M (41%) > 128M (37%) runtime.pclntab > 7.3M (23%) > 26M (36%) > 85M (54%) > 281M (80%) Other oddities the Go compiler and linker always produce and keep the following entries, even when they are only used in functions elided by the linker because they are not used: Go interface conversion tables ( go.itab. ) between every pair of interface ever mentioned in the source code.This accounts for about 1% of the CockroachDB 19.1 binary, and is predicted to increase with the introduction of more inter-component interfaces for testing in 19.2. Type equality and hashing functions ( type.. ).This accounts for about 1.2% of the CockroachDB 19.1 binary, and is predicted to increase with the increasing use of code generation for optimizations inside CockroachDB 19.2. as discussed in my previous article , Go uses memory instead of registers to pass arguments and return values across function calls. On x86/x86-64, memory-accessing instructions use longer machine encodings than register-only instructions.In my experience, this is specifically the inflection point for the ratio between source code size and compiled code size in a monomorphic C-like language: when targeting fixed-length instruction encodings and/or a memory-heavy calling convention, the size of the compiled code grows larger than the source code (excluding comments and whitespace). We can see this with both C on ARMv6 (no Thumb) or Go on x86(-64).When targeting variable-length instruction encodings and the calling convention suitably utilizes the more compact instructions, the size of the compiled code becomes smaller than the source code. We can see this with C on x86(-64) with a decent compiler, but, as examined here, not with Go. Summary and Conclusion To understand the internal structure of executable files compiled from Go, I built a D3 visualization using zoomable tree maps . This visualization is published on GitHub and has been tested to work with any executable produced by Go versions between 1.4 and 1.12. Using this tool, I have analyzed the space usage inside the monolithic CockroachDB binary, cockroach . I discovered that the majority of the growth of cockroach over time is concentrated in one object, runtime.pclntab . This object is automatically generated by the Go compiler to support the generation of textual stack traces at run-time, for example in error reports. A design choice made in Go 1.2 causes this object to grow super-linearly in the number of functions in a program, in addition to the sum of their sizes. Between CockroachDB 1.0 and 19.1, runtime.pclntab grew by 230% while the total amount of source code only increased by 40% and compiled code by 100%. This design choice was intended to lower the start-up time of programs, and contrasts with the previous design using compressed tables—which is, incidentally, the industry standard for other C-like languages, even in contemporary compilers. This performance goal is not relevant to server software with long-running processes, like CockroachDB, and its incurred space cost is particularly inconvenient for large, feature-rich programs. Copyright and licensing Copyright © 2019, Raphael ‘kena’ Poss. Permission is granted to distribute, reuse and modify this document according to the terms of the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/ .", "date": "2019-04-18"},
{"website": "CockroachLabs", "title": "Choosing Index Keys for CockroachDB", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/how-to-choose-db-index-keys/", "abstract": "3 Basic Rules for Choosing Indexes There are a few basic rules to keep in mind when choosing indexes for a database. A good index should have these three properties: Usefulness: Speed up the execution of some queries (or enforce a constraint) Clustering: Keep records that are likely to be accessed together near each other Scattering: Keep records that are unlikely to be accessed together far apart Usefulness The first rule is simply a reminder that indexes aren’t free, and if it’s not helping the application somehow you’re better off without it. Each additional index makes all writes to the table slower, but they can make some reads much faster. The best case for write performance is a table with a primary key but no secondary indexes. The first secondary index has a high cost because it means any insert to the table becomes a distributed transaction, but the cost of each additional secondary index is smaller. Note that it’s never helpful to remove the primary key--CockroachDB will create a hidden primary key for any table that doesn’t have one, and this hidden key cannot be used for by any queries, so it’s always better to have a real primary key that you choose. Do not let this scare you away from secondary indexes, however. They’re not free, but they have such a transformative impact on read performance that it’s nearly always worthwhile to ensure that every query you run has a suitable index. Clustering The second rule, clustering, is a little more subtle. When an application needs to load multiple records at once (which could happen, for example, due to a JOIN or the use of the IN operator), it’s best for performance if those records are near each other. Originally, this advice came about to minimize the number of seeks that must be performed on spinning HDDs. In a distributed database like CockroachDB, the same guideline serves to minimize the number of network operations to access data remotely. For instance, in a social network news feed, most page views only need data from the current day, so organizing the data by time may provide the best data clustering and cache efficiency (or maybe not, as we’ll see below). CockroachDB offers a few SQL extensions that can further improve data clustering, including interleaved tables and storing indexes . Scattering The third rule, scattering, is in some sense the converse of the second: when similar records are near each other, different records naturally have to go somewhere else. However, it’s not always the case that improving clustering also increases scattering. In the social network news feed example, organizing records by time maximizes clustering, but it also creates a hotspot because all the posts happening right now are trying to write to the same place. This is a severe limitation on the application’s ability to scale - if a hotspot like this exists, it’s not necessarily possible to serve more users by adding more nodes to a CockroachDB cluster. In practice, clustering and scattering are more often in tension with each other than they are mutually reinforcing. Options for Selecting Unique IDs If a table doesn’t have a natural primary key, you’ll probably want to synthesize some sort of unique identifier for each record. For this, you have a few options: Sequences The simplest (but often the least performant) way to generate unique IDs is to start with 1 and count up. This is what you get from the SEQUENCE feature in CockroachDB and PostgreSQL, or the AUTO_INCREMENT keyword in MySQL. This produces IDs that roughly correspond to insertion order, and many users like the fact that the IDs produced are small integers (on the other hand, sequential IDs can give away the details of how many users/photos/etc your application has). Unfortunately, sequential IDs are not ideal for distributed databases. The sequence becomes a bottleneck that all insertions must wait for, so throughput is limited by the nodes responsible for the sequence counter, and adding more nodes to the cluster won’t necessarily improve performance. Note: Sequences are non-transactional Why do IDs only roughly correspond to insertion order? (This is true in most databases, not just CockroachDB) Because when a transaction inserts a record, it gets an ID when it reaches the INSERT statement, but that record doesn’t become visible to other readers until the transaction COMMITs. That means that it’s possible to see records appear to be out of order, like this: Transaction A inserts a record which is assigned ID 1 Transaction B inserts a record which is assigned ID 2 Transaction B commits Transaction C reads from the table and sees record 2 Transaction A commits Transaction C reads from the table and sees records 1 and 2 If your application requires IDs to strictly correspond to insertion order, you can do something like INSERT INTO tbl (id, …) VALUES ((SELECT max(id)+1 FROM tbl), …) . However, this has a very high performance cost since it makes all insert transactions wait for their turn to insert the next ID, so only do this if your application really requires strict ID ordering. Using change data capture (CDC) can help avoid the requirement for strict ID ordering in many applications, letting you use higher-performance ID strategies. Timestamps Timestamps are roughly ordered and nearly unique (and collisions can be handled by adding random bits or using UNIQUE constraints in the database). They’re more scalable than sequences since they don’t require a single-key bottleneck to maintain the sequence counter, and therefore they’re a good fit for distributed databases. CockroachDB uses timestamps plus random bits as the default for the SERIAL column type and the unique_rowid() function. When insertion-order clustering is more important than scattering for your application, we recommend timestamp-based IDs. Randomness The third major option for ID generation is to use large random numbers, usually via the 128-bit UUID type. As you might expect, random IDs maximize scattering but don’t give any clustering. Random IDs usually give the best raw INSERT performance in CockroachDB because they allow all the nodes in the cluster to be fully utilized, although the lack of clustering can hurt the performance of some queries. Hybrid approaches Even though timestamps avoid the worst bottlenecks of sequential IDs, they still tend to create a bottleneck because all insertions are happening at around the current time, so only a small number of nodes are able to participate in handling these writes. If you need more write throughput than timestamp IDs offer but more clustering than random UUIDs, you can use sharded keys to spread the load out across the cluster and reduce hotspots. There are many variations on the idea of sharded keys, but for this article we’ll focus on a composite key that combines the real key with a shard ID derived from a hash. This is a static sharding scheme, so we’ll need to choose a number of shards in advance. This number should be somewhat higher than the number of nodes in your cluster (or the number of nodes you expect to grow into in the near future). For this example, we’ll use 16 shards, and represent it as a string containing a single hex character (leaving room for expansion in the future). Consider this table containing posts on a social media site: CREATE TABLE posts ( id SERIAL PRIMARY KEY, author_id  INT8, ts TIMESTAMP, content TEXT, INDEX (author_id, ts)); The SERIAL primary key is a write bottleneck: it forces all inserts to this table to go to a small number of ranges. We’d like to change this to allow more write throughput, but going all the way to UUIDs could hurt the performance of our most important read query (which looks something like SELECT * FROM posts WHERE author_id IN ? ORDER BY ts DESC LIMIT 50 ) by reducing clustering. Instead, we’ll shard the primary key 16 ways, which will let us use up to 16 times as many nodes for increased write throughput. The revised table definition looks like this: CREATE TABLE posts ( shard STRING AS (substr(sha256(id::string), 64)) STORED, id SERIAL, author_id INT8, ts TIMESTAMP, content TEXT, PRIMARY KEY (shard, id), INDEX (author_id, ts)); The awkward substr(sha256(id::string), 64)) expression gives us a shard key that A) has 16 distinct values (one hex digit), B) is uniformly distributed, and C) is easily and deterministically computed from the generated id (it’s a little more expensive than necessary, but it’s unlikely to be a big deal in practice and using well-known functions like sha256 makes it easy to reproduce this computation in other languages if needed). Note that only the primary key changed - the index by author and time stayed the same because one user can’t post so often that the per-author index becomes a bottleneck. In other applications, it might be necessary to apply sharding to secondary indexes as well. Our table is now more complicated: how do we use this new shard column? The STORED column feature means that this column’s value will be generated automatically when the row is inserted or updated, so we don’t need to change our INSERT statements. However, any query that specifies a value for the id column must now specify a shard as well. This includes updates, deletes, and queries by id (but not queries by other indexes like the (author_id, ts) index). The shard should be computed as a function of the id column. You can do this however you want, in SQL or in your application code, but the important thing is to be consistent. Queries that used to be SELECT * FROM posts WHERE id=? become SELECT * FROM posts WHERE id=? AND shard=substr(sha256(?::string), 64) (or you could save the shard value from a previous access to this row. The trickiest thing about sharding is when we want to do a query that touches all shards. For example, in the original schema, it was easy to generate a “firehose” of recent posts with SELECT * FROM posts ORDER BY id DESC LIMIT 10 . This is a very expensive query on our new schema because it requires a sort of the entire table! Instead, we must ask the database to search each shard separately: SELECT * FROM posts WHERE shard IN ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f') ORDER BY id DESC LIMIT 10 . This is admittedly tricky and intrudes into application code. We’re exploring ways to make this process more automatic. In our 19.2 release , we’ll have a feature that can infer IN clauses like the one above automatically from CHECK constraints on the schema, and we may introduce ways to abbreviate some of the tricks shown here. We’ve focused on timestamps here, but the principles of key clustering and scattering apply to other domains too. For example, a web crawler may be concerned about the distribution of URLs, and may want to use a similar sharding scheme to the one described here. Interleaving Interleaved tables (a CockroachDB extension to SQL) change the cost/benefit analysis of the different key options. A top-level table with interleaved children cannot perform scans that take advantage of ordered keys, while the children inherit any scattering from their parents’ keys. Therefore, when using interleaving, it’s usually best to use random (UUID) keys on the top-level table, and ordered keys (SERIAL or timestamp) on all child tables. If we reconsider our social network table above and interleave the posts table into a users table, the schema could look like this: CREATE TABLE users ( id UUID PRIMARY KEY, name STRING, ); CREATE TABLE posts ( author_id UUID, ts TIMESTAMP, content TEXT, PRIMARY KEY (author_id, ts), FOREIGN KEY (author_id) REFERENCES users (id) ON DELETE CASCADE, ) INTERLEAVE IN PARENT users (author_id); Compared to the sharded schema above, the interleaved version would be more efficient for reading a single user's posts, but less efficient for gathering posts from a large number of users. If optimizing indexes for distributed environments makes your brain tingle, come join the Cockroach Labs team!", "date": "2019-07-09"},
{"website": "CockroachLabs", "title": "Why We Run Managed CockroachDB on Kubernetes", "author": ["Josh Imhoff"], "link": "https://www.cockroachlabs.com/blog/managed-cockroachdb-on-kubernetes/", "abstract": "There’s this really fun game I like to play. You get a bunch of SREs in a room and you see how quickly you can rile them up. Here are some things to say next time you’re in a room of SREs: “SRE team’s only job is to keep the service in SLO.” “This SLO doesn’t mean anything anyway so I don’t care about it.” If you say one of these things, you’ll have all the other SREs in the room screaming and sweating and breaking their laptops over their knees in no time. For extra fun, pair with a friend and each take different sides! Another topic that works really well is Kubernetes : “You aren’t on Kubernetes? Wow, that’s a mistake.” “You are on Kubernetes? Wow, that’s a mistake.” See this thread on Hacker News for more fun. Seven months ago, Cockroach Labs began building a managed service offering called Managed CockroachDB . So for the last seven months, the SRE team has been evaluating different automation and orchestration technologies on top of which to build Managed CockroachDB. Recently, we took a hard look at Kubernetes. We didn’t break any laptops over our knees but it was quite the journey nonetheless. Each SRE started with one perspective, loaded up their cocoon with Kubernetes docs and tech talks and bug reports, spent a month in the darkness of the cocoon thinking deep thoughts, and finally emerged as a beautiful Kubernetes butterfly with that strange Kubernetes boat logo tattooed on their wings. In the end, we chose Kubernetes as our automation technology; stay tuned for the launch of self-service Managed CockroachDB (built on top of Kubernetes) later this year! This blog post describes the journey to Kubernetes in detail. Container orchestration at Google and at Cockroach Labs I started my career as an SRE at Google. At Google, we used Borg , a container orchestration system that is internal to Google and was the predecessor to Kubernetes. Users of Borg declare that they want to run such and such binary with such and such compute requirements (10 instances, 2 vCPU and 4 GB of RAM per instance), and Borg decides what physical machines the applications should run on and takes care of all low-level details such as creating containers, keeping the binary running, etc. I was used to this model when I joined Cockroach Labs. Since Kubernetes was widely used in industry, why wouldn't we use it at Cockroach Labs? An experienced teammate pointed out that even if Borg and Kubernetes are similar, the fact that Borg is a good fit for Google doesn’t imply that Kubernetes is a good fit for Cockroach Labs. The key point my teammate was making is that Google and Cockroach Labs are not the same! I began to think more about the differences between Google and Cockroach Labs. Here are some things that are true of Google: Google runs its own data centers. Google runs as wide a variety of services as any company on this Earth. There are big services like websearch and YouTube, which are made of many microservices and operated by hundreds of engineers, and there are small services like code source repositories, which are operated by teams of fifteen or so engineers. There are also user facing parts of services, such as the system that serves websearch results to users and batch parts of services, such as the system that crawls and indexes the web so as to keep results fresh and relevant. Google has many software engineers working for them. Internet searches ballpark the number of engineers between 5K and 20K. Here are some things that are true of Cockroach Labs: Managed CockroachDB leverages public clouds such as GCP and AWS. Managed CockroachDB provides a single service: CockroachDB as a service. Basically, the whole engineering organization at Cockroach Labs works on CockroachDB. CockroachDB is a single binary; in order to provide survivability and scalability, multiple “nodes” are run, which join together to form a “cluster”. Cockroach Labs has around forty engineers right now. Not the same at all! In the Google case, a major benefit of container orchestration is that it decouples machine planning from the concerns of the software engineering teams. Google can plan compute in aggregate; then software engineering teams can use Borg to schedule their heterogeneous workloads onto machines. Borg will binpack the workloads so as to use compute efficiently; multiple workloads will run on one machine in general. If software engineers had to work at the level of individual machine, Google would not use compute nearly as efficiently. This is a very big problem when you are at Google’s scale. On the other hand, does binpacking do Cockroach Labs any good at all? The public clouds allow for elastic compute at different shapes; why not just ask the public clouds for VMs, perhaps via a tool like Terraform ? Note that today Cockroach Labs runs a separate instance of CockroachDB per customer, in dedicated GCP projects or AWS accounts, since CockroachDB doesn’t yet support multi-tenancy. The database requires at least 2 vcpu of compute per node to run well, and cloud providers provide machines of this shape on demand. In this world, binpacking does not reduce costs in any meaningful way. It’s not just about binpacking Google has written an excellent whitepaper on the history of Borg, Omega, and Kubernetes . This part is very relevant to this discussion: “Over time it became clear that the benefits of containerization go beyond merely enabling higher levels of utilization. Containerization transforms the data center from being machine-oriented to being application-oriented.” Having used (and loved) Borg, I felt this to be true deep in my bones. Operators describe an application’s requirements declaratively via a domain specific logic called GCL (incidentally, the language is a fever dream of surprising semantics), and Borg handles all the low-level details of requesting compute, loading binaries onto machines, and actually running the workload. If you want to update to a new version of a binary, an operator simply writes a new GCL file to Borg with the new requirements. In summary, Borg provides a wide variety of powerful automation primitives, which make software engineering and SRE teams efficient; they are able to focus on improving their services rather than doing repetitive and uninteresting service operation tasks. Wouldn’t Cockroach Labs benefit from access to solid out-of-the-box automation primitives also? Kubernetes pros: easy automation and orchestration The goal of Managed CockroachDB is to provide CockroachDB to customers without them having to run the database themselves. Distributed databases are complex; not all companies want to develop the expertise in house needed to run them reliably. Managed CockroachDB will eventually allow customers to request the following operations to be run on their cluster via a convenient web UI: Create a cluster running in this and that region with this much compute per node Add or remove a node to a running cluster Add or remove a region to a running cluster Update the database version to the latest one Let’s consider #4 in detail. Without Kubernetes, you could either build the automation yourself or shop around for some other automation tool. Why is building it yourself a bad idea? Let’s consider the requirements. There is no more efficient way to break a piece of software than updating to a new version. This argues that the update must proceed gradually, updating one CockroachDB node at a time. If the database starts to fail (e.g. if health checks fail), the update must halt without operator involvement. It should also be possible to rollback quickly if there are unforeseen problems. At the same time, if the update often halts unnecessarily due to the flakiness of the automation tech, then oncall load increases. The above requirements are tricky to get right, and we have not even considering all the low level details involved in building this yourself, such as SSHing into machines to stage binaries and sending HTTP requests to do health checks. Here’s how to do #4 with Kubernetes: $ kubectl rolling-update crdb --image = CockroachDB:19.1.1 That’s it! Note that the speed of the rolling update is configurable, the automation is robust to transient failures, and it supports configurable health checks. Similar arguments apply to #1, #2, and #3. In fact, the case for Kubernetes is even stronger in these cases, since safely changing the footprint of a stateful application that is serving customer traffic is not something automation technologies other than Kubernetes do particularly well. Previously I asked the following question: “Wouldn’t Cockroach Labs  benefit from access to solid out-of-the-box automation primitives also?” As we built prototypes, we felt the answer was definitely yes. On the other hand, the fact that access to automation primitives is a benefit of using Kubernetes doesn’t imply there aren’t also downsides. Remember the screaming masses on Hacker News please! Kubernetes con: complexity costs One of the main complaints about Kubernetes is that it is complicated. Let’s add some color to the claim that Kubernetes is complicated. In what senses is it complicated? It’s a stateful service. It leverages etcd, a stateful distributed system, to store its state. There’s a hierarchy of independent controllers that work together to automate tasks. The different controllers have pretty clear and separable functions but still it is initially hard to get your head around the dance that they are doing together. Julia Evans has written an excellent blog post describing the dance in detail. In order to allow multiple pods to serve on the same port on one machine, Kubernetes must assign a unique IP to each pod. So the IPs of the VMs themselves are not used directly. According to this video , for GKE specifically, routing rules in GCP, unix networking configuration on the k8s nodes themselves, and the underlying VPC network work together to create the abstraction. Note that on other cloud providers, the model is the same, but the abstraction might be implemented differently. If a user wants to run CockroachDB across multiple geographic regions, then multiple Kubernetes clusters must be joined together in some way. Former Cockroach Labs engineer Alex Robinson has written a blog post about this exploring the options; none of them are simple. 1 and 2 are control plane only. That is, if etcd or the controllers break, we cannot add a node to a CockroachDB cluster running on Kubernetes, but the cluster will keep serving SQL queries all the same. Also, there are managed Kubernetes offering such as GKE that reduce how much Cockroach Labs SRE needs to know about etcd for example. 3 and 4 are data plane. These are the scary ones. If the networking stack breaks, the cluster will NOT keep serving SQL queries. SRE feels we must understand the networking stack deeply, if we are to depend on Kubernetes in production. This is no easy task, especially when already holding onto lots of cognitive load about CockroachDB itself. Why we chose Kubernetes Let’s summarize the situation. Google has used container orchestration for a long time and is very successful with the technology. There are two main benefits: Efficient use of compute, when you are running a suite of heterogeneous services, via binpacking. Solid automation primitives, which allow software engineers and SREs to think in terms of applications, not machines. 1 doesn’t benefit Cockroach Labs  at all. 2 benefits Cockroach Labs a lot. On the other hand, Kubernetes is complicated, increasing cognitive load. Specifically, the networking stack is the main piece of complexity the SRE team worries about. What to do? We feel right now that the benefits of using Kubernetes outweigh the complexity costs. By using Kubernetes, we hope to create a highly orchestrated managed service, which will free us up to do interesting work on the reliability and efficiency of CockroachDB itself. There are also some benefits to Kubernetes that we haven’t covered in detail here, such as the fact that it provides a cross-cloud abstraction (Managed CockroachDB is already available for GCP and AWS), and the fact that many of our existing enterprise customers (not Managed CockroachDB customers) use it to deploy CockroachDB to their private datacenters. We are building out Managed CockroachDB on Kubernetes now and plan to launch it later this year. The SRE team at Cockroach Labs  has open headcount. If working at a startup that builds a super cool distributed database interests you, consider working with us ! A parting thought Why is Kubernetes networking so complicated? One reason is the requirement that each pod has its own IP address, which is needed for efficient binpacking (without requiring application level changes from Kubernetes users). Funnily enough, this requirement doesn’t do Managed CockroachDB any good, since we always run a single CockroachDB instance on each VM. This makes me wonder how many users choose Kubernetes for binpacking vs. for automation primitives vs. for both. In this brave new world of cheap public cloud elastic compute and managed Kubernetes offerings, do many users choose Kubernetes only for the automation primitives? If so, could the networking stack be made even simpler for those users?", "date": "2019-07-01"},
{"website": "CockroachLabs", "title": "Announcing ESCAPE/19: The Multi-Cloud Conference", "author": ["Jim Walker"], "link": "https://www.cockroachlabs.com/blog/announcing-escape-19-the-multi-cloud-conference/", "abstract": "Over the past few years, cloud conferences have taken center stage in the tech world. And as the three largest public cloud providers have grown, we’ve seen each launch a proprietary conference. Re:Invent, Next and Ignite have emerged as the main forums for practitioners tp share best practices and vendors announce major product advancements. But since these shows are run by specific cloud providers, the content is typically presented in the context of that single large cloud provider. Hybrid cloud strategy comes up at these events, but the technical complexities and business ramifications of multi-cloud deployments are barely addressed. In fact, some shows ban the word “multi-cloud” and censor it from materials as they don't want competitors to even be mentioned. All the while, 86% of organizations are either exploring a hybrid cloud approach or already characterize their strategy as multi-cloud, according to a 2018 study conducted by Forrester and Virtustream . There’s a huge disconnect here. Many organizations are already in a multi-cloud world and are addressing some of these difficult challenges, but don’t have the forum to share best practices and learnings. Where can we go to learn about the latest federation capabilities in orchestration platforms that will allow us to run workloads across multiple, global clouds? How are we to think about data in a multi-cloud world? How can we reduce the risk associated with standardizing on a single cloud provider? How can we control costs and gain leverage by researching multiple public cloud options? Can we get to global coverage while optimizing cost? And what about security in a multi-cloud world? There’s a need for a new, unopinionated conference that addresses these concerns. Enter ESCAPE/19 ESCAPE/19 is the first conference focused solely on these hybrid and multi-cloud issues. It’s an open forum to freely speak about multi-cloud without any preconceived opinion or predetermined context. It’s for builders, operators, and executives architecting the next generation of infrastructure. It’s for organizations that want to escape a single cloud and gain the freedom to architect their apps and services across multiple clouds. The inaugural conference will be October 16-17, 2019, in New York City. At ESCAPE/19, leading practitioners will share the best practices that help organizations get the most out of having relationships with multiple cloud providers. It’s a gathering of the business owners of this complex architecture as well as the systems engineers and application developers that are architecting systems in a multi-cloud, global world. We would love for you to join us. Early bird tickets are now available at escapeconference.io . Cockroach Labs, Slower and a balanced agenda Data is our focus at Cockroach Labs, but there are so many additional challenges that need to be addressed. In order to help build out a balanced agenda, we’re excited to partner with stealth, multi-cloud services firm Slower to help as a founding sponsor and content partner to help curate content. Over the next few weeks, we’ll announce a series of industry leaders and hands-on practitioners as speakers for the event. We have already secured a great list of speakers from global companies and bleeding-edge startups but are looking for more. We’ve opened the call for papers and are accepting talks that outline real-world multi-cloud issues. Great talks can be technical or business-related. Stay tuned at escapeconference.io or @theescapeconf for more news about speakers and events, or get your early bird tickets today .", "date": "2019-07-23"},
{"website": "CockroachLabs", "title": "Getting Started with CockroachDB and Flowable", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-and-flowable/", "abstract": "Last week, our friends at Flowable released Flowable 6.4.2 , which officially adds support for CockroachDB. Flowable provides a workflow and Business Process Management (BPM) platform for developers, system admins and business users, and now integrates with CockroachDB in just a few steps, thanks to a recent collaboration . Cockroach Labs and Flowable engineers have been teaming up since the early stages of CockroachDB because it makes deploying Flowable that much easier. Plus, as they write in their blog , “What makes CockroachDB very interesting for Flowable is that it’s scalable and distributed database... In the world of processes and cases, atomicity is utmost importance, as data correctness is sacred.” In their recent blog post about the integration , they show how to get started with running the Flowable engines on top of CockroachDB. We've replicated the steps below so you can get started: Step 1: Start CockroachDB Download the CockroachDB binaries . Start it in a terminal by executing ./cockroach start --insecure --host=localhost Note that this is a simple (non-production) setup with only one node in insecure mode. See the docs for other ways of running it, including clustered. We need a database and user to connect from our application. From the terminal, execute following commands which will create a flowable database and a flowable user. ./cockroach sql --insecure -e 'CREATE DATABASE flowable' ./cockroach sql --insecure -e 'CREATE USER flowable' ./cockroach sql --insecure -e 'GRANT ALL ON DATABASE flowable to flowable' Step 2: Create the project Create a new Spring Boot application (e.g. by going to http://start.spring.io/ and generating an empty project) and add the following dependencies: <dependency> < groupId >org.flowable</ groupId > < artifactId >flowable-spring-boot-starter</ artifactId > < version >6.4.2</ version > </ dependency > < dependency > < groupId >org.postgresql</ groupId > < artifactId >postgresql</ artifactId > < version >42.2.6</ version > </ dependency > < dependency > < groupId >org.springframework.boot</ groupId > < artifactId >spring-boot-starter-web</ artifactId > </ dependency > This adds: The Flowable Spring Boot starter dependency. This will automatically create all Flowable engines (bpmn / cmmn / dmn) using the CockroachDB (CRDB) datasource. The PostgreSQL jdbc driver. CRDB is compatible with PostgreSQL (read about that in detail here ), which makes it easy to reuse a plethora of tools out there. The Spring Boot web starter that we need to craft the REST endpoint Step 3: Configure the application Flowable needs to know how to connect to the database. Add an application.properties file to the resources: spring.datasource.url = jdbc:postgresql://127.0.0.1:26257/flowable?sslmode=disable\n\nspring.datasource.username = flowable\n\nspring.datasource.password = Step 4: Start the application The application is a simple Spring Boot application. It has a RestController that has one method that starts a process instance (the process definition is actually in the processes folder of the resources and it automatically picked up). The ProcessEngine and its services are created automatically and configured to use the CRDB database. As shown in the example below, they are automatically injected in the controller: @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication. run (Application. class , args); } @RestController class MyRestController { private RuntimeService runtimeService ; private TaskService taskService ; public MyRestController( final RuntimeService runtimeService, final TaskService taskService) { this . runtimeService = runtimeService; this . taskService = taskService; } @GetMapping( \"/start\" ) public void startProcessInstance() { runtimeService .startProcessInstanceByKey( \"helloWorld\" ); System. out .println( \"Number of tasks created: \" + taskService .createTaskQuery().count()); } } } Step 4: Run it After starting the application (from your IDE or terminal), the REST endpoint can now be called: curl http://localhost:8080/start which will output Number of tasks created: 1 Number of tasks created: 2 Number of tasks created: 3 ... Go ahead and query the database using any SQL tool to verify the data is in fact there. You’ll be able to validate that using CockroachDB with Flowable is exactly the same as running it against any other relational database.", "date": "2019-07-16"},
{"website": "CockroachLabs", "title": "Feature of the Week: Core Changefeeds in CockroachDB 19.1", "author": ["Roland Crosby"], "link": "https://www.cockroachlabs.com/blog/feature-of-the-week-core-changefeeds-in-cockroachdb-19-1/", "abstract": "Our latest CockroachDB Feature of the Week , core changefeeds, is an exciting new way for event-driven apps to consume data from CockroachDB. With the CHANGEFEED FOR statement, a SQL client can tell the database to monitor a table and tell it when writes take place. CockroachDB will then hold the connection open until the client closes it and stream records to the client for all writes. You can use changefeeds to build event-driven applications, perform data integration, and do all kinds of other tasks that depend on watching the state of a table. Here's what a core changefeed looks like in practice: SQL connection 1 SQL connection 2 root@:26257/defaultdb> CREATE TABLE office_dogs ( id SERIAL PRIMARY KEY, name STRING ); CREATE TABLE Time: 11.598ms root@:26257/defaultdb> SET CLUSTER SETTING kv.rangefeed.enabled = true; SET CLUSTER SETTING Time: 7.139ms root@:26257/defaultdb> \\set display_format csv root@:26257/defaultdb> EXPERIMENTAL CHANGEFEED FOR TABLE office_dogs; root@:26257/defaultdb> INSERT INTO office_dogs (name) VALUES ('Carl'), ('Petee'); INSERT 2 Time: 8.87ms root@:26257/defaultdb> table,key,value office_dogs, [472990757278121985],\"{\"\"after\"\": {\"\"id\"\": 472990757278121985, \"\"name\"\": \"\"Carl\"\"}}\" office_dogs,[472990757278187521],\"{\"\"after\"\": {\"\"id\"\": 472990757278187521, \"\"name\"\": \"\"Petee\"\"}}\" root@:26257/defaultdb> INSERT INTO office_dogs (name) VALUES ('Frankie'); INSERT 1 Time: 3.081ms office_dogs, [472991019430084609],\"{\"\"after\"\": {\"\"id\"\": 472991019430084609, \"\"name\"\": \"\"Frankie\"\"}}\" In the output, the `table` column is the name of the table being watched, the `key` column contains each record's primary key columns (represented as a JSON array), and the `value` column is a JSON object, with the \"after\" key containing the state of all of the record's columns after the table is written to. Inserts and updates are represented as shown above; deletes are represented with \"null\" in the \"after\" field. Changefeeds continue to function even when online schema changes take place: SQL connection 1 SQL connection 2 root@:26257/defaultdb> ALTER TABLE office_dogs ADD COLUMN good_boy BOOL DEFAULT true; ALTER TABLE Time: 117.968ms office_dogs,[472990757278121985],\"{\"\"after\"\": {\"\"id\"\": 472990757278121985, \"\"name\"\": \"\"Carl\"\"}}\" office_dogs,[472990757278187521],\"{\"\"after\"\": {\"\"id\"\": 472990757278187521, \"\"name\"\": \"\"Petee\"\"}}\" office_dogs,[472991019430084609],\"{\"\"after\"\": {\"\"id\"\": 472991019430084609, \"\"name\"\": \"\"Frankie\"\"}}\" office_dogs,[472990757278121985],\"{\"\"after\"\": {\"\"good_boy\"\": true, \"\"id\"\": 472990757278121985, \"\"name\"\": \"\"Carl\"\"}}\" office_dogs,[472990757278187521],\"{\"\"after\"\": {\"\"good_boy\"\": true, \"\"id\"\": 472990757278187521, \"\"name\"\": \"\"Petee\"\"}}\" office_dogs,[472991019430084609],\"{\"\"after\"\": {\"\"good_boy\"\": true, \"\"id\"\": 472991019430084609, \"\"name\"\": \"\"Frankie\"\"}}\" root@:26257/defaultdb> DELETE FROM office_dogs WHERE name = 'Frankie'; DELETE 1 Time: 4.464ms office_dogs,[472991019430084609],\"{\"\"after\"\": null}\" Core changefeeds provide the same ordering and correctness guarantees as our enterprise changefeed functionality, but enterprise changefeeds can write their data to Kafka or cloud storage from multiple nodes at once, instead of delivering their data over a SQL connection. Take a look at our change data capture documentation to try out core changefeeds. And follow Cockroach Labs on Twitter to keep up with all our Features of the Week.", "date": "2019-07-30"},
{"website": "CockroachLabs", "title": "A $55M Milestone in our Mission to Make Data Easy", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/series-c/", "abstract": "It’s shaping up to be a big year at Cockroach Labs. We launched CockroachDB 19.1 and improved our ability to serve global businesses including Lush, Bose , and WeWork. We grew to four offices worldwide and surpassed the 100 employee mark. We changed our licensing to simultaneously serve the developer community while growing the company. And today, we’re proud to announce that Cockroach Labs has secured $55 million in Series C funding. The round is led by Altimeter Capital, Tiger Global, and existing investor GV, with additional participation from our existing investors at Benchmark, Index Ventures, Redpoint Ventures, FirstMark Capital, and Work-Bench. We started our journey to Make Data Easy in 2015. Over the past four years, our roadmap and strategy have evolved in the direction of supporting this mission. This round of funding will help us redouble our efforts on that front. As we work with more and more customers, it’s become clear that making data easy encompasses a larger host of commitments. It means making deployment easy. It means making performance easy. It means making resiliency easy. This funding will allow us to build products that answer those needs, and accelerate our growth in the database space and beyond. So what can you expect from us in the coming year? We’re sparking conversations about what multi-cloud deployments look like and how to make them work at ESCAPE/19 this October . We’re improving upon our Managed CockroachDB product, which will further enable developers to launch a database without much upfront planning and investment. We’re expanding our distributed team, and are hiring across all departments in the United States and Europe. Want to stay in touch on our journey to Make Data Easy? Join our mailing list to stay up to date with the latest product releases, news, and events.", "date": "2019-08-06"},
{"website": "CockroachLabs", "title": "Flex Fridays: Amruta Ranade Vlogs about Tech Writing", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/flex-fridays-amruta-ranade-vlogs-about-tech/", "abstract": "Perhaps the best-known benefits at Cockroach Labs is our Flex Friday policy. Cockroach Labs uses Flex Fridays as a way to provide our employees with balance. While it is interpreted differently by each department, we're excited to share what some employees work on during these days. Amruta Ranade is a Senior Technical Writer on our Education team. While in the office, you can find her documenting our Managed Service Offering; outside the office, you can find her creating Youtube videos. -- Flex Fridays are a dedicated time to focus on my side-quests which include learning and experimenting with different ways of communicating technical information and then finding ways of incorporating them into my documentation tasks. My ongoing Flex Friday project is making YouTube videos about tech writing and life in tech. I was an avid YouTube fan even before I started my channel. I used to binge-watch videos by The Curious Engineer , vlogbrothers , and Charli Marie . As a technical writer, I was fascinated by the way they used the medium to convey complex topics in an entertaining way. I started making my own amateurish tech videos and received tremendous encouragement from my fellow Roachers. This motivated me to keep going with the project which has now developed into a creatively satisfying hobby that’s potentially helpful to others. Some of my favorite video projects: Database Sec urity Concepts This is a 3-minute explainer video about database security topics, namely authentication, authorization, encryption, and audit logging. I made this video when I was working on the CockroachDB security docs and realized that these topics are important yet intimidating even for experienced engineers. I wanted to find an easily-digestible and entertaining way to educate our audience about the different aspects of database security. Conceptualizing and story-boarding the video was the most difficult part of the process, whereas creating the animations was the most fun part. Tech Experiment: My Plan and Process of Building an App This video gives a behind-the-scenes look at my process of learning a new tech stack. As technical writers, we are often told that we should constantly work on our tech skills, but there isn’t enough guidance on how to go about it. One of my goals for the channel is to be transparent about how messy the tech learning process is and share how I decide what to learn and find the right resources to learn it. Since I am not a software engineer, working on this series makes my imposter syndrome worse. However, I take solace in the fact that being transparent about my struggles with learning new tech encourages other non-engineer technical writers to tackle unfamiliar tech stacks. Writing Experiment: I tried (tech) writing like Kate Cavanaugh In this video, I tried the writing routine of my favorite authortuber (an author who makes YouTube videos). This is my favorite video so far. It helped me break out of my writing rut and pushed me to the limits of my video-making capabilities. I hadn’t done a day-long vlog before and found it to be tiring but extremely fun. I look forward to another writing experiment vlog next quarter. In short, Flex Fridays allow me to indulge my creativity and stay engaged in my job without burning out. Beyond that, they also help me develop my personal brand as a credible voice in the tech community.", "date": "2019-08-07"},
{"website": "CockroachLabs", "title": "Onboarding as a Team Sport", "author": ["Chelsea Lee"], "link": "https://www.cockroachlabs.com/blog/onboarding-as-a-team-sport/", "abstract": "For new employees, starting at a new company isn't always a comfortable transition. Even if they’re excited about their new role, day 1 at a new office can be scary. They’ve left the familiar feeling of their old position for the unknown situation of a new company. Onboarding is meant to answer some of these unknowns. While the HR team can sit you down and walk through how the company came to be and when you’ll be paid, there’s some amount of onboarding where a meeting with HR doesn’t cut it. At Cockroach Labs, we’ve made onboarding a team sport. New hires are paired with a Roachmate -- their go-to resource -- with many other Roachers helping to get new hires up to speed on our product and how things work. The process is organized by our People Ops team, but with lots of help from every department at Cockroach Labs. We also refer to our onboarding process as MOLTing. Of course, we're a company called Cockroach Labs, and if you didn't already know, cockroaches molt. It's a process they go through early on in their lifecycle to shed their skin and build that resilient outer shell for which they're known. This reflects our onboarding program in that we hope you learn useful things during the start of your time here to become a resilient Roacher. What’s a Roachmate? All of our new Roachers (interns included!) are paired with a Roachmate, or onboarding buddy. Ever since our first People Operations hire, Cockroach Labs employees have had this onboarding experience. Roachmates serve the purpose to be a sturdy bridge for you to acclimate to what we're doing and how we do it at Cockroach Labs. In your first week, you are guaranteed to have someone who wants to introduce you to our team and show you around the office. Most importantly, your Roachmate is an open door to ask endless questions, anything from \"when is lunch served?” (11:45 am, we eat on the early side) to “can I feed Carl a piece of beef jerky?” (no, even if he begs for it)  to \"how do I set up a testing cluster?\" (use roachprod ). When we did a revamp of the Roachmate Program, there were common themes coming from those who were Roachmates and those who had recently joined the company. The key takeaways were that there is value in having a Roachmate but there needed to be more structure around what's expected of them. While it was formerly unclear how often the pairs should be meeting, we set standards for the Roachmate Program being one 1:1 weekly during a new employee’s first four weeks. We understand that being a Roachmate is a time commitment so we wanted to set clear expectations on both sides. From this feedback, we’ve incorporated some significant changes to our Roachmate program.  We’ve put more focus on the Roachmate Program being a learning opportunity for current employees to gain mentorship skills to assist their career growth. With that, the responsibilities will be more clear as we equip our Roachmates with checklists and topics to cover with the new employees. Lastly, we’ve chosen products to streamline the onboarding process that integrates with the products we already use. With all of these updated aspects together, we hope to create a hassle-free onboarding period for not only the new employee but our employees wanting to be involved in this period of ramp-up. Roachmates are a huge benefit to new employees that join us, especially as the company continues to grow. While the HR team would love to be a Roachmate to all new employees, it isn’t feasible as people are joining various functions of our company. We’re thoughtful when it comes to these pairings, as we pair those who will have valuable skillsets to their new role, like product managers with an engineer of that product area or recruiters with the hiring managers they’ll be working closely with. We’re appreciative of those who want to step up and take on these responsibilities so that we can better support a growing product and company. Your First Weeks Enterprise technology is not an easy industry to join. While many people can quickly name their favorite apps and tools, it’s rarer to find someone listing off their favorite enterprise technologies.  During your first weeks at Cockroach Labs, we want to make sure you are given enough time not only figure out how we do things day to day but also get up to speed with understanding what CockroachDB does and where we fit into the database space. As you’d expect, we cover the fun general and administrative onboarding during Week 1. This covers a number of topics that apply to all new employees that join us. Day 1 starts off with sharing our company values, how we’re organized and making sure you’re filling out all the correct employment documents. We understand that most of this day is also spent setting up your email filters and making sure your tooling environment is productive. Day 2 brings insight into how we do things around communication methods and meeting culture. Then, we dive into our software and tools that we use internally and wrap up the day with you introducing yourself at the Company Wide Meeting. HR will close out on their sessions on Day 3, with a final presentation on answering all of your questions about payroll, benefits, and perks. (Parents, it’s true, we cover you and your dependents’ health insurance 100%!) Since enterprise tech is not always the most popular topic of discussion, it’s common to not have any familiarity with a database or what it is. We’ve built-in sessions to help ramp everyone up to speed on what a database is and where CockroachDB fits into the database landscape. We also show you how one could use CockroachDB and set up a cluster and lastly, share stories about how our customers are using us. Starting here and familiarizing yourself with databases during Week 1 sets you up for success during Week 2 and beyond, as you begin focus on more role-specific projects. Each of these sessions are run by other Roachers across our company, in the Product, Marketing, Documentation and Customer Success team. It’s a commitment of time from these team members that participate in onboarding to want to help educate the next generation of Roachers. Over the next couple of weeks of the month, new Roachers will sit through sessions focused on How \\\\[Team Name] Works, to give insight into what other teams at Cockroach Labs do. The \"How Marketing Works\" session is presented by our CMO, who shares what defines a MQL (Marketing Qualified Lead) for us.\"How Product Design Works\" is shared by our Director of Product Design who shows the latest wireframes created and feedback from user interviews. \"How Recruiting Works\" is run by a recruiter who also covers training new employees on our exercised based interviewing process. For each of these teams, it’s a great opportunity for a call to action in how others can collaborate with them on projects and share all the amazing work that their team has done. Not just that, they get to understand how all our teams function and work together to make progress on the company goals. Future Iterations Onboarding is viewed as a continuous work in progress, we can only expect more changes to come as we build out new features that need to be explained and more teams are created. After their first 30 days, we take time to check in with our new Roachers and figure out what’s been going well, what could be added and where we can improve. It’s a helpful way to keep our finger on the pulse of onboarding as the company grows and shifts. At Cockroach Labs, we strive towards making everyone successful and want to equip them in the best way possible. We hope that having these built-in resources like Roachmate and Your First Weeks supports these goals into enabling our employees to come in and be able to make an impact as soon as possible! Just like our Open Sourced Interview Process, where we share our exercised based interview prompts, we want to share Your First Weeks with you, future Roacher ;) in hopes that you feel more well-prepared when joining our team.", "date": "2019-08-01"},
{"website": "CockroachLabs", "title": "Learning is Good: Our 3 Favorite Talks at GopherCon 2019", "author": ["Alfonso Subiotto Marques"], "link": "https://www.cockroachlabs.com/blog/learning-is-good-alfonso-subiotto-marques-goes-to-gophercon/", "abstract": "Learning is Good is a program at Cockroach Labs that provides a stipend for employees to take advantage of resources for their professional development. Employees are encouraged to take courses, participate in workshops, attend conferences or obtain relevant certifications. Alfonso Subiotto Marques is a Member of Technical Staff on our SQL Execution team. In the office, you can find him working away on distributed query execution infrastructure; outside of the office, you can find him watching soccer or writing music. Using \"Learning is Good\" to attend GopherCon 2019 Cockroach Labs supports the professional and personal development of employees through initiatives such as Flex Fridays (which I’m using to write this blog post) and Learning is Good. I recently took advantage of the Learning is Good program to attend GopherCon 2019 in San Diego, CA, and I wanted to highlight a couple of talks that stood out. GopherCon is an annual conference about the Go programming language. Given that we use Go extensively here at Cockroach Labs, going to GopherCon seemed like a good way to meet other Go users and possibly learn about how to better use it in my day-to-day. Below are a couple of my favorite talks and what I learned from each one: Go, pls stop breaking my editor The most important message of this talk for me was that the Go team considers it time to take responsibility for Go developer tools. This means taking ownership of the current widely-used tools, but also offering an interface to more easily write these kinds of tools. This is something that one of our engineers, Matt Jibson, has recently taken advantage of to rewrite some of our code linters. I found that this talk, coupled with mentions of go modules and module proxy servers in other talks, served as an example of how Go is maturing. Better x86 Assembly Generation with Go This was a talk where the author of the avo x86 generation tool walked through an example of speeding up a vector dot product computation by generating x86 vector instructions. Avo is a tool that makes it easier to write x86 assembly by offering a layer of abstraction on top of writing x86 yourself. Since I work on the SQL Execution team and we’re implementing a vectorized execution engine (learn more here and here ), this talk was an introduction to a tool that could prove to be invaluable down the line. Two Go programs, three different profiling techniques Some of the most engaging presentations are live coding tutorials. In this one, Dave Cheney, an open source contributor for the Go programming language, used CPU and memory profiling to improve the performance of a word counting program. He also used the execution trace tool to analyze and improve the parallelism of a Mandelbrot generator. I had used the profiling tools before but found the execution trace tool too complicated and under-documented to use effectively. This served as a useful introduction to the execution trace tool and a glimpse at how powerful it can be in certain cases. In addition to these, there were talks about the future of Go , a proposal for the addition of generics and a very interesting deep dive into diagnosing high CPU usage in a video-streaming server . Overall, I found the GopherCon experience fun and extremely valuable. I discovered new tools, learned how to use the ones I knew better, and increased my awareness of the Go landscape by getting a better understanding of the future of the language and learning about other interesting production use-cases. As a result, I believe that attending GopherCon has made me a better engineer. I’m grateful that Cockroach Labs supports learning and development this way and will be on the lookout for more opportunities to use Learning is Good!", "date": "2019-08-14"},
{"website": "CockroachLabs", "title": "Vectorizing the Merge Joiner in CockroachDB", "author": ["George Utsin"], "link": "https://www.cockroachlabs.com/blog/vectorizing-the-merge-joiner-in-cockroachdb/", "abstract": "Everybody loves a fast query. So how can we make the best use of the existing information to make joins on sorted data faster? The answer is lies in vectorizing the merge join operator. Today we’ll be looking into what a merge joiner is (or what it used to be), followed by what vectorization means and how it changes the problem, and ending with how we decided to make the merge join operator faster and what this means for your queries. CockroachDB is an ultra resilient SQL database for global business, which means that as a database, it has to meet the needs of scalable business critical applications. On top of reliably storing and replicating data, a distributed database also needs methods to query said data, whether it be a single row at a time, or a complex operation involving multiple tables. One of the more complex operations is a SQL Join, which combines two tables, such that for every pair of rows between the two tables, if there is a matching column between them, then the rows are added to the output. Consider the following example: SQL Join combining two tables. In this example, the blue, red and green rows matched so they were added to the output. Under the hood, the join operator may work in various ways, depending on the kind of information available at hand in the database. The Traditional Merge Join Algorithm The Merge Join (or Sort-Merge-Join) is a type of join that makes use of the sorted ordering of the inputs to process the rows more efficiently. Using a merge join works best when there is an existing index on each of the input tables, since an index can be used to efficiently retrieve the rows in sorted order. To understand exactly what a traditional merge joiner looks like, consider the following algorithm: Source: http://www.dcs.ed.ac.uk/home/tz/phd/thesis/node20.htm The crux of this algorithm is essentially the fact that it’s a linear scan on each of the two tables. Consider two “fingers” that run down the input, one for each input table. If the fingers see a pair of rows that match, then the rows get added to the output. If they don’t, then the finger pointing to the lesser value gets incremented and points at the next value. Since both inputs are sorted, this means we never miss a pair as the fingers are bound to converge on a matching pair if it exists. At the same time, this algorithm is also efficient because it “skips” values that don’t match and are irrelevant to the output. The algorithm is visualized below: Reasons to Vectorize While a traditional merge join algorithm is efficient in theory, it fails to take into account the overhead of integrating this algorithm into a system. Due to the nature of how data is stored and the interfaces used to process the data within the database, there are a few key factors that make this algorithm behave slower in practice than in theory. In a database, each row of data is encoded in a format that is optimized for a specific purpose, which may not necessarily be the same purpose across all cases. To be specific, the current state of the row by row engine stores each column of every row in something called a datum. To be able to compare this datum to another datum, there is extra overhead in each call to retrieve the value, as first the type of that value has to be determined after which the value has to be decoded. Now if this process happens for every single column of every single row, it is clear that isn’t the most efficient use of processor time. Instead, it would make sense if the type check and conversion could happen once per column, since every value in the column has the same type. This is called vectorized execution, since the idea is to operate on values a column at a time. Constraints and Requirements CockroachDB is currently actively working on utilizing this vectorized approach in its operators, so an interface for the vectorized model has been set up. This implies there are several constraints on how an operator can be built. The first and most important constraint is that the vectorized merge join operator has to adhere to the higher level idea of operating on only one column per table at a time. This is a necessary conviction to ensure that shortcuts aren’t taken and that the operator as a whole doesn’t regress back to the row by row model where values of an arbitrary column can be accessed at any time. The next constraint on the operator is that it has to adhere to the interface and properties of the other vectorized operators. This means being able to support batching, which is the process of taking large tables of input and breaking them into batches of rows, typically 1024 rows long. Each of these batches is accompanied by a null bitmap which indicates which rows are null and a selection vector. The selection vector is similar to the bitmap in that it indicates which rows are actually in the batch. This allows the flow of the operators to delay coalescing the values returned by each successive operator, saving time. The merge join operator also has the requirement that it works for all the use cases of a typical join operator. In other words, there has to be support to: join two tables on one or more equality columns join on different types (ex: integers, floats, decimals, booleans, etc) handle different types of joins (inner, outer, left, anti, semi joins, etc) gracefully handle `COUNT` queries when there is no output Lastly, the operator must be generally correct in that it doesn’t produce the wrong output, it is more efficient than the current row by row approach and that the code is maintainable regardless of the computation complexity. Design Choices While there were several iterations of the vectorized merge join operator, there were a few design decisions that ended up being essential in the formation of the operator. The first of these decisions was the need for an explicit probing and building phases. In a traditional merge joiner, the output is generated on the fly as input rows are processed since all the columns are available at once. This is not the case in a vectorized merge joiner, as we are only able to work on one column at a time and there can be multiple columns on which we have to perform equality checks. Thus, the merge joiner is split into a probe phase and build phase. In the probe phase, we determine all the groups that need to be materialized to output. A group is defined to be a pair of ordinal ranges, one ordinal range for each input table, such that the equality columns in each of the ranges are equivalent. In the build phase, we take these groups and materialize them a column at a time, which is a non obvious conversion from row by row to column at a time operators. Finally, each of these phases benefits from code templating, which is a technique that provides for efficient, yet readable code. Probing for Cardinality In a traditional merge joiner, each row is compared once, since we have access to all the columns at any point in time. In vectorized execution, we instead have to compare each column once, but determining if two rows match is dependent on all the equality columns in a row. Thus, translating the equality comparisons to vectorized execution requires saving state about the rows we are interested in. This state ends up boiling down to a list of groups for each batch, as this list of groups can be generated efficiently using an iterative approach. To generate the list of groups, we can use a filter approach, where we first start off with the maximal match between the batches. Then, we use each equality column to filter out the rows that don’t match in each of the groups from the previous iteration. With this approach, we can effectively ignore all the extraneous information about the table and focus only on the ordinal ranges that need to be materialized. This approach addresses the first constraint of only being able to look at one column at a time, as metadata flows between columns through the additional state about groups instead of having to index into each column for each row. Further, it also addresses points a) about joining two tables on multiple equality columns, and point b) about being agnostic to typing within the columns, since the ordinal ranges in the groups have no need for type information. Materializing Output Materializing output in the merge join operator involves taking the groups/ordinal range information from the probing phase, and creating the output cross product for each group. A cross product for two input tables is defined as a new table that pairs every row of one input with every row of the other: This cross product is intuitive to construct in a row by row fashion: create two nested loops that create every pair rows one by one and add those to the output. The interesting part is that the cross product can also be created in a column by column fashion with a key realization. Looking at the result of the cross product, there are some patterns in the output form that can be exploited to increase performance of materialization. In the example above, it's clear that the left column of the output seems to be an “expansion” of the left input, and the right column of the output seems to be a “repetition” of the right input. A key observation is that the expansion/repetition information is actually encoded in the opposite input’s length! Thus, if you have a pair of inputs and both of their lengths, you can actually materialize the cross product in a column by column fashion: Left cross product of join Right cross product of join Making use of column at a time materialization has several impacts, the most important one being performance. Materialization is one of the bottlenecks in any join operator, since copying values from the input to the output is a relatively expensive process. This means that by materializing the output column by column, we can get a significant performance boost over the traditional approach, as we don’t need to perform type inference before we copy each value. Further, this approach also satisfies the constraints of making use of the null and selection vector, as this additional information can be evaluated during the build phase, making it a good point at which to coalesce the data. The build phase addresses point c) since supporting different types of joins can then be handled exclusively by the build phase, and d) since the build phase can be skipped in its entirety if the query is a `COUNT` and there is no output. Templating as an Optimization Choosing to template code is a design decision that allows for more readable and maintainable code, since it provides two views to the same logic: one that is human friendly and one that is machine friendly. Templating was originally designed for static web pages with dynamic elements. Imagine a welcome screen after a user logs in; most of the content on the web page is static, except for a few elements like the username for example. Templating involves using special syntax to allow for certain fields to be interpolated in a larger document, like the username in the webpage. It becomes interesting when the same templating construct can be used to generate source code. This can be desirable in a few cases when efficiency is a primary concern. Take into consideration the following code example: for i := range src { if len (src) > 5 { dest[i] = src[i] } else { dest[i] = src[foo[i]] } } While this may get the job done, there are obviously some inefficiencies in the fact that every iteration of the inner loop checks a property that doesn’t change for the duration of the loop. Optimizing this code would involve moving that check out of the inner loop: if len (src) > 5 { for i := range src { dest[i] = src[i] } } else { for i := range src { dest[i] = src[foo[i]] } } However, the problem with this code is that it violates the DRY (Do Not Repeat Yourself) principle which renders it unmaintainable, especially if the for loops grow in size or start diverging. The solution in this case is templating, since most of the for loop in the example stays the same, and only the element being indexed changes. The idea with templating is that we treat the source code like a string, and so the `[i]` index can be easily swapped out for `[foo[i]]`! Thus, the templated code would look like the following: {{ define forLoop }} for i := range src { dest[i] = src[_SRC_INDEX] } {{ end }} if len (src) > 5 { _FOR_LOOP( false ) } else { _FOR_LOOP( true ) } This block of code would then be expanded out to become the optimized snippet we saw before, as the `_FOR_LOOP` would be replaced or “copy/pasted” in a sense. This technique is useful in the merge joiner since it can be used to deal with various states of selection vector or types. Since the selection vector and type information is relevant to the column as a whole, we can perform the switching necessary to accommodate the state outside of the tight loop, similar to the technique described in the example above. Although understanding templated code has a bit of a learning curve, this technique satisfies the criteria of both efficiency and readability/maintainability. The End Result: 3x Improvement on Standard Merge Joins The vectorization of the merge join operator has a few key impacts on CockroachDB. Firstly, it brings the vectorized execution engine one operator closer to being production ready, as it is currently held under an experimental/beta flag. Second, when the operator is planned and invoked, it provides a significant speed improvement over the existing row by row based model. In fact, we can see up to a 20x improvement in end to end latency for `COUNT` queries involving joins and a 3x improvement for standard merge joins: root@:26257/tpch> set experimental_vectorize=off; # Use the row by row merge joiner root@:26257/tpch> SELECT COUNT(*) FROM partsupp INNER MERGE JOIN lineitem ON ps_suppkey = l_suppkey INNER MERGE JOIN supplier ON s_suppkey = ps_suppkey; ... Time: 1m54.55565s root@:26257/tpch> SELECT ps_suppkey FROM partsupp INNER MERGE JOIN lineitem ON ps_suppkey = l_suppkey INNER MERGE JOIN supplier ON s_suppkey = ps_suppkey LIMIT 100000000; ... Time: 31.135729s root@:26257/tpch> set experimental_vectorize=on; # Use the vectorized merge joiner root@:26257/tpch> SELECT COUNT(*) FROM partsupp INNER MERGE JOIN lineitem ON ps_suppkey = l_suppkey INNER MERGE JOIN supplier ON s_suppkey = ps_suppkey; ... Time: 6.072681s root@:26257/tpch> SELECT ps_suppkey FROM partsupp INNER MERGE JOIN lineitem ON ps_suppkey = l_suppkey INNER MERGE JOIN supplier ON s_suppkey = ps_suppkey LIMIT 100000000; ... Time: 11.286102s Because of this, the bottleneck becomes not the joiner itself but the speed at which data can be read from the disk. This is still important as a more optimized merge joiner can free up CPU cycles to be used elsewhere, on another query perhaps. Want to help us build that even further optimized merge joiner? We're hiring! Check out our careers page for more information. This post was written by George Utsin, a former Cockroach Labs intern.", "date": "2019-06-18"},
{"website": "CockroachLabs", "title": "SQL Prober: Black-box Monitoring in Managed CockroachDB", "author": ["Jay Lim"], "link": "https://www.cockroachlabs.com/blog/sql-prober-black-box-monitoring-in-managed-cockroachdb/", "abstract": "This blog post reflects my work as an intern this summer with the SRE team at Cockroach Labs. Earlier this year, we started building a service offering called Managed CockroachDB . While working with a group of beta customers, we found an unconventional solution to a huge problem: how do we create black-box monitoring in a distributed system? Improving our monitoring system: black-box monitoring! Four months into our beta release, we had an incident: creating new databases and tables was broken globally on one of our internal beta clusters. The Core team tracked the issue down to a bug that involved Raft, and it was presumed to be caused by a network partition. The temporary solution was easy: restarting the Cockroach node with the broken range fixed the issue. While it was an easy fix, it alerted us to a much bigger issue: we needed something in place that would reduce our Mean Time to Detect (MTTD). All of our alerts during that time were built on top of metrics collected through CockroachDB. These metrics, also known as white-box metrics, are data collected based on the internals of the system. Furthermore, when the issue was resolved, there wasn’t a way to measure the impact of the incident. We know for sure that the customer was not able to run queries for a period of time, but how long was it exactly? We didn’t know. Why we built black-box monitoring There were a few ways to address this problem. We could either rely on the existing white-box metrics that we had and add more alerting rules, or build a new monitoring tool that uses black-box metrics. Since we wanted something that would allow us to test the behavior of clusters as our customers would see them, the latter approach (also known as black-box monitoring) seemed more appropriate. By doing this, we could also avoid the situation in which our monitoring system goes down if the system that is being monitored goes down. In the end, we built SQL Prober, an internal black-box monitoring system [1] . Our initial goal was to reduce MTTD and measure the uptime of our customers’ clusters. The framework for SQL Prober is simple. It functions like a cron (a time-based job scheduler) and executes a set of probe tasks periodically at fixed times. On top of that, it contains a service discovery mechanism built using CockroachDB’s node liveness data [2] . Node liveness data provide us with authoritative information about which nodes are live at any given point in time, and if that breaks, most of the core features in CockroachDB will fail to function. A unique architecture requires a unique solution How can we collect black-box metrics through SQL Prober? Well, we could add a task that issues SELECT 1 queries to the database and collect appropriate metrics. But this will not exercise all the layers that CockroachDB has, and in particular, the key-value storage. We need queries that will cover the key-value layer because that is where all the data is coming from. There are a couple of constraints that we needed to satisfy. We wanted: Queries that will hit the key-value layer. Queries that will not affect our customer’s data. Queries that can detect when a node will fail to serve customer’s data. We could easily satisfy (1) and (3) by issuing read and write queries to all of our customer’s schemas. However, this does not satisfy (2) and there is a possibility that there will be a lot of schemas and data to work with. Satisfying (2) with this approach might be possible by modifying CockroachDB itself, but that involves quite a bit of work, and we still need to solve the latter problem. On the other hand, we could satisfy (1) and (2) by issuing read and write queries to our own custom schema, but how do we ensure we could satisfy (3)? In CockroachDB, data get scattered all over the cluster in the form of ranges. There could be a possibility in which nodes that store customer’s data do not overlap with nodes that store our own data, so issuing queries to our own schema will not help with (3). Luckily, we have a solution that could satisfy all three constraints: replication zones . We could use replication zones to ensure that every node in the cluster contains at least one range of our data and the leaseholders for those ranges are scattered evenly across the cluster. That way, sending a single SELECT * statement will reach all the nodes in the cluster. The assumption that we are relying on here is that if a node fails to serve a single range, it might fail to serve other ranges as well. That is how we satisfy (3). Using geo-partitioning in black-box monitoring As we have defined previously , geo-partitioning grants developers row-level replication control. We created a nodes table that contains just a single node_id column (also the primary key), and attempted to store data for each row onto the relevant nodes based on the node’s ID. The table stores the node IDs of all the nodes in the cluster. Here’s the CREATE TABLE statement for the nodes table: CREATE TABLE IF NOT EXISTS `nodes` (\n  node_id INT NOT NULL,\n  PRIMARY KEY (node_id)\n) The ALTER TABLE ... PARTITION BY statement is then used to create a partition for each row. (Each node can only correspond to a single row since node_id is the primary key of the nodes table.) For example, the ALTER TABLE ... PARTITION BY statement for a three-node healthy CockroachDB cluster is shown below: ALTER TABLE nodes PARTITION BY LIST (node_id) (\n  PARTITION node_1 VALUES IN (1),\n  PARTITION node_2 VALUES IN (2),\n  PARTITION node_3 VALUES IN (3),\n  PARTITION \"default\" VALUES IN (DEFAULT)\n) Now that we have defined partitions on the nodes table, we will need to configure location-specific replication zones for these partitions. For this to work, each CockroachDB node is configured at startup with hierarchical information about its locality. These localities are then used as targets of replication zone configurations. To set these zone configurations, the ALTER PARTITION ... CONFIGURE ZONE statement is used. Since we want each partition to link to its corresponding node, we will need to create a replication zone that is unique to each node using both constraints and lease_preferences . Node IDs seem like the perfect key to use for these constraints. Unfortunately, locality flags can only be specified during startup at the moment, and there isn’t a way to obtain those node IDs during that time. We resorted to using the hostnames of nodes to identify each node. By default, each non-system range has three replicas. We used a per-replica constraint to pick the exact location of just one replica of each range, and locate the lease of each range to be the same location as the replica that we just picked. By doing that, we know that if that specific node is healthy, the lease must be located on that node, and when its corresponding row is queried, the data will be obtained from that node. Here’s an example. Assuming that node 1 has a hostname of localhost:26257 , the statement below ensures that at least one replica for partition node_1 is stored on node 1, and the lease for ranges of that partition is preferred to be located on node 1. ALTER PARTITION \n  node_1\nOF TABLE \n  nodes \nCONFIGURE ZONE USING\n  constraints='{\"+dns=localhost:26257\": 1}',\n  lease_preferences = '[[+dns=localhost:26257]]' The only caveat to using this approach is that if the hostname of a specific node changes (e.g. caused by a node restart), we will need to update the zone configuration corresponding to that specific node’s partition. The Probe Task Now that we have geo-partitioning set up, the only thing left is to query from the nodes table and ensure that the request is served by our desired nodes by checking the leaseholders of those ranges. Note that depending on when entries are inserted into the table, there is a possibility that a lease transfer needs to happen. To solve that, we will wait approximately one minute before executing the probe task for the constraints to be satisfied and leases to be transferred. We cannot guarantee that all leases will be transferred after a minute, but that period should be sufficient for lease transfers to occur. If you are familiar with replication zones, you might be wondering: why not reduce the replica count to 1 and avoid checking leaseholders? That is something that we could potentially work on in the future. At the moment, it seems like querying the internal crdb_internal.ranges schema will hang when any range has a loss of quorum. Some investigation needs to be made, and we might need to make changes to the database itself. To move forward with the SQL Prober, we resorted to checking the leaseholders as a first step. (See #29638 for more information.) For every probe task, we will pick a random node to be the gateway node from the list of healthy nodes. (Note that this gateway node is SQL Prober specific and is unrelated to the gateway node used in CockroachDB as described in the Life of a Distributed Transaction article.) This list is kept updated by the service discovery mechanism that we have described earlier. In order to verify that we could actually read data from all the healthy nodes, we will need to: Verify leaseholders for ranges of the nodes table. Read data from the nodes table. Earlier, we applied the lease_preferences constraints to partitions. This will attempt to place the leaseholders for ranges of the nodes table in specific locations, and if that fails, it will fallback to locations of other replicas which are available. Step 1 is crucial because there is a possibility that the leaseholder for a specific range is not held by our desired node. If verification of leaseholder fails, it is meaningless to run Step 2 since that range request will be served by a different node. Step 1: Verify leaseholders for ranges of the nodes table We have created a custom nodes_partitions view for leaseholder verification. nodes_partitions is a view that returns the ranges of the nodes table, specifying which partition the ranges belong to and the current leaseholders for those ranges. With that view, it is trivial to verify leaseholders for ranges of the nodes table. (Details about the view will be described in the next section.) Here’s an example output of the nodes_partitions view: root@:26257/monitoring> select * from nodes_partitions;\n\n  node_id | lease_holder | range_id | replicas\n+---------+--------------+----------+----------+\n        1 |            1 |       82 | {1,2,3}\n        2 |            2 |       83 | {1,2,3}\n        3 |            3 |       84 | {1,2,3}\n(3 rows) We could issue a query to verify if the values in both the node_id and lease_holder columns match. A mismatch signifies that either a node is down or the lease has not been transferred yet. Step 2: Read data from the nodes table Now that we have verified leases for ranges that we care about, we will execute a simple SELECT statement to read from the nodes table for nodes that we care about. Queries should return within a timeout period, and if they do, we are done. A deep-dive into the nodes_partitions view Based on the schema of the nodes table and how partitions were created, we can be sure that a partition can only have one range. We will now attempt to understand how ranges of the nodes table are mapped to CockroachDB nodes based on the structure of those ranges. Ranges are contiguous chunks of CockroachDB’s key-space, and every range contains starting and ending markers. These markers define what keys are stored in a particular range. The keys are represented using the following structure: /<table Id>/<index id>/<indexed column values> The table itself is stored with an index_id of 1 for its PRIMARY KEY column(s). These keys are actually used in our underlying key-value storage to map table data to key-value storage. (If you want to learn more about this, feel free to check out the article about the Distribution Layer in CockroachDB and this blog post on mapping table data to key-value storage .) By default, a newly created empty table will have one range corresponding to it. Partitioning a table by list without applying any zone configurations will not result in range splits unless the size of the default range exceeds range_max_bytes for that specific zone (which is unlikely), a manual range split is triggered, or whenever ranges are eligible for load-based splitting . We will illustrate range splits due to applying zone configurations using the example below. For the purpose of this example, we will assume a three-node CockroachDB cluster and that the ID of the created nodes schema is 677. Here’s the complete schema of the nodes table with partitions defined: CREATE TABLE IF NOT EXISTS nodes (\n  node_id INT NOT NULL,\n  PRIMARY KEY (node_id)\n) PARTITION BY LIST (node_id) (\n  PARTITION node_1 VALUES IN (1),\n  PARTITION node_2 VALUES IN (2),\n  PARTITION node_3 VALUES IN (3),\n  PARTITION \"default\" VALUES IN (DEFAULT)\n) By default, we have the following range for the nodes table: root@:26257/monitoring> SELECT range_id, start_pretty, end_pretty FROM crdb_internal.ranges WHERE table_name = 'nodes';\n\n  range_id | start_pretty | end_pretty\n+----------+--------------+------------+\n       802 | /Table/677   | /Max\n(1 row) Usually, keys of a specific range will span from /Table/<table id> to /Table/<next table id> . If the next table ID does not exist, the key will be /Max . Applying a zone configuration on partition node_3 will result in range splits: root@:26257/monitoring> SELECT range_id, start_pretty, end_pretty FROM crdb_internal.ranges WHERE table_name = 'nodes';\n\n  range_id |  start_pretty  |   end_pretty\n+----------+----------------+----------------+\n       802 | /Table/677     | /Table/677/1/3\n       773 | /Table/677/1/3 | /Table/677/1/4\n       774 | /Table/677/1/4 | /Max\n(3 rows) Observe that two splits occurred - one at /Table/677/1/3 , and another one /Table/677/1/4 . Since the index used here is the primary key of the table, which is an integer, we would expect that the next indexed column value is 4. We can now infer that range 773 corresponds to partition node_3 , and in fact, if we were to continue to apply zone configurations to the remaining partitions, more range splits will occur and we will notice that the partition that the range belongs to can be determined by just looking at the starting key (with a small caveat that will be described below). We will leverage this idea to map ranges to CockroachDB nodes. Now that we know how to map ranges to CockroachDB nodes, we can generate our nodes_partitions view for leaseholder verification. There are two edge cases that we will need to consider when generating the view: Range splits that are made on non-consecutive node IDs. For example, node_id = 1 and 4, resulting in the ranges below: range_id |  start_pretty  |   end_pretty\n+----------+----------------+----------------+\n       800 | /Table/677     | /Table/677/1/1\n       801 | /Table/677/1/1 | /Table/677/1/2 --> Node 1\n       802 | /Table/677/1/2 | /Table/677/1/4\n       803 | /Table/677/1/4 | /Table/677/1/5 --> Node 4\n       804 | /Table/677/1/5 | /Max To avoid this situation, we will need to verify that the size of the range ( end_key - start_key ) is 1. The view could possibly show ranges for partitions that we do not care about. For example, if we apply zone configurations to partitions for nodes 1 and 3, the nodes_partitions view generated using the approach described above will show ranges belonging to partitions for nodes 1, 2, and 3. This can be easily avoided by applying a filter to only project the nodes that we care about when querying the view. And finally, here’s the CREATE VIEW statement for our nodes_partition schema: CREATE VIEW nodes_partitions AS SELECT\n  start_key AS node_id,\n  lease_holder,\n  range_id,\n  replicas\nFROM [SELECT\n    SUBSTR(start_key, 2)::int AS start_key,\n    SUBSTR(end_key, 2)::int AS end_key,\n    lease_holder,\n    range_id,\n    replicas\n  FROM [SELECT\n      crdb_internal.pretty_key(start_key, 2) AS start_key,\n      crdb_internal.pretty_key(end_key, 2) AS end_key,\n      crdb_internal.lease_holder(start_key) AS lease_holder,\n      range_id,\n      replicas\n    FROM crdb_internal.ranges_no_leases\n    WHERE database_name = 'monitoring' AND table_name = 'nodes']\n  WHERE LENGTH(start_key) > 0 AND LENGTH(end_key) > 0]\nWHERE end_key = start_key + 1 In the statement above, we have used a few built-in functions : crdb_internal.pretty_key(raw_key: bytes, skip_fields: int) : To convert the keys in bytes to strings. In our case, we would want skip_fields = 2 to skip <table id> and <index id> . crdb_internal.leaseholder(raw_key: bytes) : To fetch the leaseholder corresponding to raw_key . length(val: string): Calling pretty_key(...) on /Table/<table id> or /Max will yield an empty string, and hence the length(...) call to remove all boundary ranges that are irrelevant. substring(input: string, substr_pos: int) : Used to trim the / prefix from the output of pretty_key(...) . Future Work for SQL Prober Now that we have the basic framework for SQL Prober, we could easily add more tasks to it. One important future work is to make sense of the metrics data that we have collected. For example, building a dashboard and linking metrics to our existing alerting system. There are also a couple of ideas that we might be working on next: There’s quite a bit of bookkeeping that needs to be made for leaseholder verification. One thing that we could potentially explore is to rely on just the constraints when applying zone configurations and drop the leaseholder preferences. For that to work, we will need to change the replica count to 1. However, if we were to do that, the unavailability of a single node might cause some queries to timeout when we have a loss of quorum. See #29638 for more information. I do not know if there are any side effects to this (i.e. if anything is relying on the crdb_internal.ranges schema), so some investigation needs to be made. Only reads are being tested at the moment. We could extend the nodes table to test writes as well. Database-level replication zones can be used so that Data Definition Language (DDL) statements such as CREATE and ALTER can be tested as well. TL;DR Here’s a quick summary of lessons I learned at my internship: with SQL Prober, we can not only measure the impact of incidents when they occur, but also reduce our MTTD since we have an improved monitoring system. Black-box monitoring helps to catch issues that white-box monitoring couldn’t. Building a monitoring system needs to be carefully planned. We should collect metrics for everything and only alert on the important ones. This is not something that can be done in a short period of time as both metrics and alerts need to be refined over time for a good monitoring and alerting system. Geo-partitioning is usually used for multi-region workloads to improve latency. I find it to be an amazing feature, and it is also a key differentiator for CockroachDB. In this blog post, we have explored one unconventional way of using geo-partitioning, which is for black-box monitoring. What else can you do with geo-partitioning? If you have not tried out geo-partitioning yet, I would suggest you to check out the following articles: Geo-Partitioning: What Global Data Actually Looks Like How to Leverage Geo-partitioning Geo-Partitioning Demo Are you interested in monitoring distributed systems or helping build a Database as a Service (DBaaS) on Kubernetes? We’re hiring for SREs and other positions! Check out our careers page to learn more. Footnotes [1] For more details on white-box and black-box monitoring, check out this article which is based on Google's SRE book: Monitoring Distributed Systems . [2] See CockroachDB’s internal virtual database, crdb_internal , specifically the gossip_liveness and gossip_nodes schemas, for more information.", "date": "2019-08-15"},
{"website": "CockroachLabs", "title": "ESCAPE/19: Why We're Building a Multi-Cloud Conference", "author": ["Jim Walker"], "link": "https://www.cockroachlabs.com/blog/escape-19-why-were-building-a-multi-cloud-conference/", "abstract": "We attend and sponsor a lot of opinionated cloud shows each year and AWS re:Invent, Google Cloud Next and Microsoft Ignite have become the centers of discussion for all things cloud. And while each of these shows provides great content and an incredible range of speakers, we noticed that there was a lack of an un-opinionated conversation about multi-cloud or hybrid deployments. Within one of these events, the mention of the term ‘multi-cloud’ is contractually not allowed and sponsor materials are inspected and censored clean of any mentions of the word. And even when the word ‘multi-cloud’ is allowed, the conversation is still limited as it is within the context of the show host. Meanwhile, nearly every industry pundit claims multi-cloud is the future. A multi- or hybrid-cloud architecture leads to better pricing leverage, greater innovation in the space, and more flexibility. As Corey Quinn , cloud economist at the consultancy The Duckbill Group puts it, “The typical multinational has a bunch of different divisions and each has its own cloud. Do you turn them off? Not unless you enjoy pain.” This is where the idea for ESCAPE/19 was born. T here is room for a new show. A show that is an open forum to hold a general conversation about all of the clouds and the unique challenges we face as we analyze and discover how we leverage all of them, especially in conjunction with existing on-premise solutions. The multi-cloud challenge Public cloud, private cloud, multi-cloud, hybrid cloud… they all have some similar challenges. We've seen people struggle to get their strategy “right” across many different vectors. While we naturally gravitate to think about this challenge as a technical problem, there is so much more. Let’s start with the technical. Last week, the imitable Kelsey Hightower tweeted out \"Multi-cloud is like having a Pixel and an iPhone trying to use the same SIM card\". Amen. We couldn't agree more. The cloud platforms are very different and your workloads will struggle to work everywhere. Sure, we can containerize and run a service on Kubernetes, which is great for portability but what about federation of clusters? And then, beyond compute, there are additional issues with everything that surrounds your applications. What about security, networking, and storage. How do you deploy, manage and monitor these things in disparate environments? What about the services around your application? What about the database service, your streaming engine, your enterprise search? Will they work the same on all platforms? How do you monitor them consistently? How can you choose the right service on the right cloud provider for your application?  Will the software you use on one cloud work on the next? Can you span multiple clouds? How will you ever migrate? Do you have the freedom to architect what you want despite the platform on which your apps and services run? It’s more than a technical challenge There’s also a people side of the challenge that complements the technical challenges. How do you hire and train the right people to become experts on all the competing services? For that matter, do you even have a handle on the inventory across your organization of what services you use and don't within the major cloud providers? And what about budgets? Are you getting the best price for compute across all locations and regions in which you operate? Are you able to leverage relationships or pricing structures from one cloud provider against the next, while still maintaining technical alignment? What about privacy and the protection of data? There is an ever-increasing complex web of policy and regulation that requires organizations to get a handle on the data privacy and compliance. How do we actually control where data is stored? ESCAPE/19 These technical and administrative concerns are what ESCAPE/19 is about. It is an open forum to have a conversation about ALL of these challenges outside the opinion of any single public cloud provider. Over the last few weeks, our team along with our content partner, Slower, have already secured some great speakers and our end goal is to deliver the right mix of technology and business leaders across a broad agenda of multi-cloud topics, so we can all start to benefit from a better definition of what it means to be multi-cloud, across all its inherent challenges. Finally, we want to do this where no big cloud show lives: on the East coast. If you’ve been thinking about or solving these problems yourself, we would LOVE to have you join us, either as a speaker or as an attendee. The call for proposals is still open and the early bird discount is still available until September 6!", "date": "2019-08-28"},
{"website": "CockroachLabs", "title": "Join Ordering Part II: The 'SQL'", "author": ["Justin Jaffray"], "link": "https://www.cockroachlabs.com/blog/join-ordering-ii-the-ikkbz-algorithm/", "abstract": "Even in the 80’s, before Facebook knew everything there was to know about us, we as an industry had vast reams of data we needed to be able to answer questions about. To deal with this, data analysts were starting to flex their JOIN muscles in increasingly creative ways. But back in that day and age, we had neither machine learning nor rooms full of underpaid Excel-proficient interns to save us from problems we didn’t understand; we were on our own. We saw in the previous post how to think about a join ordering problem: as an undirected graph with a vertex for each relation being joined, and an edge for any predicate relating two relations. We also saw the connectivity heuristic , which assumed that we wouldn’t miss many good orderings by restricting ourselves to solutions which didn’t perform cross products. It was discussed that in a general setting, finding the optimal solution to a join ordering problem is NP-hard for almost any meaningful cost model. Given the complexity of the general problem, an interesting question is, how much do we have to restrict ourselves to a specific subclass of problems and solutions to get instances which are not NP-hard? The topic of our story is the IKKBZ algorithm, and the heroes are Toshihide Ibaraki and Tiko Kameda. But first, we need to take a detour through a different field. We will eventually find ourselves back in JOIN-land, so don’t fear, this is still a post about databases. Maximize Revenue, Minimize Cost Operations research is a branch of mathematics concerned with the optimization of business decisions and management, and it has led to such developments as asking employees “what would you say you do here?”. One sub-discipline of operations research is concerned with the somewhat abstract problem of scheduling the order in which jobs in a factory should be performed. For example, if we have some set of tasks that needs to be performed to produce a product, what order will minimize the business’s costs? The 1979 paper Sequencing with Series-Parallel Precedence Constraints by Clyde Monma and Jeffrey Sidney is interested in a particular type of this problem centered around testing a product for deficiencies. They have a sequence of tests to be performed on the product, some of which must be performed before others. The paper gives the example of recording the payments from a customer before sending out their next bill, in order to prevent double billing - the first task must be performed before the second. The kind of problem Monma and Sidney cover is one where these tests (henceforth “jobs”) and the order they must be performed in can be laid out in a series of parallel “chains”: Where a legal order of the jobs is one in which a job’s parent is carried out before it is, so X, A, B, U, C, V, D, Y, Z is a legal execution here, but A, C, \\ldots is not, since B must occur before C . Such restrictions are called sequencing constraints . This is a specific case of the more general concept of a precedence graph . A precedence graph can just be any directed acyclic graph, so in the general (non-Monma/Sidney) case, such a graph could look like this: In the paper’s setting, each “job” is a test of a component to see if it’s defective or not. If the test fails, the whole process is aborted. If it doesn’t, the process continues. Each test has some fixed probability of failing. Intuitively, the tests we do first should have high probability of detecting a failure, and a low cost: if a test is cheap and has a 99% chance of determining that a component is defective, it would be bad if we did it at the very end of a long, expensive, time-consuming process. Next, a job module in a precedence graph is a set of jobs which all have the same relationship to every job not in the module. That is, a set of jobs J is a job module if every job not in J either must come before every job in J , must come after every job in J , or is unrestricted with respect to every job in J according to the problem’s precedence constraints. For instance, in this example, \\{B,C\\} , \\{X, Y, Z\\} , and \\{V\\} are job modules, but \\{X, Z\\} is not a module because Y must come before Z , but is after X . Similarly, \\{A, U\\} is not a module because V must come after U but is unrestricted relative to A . When we use j to represent an object, we’re talking about one single job, and when we use \\vec s and \\vec t , we’re talking about strings of jobs, as in, \\vec s = j_1 j_2 \\ldots j_n , which cannot be further split apart, and are performed in sequence. In general, these are somewhat interchangeable. In this setting, jobs are just strings of length one. If two strings are in a module together, we can concatenate them in some order to get a new string. Since the constituent jobs and strings are in a module (and thus have all the same constraints), there’s only one natural choice of constraints for this new string to have. It’s generally easier to reason about a problem with fewer jobs in it, so a desirable thing to be able to do is to concatenate two jobs to get a new, simpler problem that is equivalent to the original one. One of the key challenges we’ll face is how to do this. We use f(\\vec s_1, \\vec s_2, \\ldots) to refer to the cost of performing the string \\vec s_1 , followed by the string \\vec s_2 , and so on (think of this function as “flattening” of the sequences of jobs). q(\\vec s) is the probability of all of the tests in \\vec s succeeding. We assume all the probabilities are independent, and so the probability of all of the jobs in a string succeeding is the product of the probability of success of each individual job. If the sequencing constraints constrain a string \\vec s to come before a string \\vec t , we write \\vec s \\rightarrow \\vec t (read “ \\vec s comes before \\vec t ”). We’re going to use strings to progressively simplify our problem by concatenating jobs into strings. The expected cost of performing one sequence of jobs followed by another sequence of jobs is the cost of performing the first sequence plus the cost of performing the second sequence, but we only have to actually perform the second sequence if the first sequence didn’t fail. Thus, in expectation, the cost is f(\\vec s, \\vec t) = f(\\vec s) + q(\\vec s)f(\\vec t) . Since this is recursive, we need a base case, and of course, the cost of performing a single job is simply the cost of the job itself: f(j) = c_j . An important property of this definition is that f is well-defined: f(S_1, S_2) = f(S_1', S_2') whenever S_1S_2 = S_1'S_2' (verifying this is left as an exercise). An adjacent sequence interchange is a swap of two adjacent sequences within a larger sequence: This is an adjacent sequence interchange of X,Y,Z with U,V . A rank function maps a sequence of jobs j_1j_2\\ldots j_n to a number. Roughly, a rank function captures how desirable it is to do a given sequence of jobs early. r(\\vec s) < r(\\vec t) means we would like to do \\vec s before \\vec t . A cost function is called adjacent sequence interchange (ASI) relative to a rank function r if the rank function accurately tells us when we want to perform interchanges: f(a, \\vec s, \\vec t, b) \\le f(a, \\vec t, \\vec s, b) \\text{ if } r(\\vec s) \\le r(\\vec t) And a cost function f is ASI if there exists a rank function it is ASI relative to. I’m sure you’ll be shocked to learn the f we derived above ( f(\\vec s, \\vec t) = f(\\vec s) + q(\\vec s)f(\\vec t) ) is ASI. Note that even though we may have a rank function which tells us we would like to do a given sequence of jobs early, the sequencing constraints might prohibit us from doing so. The ASI theorem (Monma and Sidney call this “Theorem 2”, but we’re going to use it enough for a real name) mildly paraphrased: Let f be ASI with rank function r , and let \\vec s and \\vec t be strings. Consider a job module \\{\\vec s, \\vec t\\} in a general precedence graph, where \\vec s \\rightarrow \\vec t and r(\\vec t) \\le r(\\vec s) . Then there is an optimal permutation with \\vec s immediately preceding \\vec t . Let’s break down this theorem. First of all, note that it applies to a general precedence graph. This means we’re not restricting ourselves to the “parallel chains” kind of graph described above. \\vec s \\rightarrow \\vec t means \\vec s must come before \\vec t ( \\vec s and \\vec t are already strings in the current iteration of our problem, and they inherit the constraints of the jobs they are composed of). However, r(\\vec t) \\le r(\\vec s) means we would like to put \\vec t before \\vec s . What this theorem is saying is that in this scenario, we’re not forgoing optimality by ordering \\vec s immediately before \\vec t . This is very useful, because it allows us to take two strings, \\vec s and \\vec t , and replace them with the new string \\vec s \\vec t which is their concatenation. Importantly, this is saying that there is an optimal permutation with \\vec s immediately preceding \\vec t , not that in any every optimal permutation this is the case. The proof of this theorem is pretty simple so I’ve included it. Proof of the ASI theorem : Every optimal permutation looks like \\langle u, \\vec s, v, \\vec t, w \\rangle since \\vec s \\rightarrow \\vec t . If v is empty, we’re done. otherwise, if r(v) \\le r(\\vec s) , then by ASI we can swap \\vec s and v without dropping the cost to get \\langle u, v, \\vec s, \\vec t, w \\rangle . If r(\\vec s) < r(v) , then by transitivity r(\\vec t) < r(v) and again by ASI we can swap v and \\vec t without dropping the cost to get \\langle u, \\vec s, \\vec t, v, w \\rangle . These swaps must be legal since \\{\\vec s, \\vec t\\} is a job module. \\square Finally, our friends Monma and Sidney describe the parallel chains algorithm . It goes like this: If, for every pair of strings \\vec s and \\vec t , \\vec s \\rightarrow \\vec t implies r(\\vec s) < r(\\vec t) (meaning that the constraints and the rank agree about the correct order for the two), go on to step 2. If the two relations disagree somewhere, then we can find a job module \\{\\vec s, \\vec t\\} for which they disagree [1]. Then, by the ASI theorem, we can concatenate \\vec s and \\vec t into the sequence \\vec s \\vec t (since we know there’s some optimal solution where they’re adjacent), which we then treat as a new string. Now repeat step 1. Our problem has exactly one less string in it now, so this process can’t go on forever. Sort the remaining strings by their value under r . Since step 1 terminated, the resulting order is legal, and since f is ASI, it’s optimal. The core idea of the algorithm is that we’d like to just blindly order all the jobs by their value under the rank function, but situations where the rank function and the precedence constraints disagree prohibit that. The ASI theorem gives us a tool to eliminate precisely these scenarios while keeping access to optimal solutions, so we repeatedly apply it until we are free to simply sort. Back to Databases Back to the topic of Join Ordering. For their paper “On the Optimal Nesting Order for Computing N-Relational Joins,” Toshihide Ibaraki and Tiko Kameda were interested in instances of the join ordering problem for which solutions were restricted to those that were left-deep and contained no cross products the only join algorithm used was the nested-loop join, and the query graph was a tree. As we saw before , a plan is left-deep if the right input to every join operator is a concrete relation, and not another join operation. This gives a query plan that looks something like this, with every right child being a “base relation” and not a join: and not like this, where a join’s right child can be another join: Since the shape of the plan is fixed, we can talk about the plan itself simply as a sequence of the relations to be joined, from left to right in the left-deep tree. We can take a query plan like this: and represent it like this: When we say that the query graph must be a “tree”, we mean in a graph theoretical sense—it’s a graph that is connected and has no cycles. If you primarily identify as a computer science person and not a mathematician, when you hear “tree” you might picture what graph theorists call a “rooted” tree. That is, a tree with a designated “special” root vertex. It’s like you grabbed the tree by that one designated vertex and let everything else hang down. Here’s a tree: Here it is rooted at A : Here it is rooted at D : This distinction is important for our purposes because it’s easy to get a rooted tree from an unrooted tree: you just pick your favourite vertex and call it the root. Query graphs and query plans are fundamentally different. The query graph describes the problem and a query plan the solution . Despite this, they’re related in very important ways. In a left-deep query plan, the relation in the bottom-left position is quite special: it’s the only one which is the left input to a join. If we are obeying the connectivity heuristic (no cross products), this gives us a way to think about how exactly it is we are restricted. In the plan above, A and B must share a predicate, or else they would cross-product with each other. Then C must share a predicate with at least one of A or B . Then D must share a predicate with at least one of A , B , or C , and so on. Let’s assume for a second that given a query graph, we know which relation will wind up in the bottom-left position of an optimal plan. If we take this special relation and make it the root in our query graph, something very useful happens. Given our rooted query graph, let’s figure out what a plan containing no cross products looks like. Here’s an example of such a rooted query graph: Since we must lead with A , which vertices can we then add to get a legal ordering? Well, the only three that don’t form a cross product with A are B , C , and D . Once we output, C , it becomes legal to output F , and so on. As we go down the tree, we see a necessary and sufficient condition for not introducing cross products is that the parent of a given relation must have already been output. This is effectively the same as the precedence constraints of the job-scheduling problem! Of course, beforehand, we don’t know which relation will be the optimal one to place in the bottom left. This is going to turn out to be only a minor problem: we can just try each possible relation and choose the one that gives us the best result. So, from here on, we’ll think of our query graph as a rooted tree, and the root will be the very first relation in our join sequence. Despite our original graph having undirected edges, this choice allows us to designate a canonical direction for our edges: we say they point away from the root. A Tale of Two Disciplines I can only imagine that what happened next went something like this: Kameda sips his coffee as he ponders the problem of join ordering. He and Ibaraki have been collaborating on this one for quite a while, but it’s a tough nut to crack. The data querying needs of industry grow ever greater, and he has to deliver to them a solution. He decides to take a stroll of the grounds of Simon Fraser University to clear his mind. As he locks his office, his attention is drawn by the shuffling of paper. A postdoc from the operations research department is hurrying down the hall, an armful of papers from the past few years tucked under her arm. Unnoticed, one neatly stapled bundle of papers drops to the floor from the postdoc’s arm. “Oh, excuse me! You dropped—” Kameda’s eyes are drawn to the title of the paper, Sequencing With Series-Parallel Precedence Constraints, as he bends down to pick it up. “Oh, thank you! I never would have noticed,” the postdoc says, as she turns around and reaches out a hand to take back the paper. But Kameda can no longer hear her. He’s entranced. He’s furiously flipping through the paper. This is it. This is the solution to his problem. The postdoc stares with wide eyes at his fervor. “I…I’ll return this to you!” He stammers, as he rushes back to his office to contact his longtime collaborator Ibaraki. If this isn’t how it happened, I’m not sure how Ibaraki and Kameda ever made the connection they did, drawing from research about a problem in an entirely different field from theirs. As we’ll see, we can twist the join ordering problem we’re faced with to look very similar to a Monma-and-Sidney job scheduling problem. Recall that the selectivity of a predicate (edge) is the amount that it filters the join of the two relations (edges) it connects (basically, how much smaller its result is than the raw cross product): sel(p) = \\dfrac{|A \\bowtie_p B|}{|A \\times B|} Designating a root allows us another convenience: before now, when selectivity was a property of a predicate, we had to specify a pair of relations in order to say what their selectivity was. Now that we’ve directed our edges, we can define a selectivity for a given relation (vertex) as the selectivity of it with its parent, with the selectivity of the root being defined as 1. So in our graph above, we define the selectivity of H to be the selectivity of the edge between F and H . We refer to the selectivity of R by f_R . This definition is even more convenient than it first appears; we’re restricted by the precedence constraints already to include the parent of a relation before the relation itself in any legal sequence, thus, by the time a relation appears in the sequence, the predicate from which it derives its selectivity will apply. This allows us to define a very simple function which captures the expected number of rows in a sequence of joins. Let N_i = |R_i| , then for some sequence of relations to be joined S , let T(S) be the number of rows in the result of evaluating S . Then: T(\\Lambda) = 1 for the empty sequence \\Lambda , since the “identity” relation is that with no columns and a single row, and T(S) = f_{R_{i1}}N_{i1}f_{R{i2}}N_{i2} \\ldots f_{R_{ik}}N_{ik} for S = R_{i1}R_{i2}\\ldots R_{ik} T(S) is simply the product of all the selectivities and the sizes of the relations contained within it. This follows directly from the definition of selectivity from the first post and the precedence constraints. Further, to evaluate how good a query plan is, we need to have a cost model that tells us how expensive it is to execute. Intuitively, the cost of executing a sequence of joins is the cost of executing a prefix of the joins, plus the cost of executing the rest of the joins by themselves, times the number of rows in the prefix (since we’re doing a nested-loop join). This gives us this recursive definition of the cost function for a sequence: C(\\Lambda) = 0 for the empty sequence \\Lambda , C(R_i) = f_{R_i}N_i for a single relation R_i , C(S_1S_2) = C(S_1) + T(S_1)C(S_2) for sequences of relations S_1 and S_2 . Again, things are looking very similar to the world of Monma and Sidney. We can even define a rank function for this setting to be r(S) = \\dfrac{T(S)-1}{C(S)} . Recall the property we want of a rank function in order for f to be ASI. We want it to be the case that: C(AS_2S_1B) \\le C(AS_1S_2B) \\text{ if } r(S_2) \\le r(S_1) Through some opaque algebra, we can see this is true (don’t worry too much about following this, you can just trust me that it shows that f is ASI): C(AS_1S_2B) = C(AS_1) + T(AS_1)C(S_2B) =C(A)+T(A)C(S_1) + T(A)T(S_1)[C(S_2) + T(S_2)C(B)] . Thus, C(AS_1S_2B) - C(AS_2S_1B) = T(A)[C(S_2)(T(S_1) - 1) - C(S_1)(T(S_2) - 1)] = T(A)C(S_1)C(S_2)[r(S_1) - r(S_2)] . And so C(AS_1S_2B) - C(AS_2S_1B) is negative precisely when r(S_1) - r(S_2) is. \\square (As an aside, I don’t have a good intuitive interpretation of what this particular rank function represents—this is one of those cases in math where I note that the algebra works out on paper and then promptly start treating r(S) as a black box.) We’re nearly there. However, we can’t apply the parallel chains algorithm, because our query graph might not be a parallel chain graph (it’s a rooted tree). How we’re going to approach this is by iteratively applying “safe” transformations to the query graph until it becomes a single chain. We do this by looking for wedges . A wedge is two or more chains joined at a vertex: W here is a wedge, and C is the “root” of W . Note that wedges are precisely the structure that make our graph not a chain, so if we can eliminate one, we can bring ourselves closer to a chain. Ibaraki and Kameda first define a process they call \\text{NORMALIZE}(S) (…it was the 80s. That was how algorithm names were written), where S is one chain of a wedge: While there is a pair of adjacent nodes S_1 and S_2 with S_1 followed by S_2 in S , such that r(S_1) > r(S_2) : find the first such pair (starting from the beginning of S ) and replace S_1 and S_2 by a new node S_1S_2 , representing a subchain S_1 followed by S_2 . Here’s an example of applying NORMALIZE : In this chain, B and C are such a pair of misordered nodes. Applying NORMALIZE , we get a new node, BC , with a new rank: This is the same as the trick pulled in the parallel chains algorithm: the ASI theorem guarantees that by performing this merging operation we don’t lose access to any optimal solutions. Once the chains in W are completely normalized (meaning they’re ordered by their value under the rank function) we can merge them into a single chain which maintains that ordering. Any interleaving of them is legal, since they’re unrelated by any precedence constraints. We then continue this process of eliminating wedges until there are none left, and we are left with the optimal ordering. The process of eliminating all the wedges in a tree can be done in time O(n \\log n) , but there’s a catch! Since we rooted our tree by picking an arbitrary vertex before, this solution is only guaranteed to be optimal for that particular choice of vertex . The fix is easy: we perform the algorithm for every choice of starting vertex and take the best one. Running the algorithm for a single choice of root takes O(n\\log n) to run, and so performing it for every choice of root takes O(n^2\\log n) . This is the algorithm and runtime that Ibaraki and Kameda presented in their paper. My queries are still taking too \\log n ! There’s one final piece to fall into place to get the final IKKBZ algorithm. The IK in IKKBZ indeed stands for Ibaraki and Kameda. At VLDB 1986, Ravi Krishnamurthy, Haran Boral, and Carlo Zaniolo presented their paper Optimization of Nonrecursive Queries . Check out this glorious 1986-era technical diagram from their paper: Their trick is that if we pick A as our root and find the optimal solution from it, and then for our next choice of root we pick a vertex B that is adjacent to A , a lot of the work we’ll have to do to solve from B will be the same as the work done from A . Consider a tree rooted at A : (Where X is all the nodes that live beneath A and Y is all the nodes that live beneath B ) and say we have the optimal solution S for the A tree. We then we re-root to B : KB&Z recognized that the X and Y subtrees in these two cases are the same, and thus when we eliminate all the wedges beneath A and B , the resulting chains will be in the same order they were in for the tree rooted at A , and this order is the same as the order implied by S . So we can turn the X and Y subtrees into chains by just sorting them by the order they appear in S . This means that we can construct the final chains beneath A and B without a O(n \\log n) walk, using only O(n) time to merge the two chains, so in aggregate, we do this for every choice of root, and arrive at a final runtime of O(n^2) . OK, let’s take a step back. Who cares? What’s the point of this algorithm when the case it handles is so specific? The set of preconditions for us to be able to use IKKBZ to get an optimal solution was a mile long. Many query graphs are not trees, and many queries require a bushy join tree for the optimal solution. What if we didn’t rely on IKKBZ for an optimal solution? Some more general heuristic algorithms for finding good join orders require an initial solution as a “jumping-off point.” When using one of these algorithms, IKKBZ can be used to find a decent initial solution. While this algorithm only has guarantees when the query graph is a tree, we can turn any connected graph into a tree by selectively deleting edges from it (for instance, taking the min-cost spanning tree is a reasonable heuristic here). The IKKBZ algorithm is one of the oldest join ordering algorithms in the literature, and it remains one of the few polynomial-time algorithms that has strong guarantees. It manages to accomplish this by being extremely restricted in the set of problems it can handle. Despite these restrictions, it can still be useful as a stepping stone in a larger algorithm. Adaptive Optimization of Very Large Join Queries has an example of an algorithm that makes use of an IKKBZ ordering as a jumping off point, with good empirical results. CockroachDB doesn’t use IKKBZ in its query planning today, but it’s something we’re interested in looking into in the future. Appendix Sketch of a proof of [1]: Since the two are related by the precedence constraints, they’re in the same chain. If they’re not adjacent in that chain, there’s some x in between them, so \\vec s \\rightarrow \\vec x \\rightarrow \\vec t . If r(\\vec x) < r(\\vec t) , then \\{\\vec x, \\vec t\\} is a “closer” bad pair. If r(\\vec x) \\ge r(\\vec t) then \\{\\vec x, \\vec s\\} is a “closer” bad pair. Then by induction there’s an adjacent pair.", "date": "2019-09-05"},
{"website": "CockroachLabs", "title": "From Intern to Full-Time Engineer at Cockroach Labs", "author": ["Bilal Akhtar"], "link": "https://www.cockroachlabs.com/blog/from-intern-to-full-time-engineer-at-cockroach-labs/", "abstract": "Throughout the year, we offer internships at Cockroach Labs to give students opportunities to gain industry experience and work on challenging problems within distributed systems. Bilal Akhtar is a Member of Technical Staff at Cockroach Labs, working on the Core Storage team and toying with KV storage engines. Outside of work, you’ll see him reading non-fiction, or giving urban photography a shot. I interned at Cockroach Labs twice while in school. I’ve now come back as a full-time engineer working on the Core Storage team. We’ve published blog posts about the work interns have done previously, as well as how we set our internship apart by offering equity . In this post, I’ll expand on what stands out about internships at Cockroach Labs. When finding and applying for internships, there are a lot of factors to consider: scope of impact, quality of the learning experience, company size, and location, just to name a few. I come from the University of Waterloo, a school known for its co-op program that embeds up to 6 internships as a degree requirement. Since it’s seemingly impossible to avoid discussing internships on campus, it’s easy to fall for pressure and aim almost exclusively for tech giants like Facebook or Google. There’s valuable experience to be gained working at companies like those, but I also found it important to do at least one if not more internships at smaller companies. At smaller, close-knit, yet fast-growing companies like Cockroach Labs, I got to work with things that were more critical to the product’s success than, say, something that’s purely exploratory or a side-project. I also got more choice over what project or team I wanted to work on - just because there was never a shortage of projects in the pipeline. First internship: SQL Execution team When I first interned at Cockroach Labs in 2017, the company was about a third of the size that it is now. It was the pre-1.0 days when new functionality was being added left and right, and my work was no exception. I got to work on adding a couple of new statements to the product to allow for query/session monitoring and cancellation. Here’s an example of one of those statements, `SHOW QUERIES`, in action: SELECT node_id, user_name, query FROM [SHOW QUERIES] ---- node_id user_name query 1 root SELECT node_id, user_name, query FROM [SHOW CLUSTER QUERIES] Finding help and mentorship inside the company back then was not as structured or straightforward as it is today. Either way, I was able to get a lot of help without even asking - the team was small but very collaborative. It was my first time working on something this performance sensitive with a lot of moving parts, and this form of learning is hard to find elsewhere. Especially as a third year student. And since the company was so small, I got a lot of visibility into what other parts of the company were doing, and how my work impacted them. This sort of insight is not a necessity, but it really puts everything in perspective. It’s also very satisfying to hear customers talk about (and praise!) a feature that you built. Second internship: SQL Optimizer team I decided to return to CockroachDB a year later. The company was changing quickly, the user base had grown a lot, and the product had reached some impressive performance / feature milestones in the 2.0 release. I figured the only way to accurately gauge the company at that point would be to work there again, and I’m really glad I did. Onboarding processes had become much more comprehensive. Interns were now assigned “Roachmates” who were expected to be a point-of-contact for technical questions throughout the internship. I opted to join the SQL Optimizer team. A cost-based optimizer had been recently added to CockroachDB There was a lot of interesting work to be done in the optimizer; and work there was more math and statistics-driven than elsewhere, which I found to be an interesting change. Work I did that term included added null counts as an additional statistical signal to the optimizer. Many queries are implicitly null-eliminating, and so the optimizer can make better predictions if it knows the proportion of values that are null for a given column/table. I also implemented the optimizer pieces of a new type of join called the zigzag join. It’s somewhat similar to a merge join, except it is able to take advantage of multiple indexes to efficiently find the intersection of multiple WHERE predicates. The execution pieces for this join had already been implemented by another intern; I just had to investigate cases where this join was efficient, and plan it there. Seeing a significant percentage improvement in the runtime of some benchmarks after a couple weeks of work is really satisfying, and is something that happens relatively often at Cockroach Labs, even with intern projects. Here’s an example query, and a zigzag join plan associated with it: EXPLAIN SELECT a,b,c FROM zigzag WHERE b = 5 AND c = 6.0 ---- tzigzag-join  · · │ type inner │ pred (@2 = 5) AND (@3 = 6.0) ├── scan · · │ table zigzag@b_idx │ fixedvals  1 column └── scan · · table zigzag@c_idx fixedvals  1 column In every internship, there are roadblocks, slow-moving parts, and technical concepts that are simply hard to understand. What I really appreciated in both of my internships was that the whole company understood the value of quality work, even if it took longer. I didn’t feel pressured to rush something through just to get it over the finish line within my internship; the project would just get re-scoped if that’s more appropriate. Returning for full time Cockroach Labs wasn’t the only company I had interned at. But not a lot of workplaces combine impactful, specialized work being given to new grads and interns, have a technically impressive product that you believe in, and live up to values around work/life balance and quality of work. All these factors played a major role when I decided to return for full time. Upon returning, I joined the Core Storage team. Core as a whole works on the transactional key-value store below the SQL layer of the database, and anything connected to it, such as RocksDB. Core layers and concepts like distributed transactions are what make Cockroach unique from other SQL databases, and work there is more distributed systems heavy than in other teams. Given my internships were all in SQL land, this was quite a transition and a new challenge. I always wanted to learn more about the core layers, so what better way to solidify my understanding than by working on it? I was also encouraged by peers and past managers to make this jump. And given how transparent the company is internally, I knew what I was getting myself into. I’m a month in at the time of writing this blog post, and there have been no unpleasant surprises - only good ones. If distributed SQL databases are your cup of tea, we're hiring !", "date": "2019-09-13"},
{"website": "CockroachLabs", "title": "How a global e-lock manufacturer modernized their IAM system with Managed CockroachDB", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/identity-access-management-case-study/", "abstract": "A global electronic lock company wanted to upgrade their identity access management (IAM) system to achieve global scale with no manual sharding. They wanted to move away from sharding and evolve the app’s architecture from monolithic to microservices. With no in-house site reliability team, they started evaluating database-as-a-service (DBaaS) products. Their requirements included: Data domiciling: The manufacturer has customers across Europe, Asia, and North America, and they needed to ensure European customer data remained in Europe in order to remain compliant with the General Data Protection Regulation (GDPR). If possible, they wanted to pin row-level data to individual countries. Global scale without manual sharding: The team wanted to scale globally to reach its widespread customer base, but they needed low-latency reads for their time-sensitive use cases. Resiliency: In the event of a regional failure, they needed their database to remain available so customers could access important facilities. Data log: log of their transactional updates into their Elasticsearch, Logstash, and Kibana stack as an event bus for microservices. The team landed on using Managed CockroachDB, which is able to hit all four of the requirements through a managed , geo-partitioned global database with change data capture capabilities . Their new global setup has three regions, which not only improves latency for customers living in each region, but gives them the ability to domicile European data for GDPR compliance. Managed CockroachDB is now a critical part of the application’s infrastructure, and the manufacturer’s small engineering team has more free time to handle development. Want more detail? Read the full case study here.", "date": "2019-10-01"},
{"website": "CockroachLabs", "title": "“Agnostic & Awesome” -- 5 Multi-Cloud Talks You Don’t Want to Miss at ESCAPE/19", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/agnostic-awesome-4-multi-cloud-talks-you-dont-want-to-miss-at-escape-19/", "abstract": "Would you believe us if we told you the word “multi-cloud” was banned from one of the top 3 Big Cloud shows? We couldn’t believe it either. So we wanted to create a wholly different type of show where this concept would actually take main stage… and we have. ESCAPE/19 is a conference where next generation infrastructure will be explored at depth across both technical and business challenges we will all face in our multi-cloud futures. Where else could you hear about the ethics of the cloud or get a pragmatic viewpoint on where we stand as an industry on the technology required to make multi-cloud happen? Here are five talks we cannot wait to get on stage. If you want in on the conversation, use promo code WEB for 40% your ticket . 1. The Ethical Cloud Day 2 // Oct. 17, 2019 // 9:20am - 9:40am Rob Reid // Principal Engineer at Lush As the world's largest cloud service providers clamber to stay ahead of our unquenchable demand for Internet, we as consumers need to take a step back and appreciate the environmental and human costs of this unprecedented growth, along with the less obvious costs to our civil liberties. 2. Your Multi-Cloud Visibility and Security Should Be Agnostic and Awesome… Day 2 // Oct. 17, 2019 // 3:20pm - 4:00pm Dan Papandrea // Field CTO at Sysdig HELP!! How do I understand what services are where on a multi-cloud deployment? How do I secure my applications across clouds based on the context I'm being provided in Kubernetes or my Cloud Provider (AWS, GKE, or Azure)? Dan Papandrea will walk through real world examples of deploying clusters and applications across clouds with realtime golden signal visibility, alerting, and lifecycle security for building, running, and post mortem security on mission critical applications across your clouds. 3. [panel] Going Multi-Cloud For Realz: End User Stories From the Real World! Day 1 // Oct. 16, 2019 // 2:00pm - 2:40pm Moderator: Lisa-Marie Namphy // Dev Advocate & Community Architect at Portworx Panelists: Allan Naim // Google; Joseph Sandoval // Adobe; Eric Han // Portworx Multi-cloud is a long-term strategy for how enterprises source infrastructure that also informs a practical approach for running across disjoint, hybrid environments. Being able to move entire applications into any infrastructure requires both a mind shift in operations and new set of tooling. It can be hard for teams to decide how to approach multi-cloud workflows, and operationalize, and push their infrastructure providers as organizations move to a multi-cloud world. This panel brings together end user practitioners and infrastructure providers who have passionately built this multi-cloud world and will offer their perspective on: How multi-cloud changes the development process What gains can teams expect in the early days of multi-cloud (and how to achieve them) What kinds of workloads are being moved What are the best successes and worst horror stories 4. Herding Cats - A CISO’s Perspective on Securing Multi-Cloud Infrastructure Day 1 // Oct. 16, 2019 // 4:20pm - 5:00pm Fritz Wetschnig // CISO & VP of Enterprise IT at Flex Several years ago most public cloud conversations were between enterprise leadership and architects with security being an afterthought. Today, security is very much part of the conversation however it’s become a game of catchup and things have gotten far more complicated. This session intends to get into the major issues facing security teams as they must retrofit and reimagine securing users and data from a multitude of SaaS, IaaS and legacy applications. 5. The Evolution of Cloud Abstractions Day 1 // Oct. 16, 2019 // 5:00pm - 5:40pm Cornelia Davis, VP of Technology at Pivotal Openstack was arguably the first attempt at standardizing an API for the cloud, and while initially showing signs of promise, the fact is that today utilization of the various cloud offerings almost ubiquitously occurs through proprietary APIs. But this makes leveraging multiple clouds very difficult, requiring unique tooling, scripts and even practices for each of the environments. And while Kubernetes is emerging as a standardized “dial-tone” that can be used to deploy and manage workloads across various clouds, how the infrastructure is abstracted FOR Kubernetes remains a work in progress. In this session we will have a look at a number of different approaches to infrastructure abstraction including Openstack, BOSH and cluster-api, highlighting the core tenets, and studying the pros and cons of each. Check out the full agenda here , and remember to use promo code WEB at checkout for 40% off your ticket .", "date": "2019-10-04"},
{"website": "CockroachLabs", "title": "Creating a Fair Hiring Process", "author": ["Dave Delaney"], "link": "https://www.cockroachlabs.com/blog/creating-a-fair-hiring-process/", "abstract": "David Delaney is a Recruiter at Cockroach Labs. During office hours you'll find Dave interviewing and reaching out to engineers or thinking about how to improve the candidate experience. Outside of work most of his time is dedicated to his wife Rachel and son Ezra, but he does find time on Sundays to run around a bit in his old guys soccer league. -- Startups tend to be a breeding ground of ideas that push the envelope.  The lack of red tape and bureaucracy paired with limited resources creates a perfect environment for innovation, experimentation, and a bit of scrappiness. In my experience, teams like Engineering, Design, and Marketing have excelled at this while People Operations tend to be stuck in the past.  Sure, a new sourcing tool or Applicant Tracking System (ATS) might stir the pot a bit, but hiring and interviewing look very similar today compared to ten years ago. And ten years in the startup world might as well be 100. The fact of the matter is that the traditional hiring process is fundamentally broken . Companies often complain about how hard it is to hire great people, to have diversity on the team, and to get candidates engaged throughout the process.  Yet, those same companies refuse to adjust their approach, simply throwing their hands up wondering why nothing gets better. The cynic would say it’s lip service - they know, for example, that diversity is something they are supposed to care about but they aren’t willing to put the effort in.  But as I talk to more and more Talent Acquisition professionals, it has become clear that part of the problem is far less nefarious... companies just don’t know where to start. When I spoke to our Chief People Officer, Lindsay Grenawalt, before deciding to join CRL, I was thrilled to hear her describe the founders’ willingness to approach things in a different way.  To be fair, they did start a company called Cockroach Labs, so I shouldn’t have been completely shocked. Hiring a Head of People Operations as one of the first employees at the company was proof that they were (and still are) dedicated to being trailblazers with not only our product, but also with our culture and hiring practices. So, let’s take a look at how we’re attempting to build the most talented, diverse, engaged team possible. I’ll be touching on three key practices that we have implemented: Removing resumes throughout the interview process Exercise-based interviews Open-sourced interviews Removing Resumes Of the three areas I’ll discuss, this is the most controversial. Traditional hiring practices are so ingrained in a lot of people that the idea of not having a candidate's resume is just a bit far-fetched. In fact, I’ve had people laugh when I bring it up because they are sure I’m joking.  But, we have found that once interviewers and candidates alike get past that initial shock and actually see the experience it in action, it’s generally well-received. For interviewers, they can focus entirely on the question(s) posed to the candidate, judging them purely on performance as opposed to having it skewed by details like the candidate’s current company or academic background. Similarly, candidates can be confident that they are being assessed on their abilities rather than a sheet of paper. One thing worth noting is that recruiters still have resumes for the introductory phone screen.  There are two reasons that I bring this up. First, to acknowledge that there’s still going to be some level of bias that occurs at the top of the funnel.  A recruiter is a human and humans have bias; so, it’s better to acknowledge it than pretend it doesn’t exist. And second, this process requires a massive amount of trust and respect between recruiting and hiring teams. They are relying on us to put qualified people into the pipeline, and we need to be true experts on each of our roles to be successful.  No one said this was going to be easy! Exercise-Based Interviewing A lot of people I talk to have some horror story about interviewing at a company, but even the “good” experiences tend to go a similar route.  You walk into a series of interviews where you talk to a handful of people who all ask some variation of the same questions, and you spend a few hours repeating yourself.  If the questions the interviewers choose happen to touch on areas you’re familiar with, great you’re a fit! If they happen to focus on one particular area you don’t have much experience in, you get to spend the day wondering when the pain will end and second-guessing why you ever thought this was a good idea. “What’s your biggest strength?” “What’s your biggest weakness?” “Where do you see yourself in 5 years?” These questions aren’t completely unreasonable, but I do cringe every time I hear them. They really don’t give any actual insight into the candidate's ability.  Even asking candidates about their biggest accomplishment falls short - it can give them an opportunity to talk about what they’ve done, but it doesn’t always translate into their ability to actually do the work.  And at the end of the day, while communication skills are important, they aren’t the only skills we want to assess. To address this, we get creative as a team and create exercises for each stage of the interview process.  This could be a take-home project, a presentation, a case study, or anything that allows the candidate to show what they are capable of producing.  For example, my onsite interview at CRL for a recruiting role included interviews such as “Pipeline Development / Sourcing Strategy” and “Extending an Offer”.  The daily responsibilities of a recruiter were turned into a role-playing, output-focused interview where I was able to showcase my experience in these areas as well as give the team a glimpse into what it would be like to partner with me.  I was able to highlight my strengths, and I left feeling as though the process was fair and a good use of everyone’s time. Though the interviews were challenging and nerve-wracking (extending an offer to the CEO / founder of the company did make me sweat), the process was far more engaging and enjoyable than any interview I’ve done in recent memory.  Now that I’m a part of the team, the interviewers have shared with me that exercise-based interviews give them a better understanding of a candidate’s ability to do the job and bonus they no longer think of conducting interviews as a burden! Open-Sourced Process We are an open-source software company, so transparency is in our DNA.  Before I became familiar with how the People Ops team operated, I assumed this ended with our product.  I couldn’t have been more wrong. The open-source concept is something we extend to other aspects of how we operate on a day to day basis.  For example, team meeting notes are shared with the entire company so all of us are aware of what other teams are working on and can ask questions or provide feedback. On the Talent Acquisition team, we have created an open-sourced interview process where we give candidates insight into what to expect throughout each stage of the process.  Think of it like a teacher giving students an outline of what will be on their exam. We’re not looking to catch candidates off guard, instead we are much more interested in how they apply their knowledge and skills. To keep the school metaphor going: we want employees who can apply, not memorize. There is something to be said about a candidate’s ability to think through a problem that they are seeing for the first time. We are a startup after all, so not every day will be well-defined or consistent.  However, our belief is that this should only be a portion of what we assess during the interview process as opposed to being the majority. We approach this by allowing for “curveball” questions. These are extensions of the interview that we don’t explicitly define in our open source documentation — we let candidates know these are coming so they don’t feel misled.  In addition to seeing how candidates work under pressure, it can also be used as an opportunity to dig in further with strong candidates who make it through the first part of the interview quickly. These curveball questions can lead to helpful insight when attempting to separate a candidate who meets the role’s expectations from one who excels.  It is important to remember that this is only a small portion of the interview and doesn’t negate performance from the rest of the question. Conclusion I said it before and I’ll say it again: removing resumes is hard work! It requires having leadership who believe in the value of a fair, thorough hiring process. It requires a recruiting team to be willing to explain the process and update it regularly for every single interview.  It requires interviewers to be open-minded and willing to step outside of their comfort zone in an attempt to make better hires. At the end of the day, our experience has been that this experiment has been (and still is) a success. Anecdotally, I believe this to be the most talented, collaborative, and passionate group of co-workers I’ve ever had the good fortune to work alongside — and despite what some people believe about making changes to the hiring process, none of these things have slowed us down.   On October 1st, 2018 my first day at CRL we were 56 Roachers strong. One year later (on my first Roachiversary) we were 117, meaning we've more than doubled in the last 12 months. One last note - if you’re not currently taking any of these steps in your recruiting process that’s okay! It can feel overwhelming to make sweeping changes, and trying to implement all of these at once would probably not be feasible.  Start small. Perhaps pick one role to pilot exercise-based interviewing and find a champion internally who is willing to give it a try. And then acknowledge that there will be bumps along the way; but, that doesn’t mean it’s impossible.  Remember, it’s not as though traditional hiring goes perfectly every (or even most of the) time. We love the way we hire here at CRL, but we also know we always have to be thinking about how to refine the process and make it better.  So, if you have thoughts about what we could do better or questions about our hiring process, we’d love to hear from you. You can reach me at recruiting@cockroachlabs.com .  And if you want to take a look at some of the documentation, head on over to our Github page where it’s described in even further detail.", "date": "2019-10-09"},
{"website": "CockroachLabs", "title": "Exercise-Based Interviewing: Devonaire's Road to Recruiting", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/exercise-based-interviewing-devonaires-road-to-recruiting/", "abstract": "Exercise-Based Interviewing is part of our commitment to reducing bias through the hiring process. Devonaire Ortiz is a Recruiter at Cockroach Labs. Inside the office, you can find him chatting it up with a candidate in one of our phone booths. Outside of the office, you're more likely to find him writing at a cafe. -- When I first heard about Cockroach Labs, I was ten months out of college and working at a small recruiting agency. I heard tell of their many perks— work-life balance, made possible by Flex Fridays, flexible hours and time off for employees, and the opportunity to work from home when needed; learning benefits, including a Udemy account for every member of the team and a Learning is Good budget to use on your professional development; and then add healthcare, parental benefits, and more to the mix, and I was already excited that they had an opening for a Recruiter. Still, what put me over the line and compelled me to apply was a blog post I read by our Chief People Officer, Lindsay Grenawalt, outlining how the team here reduces bias in its interview process by removing resumes and conducting exercise-based interviews. That commitment to diversity and inclusion at so early-stage a company showed me how lucky I would be to work there myself. I then learned about the company’s Open Sourced Interview Process , which takes this commitment one step further by giving candidates tools to prepare for each stage of the interview process. A few days after applying, I was on a call with Lindsay in which I learned more about the values that drive how Cockroach Labs hires and operates: excellence, transparency, balance, and respect, all of which deeply resonated with me. I was overjoyed to find out that I’d be moving on to next steps, which I prepared for using our Open Sourced interview guide. Now I’ll fast forward a bit to the day I came onsite for my final-round interviews— I had four unique exercises with different members of the team, including two mock hiring kick-offs and searches, one campus event planning meeting, and one mock offer-call with a highly sought-after engineer. All of these gave me the opportunity to show the team that I could do what was required of a tech recruiter at a high-growth company. Below is a more detailed overview of those exercises. Kick-off Meeting for Android Developer Objective: As part of your take-home exercise, you will prepare a kick-off and search for an Android developer, including criteria and sourcing. Onsite, your interviewer will act as the hiring manager for this role and will review your kick-off materials. Candidate Prep: We ask that you establish a clear process for the position, including a job requisition, a process overview, and breakdown of responsibilities between recruiter and hiring manager. Extending an Offer Objective: You will extend an offer to a candidate with compensation details we provide you with on-site. Candidate Prep: It is important to ask any questions necessary to begin your offer conversation with enough information. Think about what might motivate a candidate's job search and how to make the most compelling case possible. Pipeline Development for a Product Manager Objective: You will prepare a kick-off and search for a Product Manager, including criteria and sourcing. Your interviewer will act as the hiring manager for this role and will provide you with the details of the search. Candidate Prep: For this exercise, think about what questions you ask a hiring manager in order to understand their needs and who they would like to target. Consider what tools you've found most effective for sourcing. College Recruiting Event Objective: You will plan an info session at the University of Waterloo. Candidate Prep: For this exercise, consider how to effectively engage a college audience, your goals for the event, and what you should do before, during, and after the event. This background information provided me with insight into how what skills I would need to engage to do well during my interviews-- we provide every candidate with these tools to prepare. -- Fast-forwarding a bit more, I got hired! (for a more in-depth firsthand account of interviewing with Cockroach Labs, see Amruta Ranada’s post here ) As I write this, I have the pleasure of working as a recruiter tasked with building and leading our campus recruiting program, hiring engineers in Seattle and Toronto and Technical Operations staff in New York City, and my first project here was to overhaul our Open Sourced Interview repository. Keep in mind that I’ve been here for just over four months. Put simply, thanks to an interview process that prizes aptitude over lines on a resume, I was welcomed into a role where I get to have a high impact on Cockroach Labs’ future every single day. Now that I’m a member of the team, I can say proudly that those values Lindsay shared with me ring true here. We have candid conversations in the spirit of transparency, emphasize respect for one another, put our best foot forward to push the company, our team, and our product in the right direction, and take the time we need to recharge. With that said, you needn’t take my word for it. Check out our careers page and see if anything catches your eye.", "date": "2019-08-29"},
{"website": "CockroachLabs", "title": "Announcing CockroachCloud: CockroachDB as a Service", "author": ["Lakshmi Kannan"], "link": "https://www.cockroachlabs.com/blog/cockroachcloud-release/", "abstract": "Today we announced the beta program for CockroachCloud, a self-service, fully managed cloud offering of CockroachDB. CockroachCloud delivers simple and easy to onboard distributed SQL, while also offering a secure and flexible experience. Whether you’re building a small application on a single node, a resilient application across multiple nodes or a multi-region application serving users all over the world, CockroachCloud is the distributed SQL database for modern application developers. “As-a-service” has become the norm for how developers consume software. A year ago, we announced a limited availability of Managed CockroachDB, which was the first step in our cloud journey, and an important milestone in our mission of Making Data Easy. Since then, we’ve learned a lot from our earliest users. CockroachCloud is the culmination of all those learnings, and we’re excited to now open up the offering to a broader audience. Simple CockroachCloud is simple to set up and maintain. Creating a cluster is completed with a few clicks on the CockroachCloud UI - pick your cloud provider and region, number and size of nodes, and you’re off! Then you hand over all the day-to-day operations of running a distributed SQL database to us - the creators and experts of CockroachDB. We take care of provisioning servers, setting up monitoring and alerting, upgrading to major and minor versions, and daily full and hourly incremental backups. Our SRE team is on call 24/7 to ensure that your business is always on. All of our enterprise CockroachDB features are available on CockroachCloud, such as change data capture (CDC), role based access control (RBAC) and geo-partitioning. “With CockroachCloud, we don’t have to worry about scalability issues or troubleshooting performance. This gives back a lot of time to our developers.” -Global Head of Infrastructure at Education First Secure CockroachCloud is configured with enterprise-grade security. Each CockroachCloud cluster is deployed in a dedicated VPC with firewalls. Access to the cluster must be granted via IP whitelists. We use a combination of certificates (for node authentication) and usernames and passwords (for client authentication). All network traffic is encrypted using Transport Layer Security (TLS), and encryption at rest is on by default for all your data, including backups. Finally, role based access control can be used for both access to the CockroachCloud console as well as SQL users in the database. To connect to your database, simply create a SQL user, whitelist an IP address, and get a single connection string that points to the load balancer in front of your cluster. Flexible and resilient You can set up CockroachCloud to fit your needs. You can choose to run it in AWS or GCP, with support for more public cloud providers coming in the future. You can start with one node, which we recommend for testing, or three nodes, which is our preferred minimum production deployment. In a three-node deployment, we automatically provision nodes across three availability zones, thus ensuring your cluster is resilient to datacenter outages. If you want a globally available cluster, we will ensure your nodes are spread across multiple regions (and AZs within regions) to survive entire cloud provider region outages. “We’ve been using multi-region CockroachCloud in production since early 2018 as our data backbone. It’s incredible how high the quality bar is for these folks.” -Gorka Lerchundi Osa, Principal Architect at SALTO Systems Get Started with CockroachCloud Sign up HERE for your CockroachCloud cluster. We can’t wait to see what you build with CockroachCloud.", "date": "2019-10-16"},
{"website": "CockroachLabs", "title": "Get started geo-partitioning data with our command-line CockroachDB demo", "author": ["Rohan Yadav"], "link": "https://www.cockroachlabs.com/blog/get-started-geo-partitioning-data-with-our-command-line-cockroachdb-demo/", "abstract": "CockroachDB offers a number of powerful enterprise features, most notably those related to geo-partitioning tables . Geo-partitioning allows users to control where their data lives geographically, at the row-level. To make geo-partitioning easier for users to try out, we made some updates to cockroach demo that enable you to check out enterprise features without the need for a full deployment. Note that all of the features discussed in this blog will be available in CockroachDB version 19.2. When you run cockroach demo in a terminal, CockroachDB starts a temporary, in-memory cluster, and then opens a SQL shell to that cluster. The in-memory cluster persists only as long as the shell is open, and the data is lost once the shell is closed. cockroach demo also automatically acquires a temporary enterprise license for each demo session, so you can use enterprise CockroachDB features right now. After you install CockroachDB , no further set up is necessary. So let’s dive right into the new features available for use with cockroach demo ! To get started, simply run cockroach demo from the command line. After you start up cockroach demo , you’ll notice that the movr database is preloaded to the demo cluster. This database contains several pre-populated tables that store user, ride, and vehicle data for the global fictional vehicle-sharing company MovR . For MovR to service users all over the world, its database should be used on a distributed multi-region and multi-node deployment of CockroachDB. Take a look at how some new additions to cockroach demo can make setting up a virtual multi-region application easy. The first requirement for a distributed, multi-region cockroach cluster is to have more than one cockroach node. Using the --nodes <N> flag to cockroach demo , you can spin up an n -node in-memory cockroach cluster. For example, to start up a 9-node cluster, run the following command: $ ./cockroach demo --nodes 9 On this 9-node demo cluster, you can use EXPLAIN statements to get a more detailed look at how queries are planned and executed across the different nodes. To look at the distributed plan for a query, use the EXPLAIN (DISTSQL) prefix before the query. This returns a URL containing an interactive visualization of the query plan. For example, suppose you wanted to find all users who own vehicles by joining the vehicles and users tables where the vehicle owner_id and user id columns are equal. > EXPLAIN(DISTSQL) SELECT * FROM vehicles JOIN users ON vehicles.owner_id = users.id automatic |  url +-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ true    | https://cockroachdb.github.io/distsqlplan/decode.html#eJzEk0GL2zAQhe_9FWFOuzBLLFnb7hoKc-z2sClpb8UHxZrGBscyklwaSv57sR2IHRI3bqE9WjPfmzfz8E-orOFXvWMPyVcQgPAECM-QItTOZuy9dW2pb3wxPyCJEIqqbkL_HIpQMiTQVNYZdmwAwXDQRdnW00OKkFnHkJxaX-2DrZfqrBHBNuEomyL4oLcMiTrgYLQYjL4g_EVvSl6zNuyW0UgealfstNvTd86LrGQPCJ9rXflk8QDj2Se5zX6Ra5-PhUhBt9TRn7jq7_enwT-63gft84-2qNgt5dhayd_CHan7967Y5uGOxD0grJqQLEggSaQYSSE9Ir1Feof0hPSMJCIkIZCERBIxXAsiHi0qbw9CXA6i8ez-IgUxSkFeNfcPUoj_WwrRdApr9rWtPN_0o0Xtlmy23F_N28Zl_MnZrBvTf646rnsw7ENffew_Xqq-1BocwuIcFkNYjmAxD1bTsJy0HU3D8Qzbch6spmE1y3Z6ePMrAAD__8ep3nk= (1 row) The visual execution plan shows you some different components of the execution plan, including components that read rows from the tables, and other components that perform a Hash Join. You can also see how the query is distributed among nodes in the cluster. Nodes 8 and 9 perform the read and join, and then stream their data to node 1. In order to distribute the movr dataset across multiple regions, the CockroachDB nodes need to know where they are located geographically, or perhaps what kind of hardware they have access to. You can assign locality information, like regions and availability zones, to each node in the demo by using the --demo-locality flag. Make a 9-node demo cluster, where each node is virtually located in a different region in the US and Europe, with different availability zones: $ ./cockroach demo --nodes 9 --demo-locality=region=us-east1,az=b:region=us-east1,az=c:region=us-east1,az=d:region=us-west1,az=a:region=us-west1,az=b:region=us-west1,az=c:region=europe-west1,az=b:region=europe-west1,az=c:region=europe-west1,az=d … > SELECT node_id, locality FROM crdb_internal.gossip_nodes; node_id |         locality +---------+--------------------------+ 1 | region=us-east1,az=b 2 | region=us-east1,az=c 3 | region=us-east1,az=d 4 | region=us-west1,az=a 5 | region=us-west1,az=b 6 | region=us-west1,az=c 7 | region=europe-west1,az=b 8 | region=europe-west1,az=c 9 | region=europe-west1,az=d (9 rows) If you don’t specify localities for each node in your cluster, cockroach demo automatically assigns some location information for you. $ ./cockroach demo --nodes 9 … node_id |         locality +---------+--------------------------+ 1 | region=us-east1,az=b 2 | region=us-east1,az=c 3 | region=us-east1,az=d 4 | region=us-west1,az=a 5 | region=us-west1,az=b 6 | region=us-west1,az=c 7 | region=europe-west1,az=b 8 | region=europe-west1,az=c 9 | region=europe-west1,az=d (9 rows) The default locality settings mimic a Google Cloud Platform multi-region cluster in the US and Europe, where each node belongs to a different availability zone within each region. The default location information is also connected to the Admin UI’s Node Map. Go to the Admin UI in your browser, and see the nodes and their locations in the map. To open the Admin UI, go to the URL that appears when cockroach demo starts up. $ ./cockroach demo --nodes 9 # # Welcome to the CockroachDB demo database! # # You are connected to a temporary, in-memory CockroachDB cluster of 9 nodes. # # This demo session will attempt to enable enterprise features # by acquiring a temporary license from Cockroach Labs in the background. # To disable this behavior, set the environment variable # COCKROACH_SKIP_ENABLING_DIAGNOSTIC_REPORTING=true. # # The cluster has been preloaded with the \"movr\" dataset # (MovR is a fictional vehicle sharing company). # # Reminder: your changes to data stored in the demo session will not be saved! # # Web UI: http://127.0.0.1:58004 # # Server version: CockroachDB CCL v19.2.0-alpha.20190606-2479-gd98e0839dc (x86_64-apple-darwin18.7.0, built 2019/09/25 15:18:08, go1.12.6) (same version as client) # Cluster ID: 98a34894-80ea-4ffb-8f0d-9b2a300edca9 # Organization: Cockroach Demo # # Enter \\? for a brief introduction. # ... Without any geo-partitioning, the data in our multi-region 9 node cluster is distributed across the globe, and, with enough data, some of the tables could have rows in all 3 of the regions! In CockroachDB, tables are separated into ranges of data, each containing up to 64 MB of contiguous data. After the data is split up into ranges, the ranges are then distributed among the nodes in the cluster, and operations on ranges are distributed to the nodes that hold the ranges. You can use the SHOW RANGES command to show information about the ranges in a database, including the nodes that they are running on and the size of ranges. > SHOW RANGES FROM DATABASE movr; table_name         | start_key | end_key | range_id | range_size_mb | lease_holder  |  lease_holder_locality  | replicas  | replica_localities +----------------------------+-----------+---------+----------+---------------+--------------+--------------------------+----------+----------------------------------------------------------------------------+ promo_codes                | NULL      | NULL    |   25     | 0.229083      |   8           | region=europe-west1,az=c | {1,6,8}  | {\"region=us-east1,az=b\",\"region=us-west1,az=c\",\"region=europe-west1,az=c\"} rides                      | NULL      | NULL    |   23     | 0.175825      |   9           | region=europe-west1,az=d | {2,4,9}  | {\"region=us-east1,az=c\",\"region=us-west1,az=a\",\"region=europe-west1,az=d\"} user_promo_codes           | NULL      | NULL    |   26     | 0             |   9           | region=europe-west1,az=d | {1,5,9}  | {\"region=us-east1,az=b\",\"region=us-west1,az=b\",\"region=europe-west1,az=d\"} users                      | NULL      | NULL    |   21     | 0.005563      |   4           | region=us-west1,az=a     | {2,4,7}  | {\"region=us-east1,az=c\",\"region=us-west1,az=a\",\"region=europe-west1,az=b\"} vehicle_location_histories | NULL      | NULL    |   24     | 0.086918      |   5           | region=us-west1,az=b     | {1,5,8}  | {\"region=us-east1,az=b\",\"region=us-west1,az=b\",\"region=europe-west1,az=c\"} vehicles                   | NULL      | NULL    |   22     | 0.003585      |   6           | region=us-west1,az=c     | {1,6,9}  | {\"region=us-east1,az=b\",\"region=us-west1,az=c\",\"region=europe-west1,az=d\"} (6 rows) The replicas column in the output of SHOW RANGES shows that our tables are scattered around the (virtual) globe, but, without any partitioning, this cluster could be very slow to respond to a user’s query. Without telling CockroachDB anything about the layout of the tables, it’s possible for ranges of tables with European users to end up in nodes in the US West region. This would mean that European users’ queries would need to contact nodes in the western United States to get data for the query. The latency for communicating the thousands of miles separating the West Coast and Europe would be on the order of 100 ms, making an end user’s application unresponsive. Using CockroachDB’s geo-partitioning features, you can force data relating to users in Europe, US West and US East to live in each region respectively, so that a user’s request never has to leave the region where it belongs. cockroach demo knows how to automatically apply the Geo-Partitioned Replicas topology onto our 9-node cluster. At a high level, the Geo-Partitioned Replicas topology places all data relating to a region and its replicas in that region to allow for both fast region-specific reads and writes. To set up a 9-node, geo-partitioned demo cluster, simply use the --geo-partitioned-replicas flag on cockroach demo : $ ./cockroach demo --geo-partitioned-replicas Now take a look at the users table, using the SHOW PARTITIONS command. > SELECT table_name, partition_name, column_names, partition_value, zone_config FROM [SHOW PARTITIONS FROM TABLE users]; table_name | partition_name | column_names |                 partition_value | zone_config +------------+----------------+--------------+-------------------------------------------------+----------------------------------------+ users      | us_west       | city | ('seattle'), ('san francisco'), ('los angeles') | constraints = '[+region=us-west1]' users      | us_east       | city | ('new york'), ('boston'), ('washington dc')     | constraints = '[+region=us-east1]' users      | europe_west    | city | ('amsterdam'), ('paris'), ('rome')              | constraints = '[+region=europe-west1]' (3 rows) Time: 75.957ms As you can see, cockroach demo created 3 partitions of the movr data: us_west , us_east , and europe_west . The us_west partition contains all rows in the users table that have a city value of either seattle , san francisco , or los angeles . Additionally, we see that the us_west partition has a zone_config of +region=us-west1 , meaning that data within this partition must live on a node that has the locality attribute region equal to us-west1 . This zone configuration places the us_west partition of the users table within the group of nodes in our cluster that “live” in the US West region. Every request to the users table that touches rows with a city value equal to seattle , san francisco or los angeles will never have to leave the US West region! If you look around at other tables in the MovR dataset, you will see that partitions have been applied to the other tables in a similar way, and similar restrictions apply to the partitions us_east and europe_west . To see the exact commands used to apply these partitioning rules upon the tables, you can use the SHOW CREATE <table name> command. To explore more of the Admin UI post-partitioning, you can run a sample workload against the demo cluster, by adding the --with-load flag when starting the demo cluster. This runs a light workload aimed at mirroring the operations on the database of a vehicle-sharing app, with users starting and stopping rides. After running cockroach demo--with-load , you can navigate to the Admin UI and start to see what your demo cluster looks like under load. In the Admin UI you can explore different statistics about the cluster, such as different percentile query latencies, and range replication information. With cockroach demo , you can check out some of the work we’ve done this quarter to improve the usability of CockroachDB’s enterprise features. Please take a look and play around, and see if CockroachDB is a good fit for your workload!", "date": "2019-10-10"},
{"website": "CockroachLabs", "title": "Announcing CockroachDB 19.2", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/cockroachdb-19dot2-release/", "abstract": "Today, we’re proud to announce the release of CockroachDB 19.2, which significantly improves the latency, reliability, and usability of CockroachDB. CockroachDB 19.2 pushes the bounds of what a distributed SQL database can do. We’re bringing the latency of distributed transactions closer to the theoretical minimum, continuing to bolster the resilience and reliability of our enterprise and open core product, and we’re making CockroachDB easier to use than ever before. Not only does 19.2 introduce new features for better ease-of-use, we’re also launching Cockroach University to improve the experience of learning and using the product. Cockroach University is a free online learning tool for developers and architects who want to gain a fundamental understanding of distributed databases and deep knowledge of CockroachDB’s functionality and architecture. The first course, “Getting Started with CockroachDB,” uses videos, exercises, and quizzes to teach users about the key characteristics of a distributed SQL database, then walks them through how to spin up a CockroachDB instance, run basic queries, and test out CockroachDB’s unique capabilities. For additional information or to register visit Cockroach University.\nFor more info on what you can expect in CockroachDB 19.2, read on, or head straight to our release notes for the full list of product enhancements.\nReduced latency in both multi- and single-region deployments\nWe struggled to find an atomic commit protocol that enabled speedy (and ACID compliant) distributed transactions, so we built our own. It’s called Parallel Commits, and it halves the latency of all distributed transactions by reducing the rounds of consensus required for a transaction from 2 to 1. It brings the speed of distributed transactions that much closer to the theoretical minimum: the sum of all read latencies plus one round of consensus latency. While its primary use case is for speeding up globally distributed, multi-region deployments, it also improves secondary indexes tremendously in single region deployments. 19.2 also enables vectorized execution by default, which improves the performance of some analytical queries by up to 20x.\nImproved reliability and ease-of-use for distributed clusters\nIn 19.2, you can control the exact locations to which certain records are backed up. This can reduce cloud storage data transfer costs by keeping data within specific cloud regions and can help users comply with data domiciling requirements like GDPR.\nThere are also a number of new features within both the enterprise and OSS versions of the product that ease the complexity of configuring a geo-distributed database. Zone configurations, for example, are now much easier for users to set up. Zone configurations make distributed SQL more accessible by defining where data lives as well as where lease preferences are for any given subset of data (down to the table and row level). We’ve also introduced improvements to partitioning tables, so you can easily view partitions you’ve created on tables, indexes, or even the entire database.\nConclusion\nTo see the complete list of features in this release, you can reference our release notes. We’re also hosting a 19.2 technical demo on November 21 to introduce more features of the release and answer your questions live. And if you have any questions, please join in the conversation over at our new Slack channel. Or you can watch the webinar in which we talk about the release and answer questions from the community:", "date": "2019-11-12"},
{"website": "CockroachLabs", "title": "5 Lessons from ESCAPE/19: The Multi-Cloud Conference", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/5-lessons-from-escape-19-the-multi-cloud-conference/", "abstract": "Following his keynote on the big stage at ESCAPE/19 Armon Dadger, CTO and Co-Founder of HashiCorp, sat down with John Furrier of The Cube and said, “If you had this exact same conference 3 or 4 years ago everyone would’ve been like, what Multi-Cloud ? Multi-Cloud is not real . And now nobody questions the premise. They’re like, ‘ obviously we’re going to be multi-cloud .’” The inevitability of multi-cloud permeated the speaking sessions at ESCAPE/19 - the inaugural Multi-Cloud Conference hosted in downtown NYC. Technologists demo’d software to navigate the storage, networking, and compute challenges of multi-cloud. Business leaders took the stage to confront the price of multi-cloud deployments, the importance of moving data closer to users, and the business value of multiple vendors. In stark contrast to the big cloud shows, ESCAPE/19 speakers were encouraged to get into the multi-cloud weeds. On stage, in the breakout rooms, and in the hallway conversations multi-cloud trends took shape. This blog will focus on five trends that emerged and are important to watch as the multi-cloud transition continues to gather momentum. Multi-Cloud Security Starts at the Kernel Level “In order to have a true multi-cloud experience we need to start at the fundamental layers and work up.” - Kris Nova, Sysdig Security is a challenge in multi-cloud deployments. Amazon has great security tools but they’re all irrelevant if you want to move to Google. The challenge that Kris Nova addressed in her talk (and in her work at Sysdig) is how to build an agnostic set of tools that enable security privileges and observability information regardless of which cloud an application is running on. Sysdig does this by standardizing on the Linux kernel level utilizing cgroups and namespaces. Cgroups (a Linux kernel feature ) enable the system admin to define a custom set of rules. If a process tries to break a rule the cgroup will use the kernel to govern what an application can and cannot do. Namespaces work similarly, the difference being that they decide what a process can and cannot access. Nova explained that if you can audit at the kernel level then you can see the same issues regardless of where an application is running. From the kernel’s perspective, it doesn’t matter if an application is running in a container or on a host. Tools like Terraform make multi-cloud infrastructure easier by sitting above the clouds and abstracting away some of the complexity caused by the arbitrary differences in the cloud APIs. Kris Nova is making multi-cloud security easier by working way down below the clouds at the kernel. The Environmental Footprint of the Cloud “It’s not just about securing the cheapest price.” - Rob Reid, Lush To begin the second day of ESCAPE/19, Rob Reid gave a thoughtful, data-driven presentation called The Ethical Cloud in which he pointed out the impact that internet consumption has on the planet and the people involved. The three major cloud providers deliver varying degrees of effort towards achieving sustainability. Amazon , Microsoft and Google each have a webpage dedicated to their sustainability intentions. The TLDR summary is that the rapid growth of companies like Amazon is driving massive investments in fossil fuels (not renewable resources). Currently, the internet uses roughly 3% of global power consumption and is generating as much emissions as the aviation industry. This is an issue that can only be resolved by powering data centers with 100% renewable resources. While Amazon, and others, are planning to invest more into renewable energy the fact is that “we” the users have to participate in curbing the trend. Here are four actions that Rob recommends: Move to the cloud if you aren’t there already. There are more resources and energy efficiencies available in the cloud. Choose your provider carefully. Which ones are working towards increasing renewable energy usage? Minimize of over-provisioning - This will reduce the energy used and is more cost-effective. Ask questions - Make your cloud provider aware of your concerns. Microservices Broke the Local Development Experience “The development experience has gotten a lot worse precisely because of the features of microservices and containers that make multi-cloud possible.” -Ethan Jackson, Kelda To be clear, Ethan Jackson believes in the direction that multi-cloud is headed. In his talk on stage at ESCAPE/19 he said, “True portability is finally possible. We’re not all the way there yet - but we’re on the path to true multi-cloud.” His chief concern is what the path to true multi-cloud is doing to application developers. The same microservices that created the portability needed to keep up with continuous deployment also created a poor developer experience. There are more moving parts now than there were in the monolith. The standardization on complex tools means more complex infrastructure. And the transformed tech stack requires that developers have a larger variety of specialized skills. The problem is that we don’t live in a world where every developer knows the entire stack from assembly to front end. For DevOps engineers, Kubernetes is easy. But the application developers that are writing code every day are not interested in containers. A lot of the increased complexity happens in production. Developers that Ethan has spoken with say that they’re wasting 20-30% of their time fighting the local development environment. Tools like minikube would be helpful but they’re too difficult for most developers. The result is that developers don’t have confidence that the code they’re running will work in development. Ethan doesn’t see any reason why the development experience with microservices can’t improve over time. The “elite tech companies in San Francisco that have nice cafeterias solve the problem by hiring large teams to build out a development process that’s really smooth.” They solve it by giving each developer a sandbox, a shared kubernetes cluster (managed by DevOps team), a centralized configuration, a simple command-line interface, and instant code synchronization. The next step is to make that process accessible to developers who don’t work at these elite tech companies. The Cloud is Getting More Political “If you are a large enterprise buyer you have to care about politics.”  - Zac Smith, Packet In his talk, “The 5 P’s of Multi-Cloud,” Zac addressed price, performance, proximity, pride, and politics. Politics, he said, is hitting the enterprise hard in the form of data localization regulations popping up all over the globe. What’s interesting about politics is that the impact cannot be predicted or negotiated. If you’re a bank and a government says ‘ All your data has to stay in this country ,’ you have no choice but to figure it out. Places like Russia, China, Vietnam, India, and South Korea are a few of the 120 countries (and counting) in which there are data localization regulations. According to the Turbonomic 2019 State of Multi-Cloud study, 83% of infrastructure engineers believe that workloads will eventually move freely between multiple clouds. The fact is that infrastructure engineers need it to move freely between clouds because they’re subject to the whims of politicians and the growing quantity of data storage laws. Unfortunately, this free inter-cloud movement is likely years away. In the meantime, navigating the politics of the cloud means setting up a database that can react fluidly to political whims without compromising business goals or end-user experience. 'Accidental' Multi-Cloud is Pretty Common “You might say: I’m going to go all-in on Cloud A and then I buy a company that uses Cloud B. Now I’m multi-cloud. The pragmatic reality for the global 10,000 is that you’re going to be a multi-cloud company whether you want to or not.” -Armon Dadger, HashiCorp This one is for the non-believers. In various talks throughout the two-day event, the concept of “accidental multi-cloud” came up often. This trend began with the maturation of Google Cloud and Azure and has become an inevitable reality for many companies through mergers and acquisitions. Eric Han from Portworx gave the credit for coining the term accidental multi-cloud to Joseph Sandoval from Adobe Advertising who explained what he meant, “Adobe is buying different companies that may be on AWS (which a lot of us are) or some are on Google Compute Engine. And all of a sudden I have these different entities running on these different platforms - it just kind of happens. And then as leaders, you have to figure out how to make these things work together.” We built ESCAPE/19 to help people figure out this challenge: how to make multi-cloud work. And as speakers like Armon and Eric pointed out, that challenge is going to apply to everyone in years to come--whether they anticipate it or not. If you couldn’t make it to the conference, don’t worry - here is a link to all the videos from ESCAPE/19. Hopefully, we’ll see you there next year.", "date": "2019-11-05"},
{"website": "CockroachLabs", "title": "Database Migration Guide: How to Migrate from Oracle to CockroachDB", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/database-migration-guide-oracle-to-cockroachdb/", "abstract": "Why do Companies Migrate from Oracle to CockroachDB? There is not one specific answer to this question, but rather a list of consistent trends that we have seen over the years. Companies migrate from Oracle to CockroachDB because of the high Total Cost of Ownership at Oracle - in which we include the operational overhead required to run an Oracle database. Companies migrate because they want to move to the cloud and, in particular, want to build on a distributed database that is compatible with Kubernetes . Companies want the flexibility to deploy on-prem and in different cloud platforms which strengthens their resiliency ( think AWS outages ) and strengthens their ability to serve a global audience. Companies never want to shard again . What’s not mentioned in that list, and what bears mentioning, is the digital acceleration caused by the pandemic. Companies have stressed and exceeded the boundaries of legacy tech stacks. They have had no choice but to look for modern solutions to their new tech requirements and, in the process, they’re looking for the solutions that are built for the future. Oracle Migration Guide Regardless of your reasons for the migration (and the lush green grass on the other side) every database migration conversation should begin with an acknowledgment of the challenge that migration represents. No ‘migration guide’ is a magic carpet to carry your data and applications from a traditional database to a cloud-native database. That being said, the challenge of migration is less daunting with a comprehensive guide to steer you. If you’re using Oracle and you’re reading this blog, chances are you’ve experienced the limitations of legacy database technology and are planning to use a cloud-native database that will take advantage of the reduced complexity and ease of scale in the cloud. At Cockroach Labs we have years of experience helping c ustomers migrate workloads from Oracle over to CockroachDB. We’ve used that experience to polish this guide that’s intended to help you scope out and execute a migration from Oracle to CockroachDB. We hope that you find it to be a helpful tool. If you read the guide and find that you need additional support please don’t hesitate to contact us via https://support.cockroachlabs.com . How to Migrate from Oracle to CockroachDB Extract Schema(s) Tables, views, sequences, indexes, constraints Users, privileges, groups and roles Refactor the Schema to Cockroach dialect Evaluated database embedded code Stored Procedures, Jobs, and Triggers aren’t supported by CRDB CRDB doesn’t support PLSQL Must rewrite application to support outside of the database Extract Data using bulk or conventional methods Load Schema and Data either through bulk or replication methods Refactor Application SQL The reduction of an Oracle migration into five steps is a comic oversimplification. We know that the process is challenging. And we’re here to help. We have engineers that have worked with both companies and understand all the nuances involved in a migration. If you’re ready to get started just download the Oracle to CockroachDB migration guide today. If you’re still in the process of measuring the value of a migration then check out this comparison of High Availability architecture in Oracle and CockroachDB .", "date": "2019-09-26"},
{"website": "CockroachLabs", "title": "CockroachDB Passes TPC-C at 100k Warehouses", "author": ["Andy Woods", "Jesse Seldess", "Andrew Werner"], "link": "https://www.cockroachlabs.com/blog/tpcc-100k/", "abstract": "[For CockroachDB's most up-to-date performance benchmarks, please read our Performance Overview page ] Last fall we wrote about how CockroachDB was 50x more scalable than Amazon Aurora as evidenced on the industry-standard TPC-C benchmark. We're pleased to announce that CockroachDB has doubled that performance benchmark by successfully passing TPC-C at 100,000 warehouses. And with a max throughput of 1.2m tpmC, CockroachDB can now process 100X the throughput of Amazon Aurora’s last published benchmark. Unlike Aurora and other databases that selectively degrade isolation levels for performance, CockroachDB can achieve massive scale while maintaining serializable isolation , protecting your data from fraud and data loss. TPC-C Refresher Cockroach Labs measures performance through many diverse tests, including the industry-standard OLTP benchmark TPC-C , which simulates an e-commerce or retail company. Created in 1992, TPC-C has withstood the test of time and remains the most mature industry benchmark for OLTP workloads, and the only objective comparison for evaluating OLTP performance. In its own words, TPC-C: “…involves a mix of five concurrent transactions of different types and complexity either executed on-line or queued for deferred execution. The database is comprised of nine types of tables with a wide range of record and population sizes. While the benchmark portrays the activity of a wholesale supplier, TPC-C is not limited to the activity of any particular business segment, but, rather represents any industry that must manage, sell, or distribute a product or service.” As a result, TPC-C includes create, read, update, and delete (e.g., CRUD) queries, basic joins, and other SQL statements used to administer mission-critical transactional workloads. It includes detailed specifications for concurrency and workload contention. TPC-C 100,000 Note, this is an unofficial TPC-C result. If this run were an official run it would be the 28th largest run on TPC-C leaderboard . We compared our unofficial TPC-C results to Amazon Aurora RDS’ unofficial TPC-C results from AWS re:Invent 2017 . We also used Aurora’s SIGMOD 2017 paper for additional information as to their test setup and load generator. As such, based upon their last published metrics, CockroachDB is now 100 times more scalable than Amazon Aurora, supporting 50 billion rows and more than 8 terabytes of frequently accessed data. Note, CockroachDB scales out to many more nodes than Aurora while still achieving the requirements posted in the TPC-C spec which demonstrates how differences in database architecture can directly influence scale. Amazon Aurora CockroachDB 18.2* CockroachDB 19.2 Max Throughput (tpmC) 12,582 631,851 1,245,462 Efficiency 97.84% 98.27% 98.81% p95 Latency for newOrder Transactions (ms) Not Reported 1140.90 486.50 Max Warehouses with Max Efficiency (Warehouses) 1,000 50,000 100,000 Max Number of Rows (Billion) 0.499 24.9 49.8 Max Unreplicated Data (TB) 0.08 4 8 Machine type 2 r3.8xl 135 n1-highcpu-16 81 c5d.9xlarge *CockroachDB 18.2 was previously marketed under the name CockroachDB 2.1. Consult this blog to learn more about why we switched to calendar versioning . CockroachDB Performance Page In addition to publishing these results, we are also pleased to announce that we have created a permanent performance page . This page will always contain the latest information on CockroachDB’s performance on industry-standard tests like TPC-C and Sysbench. You can visit this page to learn more about how CockroachDB provides predictable scaling, throughput, and latency. If you fail to achieve similar performance profiles, there is likely a problem in either the hardware, workload, or test design. We stand by these profile characteristics and provide reproduction steps for all published benchmarks. To that end, Cockroach Labs believes that a benchmark isn’t a benchmark if it can’t be reproduced. We will always include reproduction steps for any benchmark we publish. You can visit this page to find our most recent results as well as reproduction steps for TPC-C 1K, 10K, and 100K. Click here to learn more about how CockroachDB can provide you a managed CockroachCloud cluster that can provide a hassle-free way to meet your application’s performance needs.", "date": "2019-11-14"},
{"website": "CockroachLabs", "title": "How We Built a Vectorized Execution Engine", "author": ["Alfonso Subiotto Marques", "Rafi Shamim"], "link": "https://www.cockroachlabs.com/blog/how-we-built-a-vectorized-execution-engine/", "abstract": "CockroachDB is an OLTP database, specialized for serving high-throughput queries that read or write a small number of rows. As we gained more usage, we found that customers weren’t getting the performance they expected from analytic queries that read a lot of rows, like large scans, joins, or aggregations. In April 2018, we started to seriously investigate how to improve the performance of these types of queries in CockroachDB, and began working on a new SQL execution engine. In this blog post, we use example code to discuss how we built the new engine and why it results in up to a 4x speed improvement on an industry-standard benchmark. OLTP databases, including CockroachDB, store data in contiguous rows on disk and process queries a row of data at a time. This pattern is optimal for serving small queries with high throughput and low latency, since the data in the rows are stored contiguously, making it more efficient to access multiple columns from the same row. Modern OLAP databases, on the other hand, typically are better at serving large queries, and tend to store data in contiguous columns and operate on these columns using a concept called vectorized execution . Using vectorized processing in an execution engine makes more efficient use of modern CPUs by changing the data orientation (from rows to columns) to get more out of the CPU cache and deep instruction pipelines by operating on batches of data at a time. In our research into vectorized execution, we came across MonetDB/X100: Hyper-Pipelining Query Execution , a paper that outlines the performance deficiencies of the row-at-a-time Volcano execution model that CockroachDB’s original execution engine was built on. When executing queries on a large number of rows, the row-oriented execution engine pays a high cost in interpretation and evaluation overhead per tuple and doesn’t take full advantage of the efficiencies of modern CPUs. Given the key-value storage architecture of CockroachDB, we knew we couldn’t store data in columnar format, but we wondered if converting rows to batches of columnar data after reading them from disk, and then feeding those batches into a vectorized execution engine, would improve performance enough to justify building and maintaining a new execution engine. To quantify the performance improvements, and to test the ideas laid out in the paper, we built a vectorized execution engine prototype , which yielded some impressive results. In this tutorial-style blog post, we take a closer look at what these performance improvements look like in practice. We also demonstrate why and how we use code generation to ease the maintenance burden of the vectorized execution engine. We take an example query, analyze its performance in a toy, row-at-a-time execution engine, and then explore and implement improvements inspired by the ideas proposed in the MonetDB/x100 paper. The code referenced in this post resides in https://github.com/asubiotto/vecdeepdive , so feel free to look at, modify, and/or run the code and benchmarks while you follow along. What’s in a SQL operator? To provide some context, let’s look at how CockroachDB executes a simple query, SELECT price * 0.8 FROM inventory , issued by a fictional retail customer that wants to compute a discounted price for each item in her inventory. Regardless of which execution engine is used, this query is parsed, converted into an abstract syntax tree (AST) , optimized, and then executed. The execution, whether distributed amongst all nodes in a cluster, or executed locally, can be thought of as a chain of data manipulations that each have a specific role, which we call operators. In this example query, the execution flow would look like this: You can generate a diagram of the physical plan by executing EXPLAIN (DISTSQL) on the query. As you can see, the execution flow for this query is relatively simple. The TableReader operator reads rows from the inventory table and then executes a post-processing render expression, in this case the multiplication by a constant float. Let’s focus on the render expression, since it’s the part of the flow that is doing the most work. Here's the code that executes this render expression in the original, row-oriented execution engine used in CockroachDB (some code is omitted here for simplicity): func (expr *BinaryExpr) Eval(ctx *EvalContext) (Datum, error) {\n\tleft, err := expr.Left.(TypedExpr).Eval(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tright, err := expr.Right.(TypedExpr).Eval(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn expr.fn.Fn(ctx, left, right)\n} The left and right side of the binary expression ( BinaryExpr ) are both values wrapped in a Datum interface. The BinaryExpr calls expr.fn.Fn with both of these as arguments. In our example, the inventory table has a FLOAT price column, so the Fn is: Fn: func(_ *EvalContext, left Datum, right Datum) (Datum, error) {\n        return NewDFloat(*left.(*DFloat) * *right.(*DFloat)), nil\n} In order to perform the multiplication, the Datum values need to be converted to the expected type. If, instead, we created a price column of type DECIMAL , we would cast 0.8 to a DECIMAL and then construct a BinaryExpr with a different Fn specialized for multiplying DECIMAL s. We now have specialized code for multiplying each type, but the TableReader doesn’t need to worry about it. Before executing the query, the database creates a query plan that specifies the correct Fn for the type that we are working with. This simplifies the code, since we only need to write specialized code as an implementation of an interface. It also makes the code less efficient, as each time we multiply two values together, we need to dynamically resolve which Fn to call, cast the interface values to concrete type values that we can work with, and then convert the result back to an interface value. Benchmarking a simple operator How expensive is this casting, really? To find the answer to this question, let’s take a similar but simpler toy example: type Datum interface{}\n\n// Int implements the Datum interface.\ntype Int struct {\n    int64\n}\n\nfunc mulIntDatums(a Datum, b Datum) Datum {\n\taInt := a.(Int).int64\n\tbInt := b.(Int).int64\n\treturn Int{int64: aInt * bInt}\n}\n\n// ...\n\nfunc (m mulOperator) next() []Datum {\n\trow := m.input.next()\n\tif row == nil {\n\t\treturn nil\n\t}\n\tfor _, c := range m.columnsToMultiply {\n\t\trow[c] = m.fn(row[c], m.arg)\n\t}\n\treturn row\n} This is a type-agnostic single operator that can handle multiplication of an arbitrary number of columns by a constant argument. Think of the input as returning the rows from the table. To add support for DECIMAL s, we can simply add another function that multiplies DECIMAL s with a mulFn signature. We can measure the performance of this code by writing a benchmark ( see it in our repo ). This will give us an idea of how fast we can multiply a large number of Int rows by a constant argument. The benchstat tool tells us that it takes around 760 microseconds to do this: $ go test -bench “BenchmarkRowBasedInterface$” -count 10 > tmp && benchstat tmp && rm tmp\nname               \ttime/op\nRowBasedInterface-12\t760µs ±15% Because we have nothing to compare the performance against at this point, we don’t know if this is slow or not. We’ll use a “speed of light” benchmark to get a better relative sense of this program’s speed. A “speed of light” benchmark measures the performance of the minimum necessary work to perform an operation. In this case, what we really are doing is multiplying 65,536 int64 s by 2. The result of running this benchmark is: $ go test -bench \"SpeedOfLight\" -count 10 > tmp && benchstat tmp && rm tmp\nname               \ttime/op\nSpeedOfLight-12    \t19.0µs ± 6% This simple implementation is about 40x faster than our earlier operator! To try to figure out what’s going on, let’s run a CPU profile on BenchmarkRowBasedInterface and focus on the mulOperator . We can use the -o option to obtain an executable, which will let us disassemble the function with the disasm command in pprof . As we will see below, this command will give us the assembly that our Go source code compiles into, along with approximate CPU times for each instruction. First, let’s use the top and list commands to find the slow parts of the code. $ go test -bench \"BenchmarkRowBasedInterface$\" -cpuprofile cpu.out -o row_based_interface\n…\n$ go tool pprof ./row_based_interface cpu.out\n(pprof) focus=mulOperator\n(pprof) top\nActive filters:\n   focus=mulOperator\nShowing nodes accounting for 1.99s, 88.05% of 2.26s total\nDropped 15 nodes (cum <= 0.01s)\nShowing top 10 nodes out of 12\n  \tflat  flat%   sum%    \tcum   cum%\n \t0.93s 41.15% 41.15%  \t2.03s 89.82%  _~/scratch/vecdeepdive.mulOperator.next\n \t0.47s 20.80% 61.95%  \t0.73s 32.30%  _~/scratch/vecdeepdive.mulIntDatums\n \t0.36s 15.93% 77.88%  \t0.36s 15.93%  _~/scratch/vecdeepdive.(*tableReader).next\n \t0.16s  7.08% 84.96%  \t0.26s 11.50%  runtime.convT64\n \t0.07s  3.10% 88.05%  \t0.10s  4.42%  runtime.mallocgc\n     \t0 \t0% 88.05%  \t2.03s 89.82%  _~/scratch/vecdeepdive.BenchmarkRowBasedInterface\n     \t0 \t0% 88.05%  \t0.03s  1.33%  runtime.(*mcache).nextFree\n     \t0 \t0% 88.05%  \t0.02s  0.88%  runtime.(*mcache).refill\n     \t0 \t0% 88.05%  \t0.02s  0.88%  runtime.(*mcentral).cacheSpan\n     \t0 \t0% 88.05%  \t0.02s  0.88%  runtime.(*mcentral).grow\n(pprof) list next\nROUTINE ======================== _~/scratch/vecdeepdive.mulOperator.next in ~/scratch/vecdeepdive/row_based_interface.go\n \t930ms  \t2.03s (flat, cum) 89.82% of Total\n     \t.      \t. \t39:\n  \t60ms   \t60ms \t40:func (m mulOperator) next() []Datum {\n \t120ms  \t480ms \t41:    row := m.input.next()\n  \t50ms   \t50ms \t42:    if row == nil {\n     \t.      \t. \t43:   \t return nil\n     \t.      \t. \t44:    }\n \t250ms  \t250ms \t45:    for _, c := range m.columnsToMultiply {\n \t420ms  \t1.16s \t46:   \t row[c] = m.fn(row[c], m.arg)\n     \t.      \t. \t47:    }\n  \t30ms   \t30ms \t48:    return row\n     \t.      \t. \t49:}\n     \t.      \t. \t50: We can see that out of 2030ms , the mulOperator spends 480ms getting rows from the input, and 1160ms performing the multiplication. 420ms of those are spent in next before even calling m.fn (the left column is the flat time, i.e., time spent on that line, while the right column is the cumulative time, which also includes the time spent in the function called on that line). Since it seems like the majority of time is spent multiplying arguments, let’s take a closer look at mulIntDatums : (pprof) list mulIntDatums\nTotal: 2.26s\nROUTINE ======================== _~/scratch/vecdeepdive.mulIntDatums in ~/scratch/vecdeepdive/row_based_interface.go\n \t470ms  \t730ms (flat, cum) 32.30% of Total\n     \t.      \t. \t10:\n  \t70ms   \t70ms \t11:func mulIntDatums(a Datum, b Datum) Datum {\n  \t20ms   \t20ms \t12:    aInt := a.(Int).int64\n  \t90ms   \t90ms \t13:    bInt := b.(Int).int64\n \t290ms  \t550ms \t14:    return Int{int64: aInt * bInt}\n     \t.      \t. \t15:} As expected, the majority of the time spent in mulIntDatums is on the multiplication line. Let’s take a closer look at what’s going on under the hood here by using the disasm (disassemble) command (some instructions are omitted): (pprof) disasm mulIntDatums\n     \t.      \t.\t1173491: MOVQ 0x28(SP), AX                   \t;row_based_interface.go:12\n  \t20ms   \t20ms\t1173496: LEAQ type.*+228800(SB), CX          \t;_~/scratch/vecdeepdive.mulIntDatums row_based_interface.go:12\n     \t.      \t.\t117349d: CMPQ CX, AX                         \t;row_based_interface.go:12\n     \t.      \t.\t11734a0: JNE 0x1173505\n     \t.      \t.\t11734a2: MOVQ 0x30(SP), AX\n     \t.      \t.\t11734a7: MOVQ 0(AX), AX\n  \t90ms   \t90ms\t11734aa: MOVQ 0x38(SP), DX                   \t;_~/scratch/vecdeepdive.mulIntDatums row_based_interface.go:13\n     \t.      \t.\t11734af: CMPQ CX, DX                         \t;row_based_interface.go:13\n     \t.      \t.\t11734b2: JNE 0x11734e9\n     \t.      \t.\t11734b4: MOVQ 0x40(SP), CX\n     \t.      \t.\t11734b9: MOVQ 0(CX), CX\n  \t70ms   \t70ms\t11734bc: IMULQ CX, AX                        \t;_~/scratch/vecdeepdive.mulIntDatums row_based_interface.go:14\n  \t60ms   \t60ms\t11734c0: MOVQ AX, 0(SP)\n  \t90ms  \t350ms\t11734c4: CALL runtime.convT64(SB) Surprisingly, only 70ms is spent executing the IMULQ instruction, which is the instruction that ultimately performs the multiplication. The majority of the time is spent calling convT64 , which is a Go runtime package function that is used (in this case) to convert the Int type to the Datum interface. The disassembled view of the functions suggests that most of the time spent multiplying values is converting the arguments from Datum s to Int s and the result from an Int back to a Datum . Using concrete types To avoid the overhead of these conversions, we would need to work with concrete types. This is a tough spot to be in, since the execution engine we've been discussing uses interfaces to be type-agnostic. Without using interfaces, each operator would need to have knowledge about the type it is working with. In other words, we would need to implement an operator for each type. Luckily, we have the prior research of the MonetDB team to guide us. Given their work, we knew that the pain caused by removing the interfaces would be justified by huge potential performance improvements. Later, we will take a look at how we got away with using concretely-typed operators to avoid typecasts for performance reasons, without sacrificing all of the maintainability that comes from using Go's type-agnostic interfaces. First, let’s look at what will replace the Datum interface: type T int\n\nconst (\n    // Int64Type is a value of type int64\n    Int64Type T = iota\n    // Float64Type is a value of type float64\n    Float64Type\n)\n\ntype TypedDatum struct {\n    t T\n    int64 int64\n    float64 float64\n}\n\ntype TypedOperator interface {\n    next() []TypedDatum\n} A Datum now has a field for each possible type it may contain, rather than having separate interface implementations for each type. There is an additional enum field that serves as a type marker, so that when we do need to, we can inspect a type of a Datum without doing any expensive type assertions. This type uses extra memory due to having a field for each type, even though only one of them will be used at a time. This could lead to CPU cache inefficiencies, but for this section we will skip over those concerns and focus on dealing with the interface interpretation overhead. In a later section, we’ll discuss the inefficiency more and address it. The mulInt64Operator will now look like this: func (m mulInt64Operator) next() []TypedDatum {\n    row := m.input.next()\n    if row == nil {\n   \t return nil\n    }\n    for _, c := range m.columnsToMultiply {\n   \t row[c].int64*= m.arg\n    }\n    return row\n} Note that the multiplication is now in place. Running the benchmark against this new version shows almost a 2x speed up. $ go test -bench \"BenchmarkRowBasedTyped$\" -count 10 > tmp && benchstat tmp && rm tmp\nname               \ttime/op\nRowBasedTyped-12    \t390µs ± 8% However, now that we are writing specialized operators for each type, the amount of code we have to write has nearly doubled, and even worse, the code violates the maintainability principle of staying DRY ( D on’t R epeat Y ourself ). The situation seems even worse if we consider that in a real database engine, there would be far more than two types to support. If someone were to slightly change the multiplication functionality (for example, adding overflow handling), they would have to rewrite every single operator, which is tedious and error-prone. The more types, the more work one has to do to update code. Generating code with templates Thankfully, there is a tool we can use to reduce this burden and keep the good performance characteristics of working with concrete types. The Go templating engine allows us to write a code template that, with a bit of work, we can trick our editor into treating as a regular Go file. We have to use the templating engine because the version of Go we are currently using does not have support for generic types. Templating the multiplication operators would look like this (full template code is in row_based_typed_tmpl.go ): // {{/*\ntype _GOTYPE interface{}\n\n// _MULFN assigns the result of the multiplication of the first and second\n// operand to the first operand.\nfunc _MULFN(_ TypedDatum, _ interface{}) {\n\tpanic(\"do not call from non-templated code\")\n}\n// */}}\n// {{ range .}}\ntype mul_TYPEOperator struct {\n\tinput             TypedOperator\n\targ               _GOTYPE\n\tcolumnsToMultiply []int\n}\nfunc (m mul_TYPEOperator) next() []TypedDatum {\n\trow := m.input.next()\n\tif row == nil {\n\t\treturn nil\n\t}\n\tfor _, c := range m.columnsToMultiply {\n\t\t_MULFN(row[c], m.arg)\n\t}\n\treturn row\n}\n// {{ end }} The accompanying code to generate the full row_based_typed.gen.go file is located in row_based_type_gen.go . This code is executed by running go run . to run the main() function in generate.go (omitted here for conciseness). The generator will iterate over a slice and fill the template in with specific information for each type. Note that there is a prior step that is necessary in order to consider the row_based_typed_tmpl.go file valid Go. In the template, we use tokens that are valid Go (e.g. _GOTYPE and _MULFN). These tokens’ declarations are wrapped in template comments and removed in the final generated file. For example, the multiplication function ( _MULFN ) is converted to a method call with the same arguments: // Replace all functions.\nmulFnRe := regexp.MustCompile(`_MULFN\\((.*),(.*)\\)`)\ns = mulFnRe.ReplaceAllString(s, `{{ .MulFn \"$1\" \"$2\" }}`) MulFn is called when executing the template, and then returns the Go code to perform the multiplication according to type-specific information. Take a look at the final generated code in row_based_typed.gen.go. The templating approach we took has some rough edges, and certainly is not a very flexible implementation. Nonetheless, it is a critical part of the real vectorized execution engine that we built in CockroachDB, and it was simple enough to build without getting sidetracked by creating a robust domain-specific language. Now, if we want to add functionality or fix a bug, we can modify the template once and regenerate the code for changes to all operators. Now that the code is a little more manageable and extensible, let’s try to improve the performance further. NOTE: To make the code in the rest of this blog post easier to read, we won’t use code generation for the following operator rewrites. Batching expensive calls Repeating our benchmarking process from before shows us some useful next steps. $ go test -bench \"BenchmarkRowBasedTyped$\" -cpuprofile cpu.out -o row_typed_bench\n$ go tool pprof ./row_typed_bench cpu.out\n\n(pprof) list next\nROUTINE ======================== _~/scratch/vecdeepdive.mulInt64Operator.next in ~/scratch/vecdeepdive/row_based_typed.gen.go\n \t1.26s  \t1.92s (flat, cum) 85.71% of Total\n     \t.      \t.  \t8:    input         \tTypedOperator\n     \t.      \t.  \t9:    arg           \tint64\n     \t.      \t. \t10:    columnsToMultiply []int\n     \t.      \t. \t11:}\n     \t.      \t. \t12:\n \t180ms  \t180ms \t13:func (m mulInt64Operator) next() []TypedDatum {\n \t170ms  \t830ms \t14:    row := m.input.next()\n     \t.      \t. \t15:    if row == nil {\n     \t.      \t. \t16:   \t return nil\n     \t.      \t. \t17:    }\n \t330ms  \t330ms \t18:    for _, c := range m.columnsToMultiply {\n \t500ms  \t500ms \t19:   \t row[c].int64*= m.arg\n     \t.      \t. \t20:    }\n  \t80ms   \t80ms \t21:    return row\n     \t.      \t. \t22:} This part of the profile shows that approximately half of the time spent in the mulInt64Operator.next function is spent calling m.input.next() (see line 13 above). This isn’t surprising if we look at our implementation of (*typedTableReader).next() ; it’s a lot of code just for advancing to the next element in a slice. We can’t optimize too much about the typedTableReader , since we need to preserve the ability for it to be chained to any other SQL operator that we may implement. But there is another important optimization that we can do: instead of calling the next function once for each row, we can get back a batch of rows and operate on all of them at once, without changing too much about (*typedTableReader).next . We can’t just get all the rows at once, because some queries might result in a huge dataset that won’t fit in memory, but we can pick a reasonably large batch size. With this optimization, we have operators like the ones below. Once again, the full code for this new version is omitted, since there’s a lot of boilerplate changes. Full code examples can be found in row_based_typed_batch.go . type mulInt64BatchOperator struct {\n    input         \tTypedBatchOperator\n    arg           \tint64\n    columnsToMultiply []int\n}\n\nfunc (m mulInt64BatchOperator) next() [][]TypedDatum {\n    rows := m.input.next()\n    if rows == nil {\n   \t return nil\n    }\n    for _, row := range rows {\n   \t for _, c := range m.columnsToMultiply {\n   \t\t row[c] = TypedDatum{t: Int64Type, int64: row[c].int64 * m.arg}\n   \t }\n    }\n    return rows\n}\n\ntype typedBatchTableReader struct {\n    curIdx int\n    rows   [][]TypedDatum\n}\n\nfunc (t *typedBatchTableReader) next() [][]TypedDatum {\n    if t.curIdx >= len(t.rows) {\n   \t return nil\n    }\n    endIdx := t.curIdx + batchSize\n    if endIdx > len(t.rows) {\n   \t endIdx = len(t.rows)\n    }\n    retRows := t.rows[t.curIdx:endIdx]\n    t.curIdx = endIdx\n    return retRows\n} With this batching change, the benchmarks run nearly 3x faster (and 5.5x faster than the original implementation): $ go test -bench \"BenchmarkRowBasedTypedBatch$\" -count 10 > tmp && benchstat tmp && rm tmp\nname               \ttime/op\nRowBasedTypedBatch-12   137µs ±77% Column-oriented Data But we are still a long ways away from getting close to our “speed of light” performance of 19 microseconds per operation. Does the new profile give us more clues? $ go test -bench \"BenchmarkRowBasedTypedBatch\" -cpuprofile cpu.out -o row_typed_batch_bench\n$ go tool pprof ./row_typed_batch_bench cpu.out\n\n(pprof) list next\nTotal: 990ms\nROUTINE ======================== _~/scratch/vecdeepdive.mulInt64BatchOperator.next in ~/scratch/vecdeepdive/row_based_typed_batch.go\n \t950ms  \t950ms (flat, cum) 95.96% of Total\n     \t.      \t. \t15:func (m mulInt64BatchOperator) next() [][]TypedDatum {\n     \t.      \t. \t16:    rows := m.input.next()\n     \t.      \t. \t17:    if rows == nil {\n     \t.      \t. \t18:   \t return nil\n     \t.      \t. \t19:    }\n \t210ms  \t210ms \t20:    for _, row := range rows {\n \t300ms  \t300ms \t21:   \t for _, c := range m.columnsToMultiply {\n \t440ms  \t440ms \t22:   \t\t row[c] = TypedDatum{t: Int64Type, int64: row[c].int64 * m.arg}\n     \t.      \t. \t23:   \t }\n     \t.      \t. \t24:    }\n     \t.      \t. \t25:    return rows\n     \t.      \t. \t26:} Now the time calling (*typedBatchTableReader).next barely registers in the profile! That is much better. The profile shows that lines 20-22 is probably the best place to focus our efforts next. These lines are where well above 95% of the time is spent. That is partially a good sign, because these lines are implementing the core logic of our operator. However, there certainly is still room for improvement. Approximately half of the time spent in these three lines is just in iterating through the loops, and not in the loop body itself. If we think about the sizes of the loops, then this starts to become more clear. The length of the rows batch is 1,024 , but the length of columnsToMultiply is just 1 . Since the rows loop is the outer loop, this means that we are setting up this tiny inner loop -- initializing a counter, incrementing it, and checking the boundary condition -- 1,024 times! We could avoid all that repeated work simply by changing the order of the two loops. Although we won’t go into a full exploration of CPU architecture in this post, there are two important concepts that come into play when changing the loop order: branch prediction and pipelining. In order to speed up execution, CPUs use a technique called pipelining to begin executing the next instruction before the preceding one is completed. This works well in the case of sequential code, but whenever there are conditional branches, the CPU cannot identify with certainty what the next instruction after the branch will be. However, it can make a guess as to which branch will be followed. If the CPU guesses incorrectly, the work that the CPU has already performed to begin evaluating the next instruction will go to waste. Modern CPUs are able to make predictions based on static code analysis, and even the results of previous evaluations of the same branch. Changing the order of the loops comes with another benefit. Since the outer loop will now tell us which column to operate on, we can load all the data for that column at once, and store it in memory in one contiguous slice. A critical component of modern CPU architecture is the cache subsystem. In order to avoid loading data from main memory too often, which is a relatively slow operation, CPUs have layers of caches that provide fast access to frequently used pieces of data, and they can also prefetch data into these caches if the access pattern is predictable. In the row based example, we would load all the data for each row, which would include columns that were not at all affected by the operator, so not as much relevant data would fit into the CPU cache. Orienting the data we are going to operate on by column provides a CPU with exactly the predictability and dense memory-packing that it needs to make ideal use of its caches. For a fuller treatment of pipelining, branch prediction, and CPU caches see Dan Luu’s branch prediction talk notes , his CPU cache blog post , or Dave Cheney's notes from his High Performance Go Workshop . The code below shows how we could make the loop and data orientation changes described above, and also define a few new types at the same time to make the code easier to work with. type vector interface {\n    // Type returns the type of data stored in this vector.\n    Type() T\n    // Int64 returns an int64 slice.\n    Int64() []int64\n    // Float64 returns a float64 slice.\n    Float64() []float64\n}\n\ntype colBatch struct {\n    size int\n    vecs []vector\n}\n\nfunc (m mulInt64ColOperator) next() colBatch {\n    batch := m.input.next()\n    if batch.size == 0 {\n   \t return batch\n    }\n    for _, c := range m.columnsToMultiply {\n   \t vec := batch.vecs[c].Int64()\n   \t for i := range vec {\n   \t\t vec[i] = vec[i] * m.arg\n   \t }\n    }\n    return batch\n} The reason we introduced the new vector type is so that we could have one struct that could represent a batch of data of any type. The struct has a slice field for each type, but only one of these slices will ever be non-nil. You may have noticed that we have now re-introduced some interface conversion, but the performance price we pay for it is now amortized thanks to batching. Let’s take a look at the benchmark now. $ go test -bench \"BenchmarkColBasedTyped\" -count 10 > tmp && benchstat tmp && rm tmp\nname               \ttime/op\nColBasedTyped-12   \t38.2µs ±24% This is another nearly ~3.5x improvement, and a ~20x improvement over the original row-at-a-time version! Our speed of light benchmark is still about 2x faster than this latest version, since there is overhead in reading each batch and navigating to the columns on which to operate. For the purposes of this post, we will stop our optimization efforts here, but we are always looking for ways to make our real vectorized engine faster. Conclusion By analyzing the profiles of our toy execution engine’s code and employing the ideas proposed in the MonetDB/x100 paper, we were able to identify performance problems and implement solutions that improved the performance of multiplying 65,536 rows by a factor of 20x. We also used code generation to write templated code that is then generated into specific implementations for each concrete type. In CockroachDB, we incorporated all of the changes presented in this blog post into our vectorized execution engine. This resulted in improving the CPU time of our own microbenchmarks by up to 70x, and the end-to-end latency of some queries in the industry-standard TPC-H benchmark by as much as 4x. The end-to-end latency improvement we achieved is a lot smaller than the improvement achieved in our toy example, but note that we only focused on improving the in-memory execution of a query in this blog post. When running TPC-H queries on CockroachDB, data needs to be read from disk in its original row-oriented format before processing, which will account for the lion’s share of the query’s execution latency. Nevertheless, this is a great improvement. In CockroachDB 19.2, you will be able to enjoy these performance benefits on many common scan, join and aggregation queries. Here’s a demonstration of the original sample query from this blog post, which runs nearly 2 times as fast with our new vectorized engine: oot@127.0.0.1:64128/defaultdb> CREATE TABLE inventory (id INT PRIMARY KEY, price FLOAT);\nCREATE TABLE\n\nTime: 2.78ms\n\nroot@127.0.0.1:64128/defaultdb> INSERT INTO inventory SELECT id, random()*10 FROM generate_series(1,10000000) g(id);\nINSERT 100000\n\nTime: 521.757ms\n\nroot@127.0.0.1:64128/defaultdb> EXPLAIN SELECT count(*) FROM inventory WHERE price * 0.8 > 3;\n       tree      |    field    |     description\n+----------------+-------------+---------------------+\n                 | distributed | true\n                 | vectorized  | true\n  group          |             |\n   │             | aggregate 0 | count_rows()\n   │             | scalar      |\n   └── render    |             |\n        └── scan |             |\n                 | table       | inventory@primary\n                 | spans       | ALL\n                 | filter      | (price * 0.8) > 3.0\n(10 rows)\nTime: 3.076ms The EXPLAIN plan for this query shows that the vectorized field is true , which means that the query will be run with the vectorized engine by default. And, sure enough, running this query with the engine on and off shows a modest performance difference: root@127.0.0.1:64128/defaultdb> SELECT count(*) FROM inventory WHERE price * 0.8 > 3;\n   count\n+---------+\n  6252335\n(1 row)\nTime: 3.587261s\n\nroot@127.0.0.1:64128/defaultdb> set vectorize=off;\nSET\nTime: 283µs\n\nroot@127.0.0.1:64128/defaultdb> SELECT count(*) FROM inventory WHERE price * 0.8 > 3;\n   count\n+---------+\n  6252335\n(1 row)\nTime: 5.847703s In CockroachDB 19.2, the new vectorized engine is automatically enabled for supported queries that are likely to read more rows than the vectorize_row_count_threshold setting (which defaults to 1,024). Queries with buffering operators that could potentially use an unbounded amount of memory (like global sorts, hash joins, and unordered aggregations) are implemented but not yet enabled by default. For full details of what is and isn’t on by default, check out the vectorized execution engine docs . And to learn more about how we built more complicated vectorized operators check out our blog posts on the vectorized hash joiner and the vectorized merge joiner . Please try out the vectorized engine on your queries and let us know how you fare. Thanks for reading!", "date": "2019-10-31"},
{"website": "CockroachLabs", "title": "Yugabyte vs CockroachDB: Unpacking Competitive Benchmark Claims", "author": ["Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/", "abstract": "Competition is good. It focuses your energies. It keeps you sharp and pushes you to be better. At Cockroach Labs we welcome competition. It hasn’t escaped our notice that a new entrant in the database landscape is frequently comparing themselves against us, making claims about their performance and functionality vis-a-vis our own. We decided to take a closer look. This is our analysis of CockroachDB vs Yugabyte v2.0.0. All benchmark comparisons were done with CockroachDB v19.2.0 . TLDR: CockroachDB vs Yugabyte On Yugabyte’s custom benchmark we discovered an apples to oranges performance comparison due to the behavior of the benchmark and Yugabyte’s choice to use hash partitioning by default. We unpack why using hash partitioning by default is a dubious choice for a SQL database. After adjusting for these discoveries, CockroachDB outperforms Yugabyte on all of the workloads. On Sysbench we discovered problems in Yugabyte with schema changes and batching. The schema change problems are architectural and will be challenging to solve. The performance problems with batching look to be solvable with a bit of elbow grease. On Sysbench we discovered that Yugabyte returns transaction retry errors in situations that neither PostgreSQL or CockroachDB do. These transaction retry errors cause Sysbench to fail to complete on several workloads. We were only able to gather results for Yugabyte on 4 of the 9 Sysbench workloads. CockroachDB can successfully complete all 9 workloads. On YCSB we compared Yugabyte’s SQL performance to CockroachDB and showed CockroachDB performance is better across all workloads. We discovered that Yugabyte SQL tables have a maximum of 50 tablets. We discovered that Yugabyte tablets do not split or merge which requires the operator to make an important upfront decision on their data schema. We discovered that a Yugabyte range partitioned table is limited to a single tablet which limits the performance and scalability of such tables. We discovered differences between CockroachDB’s distributed SQL execution which decomposes SQL queries in order to run them close to data, and Yugabyte’s SQL execution which moves data to a gateway node for centralized processing. We discovered that without application-level coordination Yugabyte schema changes can easily lead to data inconsistencies. An Initial Look at the Yugabyte Benchmark Our first step in analyzing Yugabyte’s claims was to take a look at their custom benchmark. The Yugabyte defined benchmark comes with three different workloads: SqlInserts : sample key-value app with concurrent writers SqlSecondaryIndex : sample key-value app with secondary index SqlForeignKeysAndJoins : sample user orders app We set up a 3-node cluster across multiple availability zones on AWS using the c5d.4xlarge machine type, with a 4th node running the benchmark itself (NOTE: this is the same setup we used for all of the benchmarking described in this analysis). When we ran the SqlInserts workload, we saw that Yugabyte’s throughput hovered around 58k qps while CockroachDB’s throughput hovered around 35k qps. Digging in, a quick look at CPU metrics show that Yugabyte is making full use of the 3 node cluster while 2 of the 3 Cockroach nodes are mostly idle. What causes this imbalance? First off, what is the SqlInserts workload doing? We dove into the source code and found that it is inserting keys in sequential order. This is surprising, as the code specifically mentions that isn’t the case, but sometimes comments become out of date. Regardless, we determined that keys were being written in sequential order to the database, meaning that all writes were going to a single range in CockroachDB. This is a classic case of a write hotspot. So how is Yugabyte getting such good performance? The short answer is that SQL tables in Yugabyte use \" hash partitioning \" by default: rows are spread evenly across a cluster by hashing each row key. By contrast, Cockroach uses what is known as \" range partitioning \": rows are clustered together in chunks that are sorted by key. One way to think about hash vs range partitioning is that the key-value store in Cockroach is implemented as a distributed balanced tree (most closely resembling a B+ tree) and the key-value store in Yugabyte is implemented as a distributed hash map. Defaulting to hash partitioning is a dubious choice, as it differs from the PostgreSQL default and the normal expectations for a SQL table. Many NoSQL systems use hash partitioning which is a primary factor in the difficulty of implementing full SQL on top of such systems. The performance difference between hash and range partitioning can vary from modest to significant depending on the workload. Hash partitioning transforms sequential inserts into random inserts, thus providing good load distribution. Reading of specific rows is also fast with hash partitioning. The big caveat to hash partitioning is that any sort of range scan will be very slow. We’ll see this come up in later benchmarks . Load balancing and load hotspots are fairly common concerns in distributed systems. Even though distributed systems may scale horizontally in theory, developers still need to be careful to access them in ways that can be distributed. In a range partitioned system, load can become imbalanced if it focuses on a specific range of data. In a hash partitioned system, load can become imbalanced if it focuses on a specific hash bucket due to hash collisions. In the SqlInserts workload, we see that all the load is focused on a single range in CockroachDB because the workload was writing to sequential keys. This would be considered an anti-pattern in CockroachDB, and it’s why we recommend avoiding such access patterns in our documentation. CockroachDB offers convenient datatypes like UUIDs to avoid such issues. Hash partitioning is easy to implement on top of range partitioning, so it’s straightforward to solve the hotspot issue in CockroachDB by prefixing the SQL primary key with a hash of the user-provided primary key (via a computed column). In addition to running CockroachDB in this \"Hash\" mode, we also configured and ran Yugabyte using range partitioning to allow comparisons of all of the apple varieties against each other. We ran this test and obtained the following results: These numbers show that hash partitioning is a clear winner for the insert workloads, though range partitioning delivers the top performance for the SqlForeignKeysAndJoins workload. When comparing apples-to-apples configurations (Range vs Range or Hash vs Hash), CockroachDB outperforms Yugabyte in all three workloads. Throughput is not the only measure of performance. We also have to consider latency. Latency and throughput are often correlated, though the relationship can become complicated. Higher latency generally corresponds with lower throughput, though a system which accommodates concurrency can upset that generalization. In the following benchmark, we measure average latency at peak throughput. The peak throughput for CockroachDB and Yugabyte depend on the concurrency of the workload. Once that peak throughput is reached, further increases to concurrency only increase latency without providing a corresponding improvement in throughput (in fact, throughput may decrease due to queueing overhead). In our experimentation, both CockroachDB and Yugabyte achieved peak throughput on the SqlInserts workload with 192 concurrent workers for range partitioning and 256 concurrent workers for hash partitioning. For the other workloads, both systems achieved peak throughput with 128 concurrent workers. Again, CockroachDB outperforms Yugabyte in all three workloads. A takeaway from this initial analysis is that benchmarking is hard to do correctly. It is often easy to construct a benchmark which is not testing what you initially set out to test. Databases have particularly complex modes of operation and simple benchmarks are inadequate at best, and better avoided. Left to their own devices, database vendors often select workloads that produce favorable numbers for their system. Yugabyte went a step beyond that, by creating their own custom, unvetted benchmark, rather than relying on any one of the standard, third-party benchmarks for database systems. Sysbench We next turned our attention to Sysbench, a standard benchmarking suite that contains a collection of simple SQL workloads. These workloads perform low-level SQL operations like running concurrent INSERT statements as fast as possible or UPDATE-ing rows as fast as possible. The idea of the benchmark is primarily to get a picture of the capacity of a system under different access patterns. Unlike TPC-C , Sysbench does not attempt to model a real application, which makes it a poor benchmark for holistic database performance comparisons. However, its popularity makes it worthwhile to take a closer look. Sysbench is split into the following workloads: oltp_point_select : single-row point selects oltp_insert : single-row inserts oltp_delete : single-row deletes oltp_update_index : single-row update on column that requires update to secondary index oltp_update_non_index single-row : update on column that does not require update to secondary index oltp_read_only : transactions that run collection of small scans oltp_read_write : transactions that run collection of small scans and writes oltp_write_only : transactions that run collection of writes Data Loading Running a sysbench workload is split into two stages. First, we create the schema and load a dataset into the cluster. Next, we run the workload. Each workload has slightly different data loading requirements. We started with oltp_point_select and loaded a dataset with 10 tables and 1,000,000 rows per table. On CockroachDB, the data loading finished in 30s. On Yugabyte, we ran into problems. The first problem was a \"Catalog Version Mismatch\" error that we tracked down to being caused by performing multiple schema changes at the same time. To understand this problem we have to take a detour . Distributed SQL Architectures CockroachDB and Yugabyte are both distributed SQL databases that strive for PostgreSQL compatibility, though they have taken different roads to that goal. CockroachDB: Distributed SQL Execution CockroachDB implements a fully-distributed SQL engine, including both optimization and execution. The SQL implementation has been architected and built from the ground up to work on top of CockroachDB’s internal distributed KV store. The from scratch implementation allows the execution of SQL to be distributed by decomposing a query into parts that can be executed anywhere in a cluster, usually as close to where the data is stored as possible. This enables CockroachDB to exploit the aggregate compute resources of the cluster for executing a single query. CockroachDB delivers a \"code-shipping\" instead of a “data-shipping” architecture. This is of vital importance in a distributed architecture. Yugabyte: Monolithic SQL Execution on a Distributed KV Store Yugabyte uses PostgreSQL for SQL optimization, and a portion of execution. The integration with PostgreSQL is done by adding a new LSM ( Log-Structured Merge tree ) access method to PostgreSQL, and replacing PostgreSQL’s normal heap storage system with Yugabyte-implemented routines. (Note that in this context, heap storage refers to the on-disk structure used to store table data, not to the in-process component for managing memory). Both the LSM access method and heap storage are implemented on top of Yugabyte’s DocDB distributed KV store. Yugabyte has modified PostgreSQL so that the normal BTREE and HASH access methods are redirected to the LSM access method. Using PostgreSQL in this manner allows Yugabyte to add SQL support without implementing distributed SQL. However, implementing a new access method for PostgreSQL is not a trivial task, and Yugabyte’s current implementation doesn’t yet provide the same semantics as the builtin Postgres access method. Rather than a distributed SQL database, Yugabyte can be more accurately described as a monolithic SQL database on top of a distributed KV database. The \"Catalog Version Mismatch\" problem noted above is related to these architectural differences and is the result of schema change issues. It is a direct consequence of their decision to use the PostgreSQL frontend. Because PostgreSQL is not distributed, its only link to other concurrently running PostgreSQL frontend processes is through the LSM access method to the Yugabyte backend. There’s no coordination during schema changes, which leads to problems like the “Catalog Version Mismatch” error, and can also be demonstrated via a simple script which runs a schema change on one node and a concurrent operation on another node. Because there is no coordination, the two operations can conflict. Below is a simple script with two clients talking to two Postgres server instances that highlights this issue: Node 1 Node 2 1 CREATE TABLE t (k INT PRIMARY KEY) 2 INSERT INTO t (SELECT FROM generate_series(1, 100)) 3 SELECT COUNT(*) FROM T -> 100 4 BEGIN 5 INSERT INTO t VALUES (0) 6 BEGIN 7 CREATE INDEX t_idx ON t (k ASC) 8 COMMIT 9 COMMIT 10 SELECT COUNT(*) FROM t -> 101 11 SELECT COUNT(*) FROM (SELECT FROM t ORDER BY k) as foo -> 100 12 SELECT MIN(k) FROM t -> 1 What happened? We created a table, added some data, and then added an index. While the index was being added, we concurrently inserted into the table on another node. The end result was an index that is out of sync with the main table data. Handling schema changes online is possible, though it requires significant engineering effort. With CockroachDB, the above script, and any other contrived example you can think of, perform correctly without locking any tables, using online schema changes . Data Loading Redux Given Yugabyte’s limitations with concurrent schema changes, we have two options in running Sysbench: we either a) load with a concurrency of 1 to side-step the issue or b) we abandon running across multiple tables (multiple tables are the norm for sysbench). Since the data load step would take too long to load with a concurrency of 1, we opted to run with only a single table. To maintain the same number of rows, we must increase to using 10,000,000 rows in the table. CockroachDB completed the data loading in 2m42s. After 30m, we cancelled the load with Yugabyte. We reduced the number of rows to 1,000,000. At 1,000,000 rows, CockroachDB’s load completed in 28s, Yugabyte’s in 39m. What explains this significant performance difference in load times? During the loading phase, Sysbench issues INSERT statements with 2000 rows each. That is, there is a single INSERT statement specifying 2000 rows via a VALUES clause. On Yugabyte, the performance of the INSERT statement appears to slow down linearly with the batch size. We ran some experiments here and verified this by loading 1000 rows and 2000 rows with a concurrency of 1. After subtracting out the table creation time, the former took 9.3s and the latter took 16.2s. The Yugabyte code (see ExecModifyTable ) loops over the rows to be inserted and performs the inserts one at a time. There is no fundamental reason for this lack of batching; Yugabyte will likely address this with time, though this example serves to highlight the large performance surface area of a SQL database. Single row inserts can be fast, while multi-row inserts are comparatively slow. In CockroachDB, batching is performed whenever possible, as reducing network operations is a big part of optimizing performance in a distributed database. For completeness, here are relative loading times we measured between the two systems. CockroachDB YugabyteDB Difference 100 tables, 100 rows, 1 thread 2.876s 8m24.786s 175x slower 1 table, 10,000 rows, 1 thread 0.639s 0m29.508s 49x slower 1 table, 10,000 rows, 64 threads 0.556s 0m35.831s 64x slower 1 table, 100,000 rows, 64 threads 2.591s 2m12.280s 51x slower 1 table, 1,000,000 rows, 64 threads 27.920s 39m0.027s 84x slower To keep the loading times reasonable, we decided to run the Sysbench tests with very small tables containing only 100,000 rows. Running Sysbench Workloads For each workload, we wiped the clusters, loaded the data, ran the benchmark for a 2 minute warmup period to prime the system, and then ran for the official 2 minutes. We ran the benchmarks both with and without secondary indexes. Note that priming the system with a ramp period is important for a number of reasons: populating data caches, populating metadata caches, and in CRDB allows load-based splitting and rebalancing to occur. The first thing to notice is that this chart only lists 4 workloads, instead of the 9 mentioned earlier. The reason for this is because any Sysbench workload that runs UPDATE statements causes retryable errors to be returned from Yugabyte, even when these errors are hit in single-statement transactions. The PostgreSQL client used by Sysbench cannot handle these retryable errors. Also note, for the workloads we could run, we used snapshot isolation for Yugabyte as this is their default. We use serializable isolation for CockroachDB. CockroachDB makes every effort to avoid returning retryable errors to the client when they can be retried transparently by the database. This is difficult for Yugabyte because they don’t have full control over their SQL engine (i.e. the PostgreSQL frontend). We saw errors with Yugabyte running the oltp_update_index , oltp_update_non_index , oltp_read_write , and oltp_write_only workloads so we’ve excluded them from the remainder of this analysis. Ignoring those issues, we looked at each of the four workloads that Yugabyte can run. The first workload is oltp_point_select. This workload runs SQL statements that look like SELECT c FROM t WHERE id=? . Remember that because we’re running on such a small dataset, all of these lookups should hit RocksDB’s block cache in both databases. Yugabyte’s performance slightly edges out CockroachDB’s. The oltp_insert workload runs SQL statements that look like INSERT INTO c VALUES (...) . Again, Yugabyte’s throughput is slightly higher than CockroachDB’s. Overall, Yugabyte does an admirable job with these single row reads and writes. The oltp_insert_2nd_idx workload is similar to the oltp_insert workload except that there is a secondary index on the table being inserted into. In both Yugabyte and CockroachDB, this results in the implicit transactions surrounding the INSERT statements becoming distributed transactions which voids the single-range/single-tablet transaction fast-path both systems contain. CockroachDB’s throughput in this workload is significantly higher than Yugabyte’s. This is evidence that CockroachDB’s transaction commit protocol is significantly more efficient for such cross-range/tablet transactions which has a significant impact on real world applications. Finally, let’s take a look at oltp_read_only workload. This workload is one of three sysbench workloads that run multi-statement transactions (the other two hit retryable errors). The transactions in this workload are a combination of the following five statements: SELECT c FROM t WHERE id=?\nSELECT c FROM t WHERE id BETWEEN ? AND ?\nSELECT SUM(k) FROM t WHERE id BETWEEN ? AND ?\nSELECT c FROM t WHERE id BETWEEN ? AND ? ORDER BY c\nSELECT DISTINCT c FROM t WHERE id BETWEEN ? AND ? ORDER BY c CockroachDB’s performance is 340x higher than Yugabyte’s. Why is this? Well, this is where hash partitioning reveals its weakness. Due to the use of hash partitioning, simple range scans as performed by oltp_read_only turn into table scans. To complete the picture, we hacked up some scripts so that we could use range partitioned tables in Yugabyte. As expected, Yugabyte’s performance on oltp_read_only increased significantly because it could now take advantage of the ordered primary key index. Unfortunately, that improvement came at the price of significantly reduced performance on the other workloads. Again, when both systems are using range partitioning, CockroachDB exhibits significantly higher throughput across all workloads. Large Scans We saw in the previous tests that the performance of ordered scans was dramatically impacted by the choice of partitioning scheme used in Yugabyte. Range partitioning resulted in significantly faster scans than hash partitioning. This is why CockroachDB chose range partitioning as its default scheme. The oltp_read_only workload provides the ability to adjust the size of the scans that it performs through its --range_size flag. The flag defaults to 100 rows, but other sizes of scans are also of interest. Using the flag and variable table sizes, we re-ran the tests to get an understanding of how each partitioning scheme performs and scales across total table size and scan size. This resulted in the following output: There are a few takeaways from this experiment: Hash partitioning causes the cost of scans to scale linearly with respect to the size of the table . We can see this because the throughput is constant when using hash partitioning for a given table size. However, the throughput scales inversely proportional to the table size. Growing the table from 10k rows to 100k rows reduces throughput by 10x. This is what we would expect from a hashing structure, because even small scans need to scan every single row. Range partitioning causes the performance of scans to scale linearly with respect to the size of the scan . We can see this because the throughput is roughly constant when using range partitioning for a given scan size. However, the throughput scales inversely proportional to the scan size. This is what we would expect from an ordered structure, where only the requested rows are scanned. CockroachDB outperforms Yugabyte with either partitioning scheme across the board. This is especially true on large scans, where the difference grows to 6x the throughput. YCSB The Yahoo! Cloud Serving Benchmark is another industry standard benchmark that is interesting to look at. The benchmark, which was originally introduced with this paper , is often used to compare relative performance of NoSQL, but has grown to benchmark a wide range of database systems. During this comparison, we used the YCSB implementation built into CockroachDB’s workload tool. The implementation has been built to mirror the official YCSB workload as faithfully as possible, but these results should be understood as not using the official YCSB workload tool. Yugabyte has published YCSB numbers, but only when using their CQL (Cassandra Query Language) interface. We’ve restricted our analysis to SQL vs SQL in order to provide an apples-to-apples comparison. SQL on top of KV The only modification made specifically for CockroachDB was the option to configure the workload’s table with a column family per column. What is a column family? In CockroachDB, by default a row in a SQL table becomes a single key/value record. The columns that are part of the primary key for the table are encoded into the KV-level key, and the remaining columns are encoded into the value. Column families can be specified at table creation time to group columns into separate key/value records which can be used to reduce contention, and to reduce write traffic if only a portion of a row is frequently modified. The default of a single key/value record per row provides better read performance in common workloads. In Yugabyte, a row in a SQL table becomes N key/value records where N is the number of columns that are not part of the primary key. The columns that are part of the primary key are encoded into the prefix of the KV-level key, and the column’s ID is encoded as the suffix. The column value is stored in the KV-level value. Using N key/value records per row negatively impacts scan speed. Using a column family per column is the recommended way of running YCSB against CockroachDB. Doing so actually eliminates a difference between CockroachDB’s and Yugabyte’s defaults. YCSB Results As mentioned above, we configured CockroachDB to use column families. This is an important optimization for heavily contended workloads like YCSB’s workload A, where it can reduce interactions between transactions that operate on disjoint columns in a row. Without this configuration, CockroachDB’s performance on workload A drops roughly in half (\\\\\\~14k). However, its performance increases on workloads C and D (\\\\\\~90k on C and ~80k on D). This demonstrates some of the tradeoffs with using column families. Yugabyte doesn’t provide this configuration and always behaves as if every column is placed in its own column family. TPC-C With any benchmark, the composition of the workloads that are tested have a huge impact on the results. This is why self-created benchmarks are highly susceptible to tampering. In response to this, the database industry came together to create the Transaction Processing Council (TPC) benchmarks. TPC, an industry consortium, created a benchmark specifically for OLTP workloads called TPC-C. Despite being created in 1992, it’s still the most mature and relevant industry standard measure for OLTP workloads. The TPC benchmarks are far more rigorous in their specification than other benchmarks, even YCSB and Sysbench which we used above. While TPC-C was developed years ago it has withstood the test of time. \"Good benchmarks are like good laws. They lay the foundation for civilized (fair) competition.\" - tpc.org We run TPC-C nightly as part of our standard testing of CockroachDB, and we periodically test to see where our performance envelope lies. We’ve demonstrated steady progress, hitting 10k warehouses in March, 2018, then 50k warehouses in November, 2018. In our most recent measurement we reached 100k warehouses . In TPC-C, higher performance numbers require scaling both the throughput and the dataset size while adhering to latency requirements. The cumulative effect is to stress multiple facets of a database. Unfortunately, running TPC-C is a complex endeavor. TPC-C is not actual software, but a specification for a benchmark with requirements on the operations to be performed, the size of the data, the randomness of the data, and the randomness of the operations. CockroachDB’s implementation of TPC-C utilizes functionality not present in Yugabyte and we made a determination that porting it to work on Yugabyte and doing the requisite work to tune Yugabyte for TPC-C was beyond the effort we were willing to invest. Architectural Differences We’ve already highlighted a few architectural differences between CockroachDB and Yugabyte but we came across several more while performing the above benchmarks. SQL tables and indexes are broken down into ranges (CockroachDB) and tablets (Yugabyte). Ranges/tablets are replicated via Raft. Distributed transactions are provided across arbitrary ranges/tablets. Replicas of ranges/tablets store data in RocksDB (a single-node key-value store). (*Since this post was published, CockroachDB built their own open source KV store called Pebble . Pebble is inspired by RocksDB, but more closely aims at the distributed SQL needs of CockroachDB.) CockroachDB is a single binary (cockroach) which acts as the SQL gateway, SQL execution engine, and KV storage system. Yugabyte is composed of 3 processes: postgres, yb-master, and yb-tserver (tablet server). The postgres process performs SQL execution, while the yb-master and yb-tserver processes implement the distributed, transactional KV store (DocDB). The yb-master process handles cluster wide metadata such as the location of tablets. The yb-tserver (a.k.a. yb-tablet-server) process handles tablet operations and replication. Ranges vs Tablets In CockroachDB, ranges split, merge, and rebalance automatically throughout the cluster in order to adjust to the workload. A SQL table is composed of 1 or more ranges. Data within a table is range partitioned which means that adjacent rows in a table or index become adjacent records in the ranges. Ranges are indexed which allows tables and indexes to be scanned in the order defined by the indexed columns. Yugabyte tablets rebalance automatically in the cluster in response to node outages, but they have no provision for splitting or merging. The number of tablets for a table is set at table creation time and is currently unalterable afterwards. The default number of tablets for a table is twice the number of tablet servers in the cluster, though this is configurable via extensions to the CREATE TABLE syntax. There is currently a maximum of 50 tablets per table, though it is unclear what the fundamental factors behind that limit are. As noted above, Yugabyte defaults to hash partitioning for tables and indexes. With hash partitioning, logically adjacent rows in a table can map to different tablets. This default differs from the PostgreSQL default and may cause significant user surprise. Hash partitioned tables are fast for single key lookup, but extremely slow if a range scan is performed. A range partitioned table can be requested by indicating the sort direction for a column in an index (either ASC or DESC). Range partitioned tables improve range scan performance, but come with a large caveat at this time: range partitioned tables can only have a single tablet. This means that entire range partitioned table is stored on a single machine, limiting both the size and performance of the table. As mentioned earlier, hash partitioning can be built on top of range partitioning, but the reverse is not true. Transactions Yugabyte’s DocDB provides support for distributed transactions. The functionality and limitations of the DocDB transactions are directly exposed to SQL. For example, DocDB transactions provide no pessimistic locks for either read or write operations. If two transactions conflict, one will be aborted. This behavior is similar to the original CockroachDB transaction support. Correctness is simple, but performance is problematic under contended workloads. Besides performance, aborting one of the transactions during conflicts requires the application to add retry loops to their application code. CockroachDB now has a pessimistic locking mechanism to improve the performance under contention and to reduce user-visible transaction restarts. The general structure of distributed transactions in CockroachDB and Yugabyte is similar. Transactions have an associated transaction record stored in a KV record. Writes performed within the transaction add a marker to the written row (intent in CockroachDB, provisional write in Yugabyte). The transaction is committed via flipping the commit bit in the transaction record which can be done atomically via a Raft write. After the transaction record has been marked as committed, the intents / markers are cleaned up asynchronously. The above transaction protocol is correct, but it suffers from multiple Raft consensus round-trips: one per statement in the transaction. CockroachDB’s current protocol has parallelized every step except the cleanup of intents. In CockroachDB 19.2, a multi-statement distributed transaction can be committed with the latency of a single Raft consensus round-trip (multiple Raft consensus operations are performed in parallel). Database Schema Changes In CockroachDB, schema changes are performed asynchronously via the same bulk loading mechanisms that are used for restore and import. Adding an index is done via a distributed operation which scans the rows of the table and builds up a series of RocksDB sstables which are ingested. Building sstables externally from RocksDB and then ingesting them is significantly faster than going through the front door and performing normal RocksDB write operations. In CockroachDB, schema changes are \"online\": data in the table can be manipulated (rows added or deleted) while the schema change is taking place. This is accomplished by carefully controlling the caching of table metadata on each node and decomposing a schema change into steps for which it is valid to have different versions of the metadata in use at the same time. In Yugabyte, schema changes are performed via the normal PostgreSQL access method interface. For example, adding an index performs a scan on the PostgreSQL frontend of the existing rows in the table and for each row adding an entry to the index. The addition of each row is performed via an RPC to Yugabyte’s DocDB. In Yugabyte, schema changes need to be coordinated by the application. Concurrent addition of an index on one node and modifications to a table on another node can result in an inconsistent index. Conclusion CockroachDB is a production-ready, distributed SQL database, architected to provide high levels of performance, correctness, and stability. Yugabyte is a SQL database grafted onto a distributed KV database. The above analysis details performance of the two systems on a variety of benchmarks, unpacking Yugabyte’s claims and showing the architectural differences between the two systems. Note: We will publish reproduction steps for the primary tests in this document and will update this post as soon as they are available.", "date": "2019-11-19"},
{"website": "CockroachLabs", "title": "Parallel Commits: An Atomic Commit Protocol For Globally Distributed Transactions", "author": ["Nathan VanBenschoten"], "link": "https://www.cockroachlabs.com/blog/parallel-commits/", "abstract": "Distributed ACID transactions form the beating heart of CockroachDB. They allow users to manipulate any and all of their data transactionally, no matter where it physically resides. Distributed transactions are so important to CockroachDB's goal to \"Make Data Easy\" that we spend a lot of time thinking about how to make them as fast as possible. Specifically, CockroachDB specializes in globally distributed deployments, so we put a lot of effort into optimizing CockroachDB's transaction protocol for clusters with high inter-node latencies. Earlier this year, we published a blog post about a new feature called Transactional Pipelining in our 2.1 release . The feature reworked CockroachDB's transaction protocol to pipeline writes within interactive transactions in order to reduce their end-to-end latency. Customers saw a marked improvement in the speed of their transactions when this feature was released. We closed out that discussion with a preview of how we planned to continue the work of speeding up global transactions. To do so, we needed to take a hard look at the atomic commit protocol we used when committing transactions. We found that it was overly rigid and hindered performance when run over a collection of individually replicated consensus groups (i.e. “ranges” in CockroachDB). So we went back to the drawing board and developed a new atomic commit protocol optimized for globally distributed transactions running in a system of partitioned consensus groups. This post will explore the new atomic commit protocol, which we call Parallel Commits. Parallel Commits will be part of CockroachDB's upcoming 19.2 release. The feature promises to halve the latency of distributed transactions by performing all consensus round trips required to commit a transaction concurrently. The post will pick up where the previous post about transactional pipelining left off. Readers who have read that blog post will be able to jump right into this one. Readers who haven't may want to skim through that post to get a feel for the cluster topology assumptions we will continue to make here. Transaction Atomicity Atomicity is likely the first property that people think of when they hear the word \"transaction\". At its core, a transaction is simply a collection of operations, and it is said to be atomic if all of its composite operations succeed or fail together as a single unit. In other words, atomicity ensures that a transaction is \"all or nothing\" - it either commits or rolls back instantaneously. Databases typically ensure atomicity through a delicate dance of locking, indirection, and coordination between conflicting transactions. The details of how this all fits together is specific to each database implementation, but they each share common themes. For example, in PostgreSQL , individual tuples (i.e. versioned rows) each contain the ID of the transaction that wrote it. When a transaction is still in progress, no other transaction should consider these provisional tuples to be \"visible\", as doing so would be a violation of isolation at even the weakest isolation levels. PostgreSQL enforces this by maintaining each transaction's disposition in a single place, the commit log (\"clog\"). When a transaction finds a tuple from a possibly live conflicting transaction, it checks the commit log to determine whether the tuple is visible or not. When a transaction commits, its changes should all suddenly become visible at once. How is this accomplished? In PostgreSQL, the trick is to write the fact that the transaction committed to the commit log . This ensures that from that point on, any conflicting transaction that runs into one of its tuples will observe the transaction as committed when it checks the commit log. PostgreSQL then updates each tuple with a \"hint bit\" to inform conflicting transactions that the tuple is committed and that no commit log lookup is necessary. This last part is strictly an optimization and not necessary for atomicity. When we think about this dance carefully, we realize that the challenge of atomicity isn't one of performing multiple changes simultaneously. Instead, it's fundamentally a game of managing \"visibility\" of the transaction's operations such that all of a transaction's operations appear to be committed (or rolled back) instantaneously to all observers. Atomicity in CockroachDB In a distributed database like CockroachDB, things are fairly similar. The only real difference is that a transaction may perform writes across a series of different machines. We've seen in past blog posts how CockroachDB handles this with an ID-addressable transaction record and write intents that contain their transaction's ID. This is discussed in the CockroachDB architecture documentation and in this blog post . Just like in PostgreSQL, any transaction that stumbles upon a write intent is forced to look up the corresponding transaction record to determine the intent's visibility. Also similar is the fact that a transaction commit simply flips a bit on its transaction record to mark the transaction as committed. The only difference here is that unlike PostgreSQL's commit log, these transaction records are distributed throughout a cluster instead of centralized to improve locality of these transaction record accesses. Finally, after the transaction commit completes, each of the write intents is \"cleaned up\" asynchronously to avoid unnecessary transaction record lookups. Like hint bits in PostgreSQL, this is a performance optimization and not strictly necessary for atomicity. Let's take a look at the timeline of a transaction in CockroachDB to get a feel for how this behaves over time. For the sake of simplicity, we'll focus on a write-only transaction. Of course, CockroachDB supports generalized read-write SQL transactions, and the handling of read-only statements within a read-write transaction is straightforward due to multi-version concurrency control (MVCC) . Below, we see that CockroachDB navigates SQL transactions by incrementally laying down write intents as each mutation statement is issued. The transaction also writes its transaction record with a PENDING status at the same time as writing its first intent. We say that each of these intent writes is \"pipelined\" because we don't wait for them to succeed before responding to the SQL client and informing it to issue more statements. Eventually, the SQL client issues a COMMIT statement. At this point, CockroachDB waits for all of its intent writes to complete the process of durable replication. Once this is complete, the transaction modifies its transaction record to mark it as committed. This change is also replicated for durability, and the transaction is considered to be committed as soon as this operation completes. The success of the COMMIT statement is then acknowledged to the SQL client. Figure 1: Transaction Timeline, Without Parallel Commits Asynchronously, after the client has been acknowledged, the transaction goes through and resolves each of its intents. Once this has finished, it can delete its transaction record entirely. Or at least, this is how transaction atomicity has worked in CockroachDB up until version 19.2. The Problem with Two-Phase Commit Those familiar with distributed systems may recognize this multi-step process as a variation of the two-phase commit protocol (2PC) . Specifically, the process of writing intents is analogous to the \"prepare\" phase of 2PC and the process of marking the transaction record as committed is analogous to the \"commit\" phase of 2PC. One commonly cited problem with two-phase commit is that it is \"blocking\". After each participant has voted to commit but before the transaction coordinator has made its final decision, the system enters a fragile state. If the transaction coordinator were to crash, then it would be impossible for others to know the final outcome of the transaction. This would stall the transaction itself and any that conflicted with it. To avoid this issue, systems like CockroachDB run 2PC on top of a consensus protocol, ensuring that transaction state is just as highly available and resilient to failure as the rest of the system. Specifically, in CockroachDB this means that a transaction's transaction record is replicated in the same way that its write intents are. As we demonstrated in the previous blog post , the major downside to this strategy, when run in a partitioned-consensus system like CockroachDB, is that it requires two sequential rounds of consensus writes. For example, if a transaction writes to two ranges, the writes to each range will achieve consensus concurrently. However, the transaction must wait for both of these writes to succeed before marking its transaction record as committed. This write to the transaction record then requires a second round of consensus. This is why the latency model we derived in the previous post showed the latency of a transaction to be twice the latency of a single consensus write. So we see that this structure of layering two-phase commit on top of a system of disjoint consensus groups (i.e. ranges) is responsible for increased transaction latency, which was something we wanted to get rid of. Proposed Solutions from Academia A number of academic groups have taken notice of this problem and have proposed unique solutions to addressing it. The motivating example for much of this work has been the original Google Spanner paper from 2012 , which also served as an inspiration for CockroachDB. In 2013, Kraska et al. presented MDCC: Multi-Data Center Consistency , which achieves atomic transaction semantics by replicating update options to all storage nodes instead of replicating the update values directly. In MDCC, as soon as the system's application server learns the options for all the records in a transaction, it is able to consider the transaction committed and asynchronously notify the storage nodes to execute the outstanding options. The protocol is then optimized by allowing clients to propose Paxos writes without communicating with a leader and by permitting updates to commute with one another. These two optimizations are made within the framework of Generalized Paxos . Later in 2013, Mahmoud et al. published Low-Latency Multi-Datacenter Databases using Replicated Commit . The paper suggested running Two-Phase Commit multiple times in different data centers while using Paxos to reach consensus among each data center as to whether a transaction should commit. This strategy, called Replicated Commit, avoids several inter-data center communication trips by placing an independent transaction coordinator in each data center. In 2015, Zhang et al. approached the same problem from yet a different angle in TAPIR: Building Consistent Transactions with Inconsistent Replication . The project focused on reducing coordination through the use of a new transaction protocol layered on top of a replication scheme that on its own provides no consistency. The proposed replication technique, called inconsistent replication, provides fault-tolerance without ordering, which allows it to avoid synchronous cross-replica coordination or designated leaders. The transaction protocol is then made aware of the limited guarantees of the replication layer, allowing the system to commit a distributed read-write transaction in a single round-trip. Finally, in 2018, Yan et al. introduced Carousel: Low-Latency Transaction Processing for Globally-Distributed Data . The system reduced the number of sequential wide-area network round-trips required to commit a transaction and replicate its results while maintaining serializability. It did so by targeting 2-round Fixed-set Interactive transactions, where read and write keys must be known at the start of the transaction, but where reads in the first round can influence writes in the second. Each of these projects reduced the latency of transactions running within a system of replication groups in different ways, and each required different trade-offs to its system's transaction model to do so. Ultimately, the projects confirmed that the slowdown inherent with running two-phase commit on top of strong replication can be avoided by rethinking how transactions and replication fit together. However, none of them provided a clear blueprint for how an existing system like CockroachDB could address the problem. For that, we had to create our own solution. Parallel Commits Parallel Commits is CockroachDB's approach to solving this performance problem. The goal of the new atomic commit protocol is to reduce the latency of transactions down to only a single round-trip of distributed consensus. To accomplish this goal, we had to throw away two-phase commit and rework how transactions in CockroachDB arrive at a committed state. Before discussing how Parallel Commits works, let's consider the requirements that we needed to maintain while adjusting our commit protocol. The first was that we needed to continue to enforce the property that transactions can't commit until all of their writes are replicated. This is a critical property for ensuring durability of transaction writes and continuing to provide consistency of each individual write, in the CAP sense of the word. In addition, we needed to maintain the property that the commit status itself is durably replicated through some form of replicated transaction record. We have seen systems forgo this requirement and get themselves in trouble as a result. Simply put, a system can't compromise the availability of transaction commit information without compromising the availability of the entire system. Third, we wanted to maintain the pessimism and eager discovery of transaction conflicts that had influenced our design of transactional pipelining. Early in the development of CockroachDB, we moved away from pure optimistic concurrency control. This was due to its negative effects on the performance of contended workloads and because of the complications that requiring transaction retry loops imposed on client applications. Instead, we moved towards a model more closely aligned with traditional two-phase locking, where each statement in a transaction discovers any conflicting writes synchronously and queues behinds the writers before proceeding. This avoids entire classes of transaction restarts. Finally, we determined that we couldn't use any approach that required the write set for a transaction to be known before the transaction began. This sounds like a strange requirement, but requiring transactions to pre-declare which rows they intend to write to ahead of time is a common theme in various attempts to solve this problem. For example, see some of the academic solutions above, along with more exotic approaches like Thomson et al.'s Calvin: Fast Distributed Transactions for Partitioned Database Systems . Unfortunately, it doesn't work in a SQL system where transactions are interactive. With these requirements in mind, we went back to the drawing board and developed a new atomic commit protocol that we call Parallel Commits. Let's see how it works. Changing the Commit Condition To accomplish its goal while adhering to these requirements, the Parallel Commits protocol changes the condition under which we consider a transaction to be committed. Before, we defined a committed transaction as one with a transaction record in the COMMITTED state. This meant that transactions needed to wait until all of their intent writes had succeeded before beginning the process of writing a transaction record in this state. We can visualize the structure of a committed transaction record like: TransactionRecord{\n    Status: COMMITTED,\n    ...\n} With Parallel Commits, we introduce a new transaction record state called STAGING. The meaning of this state is that the transaction has determined all of the keys that it intends to write to, even if those writes haven’t yet completed successfully. In addition to containing a STAGING status, a transaction record in this state lists the key of each of these in-progress writes. A transaction is considered committed if its record is in this state and an observer can prove that all of the writes listed in its transaction record have successfully achieved consensus. We can visualize the structure of the new style of committed transaction record like: TransactionRecord{\n    Status: STAGING,\n    Writes: []Key{\"A\", \"C\", ...},\n    ...\n} The key here is that an observer of a transaction record in the STAGING state is only allowed to consider the transaction committed if it knows that all of the transaction’s writes have succeeded. This is now a distributed commit condition. For a transaction to be committed, multiple individual conditions must hold, and these conditions are no longer satisfied in a single centralized location. The major benefit we get from this definition is that the transaction's coordinator no longer needs to wait for a transaction's writes to succeed before writing its transaction record in this state. Instead, it can pipeline each of its intent writes as SQL statements are executed and can then immediately pipeline a write to its transaction record that records all of these in-flight intent writes when a COMMIT statement arrives. Pipelining still discovers transaction conflicts eagerly and we don't require a client to declare all writes ahead of time, but the cost of the distributed consensus latency is only paid once. Transactions only pay this cost once because the replication process for all writes, including the transaction record, is parallelized. The transaction's coordinator only begins waiting for any write to succeed once every write has been initiated. After all of these writes have succeeded, the transaction's coordinator can immediately respond to its SQL client, informing it of the successful transaction commit. “A transaction is considered committed if its record is in the STAGING state and an observer can prove that all of the writes listed in its transaction record have successfully achieved consensus.” Let's see what this looks like in practice, using the same SQL transaction we used earlier. As before, CockroachDB navigates SQL transactions by incrementally laying down write intents as each mutation statement is issued. Eventually, the SQL client issues a COMMIT statement. Using the Parallel Commit protocol, CockroachDB doesn't need to wait for the intent writes to succeed before writing the commit marker to its transaction record. Instead, it immediately pipelines a write to the transaction record to move it to a STAGING status. Additionally, this write records the key of each write that the transaction has in-flight. CockroachDB then waits for all of its writes to complete the process of distributed consensus in parallel. Once they have, the transaction is considered committed and the success of the COMMIT statement is acknowledged to the SQL client. Figure 2: Transaction Timeline, With Parallel Commits As before, once the coordinator knows that the transaction has committed, it launches an asynchronous process to clean up the transaction's intents and to delete the transaction record. We can compare this timeline with the one from before to visualize the latency improvement provided by Parallel Commits. Instead of waiting for two synchronous round-trips of distributed consensus before acknowledging a transaction commit, CockroachDB now only waits for one. Status Recovery Procedure The transaction coordinator itself knows when each of its writes has succeeded, so it has little trouble deciding when to send a commit acknowledgment to its client. However, other transactions aren't as lucky. I said above that other observers of a transaction record in the STAGING state can only consider the transaction committed if the observer can prove that all of the transaction’s writes have succeeded. We call this process the \"Transaction Status Recovery Procedure\", and it is the key to providing atomicity on the slow-path of Parallel Commits (e.g. in the presence of machine failures). The procedure works as follows. When a transaction observes a write intent from a conflicting transaction, it looks up the intent’s transaction record, just like it always has. The difference now is that if it finds a STAGING record, the status of the transaction is indeterminate - it could be committed or it could be aborted. To find out, the contending transaction needs to kick off the transaction status recovery procedure. During transaction recovery, each intent write recorded into the STAGING transaction record is consulted to see if it has succeeded. If any intents are missing, we use an in-memory data structure in CockroachDB called the \"timestamp cache\" to prevent the missing write from ever succeeding in the future. We then consider the transaction ABORTED. However, if all intent writes have been successfully replicated, we consider the transaction COMMITTED. Either way, at this point the observer is able to rewrite the transaction record with the new consolidated status to avoid future observers needing to go through the full recovery process. This process is critical for determining the state of transactions while in the STAGING state. Yet, it's fairly expensive, so we'd prefer for no observers of transactions to ever need to go through the process. We avoid this in two ways. First, transaction coordinators asynchronously mark transaction records as COMMITTED as soon as they can. This ensures that transactions move out of the STAGING state soon after committing. Second, transaction coordinators in CockroachDB periodically send heartbeats to their transaction record. This allows contending transactions to determine whether a transaction is still in progress. We use this notion of liveness to avoid ever kicking off the status recovery procedure for any transaction that has not expired. The intuition here is that as long as the transaction's coordinator is still alive, it is faster to let it update its transaction record with the result of the Parallel Commit than it is to go through the whole status recovery process ourselves. Put together, these two techniques mean that in practice the transaction status recovery procedure is only ever run in cases where the transaction coordinator dies. This is where the procedure got its name, because its primary purpose is to recover from an untimely crash, and it is never expected to be run on the hot path of transaction processing. Benchmark Results So with all these changes, what did we gain? Using Parallel Commits, CockroachDB is now able to commit cross-range transactions in half the time it previously was able to. Secondary indexes in SQL provide an easy way to visualize this improvement. Over the past few releases, users of CockroachDB have repeatedly been surprised that the addition of the first secondary index to a table appeared to double the time it took to insert into that table. This was because each index in a SQL table is stored on a separate set of ranges. So the process of adding the first secondary index to a table had the effect of promoting transactions that inserted into the table from single-range transactions into full cross-range transactions. This transition forced CockroachDB to fall back from its \"one-phase commit\" fast-path to the standard two-phase commit protocol. This had the effect of doubling the latency of transactions once the first secondary index was added to a table. Parallel Commit fixes this issue. Now, single-range and cross-range transactions should each take roughly the same amount of time. To demonstrate this, we performed an experiment where we ran INSERT statements to add new rows to a table with a variable number of secondary indexes. The table’s data was replicated across VMs in us-east1-b , us-west1-b , and europe-west2-b . Figure 4: Transaction Latency with a Variable Number of Secondary Indexes Figure 3 demonstrates the median latency of an INSERT statement into this table as the number of secondary indexes on the table is varied. Before Parallel Commits, there was a latency cliff when the INSERT statement's transaction was promoted to a cross-range transaction, which was exactly when the first secondary index was added. After Parallel Commits, this latency cliff disappears. Secondary indexes aren’t the only case that Parallel Commits improves. Any uncontended transaction that spans ranges should expect a similar speedup. The TPC-C benchmark provides another avenue to explore this latency improvement. We've talked about TPC-C frequently in the past and have published guidelines for running the benchmark against CockroachDB here . At the heart of the benchmark is a transaction type called New Order . The New Order transaction simulates the process of recording a customer order into the order entry system of an arbitrary business. In doing so, it performs a series of 4 read statements and 5 write statements, modifying up to 33 rows in the process. Figure 5: TPC-C New Order Transaction Latency with a Variable Inter-Node RTT Figure 4 measures the median latency of New Order transactions in CockroachDB with and without Parallel Commits enabled. In the experiment, we replicate the TPC-C dataset across three VMs and used the tc linux utility to manually adjust inter-node round trip latency. As the inter-node round trip time grows in each case, we see transaction latency grow proportionally due to synchronous replication between nodes in the cluster. However, the slope of the two lines are different. Without Parallel Commits, we see the client-perceived latency of the transaction grow at twice the rate that the round trip time (RTT) grows. With Parallel Commits, we see the latency grow at exactly the same rate as the RTT. The explanation for this is simple. With Parallel Commits, transactions have to wait for only a single round-trip of distributed consensus. Theory and Verification As computer scientists, we weren't satisfied with an implementation that appeared to work and benchmark results that demonstrated the desired speedup. In order to claim the new commit protocol was complete, we felt we needed to formalize the protocol in terms of existing computer science theory and verify that the protocol was sound using formal methods. Theory Something that has always stood out to us as unfortunate is the apparent split in the academic literature between discussion on distributed transactions and discussion on distributed consensus. Even in CockroachDB, we have fallen into the trap of separating our distributed transaction layer from our partitioned replication layer and considering them to be disjoint concerns. We speculate that this divide is because transactions are often tasked with coordinating a series of different changes across participants (i.e. different updates per partition) while consensus is often tasked with coordinating the same change across participants (i.e. replication). During the design of our new atomic commit protocol, Parallel Commits, we were forced to take a step back and think about the process of distributed ordering and agreement holistically. This led us to attempt to unify the concepts of distributed transactions and distributed consensus under a single umbrella. Flexible Paxos Heidi Howard's Flexible Paxos gives us the framework to do so. Flexible Paxos is the simple observation that it is not necessary to require all quorums in Paxos to intersect. It is sufficient to require that the quorum used by the leader election phase will overlap with the quorums used by previous replication phases The implied result of this observation is that not all quorums in Paxos need to be majority quorums. Instead, quorums in Paxos can be arbitrarily sized as long as they meet certain criteria. To formalize this condition, Flexible Paxos defines the terms \"Q1 quorum\", which is a leader election quorum and \"Q2 quorum\", which is a replication quorum. This generalization allows us to frame the problem of a distributed transaction run over partitioned consensus groups as one of hierarchical Paxos consensus. At the lower level of this hierarchy, each replication group performs consensus to come to an agreement about the outcome of individual writes in a transaction: \"can an intent be written?\". At the upper level of this hierarchy, all replication groups perform a form of consensus to come to an agreement about the outcome of a transaction as a whole: \"is the transaction committed or aborted?\". This upper-level outcome is based on the result of individual lower-level outcomes. The intuition behind Flexible Paxos is critical here because it gives us the ability to reconfigure the quorums at each level to our needs. At the lower ( intent-scoped ) level, we want to maximize availability of individual writes so we use majority quorums for both the leader election phase ( Q1 ) and the replication phase ( Q2 ) (the default Raft configuration). Each change at this level operates within the context of a single range, so the process of achieving consensus is simply the process of coordinating the change within the range's replication group. In Raft terminology, each change must be committed to the Raft group's log. Figure 6: The Consensus Hierarchy of Transactions At the upper ( transaction-scoped ) level, we want all nodes to be able to perform possibly conflicting transactions concurrently, so we need to avoid a \"transaction leader\" ( Q1 ). Instead, we opt for a first phase quorum size of a single node, meaning that the first phase of consensus is resolved implicitly on a transaction's coordinator node without any external agreement. Flexible Paxos mandates that this lead to a second phase quorum ( Q2 ) of all participants (i.e. all keys) in a cluster. Of course, in reality, not all transactions touch all keys, and two transactions that touch disjoint sets of keys never conflict. Expressed in a different way, the interaction between two possibly conflicting transactions can be determined fully from their interaction on mutually conflicting keys. We can use this to trivially reduce the second phase quorum size down to just those keys where the transaction actually makes changes. This formalization is fascinating, as it reduces the problem of distributed transactions run over partitioned consensus groups to one of achieving consensus across all keys that the transaction writes to within those consensus groups. This is made possible by treating each individual intent write as a \"vote\" towards the commit of its transaction. This voting process is exactly what we end up doing with Parallel Commits. If all intent writes succeed in achieving consensus then they have all voted for the transaction to commit, so the transaction itself has achieved consensus. This formalization also allows us to quickly answer a number of questions. For instance, a natural question to ask would be whether the need for all participants to vote in transaction-level consensus reduces system availability. The answer here is that all participants in the transaction-level consensus decision are keys, which are already individually replicated for high-availability, so the system remains highly available as a whole. Changing Quorum Sizes This all leads to another question: What if we had a larger Q1 quorum at our transaction level? The Flexible Paxos equation says that if every participant voted for the Q1 quorum at the transaction level then consensus could be achieved without any remote Q2 votes. This means that we could consider a transaction committed before any of its individual writes had achieved consensus. This seems a little out there. To get an intuition for what this would mean in practice, let's ask what it would mean to have a full Q1 quorum at the transaction level. In the terminology of Flexible Paxos, a Q1 quorum is a leadership election quorum, so we would be electing a \"transaction leader\". What do leaders do in distributed consensus? They assign an ordering to operations so that these operations can't conflict. In other words, the transaction leader would sequence transactions before actually executing them so that none of them would ever conflict and abort each other. If we squint, this ends up looking a lot like a deterministic database along the lines of CalvinDB . So this two-level Paxos framework we've built here can be specialized to arrive at CockroachDB or to arrive at CalvinDB, all by changing the size of quorums. That's a pretty cool result and it raises the question of whether there are any useful configurations between these two extremes. Verification In addition to our desire to determine how Parallel Commits fits into the broader landscape of distributed systems theory, we also wanted to formally specify the protocol and prove its safety properties through verification. To do so, we turned to TLA+ , a formal specification language developed by Leslie Lamport. TLA+ has been used to great success to verify systems and algorithms ranging from DynamoDB and S3 all the way to the Raft Consensus Algorithm used by CockroachDB. It just so happened that around the time that we were finishing up work on Parallel Commits, we hosted an internal workshop for Cockroach Labs engineers to learn about TLA+ from Hillel Wayne , the author of Practical TLA+ . Over the course of a week, we developed a formal specification of Parallel Commits with an associated model that asserted the atomicity and durability properties we expected from the new commit protocol. The full specification can be found here in ParallelCommits.tla . Let’s take a look at few short snippets to get a feel for the kinds of safety and liveness invariants it enforces. The spec starts off with a few definitions. ImplicitlyCommitted == /\\ RecordStaging\n                       /\\ \\A k \\in KEYS:\n                         /\\ intent_writes[k].epoch = record.epoch\n                         /\\ intent_writes[k].ts   <= record.ts The ImplicitlyCommitted operator defines what it means for a transaction to be in the new distributed commit state that Parallel Commits introduced. We read this just like we would a mathematical statement: “A transaction is implicitly committed if and only if its record has a STAGING status and for all of its intent writes, each intent write has succeeded with the correct epoch and timestamp”. This is exactly the definition we gave earlier in this blog post, only expressed using formal notation. ExplicitlyCommitted == RecordCommitted The ExplicitlyCommitted operator is more straightforward. It defines what it means for a transaction to be in the traditional centralized commit state. The only condition here is that the transaction’s record must have a COMMITTED status. The spec then defines some correctness properties that must hold for Parallel Commits to be considered correct. \\* If the transaction ever becomes implicitly committed, it should\n\\* eventually become explicitly committed.\nImplicitCommitLeadsToExplicitCommit == ImplicitlyCommitted ~> ExplicitlyCommitted\n\n\\* If the client is acked, the transaction must be committed.\nAckImpliesCommit == commit_ack => ImplicitlyCommitted \\/ ExplicitlyCommitted The first of these properties, ImplicitCommitLeadsToExplicitCommit , is a liveness property. It uses the leads to temporal operator ( ~> ) to assert that if a transaction ever enters the implicit commit state, it eventually enters the explicit commit state. The TLA+ model checker ensures this property holds even in the presence of transaction coordinator failures, so long as some other actor remains around to kick off the Transaction Status Recovery Procedure. The spec indicates that the transaction coordinator process can terminate at any time by marking it as an unfair process. The second property, AckImpliesCommit , is a safety property. It uses the implication propositional-logic operator ( => ) to assert that if a transaction coordinator acknowledges the success of a commit operation to its client then its transaction must be committed. Under no circumstances should the client ever be acknowledged before a transaction has committed successfully. The TLA+ model checker will enumerate the entire state space of the Parallel Commits protocol and check that this invariant holds in all of them. Notice that the invariant doesn’t say whether the transaction is implicitly or explicitly committed, just that the transaction is in either of the two states. However, the previous property ensures that the transaction will eventually make it to the explicit commit state. Additionally, another property in the spec ensures that once a transaction is committed, it stays committed, no matter what. We found that the process of writing this specification gave us more confidence in the Parallel Commit protocol itself and in its integration into CockroachDB. Anyone with access to the TLA+ Toolbox can download the specification and model and run it with the TLA+ model checker. COMMIT In conclusion, Parallel Commits combines with transactional pipelining to provide interactive SQL transactions that complete in a single round-trip of distributed consensus. For geo-replicated clusters, we expect this to result in a dramatic reduction in client-observed latency. The new atomic commit protocol is enabled by default in CockroachDB v19.2, and users should upgrade when 19.2 is released to begin taking advantage of the change. In addition to the work on Parallel Commits, we still have big plans to continue pushing the performance of transaction processing forward in CockroachDB. Over the next few releases, we intend to reduce the contention footprint of transactions, reduce write amplification of transactional writes, improve the facilities CockroachDB provides to localize data accesses within transactions, and much more. If improving the performance of strongly consistent distributed transactions interests you, check out our open positions .", "date": "2019-11-07"},
{"website": "CockroachLabs", "title": "Availability and Region Failure: Joint Consensus in CockroachDB", "author": ["Tobias Grieger"], "link": "https://www.cockroachlabs.com/blog/joint-consensus-raft/", "abstract": "At Cockroach Labs, we write quite a bit about consensus algorithms. They are a critical component of CockroachDB and we rely on them in the lower layers of our transactional, scalable, distributed key-value store. In fact, large clusters can contain tens of thousands of consensus groups because in CockroachDB, every Range (similar to a shard) is an independent consensus group. Under the hood, we run a large number of instances of Raft (a consensus algorithm), which has come with interesting engineering challenges. This post dives into one that we’ve tackled recently: adding support for atomic replication changes (“Joint Quorums”) to etcd/raft and using them in CockroachDB to improve resilience against region failures. A replication change is a configuration change of a Range, that is, a change in where the consistent copies of that Range should be stored. Let’s use a standard deployment topology to illustrate this. The above deployment has three regions (i.e. data centers). CockroachDB enables globally deployed applications, so these regions may well be placed across the globe. We see that there are two nodes in each of the regions X, Y and Z, and we see a Range which has one replica (“copy”) in each of the regions. This deployment survives the failure of a single region: consensus replication will continue to work as long as a majority of replicas are available. If a region fails, we lose at most one replica and two - a majority - remain, so the database will continue to operate normally after a short timeout. If we placed two replicas in, say, Z and the third replica in Y, a failure of region X would take two of the replicas with it, leaving only a single replica available; this single survivor would not be able to serve requests: CockroachDB dynamically adjusts the data placement to account for shifts in node utilization. As a result, it may want to laterally move the replica from one node to another within, say, region X. To make it concrete, let’s say we want to move it from X_2 to X_1 . We may want to do this because the operator has specified that X_2 should go down for maintenance, or X_2 has much higher CPU usage than X_1 . As the diagram shows, the Raft group under consideration is initially located on X_2, Y_1, Z_2 . The active configuration of this group will be the majority configuration X_2, Y_1, Z_2 , meaning two out of these three are required to make a decision (such as electing a leader, or committing a log entry). We want to laterally move from X_2 to X_1 , that is, we’d like to end up with replicas on X_1, Y_1, and Z_2 in the corresponding configuration (X_1, Y_1, Z_2) . But how does that actually happen? In Raft, a configuration change is initiated by proposing a special log entry which, when received by a replica, switches it over to the new configuration. Since the replicas forming the Range receive this command at different points in time, they do not switch over in a coordinated fashion and care must be taken to avoid the dreaded “split brain” scenario, in which two distinct groups of peers both think they have the right to make decisions. Here’s how this could happen in our particular example: Y_1 is the first node to receive the configuration change in its log, and it immediately switches to C_2=X_1 , Y_1 , Z_2 . It catches up X_1 , which consequently also switches to C_2 . X_1 and Y_1 form a quorum of C_2 , so they can now append log entries without consulting with either X_2 or Z_2 . But - both X_2 and Z_2 are still using C_1=(X_2 , Y_1 , Z_2 ) and have no idea a new configuration is active elsewhere. If they happen to not learn about this in a timely manner from Y_1 - imagine a well-placed network disruption - they might decide to elect a leader between themselves and may start appending their own entries at conflicting log positions - split brain. Membership changes are tricky! One way to look at the above is that we were trying to “change too much at once”: we effectively added a node ( X_1 ) and removed a node ( X_2 ) at the same time. Maybe things would turn out OK if we carried these changes out individually, waiting for a majority to have received the first change before starting the second? It turns out that this is true. Let’s add X_1 first before removing X_2 . This means that in the above illustration we’ll have C_2 = (X_1 , X_2 , Y_1 , Z_2 ). Note that now that there are four nodes, there are three nodes required for majority consensus. This means that when X_1 and Y_1 have both switched to the new configuration, they can’t make their own decisions just yet - they need to loop in one additional peer (and tell it about C_2 ), that is, either X_2 or Z_2 . Whichever one they pick effectively leaves a single replica using C_1 , but without a friend to form a separate quorum with. Similarly, we can convince ourselves that removing one node at a time is safe, too. Breaking complex configuration changes such as lateral moves into “safer” individual parts is how CockroachDB worked for a long time. However, does it work with our deployment above? Let’s take another look at the intermediate state we’re in after adding X_1 , but before removing X_2 (similar problems occur if we remove X_2 first, then add X_1 ): Remember how we realized earlier that by placing two replicas in a single region, we could run into trouble? This is exactly what we were forced to do here. If region X fails, we’re in trouble: we lose two replicas at once, leaving two survivors unable to muster up the third replica required for a healthy majority (recall that a group of size four needs three replicas to make progress). As a result, this Range will stop accepting traffic until region X comes back up - violating the guarantees we were expecting from this deployment topology. We might try to argue that configuration changes are rare enough to make this a non-issue, but we’ve found that this does not hold. A CockroachDB cluster maintains many thousands of Ranges; at any given time, there might be a configuration change going on on some Range. But even without taking that into account, compromising availability in ways not transparent to the user is deeply unsatisfying to us. Until recently, CockroachDB mitigated this problem by carrying out the two adjacent membership changes as fast as possible, to minimize the time spent in the vulnerable configuration. However, it was clear that we couldn’t accept this state of affairs permanently and set out to address the issue in the 19.2 release of CockroachDB . The solution to our problem is outlined in the dissertation in which the Raft consensus algorithm was first introduced, and is named Joint Consensus. The idea is to pick a better intermediate configuration than we did in our example above - one that doesn’t force us to put two replicas into a single region. What if our intermediate configuration instead “joined” the initial and final configuration together, requiring agreement of both? This is exactly what Joint Consensus does. Sticking to our example, we would go from our initial configuration C_1=X_2, Y_1, Z_2 to the “joint configuration” C_1 && C_2 = ( X_2, Y, Z_2) && ( X_1, Y_1, Z_2) : In this configuration, making a decision requires agreement of a majority of C_1 as well as a majority of C_2 . Revisiting our earlier counter-example in which X_2 and Z_2 had not received the old configuration yet, we find that the split-brain is impossible: X_1 and Y_1 (who are using the joint configuration) can’t make a decision without contacting either X_2 or Z_2 , preventing split-brain. At the same time, the joint configuration survives a region outage just fine, since both C_1 and C2 do so individually! Hence, the plan was clear: implement joint configuration changes, and use them. This provided a welcome opportunity to contribute back to the community, as we share a Raft implementation with the etcd project . etcd is a distributed key-value store commonly used for configuration management (notably, it backs Kubernetes ), and we’ve been an active maintainer (and user) of its etcd/raft library well before Cockroach Labs even sprung into existence in 2015. At this point, it’s time for a juicy confession: etcd/raft doesn’t actually really implement the Raft consensus algorithm. It does closely follow the specification for the most part, but with one marked difference: configuration changes. We’ve explained above that in Raft, a peer should switch to the new configuration the moment it is appended to its log. In etcd/raft, the peer switches to the new configuration the moment is committed and applied to the state machine. The difference may seem small, but it carries weight. Briefly put, the “Raft way” is proven correct in the paper, but more awkward use from the app, while the “etcd/raft way” comes with subtle problems that require subtle fixes, but has a more natural external API. We took the opportunity to discuss with the other maintainers whether etcd/raft should fall in line with the spec. In the process, we uncovered some previously unknown potential correctness problems. A little later, Peng Qu over from PingCap (they’re using a Rust implementation of Raft very similar to etcd/raft) alerted us to yet another problem . After we found and implemented solutions for both problems, we arrived at a good understanding about the additional invariants that truly make etcd/raft’s approach safe. At this point, neither we nor the maintainer community felt that changing to the “Raft way” now provided a good return on what would have been a very large investment in etcd/raft and all of its implementers (!). In this particular case, it seemed better to be more complicated internally, remain easy to use externally (though with a wart or two), while keeping the battle-tested code we had in place mostly intact. With this detour out of the way, we went ahead and implemented joint configuration changes. Now, a few months and 22 pull requests later, anyone using etcd/raft can enjoy the well-maintained fruits of our work. Additionally, we added datadriven testing machinery that significantly simplifies testing complex interactions within Raft peers (see here for a sample). This significantly simplifies testing and provides fertile grounds for future work or even just explorations. Naturally we also started using this new functionality in CockroachDB’s recent 19.2 release . If you haven’t given us a try yet, it’s easy to do so either locally or in the cloud .", "date": "2019-11-26"},
{"website": "CockroachLabs", "title": "Introducing Cockroach University", "author": ["Will Cross"], "link": "https://www.cockroachlabs.com/blog/introducing-cockroach-university/", "abstract": "Last week, we launched Cockroach University with our first course, Getting Started with CockroachDB . Our goal in building this was to provide a way for people to learn CockroachDB in an engaging and structured environment with lots of opportunities for hands-on practice, as well as a chance to show off what they’ve done. We included a graded final exam, and those who pass will receive a Certificate of Completion. In Getting Started with CockroachDB , students launch an insecure cluster on their computer, learn how CockroachDB distributes and replicates data among the nodes of the cluster, and find out about the basic guarantees CockroachDB provides, so that developers can build programs without worrying that their database will do something unexpected. It’s taught by myself and my colleague Lauren Hirata, who took some time off from writing technical documentation to script and record course lessons and write labs. The course itself is made up of short videos, peppered with quizzes to check understanding and labs to practice newly-learned skills. The final exam at the end will determine if a student passes and receives the Certificate of Completion. Should students wish to discuss anything they encounter, we’ve set up a Slack channel so they can chat with one another and our staff, with office hours twice a week when we’ll be available to answer questions. I started as the founding Training Manager on the Education team a short six months ago, and am very proud to be releasing a course. It was a team effort, with a lot of help from other teams. The Training team is now growing, and Getting Started will be far from our last course. We’re already starting to plan a second course, this one more advanced, and focused on teaching Developers how to write applications backed by CockroachDB. We hope that soon, Cockroach University will be the go-to resource for anyone ready to start learning CockroachDB!", "date": "2019-11-20"},
{"website": "CockroachLabs", "title": "Acks-giving, or How We Give Thanks at Cockroach Labs", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/acks-giving-or-how-we-give-thanks-at-cockroach-labs/", "abstract": "Every Sunday, my husband and I share what we are thankful for. While we give gratitude throughout the week, on Sundays, we make a point of saying our gratitude out loud to each other. “Sunday thankfuls” are a tradition we will teach our 9-month-old, and that we encourage our friends and family to join should they find themselves at our dinner table. It’s important to us that we vocalize our gratitude out loud. Expressing gratitude in this way helps to keep us grounded by reminding us of the support networks, helping hands, and small acts of kindness that contribute to our overall happiness. For many people, interactions in the workplace and connections between coworkers are deeply important to overall wellbeing. We learned in the early days of Cockroach Labs that expressing gratitude and praise for the good work of our peers was deeply important to our culture, and that there existed a desire to acknowledge and appreciate each other publicly. This developed into what we call “peer acks”, short for peer acknowledgments, a forum for celebrating the good work of our peers. The evolution of peer acks is humble, and their origin, quite literally, treacly sweet. In the 2016 Q2 Employee Engagement Survey, engineer Raphael “kena” Poss wrote in a suggestion: “ When people ask meaningful or insightful questions, peers can give them a cookie. Mini-stroopwafels.” We didn’t institute an actual cookie system, but this did get wheels turning on how to simply and publicly recognize peers for their work. In our initial iteration, employees could email in their shoutouts, and Ben Darnell, fellow co-founder and Chief Architect, would read them aloud at the weekly Tuesday all-hands, keeping the senders anonymous. Jordan for quickly resolving a variety of TeamCity issues. We're quickly getting to the point where we can switch over to it. -- The first peer ack (August 2, 2016) Our company grew, peer acks flourished, and soon, managing the incoming acks became unwieldy. More often than not, they would flood in the afternoon of -- and frequently in the hour before -- our Tuesday all-hands, leaving Chelsea Lee, our Employee Experience and Development Programs Manager, to frantically paste them into the agenda document in the few minutes before the meeting began. A wonderful problem to have, and also a micro-tornado every Tuesday afternoon. Andrei Matei, a CockroachDB engineer and early champion of peer acks, came to the rescue. He developed a simple web app, built on CockroachDB (one of the first internal apps to dogfood CockroachDB), that streamlined submissions, making celebrating the work of fellow Roachers even easier. The original web app included two submission boxes: one for single line acknowledgements, separated by a carriage return, and the other for poetry, which supported multi-line acks. Popular opinion suggests that “poetry” was likely an artifact of Andrei’s sense of humor, but nevertheless, “acknowledgements-as-poetry” are now a much cherished and anticipated echelon of peer acks. Our Education (neé Docs) team, leveled up the wordplay with their Docs FixIt Day in early 2018, capping off a day of volunteer bug-hunting, documenting, and tutorial-writing with an evening of poetry. Each tech writer chose a particularly impactful fellow Roacher and celebrated their altruism with a long-form poem. Time flies by and productivity dies when you wait thirty minutes for TeamCity to try all of the links in your docs. We were sitting ducks waiting for our builds to finish like a bunch of schmucks with very little fu– patience left to give. The build times were tragic and the errors were idiosyncratic until Nikhil came along and worked his magic to get TeamCity down to a minute. So let’s raise a toast to Nikhil who can boast that his ingenuity was the most impactful to the Docs team– prost! Today, peer acks are given through “/ack”, a homegrown Slack app written by one of our engineering managers, Ken Liu. Ken used the writing of the app to familiarize himself with Go, Kubernetes, and of course, CockroachDB, hosting the app on CockroachCloud to boot. And true to our open source roots, Ken open-sourced the app “Peer Acks v2”, with the repo available on GitHub . While the system of delivering our acks has evolved since 2016, our intentions have remained the same: celebrating and giving thanks to each other. So perhaps it’s time for a rebrand from “peer acks” to “peer grats”. Because to me, it has always been about gratitude. Thank you to my coworkers who have helped in a pinch. Thank you to those who go above and beyond. Thank you to our friends, family, and community who have supported us all these years. @all Thank you, and a Happy Thanksgiving!", "date": "2019-11-27"},
{"website": "CockroachLabs", "title": "Peewee ORM + CockroachDB", "author": ["Charles Leifer"], "link": "https://www.cockroachlabs.com/blog/peewee-orm-cockroachdb/", "abstract": "This article was originally posted on the personal blog of the Peewee ORM founder, Charles Leifer. Peewee is a simple and small Python ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use. And as of Peewee's most recent release (3.13.0), it supports CockroachDB ! I'm pleased to announce that Peewee now supports CockroachDB (CRDB), the distributed, horizontally-scalable SQL database. I'm excited about this release, because it's now quite easy to get up-and-running with a distributed SQL database that can scale out with minimal effort ( documentation ). Minimal example of configuring a CockroachDatabase instance: from playhouse.cockroachdb import CockroachDatabase\n\ndb = CockroachDatabase( ' my_app ' , user = ' root ' , host = ' 10.1.0.8 ' , port = 26257 ) CRDB conveniently exposes a very similar SQL API to Postgres, which has been well-supported for many years, allowing you to use features like jsonb and arrays , in addition to the regular complement of field-types. Additionally, CRDB speaks the same wire-protocol as Postgres, so it works out-of-the-box using the popular psycopg2 driver. CRDB provides a client-side transaction retry API, which Peewee supports using a helper-method. To use client-side retries with Peewee and CRDB, provide a callable that executes your transactional SQL and pass it to the run_transaction() method: from playhouse.cockroachdb import CockroachDatabase\n\ndb = CockroachDatabase( ' my_app ' ) def create_user_and_token ( username , timestamp ): # The run_transaction() helper accepts a callable that contains your # transactional SQL statements. The transaction will be automatically # retried until it can be committed. def callback ( db_ref ):\n        user = User.create( username = username, created = timestamp)\n        token = Token.create( user = user, expires = timestamp + timedelta( days = 1 ), code = generate_random_token()) return user, token.code # Upon success, the return-value of the callback is passed back to # the caller. If an error, such as a constraint violation, occurs, # the error is propagated back up the call-stack. return db.run_transaction(callback, max_attempts = 10 )\n\nuser, code = create_user_and_token( ' huey@kitten ' , datetime.now()) Links CockroachDB documentation Using CockroachDB with Peewee", "date": "2019-12-06"},
{"website": "CockroachLabs", "title": "GCP Comes Out Swinging Against AWS and Azure in 2020 Cloud Report", "author": ["Andy Woods", "Charlotte Dillon", "Nathan VanBenschoten", "Paul Bardea"], "link": "https://www.cockroachlabs.com/blog/2020-cloud-report/", "abstract": "[THE 2021 CLOUD REPORT IS AVAILABLE. READ IT HERE] Since 2017, Cockroach Labs has run thousands of benchmark tests across dozens of machine types with the goal of better understanding performance across cloud providers. If there’s one thing we’ve learned in our experiments, it’s this: benchmarking the clouds is a continuous process. Since results fluctuate as the clouds adopt new hardware, it’s important to regularly re-evaluate your configuration (and cloud vendor). In 2017, our internal testing suggested near equitable outcomes between AWS and GCP. Only a year later, in 2018, AWS outperformed GCP by 40%, which we attributed to AWS’s Nitro System present in c5 and m5 series. So: did those results hold for another year? Decidedly not. Each year, we’re surprised and impressed by the improvements made across cloud performance, and this year was no exception. DOWNLOAD THE 2020 CLOUD REPORT We completed over 1,000 benchmark tests (including CPU, Network Throughput, Network Latency, Storage Read Performance, Storage Write Performance, and TPC-C), and found that the playing field looks a lot more equitable than it did last year. Most notably, we saw that GCP has made noticeable improvements in the TPC-C benchmark such that all three clouds fall within the same relative zone for top-end performance. The 2020 Cloud Report report expands upon learnings from last year’s work, comparing the performance of AWS, GCP, and new-to-the-report Azure on a series of microbenchmarks and customer-like-workloads to help our customers understand the performance tradeoffs present within each cloud and its machine types. What’s New in the 2020 Cloud Report? In the 2020 report, we've expanded our research. We: Added Microsoft Azure to our tests Expanded the machine types tested from AWS and GCP Open-sourced Roachprod, a microbenchmarking tool that makes it easy to reproduce all microbenchmarks You might be wondering, why the jump from 2018 to 2020? Did we take a year off? We’ve rebranded the report to focus on the upcoming year. So, like the fashion or automobile industries, we will be reporting our findings as of Fall 2019 for 2020 in the 2020 Cloud Report. How We Benchmark Cloud Providers CockroachDB is an OLTP database, which means we’re primarily concerned with transactional workloads when benchmarking cloud providers. Our approach to benchmarking largely centers around TPC-C. This year, we ran three sets of microbenchmark experiments (also using open-source tools) in the build-up to our TPC-C tests. In our full report, you can find all our test results, (and details on the open-source tools we used to benchmark them), including: CPU (stress-ng) Network throughput and latency (iPerf and ping) Storage I/O read and write (sysbench) Overall workload performance (TPC-C) TPC-C Performance We test workload performance by using TPC-C , a popular OLTP benchmark tool that simulates an e-commerce business, given our familiarity with this workload . TPC-C is an OLTP benchmark tool that simulates an e-commerce business with a number of different warehouses processing multiple transactions at once. It can be explained through the above microbenchmarks, including CPU, network, and storage I/O (more details on those in the full report ). TPC-C is measured in two different ways. One is a throughput metric, throughput-per-minute type C (tpmC) (also known as the number of orders processed per minute). The other metric is the total number of warehouses supported. Each warehouse is a fixed data size and has a max amount of tpmC it’s allowed to support, so the total data size of the benchmark is scaled proportionally to throughput. For each metric, TPC-C places latency bounds that must be adhered to in order to consider a run “passing”. Among others, a limiting passing criteria is that the p90 latency on transactions must remain below 5 seconds. This allows an operator to take throughput and latency into account in one metric. Here, we consider the maximum tpmC supported by CockroachDB running on each system before the latency bounds are exceeded. In 2020, we see a return to similar overall performance in each cloud. Each result above is the maximum tpmC produced by that cloud and machine type when holding the p90 latency below 5 seconds. This is the passing criteria for TPC-C and has been applied throughout any run of TPC-C data in this report. TPC-C Performance per Dollar Efficiency matters as much as performance. If you can achieve top performance but have to pay 2x or 3x, it may not be worth it. For this reason, TPC-C is typically measured in terms of price per tpmC. This allows for fair comparisons across clouds as well as within clouds. In this analysis, we use the default on-demand pricing available for each cloud because pricing is an extremely complex topic. GCP, in particular, was keen to note that a true pricing comparison model would need to take into account on-demand pricing, sustained use discounts, and committed use discounts. While it’s true that paying up-front costs is expensive, we applied this evenly across all three clouds. We recommend exploring various permutations of these pricing options depending upon your workload requirements. Producing a complex price comparison across each cloud would be a gigantic undertaking in and of itself, and we believe that Cockroach Labs is not best positioned to offer this kind of analysis. To calculate these metrics we divided max tpmC observed by 3 years of running each cloud’s machine type (i.e., on-demand hourly price * 3 *365 *24). It’s also important to note that these are list prices for all cloud providers. Depending upon the size of your organization--and what your spend is with each provider overall--you may be able to negotiate off-menu discounts. Again, all three clouds come close on the cheapest price per tpmC. However, this year we see that the GCP n2-highcpu-16 offers the best performance per dollar in the tested machine types. If price is less of a concern, AWS is the best performer on throughput alone, but when is price not a factor? Reproduction Steps for the 2020 Cloud Report All benchmarks in this report are open source so that anyone can run these tests themselves. As an open source product, we believe in the mission of open source and will continue to support this mission. We also vetted our results with the major cloud providers to ensure that we properly set up the machines and benchmarks. All reproduction steps for the 2020 Cloud Report can be found in this public repository . These results will always be free and easy to access and we encourage you to review the specific steps used to generate the data in this blog post and report. Note: If you wish to provision nodes exactly the same as we do, you can use this repo to access the source code for Roachprod, our open source provisioning system. Read the Full 2020 Cloud Report You can download the full report here , which includes all the results, including the lists of the highest performing machines, more details on TPC-C performance, and microbenchmarks for: CPU Network Throughput Network Latency Storage Read Performance Storage Write Performance Happy reading! DOWNLOAD THE 2020 CLOUD REPORT", "date": "2019-12-11"},
{"website": "CockroachLabs", "title": "How Education First Increased Developer Efficiency with CockroachCloud", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/education-first/", "abstract": "An often overlooked privilege of building database technology is the opportunity to team up with companies that are making a positive impact in the world. For Cockroach Labs, EF (Education First) is one of those companies. Since 1965, Education First has offered a range of educational programs including study abroad, student exchange, educational travel courses and language classes; all in an effort to \"open the world through education.\" Currently, Education First has 612 offices and schools in over 50 different countries. In 2019 it launched a digital learning platform to remove the distance between teachers and students in different countries. The services that Education First provides help to grow the work and travel possibilities for people of all ages from all countries. Today, Cockroach Labs helps to power Education First’s mission by providing their developers with a fully managed relational database that serves as the backbone for the new digital learning platform that's connecting students with teachers across the world. Comparing Amazon Aurora to CockroachDB As Education First grew its global footprint, they outgrew their database infrastructure. This is not an uncommon problem for similarly longstanding and successful companies who experience a business expansion over time that exceeds the capabilities of their database. For years, Education First has been cobbling together different databases from MySQL to Cassandra and others in order to try and keep pace with their company’s global needs. With CockroachCloud, we don’t have to worry about scalability issues or troubleshooting performance. This gives back a lot of time to our developers. --Rocco Donnarumma Rocco Donnarumma, the CTO of Education First, decided that the company reached a point where it could no longer sustain an efficient software development team while also shouldering the burden of maintaining disparate databases. Donnarumma turned his attention towards migrating to a resilient, consistent, cloud-native database that could guarantee ACID compliance, and remove the operational complexity of manual sharding. His evaluation eventually narrowed to Amazon Aurora and CockroachDB. The details of Education First’s migration strategy and outcome are available in this case study . The TL;DR version is that Amazon Aurora was disqualified because it cannot deliver synchronous global transactions. This meant that Donnarumma was left with one final decision: To self-host CockroachDB or to go with CockroachCloud, the fully managed version of CockroachDB. CockroachCloud Saves Time for Developers In an effort to give time and efficiency back to his developers who were building the digital learning platform, Donnarumma decided to utilize CockroachCloud . \" With CockroachCloud, we don’t have to worry about scalability issues or troubleshooting performance. This gives back a lot of time to our developers ,” says Donnarumma. The CockroachCloud team removes the operational complexity of setting up and maintaining a distributed database so that application teams can focus on building their business applications. And because CockroachCloud doesn’t require constant maintenance, and all the developers have to do to scale is simply spin up another node and point it at the cluster, there is no need for extensive troubleshooting or downtime. Developers at Education First have been able to jump right into CockroachCloud to create their new Teachers First application, stating that it’s no different than working in any other database because of CockroachDB’s PostgreSQL wire compatibility. For Donnarumma, that’s the best kind of feedback, because the developers don’t see the underlying complexity and can just get to work. Read the full case study to learn more about the technical challenges Education First faced as a global company and the ways that they’ve leveraged CockroachCloud to overcome those challenges and create new efficiencies for their developers.", "date": "2019-12-17"},
{"website": "CockroachLabs", "title": "How to Run Chaos Tests in a Multi-Cloud Environment", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/multi-cloud-chaos-engineering/", "abstract": "This year, as every year, Black Friday and Cyber Monday stressed e-commerce systems to their breaking points. Major companies like H&M, Nordstrom Rack, and other retailers experienced the kinds of costly outages that keep SREs up at night. Multi-cloud infrastructure is sometimes offered as a panacea to these kinds of outages. But multi-cloud deployments are not a band-aid. In fact, they often introduce new complexities into the system that need to be sniffed out. But sniffing out bugs in multi-cloud environments is, by nature, complicated. Ana Medina, a chaos engineer from Gremlin , spoke at ESCAPE/19 about how to do it, including a detailed list of the kinds of errors to search for and checklists of questions to ask. Maslow’s Hierarchy of Multi-Cloud Chaos Tests Chaos engineers create tests to proactively expose bugs. In a multi-cloud environment, this means presuming that your cloud vendors are going to fail. A standard course of chaos engineering tests for multi-cloud architectures starts by stress-testing your cloud providers to their breaking points, and moves up the hierarchy of needs to things that are less crucial. Here's what Ana suggests you pay attention to in your chaos tests, in order of importance: 1. Basic Functionality: Does your failover system work? At the very core, you need to test whether the system functions. Ana suggests spinning up clusters to see how each cloud you use (including your bare metal machines, if applicable!) behaves in stressful situations. Make sure to measure the following outcomes: How does each cloud handle failure? If a host is shut down, how long does it take to spin up another? How long does it take for the monitoring to catch this exchange? Are the clouds located somewhere that makes sense for the applications in question? Is the multi-cloud control plane working? When a Black Friday traffic overload incident inevitably occurs, you’ve run a fire drill for what happens when your provider goes down. 2. Data Consistency: Is your data correct across clouds? Another important chaos test: measure consistency between clouds. In Ana’s words, if a company can see ahead of time that data loss is a possibility in their current failover plan then they can adjust accordingly before actual customer data is lost. If your primary is shut down on the first cloud provider and you have your replica on a primary in the second cloud provider, is Cloud 1 primary consistent with Cloud 2 primary? If you shut down the primary in one of them does the replica come back as primary without suffering any data loss? How does latency affect the connection between clouds? Is your cache layer working properly? 3. Cost: What does your failover multi-cloud pricing look like? You might be able to survive a data center outage by failing over to AWS, but get hit with a surprising bill after the fact. Once the foundation of a multi-cloud infrastructure has been laid and tested, further chaos engineering can help you probe into the pricing during a failover. For example, Ana suggests you ask your team: What are our compute costs in different failover scenarios? Are any SLAs in danger, and what might those cost in fees? Are we hitting limits on our cloud providers? Use Chaos Engineering to Verify Your Multi-Cloud Strategy Failures are going to happen, whether it’s the fault of a vendor, a natural disaster, or some code glitch that got overlooked. In a multi-cloud environment, preparing for these failures with chaos engineering best practices will not only help you prepare for these failures, it’ll help confirm whether your multi-cloud strategy is working. To see all of Ana’s talk (including a live demo of what chaos tests look like in Gremlin), her whole presentation is available here . ESCAPE/19 hosted other speakers with expertise on securing services in multi-cloud infrastructure including Dan Papandrea’s talk about security in multi-cloud and Spencer Kimball’s talk about CockroachDB and the challenge of application data in global, multi-cloud deployments . Visit this page to watch all the talks from ESCAPE/19.", "date": "2019-12-09"},
{"website": "CockroachLabs", "title": "How Microservices Enable Multi-Cloud at the Expense of Developers", "author": ["Ethan J. Jackson"], "link": "https://www.cockroachlabs.com/blog/multicloud-microservicecs/", "abstract": "This article was originally posted on the Kelda.io blog by CEO and Founder, Ethan J. Jackson. Kelda is Docker compose for Kubernetes. It allows you to quickly test your code changes in a remote environment that matches production, without the complexity of interacting with Kubernetes directly. I recently had the pleasure of speaking about Kelda at ESCAPE/19 - the multi-cloud conference, in New York City. It was a fantastic event packed full of sharp folks with interesting perspectives. The talk, How Microservices Enable Multi-Cloud at the Expense of Developers , describes how microservices and CI/CD led to the development of Kubernetes which itself promises to make multi-cloud viable for the first time. However, these advantages are not without costs, particularly for developer productivity. The talk is summarized below. Cloud Providers Like Vendor Lock-in While cloud providers typically aren’t explicit about this, multi-cloud isn’t in their interest. The more challenging a cloud provider makes it for customers to switch to one of their competitors, the less difficulty they will have retaining and growing those customers, and the more power they will have in pricing negotiations. Cloud providers historically have encouraged vendor lock-in by diverging their APIs. Everything from trivial differences in the semantics of the commands required to create a VM, to major differences in features and functionality all serve to make it difficult for users to shift cloud providers once they’ve made their initial choice. And while there have been efforts with varying success to standardize the cloud API, true portability has never been achieved. Microservice & CI/CD Lead to Standardization Finally, seemingly out of nowhere, two massive trends collided: Microservices, the idea that large single process applications (monoliths) should be broken down to smaller independent pieces (microservices). CI/CD, the idea that these small pieces should be deployed automatically and frequently. As a result, we now deploy more things to production more frequently, and while not necessarily evident at the time, this placed massive pressure on deployment artifacts and processes to standardize. A broken, non-standard, deployment process on a monolith application is an annoyance. On a microservice application with 30 parts changing five times a day, it’s a disaster. Standardization of the Cloud API As an almost inevitable result, containers emerged to standardize the packaging and life cycle of individual microservices. And then, soon after (and with a little help from Google), Kubernetes emerged to standardize the orchestration of containers. While this wasn’t the original motivating intention of these trends, for the first time since the beginning of the cloud, all cloud providers support a single completely standard, completely portable interface for booting and maintaining software – the Kubernetes API. Having a standard API finally makes it possible to move applications from one cloud to another without significant engineering effort, removing what I believe is one of the last hurdles to the realization of multi-cloud. Microservices Hurt Developers Despite its numerous advantages, this new status quo introduces significant complexity to the developer experience. There are many more moving parts, operating on a complex substrate, that developers have to understand to do basic testing. For those interested in DevOps, this may not seem like a heavy lift, but for those of us who specialize in other areas, these issues can add up into a real productivity drag. At its core, the issue comes down to how dependencies are handled. With a monolith, dependencies are simply software libraries that are compiled into the binary, or linked in at runtime. Individual microservices, of course, also depend on software libraries, but in addition, they depend on other microservices . This vastly complicates the local testing process for numerous reasons: The tools required to manage these dependencies (Minikube/Docker Compose) are difficult for many developers to understand. The tools used in local testing are often wildly different than those used in production. This reduces the confidence that tests run locally to guarantee the correctness of the final result. The CPU/Memory requirements of running even relatively small numbers of microservices on a developer laptop can be prohibitive. This Problem is Solvable The good news is the problem is solvable. We’ve found that large teams who have been working with microservices for a long time tend to build a custom tool based on three core principles: Fast & Efficient . The developer experience should be extremely fast. Every second, a developer waits for their environment to boot is pure waste. Approachable . Not all developers are DevOps experts, nor should they have to be. Developers of all skill levels should be productive quickly without a lot of hassle. Similar to Production . Developers need confidence that if their code works in test, it will work in production. For this reason, developing in the cloud on an environment specifically designed to be as similar to production as possible is crucial. At Kelda , we’re building a development environment with exactly these properties. With a proper developer experience, organizations can get all of the benefits that microservices promise, without the drag on developer productivity.", "date": "2019-12-19"},
{"website": "CockroachLabs", "title": "Reducing Multi-Region Latency with Follower Reads", "author": ["Andrew Werner", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/follower-reads/", "abstract": "At Cockroach Labs, we're focused on making data easy for our customers. CockroachDB is designed as a vendor-agnostic, cloud-native database for transactional workloads. We offer a number of benefits over traditional relational databases including serializable isolation, online schema changes, and high availability fault-tolerance. Today, we want to demonstrate another CockroachDB differentiator: multi-region support for global scale. In this blog post, we introduce Follower reads , a key feature for supporting multi-region reads with low latency when your use case can accept stale data. To introduce Follower reads, we built a demo global application called Wikifeedia , with the entirety of the source code available for you to play along with. To allow you to see the application in action, we’re hosting the app using CockroachCloud for the next six months. Wikifeedia Overview Wikifeedia is an application built on top of the public APIs from Wikipedia . It shows users a globally sorted index of content based on the most reviewed content in each language for the previous day. Since its target audience is global, it needs to be accessible from anywhere in the world, with low latency. But, like many news aggregations, the content isn’t changing from second to second. As such, it can tolerate slightly stale data. The application is hosted on Google Cloud Kubernetes Engine, while the underlying database is hosted using our own Cockroach Cloud managed service offering. As a side note, this application has become a personal favorite of ours as we find ourselves eagerly checking out Wikifeedia to determine what’s trending on Wikipedia in any given day. Optimizing Wikifeedia for Low Latency Low database latency is critical to providing a low latency application experience around the world. But it’s not the only factor at play. Here are a couple other ways we initially optimized Wikifeedia to provide low latency. CockroachDB WebUI for Wikifeedia HTTP Caching Every time a browser loads Wikifeedia, it downloads static assets (e.g., HTML, CSS, JavaScript, and images) to display content. Static assets use a significant amount of data and take a  lot of time to load. Slow page times can frustrate users, and cause them to abandon their attempt to load a page if latency is too high. This can be problematic as we want our users to see a crisp and fast Wikifeedia. In addition to providing fast response times to Wikifeedia users, we want to minimize the number of requests the server receives. We can address both of these concerns by employing browser caching to store files in the user’s browser. Cached files load faster, and they obviate the need for more server requests when the data is already present in the browser. Of course, this doesn’t apply to a user’s first visit, but it can dramatically improve response times for repeat users. Any modern web application should partner HTTP caching with good database management to reduce latency and load time. Furthermore, by setting the appropriate caching headers , service writers can utilize CDNs to cache and serve static assets with low latency. Utilizing CDNs for static assets is a good first step when looking at reducing latency for global web applications. Check out this MDN Guide on how to set cache control headers to effectively utilize CDNs and browser caches. Global Load Balancing It is also critical to consider routing when deploying global applications; a simple deployment using round-robin DNS over global application server IP addresses would suffer from intermittent poor performance for all users rather than the reliable, good performance for some users in the single-region deployment. To solve this problem we recommend using a global load balancer. With Google’s Cloud Load Balancer , for example, you can reserve a single IP address that uses anycast to route to a nearby point-of-presence, and then route to the lowest latency backend. An alternative might be to use GeoDNS to attempt to resolve a nearby application server based on the requesting location’s AWS Route 53. Reducing Latency: Performance Analysis Over the next few sections we’ll explore example latency profiles observed loading the application from different locations (i.e., New York, Berlin, and Singapore) with different deployment methodologies (i.e., US east-only deployment, global deployment without Follower reads, and global deployment with Follower reads). All of the measurement was performed using the Uptrends Free Website Speed Test . Deployment 1: US East Deployment For our first deployment of Wikifeedia, we deployed both the database and application in a single region. In this initial deployment strategy, Wikifeedia’s database is deployed in AWS us-east-1 (Northern Virginia), and its application servers are in GCP us-east4 (i.e., Northern Virginia). Note that the application servers are in GCP and the database servers are in AWS. This unconventional, cross-cloud deployment topology is not recommended, but was utilized to help dog-food AWS cluster creation while only having access to a GCP Kubernetes cluster. To reduce complexity, we recommend staying strictly within either cloud as they both offer global load balancing and nodes in each of the regions. This deployment provides excellent latency for users in New York, but increasingly unpalatable latencies for users further away from US East, like those in Berlin and Singapore. The first thing to notice is that the latency is dominated by the time taken to download the static assets for the page. These static assets are comprised of an index.html page which is less than 2kB and CSS and JavaScript assets which are collectively less than 400kB. The high latency between Singapore and the application server in Northern Virginia (i.e., GCP us-east4) has severe costs both while fetching the static assets and while fetching the list of top articles, but the cost of fetching the assets dominates. Deployment 2: Global Application Deployment, No Follower reads For the next iteration, we deployed Wikifeedia globally to reduce the latency from clients to application servers. For this strategy, we deployed two additional data centers in Europe. The application servers are in GCP europe-west1 (i.e., Belgium) and the database is in AWS eu-west-1 (i.e., Ireland), and Asia (i.e., Singapore), with application servers in GCP asia-southeast1 (i.e., Singapore) and database in AWS ap-southeast-1 (i.e., Singapore). By deploying the application servers nearer to the client, we dramatically reduce the loading time for the static assets. We can see that this global deployment strategy dramatically improves the user experience. In this configuration, we demonstrate using application servers near the client with a single database in the US. In practice, there are database nodes near each application server, but the leaseholder is set to remain in the US so the Asia and Europe nodes always need to communicate over these high-latency links. Notice how the database call now dominates the latency. What can we do about that? Make the database global, and use Follower reads! Deployment 3: Global Application with Follower reads In the final deployment model, we adopt a new feature in CockroachDB called Follower reads in our business logic. This allows each of the replicas to serve low-latency reads. Here we can see that the distance from the client to the application server and the distance from the application server to the database node contributes to our latency variation! For users in Singapore, this represents an 8x latency improvement over our first deployment. Let’s look a bit deeper at the Singapore data via the Uptrends report above. Without Follower reads, users in Singapore observe a latency of ~430 ms (0.08 to 0.55) retrieving the top articles from the application server. However, if we deploy a global database with Follower reads Singaporean users can end-to-end query latency closer to 20 ms. Database Latency Compared As we mentioned above, the database latency is only part of the total picture. But it is an important part! We’ve pulled out the database latency into a standalone chart comparing latency between a database only deployed in US-East and a global database. The below charts show the time observed by the application servers and the database nodes to fetch the data. One point of interest is the latency observed by the European application server. This is due to the relatively high latency between the application server in Belgium and the database server in Ireland. All of the latencies between the application servers and database nodes are several milliseconds due primarily to the cross-cloud deployment. The difference between 500ms and 20 milliseconds makes a real difference to users, the difference between 20ms and 2ms matters less. A global application deployment needs a global database deployment. You can try out Wikifeedia yourself by visiting https://wikifeedia.crdb.io . As a side note, this application has become a personal favorite of ours as we find ourselves eagerly checking out Wikifeedia to determine what’s trending on Wikipedia in any given day. Follower Reads Overview You might be wondering, how does CockroachDB enable low-latency reads anywhere in the world for this globally sorted content? How did we hit ~1 ms reads in a global cluster? The answer is using our enterprise Follower reads feature which we introduced in version 19.1. Follower reads are a mechanism to let any replica of a range serve a read request, but are only available for read queries that are sufficiently in the past, i.e., using AS OF SYSTEM TIME. In widely distributed deployments, using Follower reads can reduce the latency of read operations (which can also increase throughput) by letting the replica closest to the gateway serve the request, instead of forcing the gateway to communicate with the leaseholder, which could be geographically distant. While this approach may not be suitable for all use cases, it’s ideal for serving data from indexes which are populated by background tasks. In our topology pattern guidance , we explain that: “Using this pattern, you configure your application to use the Follower reads feature by adding an AS OF SYSTEM TIME clause when reading from the table. This tells CockroachDB to read slightly historical data (at least 48 seconds in the past) from the closest replica so as to avoid being routed to the leaseholder, which may be in an entirely different region. Writes, however, will still leave the region to get consensus for the table.” Let’s dive deeper into the data to see how Follower reads work in practice with Wikifeedia. Wikifeedia & Follower Reads Let’s evaluate the latency of Wikifeedia with the following query: SELECT\n  project, article, title, thumbnail_url, image_url, abstract, article_url, daily_views\nFROM\n   articles\nWHERE\n   project = $1\nORDER BY\n   daily_views DESC\nLIMIT\n   $2; This query selects the top $2 articles from project $1. The home page would have values (“en”, 10). Once again, to demonstrate latency anywhere in the world, we’ll use the Uptrends Website Speed Test . Uptrends allow us to simulate users access the Wikifeedia from many geographies including New York, Berlin, and Singapore. First, let’s start with a baseline of latency on Wikifeedia without Follower reads. Here are the results of Wikifeedia when we do not serve Follower reads as seen in our native CockroachDB WebUI: With this enhancement to our SQL, we can enable Follower reads. --- before\n+++ after\n@@ -3,3 +3,3 @@\n FROM\n- articles\n+   articles AS OF SYSTEM TIME experimental_follower_read_timestamp()\n WHERE We’ve added AS OF SYSTEM TIME and experimental_follower_read_timestamp() to specify to the system that it is appropriate to serve a historical read in exchange for improving latency. As a result of enabling this feature, we can see the latencies in nodes 2 and 3 drop from 137 ms and 450 ms to 1.8 ms and 2.0 ms respectively! Conclusion All global applications should use both a global application deployment (including a global CDN) and a globally deployed database. Further, CockroachDB’s Follower reads pattern is a good choice for global deployments with tables with the following requirements: Read latency must be low, but write latency can be higher. Reads can be historical. Rows in the table and all latency-sensitive queries cannot be tied to specific geographies (e.g., a reference table). Table data must remain available during a region failure. Also, remember that Follower reads are just one tool in our multi-region toolkit. They can actually be used in conjunction with many of CockroachDB’s other multi-region features. We’ve previously written about What Global Data Actually Looks Like and How to Leverage Geo-partitioning . We’ve even provided a step-by-step tutorial on How to Implement Geo-partitioning. Consult our topology guide to learn more about how you can tailor CockroachDB to better support your multi-region use case or contact sales for personalized support.", "date": "2019-12-03"},
{"website": "CockroachLabs", "title": "Why MyWorld Switched from Cassandra to CockroachDB", "author": ["Daniel Perano"], "link": "https://www.cockroachlabs.com/blog/cassandra-to-cockroachdb/", "abstract": "Today's guest author, Daniel Perano, is a Full-Stack Developer & Founder of MyWorld . He is one of many developers building interesting projects with CockroachDB. If you'd like to guest author a blog about your project please reach out on our community slack channel . MyWorld is a next-generation virtual world startup. Current social virtual worlds (like Second Life and OpenSimulator ) are built on decades-old technology and are fundamentally limited in their design. MMOG (Massive Multiplayer Online Game) developers lack a common, extensible platform, meaning that multiple years of work and millions of dollars are required to build a custom engine for almost every MMOG - forcing indie studios and many other small developers out of the market entirely. At MyWorld, we’re building a brand new virtual world platform from the ground up, using modern tools and technologies to create a fast, scalable, and extensible platform to power the next generation of social virtual worlds and massive multi-player online games. To accomplish this, we need a database that can: Scale linearly along with the simulation cluster, Replicate data to minimize the odds of downtime and data loss, Handle the database traffic needed to support millions of concurrent players, Easily handle a multi-terabyte dataset. In addition, we want a database that follows our own philosophy of making software that self-maintains as much as possible and requires minimal human interaction to operate. Lastly, we need any database we use to be open-source or open-core - this openness is central to MyWorld’s heritage and design philosophy, and since MyWorld will be open-sourced at some point after the alpha and beta releases we could not consider closed-source or vendor-locked databases. Under the hood, our tech stack builds heavily on a combination of in-house code and some of the best open-source software in the industry. The database layer is based on a lightweight ORM pattern - the Postgres JDBC driver , a HikariCP Connection pool , and the jNimble ORM with a thin layer on top of it to map our domain objects to/from SQL (all queries are handwritten for design flexibility and performance - we don’t use the automatic mapping features of jNimble right now). At a higher level, our physics engine is Bullet and we use the jMonkeyEngine game engine both server side and client side. The world simulation is handled through an in-house entity system, and scripting is done in a powerful high-level in-house scripting language (object oriented with classes, first-class functions, dynamic typing, and polymorphism). We use the Jetty HTTP server for our embedded HTTP needs, and the client-side UI is done with JavaFX . Cassandra Is Scalable But Limits Design Choices Prior to porting our database backend to use CockroachDB , we were using Cassandra. Initially, this went well, as our data model has always been designed to be as simple and straightforward as possible in terms of database representation. However, as we grew and discovered more and more query needs, Cassandra’s denormalization patterns and lack of flexible indexing began to be a real and very noticeable constraint, especially since its severely limited range queries meant that implementing our own higher-level indices on top of its out-of-the-box functionality would be a very difficult endeavor. We held out on Cassandra for as long as we could - it’s a very fast and scalable database, and it’s been tried and proven at a scale unlike any other ( Apple’s cluster surpassed 75k active nodes ). True horizontal scalability and the ability to keep operating with nodes down is critical for us. However, the more we ran into Cassandra’s limitations on our data model, the more aware we became of Cassandra's impact on our design: Using Cassandra was unduly influencing the model, restricting our higher-level design choices, and forcing us to maintain certain areas of data consistency at the application level instead of in the database. Some design trade-offs always have to be made in a distributed environment, but Cassandra was influencing higher-level design choices in ways a database shouldn’t. Cassandra is outstanding, but our needs were simply outgrowing what Cassandra was designed to handle. Additionally, while Cassandra’s “ccm” tool worked well for managing local-only database clusters in development, we had growing concerns about the level of operational overhead Cassandra might require in production. SQL Creates Flexibility For Modeling Data: From Cassandra to CockroachDB Because of our investment in Cassandra and our concerns about performance trade-offs, we were slow to initially evaluate CockroachDB. Once we tried porting some of our database code and experimenting with the areas that had been difficult with Cassandra, however, the decision almost made itself. We could model our data whichever way suited each area best, and if we needed, say, a range query over several unrelated numeric columns (which is very difficult to do efficiently in Cassandra), we could always add an index and more or less forget about it. Switching to SQL meant that we could model our data the way it ought to be modeled and rely on full multi-table transactions and SQL constraints to maintain data integrity. The migration from Cassandra to SQL was surprisingly painless - our data model was already in a strongly-typed tabular form, and since we had opted to write our queries as handwritten CQL (Cassandra Query Language, which is essentially a subset of SQL) instead of using an ORM, we only had to make minimal syntactic and type changes to existing queries in order to have a functional starting point running with CockroachDB. Choosing SQL meant gaining flexible indices, integrity constraints, full-featured transactions, and the ability to efficiently model our data with far more flexibility than Cassandra allowed. Choosing CockroachDB in particular meant that we could have the power of SQL, make only minimal performance trade-offs (and more importantly, have the ability to make trade-offs in ways that Cassandra just couldn’t support), make effectively no compromises on the scale of traffic and data we could handle, and operate with little human intervention. CockroachDB’s compatibility with Postgres was another big plus for us - that meant developing our code with tried-and-proven Postgres drivers & libraries, and in the worst case, the ability to switch to Postgres with little to no code modifications (albeit with a punishing hit to scalability) should something go catastrophically wrong with CockroachDB. Last but not least, CockroachDB just works in development. When I first tried it out I tossed together a bare-bones 3 line shell script to start a 3 node cluster. The initial installation and setup only took about 30 minutes, including fiddling with the clustering parameters and the built-in web dashboard. This shocked me - this just doesn’t happen with databases, especially in a clustered configuration. You don’t download the software and then have a 3 node cluster up and running just like that. After I got an initial configuration that I liked, I’ve never had to spend any time fixing anything - it just doesn’t break. Cassandra’s “ccm” tool was also very effective, but every few months during a version upgrade (or just plain bad luck) something would go wrong and I would spend hours just getting the database cluster back up on my development machine. That hasn’t happened once with CockroachDB. CockroachDB’s trivial ability to adapt to both server and local development environments is a tremendous benefit for us. What’s Next for MyWorld? We have not yet launched the MyWorld platform to the public. Our first public beta releases are scheduled to happen in Q2 of 2020. Right now, we’re running a lightweight 3 node cluster on a dev machine. We anticipate adoption around the globe, which means we’ll need to geo-distribute our clusters to keep latencies down for users and to comply with the GDPR and other data localization regulations worldwide. While we’re committed to gathering only as much personal user data as is critical for operation (such as email addresses for accounts and IP address logs for intrusion detection), the data we do collect may be subject to data locality laws - so we’ll likely need CockroachDB’s geo-partitioning feature to keep our users’ data in the countries where it belongs. If you’re evaluating CockroachDB for your own product, I’d highly recommend just downloading it and running a few queries from the command line - you’ll quickly get a good feel for how it handles in development. [ Editor's note : CockroachCloud is another option for getting up and running quickly--it's our fully managed and hosted database-as-a-service. Get started here . ] If you have an existing codebase that you’re considering porting, take your time to go through the migration guides and the feature support page - I’ve found the documentation to be comprehensive and clear about what’s supported, what’s not, and any caveats you should be aware of. I’ll follow up on this blog post with a ‘part 2’ to discuss database performance in production after we’ve launched the MyWorld platform and grown our user-base. In the meantime, if you’d like to know more about MyWorld or follow our progress, check out our website , follow us on Twitter or Facebook , or come chat with us on Discord or in the CockroachDB community slack !", "date": "2020-01-16"},
{"website": "CockroachLabs", "title": "The Ethical Cloud: How to Evaluate the True Cost of Your Cloud Platform", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/the-ethical-cloud/", "abstract": "Cloud services abstract away a lot of complexities involved in data center provisioning. Usually, we think of this as a good thing. It’s a huge boon to developer productivity, lowers overhead costs, and allows individual developers and small companies to experiment and grow like never before. But as Rob Reid spoke about at ESCAPE/19 , this abstraction obfuscates some darker elements of data center provisioning. Like it or not, cloud computing has real ethical costs attached to it, and due to the “service” part of “cloud services”, we won’t see those costs unless we actively look for them. Reid is a principal engineer at LUSH Digital , and his team has been actively looking at those costs for quite some time now. While he doesn’t purport to have all the answers, his talk on what he’s calling “The Ethical Cloud” invites companies to start asking some of the thornier questions, he spoke openly about these abstractions at ESCAPE/19. Here are the three dimensions of the ethical aspects of cloud computing according to Rob, and his suggestions on what you can do to start investigating your cloud computing impact. How Your Data Centers Impact the Environment Cloud services represent about 3% of global carbon emissions. That’s about the same as the aviation industry and around 30% to 40% of that is data center cooling alone. That number, of course, is only going to grow. In 2019, cloud spending was around $200 billion, and by 2022, that number is expected to grow six fold, to about $1.3 trillion. While those numbers are pretty scary, if you’re just a small company, it can feel like they have nothing to do with you. If you’re one of the thousands of companies using a major cloud provider, you’re just a drop in the bucket. So what can you do? 1. Move to the cloud Minimize your overprovisioning. The more efficiently your applications run, the less resources they’ll consume and the less power they’ll require. If you haven’t done it already, this might necessitate a move to the cloud, or a transition to a public cloud. Virtualization technologies have rendered public cloud providers more efficient than most private clouds. This is one of those nice instances where economic and environmental incentives are aligned: minimizing data center overhead lowers costs, as well as carbon emissions. 2. Be picky with your cloud provider Not all clouds are created equal. We know this when it comes to comparing cloud performance , but the same logic applies to the clouds’ impact on the environment. While there’s no comprehensive report comparing the environmental impact across the top three providers, a bit of research reveals that they do not all have the same ethics when it comes to a commitment to clean energy. One unnamed major cloud provider, as Rob notes, have recently scaled up their operations in Virginia by 60% in the last two years. By some calculations, more than 70% of the world’s data flows through Northern Virginia data centers. In order to keep up with this demand, the state is helping build the controversial Atlantic Coast Pipeline to further aid this growth through fossil fuels. While some cloud computing companies have spoken up against this project (and other fossil-fueled data centers) some have stayed all too silent. Here are some resources on how the three major cloud providers view sustainability: Amazon’s sustainability pledge Google’s sustainability pledge Microsoft’s sustainability pledge How Cloud Services Treat Your Data Privacy Data privacy laws are constantly changing, and unless understanding these is your full-time job, you likely will not be fully up-to-date. In working with third party cloud providers, we’re trusting them with a lot of data. Cloud providers’ encryption at rest is a link in that chain, and at some points, can be the only link. So what can you do? 1. Keep stored customer data to a minimum One thing every company can do to protect users’ privacy is to keep stored customer data to the absolute minimum. Consider domiciling sensitive data in countries with stricter protection laws like Iceland, which Rob notes is known as the “Switzerland of data.” 2. Know your cloud provider’s stance on transparency In addition to keeping up with the legal side of data privacy, it’s important to understand how your cloud providers interpret those laws. The three major clouds have different stances on what their role in data privacy should be. For more information, check out each company’s most recent Information Request reports, which reveal how frequently they shared customer data when requested by authorities. Amazon Information Request report Microsoft Information Request report Google Information Request report The Human Cost of Cloud Computing Data centers, and the companies that we use to provision them, all use conflict minerals such as tungsten, tin, and gold. The use of these conflict minerals, by definition, implies that there are real human costs: their profits fund wars. So if you’re not a hardware company working to build computers without conflict minerals, what can you do to improve the state of the world? Reid says the beginning is just asking questions. Ask your cloud provider for transparency about their production pipeline, and start to hold them accountable. Do your research Understand the working conditions, mistreatment and rights violations of people working in the assembly of components. Understand the conditions that people in places like Agbogbloshie, Ghana, are living in---a place which has become so polluted with e-waste that it’s now considered to be more polluted than Chernobyl. Next Steps for The Ethical Cloud The buying team at LUSH has a phrase they live by: “It’s our job to know, and once we know, it’s our responsibility to act.” As Rob concluded in his talk, people are suffering because of our demand for the internet. It may not be visible on the surface, or in your day-to-day work, but if you interact with data centers, or cloud services at large, there’s something you can do. Rob asks us to be accountable. It’s not just about the cheapest price. There is no higher power taking on your responsibility for this. Ask the questions so you can know, and once you know, take action.", "date": "2020-01-21"},
{"website": "CockroachLabs", "title": "Building a College Recruiting Program for Tomorrow's Tech Industry", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/building-a-college-recruiting-program-for-tomorrows-tech-industry/", "abstract": "Working at a startup presents a number of challenges for hiring. You have limited resources, you’re lesser-known to those outside of your industry, and you innovate at breakneck speed, making it difficult to articulate what you do. In an industry where candidates are inundated with information, cutting through the noise requires new, ever-evolving strategy. Compound those challenges with an already-competitive job market, a broad, systemic problem regarding diversity and inclusion in STEM, and inconsistent technical education quality at both the secondary and university level, and what becomes clear is that tech companies need to invest in solutions to disrupt the status quo. In my first few months at Cockroach Labs, Lindsay Grenawalt (our Chief People Officer) and I sat down to discuss areas for improvement within our recruiting strategies. Given my passion for both education and building vibrant teams, I took on building a national university recruiting program focused on hiring computer science students. 1. Planning and Strategy In developing university programs, it’s important to begin with two acknowledgments. The first of which is that these programs are long-term investments and the second is that one must develop a data-driven framework for measuring their success over time. Working with these points in mind, we set the following objectives for our program: Build a thoughtful, cost-effective, and repeatable college outreach and hiring effort. Establish long-term connections with faculty, staff, and students in computer science. Increase employer awareness for short- and long-term hiring purposes. Leverage college recruiting as a vehicle for developing robust and diverse talent. After setting objectives, the next step is to understand the state of the field to identify focus areas and leverage best practices. According to the National Association of Colleges and Employers , an average of 56% of interns convert to full-time hires at companies across the United States. In 2019, only one of four non-intern, new graduate software engineers offered a full-time role at Cockroach Labs accepted. Five of eight former interns invited to return, however, did. The takeaway? Invest in your internship program. Not only did we develop our college recruiting program to accomplish as much as possible within the challenges I mentioned earlier, but to confront them, as well. After all, the onus of an innovative recruiting team is not only to grow the company we have, but to work towards the company we want to be— in our case, a diverse one. We know that more diverse companies perform better , but also that diversity leads to better, faster decision-making (think product strategy, hiring committees, marketing, and more). Performance aside, providing equal opportunity for all allows the technology industry to make a meaningful social impact, even in database software. All of that is to say that at Cockroach Labs, we deeply believe in diversity. Sadly, we do not have a diverse technology team. We know that. We’re hard at work to change it . My goal is to make sure that university recruiting is part of the solution. Recognizing that much of the problem begins with schooling, it became clear that by focusing on education and awareness, we might significantly impact the kinds of applications we see for internships and full-time roles. 1.1 College Recruiting for Diversity In building our program, we asked ourselves how we might move the needle towards equity. Part of the answer is education. While women make up over 48% of the private workforce in the U.S., they make up only 35% of the tech industry (Figure 1). Similarly, Black and Latino people each make up 14% of the private workforce in the U.S. but only 7.5% of the tech industry (Figure 2). These disparities are significant. They’re even worse for infrastructure teams. More women, at 56%, than men, at 44%, are enrolled in institutions of higher learning. Black and Latino people also make up 14% and 19% of the university student population, respectively. Still, those numbers drop again when we look at the percentage of students from those groups graduating with Bachelor’s degrees in computer science. Women receive only 18% of computer science degrees, a precipitous drop. Black and Latino students each contribute 11% each to the total number of computer science degrees conferred annually. These statistics represent a multi-faceted failure to engage and support historically underrepresented groups in computer science, especially in environments best suited to prepare them for careers in technology. We realized that if we could get into classrooms and workshops at computer science programs across the country, we could not only share what CockroachDB is with a diverse group of students, but also teach them how it works. The breakthroughs our team has made in computer science are noteworthy. We wanted to shed more light on them. Source: 1. EEOC, “Special Report on Diversity in High Tech,” 2. Notre Dame University of Maryland, “5 Changing Demographics in Higher Education,” and 3. NSF, Science & Engineering Indicators, 2018. Source: 1. EEOC, “Special Report on Diversity in High Tech” and 2. National Center for Education Statistics, “Status and Trends in the Education of Racial and Ethnic Groups 2018” For our team, this was an easy decision. The value our co-founders place on open source software comes from a commitment to access, collaboration, and community benefit. An education-first university program takes that commitment further, generates excitement about our technology, and provides students with a path to using it (or even contributing on Github). This bridges an important gap for undergraduates between what they learn in the classroom and how it can be applied practically. In doing so, we hope to convince more students from diverse backgrounds to stay the course and graduate from computer science programs in greater numbers. An education-first approach allows us to meet our broader objectives, too. Here’s how: Thoughtful, Cost-Effective, and Repeatable: By focusing on a targeted presence in classrooms across the country, we reach a more relevant student population. When we do it right, we’ll be welcomed back next year! Build Short- and Long-Term Relationships in Computer Science: By engaging with faculty and students at some of the best computer science programs in the country, we get to work with the brightest minds in the field today and in the future. Today they’re interns, tomorrow they’ll be engineers. Increase Employer Awareness: By putting our technology and engineers in the classroom, we raise awareness about Cockroach Labs and CockroachDB. Giving out swag is just a sweetener (though our t-shirts are pretty sweet). Develop a Diverse Talent Pool: We can only engage diverse student groups and get them excited to apply if we show up , so we made sure to look beyond top 20 CS programs and pay attention to events and programs fostering inclusion. Over 20% of both Black and Latino college graduates come from Historically Black Colleges and Universities (HBCUs) and Hispanic-Serving Institutions (HSIs). This is why our program includes both. We also paid attention to which schools were investing in diversity in STEM to become early partners in their efforts. Each of these foci represents a shift in our strategy. In the past, we’ve recruited heavily from the University of Waterloo in Canada with great success. By allocating greater resources to our university program, we have an opportunity to build a broader, more inclusive blueprint. With a plan in place, we then moved to put it in motion. 2. Action Define Engagement Once you have a framework to guide your efforts, you can leverage it to select target universities and define engagement strategy. Ours is driven by seeking out the highest volume of students studying relevant technology while keeping an eye on cost. This meant focusing on class lectures, technical information sessions, hackathons, and other educational efforts. Starting with an outreach campaign to some of the country’s leading academics in database science, we created a 16-event, 13-college program in just under 3 months. We shared our architecture with a Database Management Systems class at Brown University, spent a weekend at CalHacks helping students build projects with CockroachDB , and are planning a workshop to introduce New York City students to the world of open source. Build Your Tools (and Prepare to Iterate) To ensure that your program’s impact is consistent, it’s necessary to develop resources that tie back to your goals. This will also make it easier to operationalize a university program with a high number of volunteers from across your company. For us, this meant preparing presentations, documentation with frequently asked questions, and leaving students with reference material to ensure that our interactions don’t begin and end with our university recruiting visits. They’re a starting point, not an outcome. To prepare for the program’s kickoff, I partnered with Amruta Ranade, a Senior Technical Writer, and Ben Darnell, our Chief Architect and co-founder, to develop an architecture talk specifically for the classroom. Their expertise in both articulating our software’s design and making it accessible to new audiences proved crucial to ensuring that our lectures would be both technically interesting and easy to understand. Following each engagement, I sat down with our staff volunteers to learn what resources were missing so that we assemble them ahead of our next one. Your program can and should get better in real time. Reach Your Audience To raise awareness for strategic events, we launched email campaigns and brainstormed incentives to drive attendance.  After brainstorming more ways to catch student’s eyes, we launched targeted Facebook ad campaigns ahead of our visits to new campuses. What we learned is that while food is good, substance and smart technology are better. In 2019, we went to Howard University and Spelman College. In 2020, we’ll go to at least two HSIs (Hispanic-Serving Institutions), as well. As I write this, we have completed 12 of our planned events and are putting together analytics dashboards to monitor the program’s impact on our awareness and hiring efforts. 3. Measuring Success of the College Recruiting Program University recruiting programs are first and foremost meant for students. That’s why we collect feedback from students and their instructors to ensure our talks are engaging, educational, and adding value to their learning. Otherwise, our program isn’t working. The feedback doesn’t end there. We check in with our team after each event to make sure they have what they need to realize the program’s mission and to gain their perspective on whether we’re seeing the right results. From these check-ins, we learned which universities and events had uniquely diverse audiences. Needless to say, we’ll be going back! To measure awareness, we’re collecting information at our events to learn how many students hear about Cockroach Labs or CockroachDB for the first time at one of our engagements. Working with Shannon Zwicker, our Recruiting Operations Manager, we outlined a number of metrics for the program. As an overall baseline, it’s important to measure the total number of applications to entry-level roles year over year. This can tell us whether we’re raising awareness (alongside our marketing team, of course) and getting enough applications to fill our roles. All of the data we gather will also be evaluated for each school we visit to measure and compare outcomes. To better understand strengths and pain points in the hiring process, we will collect data on pass-through rates at each stage of university recruiting interviews. This includes the total number of hires through the program, an essential performance indicator. While we won’t set quotas for hiring, we are going to monitor applications for demographics to ensure that we are conducting the appropriate outreach and trending in a more equitable direction. All of this is a work in progress, but we want to learn as much as we can to improve the program every year. 4. Conclusion We’ve set out this year with ambitious but achievable goals for our university recruiting program. They’ll only get bigger next year. Building this program now will add value to our business and address many of the hiring challenges I mentioned earlier. If you want to put diversity first in hiring, an inclusive university program matters. It means thinking outside of the box, finding new ways to reach more students, and making sure that your team is going the extra mile to open up new paths to opportunity. That’s what we’re working on at Cockroach Labs. We know that our strategies won’t work for every company, but if you want to learn more about how we’re optimizing college recruiting for diversity and inclusion, feel free to reach out to us at recruiting@cockroachlabs.com . Want to see our recruiting team in action? We’re hiring .", "date": "2020-01-02"},
{"website": "CockroachLabs", "title": "Announcing CockroachDB Support for Django ORM", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-for-django/", "abstract": "``` Django includes a full-featured ORM that simplifies interactions with a database--it’s one of the reasons it’s become one of the most popular web frameworks. Even so, you’re still left to manage scale yourself, and ensure the database is resilient and always on. That can be really hard to do with common Django databases like Postgres, SQLite, and MySQL. And that’s why today, we’re excited to announce a new CockroachDB backend for the Django ORM . ``` Using CockroachDB and Django gives you the ease of writing in Python while getting all the benefits of an open source, distributed SQL database . CockroachDB shards automatically, is naturally resilient, and is highly available. Here’s how to get started: Installing django-cockroachdb 1. Use the version of django-cockroachdb that corresponds to your version of Django. For example, to get the latest compatible release for Django 3.0.x: pip install django-cockroachdb==3.0.* The minor release number of Django doesn't correspond to the minor release number of django-cockroachdb, so use the latest minor release of each. 2. Configure the Django DATABASES setting similar to this: DATABASES = { 'default' : { 'ENGINE' : 'django_cockroachdb' , 'NAME' : 'django' , 'USER' : 'root' , 'PASSWORD' : '' , 'HOST' : 'localhost' , 'PORT' : '26257' , # If connecting with SSL, remove the PASSWORD entry above and include # the section below, replacing the file paths as appropriate. 'OPTIONS' : { 'sslmode' : 'require' , 'sslrootcert' : '/certs/ca.crt' , 'sslcert' : '/certs/client.myprojectuser.crt' , 'sslkey' : '/certs/client.myprojectuser.key' , }, }, } Helpful links: Build a Python App with CockroachDB and Django FAQs and Gotchas CockroachDB and Python", "date": "2020-01-27"},
{"website": "CockroachLabs", "title": "How to Use CockroachDB with Your Django Application on Ubuntu", "author": ["Artem Ervits"], "link": "https://www.cockroachlabs.com/blog/django-application-on-ubuntu/", "abstract": "Django is a high-level flexible framework for building Python applications quickly. Applications run on Django store data, by default, into a SQLite database file, but lots of Django users find themselves needing to switch to a more performant database in production, one with better availability or scalability. This is a short tutorial on using Django with CockroachDB as your database. At the time of writing, django-cockroachdb library is available in two versions, (2 and 3). This post focuses on version 2 specifically. This tutorial is based on the Digital Ocean tutorial \"How to Use PostgreSQL with your Django Application on Ubuntu 14.04 .\" There's one important difference between this and the PostgreSQL tutorial (which I'll highlight again, below): this tutorial uses Python3. For everything else, we'll follow the Digital Ocean PostgreSQL/Django tutorial as is. Since we’re going to need role-based access management (RBAC) features of CockroachDB, we'll require an enterprise license. Feel free to request a trial or try this tutorial in a cockroach demo environment. Using the cockroach demo command will enable enterprise features for 60 minutes---more than enough time to complete this tutorial. Install the Components from the Ubuntu Repositories sudo apt-get update\nsudo apt-get install python3-pip python3-dev libpq-dev Create a Database and Database User My CockroachDB instance is running on 10.142.0.46 in insecure mode on port 26257, but if you're playing along at home, feel free to run it locally. These details will be necessary when we’re going to configure Django. You can validate connectivity by accessing CRDB SQL shell with the following: ./cockroach sql --insecure --host=10.142.0.46 --port=26257 First things first, enable enterprise features by passing your trial license and organization to the following properties: SET CLUSTER SETTING cluster . organization = ' Acme Company ' ; SET CLUSTER SETTING enterprise . license = ' xxxxxxxxxxxx ' ; CREATE DATABASE myproject ; Set the desired database as default. SET DATABASE = myproject; Since we’re using CRDB in insecure mode, WITH PASSWORD command will be ignored. CREATE USER myprojectuser ; Create a role for our Django app. CREATE ROLE django; Verify that the role has been created. > SHOW roles;\n  role_name + -----------+ admin\n  django\n( 2 rows) Grant privileges to the Django role you created: GRANT CREATE, INSERT, UPDATE , SELECT ON DATABASE myproject TO django; Next, verify the grants exist on the database. > SHOW GRANTS ON DATABASE myproject;\n\n  database_name |    schema_name     | grantee | privilege_type + ---------------+--------------------+---------+----------------+ myproject     | crdb_internal      | admin   | ALL\n  myproject     | crdb_internal      | django  | CREATE\n  myproject     | crdb_internal      | django  | SELECT myproject     | crdb_internal      | root    | ALL\n  myproject     | information_schema | admin   | ALL\n  myproject     | information_schema | django  | CREATE\n  myproject     | information_schema | django  | SELECT myproject     | information_schema | root    | ALL\n  myproject     | pg_catalog         | admin   | ALL\n  myproject     | pg_catalog         | django  | CREATE\n  myproject     | pg_catalog         | django  | SELECT myproject     | pg_catalog         | root    | ALL\n  myproject     | public             | admin   | ALL\n  myproject     | public             | django  | CREATE\n  myproject     | public             | django  | SELECT myproject     | public             | root    | ALL\n\n( 16 rows) Add myprojectuser to Django role: GRANT django TO myprojectuser; Now, you can verify access to CockroachDB using the newly created user. ./cockroach sql --insecure --host=10.142.0.46 --port=26257 --database=myproject --user=myprojectuser Install Django within a Virtual Environment As a reminder, I’m using Python3, so the only difference between this and the Digital Ocean Django tutorial is that we're using pip3, not pip. pip3 install virtualenv Now, we’re ready to install Django and CockroachDB library. pip3 install django psycopg2 django-cockroachdb Configure the Django Database Settings Here, we'll set CockroachDB as the database engine. DATABASES = { 'default' : { 'ENGINE' : 'django_cockroachdb' , 'NAME' : 'myproject' , 'USER' : 'myprojectuser' , 'PASSWORD' : '' , 'HOST' : '10.142.0.46' , 'PORT' : '26257' ,\n    }\n} Migrate the Database and Test your Project python3 manage.py makemigrations\npython3 manage.py migrate\npython3 manage.py createsuperuser\npython3 manage.py runserver 0.0.0.0:8000' To download the CockroachDB backend for Django project, and to learn more about common gotchas and FAQs, head to PyPI. This article was originally posted on Artem Ervits' personal blog .", "date": "2020-02-10"},
{"website": "CockroachLabs", "title": "Welcome Docs: Getting Started at Cockroach Labs", "author": ["Chelsea Lee"], "link": "https://www.cockroachlabs.com/blog/onboarding-starter-projects/", "abstract": "Welcome, New Roacher! That’s how all of our Welcome Documents start, though you can expect yours to start with your name on top. Welcome Docs are how we help our new teammates get started and get past the natural new job jitters at Cockroach Labs. We understand there’s nothing worse than not knowing what to do or where to go on your first day, so we use Welcome Docs as a nice roadmap for this transition. In a previous blog about onboarding , I share how we structure Your First Weeks at Cockroach. We designed our onboarding process to be robust and informative, covering information about best practices at Cockroach Labs and providing an overview of our product, CockroachDB. However, it doesn’t account for the various nuances of each role, team, or department. That’s why our People team makes an effort to create personalized Welcome Docs to account for those specific role differences. In a partnership with hiring managers, we build the docs to make sure there’s clarity in what’s expected of you in your first 90 days. Here’s a bit more on what they look like: Resource Basics In this section of the Welcome Doc, we link some key team resources for you to bookmark and click through. These include our Corporate Wiki and our Shared Drives. We also share  resources specific to your team or role, like Talking Points for our Recruiting Team, or our Product Pitch for our Revenue Department. If you’ll be writing code here, we’ll walk you through how to add yourself to the AUTHORS file. So that you’re quickly looped in on important conversations, we list out some key Slack channels that you should join. We have a number of channels for different departments but also channels for customers, features, processes, or hobbies. As our number of Slack channels increases, it’s a good checklist to go through and make sure you’ve joined the ones that pertain to you. We also highlight the different email groups to which you’ve been added. It’s good to know these aliases when sharing information. Team Overview Here’s where things get more specific! When you join Cockroach Labs, you are paired with a Roachmate, an onboarding buddy to show you the ropes during your first month. You’ll meet with them weekly for the first four weeks and they’ll help get you up to speed on general company policies, technical explanations, and can connect you with the right people to answer your questions. We even draft a brief introduction to your Roachmate so that you know what they do here at Cockroach Labs and how best to use them as a resource. In an overview of your team, we explain where your team fits within the organization and how the team is organized. For some roles, you’ll be working more cross-collaboratively, so we highlight those collaborators here. We also name a couple of Roachers with whom we suggest setting up 1-on-1s. These are people we encourage you to introduce yourself to as they will be great partners along the way. To wrap this section up, we list your team’s objectives to help you understand and align with your new team’s goals for the upcoming year. These objectives are paired with key results to keep everyone accountable for targets and metrics. With planning occurring quarterly, be aware that these goals may shift and change throughout the year. Starter Projects This is the good stuff! Before your first day at Cockroach Labs, a member of our People team will meet with your manager to make sure we’re all set for your start date and to determine at least 3 starter projects for you. The starter projects can vary greatly in scope; some may take 2 weeks to complete, while others may not be finished until your first 90 days. Each of them is designed to support the objective and key results shared above. In each project, we link resources that have been built out already and suggest partners to whom you should ask questions. For roles where you need to be familiar with our codebase, we’ll kick you off by assigning you an open issue. Here are some examples of Starter Projects we’ve designed: Learn CockroachDB - Role: Training Manager As our Training Manager, it’s crucial for you to have a solid understanding of our product and its capabilities. So you’ll spend your first weeks learning: The architecture How to deploy and administer the database How to demo and explain its key capabilities and differentiators Resources: CockroachDB Docs Suggested partners: Jesse (VP of Education), Andrei (Your Roachmate), Drew (Sales Engineer) Open Sourced Interview Process Update - Role: Recruiter Why? We believe in supporting a fair hiring process. You can learn more about why we opened sourced the interview process here: Open Sourcing the Interview Process to Reduce Bias . It is the recruiter’s responsibility to keep the Github repo updated. However, we would like for you to review the repo and ensure it’s up to date. Benefits of this Starter Project: You will get a deep-dive into the interview process and culture at Cockroach Labs. This will give you a clear understanding of how we drive our hiring efforts and how to effectively execute hiring at CRL. Expected Duration: 2-4 weeks Create an Outbound Campaign for a Vertical - Role: Sales Development Representative Why? Knowing CockroachDB and learning the tools is a good first step for understanding personas that you will be contacting: Solutions Architects, Engineers, Systems Administrators. Targeting specific verticals will require further knowledge of research and messaging. What? Research personas within a vertical and partner with Marketing to create an outbound campaign. roachprod: Multiple zones are silently ignored without --geo - Role: Software Engineer Issue: https://github.com/cockroachdb/cockroach/issues/38542 This is a straightforward enhancement to roachprod, our internal tool for provisioning CRDB clusters on cloud infrastructure. This project will give you exposure to the CRDB codebase as well as some of the complexities related to creating a CRDB cluster and our internal development cloud infrastructure. Estimated Timeline: 1-2 weeks Suggested partners: Bob (Member of Technical Staff) -- Make Getting Started Easy These onboarding documents are meant to serve as a comprehensive resource for our new teammates to understand what they’re working on, with whom, and why. The above starter projects are just a snapshot of the kind of work our people kick off on day one. What the people say is true: joining a startup is chaotic. Growth will hit a hyper-phase where establishing processes like this assist greatly when you’re trying to keep up with your headcount. To any other People people who might want to leverage Welcome Docs, our advice is to implement the process, then make it a template. Shortly after we started these Welcome Docs, we created a template for managers to copy and create their own. Nowadays, I’ve seen managers with upcoming new hires build out Welcome Docs on their own. They will create them for employees growing into a new role or as a way to onboard consultants that are working with us for a limited time. This process and partnership greatly reduce the burden on the People team as it enables our managers to ramp up their new teammates successfully. Metrics and Data Welcome Docs came to fruition as a result of data and surveys we collect on our team. After about 30 days, we send a short New Hire Survey to get feedback around the onboarding and understanding of Cockroach Labs. In the first quarter of last year, only 67% of new Roachers agreed that they understood what was expected of them in the first 30 days. We rolled out our first Welcome Docs at the start of Q2 and watched the agreement percentage climb from there. Q1 Q2 Q3 Q4 n = 18 n = 27 n = 24 n = 13 It was clear what was expected of me the first 30 days on my job. 67% 89% 96% 100% Going into 2020, we’re revamping this question to dig deeper into what the Welcome Docs can enable our team to do. Over the next year, we’ll be measuring the agreement of “I was able to effectively contribute to the team in my first 30 days,” in hopes that our Starter Projects allow Roachers to do just that. By being transparent about expectations and providing resources to meet them, we can better set people up for success and sustain it, too. Wondering what your Welcome Doc will look like? Come join us !", "date": "2020-02-05"},
{"website": "CockroachLabs", "title": "Build a Java App with CockroachDB and jOOQ", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/announcing-jooq-support-for-cockroachdb/", "abstract": "We’re excited to announce that the latest release of jOOQ supports CockroachDB. jOOQ is a database-mapping software library in Java that lets you take advantage of your database’s SQL capabilities while still coding in Java. As of this writing, jOOQ supports over twenty relational database management system s, from PostgreSQL to Oracle. But with most of these options (especially the open source ones) developers are still left to manage database scale and ensure the database is resilient and always on. Since CockroachDB is wire compatible with Postgres , some jOOQ users had used the Postgres dialect with CockroachDB for simple integrations, in order to continue writing SQL in Java while using a database that shards automatically and is highly available. As of last week’s 3.13.0 release, jOOQ fully supports CockroachDB. The CockroachDB SQL dialect is now fully integration tested and supported for both future jOOQ and CockroachDB versions. With jOOQ for CockroachDB , you can efficiently build your apps using Java , and also avoid all the headaches of scaling and downtime. Get started today by downloading jOOQ, or checking out our tutorial on how to build a Java app with CockroachDB and jOOQ. Get started with jOOQ", "date": "2020-02-19"},
{"website": "CockroachLabs", "title": "AWS, Azure, & GCP Respond to the 2020 Cloud Report", "author": ["Paul Bardea", "Charlotte Dillon", "Nathan VanBenschoten", "Andy Woods"], "link": "https://www.cockroachlabs.com/blog/aws-azure-gcp-respond-to-the-2020-cloud-report/", "abstract": "[THE 2021 CLOUD REPORT IS AVAILABLE. READ IT HERE] In December 2019, we published the 2020 Cloud Report and an accompanying blog post , which summarized original research we conducted benchmarking the performance of Amazon Web Services (AWS), Microsoft Azure (Azure), and the Google Cloud Platform (GCP). The response from our customers and the cloud providers was tremendous. Before publishing, we shared our findings with each of the benchmarked clouds and allowed time for their review and feedback. After publishing, we followed up to hear further feedback and to improve upon next year’s Cloud Report. We also met with a number of customers to answer follow-up questions about what our learnings meant for their mission-critical workloads. Much of the feedback we heard from the cloud providers was with regards to how each cloud was tuned for performance. In our conversations with AWS, Azure, and GCP, we heard directly how we could have better tuned their products and services for optimal performance both on benchmarks and on real-world workloads. We also discussed how we might revise the testing plan for the 2021 Cloud Report in order to gain an even more thorough picture of OLTP workload performance across AWS, Azure, and GCP. The following blog post highlights some of our key learnings about how to get the best performance from each cloud, as well as changes we are contemplating to the testing suite for the 2021 Cloud Report. Background: Why We Used Default Cloud Configurations for the 2020 Cloud Report CockroachDB used each cloud’s default configuration for running the benchmarks included in the 2020 Cloud Report. We opted to use the defaults because we wanted to avoid biasing the results with claims that we better configured one cloud over another, and we thought it would be representative of performance as many cloud customers would not know how to configure machine types differently from the base defaults. Deciding how much tuning one should do versus selecting the default values is a delicate balance to strike. For example, one cloud may offer a feature as a tuning parameter, while another lists it as a seperate machine type.  Additionally, some defaults may be constrained, and in these cases there should be sufficient guidance in the documentation. While we stand by our decision to use each cloud’s default configuration, we learned that some of these default configuration settings can result in unintended consequences. Amazon Web Services’ Performance Improvement Suggestions AWS showed well in both of the previous cloud reports. As a result, the majority of their feedback has been on what additional machine types they recommend we test with in the future as well as in which tests are best in class to measure OLTP database performance. Machine Types AWS is the only cloud where we tested AMD machines. Specifically, we tested four different AMD EPYC 7000 series instance types, which each contain an “a” specifier. The CPU benchmarks do not show a significant difference one way or the other when comparing these against the Intel processors. Since there’s so much variance in processor types available on AWS, we think it's worth paying extra close attention to the processor type used with AWS. AWS recently announced support for new machine types which were not available when we ran numbers 2020 Cloud Report. AWS claims that the new “general-purpose (M6g), compute-optimized (C6g), and memory-optimized (R6g) Amazon EC2 instances deliver up to 40% improved price/performance over current-generation M5, C5, and R5 instances for a broad spectrum of workloads including application servers, open-source databases, etc.” We’re excited to test these machine types out and seeing AWS invest in even more improvements. Azure’s Performance Improvement Suggestions Azure provided a large amount of feedback on aspects of the configurations we ran in the Cloud Report, in large part because they offer so many configuration options that it can be hard to get yours right out-of-the-box. Machine Sizing As we noted in the report, testing the number of vCPUs allocated to the VM instances was out-of-scope for the 2020 Report. We only focused on 16 vCPU VMs, which offers some advantages such as a smaller price discrepancy between clouds. Although not tested, according to the documentation, Azure noted that their larger VMs “perform and scale linearly, compared to than most competitive SKUs which allocate disproportionate resources to smaller sizes.” Azure went on to claim that “AWS and GCP skew network allocation of network and remote storage on their smaller VMs whereas Azure allocates proportionately to size. As a result, as Azure scales up to larger VM sizes (32 vCPUs+), their performance typically improves much more than AWS and GCP.” We didn’t test this effect, but it's worth noting that varying CPUs sizes could have a large impact on performance, particularly with Azure. It’s also important to note that these scaling claims may be dependent on the workload. CPU CPU is mostly influenced by the processor type and (at the time of writing) most Azure systems use older Haswell/Broadwell types. Like AWS, Azure will start using new AMD based Da/Ea v4 systems soon which they note “perform comparably to the newer Intel systems” tested in this report on the other clouds. Azure will additionally offer some Skylake Dv3/Ev3 as well as newer Intel CPUs in 2020. It’s worth noting that some of Azure’s older systems (GS4, DS14v2) did well “because they’re full core vCPUs while the newer systems (Dv3/Ev3) will be hyperthreaded (like AWS and GCP).” Network Throughput & Latency Azure offers a large number of configuration options that can be tricky to get right. In the 2020 Cloud Report, we provisioned Azure VMs using the REST API command line. Accessing the REST API directly  is an advanced deployment method with precise controls and currently requires explicit flag to enable Accelerated Networking. If we had provisioned these nodes directly from Azure’s UI, however, they would have been configured with Accelerated Networking. Azure indicated to us that Accelerated Networking dramatically improves performance and they enable it by default in the CLI when the NIC is allowed to be chosen at creation time, rather than creating the NIC before the VM. This intricacy can be confusing to end users (and to us) and can make it tricky to determine how best to provision Azure nodes. Additionally, we configured both “regions” and “zones” for AWS and GCP while only configuring “locations” (e.g., regions for Azure and not “proximity placement groups” or zones). As a result, Azure pointed out that “this leads to longer physical paths (through the host and across greater distances) in the region and hence greater latency.” Further muddying the water, AWS subnets are restricted to a single zone (impacting placement) while Azure subnets are not. Since AWS restricts placement to a single zone, the machines will be physically closer together. The primary effect of this is that latency will be reduced between the machines (since the signal has to travel less physical distance). In our report, we mentioned that Microsoft did not publish network throughput expectations for their VMs. This was inaccurate and those expectations can be found in their documentation , although they were somewhat tricky for us to track down: Standard_DS14_v2: 12,000 Mbps (DS15_v2 is 25,000 Mbps) Standard_E16s_v3: 8,000 Mbps (E64s_v3 is 30,000 Mbps) Standard_F16s_v2: 7,000 Mbps (F72s_v2 is 30,000 Mbps) Standard_DS5_v2: 12,000 Mbps Standard_GS4: 16,000 Mbps (GS5 is 20,000 Mbps) Standard_DS14: 8,000 Mbps (older hardware, closer to AWS M2/M3 generation) Azure went on to share with us that Azure HB120rs (v2) can use a local Infiniband connection to hit 200 Gb/sec while HC44rs and HB60rs can do 100 Gb/sec on Infiniband, and 40 Gb/sec on local network. They again referred to their documentation here, indicating that “most of our full node VMs (e.g. D/E[a]64[s] v3) can do 30-32 Gb/sec, or 4x faster than the 16 vCPU VMs. AWS, by contrast, is only ~2.5x faster (16 -> 64 vCPUs).” Storage Azure offers a number of differentiated storage types. Again, we chose the defaults for each VM type where possible. Azure doesn’t offer default storage types so we chose the Premium SSD which appears to be equivalent to AWS and GCP. While we think this is defensible for our test, there are many options available. Azure highlighted that they expect their various permutations to behave in the following manner: Ultra Disk:  2-1 60,000 IOPS /disk; 1- 2000 MB/sec /disk (depending on disk size and configuration) Premium SSD:  120- 20,000 IOPS /disk (burst to 3,500 for disk sizes upto 512 GiB); 25- 900 MB /sec/disk (burst to 170 for disk sizes up to 512 GiB) Standard SSD :  120- 6,000 IOPS /disk; 25- 750 MB /sec/disk Standard HDD: 500- 2,000 IOPS /disk; 60- 500 MB /sec/disk Performance is also constrained by VM caps (see VM docs) Azure also provides powerful host caching options which can greatly improve persistent storage read (and write) performance. Again, like many of Azure’s capabilities, this requires custom configuration. Azure Lsv2 was not tested in the 2020 Cloud Report due to accidental omission. This machine type is Azure’s offering in the storage-optimized class. Although not included in the report this year, it will surely be considered as a candidate in 2021. Stay Tuned for Improvements from Azure Azure will be offering newer VM SKUs in 2020 with “higher CPU performance” such as the Da_v4/Ea_v4 (AMD based) and additional Intel machines. They also plan to make improvements in the read/write latency across all storage types in 2020. We’re excited to see their improvements in 2020. GCP’s Performance Improvement Suggestions GCP’s new n2 series and c2 series make a big difference across many tests and they have future planned improvements as well. GCP took a big step forward with their network and shared some insight into their storage configurations. Machine Types GCP introduced both the n2 series and the c2 series earlier this year, both of which use the new Intel Cascade Lake Processor. We see from the benchmarks these result in significantly higher performance than the corresponding n1 series instances, which use last generation Intel Skylake processors. CPU GCP suggested using --min-cpu-platform=skylake for the n1 family of machines because they have higher network performance caps. [Important note: In our report, we mentioned that GCP did not publish network throughput expectations for their VMs. This was inaccurate and the expectations can be found at here .] Like we see with other clouds, newer processors perform better than older processors. It’s critical to stay on top of the newest processor types offered by GCP and to explicitly request them when provisioning nodes, since they’re not on by default. It's also worth noting that this CPU platform is only available in some regions/zones, so you’ll need to consider availability in addition to performance. Network Throughput & Latency After the publication of the first Cloud Report in 2018 , GCP confirmed that they were planning on investing heavily in network performance in the coming year. As promised, they rolled out increased network throughput limits over the past year, and we see those performance improvements reflected in the 2020 Cloud Report numbers. This type of increase benefits customers not just because it increases the amount of Network Throughput, but because it requires no direct intervention from GCP customers. As we noted in the 2020 Cloud Report, the n1 series offers approximately the same network latency as we observed in our previous report. However, unlike in our CPU experiment, GCP’s n2 series dramatically improved its network latency. In addition, the c2 series offers even better network latency. GCP notes that customers would benefit from these improvements without even needing to reboot their VMs. GCP prefers using Ping with the -i=0 setting. This is only available if users employ sudo before running the command. We ran -i=.2 as our defaults across all three clouds. GCP claims that -i=0 might be more representative and it is something we will consider for next year’s report, also suggest investigating other benchmarking tools such as netperf with the TCP_RR option. Even GCP noted that “it will make everyone’s numbers better and more consistent.” They attribute this smoothness to the reduced awakening of cstates, something we attempted to avoid this year by switching from the less frequent default periodicity we used in the 2018 Cloud Report. Storage GCP doesn't have a storage-optimized instance, but local SSD can be attached to most VMs with either NVMe or SCSI interfaces. SCSI is the default from the CLI, so users should be careful to specify NVMe if that’s what you’re after. GCP intentionally manages the number of different machine types available to reduce the complexity for its users. Further, rather than needing to completely switch machine types, GCP provides customers the flexibility of starting with one config and evolving over time as many aspects of the machine can be tweaked to suit your workload. GCP noted that SCSI is the default since it supports more operating systems, but references users to their documentation for guidance in choosing a local SSD interface. We like the flexibility GCP provides in allowing users the ability to configure the number of SSDs attached to a single host. Other clouds are not as accommodating. Stay Tuned for Improvements from GCP We expect GCP to focus heavily on rolling out their n2 and c2 series machines in 2020 given the advantages over the older generation series. In addition, GCP is focusing on their new AMD machine types in the n2d series, and they expect to continue to improve performance across their VM offerings. GCP will also be rolling out new AMD machine types and they expect to continue to improve performance across all lines of VMs in 2020. Our Conclusions on Configuring AWS, GCP, and Azure for Performance We stand by our decision to use default configurations in this experiment, and welcome continued feedback from the clouds. Choosing how to configure your cloud is not intuitive, and obviously, many developers don’t have the benefit of speaking directly to the cloud providers to learn best in class performance configurations. With that in mind, we think it’s hugely valuable to keep benchmarking cloud performance, and will keep these suggestions in mind as we begin work on the 2021 Cloud Report. 2021 Cloud Report Test Plan Per the feedback we received while working with Amazon, Azure, and GCP, we’ve decided to make a couple tweaks to the way we run our microbenchmark tests for the next issue of the Cloud Report. CPU stress-ng For the 2020 Cloud Report, we tested all machine types using stress-ng’s matrix stressor. This stressor provides a good mix of memory, cache and floating point operations. We found its behavior to be representative of real workloads like CockroachDB. Because of this, the results we have seen on stress-ng have a strong correlation with the results we have seen in TPC-C. This is in contrast with the cpu stressor, which steps through its 68 methods in a round-robin fashion, allowing it to be disproportionately affected by changes in some of its slower, less-representative methods like stressing deeply recursive call stacks. GCP and AWS both shared concerns about the cpu stressor with us ahead of time and our own experimentation validated those concerns. The results we found using the cpu stressor were difficult to explain across CPU platforms and were not useful predictors of TPC-C performance, indicating that it is not a representative benchmark. We therefore decided only to present results using the matrix stressor. In our open source machine provisioning tool Roachprod, we used ubuntu 16.04 (and therefore stress-ng 0.05.23) with AWS and GCP but ubuntu 18.04 (stress-ng 0.09.25) with Azure. We didn’t do this maliciously, rather, it was because we added Azure to Roachprod this year and we added AWS and GCP last year. As scientists, this is a mistake because we should be holding as many variables constant as possible. As pragmatists, it is an even bigger mistake because different versions of Ubuntu install different versions of stress-ng by default through the Debian package manager. And, unsurprisingly, different versions of stress-ng stress test CPU differently resulting in incomparable data. We re-ran the CPU data after accounting for these changes and included it in the 2020 report. The updated results still put Azure in the lead, but considerably reduced their margin by approximately 50%. . Moving forward we will hold both the operating system and tooling versions consistent across each cloud. Evaluating other tools We are considering other measures of CPU for next year’s cloud report. The current measure, stress-ng, is sensitive to underlying microarchitectures and other tools may be better suited for benchmarking. Both GCP and AWS recommend Spec. We get the sense that they internally test against this regularly but are hesitant to switch to it as Spec has all sorts of legal guidelines for reporting. Further, Spec is not free and would therefore make it harder for the public to verify our benchmarking claims. As we’ve written previously, a benchmark that can’t be reproduced isn’t really a benchmark. We’ve also received recommendations to explore a combination of Linux Bench, Embedded Microprocessor Benchmark Consortium’s Coremark, and Baidu’s Deepbench to provide a more complete picture of CPU performance. Network Throughput In the 2020 Cloud Report, we used iPerf 2 to test each cloud’s network throughput. Another version of iPerf, iPerf 3, also exists. Somewhat confusingly, iPerf 3 isn't really the \"new version\" of iPerf 2. iPerf2 and iPerf3 have been maintained in parallel for several years and have diverged feature sets. We will re-evaluate the selection of iPerf 2 to ensure that it is still the version that provides the most representative results. Network Latency In the 2020 Cloud Report we used ping to measure network latency. Azure claims that ping is not representative of database IO as ping (ICMP, connectionless) latency is very different from (TCP/IP, persisted connection) latency. ICMP is a non-performance critical protocol and is not “accelerated” by Azure while TCP is accelerated. As a result, ping takes a longer path through the host than TCP data packets. In theory, ping could be responded to by a load balancer which considerably cuts down the RTT, but Azure doesn’t do this. Azure likes the tool Sockperf configured in either ul # under-load or pp # ping-pong mode with the corresponding parameters --tcp # tcp, -m MSG_SIZE # msg size, default is 14B, and -t TIME_SEC # time in seconds, we normally do 5 min/300s. We will explore this test next year. GCP similarly doesn’t care for ping. Like Azure, GCP prefers TCP. GCP recommends Netperf for testing network latency via TCP_RR. GCP is also familiar with iPerf (already used by the Cloud Report for Network Throughput). Storage We provided results from local and network-attached storage in intermingled charts, which can make it hard to differentiate between the two. Performance differences can vary as much as 100x, which makes it hard for our readers to observe differences. Next year, we’re going to l separate the storage classes into separate sections, making it easier to differentiate as well as provide numbers in the charts for easy comparison. We use Sysbench as the microbenchmark to measure storage. Azure believes that Sysbench isn’t optimally configured for the filesystem testing done and may not be the best tool for the job. They stated that focusing benchmarks that fsync frequently are not effective because they “serialize[s] writing (and some reading) and therefore limit[s] parallelism based on the latency of the device. In other configs like Direct I/O, other I/O platforms could exhibit greater parallelism and yield higher throughput at higher latency.” However, as an OLTP database with a strong focus on durability and resilience, we think tests that stress fsync performance are critical. lIn next year’s Cloud Report, we’ll definitely spend more time to fine tuning these configurations and more comprehensively explaining how we came to this configuration. Additionally, it was pointed out that the current block size is too large to maximize IOPS performance and too small to maximize transfer rate performance. We’ll re-evaluate the block sizes that we use in the I/O tests in the 2021 Cloud Report to more closely match expected workloads. TPC-C After we released the report we realized that we didn’t provide clear TPC-C reproduction steps. We have since updated the reproduction repo to include a link to TPC-C reproduction steps. In an early version of the report (which has since been updated), we didn’t make it clear that these results were obtained using the nobarrier ext4 mount option, which matched the previous Cloud Report. Some clouds believe that this is not appropriate because nobarrier can leave machine types vulnerable to disaster events. In fact, these disaster events are part of the very reason we made CockroachDB highly available by default. Unfortunately, none of the big three cloud providers share their expectations for surviving disaster events with or without nobarrier so it makes it challenging to compare their survivability. Some clouds have larger performance margins with and without nobarrier, which may have influenced this feedback. For example, in 2018 we saw a narrower gap in performance when using AWS with and without nobarrier as compared to GCP. We believe TPC-C performance benchmarking has stood the test of time, something we’ve written about extensively . Azure shared with us that they “typically prefer TPC-E” as they believe it to be “designed to be a more realistic OLTP benchmark than TPC-C” citing this study from Carnegie Mellon . We haven’t yet explored TPC-E, but will plan to in the future. For now, it’s worth noting that “TPC-E has a 9.7:1 Read/Write ratio and more varied transactions” while “TPC-C has a less typical 1.9:1 Read/Write ratio and a simpler IO model.” Conclusion AWS, Azure, and GCP all plan to continue to work with Cockroach Labs as we produce future versions of this report as they all want to put forth the most accurate numbers possible in the community. We plan to continue to reach out to them during the research phase of each report so that we can collect the best information to share with our customers.", "date": "2020-03-12"},
{"website": "CockroachLabs", "title": "Community Tutorial: Using NPoco with CockroachDB and C# / .NET Core", "author": ["Darrien Rushing"], "link": "https://www.cockroachlabs.com/blog/npoco-dotnet-cockroachdb/", "abstract": "Hi, I’m Darrien, and I’m building a distributed, near- real time, “OSINT data reconnaissance” system. For this system, I’ll be using CockroachDB for its distributed nature, and connecting it to a .NET Core API via NPoco and Npgsql . In this post, I'll give a w alkthrough of how I'm building the system, and how these connections work, so you can use them too. An Exercise in Curiosity and Data Sets When I first learned about network penetration testing I was enamored with the reconnaissance tools available for gathering data. However, I quickly learned these tools returned disparate data sets and formats. So this presented two problems: The lack of organization caused writing reports to be just as tedious as performing the test. Differing data formats make it difficult to relate and analyze the data sets from the tools. I thought introducing some level of automation could make this “gather and organize” process easier (some clever modeling could alleviate that). To take this idea a step further, if such a system was designed to be distributed across IoT hardware it could introduce new possibilities for physical network pentests. For example: synchronize WiFi signals with perimeter photos. Previous attempts at designing such a system left me with scalability issues. However, a studious suggestion from a professional mentor turned me to Apache Kafka and CockroachDB to handle the data storage and distribution concern of the project. Together, these two technologies could give me distributed message queue connected to a persistent data store! Being most of my software engineering experience has been in C#, I was left with a question of how to move data from .NET Core to this new distributed database I had never seen before. I’m electing to use the NPoco micro-ORM to make the connection a little easier. So, below I’ll be covering how I connected .NET Core, NPoco, and CRDB. Currently, this is the (loose) tech stack of the larger OSINT project: Front-end : React + redux Back-end : C#/.NET Core, SignalR, Rx.NET, NPoco “Data flow” : CockroachDB, Apache Kafka OSINT tooling/consumption : Python + Flask, metagoofil, recon-ng, FOCA, and nmap NPoco, Or: Why use an ORM? I’ll admit my immediate reasoning for using an ORM because I commonly rely on one when interfacing with a SQL-compliant database. So, it’s familiar. Also, in my opinion, it helps the data access code become predictable. A few other immediate benefits: Improved code readability around queries Easier data mapping to declared types hopefully...leading to quicker app prototyping Choosing an ORM (or micro-ORM for that matter) is almost never an objective decision. For a while EntityFramework was the go-to data framework for C#. Then later versions had some reported performance issues, hence there was a push to third-party libraries. Granted, a lot of improvements have been made with the release of Entity Framework Core, but in that time I’d already grown accustomed to NPoco . If you’d rather use something other than NPoco--go ahead! Just note that this post is specific to NPoco and CockroachDB. Alright, let’s jump into the tutorial. Tutorial: Using NPoco with CRDB and .NET Core Prerequisites: If you don’t already have a running version of CRDB, follow this tutorial by Cockroach Labs to get started. We’ll use a slightly different schema than used in that tutorial, but if you’ve never used CRDB before it’s a good place to start. It’s not required, but you may also want to read through this “ Build a C# App w/ CRDB ” article as well. I’ll be borrowing heavily from the code in that article to make a connection to our database. Step 1: Create a New .NET Core Web API Project First, we’ll walk through the normal initial steps of creating a new .NET Core Web API project in Visual Studio. Be sure to target .NET Core 3.x. Step 2: Install Packages Since CockroachDB is compatible with PostgreSQL , we are able to use Npgsql as a data provider , and don’t have to worry about one built specifically for CRDB. Further, we are able to use NPoco as a micro-ORM for the same reason: it has Postgres support built in. Step 3: Prepare the Database and create our schema Before we do anything else, we’ll need to ensure CRDB is running and start a SQL session so we can issue commands. Open a new terminal to start CRDB: cockroach.exe start --insecure In another terminal session, start a SQL session. Our work will take place here. cockroach.exe sql --insecure Note: If you still have an ‘accounts’ table in your ‘bank’ database (from the article linked above) you’ll need to drop it to continue: DROP TABLE bank.accounts; Now we create the database ‘bank’, and set it as the database we’ll be working from. CREATE DATABASE bank;\nSET DATABASE TO bank; Finally, we create our ‘accounts’ table with the following: CREATE TABLE accounts(\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\n\taccountownwer STRING,\n\tbalance DECIMAL(15,2),\n\tdatecreated TIMESTAMPTZ,\n\tlastupdated TIMESTAMPTZ); Step 4: Create a Basic Model Now that the database is setup, let’s create the class our data will be mapped to. In your project, create a folder and name it “DataAccess”. Inside that folder, create a new class: Account public class Account\n{\n    \tpublic Guid Id { get; set; }\n    \tpublic string AccountOwner { get; set; }\n    \tpublic decimal Balance { get; set; }\n    \tpublic DateTime DateCreated { get; set; }\n    \tpublic DateTime LastUpdated { get; set; }\n} Then we’ll decorate the class with these three attributes. These attributes are the mechanism NPoco uses to map between SQL and your C# classes--so they are crucial! [TableName(“accounts”)] [PrimaryKey(“id”)] [Column(“id”)] Afterwards, your class should look something like this: The TableName and PrimaryKey attributes map the underlying class to key pieces of the specified table schema. If they aren’t specified here, then you will need to specify them later in the various NPoco methods querying the database. The same importance can be placed on the Column attribute. If it’s not defined here, there is no reasonable way for NPoco to map class fields to table columns. Further, notice that all of the attribute names included are completely lowercase. Not camelCase or Pascal-cased, strictly lowercase . This was tricky until I realized how important the casing was, and it resulted in a few mapping failures. Paying close attention to error messages was key here. A Brief Note on Data Types Matching types was the most difficult part of the puzzle. I used an (educated) guess-and-check process to determine which CRDB types matched with the C# types. Several times using the wrong schema in CRDB didn’t break the application, but did result in data being inserted in an unexpected format. Determining the CRDB type for a C# Guid and DateTime were the most troublesome for me, because of the extra flexibility CRDB provides. However, discussing the minutiae of these data types is outside of the scope of this article. So, if you’d like to know more, take a look at this CRDB data type reference . Step 4: Create Basic Database Class Now that our data mappings are created, let’s make the connection from C# to CRDB. Back in the project, create a new class called “Database”. This is the class we’ll use to establish a connection to CRDB and write some basic CRUD operations. The implementation is rather simplistic, but it will work for our needs. First, we lay out a constructor for our class, and use the NpgsqlConnectionStringBuilder class to build our connection string in pieces. Notice, these values are just the defaults with the database ‘bank’ coming from our previous setup. This code is pulled directly from “ Build a C# App with Cockroach DB ”. Kicking the Tires Awesome! At this point we can start using NPoco to write some simple queries. Let’s take it for a spin. Simple Actions: Select & ‘where’ clause Let’s start with retrieving a single record. First, we’ll use a couple “ using ” blocks for creating the connection to the database and a database object. This also sets up automatic disposing of the resources we’re using for the database connection. This method selects a single record based on the ‘id’ parameter. First, NPoco opts for using the term “ Fetch ” instead of “ Select ”, but you can think of these synonymously. Then, we use the LINQ expression “ Where ” to filter the resulting list based on the “ Id ” field. Finally, we call “ FirstOrDefault() ” to return a single Account , not List<Account> . public Account GetSingle(string id)\n{\n        Account acc;\n\n        using (var conn = new NpgsqlConnection(connectionString))\n        {\n            using (var db = new NPoco.Database(conn))\n            {\n                db.Connection.Open();\n                acc = db.Fetch().Where(a => a.Id == new Guid(id)).FirstOrDefault();\n                db.Connection.Close();\n            }\n        }\n\n        return acc;\n} Simple Actions: Insert NPoco makes inserts really simple. We supply the object to be inserted as a parameter, and call “ Insert() ”. db.Insert<Account>(\"accounts\", \"id\", account); The method I’ve referenced has a few other parts to it, though. Here, “ Insert() ” becomes “ Insert<Account>() ” to specify we’re acting on the Account type. Further, an overload for “ Insert ” is used which requires specifying the table, primary key, and finally the object for insertion. This is one of those weird cases where I encountered some strange behavior of NPoco, the PostgreSql driver, and CRDB not lining up exactly. So, I resolved to supplying as much information as possible to the method. Simple Actions: Update/Upsert There are several ways to update records with NPoco, and it can be done with a lot of nuance. However, I’ll keep things simple, and cover two of the simpler ways. For an update, we supply the object after field updates have been made , so the record should still have the same unaltered ‘id’ from the database. Then we pass this to NPoco’s “ Update ” method. Remember those attributes we decorated the Account class with?--TableName, PrimaryKey, and Column. NPoco uses that metadata to create the proper update statement in SQL! An “upsert” is just as easy. The “ Save ” method in NPoco is known to perform an “upsert” operation, as documented here: https://github.com/schotime/NPoco/issues/295 . Other than that, we supply our already updated object the same as we did for “ Update ”, but we specify “ Save ”. That’s it! Simple Actions: Delete Deletes are just as easy. We supply our object as a parameter, and call “ Delete() ”. Again the metadata supplied by the attributes on the Account class allow NPoco to do its heavy-lifting for this. var res = db.Delete<Account>(account); Once again, I encountered some weirdness between NPoco, the Npgsql driver, and CRDB when I tried to supply only a record’s primary key value to perform a delete. I wasn’t able to track down what was failing, but using the whole object, like above, worked well. Benefit of This Approach Each ORM has its virtues. I chose NPoco in part because it offers the flexibility to submit complex SQL queries without leaving the pattern used in simpler queries-- and with full control over parameterization. Just for fun, below we have an (admittedly contrived) example of how we could implement a CTE query while still utilizing a minimal amount of NPoco. https://www.cockroachlabs.com/docs/stable/common-table-expressions.html string col3 = \"someColumnName\";\nconst string MyTableName = \"some_special_table\";\n\n        \tusing (var conn = new NpgsqlConnection(connectionString))\n        \t{\n            \tusing (var db = new NPoco.Database(conn))\n            \t{\n                \tdb.Connection.Open();\n\n                \tvar res = db.Execute(new Sql($@\"\n                    \tWITH bankers_cte AS (SELECT * FROM employees WHERE type == 'banker')\n\n                    \tselect col1, col2, {col3}\n                    \tfrom {MyTableName} sst\n                    \twhere sst.col1 == bankers_cte.colX\", col3));\n\n                \tdb.Connection.Close();\n            \t}\n        \t} In case you aren’t familiar with C#’s string formatting options, the ‘$’ character provides a sugar-syntax alternative to the older “ String.format() ”, and the ‘ @ ’ allows for multi-line string. A nice two-handed punch for when you want to write multi-line SQL queries in your code without confusing formatting issues. Closing: Quick Gains, a Word about a Prototype Project I’m still in the early stages of working with CRDB and NPoco, so there are no guarantees that I’ll continue with this pairing, but already it’s allowed me to get off the ground with CRDB just like I would any other SQL-compliant database. For now, it’s great to be able to query data knowing CRDB is doing the hard work underneath to keep things scalable. I’ll continue developing my prototype application, and should this crazy idea work I’ll share more about prototyping with CRDB and my adventures with its distributed and “ real-time ” capabilities. As a part of that continued effort, I’d like to experiment with adapting this architecture for IoT, specifically on Raspberry Pi. The release of the RaspberryPi 4 opened the hardware up to 64-bit processing and now 4GB of RAM. Ideally , all of this experimentation would prove a concept for a data streaming system, running on IoT hardware, across a MANET (mobile ad-hoc network). Maybe it’ll work...maybe I’ll fry a few boards. ;)", "date": "2020-01-30"},
{"website": "CockroachLabs", "title": "Hash Sharded Indexes Unlock Linear Scaling for Sequential Workloads", "author": ["Aayush Shah"], "link": "https://www.cockroachlabs.com/blog/hash-sharded-indexes-unlock-linear-scaling-for-sequential-workloads/", "abstract": "I ended an amazing internship this past fall on the KV (Key-Value) team at Cockroach Labs (responsible for the transaction , distribution and replication layers of CockroachDB). This blog post delves into my work on adding native support for creating hash sharded indexes in CockroachDB, as a way to significantly improve performance on sequential workloads. CockroachDB uses range partitioning by default, as opposed to hash partitioning. As explained in our CTO Peter Mattis’s blog post , a key-value store with range partitioning resembles a distributed balanced tree whereas one with hash partitioning is closer to a distributed hash map. In particular, range partitioning outperforms hash partitioning for the most common SQL workloads like range scans. However, load under range partitioning can become imbalanced for access patterns that focus on a specific range of data, since all traffic is served by a small subset of all the ranges. Sequential insert traffic is a common example of such an access pattern and is much better suited to hash partitioning. While it was possible to achieve hash partitioning in CockroachDB with a bit of SQL wrangling, it required relatively in-depth knowledge of how CockroachDB worked. Additionally, the syntax, when hashing multiple columns (of all the various SQL data types), gets unwieldy. Hash sharded indexes allow users to use hash partitioning for any table or index in CockroachDB. Ranges In order to understand why hash sharded indexes are important, it’s helpful to know how the KV layer (which handles distribution and replication of data) works in CockroachDB. At the lowest level of CockroachDB lies a distributed, replicated, transactional key-value store. This key-value store holds a global key-space that all SQL tables and indexes are mapped to. Now this key space is divided into sorted contiguous chunks called “ranges”, with each range being a certain maximum size (which is configurable). A new CockroachDB cluster essentially starts off with a single range, which then splits/merges into more or less ranges as data is added/removed. These ranges are replicated and stored on nodes that are part of a CockroachDB cluster. Each range is a consensus group, based on the Raft protocol. When we create an index on a CockroachDB table, we essentially create a new set of ranges that hold the set of index columns and the set of primary key columns, ordered by the set of index columns. The latter allows CockroachDB to locate the range entry for the full row, given a set of values for its index columns. So, ranges of a SQL table are ordered by the set of primary key columns whereas ranges of a secondary index are ordered by the set of columns in the index . Motivation Due to the fact that ranges are ordered by keys, workloads with sequential keys will cause all traffic to hit one of the boundaries of a range. CockroachDB tries to split frequently accessed ranges into multiple smaller ranges (referred to as load-based splitting ). It also tries to redistribute these ranges across the cluster based on load , in order to achieve an even load distribution in the cluster. However, in order to split a frequently accessed range, the system looks for a point in the range that would divide the incoming traffic roughly evenly. In case of a sequential workload hitting one of the boundaries of a range, there is no such split point . This leads to a single range hotspot . Data simply gets appended to the end of a range until it reaches its maximum size threshold and then to the end of a new range. This means we don’t get many of the benefits of a “distributed” database, as our query performance is bottlenecked by the performance of one single node (i.e. the Raft leader of the concerned range). One way to alleviate these kinds of single range hotspots is to prefix the concerned primary or secondary index with a computed shard column. This would convert the incoming sequential traffic on a key into uniformly distributed traffic , which allows load-based splitting to split the concerned range at a point that divides the incoming traffic evenly. However, as mentioned before, this hash partitioning scheme was complicated to implement in CockroachDB. Why do hash sharded indexes matter? In order to demonstrate why single range hotspots are bad, and to show the performance implications of hash sharded indexes, we wrote a benchmarking tool that measures the maximum sustained throughput a given cluster can support while keeping the average latency below a specified threshold (which is 10ms for the following benchmarks). Consider the following table: CREATE TABLE kv (\n    k INT8 NOT NULL,\n\tv BYTES NOT NULL,\n\tPRIMARY KEY (k)\n); We look at how a sequential write workload performs on kv , across two types of schemas: one based on CockroachDB’s default range partitioning (“Unsharded”) and another based on hash partitioning (“Sharded”), where the primary key is sharded into 10 buckets. We show performance for increasingly larger clusters (5, 10 and 20 nodes, respectively) in order to demonstrate how we can unlock linear scaling for sequential workloads by using hash partitioning instead of the default: Avoiding single range hotspots in sequential workloads allows CockroachDB’s various heuristics to spread the load out evenly across all the nodes in the cluster. This is further proven by the CockroachDB admin UI metrics: For the unsharded schema: This graph indicates that all traffic was served by a single node. The dips are caused when data from the benchmark run periodically exceeds the maximum range size threshold and all traffic has to go to a new range, and CockroachDB tries to colocate the range’s leaseholder and Raft leader , interfering with all write traffic on the index/table. For the sharded schema: Notice how in this case, the incoming traffic evens out, over time, as the various heuristics in CockroachDB figure out a good distribution of the table’s ranges across the cluster’s nodes. Again, this is unlike the unsharded case where one of the nodes is serving all of the traffic. Hash sharded indexes in an IoT use case Now that we understand why hash sharded indexes are useful, let’s dive into how one would use them in CockroachDB. Consider the following use case: Imagine we have an Internet-of-Things application where we are tracking a bunch of devices and each device creates events. Sometimes we want to know which devices published events in some time period. We might start with the following schema: CREATE TABLE events (\n    product_id\n        INT8,\n    event_id\n        UUID,\n    ts\n        TIMESTAMP,\n    PRIMARY KEY (event_id),\n    INDEX (ts, event_id, product_id)\n); Given that new events occur roughly in the present, this schema would have a hotspot on INDEX (ts, event_id, product_id) . At any point of time, all writes would go to the one range that happens to be responsible for the current set of timestamps. We can alleviate this hotspot by using hash partitioning for this time ordered index. Hash sharded indexes are meant precisely for situations like this. In 20.1 (with new syntax): In 20.1, CockroachDB will introduce the following syntax in order to natively support hash sharded indexes, in order to achieve hash partitioning. This syntax was inspired by SQLServer and Postgres : USING HASH WITH BUCKET_COUNT = <number of buckets> Thus, [1] the statement to create events with a sharded secondary index would look like: CREATE TABLE events (\n    product_id UUID,\n    event_id UUID,\n    ts TIMESTAMP,\n    PRIMARY KEY (event_id),\n    INDEX (ts, event_id, product_id) USING HASH WITH BUCKET_COUNT = 8\n) In 19.2 and below (without the new syntax): In order to achieve hash partitioning for a table like this in CockroachDB 19.2 and below, we would have to create a computed shard column (with a check constraint to aid the optimizer in creating better query plans) and then prefix the concerned index with this shard column [2]: CREATE TABLE events (\n    product_id\n        UUID,\n    shard\n        INT8\n        AS (fnv32(\n                 to_hex(product_id),\n                 ts::STRING,\n                 event_id::STRING\n                 ) \n            % 8) STORED\n        CHECK (shard IN (0, 1, 2, 3, 4, 5, 6, 7)),\n    event_id\n        UUID,\n    ts\n        TIMESTAMP,\n    PRIMARY KEY (event_id),\n    INDEX (shard, ts, event_id, product_id)\n); We can see that, even with a relatively simple table, this is already starting to get heavy and it’s not hard to imagine a case where the syntax would get gnarly when the index is based on more columns of heterogeneous SQL data types. Additionally, we would have to be careful not to use a slow hash function as this will negatively affect all INSERT performance on the table. In 20.1 CockroachDB’s new hash sharded index syntax will transparently shoulder all of this burden of hashing, creating the computed shard column and installing a check constraint. It will also handle the management of these shard columns and check constraints as these sharded indexes are added or removed. New improvements to CockroachDB’s query optimizer allow the system to generate better constrained plans for tables with hash sharded indexes. Additionally, in 20.1, CockroachDB will also support altering primary keys so users will be able to shard a table’s primary key without having to create an entirely new table. The new syntax used in statement [1] would create essentially the same schema as the one in statement [2]. The same syntax can also be used to shard primary keys or secondary indexes in a CREATE INDEX DDL statement. Summary To recap, CockroachDB uses range partitioning by default, which is a good default choice for many reasons , but results in single range hotspots in sequential workloads. The new syntax allows users to instead use hash partitioning for tables/indexes that expect this kind of workload. This unlocks linear throughput scaling by the number of nodes in the CockroachDB cluster. Working at Cockroach Labs My internship at Cockroach Labs has easily been my most educational one. The KV team had the most collaborative and open culture I’ve ever witnessed, which helped me gain a ton of insight into the competitive distributed database landscape. I owe a big thanks to my mentor Andrew and teammates Dan , Nathan and Rohan for answering my incessant questions and for their extremely detailed code reviews. It was an easy decision for me to come back full-time. If you’d like to work on interesting problems at the intersection of distributed systems and databases, we’re hiring !", "date": "2020-03-13"},
{"website": "CockroachLabs", "title": "In the Wake of COVID-19: Virtual Badge Scans for Women Who Code", "author": ["Jim Walker"], "link": "https://www.cockroachlabs.com/blog/virtual-badge-scan-for-women-who-code/", "abstract": "This past year, the Cockroach Labs marketing team introduced a novel way of collecting leads at large events. Instead of giving away swag (socks, tshirts, pens, etc.) to every person we met, we donated money to a non-profit. Our “contacts for charity” program was wildly successful, and at AWS Reinvent and KubeCon North America last year, we more than tripled our lead capture goals and donated over $14,500 to Women Who Code , the largest and most active community for technical women in the world. As the 2020 events season approached, we had planned to double-down on our “contacts for charity” program, continuing support of organizations who are working to change the world. Then last week, the in-person event industry basically imploded due to precautionary measures taken to slow the spread of COVID-19. On Monday morning, we wondered aloud what would happen with Kubecon EU, Google Cloud Next and RedHat Summit. And then, 24 hours later, we had our answer. A major portion of our strategy for the quarter was wiped out, and with it went our strategy to roll out our “contacts for charity” program at our spring events. At first, we were a bit shocked to see conference after conference cancel or postpone, but these cancellations inspired creativity and a new idea was born… the virtual badge scan . Today we will launch the “ virtual badge scan ” and will use it as a method of collecting leads for the myriad of virtual events that have popped up in the wake of all the canceled events. For every person who volunteers to give us their email address (lets us “scan” their “badge”), we will donate $3 to Women Who Code. It is a two-step process where we have inserted email validation so we can ensure a valid “badge scan”. We will promote this program around all of the virtual events upcoming, at additional meetups, and everywhere else we can go with it. We are super excited to get our program up and running and already have a few additional ideas designed for a phase 2! So stop by our booth and let us scan your virtual badge. Every badge scanned donates another $3 to Women Who Code . We'd also love if you shared this with your friends, so please click to tweet .", "date": "2020-03-12"},
{"website": "CockroachLabs", "title": "Build a Go App with GORM for CockroachDB", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/gorm-for-cockroachdb/", "abstract": "Back in 2015 the early Cockroach Labs engineers made a decision to write this massive, complex application in Go . Since then we have, on many occassions, discussed our use of Go in blogs and on stage. This video in which our Chief Architect, Ben Darnell, explains our user of Go in great detail is a good place to start: It made sense that at some point we would also support GORM. What is GORM? GORM is a developer-friendly ORM library for dealing with relational databases–including CockroachDB. The official support of GORM will enable Go developers to access the power of SQL with our favorite programming language. With GORM for CockroachDB, Gophers can easily manage database interactions and operations, all in one place. In this tutorial you can build a Go App with CockroachDB and GORM How to Learn Golang Golang has grown in popularity since we first adopted it back in 2015. It has now been adopted by many meaningful open source projects such as the group mentioned in this tweet from Kelsey Hightower: The Go programming language is at the foundation of some amazing open source projects. Just to name a few: Terraform Docker Kubernetes Etcd Prometheus CockroachDB NATS Vault Caddy Open Policy Agent pic.twitter.com/9QXzq14yuS — Kelsey Hightower (@kelseyhightower) August 12, 2020 But Go is still a relatively new language compared to the more common languages like Python, Java, and C# . If Golang and GORM are both new languages to you then you’re not alone. New CockroachDB engineer Paul Kernfeld has taken on the challenge of publicly learning Go in this live stream video series: Learning Golang From Zero . Join Paul on Friday mornings to learn Go right along with him. \\", "date": "2020-03-18"},
{"website": "CockroachLabs", "title": "Supporting Our Customers During COVID-19", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/supporting-our-customers-covid-19/", "abstract": "To the Cockroach Labs community, These are unprecedented times. We are all dealing with tremendous change and uncertainty as the impacts of COVID-19 reverberate around the world. We hope you, your team, and your families are and remain safe. As you manage the personal impacts on both yourselves and on your business, we are here to support you and are available to help you weather these challenges in any way we can . We know that many of you may be dealing with increased demand and reduced headcount, putting stress on your systems and teams. We are here to help: If you already use CockroachDB Core , we have increased staffing of our Slack channel to better support our community users . For our managed service customers, we have upgraded all CockroachCloud accounts to 24x7 support during this crisis. If you are not yet a customer, but struggle with scaling your applications, we are offering a limited number of free instances of CockroachCloud – providing our full capabilities at no charge for a minimum of 3 weeks. Contact us to learn more . We hope that these efforts, however modest, can help mitigate the challenges that many of you are facing. If we can support you in any other way, please contact us . Thank you for being a part of the Cockroach Labs community. -- Spencer Kimball CEO & Co-Founder", "date": "2020-04-06"},
{"website": "CockroachLabs", "title": "On Optimism and Team Engagement in the time of COVID", "author": ["Evan Atkinson"], "link": "https://www.cockroachlabs.com/blog/covid-team-engagement/", "abstract": "My guiltiest pleasure right now is watching apocalypse films. It’s less escapism than it is catharsis, and there’s usually a happy ending (or at the very least---they have an ending). When you watch these apocalyptic movies, the audience is always focused on the hero, holding the door closed against some encroaching danger. Few people are paying attention to the guy in the corner of the room sitting with others, telling some wild story or caught in the middle of some elaborate joke, trying to get a smile out of people in crisis. That’s me. I’d shake your hand, but… Nice to e-meet you. My name is Evan Atkinson and I’m the Culture and Workplace Associate here at Cockroach Labs. Traditionally my job consists of handling culture-related activities and programs and trying to inject some fun into the workweek. Now that we’ve transitioned to working entirely remotely, I’ve become somewhat of a Patch Adams for generalized anxiety. There is no way to minimize the enormity or the emotional toll of our current situation, but I am working hard to provide some much needed optimism and levity to our team. Act I: “Are you working from your closet?” In the first week of our remote transition, the main focus was on making a comfortable and effective home workstation. For a lot of people, that meant taking their laptop to the couch or figuring out how to make a kitchen table look like a hostage negotiator’s set up from 24 . For me in a New York apartment with square footage that matched the reading on the thermostat, I had to get creative. I stacked my monitor on top of a speaker, took a coffee table and flipped it sideways, and used my bed as an armrest (I was also able to snag a comfy chair and other tech with the budget that we gave each employee to set up their workspaces). I was (am) incredibly proud of my own bit of desk MacGyvering and wanted to see how my other coworkers had fared in the first few days of WFH, so I hosted an MTV Cribs style house tour for people to share what they had built. I was almost unanimously dunked on for my tiny set-up, but it was great to see how resourceful other folks had been in such a short amount of time. Engagements like these house tours have since been formalized into something called “Spinning-Up”, a 30 minute video call to get people socializing and out of their work mindset before our All-Hands team meetings each Tuesday. Act II: “I don’t know what I was expecting, but it definitely wasn’t tentacles.” Okay, great. Everyone has settled into their workspaces and we’ve all collectively learned how to set up a desk in our home that doesn’t compromise our spines. Now comes the hard part: mental wellbeing. During the sudden shift to working from home, work became much more about… well, work. I don’t bump into any coworkers on my way to grab my 3rd afternoon coffee, or anyone for that matter. It can be easy to reduce your relationship with your coworkers to something purely results based: “here is what I got done today, talk tomorrow.” At Cockroach Labs, our culture is one of our greatest assets. It’s the whole reason I’m here. Preserving a sense of connection and culture while physically isolated has allowed us to continue doing great things in spite of a world-changing event. We are built resilient. My part in all of this is to create more opportunities for organic social interaction and provide moments for levity, and frankly, to stop you from working for just a moment. The first step was creating a #watercooler channel on Slack, an always-on video call where employees can effectively “bump” into each other and have a moment to connect. Virtual Team All-Hands I’ve seen virtual jam bands pop-up, I’ve seen people playing games, at one point I ate a mystery can of food from my pantry on camera just because. Do you need to spend 15 minutes of your work day watching me try to figure out not just the how, but the why, of eating preserved octopus? Absolutely not , but I can guarantee that for those 15 excruciating minutes, you aren’t thinking about the danger at the door, or the weight on your shoulders. Act III: “...my birth cry will be the sound of every phone on this planet ringing in unison.” I’ve been thinking a lot about Lawnmower Man, a 1992 Sci-Fi film so bad that Stephen King successfully sued to have his name removed from the title. In the film, Jobe attempts to upload his consciousness to the internet to make all of the phones in the world ring at the same time (they never really explain why). In that terrible movie, Jobe is the villain, but I think that we share a similar motivation: to take humanity and bring it into the virtual space. We are all Lawnmower Man The first step in this transition was bringing effort was transitioning key legacy  legacy IRL engagements into a virtual format. Every Thursday I host different hour-long “Tea Times” that invite Roachers to participate in some kind of activity as a fun end to the workweek. Before Coronavirus (BC), Tea Times were biweekly in the NYC Headquarters and involved employees hosting a unique shared experience, maybe a grilled cheese cook-off or a potato chip taste test, something to get people away from their desks and send them off to their homes with a story. Sample Cockroach Labs trivia night question Virtual Tea Time serves the same purpose as it always has, but right now it feels deeply vital. Some past examples include movie screenings, trivia competitions (pro tip: host the trivia on Zoom so the trivia teams can strategize in smaller Google Hangouts to mitigate \"who's muted?\" confusion), and game nights. We’ve also added activities like art classes, Family Feud, and open mic. It’s heartening to see everyone laughing together at the end of another week and more practically, it’s helpful to keep track of what day it is and separate the weekend from the workweek. Act IV: “That’s the yeast of your problems” When COVID-19 hit, every person on the internet wanted to tell you about their sourdough starter and every company wrote the “Ultimate Guide to Transitioning to Remote Work.” We did, too. We’re coming up on the start of month two of a new normal for work in the tech industry, we’re still at home, and we’re connecting seemingly more than ever before. I am immensely thrilled and fortunate to be a part of an organization that values joy and encourages us to take care of ourselves. Cockroach Labs is a culture-first company and that’s what has kept us moving forward and allowed us to transition to an entirely new workplace structure with minor hiccups. In the disaster movie that we are all watching play out before our eyes, it can be so easy to focus on manning the door, focusing on this invisible danger on the other side. At Cockroach Labs, we locked the door early on, and now people can gather together after work and tell a story, or even eat an octopus. We don’t know the ending just yet, but in the meantime, we’re all here working together and sharing a laugh or two along the way.", "date": "2020-04-15"},
{"website": "CockroachLabs", "title": "Improving Application Performance with Duplicate Indexes", "author": ["Piyush Singh"], "link": "https://www.cockroachlabs.com/blog/improving-application-performance-with-duplicate-indexes/", "abstract": "When you’re distributing a SQL database geographically, it can be tough to get fast application performance. Data often has to make long, round trip journeys across the world and is restricted by a speed limit (the speed of light). So what can you do to ensure you’re getting the best performance possible for your use case? Utilizing a “ duplicate indexes ” topology pattern is a great place to start. Topology patterns are our recommendations of how to distribute and configure your data in order to optimize performance. That includes things like: how you’re replicating data, to which geographic regions you’re replicating it, and where you’re pinning leaseholders ( leaseholders are what coordinate reads and writes for a particular copy of data in CockroachDB). There are a handful of different topology patterns that we recommend in CockroachDB. Each pattern is intended to optimize a database for a particular kind of user experience. In this blog we’ll discuss how to use the Duplicate Indexes topology when you want extremely quick read performance , and are willing to sacrifice a little bit of write performance to achieve that. When Read Speed Trumps Write Speed Say you have a reference table of zip codes that matches zip codes up to geographic regions in the United States. Zip codes don’t change very often, so you’re probably only going to be writing that data once. However, you might need to read that data frequently to look up, for example, the nearest locations of retail stores based on a zip code that a customer entered into your website. This is an example of the type of use case where the write performance isn’t as much of a concern, because it happens infrequently and the read performance is super important because your users are reading data often and expecting fast performance. The same read-over-write principle applies to usernames and hashed passwords. When a user comes into your website, and logs into your service, they’re only creating their account once – except for maybe an occasional password update. Since you’re only writing that data once per user, but your users will be logging into your website often, the bulk of your performance relies on read speed. Distributed Data Improves Survivability, Not Read Speed If you’re using a legacy SQL database, you might have all your data in a single data center. Let’s say that data center is on the east coast. If you have a customer on the west coast who’s looking up their email and hashed password, it’s going to require round trip communication with your data center on the east coast. That round trip could take in the 100s of milliseconds. Since we know that experiences that take less than 100 milliseconds feel instantaneous, introducing this lag time means your users are going to be waiting on their login experience. That isn’t great when you have users who might be logging into your service every day. What we want to do is reduce the length of time it takes to fetch data from a data center so that a user’s login experience feels instantaneous, regardless of where they are geographically. A simple approach to moving data closer to users might be to create a CockroachDB cluster with nodes that are spread out across the entire country. Although you would expect this to improve performance somewhat, it might not work in exactly the way you expect. If you have your Cockroach cluster with nodes in the east coast, the west coast, and the central region of the United States, Cockroach replicates this data three times. It also tries to spread this data out across as wide a geographic range as possible in order to improve the survivability of your data. By default, you’ll end up having one copy of the data in each region. However, the issue with having one copy of the data in each region is that only one of these copies is the leaseholder, which again, is what actually coordinates all the reads and writes. So even though we have three copies of the data in each of these regions, the leaseholder is still only in one region. As a result, you might end up in a scenario where the user is talking to machines in your database that are local to them, but the user's request is routed to the leaseholder which is in a different region. In that case, we’ve improved survivability, but we haven’t improved performance. That’s where duplicate indexes come in. Duplicate Indexes Create Instantaneous Reads Let’s say you have a reference table with emails and hashed passwords. What we’re going to do is create a secondary index on the email column in all three regions and store the hashed passwords along with that secondary index. So, the table itself will be constrained to one geographic region, and we’ll have a secondary index for each of the remaining regions. The thing to keep in mind about these indexes is that they’re also replicated three times and constrained to a specific geographic region. What this means is you have three copies of the data in each of the three regions. You can see why this comes at the trade-off of write performance, since we’re now having to write this data nine times instead of three. Because we have to perform those additional writes, and because we’re writing the data in multiple geographic locations, we’re expecting write latency to get a little bit worse. However, we’ve observed that write latency is still faster than 300 milliseconds, so although it doesn’t feel instantaneous, it’s generally acceptable. What we’re striving to do is get our reads in a single-digit millisecond range. The duplicate indexes topology pattern helps achieve that, because when each of the indexes has three copies constrained to a specific geographic region, it means that each of the regions now has a leaseholder. That way, when a user makes a request for the data, it’ll always come from their local data center, as opposed to a data center in a different region. The really nice property of the duplicate indexes topology pattern is that users will be able to look up data in data centers that are close to them even if they’re migrating from one geographic region to another. That means they’ll experience fast application performance, and thus have a satisfying user experience, no matter where they are physically. If you’re ready to deliver the best user experience possible, download CockroachDB and get started today.", "date": "2020-04-14"},
{"website": "CockroachLabs", "title": "Your Database Should Work Like a CDN", "author": ["Sean Loiselle", "Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/distributed-database-performance/", "abstract": "Dealing with distributed database performance issues? Let's talk CDNs. Even though they're at different levels of your tech stack, distributed databases and content delivery networks often share a similar goal: improving the availability and speed between your service and your user. To deliver content as quickly as possible (at least when it’s static), one of the first tools teams reach for is a content delivery network. CDNs leverage a whole stack of technologies to rapidly deliver resources to users, but one of the more impactful strategies is to simply replicate data all over the globe, so user’s requests never have to travel far. In the parlance of operations teams, this is a “multi-region deployment.” Despite being geographically distributed, CDN replication is relatively straightforward: they simply distribute a file to more and more servers. Because the data changes infrequently (or never), life is easy. For distributed databases, though, it’s been another story. Because managing state across a set of machines is a hard problem to solve, they typically make unattractive trade-offs when distributed far and wide. Businesses have often been unwilling to accept these compromises (and rightfully so), causing them to shy away from and write off multi-region deployments. But not without paying a price. The Costs of Single-Region Deployments While siloing data in a single region makes things easier, the strategy actually impinge on the two things you need from your database’s performance: speed and availability. Single-Region Deployments are Slower In 2020, computers are still bound by physics, and cannot compete with the speed of light. Being farther away from a service means it takes longer to communicate with it. This is the fundamental reason we use content delivery networks and stateless services wherever we can. However, even with a razor-thin time-to-first-byte, an application’s user experience can still falter if it has to communicate with a database thousands of miles away. This problem is compounded by the fact that latencies quickly become cumulative. If your SLAs allow for a 300ms round trip between an app and a database, that's great––but if the app needs to make multiple requests that cannot be run in parallel, it pays that 300ms latency for each request . Even if that math doesn't dominate your application's response times, you should account for customers who aren’t near fiber connections or who live across an ocean: that 300ms could easily be 3000ms and requests could become agonizingly slow. If you need a gentle reminder as to why this matters for your business, Google and Amazon both have oft-cited studies showing the financial implications of latency. If your site or service is slow, people will take their attention and wallets elsewhere. A simple solution: deploy your data to the regions where your users are. Single-Region Deployments Aren't As Resilient A distributed database’s performance isn’t measured solely in ms ; uptime is also a crucial factor. No matter how fast your service normally is, if it’s down, it’s worthless. To maximize the value of their services, companies and their CTOs chase down the elusive Five Nines of uptime (which implies no more than 26 seconds of downtime per month). Achieving Five Nines requires reliable datacenters with incredible networks––but what about forces of nature beyond your control? Forrester research found that 19% of major service disruptions were caused by acts of nature that could take down a cloud host’s entire region: hurricanes, floods, winter storms, and earthquakes. As Hurricane Sandy proved, these events can be powerful enough to cripple companies deployed in only a single region, including blogosphere titans like BuzzFeed and The Huffington Post . With their sites down, they couldn’t fulfill their mission of delivering content on the world’s latest events, and instead themselves became a collateral story. Another facet of your application’s availability guarantees is defining its point of recovery (or Recovery Point Objective/RPO ) in the case of one of these catastrophes. This is particularly crucial for your customer’s data. If your data’s only located in a single region and it goes down, you are faced with a “non-zero RPO”, meaning you will simply lose all transactions committed after your last backup. If they’re mission-critical entries, you face the risk of losing not only revenue, but also your user’s trust. So, as the weather gets weirder, the best way to ensure your application stays up and doesn’t lose data is to distribute it far and wide. This way, your users’ data is safe even if swaths of the globe go dark. Single Region Deployments Aren't Compliant While latency and uptime make great headlines, there’s an ever-unfolding story that makes single-region deployments largely untenable: data regulations. General Data Protection Regulation (GDPR) , in particular, requires that businesses receive explicit consent from EU users before storing or even processing their data outside the EU. If the user declines? Their data must always (and only) reside within the EU. If you’re caught not complying to GDPR? You’ll face fines of either 4% of annual global turnover or €20 Million, whichever is greater. When you take GDPR in the context of the existing Chinese and Russian data privacy laws––which require you to keep their citizen’s data housed within their countries––there’s a clear signal that single-region deployments no longer satisfy the needs of global businesses. To comply with increasingly complex regulations, you’re left to choose from some unattractive option: Foregoing huge customer bases and global expansion Facing crippling fines Engineering unwieldy, complex, and potentially fragile domiciling logic in your applications ...or you could consider an option that actually presents upside to your team: Using a distributed database in a multi-region deployment with data domiciling So: Are CDN-like multi-region deployments worth it? Not everyone faces these concerns with equal dread. Those who can pay these costs (fines, downtime) often do, and avoid the headache of re-architecting an app from the ground up. Small, pre-revenue startups who are still trying to establish a user base (let alone an international one) can sometimes ignore these concerns––though, if your company succeeds, you’ve only put off handling a problem that becomes increasingly costly to solve later. Refactoring an app to use a performant distributed database can be dramatically more expensive than making choices with your company’s future in mind. For everyone else––businesses of any size who are concerned with the experience of their users across the globe (or even across a single country)––multi-region deployments improve crucial elements of your business. How to achieve CDN behavior at the DB level As we mentioned at the top of this post, there have been many attempts to overcome the obstacles of deploying a database to multiple regions, but most solutions make difficult-to-accept compromises. Managed & Cloud-Native Databases Managed and Cloud databases often tout their survivability because they run in “multiple zones.” This often leads users to believe that a cloud database that runs in multiple availability zones can also be distributed across the globe. However, they elide an important and misleading fact: these zones are all in the same region, and don’t have the speed or availability benefits of multi-region deployments. There are caveats to this, of course. For example, with Amazon RDS, you can create read-only replicas that cross regions, but this risks introducing anomalies because of asynchronous replication: and anomalies can equal millions of dollars in lost revenue or fines if you’re audited. In addition, this forces all writes to travel to the primary copy of your data. This means, for example, you have to choose between not complying with GDPR or placing your primary replica in the EU, providing poor experiences for non-EU users. Go with NoSQL NoSQL was conceived as a set of principles to build high-performing distributed databases, meaning it could easily take advantage of CDN-like multi-region deployments. However, the technology achieved this by forgoing data integrity. Without consistency, NoSQL databases are a poor choice for mission-critical applications. For example, NoSQL databases suffer from split-brain during partitions (i.e. availability events), with data that is impossible to reconcile. When partitions heal, you might have to make ugly decisions: which version of your customer’s data to you choose to discard? If two partitions received updates, it’s a lose-lose situation. Inconsistent data also jeopardizes an application’s recovery point objective (i.e. the point in time of your last backup). If your database is in a bad state when it’s backed up, you can’t be sure how much data you’ll lose during a restore. That all being said, if an application can tolerate inconsistent date, you get more of NoSQL’s benefits when it’s distributed across regions and close to your users. Shard Your Relational Databases Sharded relational databases come in many shapes and suffer from as many different types of ailments when deployed across regions: some sacrifice replication and availability for consistency, some do the opposite. Many require complex and fragile configurations, and others require you to tie applications to their enterprise offerings (which may or may not support multi-region deployments). With all of these trade offs, they pose headaches and risks when geographically distributed. Use CockroachDB One option to consider for a multi-region database is CockroachDB, which uniquely meets all of the requirements for a multi-region deployment. CockroachDB lets you deploy to multiple regions and ensure all data is kept close to all users, and doesn’t lock you into a specific vendor (or the places they have data centers). It offers stronger consistency and lets you control where your data is laid out with our geo-partitioning feature. If you're using a multi-region deployment, be sure to check out our multi-region deployment topology docs. Don't want to deploy it yourself? Use our database-as-a-service, Cockroach Cloud. Qualifications for a Distributed Database As we've shown above, there are lots of options available for when your applications need to provide speed and availability to a global audience. When shopping for a distributed database or evaluating your options for scaling, remember to keep the following characteristics in mind. A truly distributed database needs to: Deploy anywhere Reduce latency by performing reads and writes close to users (while still enforcing consistency, even across a distributed deployment) Maintain uptime by tolerating faults Offer granular control over geographical placement of your data for GDPR compliance To get stand up your own multi-region CockroachDB deployment, get started with Cockroach Cloud today.", "date": "2020-04-29"},
{"website": "CockroachLabs", "title": "How to Build a Multi-Region Application on CockroachDB", "author": ["Eric Harmeling"], "link": "https://www.cockroachlabs.com/blog/multiregion-app-tutorial/", "abstract": "If your company has a global customer base, you’ll likely be building an application for users in different areas across the world. Deploying the application in just a single region can make high latencies a serious problem for users located far from the application’s deployment region. Latency can dramatically affect user experience, and it can also lead to more serious problems with data integrity, like transaction contention . As a result, limiting latency is among the top priorities for global application developers. In this blog, we walk you through a low-latency multi-region application that we built and deployed for MovR, a fictional vehicle-sharing company with a growing user base. The MovR application is a Flask web application, connected to a CockroachDB cluster with SQLAlchemy . We deployed the application in multiple regions across the US and Europe, using Google Kubernetes Engine , with some additional Google Cloud Services for load balancing, ingress, and domain hosting. For the database, we used a multi-region CockroachCloud deployment on GCP. The application source code is available on GitHub , and an end-to-end tutorial for developing and deploying the application is available on our documentation website . Latency in Global Applications MovR provides a platform for people to share vehicles. Their customers can register vehicles and take rides in supported cities. MovR is based in New York City, but they want to open up their service to several other cities in the US and in Europe. As they go global, they start to worry about the latency on requests coming to their application from different parts of the world. If the MovR application and database are deployed in a single region on the east coast, latency can be a serious problem for users located in supported cities that are far from the east-coast deployment region, like Los Angeles or Rome. Broadly speaking, when a client makes a request to an application, there are two different types of latency on the request that can be improved with a well-designed deployment strategy: Database latency , which we define as the time required for applications to complete requests to a database and receive responses from the database. Application latency , which we define as the time required for clients (i.e., end users) to complete requests to the application and receive responses from the application. In Reducing Multi-Region Latency with Follower Reads , we go into more detail about different types of latency and their respective impacts on performance. In this blog, we limit the discussion to these two general categories. To limit database latency for MovR, we can deploy the database in multiple regions. We document a number of different database deployment configurations on our Topology Patterns documentation page. The geo-partitioned replicas topology pattern , the pattern we chose to use for MovR, constrains data to the database deployments closest to the regions from which a client makes requests. This reduces the distance requests must travel between application deployments and database deployments that are located in proximity. To limit application latency , we can deploy the application in multiple regions, and ensure that each application deployment communicates with the database deployment closest to it. We also need to make sure that client requests are routed to the application deployment closest to the client. The application needs to be able to determine the location from which it receives its requests. Database operations in the application can then use the client’s location to query the correct (i.e., closest) partitions of data. The Database Schema It helps to start thinking about geo-partitioning as early as possible, especially when designing the database schema. For the purposes of this example application, we decided to go with a more simplified version of the movr database that is included in the cockroach binary . The schema for this application’s database has the following tables: users vehicles rides Here’s a diagram of the database schema, generated with DBeaver : Each of these tables contains a city column in its primary index. This column tells us where a particular user, vehicle, or ride is located. We can partition each table based on these city values, mapping a subset of cities to a deployment region (e.g., Los Angeles, San Francisco, and Seattle to US-West) in the partition definitions. Then we can define zone configurations that constrain each partition to the regional deployments. Although we chose to use cities to signify the location column on which to partition, many applications could benefit from a database with location columns at a higher level of abstraction, like region, province, or country. The statements for creating and geo-partitioning these tables are all included in dbinit.sql , in the movr-flask project. The Application The application stack consists of the following components: Python class definitions that map to the tables in the movr database. Functions that wrap database read and write transactions . A backend API that defines the application's connection to the database. A Flask server that handles requests from clients. Bootstrapped HTML templates that the Flask server can render into web pages. Because we are mainly concerned with how our multi-region application will communicate with a multi-region deployment of CockroachDB, let’s focus on the Python classes, functions, and the “backend API”. First, let’s look at a class definition that maps to a table in the database. Here is the definition for the Vehicle class, defined in the models.py file: class Vehicle(Base):\n    __tablename__ = 'vehicles'\n    id = Column(UUID)\n    city = Column(String)\n    type = Column(String)\n    owner_id = Column(UUID, ForeignKey('users.id'))\n    date_added = Column(DATE, default=datetime.date.today)\n    status = Column(String)\n    last_location = Column(String)\n    color = Column(String)\n    brand = Column(String)\n    PrimaryKeyConstraint(city, id)\n    def __repr__(self):\n        return \"<Vehicle(city='{0}', id='{1}', type='{2}', status='{3}')>\".format(\n            self.city, self.id, self.type, self.status) This class maps to the vehicles table in the movr database. Each attribute of the class maps to a database object of the table, like a column or a constraint. The data types passed to each Column() constructor correspond to CockroachDB data types. When defining primary key columns, it's best practice to use UUIDs rather than sequential keys. Adding a location column to the primary key allows us to split the data up based on the location information of specific rows. Note that the PrimaryKeyConstraint is on the city and id columns, which maps to the composite primary key constraint in the vehicles table. Next, let’s look at a transaction function. This type of function defines a group of database operations that the ORM converts to SQL transactions that are sent to a running CockroachDB cluster for execution. Here is the transaction function definition for start_ride_txn , defined in the transactions.py file: def start_ride_txn(session, city, rider_id, rider_city, vehicle_id):\n    v = session.query(Vehicle).filter(Vehicle.city == city,\n                                      Vehicle.id == vehicle_id).first()\n    r = Ride(\n        city=city,\n        id=str(\n            uuid.uuid4()),\n        rider_id=rider_id,\n        rider_city=rider_city,\n        vehicle_id=vehicle_id,\n        start_location=v.last_location,\n        start_time=datetime.datetime.now(\n            datetime.timezone.utc))\n    session.add(r)\n    v.status = \"unavailable\" This function takes some input passed from the frontend, and calls a couple SQLAlchemy Session functions, using that input. session.query() reads rows in the table, and session.add() inserts rows. Note that the session.query() call first filters on city . The tables and indexes defined in dbinit.sql are partitioned on city values, and those partitions are constrained to a particular deployment region. Because these partitions are constrained to a single region per the geo-partitioned replicas topology pattern , all operations filtered on a city value only query a single partition of data, geographically located near that city. When you write SQLAlchemy operations for CockroachDB, you need to wrap the operations in a wrapper function that is included with the CockroachDB SQLAlchemy dialect, called run_transaction() . run_transaction() takes a transaction function (like start_ride_txn ) as a callback, creates a new database session , and then executes the callback using the new session. This ensures that all database operations written with the ORM do not violate CockroachDB transactional guarantees . Finally, let’s look at the “backend API” functions. The application frontend calls these functions, and then these functions call run_transaction() to make changes to the database. Here is the definition for start_ride , defined in the movr.py file: def start_ride(self, city, rider_id, rider_city, vehicle_id):\n    return run_transaction(\n        sessionmaker(\n            bind=self.engine), lambda session: start_ride_txn(\n            session, city, rider_id, rider_city, vehicle_id)) When a user hits “Start ride” in the application’s web UI, the request gets routed to start_ride() . start_ride() calls run_transaction() , with start_ride_txn() as the callback function, to update a row of the vehicles table and insert a new row into the rides table. Again, note that this only reads and writes to the partition closest to the city passed down to the transaction function. We don’t go into much detail about the other components of the application in this blog. For more information about the application, see the Develop a Multi-Region Application section of the end-to-end MovR tutorial. Deployment To deploy the database, it’s easiest to use CockroachCloud , our managed database service. To get a multi-region CockroachCloud deployment, sign up for CockroachCloud, and then request the regions and the cloud service provider that you want.* For our demo deployment of the MovR application, we used a multi-region CockroachCloud deployment on GCP. After the CockroachCloud team has deployed the multi-region cluster, we can access everything we need to connect to the cluster from the application (a connection string, a certificate, and the credentials for the authorized user) from the CockroachCloud console. For more detailed instructions about connecting to a CockroachCloud cluster, see Connect to Your CockroachCloud Cluster . From the CockroachCloud console, we can also pull up the CockroachDB Admin UI , to monitor and inspect the database. From the “Metrics” page, we can see the performance of nodes in the cluster: From the “Database” page, we can see the table, index, and partition definitions for the rides table: CockroachCloud abstracts the complexity of orchestrating a multi-node, distributed database deployment across multiple regions. It’s virtually cloud-agnostic. Application deployment, on the other hand, heavily depends on the cloud provider, making things a little trickier. For the MovR application deployment, we containerized the application project, deployed it to multiple clusters using Google Kubernetes Engine, and then set up a multi-cluster ingress using Google’s kubemci tool. An important step in setting up a multi-region application is making sure that the client location information is passed on to the application server. Fortunately, GCP allows you to define custom headers that you can forward to the application. Other web services likely allow for header forwarding, but you will probably need to reverse-geolocate an IP address (usually passed in the header X-Forwarded-For ) with client-side logic. For more details about the database and application deployments, check out the Deploy a Multi-Region Web Application of the MovR tutorial, or the README in the movr-flask project . To make things easier for development and debugging, we also provide a local database and application deployment workflow. Single-Region Deployment vs. Multi-Region Deployment To roughly measure the latency gains from our multi-region deployment, we compared the differences in latency on a subset of SQL requests between the single-region application and database deployment and the multi-region application and database deployment, as reported by the CockroachDB Admin UI. Using a proxy service, we navigated to the MovR website from Rome, and performed the following tasks: Signed up ( INSERT INTO users ) Logged in ( SELECT FROM users ) Viewed available vehicles ( SELECT FROM vehicles ) Added vehicle ( INSERT INTO vehicles , UPDATE users ) Started ride ( INSERT INTO rides , UPDATE vehicles ) Ended ride ( INSERT INTO rides ) Viewed rides ( SELECT from rides ) Logged out For the single-region deployment, we created a single-region CockroachCloud cluster on GCP in the US-East region. We then re-deployed the application, using GKE and GCP, in just the US-East region. Here are the results in the single region case: Latencies were, on average, around 150ms. Here are the multi-region results: On average, the latencies on these requests were under 2.5ms. Optimizing deployments for latency reduced the average latency on these client-database requests by over 60x! Our latency comparison exercise here does not explore higher-throughput scenarios. There would likely be many more than just a single client making requests to the application and database. We also do not explore hybrid deployment configurations (for example, a single-region database deployment and multi-region application deployment), or cross-cloud configurations (for example, CockroachCloud deployed on AWS and the application deployed using GCP services). In the Reducing Multi-Region Latency with Follower Reads blog post, we take a more rigorous approach to analyzing the latency improvements from different multi-region deployments. Conclusion Latency can be a big problem for global applications. CockroachDB’s multi-region features, geo-partitioning in particular, can help limit latency in global applications. To demonstrate these features in the context of a global application, we built and deployed an application on CockroachDB across multiple regions in the US and Europe. CockroachCloud made database deployment easy, but deploying the application wasn’t as straightforward. The example application code and SQL schema statements are publicly available on GitHub . We encourage you to fork the movr-flask repo and go through the end-to-end Develop and Deploy a Multi-Region Web Application tutorial. Also, you can interact with a deployed version of the MovR app by navigating to https://movr.cloud . *Currently, multi-region CockroachCloud deployments are not available through the CockroachCloud self-service console. To set up a multi-region CockroachCloud deployment, contact us at sales@cockroachlabs.com .", "date": "2020-04-02"},
{"website": "CockroachLabs", "title": "Announcing CockroachDB 20.1: New Features Let You Build Fast and Build to Last", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/cockroachdb-20-1-release/", "abstract": "At Cockroach Labs, we want to give you a database that allows you to quickly and simply build and manage data-intensive applications in the cloud—without worrying about the performance, scale or resilience of those apps. To support that mission, our latest release includes updates that let you build fast and build to last. In CockroachDB 20.1 we focused on delivering productivity enhancements to speed up development of apps and services built on our database. We’ve added new features that ensure CockroachDB works the way you work: with your favorite tools and environments, with straightforward monitoring and improved SQL. In addition, we’ve added performance and security enhancements to help you deliver applications that last. In CockroachDB 20.1 you can now: Get the flexibility and agility of changing primary keys while your app is online, with the new online primary key changes feature Quickly troubleshoot application issues with various new monitoring updates, including a slow query log , SQL statements diagnostics , and a network latency page in the Admin UI Work more efficiently with support for popular ORMs, including Django , SQLAlchemy , Hibernate , jOOQ , PonyORM , and more Take advantage of more tools in your toolkit with added SQL functionality Get better multi-region performance with improvements to the follower reads features Trust your sensitive data is safe with encrypted backups and more improvements to backups Maintain more control over user permissions with updates to Role Based Access Control (RBAC) , and better support for certificate authorities And more We’ve also made several security enhancements to CockroachCloud, such as adding VPC peering and achieving SOC 2 Type 1 certification. And, we placed another Enterprise feature, RBAC , into CockroachDB Core to give our community users improved security controls. With CockroachDB, you get high-end database performance (even in distributed settings), while maintaining the highest level of transactional consistency. The updates in version 20.1 extend our position as the right database for your system of record workloads in the cloud. Read on to delve into the details about CockroachDB 20.1 and then visit our docs site to learn more. When you’re ready to try it for yourself, sign up for a 30-day free trial of CockroachCloud. You can get started using the coupon code CRDB30. Build Fast: Productivity Enhancements Increase Efficiency In the modern development environment, speed to market is critical. It’s this expectation that drives you to turn a complex business concept into a functioning, reliable application or service. It’s also this that demands we deploy and iterate at an accelerated pace. It’s our belief that the database should empower you to get to market faster and more effortlessly, not slow you down. The productivity enhancements in this release deliver key capabilities that allow you to build on CockroachDB more efficiently. These updates give you more freedom, flexibility, and familiarity: More flexible data models With online primary key changes , you can migrate your schema without any downtime. This gives you flexibility to change data models in production as your business evolves. Since primary keys in CockroachDB control data locality and resilience, you can also start with a single-region deployment and then have the freedom to expand into multi-region, all with a simple ALTER TABLE command. This is an extension on CockroachDB’s existing online schema migrations functionality which allows you to make these modifications in production without downtime and have the database automatically take care of moving data around to meet your requirements. ALTER TABLE command allows you to efficiently redistribute data across a cluster without downtime And if you are looking to migrate from an existing database to CockroachDB, we’ve added better compatibility with the schema migration tool Flyway to make the process easier. Monitoring updates CockroachDB 20.1 lets you more quickly troubleshoot application bottlenecks and slowdowns. A new slow query log lets you log slow queries to a separate file for rapid analysis. In the Admin UI, the SQL statements diagnostics section lets you collect statement diagnostic information, and the network latency page displays latencies between all nodes in a cluster to help you quickly understand performance and identify issues like one-way partitions. Network latency diagnostics helps you quickly identify and troubleshoot network issues between distributed nodes of CockroachDB. Added compatibility for more programming languages and SQL syntax Save time and work efficiently with your favorite tools. CockroachDB now supports many popular ORMs, including: Django , SQLAlchemy , PonyORM , and peewee for Python developers; Hibernate and jOOQ for Java developers; and GORM for Go developers. We’ve also added more SQL functionality, including SELECT FOR UPDATE , support for nested transactions using savepoints , LATERAL joins , and recursive common table expressions . This expanded SQL syntax gives you more tools in your toolkit. Build to Last: Performance and Security Updates for Resilient Apps In CockroachDB 20.1, we’ve also expanded other existing capabilities that let our customers easily build scalable, secure single- and multi-region applications. Better multi-region performance The follower reads feature of CockroachDB allows for low latency reads in distributed clusters, by serving slightly out-of-date reads from the nearest copies of data. In CockroachDB 20.1, we’ve reduced the delay for acceptable follower reads timing from a few dozen seconds in our previous release to under five seconds or less in the past. Reads are still served with single-digit millisecond latencies. Encrypted, full-cluster distributed backups CockroachDB already encrypts data in transit and at rest, and this release provides encrypted backups , too. It also adds full-cluster backups and restore , including metadata and configurations, to simplify cluster restoration in the unlikely event that it’s needed. More control over user access and permissions Role Based Access Control (RBAC) is now more granular in CockroachDB 20.1, with user attributes, role admins (users that control roles), and more. There’s also better support for certificate authorities such as Amazon Certificate Manager, and improved authentication logs to track SQL client connections and detect anomalous data access. These updates help organizations with lots of users and sensitive data. RBAC is in CockroachDB Core We’re always evaluating which features land in CockroachDB Core, our freely available version of the database. To give our community users even more control over security, we’ve added RBAC into Core . CockroachCloud: the easiest path to an elastic, distributed SQL database In October of 2019, we released our cloud offering, CockroachCloud. We were delighted with the response from the community, and in the past six months we’ve guided many CockroachCloud customers into production. Today, CockroachCloud is self-service, so you can spin up clusters with our web UI. In CockroachCloud 20.1, we’re meeting the needs of our customers with security updates: VPC peering: connect to CockroachCloud more securely using a private network. SOC 2 Type 1 compliance: with the first step of this gold-standard security certification, we’re proud to continue safely supporting your data. Get Started Today for Free We’ve been pretty busy and these highlights are just a few of the improvements that 20.1 delivers. If you’d like to see the complete list of everything that’s new in this release, we encourage you to check out our docs site . You can also learn more at our live demo of CockroachDB 20.1 on Wednesday, May 20, where we’ll cover what’s new and answer all your questions. We’re also now offering a 30-day free trial of CockroachCloud. You can get started using the coupon code CRDB30. Or, if you’d rather download our open-source version, you can find the binary here . Finally, we love feedback and would love to hear from you. Please join our Slack community and connect with us today! Get Started for Free", "date": "2020-05-12"},
{"website": "CockroachLabs", "title": "Babyface: a Parental Leave Project", "author": ["Ben Darnell"], "link": "https://www.cockroachlabs.com/blog/babyface/", "abstract": "I recently returned to work following three months of parental leave for the birth of my first child. I spent most of that time doing what you’d expect: staring at the baby for hours, changing hundreds of diapers, and generally alternating between unwarranted confidence and complete uncertainty. But I also found a little time for some other projects, and I’d like to share one of them today. We used an app called Baby Connect to track feedings, diapers, and other vital information about the baby (this is the app that got the coveted phone home screen real estate that was freed when I uninstalled Slack for my leave). I also had a Fitbit Versa 2 smartwatch, saw that it had an SDK, and decided to try building a custom watch face that would show this data. How babies work First, here’s a quick refresher for those of you that haven’t cared for an infant in recent memory. Newborns need to eat every two to three hours, around the clock. They’ll soil about as many diapers as they have feedings. And finally, if pumping and bottle feeding are a part of the feeding plan (as they were for us), mothers are advised to aim for a combined total of at least 8 nursing and pumping sessions per day to ensure enough milk supply. Of course, every baby and every mother is different; no one fits this textbook schedule perfectly. But at least in our case it was close enough to use as a target. This gives us three kinds of events, all happening on a roughly three hour cadence: feeding (nursing or bottle), milk expression (nursing or pumping), and diaper changes (which also come in two varieties but I’m not distinguishing them here). In Baby Connect, they’re tracked as four line items: bottle, nursing, pumping, and diapers, and a summary view tells you how long it's been since the last event in each category. Visualizing the data Once we have the data, what do we do with it? I decided that I didn’t want to just replicate Baby Connect’s textual summary view on my wrist. I like the aesthetic and glanceability of an analog watch face, and I wanted to design something that fit the same mold. To decide on a visualization method, I thought about what problem I was trying to solve with the watch. Basically what I wanted was a decoder ring for the baby’s cries and fussiness. Is he hungry? Gassy? Upset that he’s sitting in his own filth? Knowing how long it’s been since he was fed or had his diaper changed would be a nice clue. For this, we don’t need to show precise times - just a rough idea of whether the last feeding was, say, within the last hour or nearly three hours ago. Something like a watch’s hour hand will do. Adding four more hands radiating from the center of the watch would be awfully cluttered, but floating a letter or icon at the end of an “invisible hand” could work, and this was my first attempt. Babyface UI, v1 This worked pretty well. I color coded the letters by age (green for less than two hours old, yellow for 2-3 hours, red for 3+) so it would be visible at a glance which needs were oldest. But because of the interactions between the different categories of events, it was a little more subtle than that. Nursing counts as both feeding and milk expression, so a recent nursing would turn both the “pumping” and “bottle” indicators green. Babyface UI, v2 On my next iteration, I added some arcs connecting the letters to the hour hand. This time I was able to make the relationship between the different variables more explicit. There were three arcs, for the three underlying needs we care about (feeding, milk expression, and diapers), and four letters for the actions we’re tracking ( B ottle, N ursing, P umping, D iaper). The arcs were offset to fit in between the actions that affect them. This had the desired effect of making the underlying needs most salient, with the letters just indicating added detail. Building the app Fitbit’s SDK is based on SVG, Javascript and CSS. They have a fairly slick online IDE ---sign in with your Fitbit account and you can write code in your browser and push it straight to your device. It’s not the best programming environment around but it gets the job done for this small project, and for ease of getting started it’s miles ahead of other mobile platforms I’ve developed for. The watch has limited computing power and battery life, so apps are split into two pieces: one running on the device itself and a “companion” app running on the linked smartphone. Only the smartphone companion app can reach the internet, so for this app the trickiest part turned out to be passing messages back and forth between the watch and the phone to get updated data without wasting battery power on either device. I ended up optimizing for battery life. When the app on the watch wakes up (generally triggered by certain wrist movements), it sends a message to the phone requesting updated data. The phone then makes a call to the Baby Connect servers and returns the results to the watch, which updates its display. This results in a delay of a few seconds during which the watch is showing old data. This was slightly annoying, but mainly because I was frequently checking the watch immediately after recording a diaper change just to watch it update. Once that novelty wore off, I stopped checking it so close to the actual events and the delayed updates weren’t a problem any more. There was one more wrinkle: even though the phone “companion” app feels like it’s running in a server-side node.js-like environment because it has no UI, it’s actually running in a hidden web view. This means that it also has to abide by a browser-like security model. Cookies are not supported in this environment, but they’re required by the Baby Connect API. I worked around this with a simple proxy that translates requests issued from the Fitbit app into the required format. The proxy is based on Tornado , and used repl.it for both writing the proxy and hosting it. Between Fitbit Studio and repl.it, I was able to complete this entire project in a browser; it wasn’t necessary to install any tools locally (there’s an optional Fitbit simulator which let me iterate a little faster). This was a new experience, but it worked well and after many years of headaches setting up local development environments, this was refreshingly easy. Growing up and outgrowing the app All new parents quickly learn that as soon as they think they’ve figured something out about their baby, things change. Three-hour feeding cycles stretch into four, and continue to lengthen and allow more precious sleep. More importantly for the purposes of my little app, it becomes possible to steer feeding times onto a more predictable schedule. Once my son was eating at the same times every day, I stopped checking times on my watch as much. Rather than keep updating the app to track changes in behavior, I switched back to a plain watch face (mainly because third-party watch faces can’t use the Versa 2’s always-on-display). Even though this was (by design) a short-lived project that went from inception to retirement in a few months, I enjoyed working on it. In many ways it was the polar opposite of my work on CockroachDB - there was no need for rigorous design or testing because the stakes are so low. The “new user signup” experience is clunky, but it didn’t matter because I was the only user. That made it easy to work on in small chunks (mainly while helpful grandparents were visiting and holding the baby). Now that I’m done using it, I’m not planning on doing any more work with this app, but in case anyone is curious, I’ve put the client and server code on GitHub.", "date": "2020-05-08"},
{"website": "CockroachLabs", "title": "Tutorial: Build an App with Spring, Java, Hibernate, Gradle, and CockroachDB", "author": ["Glenn Fawcett"], "link": "https://www.cockroachlabs.com/blog/tutorial-spring-hibernate/", "abstract": "Application development frameworks like SpringBoot have done a good job a giving developers a quick start to developing applications with Java. Layering object-relational mapping to SQL to remove proprietary syntax has further simplified the development of database applications. CockroachDB allows developers to take advantage in the evolution of globally distributed data with a simple SQL interface. But each database can bring it’s own specific syntax and cause developers to hesitate when getting started with a new database. How do you get started? This blog will use SpringBoot , Hibernate , JAVA , and Gradle to show a simple end-to-end solution with CockroachDB. Demystifying CockroachDB Datasource If you have developed an application with Postgres, developing with Cockroach will be a breeze. CockroachDB was written to be compatible with the Postgres 9.6 dialect. The connect string URL used to connect to CockroachDB is plug compatible with JDBC drivers for Postgres. This allows you to use various to tools like dbeaver to connect and run various SQL statements. For ORMs like Hibernate, you connect just like you were connecting to Postgres. Currently, Hibernate is compatible with PostgreSQL95Dialect . So to connect to CockroachDB, the following should be configured in the application.properties file with SpringBoot: spring.datasource.url=jdbc:postgresql://localhost:26257/defaultdb?sslmode=disable\nspring.datasource.driver-class-name=org.postgresql.Driver\nspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQL95Dialect That’s really it! The rest of this blog will walk you through the steps with SpringBoot quick start. Step 1: Quick Start with SpringBoot Open the quick start to create the basic framework required. For this example used the following options: Project: Gradle Project Language: Java SpringBoot: 2.2.4 Project Metadata group: example.com Artifact: first_cockroach /w Options… name: first_cockroach Description: Demo project for Spring Boot Package name: com.example.first_cockroach Packaging: JAR, Java 8 The page should look like so: Select the hamburger drop-down under dependencies to add the following items: Developer Tools Spring Boot DevTools Lombok Web Rest Repositories SQL Spring Data JPA Flyway Migration PostgreSQL Driver and... Once these are selected, you can simply click to generate the quick start package. If you use the artifact name above, you should have a first_cockroach.zip file to get started. Step 2: Unzip Artifact and Add Connectivity Resource Unzip the artifact created in Step 1 in your working environment and navigate the to src/main/resources directory. Once in this directory, create the application.properties file that defines how to connect to the database and migrate data with flyway. ### Setup Postgres driver\nspring.datasource.driver-class-name=org.postgresql.Driver\nspring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQL95Dialect\nspring.datasource.username=root\nspring.datasource.password=\nspring.datasource.type=com.zaxxer.hikari.HikariDataSource\n### Connect URL for localhost with port 26257 /w insecure connection\n### Database name is defaultdb\nspring.datasource.url=jdbc:postgresql://localhost:26257/defaultdb?sslmode=disable\n### Set baseline-on-migrate with Flyway\nspring.flyway.baseline-on-migrate=true\n### Set baseline-version for Flyway to execute migrations at version 1 or more For this simple test, I created a simple cockroach test cluster on my laptop. For an even easier demo cluster, you can simply run cockroach demo. Step 3: Create Flyway Migration File Simply create the appropriate flyway migration files in the src/main/resources/db/migration directory. For this simple test, I created a file V1__AddPetsTable.sql to create and populate the pets table. --##\n--##     Flyway Migration File\n--##         src/main/resources/db/migration/V1__AddPetsTable.sql\n--##\n--## Create pets table\n--##\nCREATE TABLE pets\n(\nid UUID DEFAULT gen_random_uuid() PRIMARY KEY,\nname STRING,\ntype STRING,\nindoor BOOLEAN DEFAULT TRUE\n);\n--##\n--## Define the herd\n--##\nINSERT INTO pets (name, type)\nVALUES ('tula', 'cat'),('ruby','dog'),('rosie','dog');\nINSERT INTO pets (name, type, indoor) In a typical project, this is where Cockroach specific syntax is used to define tables and indexes to utilize features like Geo-partitioning , Duplicate-Indexes , and Inverted Indexes . Step 4: Entities, Controllers, Repositories, and Services Spring will have created the src/main/java/com/example/first_cockroach directory with the FirstCockroachApplication.java file as a starting point for this project. Within this directory, create directories to be used to define various objects and services. cd src/main/java/com/example/first_cockroach\nmkdir entities\nmkdir controllers\nmkdir repositories\nmkdir services Step 5: Create Entities for Pets Now the table is defined it we can create an object that maps to the pets table. Create the Pets.java file in the src/main/java/com/example/first_cockroach/entities directory. //\n// Pets.java\n//\npackage com.example.first_cockroach.entities;\n//\nimport lombok.Data;\n//\nimport javax.persistence.Entity;\nimport javax.persistence.GeneratedValue;\nimport javax.persistence.Id;\nimport java.util.UUID;\n//\n@Entity(name = \"PETS\")\n@Data\npublic class Pets {\n@Id\n@GeneratedValue\nprivate UUID id;\nprivate String name;\nprivate String type;\nprivate Boolean indoor = true;\n} Step 6: Create Controller for Pets This controller defines how to insert data into the pets table via a restful controller. Create the PetsController.java file in the src/main/java/com/example/first_cockroach/controllers directory. // PetsController.java\n//\npackage com.example.first_cockroach.controllers;\n//\nimport com.example.first_cockroach.entities.Pets;\nimport com.example.first_cockroach.services.PetsService;\nimport org.springframework.data.rest.webmvc.RepositoryRestController;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.RequestBody;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n//\nimport javax.servlet.http.HttpServletRequest;\nimport java.net.URI;\n//\nimport static org.springframework.web.bind.annotation.RequestMethod.POST;\n//\n@RepositoryRestController\npublic class PetsController {\n//\nprivate final PetsService petsService;\n//\npublic PetsController(PetsService petsService) {\nthis.petsService = petsService;\n}\n//\n@RequestMapping(method = POST, value = \"/pets\")\npublic @ResponseBody ResponseEntity<?> createPets(@RequestBody Pets pet, HttpServletRequest request) {\nPets createdPet = petsService.save(pet);\n//\nURI createdUri = URI.create(request.getRequestURL().toString() + \"/\" + createdPet.getId());\nreturn ResponseEntity.created(createdUri).body(createdPet);\n}\n} Step 7: Create Repositories for Pets This controller defines how to lookup data into the pets table via a restful controller. Create the PetsRepository.java file in the src/main/java/com/example/first_cockroach/repositories directory. // PetsRepository.java\n//\npackage com.example.first_cockroach.repositories;\n//\nimport com.example.first_cockroach.entities.Pets;\nimport org.springframework.data.repository.PagingAndSortingRepository;\nimport org.springframework.data.repository.query.Param;\nimport org.springframework.data.rest.core.annotation.RepositoryRestResource;\n//\nimport java.util.List;\nimport java.util.UUID;\n//\n@RepositoryRestResource(collectionResourceRel = \"pets\", path = \"pets\")\npublic interface PetsRepository extends PagingAndSortingRepository<Pets, UUID> {\n//\nList findByName(@Param(\"name\") String name);\n} Step 8: Create Services for Pets This defines how the Services for Pets. Create the PetsService.java file in the src/main/java/com/example/first_cockroach/services directory. // PetsService.java\n//\npackage com.example.first_cockroach.services;\n//\nimport com.example.first_cockroach.entities.Pets;\nimport com.example.first_cockroach.repositories.PetsRepository;\nimport org.springframework.stereotype.Service;\n//\n@Service\npublic class PetsService {\n//\nprivate final PetsRepository petsRepository;\n//\npublic PetsService(PetsRepository petsRepository) {\nthis.petsRepository = petsRepository;\n}\n//\npublic Pets save(Pets pet) {\nreturn petsRepository.save(pet);\n}\n} Step 9: Time to Run! Now we can boot and run your application with SpringBoot and CockroachDB! Simply type ./gradlew bootRun to run. You should see the following: ./gradlew bootRun\n> Task :bootRun\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v2.2.4.RELEASE)\n2020-02-28 11:22:41.907  INFO 4390 --- [  restartedMain] c.e.f.FirstCockroachApplication          : Starting FirstCockroachApplication on MacBook-Pro-9.local with PID 4390 (/Users/glenn/git/misc_projects_glenn/workshop_java_corelogic/java_gradle_glenn_v1/first_cockroach/build/classes/java/main started by glenn in /Users/glenn/git/misc_projects_glenn/workshop_java_corelogic/java_gradle_glenn_v1/first_cockroach)\n...\n<=========----> 75% EXECUTING [1m 28s]\n> :bootRun Once it is running, open another SQL session to the database and check to see the table has been created and the initial pets have been added. cockroachdb sql --insecure\nroot@localhost:26257/defaultdb> select * from pets;\nid                  |  name  |   type   | indoor\n+--------------------------------------+--------+----------+--------+\n333cf0c1-8245-4b90-9f17-6c059de57fb7 | tula   | cat      |  true\n6f61408f-9074-4d00-80ac-2e189aacf62c | virgil | squirrel | false\n7e4dfb73-9f3d-4e64-aade-1b7a8457ac51 | ruby   | dog      |  true\nce32bd86-6485-4846-af14-55e66eaf792a | rosie  | dog      |  true\n(4 rows) Now let’s try the RESTFUL interface to retrieve the pet data from http://localhost:8080/pets : curl http://localhost:8080/pets\n{\n  \"_embedded\" : {\n    \"pets\" : [ {\n      \"name\" : \"tula\",\n      \"type\" : \"cat\",\n      \"indoor\" : true,\n      \"_links\" : {\n        \"self\" : {\n          \"href\" : \"http://localhost:8080/pets/333cf0c1-8245-4b90-9f17-6c059de57fb7\"\n        },\n        \"pets\" : {\n          \"href\" : \"http://localhost:8080/pets/333cf0c1-8245-4b90-9f17-6c059de57fb7\"\n        }\n      }\n    }, {\n      \"name\" : \"virgil\",\n      \"type\" : \"squirrel\",\n      \"indoor\" : false,\n      \"_links\" : {\n        \"self\" : {\n          \"href\" : \"http://localhost:8080/pets/6f61408f-9074-4d00-80ac-2e189aacf62c\"\n        },\n        \"pets\" : {\n          \"href\" : \"http://localhost:8080/pets/6f61408f-9074-4d00-80ac-2e189aacf62c\"\n        }\n      }\n    }, {\n      \"name\" : \"ruby\",\n      \"type\" : \"dog\",\n      \"indoor\" : true,\n      \"_links\" : {\n        \"self\" : {\n          \"href\" : \"http://localhost:8080/pets/7e4dfb73-9f3d-4e64-aade-1b7a8457ac51\"\n        },\n        \"pets\" : {\n          \"href\" : \"http://localhost:8080/pets/7e4dfb73-9f3d-4e64-aade-1b7a8457ac51\"\n        }\n      }\n    }, {\n      \"name\" : \"rosie\",\n      \"type\" : \"dog\",\n      \"indoor\" : true,\n      \"_links\" : {\n        \"self\" : {\n          \"href\" : \"http://localhost:8080/pets/ce32bd86-6485-4846-af14-55e66eaf792a\"\n        },\n        \"pets\" : {\n          \"href\" : \"http://localhost:8080/pets/ce32bd86-6485-4846-af14-55e66eaf792a\"\n        }\n      }\n    } ]\n  },\n  \"_links\" : {\n    \"self\" : {\n      \"href\" : \"http://localhost:8080/pets{?page,size,sort}\",\n      \"templated\" : true\n    },\n    \"profile\" : {\n      \"href\" : \"http://localhost:8080/profile/pets\"\n    },\n    \"search\" : {\n      \"href\" : \"http://localhost:8080/pets/search\"\n    }\n  },\n  \"page\" : {\n    \"size\" : 20,\n    \"totalElements\" : 5,\n    \"totalPages\" : 1,\n    \"number\" : 0\n  }\n } Now let’s test inserting data via POST to the pets application: curl -i -X POST -H \"Content-Type:application/json\" -d '{\"name\": \"Mazie\",\"type\": \"dog\",\"inside\": \"true\"}' http://localhost:8080/pets\nHTTP/1.1 201\nVary: Origin\nVary: Access-Control-Request-Method\nVary: Access-Control-Request-Headers\nLocation: http://localhost:8080/pets/45234458-6468-4d24-8a2d-b0dad6b8881d\nContent-Type: application/hal+json\nTransfer-Encoding: chunked\nDate: Fri, 28 Feb 2020 19:42:24 GMT\n\n{\n  \"id\" : \"45234458-6468-4d24-8a2d-b0dad6b8881d\",\n  \"name\" : \"Mazie\",\n  \"type\" : \"dog\",\n  \"indoor\" : true\n} Finally, we can query to the pets table to ensure the data is stored within CockroachDB. root@localhost:26257/defaultdb> select * from pets;\nid                  |  name  |   type   | indoor\n+--------------------------------------+--------+----------+--------+\n333cf0c1-8245-4b90-9f17-6c059de57fb7 | tula   | cat      |  true\n45234458-6468-4d24-8a2d-b0dad6b8881d | Mazie  | dog      |  true\n6f61408f-9074-4d00-80ac-2e189aacf62c | virgil | squirrel | false\n7e4dfb73-9f3d-4e64-aade-1b7a8457ac51 | ruby   | dog      |  true\nce32bd86-6485-4846-af14-55e66eaf792a | rosie  | dog      |  true\n(5 rows) Final Thoughts This was meant as a simple example of how to get started with SpringBoot and Cockroach. If you are developing a highly concurrent application with CockroachDB, you will need to take into consideration coding for Retries with Serializable transactions. This is often done with a retry operation with Springboot to handle this event. I hope this was useful to show how to get started developing SpringBoot applications that use CockroachDB . Below is are various links to building Restful applications with SpringBoot, Hibernate, Java, and Gradle. Thanks to Vinh Thai for the guidance with your example and pointers. Reference Documentation For further reference, please consider the following sections: Official Gradle documentation Spring Boot Gradle Plugin Reference Guide Spring Boot DevTools Rest Repositories Spring Data JPA Flyway Migration Gradle Build Scans – insights for your project’s build Guides The following guides illustrate how to use some features concretely: Accessing JPA Data with REST Accessing Data with JPA", "date": "2020-03-04"},
{"website": "CockroachLabs", "title": "Cockroach Labs Raises $87 Million of New Investment, Capping a Year of Exceptional Growth", "author": "Unknown", "link": "https://www.cockroachlabs.com/blog/cockroach-labs-raises-87-million-of-new-investment-capping-a-year-of-exceptional-growth/", "abstract": "We’re proud to mark a new milestone for Cockroach Labs. In a reflection of our continued growth and the overwhelming need for a new database to support cloud-native applications, we’ve closed $87 million in a series D financing round co-led by Altimeter Capital and Bond Capital. Cockroach Labs was founded on a hypothesis: modern, cloud-native applications will need a new type of database built on a distributed architecture to provide bulletproof resilience and elastic, effortless scale, and we can deliver it. We’ve been humbled and encouraged to see validation of our hypothesis over the past four years, from buzz and feedback on Hacker News , to open source adoption , to Github stars . Since launching our commercial product in 2018, we’ve seen companies large and small (including Equifax, Bose, and Comcast) adopt CockroachDB as their database of choice for mission-critical applications. And in the last twelve months, we’ve seen our first-year customers on average more than double the size of their CockroachDB deployments. Unsurprisingly, we’re not the only ones excited about this early proof we might be onto something. Today, we’re proud to announce that we’ve closed $87 million in a series D financing round, reflecting a year of amazing growth and enabling more to come. This latest round is co-led Altimeter Capital and Bond Capital , who both bring a unique and innovative perspective, with participation from our existing investors. Altimeter, who led our series C round last August , have proven to be leaders in the bleeding edge data space, with investments including Confluent and Snowflake. Bond brings thoughtful research and expert guidance to both their investors and the startup world at large. We couldn’t be happier to have such great partners by our side. How Will We Use These Funds? As we’re all seeing, the COVID-19 pandemic is changing our world. For many companies it’s become feast or famine. For some, it’s a time to trim costs and find efficiencies to protect their future. Others are seeing explosive growth that is rapidly increasing the need for data resilience and elastic scale. Worldwide stay-at-home orders are putting tremendous load on many services, putting a heavy burden on all levels of the tech stack at these companies. The last holdouts of digital transformation are finally being pushed into the future. That hypothesis we made---that the future would demand a database that could more effortlessly absorb spikes in demand and not lose data throughout hiccups and outages---isn’t a prediction anymore. It’s a reality. And it’s a reality that CockroachDB was purpose-built to meet. That’s where this new round of funding comes in. Cockroach Labs will put our series D funds to work by doubling down on product development. Whether building a new application or updating an existing application and moving to the cloud, we want to not just provide the best modern database but also make adoption easy. We’ll use these funds to continue adding rich features and capabilities to support your most critical workloads. We’re also continuing to make investments in making CockroachDB even easier to use; improving PostgreSQL compatibility, adding more integrations with development languages and common tools, and investing further in Cockroach Cloud, our database-as-a-service product. As Tony Baer, principal at dbinsight , recently said: “The cloud is reinventing databases, removing traditional shackles to scale and volume that have constrained them, clearing the way for new forms of globally-distributed applications that weren’t thinkable before.” With these funds, we’re going to make the formerly “unthinkable” the new normal. And to make all of this happen, we’re hiring strategically across all departments . Thank You to Our Customers & Community This round of funding, and the growth that enabled it, would not be possible without the support we’ve received from you, our customers and community. At a time when many are suffering from the economic fallout of the global pandemic, we’re more grateful than ever to all the people who help make Cockroach Labs what we are. To all the open core users, the beta release testers, the bug-filers, and feature-requesters, we extend a deep and profound thank-you. Consider this fundraise our continued commitment to you; we’ll continue to focus on engineering what we hope to be the ideal database for our cloud-native future, and building a company you can rely on as a strong, trusted partner that, like our namesake, is built to last.", "date": "2020-05-05"},
{"website": "CockroachLabs", "title": "3 Programs to Support Developers & COVID-19 Frontline Workers", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/do-good-with-us/", "abstract": "The global pandemic has changed the way we live and work. Work-from-home has become the new normal. Web and application traffic patterns have been turned upside-down. These abrupt transitions have added stress not only to our daily lives but also to our infrastructure and business applications. In response to these dramatic shifts, Cockroach Labs is launching new programs to better support our fellow developers and the greater community of those affected by COVID-19. Helping you become a better cloud-native developer Bulletproof, elastic, cloud-native applications are more important now than ever before. But not every developer or technical organization knows how to build a cloud-native application, nor is versed in which tools are the best fit at each layer of the tech stack. For those looking to improve their development skills and build familiarity with the modern tech stack to meet the realities of COVID-19, we have assembled \" Learn With Us ,\" a library of cloud-native resources. The library is organized into three major sections: Build, Deliver, and Deploy. Our collection is comprehensive but by no means exhaustive, so please let us know what might be missing. We are also doubling down on our support for Women Who Code , the world's largest and most active community for technical women. On behalf of every person who signs up for Cockroach University , we will be donating $3 to Women Who Code. As a bonus, every person who completes the curriculum will also receive a goodie bag in the mail, filled with CockroachDB swag. Cockroach University is 100% free. You can enroll here . Supporting workers on the frontlines We are also deeply committed to supporting those affected by COVID-19. One of our chosen non-profits is Direct Relief , an organization that equips frontline healthcare workers with protective gear and critical resources. We will donate $10 to Direct Relief on behalf of anyone who tells us to. You can tell us to donate on your behalf here . Supporting our customers and community We want everyone to succeed, no matter who they are. We hope our efforts, however modest, will be of some help to our customers and to our community. If there are other ways that we can support you, please reach out to us .", "date": "2020-05-14"},
{"website": "CockroachLabs", "title": "Cockroach University: Free, Engaging Online Curriculum on Modern Database Technology", "author": ["Will Cross"], "link": "https://www.cockroachlabs.com/blog/cockroach-university/", "abstract": "Will Cross, Ph.D., is the Training Lead at Cockroach Labs Around 6 months ago , we launched Cockroach University to provide a way for people to learn CockroachDB in a structured environment with opportunities for hands-on experience. Since then, we have learned that it’s actually much more than that. Many of our customers distribute Cockroach University internally, and their developer teams use it as a resource to learn how to build their latest application or proof-of-concept. We’ve seen architects take the course as a way to familiarize themselves with CockroachDB without having to engage with sales. We’ve seen managers use it as an onboarding tool to spin up new team members for ongoing projects. Within its short time in existence, nearly 2,000 people have enrolled in Cockroach University. Dozens in various roles have posted their Certificate of Completion on LinkedIn, showcasing to colleagues (and perhaps future employers) their new skill set. We recently introduced a swag package as an incentive for completing the course, so you'll get some Cockroach Labs gear in the mail as a graduation present. And now, when you sign up , we’ll donate $3 to Women Who Code on your behalf. We've been thrilled with the positive feedback we’ve received for the Getting Started course. We also appreciate the enthusiasm and curiosity for what’s coming next. Why should I sign up? Cockroach University isn’t just for people looking to learn about CockroachDB. Our inaugural course “Getting Started” covers the basics of distributed system, cloud-native application, and general purpose SQL database. You can expect to learn how to: Scale your database horizontally by adding nodes Monitor cluster activity & status Leverage distributed transactions with ACID guarantees Survive node & data center failure with minimal disruption Work with familiar & feature-rich PostgreSQL API with an extensive driver ecosystem Our experts have carefully curated each course to include not just engaging lessons peppered with short quizzes to ensure you're learning along the way, but hands-on exercises to apply what you're learning. Here’s what some recent grads have to say about us: “One of the best intro / overview courses I've seen for a long time. Great stuff.” “I think this is the kind of content that could really set CockroachDB apart from other DBs. I really enjoyed myself throughout the course, even though we already use it in production and test environments. I learned a bit more about geo-partitioning, and will be looking more into that in the future. “Easily one of the most enjoyable self-paced online training courses I've done. Loved the combination of labs and integrating that into the final grading is really clever too. Well done. I certainly look forward to more of these.” What’s coming next? As soon as we released the first course, we started working on the second called “CockroachDB for Python Developers”. As the name implies, this is a pretty specific course that will get more granular into the technical details that developers are interested in. We plan to release the new course this summer, but in the meantime, we are releasing short lessons via YouTube that you can access today, like this one: How can I get started? If you are brand new to Cockroach University and are interested in enrolling, sign up here . Once you register, we will donate $3 to Women Who Code. Upon passing the final exam, we’ll send you a Cockroach Labs goodie bag. Have questions along the way? Feel free to get in touch with us via our community Slack channel, #cockroach-university. We’re here to support you and make sure you get the most out of Cockroach University!", "date": "2020-05-28"},
{"website": "CockroachLabs", "title": "Louder Than Words: How to Create Positive Change", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/how-to-create-positive-change/", "abstract": "Many of our employees, customers, partners, and the communities where we work and live are hurting. The Black community in America has already suffered disproportionately from COVID-19 through illness and unemployment. Exacerbating this, we are extremely saddened by the killings of Ahmaud Arbery, Breonna Taylor, George Floyd, and countless others across the United States. The murder of George Floyd has, again, highlighted the desperate need for police reform and racial justice in this country. Things must change. We believe that as a company we need to do more than simply post a message in support of the Black community. Instead, our contribution is to empower our 150 employees to drive positive change in their communities . Our people are at their best when they are supported to make meaningful contributions in all aspects of their personal and professional lives. For that reason, Cockroach Labs is instituting Connect and Contribute Days -- dedicated time for our employees to engage with and impact the world around them. Cockroach Labs is dedicating Friday, June 5, as our first Connect and Contribute Day for employees to take action. For employees who need additional time, we are encouraging them to work with their managers to support their needs. Importantly, Connect and Contribute Days are going to be a part of Cockroach Labs going forward. Change requires action, and Cockroach Labs is committed to making it easier for our people to contribute not just to their work but to the work needed in their communities. We all have to stand against hate, violence, and racism if we want it to end. And it is going to take decisive action from each of us. Over the next few weeks, we will be sharing the actions our employees have chosen to take to improve the world around them. If you would like to see the contributions our employees make, we will be posting our activities on social media. -- Spencer", "date": "2020-06-04"},
{"website": "CockroachLabs", "title": "How Online Primary Key Changes Work in CockroachDB", "author": ["Rohan Yadav"], "link": "https://www.cockroachlabs.com/blog/online-primary-key-changes/", "abstract": "As of our 20.1 release , CockroachDB supports online primary key changes . This means that it is now possible to change the primary key of a table while performing read and write operations on the table. Online primary key changes make it easier to transition an application from being single-region to multi-region, or to iterate on the schema of an application with no down time. Let’s dive into the technical challenges behind this feature, and our approach to solving them. As part of the deep dive, let’s review how data is stored in CockroachDB, and how online schema changes work in CockroachDB. Data Layout in CockroachDB In many database systems, a primary key is just another index on a table, with extra NOT NULL and unique constraints. Changing the primary key of a table usually just involves changing what index is denoted as the primary key. In CockroachDB, the primary key is special -- all the data for a table is stored on disk as part of the primary key. In order to see how and why CockroachDB stores all data in the primary key, let’s dive into how data storage works in the database. The SQL layer of CockroachDB is built on top of a distributed, transactionally consistent key-value store (for the rest of the blog post we’ll refer to key-value as KV). The SQL system in CockroachDB uses this KV map to store SQL table data and indexes. Let's take a look at what a table and some corresponding data would look like in CockroachDB. Consider the following table and data: CREATE TABLE t ( x INT, y INT, z INT, w STRING, PRIMARY KEY (x, y) ); In CockroachDB, row entries in the table are stored as KV pairs. The primary key columns form the key, and the remaining columns form the value. A row of data would be stored something like this: Row Key Value (1, 2, 3, ‘hello’) /t/primary/1/2/ (3, ‘hello’) In order to change the primary key of a table, we have to rewrite the table data so that the KV pairs are organized according to the new primary key columns. Secondary indexes are stored in a similar way as the primary key -- the indexed columns form the key, and the value is generally empty. This encoding strategy limits data duplication, as only the indexed columns are duplicated. If we created an index using CREATE INDEX i ON t (y, z) the KV’s for index i would look something like: Row Key Value (1, 2, 3, ‘hello’) /t/i/2/3/ () However, if we use a secondary index in a query to find a particular row, we need to be able to find the rest of the row’s data from the secondary index entry. In order to “point back” to the primary index entry for a row, all secondary indexes need to implicitly contain all of the primary key columns. After including the remaining primary key columns, our index entries end up looking  like: Row Key Value (1, 2, 3, ‘hello’) /t/i/2/3/(1:x) () So our KV’s for the primary index and secondary index i together look like: Row PK Key PK Value Index Key Index Value (1, 2, 3, ‘hello’) /t/primary/1/2/ (3, ‘hello’) /t/i/2/3/(1:x) () For more details on CockroachDB’s architecture take a look at our docs or an architecture overview webinar . This strategy for encoding table data means that there is an implicit dependency between the indexes of a table and the table’s primary index. If we want to change the primary index of a table, we need to change it and all the indexes that depend on it. For example, if we changed the primary key of t to be w we’d also need to update i to contain w and not x The difficult part of performing this schema change is ensuring that users of the table only observe the table in a consistent state either before or after the primary key has changed, and that all operations on the table during the primary key change return correct data. Let’s examine how online schema changes work in CockroachDB, and how we can build support for primary key changes on top of this infrastructure. Online Schema Changes To perform a primary key change in CockroachDB, the primary key and some new indexes must be rebuilt. To do this, we can take advantage of the existing online schema change infrastructure, so let’s dive into how it works. When adding a new schema element to a table (like an index), CockroachDB won’t just add the element into the metadata of the table. In a distributed system, doing so could result in data inconsistency between nodes in the system. Talks of data inconsistency may seem out of place as CockroachDB is a serializable and strongly consistent database . However, CockroachDB needs to cache table metadata in order to be performant. Without caching this metadata, a node would need to perform a KV operation every time it needed to access information about a table! As with any cache, it needs to be invalidated when the cached data becomes stale. With this in mind, the data inconsistency mentioned above could happen in the following way: Node 1 adds an index i to a table t and starts to build it. Node 2 inserts value v into t before its metadata for t is invalidated. Since node 2 has no idea that i exists, it doesn’t make an entry for v in i . Node 1 attempts to read i after it is done building and cannot find v ! In order to safely perform schema changes, CockroachDB progresses schema elements through a series of states, where the new element is finally added at the end of this series. A key part of this state machine is that at any point during the schema change, all nodes are at most 1 state apart in the state machine. If this state machine is an ordered set of states S then nodes at any point in the schema change must have either state S_i or S_{i+1} In CockroachDB, these states are associated with different capabilities on the schema object. The capabilities of each state describe which operations on the table will also be applied to the in-progress schema element. These states are: Delete Only (referred to as D ). Schema objects in this state service only deletion operations. Delete and Write Only (referred to as DW ). Schema objects in this state service only write and deletion operations. All (referred to as R ). Schema objects in this state are now “public” and are accessible for all operations: reads, writes and deletions. To begin an index creation, CockroachDB adds the index to the table’s metadata in state D which starts the schema change state machine. Once all nodes in the cluster have agreed on the index’s state, the index progresses into the DW state. CockroachDB starts to backfill existing data into the index in the DW state, and forwards incoming writes to the index. Upon completion of the backfill and all nodes agreeing on the state DW , the index is moved to state R . Once every node agrees on state R the schema change is complete. The procedure to drop a schema element is the same process as adding a schema element, but done in reverse. Instead of incrementally adding capabilities to a schema element, we incrementally remove capabilities until the element has no capabilities. An object with no capabilities is safe to drop. To get some intuition about how this series of states ensures that new schema elements have consistent data, we can examine a few cases during the process of adding an index. Example 1: Mixe d D and DW states. Consider two nodes N_1 and N_2 where N_1 holds an index i of table t in state D and the N_2 holds i in state DW . The state machine in this case ensures that there are no spurious index entries. All write requests sent to N_2 will create an entry in the i for the written data. Because of the capabilities assigned to N_1 all delete requests sent to N_1 will also delete the index entry, ensuring that i is not left with a dangling index entry. Example 2: Mixed DW and R states. Consider two nodes N_1 and N_2 where N_1 holds an index i of table t in state DW and N_2 holds i in state R In this case, the state machine ensures that there are no missing index entries for reads. Because all writes to t on N_1 will be written to i reads on i from N_2 will find the written values. For a different perspective on online schema changes, and some extra topics like schema caching and node failure recovery during schema changes, take a look at our previous blog post on online schema changes! Primary Key Changes The schema change system in CockroachDB can support adding and dropping schema elements, but performing an online primary key change requires doing more. In particular, we have all the requirements of an online schema change, but also need to ensure that all nodes in the system either view the target table as completely before or completely after the primary key change. A node must not see that a table has the new primary key, and no updated indexes, or vice versa. Additionally, the whole CockroachDB cluster needs to operate correctly if some nodes don’t know that the primary key has been changed. Looking closely at what goes into a primary key change, there are three distinct steps that need to be performed: We need to build the new primary index along with the new versions of dependent indexes. Once the new primary key and indexes are ready to be used, we need to swap out what is designated as the primary key, and update all indexes to the new index versions. We need to drop the old primary key and indexes. An important point to notice is that the end of step 1 must be performed along with step 2 and the beginning of step 3. Specifically, as our new indexes are promoted into a readable state, the old indexes need to be demoted out of the readable state. Doing this sequence of steps as an atomic operation will ensure that a single node sees a consistent and valid view of the table -- either before or after the swap. The example below details what requests to different nodes view as the primary key of the table during this process, where old and new are the old and new primary keys of the table. As the previous diagram shows, there are states where some nodes view the primary key of the table before the swap, and some nodes will view it after. Let’s look at how all operations on the database will return consistent data in this mixed primary key state. For simplicity, consider a primary key change where only a new primary key is built. Consider two nodes N_1 and N_2 as well as old and new primary keys on a table t old and new . After the swap in step 2 commits, some nodes will view old demoted to state DW and new promoted to state R and other nodes will view old in state R and new waiting to be promoted to R . Assume that N_1 views the former, and N_2 views the latter. This new state configuration ensures both of the properties from earlier -- no dangling index entries, and no missed index entries. All mutation operations that either node issues will also be applied to whichever index the node thinks is not the current primary index. This ensures that all written values can be read by either node, and that deletes from either node won’t leave any dangling entries. This is a neat extension of the properties discussed in the section about general online schema changes. A schema change progresses safely if the only states of a schema element present in a cluster are 1 state “apart” from each other in the state machine. This property is maintained by the swapping operation: The new primary key transitions from DW to R . The old primary key transitions from R to DW . Because both the schema elements transition to a state 1 step away in the state machine, we can combine the guarantees on both transitions to ensure the resulting schema change is safe! Interaction With Existing CockroachDB Features Implementing online primary key changes required some leg work to play nicely with a variety of features in CockroachDB. I’d like to share some of the more interesting feature integrations that went on during development of online primary key changes. Column Families . In CockroachDB, only primary indexes respected KV level separation described by the column families on a table. The process for primary key changes involves building a secondary index like a primary index, and then switching which index the table thinks is “primary”. This process involved teaching secondary indexes in CockroachDB how to respect column families, as well ensuring that secondary indexes without families in existing CockroachDB clusters are still usable after a version upgrade. Foreign Keys . Our representation of foreign key metadata had to be changed to ensure that nodes could find valid indexes to use for foreign key constraint validation in the mixed index/primary key version state detailed above. This work involved tricky reasoning about metadata representations in mixed version clusters. Interleaved Tables . In CockroachDB, it is possible to store a table within the key space of another table to optimize joint access using interleaving . Getting primary key changes to work with interleaved tables had the unexpected result of allowing users to “uninterleave” tables from undesired configurations in an online manner. Other Schema Changes . The ability to change the primary key of a table may have opened up ways to perform other bulk operations within CockroachDB more efficiently. For example, adding or dropping columns could be viewed as creating a new primary key with the desired new column. Structuring bulk operations this way could lead to faster implementations that interact better with the underlying storage layer of CockroachDB. Conclusion Even on a serializable and consistent KV store, table metadata caching makes performing online schema changes difficult. Changing the primary key of a table is particularly challenging due to its nature as the primary data source of the table, as well as the implicit dependency that other indexes on the table have on it. Our approach was to extend the state machine based approach used for standard online schema changes with an atomic \"swapping\" operation and show that guarantees about correctness are maintained. If building distributed SQL engines is your fancy, we’re hiring! Take a look at our open positions here .", "date": "2020-05-21"},
{"website": "CockroachLabs", "title": "SIGMOD 2020: Cockroach Labs Publishes Research Paper on CockroachDB", "author": ["Jessica Edwards", "Rebecca Taft"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-sigmod-2020/", "abstract": "Over the past few months, a team of our engineers, technical writers, product managers, and sales engineers codified the research and learnings of CockroachDB and are now contributing this knowledge back into the very system from which we have benefited with the hope of further advancing distributed systems research and design. The research paper, \" CockroachDB: The Resilient Geo-Distributed SQL Database \", is a labor of love that we are honored to have published by SIGMOD , the Association for Computing Machinery's (ACM) Special Interest Group on Management of Data, which specializes in large-scale data management problems. \" We live in an increasingly interconnected world, with many organizations operating across countries or even continents. To serve their global user base, organizations are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP workloads to millions of users. CockroachDB is a distributed SQL DBMS that was built from the ground up to support these global OLTP workloads while maintaining high availability and strong consistency. Just like its namesake, CockroachDB is resilient to disasters through replication and automatic recovery mechanisms. This paper presents the design of CockroachDB and its novel transaction model that supports consistent geo-distributed transactions on commodity hardware. We describe how CockroachDB replicates and distributes data to achieve fault tolerance and high performance, as well as how its distributed SQL layer automatically scales with the size of the database cluster while providing the standard SQL interface that users expect. Finally, we present a comprehensive performance evaluation and share a couple of case studies of CockroachDB users. We conclude by describing lessons learned while building CockroachDB over the last five years. \" --- From CockroachDB: The Resilient Geo-Distributed SQL Database We owe a tremendous thank you to the authors of the paper: Rebecca Taft, Irfan Sharif, Andrei Matei, Nathan VanBenschoten, Jordan Lewis, Tobias Grieger, Kai Niemi, Andy Woods, Anne Birzin, Raphael Poss, Paul Bardea, Amruta Ranade, Ben Darnell, Bram Gruneir, Justin Jaffray, Lucy Zhang, and Peter Mattis. First author Rebecca Taft will be presenting the paper at SIGMOD 2020 . This is the first time that Cockroach Labs will be presenting at the SIGMOD conference. Due to COVID-19, SIGMOD 2020 will be run as a virtual conference this year from June 14-19, 2020. Rebecca's session is scheduled for Wednesday June 17 from 1:30pm-3:30pm PDT. Registration is still open , and we can't recommend enough the opportunity to hear directly from those leading the research of today's data management problems. \"CockroachDB: The Resilient Geo-Distributed SQL Database\" is an in-depth resource for someone looking to read up on the most recent advancements in modern database tech. It's an effective vehicle to consume and understand some of the core concepts of CockroachDB, and we hope it will inspire a software engineer or two or three to build the next generation beyond what is presented within. The paper is available to download and read here .", "date": "2020-06-10"},
{"website": "CockroachLabs", "title": "Improving Data Imports in CockroachDB with `nodelocal`", "author": ["Georgia Hong"], "link": "https://www.cockroachlabs.com/blog/data-imports-nodelocal/", "abstract": "This past fall, I had the opportunity to intern on the relatively new Bulk I/O team at Cockroach Labs. Our team’s focus is to improve bulk data operations on CockroachDB. My work involved creating a better experience when importing or backing up data without using an external storage service like Amazon’s S3 or Azure’s Blob Service. CockroachDB supports interacting with files in a node’s local storage system, through a feature called nodelocal . However, this has caused confusion and misuse due to its implementation, and the project I took on involved refining the user experience around nodelocal . How CockroachDB handles external storage To understand my project, it’s important to first gain some background knowledge about how CockroachDB interacts with external storage systems. Bulk data operations, such as IMPORT , BACKUP , or RESTORE often need to interact with external storage systems. Let’s take IMPORT as an example. If you want to import data into your cluster, we provide a few options for you to choose from. The first and the recommended option is to use a cloud provider’s storage system. Your import statement would look something like: IMPORT INTO my_table (id, data) CSV DATA (s3://my-bucket/path/file.csv’); The second option we provide is a similar mechanism to read the file from an HTTP server, which you could serve locally, off your computer: IMPORT INTO my_table (id, data)\nCSV DATA (http://mylocalserver/file.csv’); A third option, is to use what we call nodelocal : reading and writing files in a directory on node(s)’s local file system. That statement would look as follows: IMPORT INTO my_table (id, data)\nCSV DATA (‘nodelocal:///path/file.csv’); At first glance, this appears to be the simplest method, as it does not depend on access to a cloud provider’s storage system or a separate file server, and users often want to use this to import their data into a cluster. The idea is that you could drop your import file(s) onto a node’s local file system, and the import statement will go and import it. However, in practice, prior to my changes that are being released in CockroachDB 20.1, it wasn’t always quite that simple. Prior problems with `nodelocal` storage There are a few subtleties worth noting about nodelocal , many of which were sources of confusion for CockroachDB users. First, the nodelocal URL, nodelocal:///path/file.csv , specifies the path to a file, but not the node on which that file is located. CockroachDB is designed as a distributed, multi-node system and IMPORT takes advantage of that by using many nodes to IMPORT data. In older versions of CockroachDB, if a user places the import files on node1, but node2 ends up being assigned to run part of the import progress, it would fail because node2 would be unable to find the file when it looked in its local filesystem. This behavior was difficult to clarify to users and was often a source of confusion when using nodelocal , particularly since it would “just work” when testing or developing on a single-node cluster but then break in confusing ways when moving into multi-node production deployments. A similar issue affected BACKUP s to nodelocal , which would also succeed but could not be RESTORE ’d. In BACKUP , each node writes its own portion of the data to backup files, which, when using nodelocal , went to each node’s own local file systems. However, those backups were not restorable, since the backup was completely scattered in fragments across the local filesystems of nodes in the cluster. This too, was a source of confusion to users as they could not restore a successful backup. Making `nodelocal` work from anywhere in the cluster The main problem in both cases above is that nodelocal on node1 is not the same as nodelocal on node2 (unless it happens to be configured to point to nfs or something). In order to improve the user experience in the above cases, we planned a two part project. The first and largest part of the project was to create an internode file exchanging service, which could share large files between nodes. We will now support specifying a nodelocal URL as follows: IMPORT INTO my_table (id, data)\nCSV DATA (‘nodelocal://2/path/file.csv’); where 2 is the nodeID of the node whose file system contains the intended import file. In order to support this, the node that needs to read that file would fetch the file from the node where it is stored, and then import it. This service, which we named the BlobService, also supports writing files and thus allows making restorable backups when running the following: BACKUP db.table TO `nodelocal://1/path/backup`\n       AS OF SYSTEM TIME ‘-10s’; RESTORE db.table FROM `nodelocal://1/path/backup`; In the above example, we write the full backup in parallel on every node, each using the BlobService to write directly to the destination path on node1. We can then restore the backup the same way import works, with each node in the RESTORE fetching directly from node1’s file system. In order to integrate the BlobService into the existing infrastructure around external storage interactions in features like IMPORT and BACKUP , we needed to to implement the ExternalStorage interface, which is implemented for each external storage system we support including GCP, AWS, Azure, HTTP, and of course, nodelocal . This means our BlobService needs to be used by the nodelocal implementation of ExternalStorage which has the following interface: type ExternalStorage interface {\n\tReadFile(ctx context.Context, basename string) (io.ReadCloser, error)\n\tWriteFile(ctx context.Context, basename string, content io.ReadSeeker) error\n\tListFiles(ctx context.Context) ([]string, error)\n\tDelete(ctx context.Context, basename string) error\n\tSize(ctx context.Context, basename string) (int64, error)\n} We decided that we can create a gRPC BlobService running on every node in the cluster. We then created a BlobClient which is able to dial any other node’s BlobService and read files. The BlobClient supports the exact same interface as ExternalStorage , making it easily integratable into our existing logic. These changes, along with the URL changes to add a node ID, fixed the behavior of nodelocal to be much closer to user expectations. The Last Mile: uploading files to nodelocal The second part of my project involved making uploading files to nodelocal easier. All of the above improvements significantly improve the experience of using nodelocal , however when we looked at the user-journeys around IMPORT use cases, most users were usually starting with files on their laptop or workstation and were still sometimes getting stuck just getting those files somewhere where their nodes could IMPORT them. In some cases, a user might have a SQL connection through a load-balancer to their cluster, but didn’t have direct filesystem access to write to the nodelocal IO directory. Or even if they did, figuring out where that was and putting files in the right place was a recurring source of confusion. We wanted to mitigate this by allowing users to seamlessly upload files to CockroachDB through the same SQL connection they already have in their SQL shell. To solve this problem, we leveraged the COPY statement, which uses a special protocol to transport bulk data over to the database. We made some slight changes to how COPY statements are processed to write the data that was sent through to a file in a location specified by the user rather than the usual process that inserts it as rows into a table. This was then wrapped in a command in our Cockroach CLI, such that users can now run the following: cockroach nodelocal upload \\\n/path/to/localfile.csv /destination/path/file.csv After the addition of this feature, users who want to IMPORT a file on their local machine an entirely new workflow available to them that does not depend on using any external storage services or file servers. They can now upload their import file right from their laptop using the Cockroach CLI and their existing SQL connection string, and then IMPORT that file using nodelocal URI. Hopefully this will make new user’s onboard faster onto CockroachDB, and improve the developer experience of using nodelocal . Working on Cockroach Labs’s Bulk I/O Team Hope you enjoyed reading about my project. Working on the Bulk I/O team at Cockroach Labs over the past 4 months has been really rewarding. My project involved cross team collaboration because much of the work span across all the layers of Cockroach’s infrastructure. That is, from the SQL team which maintains the pgwire protocol and in particular its COPY sub-protocol, to the KV and replication layer which maintains and tunes many of our higher throughput streaming gRPC services. It was a great team to get an overview of the different layers of a database system, and how that could work in the world of distributed systems. A huge thanks to my teammates David and Lucy for overseeing my project and guiding me through my internship. Thanks to everyone at Cockroach Labs for an amazing 4 months!", "date": "2020-02-13"},
{"website": "CockroachLabs", "title": "Build an App with Active Record + CockroachDB", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/build-an-app-with-active-record-cockroachdb/", "abstract": "To make CockroachDB as accessible as possible, we’ve worked hard over the past six months to add compatibility with various Object Relational Mapping tools (ORMs). We started with ORMs for Python , Java , and Go . Now, we’re excited to announce full CockroachDB compatibility with Active Record , our first ORM for Ruby developers . Active Record is the ORM that powers Ruby on Rails , one of the most popular open-source development frameworks for web applications—and one of the most popular open-source projects of all time. Thousands of companies, including big-name brands like Twitch, Square, and GitHub, use Ruby on Rails as the foundation for their web applications. More than that, Rails follows the guiding principle of optimizing for developer happiness. It has a great global community that inspires new developers to build their first apps, and includes organizations like Rails Girls , which breaks down traditional barriers to programming. We want to make CockroachDB a joy to develop for, so it’s only natural that we’d provide support for Ruby on Rails. Previously, CockroachDB provided beta-level support for Active Record, but we decided that wasn’t enough. Developers need a full toolkit to easily work with CockroachDB. We’re excited that we can now provide that toolkit. Our docs team has written up a great tutorial on building a Hello World App with Active Record and CockroachDB. Follow the tutorial below and explore! Build a Hello World App with CockroachDB & Active Record", "date": "2020-06-17"},
{"website": "CockroachLabs", "title": "Nested Transactions in CockroachDB 20.1", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/nested-transactions-in-cockroachdb-20-1/", "abstract": "CockroachDB 20.1 introduces support for nested transactions , a SQL feature which simplifies the work of programmers of certain client applications. In this post, we'll explore when and where to use nested transactions, when not to use them, and how they work in a distributed environment. What are nested transactions? A regular SQL transaction groups one or more statements together and provides them the A (Atomicity) and I (Isolation) of ACID. These are guarantees over the effects of the transaction from the perspective of concurrent clients running other transactions, or subsequent transactions by the same client. Atomicity means that all the statements inside the transaction appear to execute either successfully, or not at all. Isolation means that a concurrent transaction cannot see the intermediate steps inside the transaction and can only perceive the state of the database before the first statement starts, or after the last completes. Nested transactions are additional transactions that occur within a regular transaction or other nested transactions, like Russian dolls. Nested transactions are invisible to concurrent clients, due to the atomicity and isolation of their surrounding, “outermost” transaction. Therefore, they are a feature that exists only for the benefit of the client issuing the outer transaction. More specifically, nested transactions have been invented for the benefit of software engineers of client applications. In component-based design , different programmers are responsible for the internals of different components and may not know about each other's work. If their components need to collectively operate over a database, they face an important challenge: how can each component safely contribute its portion of the database work, in the context of a multi-component transaction? If one component fails its work, it is usual to want another component to “take over” and complete the work in a different way, without giving up on the overall transaction. Nested transactions facilitate this, by providing atomicity (all succeed, or all fail) to the work of sub-components in the client app. An example of this could be an hypothetical order system of your favorite assemble-your-own-furniture store. As the customer is walking the aisle and selecting their parts, their partial order starts a transaction that gets a lock on the supplies they need, incrementally. At some point, they may be reaching the kitchen area and start a design project using the provided interactive console. As they are experimenting, they wish to move forward with their new kitchen only if all the parts are available. What if the logic that allocates the parts encounters an insufficient supply and fails to instantiate the full kitchen on their order? At that point, the customer may want to resume their shopping, without the kitchen order, but with all the other items they had in their shopping cart before: they want to roll back the kitchen sub-transaction, without giving up on the surrounding transaction. Nested transactions help achieve that. Nested transactions in client apps Nested transactions are useful in component-based design of client apps. An app large enough to use component-based design rarely lets programmers use SQL directly and database access is typically abstracted in some framework (e.g. Spring , Flyway , or similar ORMs). When database access is suitably abstracted, a nested transaction looks like a transaction “obtained from” another transaction, in contrast to regular transactions “obtained from” a database connection object. For example: # regular txn\nmyTxn := conn.BeginTxn()\nmyTxn.Execute(\"INSERT 1\")\nmyTxn.Execute(\"INSERT 2\")\nmyTxn.Commit()\n\n# regular txn with nested sub-transaction\nmyTxn := conn.BeginTxn()\nif component1.doSomething(myTxn) != success:\n     component2.doSomethingElse(myTxn)\nmyTxn.Commit()\n# then in component1:\ndef doSomething(txn): \n     subTxn := txn.BeginTxn() # implicitly starts a nested txn\n     r := subTxn.Query(\"SELECT A\")\n     if r != expected:\n         subTxn.Rollback()         # aborts the nested txn\n         return failure\n     subTxn.Execute(\"INSERT B\")\n     subTxn.Commit()             # completes the nested txn\n     return success This example provides a typical example of component-based design: the top level component logic does not need to know what the sub-components component1 and component2 are doing with the transaction. It can assume that they leave the txn they're given in a good state. It can also assume that if component1.doSomething fails, it will leave myTxn “in the state it found it upon entry”. This separation of concern would not be possible if an error in component1 was trashing the outer transaction and forced the top level logic to restart the transaction from scratch. How client drivers implement nested transactions Nested transactions look and feel like regular transactions in client code. However, this is only possible thanks to some translation magic by the client SQL driver. Under the hood, a SQL driver maps requests to begin, commit or roll back a nested transaction as follows: Begin a nested transaction: auto-generate a name for the sub-transaction; for example subtxn123123 send \" SAVEPOINT subtxn123123\" to the database. This tells the database to “start a nested txn and remember it is called 'subtxn123123'.\" Commit a nested transaction: send \"RELEASE SAVEPOINT subtxn123123\" to the database. This tells the database to “commit the nested txn called subtxn123123.\" Roll back (abort) a nested transaction: send \"ROLLBACK TO SAVEPOINT subtxn123123\" to the database. This tells the database to “rewind the nested txn called subtxn123123 to its beginning state.\" then send \"RELEASE SAVEPOINT subtxn123123\". This commits the nested txn; however, since it was just rewound by ROLLBACK, this cancels all its effects and thus aborts it. The reason why ROLLBACK TO SAVEPOINT does not automatically imply RELEASE SAVEPOINT (i.e. RELEASE is necessary in any case) has to do with how certain client drivers abstract transactions in object-oriented languages. In particular, it is common for drivers to provide transaction objects that auto-commit when they are finalized. For example: with outerTxn.BeginTxn() as nested: \n       nested.Execute(\"SELECT\")\n       if some condition:\n            nested.Rollback()\n            return\n       nested.Execute(\"INSERT\") In such a language, the destructor/finalizer of the transaction object (“nested” in this case) will issue RELEASE SAVEPOINT unconditionally. The Rollback() call issues just ROLLBACK TO SAVEPOINT. If ROLLBACK TO SAVEPOINT implied RELEASE, the RELEASE would be executed two times. Nested transactions in CockroachDB SQL CockroachDB receives just SQL statements from client drivers; from its perspective, it has just three things to care about: the SAVEPOINT, ROLLBACK TO SAVEPOINT and RELEASE SAVEPOINT statements. In this section, we ignore the special meaning that CockroachDB was giving these statements prior to v20.1 (this is covered later below) and instead consider how they work for nested transactions. From the database's perspective, the statements mean the following: SAVEPOINT: mark the beginning of a nested sub-transaction. ROLLBACK TO SAVEPOINT: rewind a nested sub-transaction to its beginning state. RELEASE SAVEPOINT: “commit” (really: forget) a nested sub-transaction. Each of these statements are given a savepoint name: the name given by the client app to the nested transaction. Because of component-based design, it's possible for a client component holding a nested transaction to give it as a starting point to a child component using its own nested transaction. The nested transactions can thus nest inside each other like russian dolls. This means that the database can receive a SAVEPOINT statement “under” another and needs to remember the nesting of names. For example, the following is valid SQL: SAVEPOINT somename;\n    SAVEPOINT somename; -- same name, but this is really a sub-nested transaction\n    RELEASE SAVEPOINT somename; -- release inner\nRELEASE SAVEPOINT somename; -- release outer In other words, the database must remember and maintain a stack of nested transaction names. This is done in CockroachDB by allocating a unique internal token for each nested transaction, and then mapping client-provided names to the internal tokens using a stack data structure. This data structure can be observed inside CockroachDB using the new statement SHOW SAVEPOINT STATUS inside a (nested) transaction. The true cost of nested transactions If one were to attempt to design an implementation of nested transactions, it's likely one would come to several ideas in succession. (For convenience, this section uses the shorthand \"ROLLBACK\" to refer to \"ROLLBACK TO SAVEPOINT\". This is different from SQL's plain \"ROLLBACK\" which we will only talk about later.) A first idea could be to make the statements in the nested transaction operate on a memory copy (or temporary disk area) of the data, and delay performing actual data updates until the RELEASE statement. This way, a ROLLBACK merely has to delete the copy and the surrounding transaction can continue as if nothing happened. The problem with this approach is that in the common case (no ROLLBACK), the work has to be done twice: once on the copy, then one on the actual data during RELEASE. This is expensive. A next idea could be to make the nested transaction update actual data in the same way the outer transaction would, but memorize all changes being made. In the common case of just a RELEASE, nothing else needs to happen. However, a ROLLBACK can then rewrite everything that has been modified so far. The limitation of this approach is that it also requires paying memory / disk space to memorize all the changes even in the common case where ROLLBACK is not issued. The solution adopted in CockroachDB, like in most other mature SQL engines, is to minimize the cost in the common case and instead incur an incremental cost on the remainder of the transaction in the less likely case ROLLBACK is used. Conceptually, this is done by: labeling every data update in a transaction with a marker that identifies the nested transaction. upon ROLLBACK, mark the corresponding nested transaction marker as \"to be ignored\" upon any read of the data, skip over any data updates with a (nested) transaction marker known to be ignored. The expense in this approach is the extra work performed by data reads when there has been at least one ROLLBACK prior. ROLLBACKs add markers to an “ignore list” and cause it to grow, and the performance of each subsequent read operation is slightly  slowed down by the increasing ignore list. How it works: update markers and the ignore list The reason why it is possible to “skip over ignored updates”, as stated above, is that CockroachDB uses an MVCC algorithm to access data. During data writes, the original data is not modified in-place. Instead, another data item is written next to it with the new value. This is called a write intent . During read operations, all write intents (and the original data record) are considered together to determine the current logical value. The intent object contains metadata about the write, not just the updated value. It contains, for example, the ID of the transaction modifying the value, so that other transactions know to skip over it (so as to guarantee isolation). Some time before v20.1 (in v19.1, in fact), CockroachDB started numbering every write in a transaction, and writing that sequence number inside the intent objects. This is the mechanism that was reused for nested transactions. The numbering itself wasn't changed; instead, read operations are now also provided a list of ranges of ignored sequence numbers (the “seqnum ignore list”).  When a read considers a write intent, it now checks whether its sequence number is included in the ignore list. If it is, it is skipped over and does not count. This makes ignored intents invisible to reads, as if the write never occurred in the first place. This makes nested transactions work as follows (approximately): The SAVEPOINT statement memorizes the current write sequence number. The ROLLBACK TO SAVEPOINT statement takes the current write sequence number (at the point ROLLBACK is issued), then the sequence number when SAVEPOINT was issued, then adds this range to the ignore list. (Overlapping ranges are combined together, as an optimization.) The RELEASE statement does... nothing. At the end of the outermost transaction, all write intents are “resolved”: If the transaction is committed, the intents are transmuted into an actual data value so that subsequent transactions don't skip over them any more. If the translation is aborted, the intents are simply deleted. The intent resolution algorithm also uses the new seqnum ignore list as input, and simply deletes all intents whose sequence number is included in the list—even when the outermost transaction commits. To summarize: In the most common case where clients do not use nested transactions at all, the ignore list remains empty all the time and there is no additional performance price to pay. In the next most common case where clients use nested transactions but do not use ROLLBACK TO SAVEPOINT, the ignore list also remains empty all the time and there is no additional price to pay during read/write operations. The only price is the management of the stack of names of nested transactions. In the occasional case where ROLLBACK TO SAVEPOINT is used, the ignore list starts to grow; then every subsequent read by the same transaction, as well as intent resolution at the end, must perform some additional work to skip over all intents marked as ignored. Error state recovery The explanation so far covers the case where ROLLBACK TO SAVEPOINT is used to cancel the effects of data writes in a transaction that's otherwise healthy (also called “open”). Additionally, if ROLLBACK TO SAVEPOINT is used after a database error, it can also cancel the error state of the transaction. This is best understood using a state diagram. With regards to errors, transactions generally operate as follows: The idea here is that when a statement inside a transaction encounters an error, the transaction is marked internally as \"Aborted\". It still exists, but further statements cannot operate and encounter errors, until the transaction is canceled by the client using either COMMIT or ROLLBACK/ABORT. (In the Aborted state, COMMIT and ROLLBACK/ABORT are equivalent.) This general principle remains valid for nested transactions. Inside a nested transaction, an error also moves the nested transaction into the Aborted state and prevents further SQL statements. However, ROLLBACK TO SAVEPOINT clears that aborted state. This can be represented as follows: This implies that it is possible to use ROLLBACK TO SAVEPOINT to “recover” from a logical error, such as that reported by a foreign key constraint check (row does not exist in referenced table), a unique index (duplicate row), etc. It can also recover from mistakes in queries (e.g. column does not exist). The client code can use ROLLBACK TO SAVEPOINT to “paper over” the error and continue with a different statement instead. (Reminder: the current error state of a transaction can be observed inside CockroachDB by issuing the statement SHOW TRANSACTION STATUS.) Current Limitations CockroachDB's support for nested transactions in v20.1 is limited in three noticeable ways, which can impact the design of new applications and compatibility with applications designed for PostgreSQL. Locking semantics Like other SQL engines, CockroachDB places locks on updated rows during write operations, so that concurrent transactions cannot operate on them until the first transaction commits (or aborts). These locks are implicit in mutation statements like INSERT or UPDATE; or explicit in the newly introduced FOR UPDATE clause for SELECT statements. As per the SQL standard, ROLLBACK TO SAVEPOINT should “rewind” all the effects performed by the nested transaction so far; this theoretically includes all the row locks. In other SQL engines, this is also what happens: when a nested transaction is rolled back, its row locks are released. In CockroachDB v20.1, this is not what happens: as described in a previous section above, ROLLBACK TO SAVEPOINT preserves all the write intents written so far. Additionally, other concurrent transactions cannot see the “ignore list” of sequence numbers, and thus cannot perceive when an intent has been marked as ignored. Therefore, locks persist upon rewinding a nested transaction , unlike in other SQL engines and unlike in PostgreSQL. To illustrate this, consider the example introduced at the beginning: that of a customer walking through their furniture store. Suppose that they had started an ambitious project to design a large kitchen with blue tiles. Then, upon placing their order, it turns out that there were not enough blue tiles in inventory to satisfy that order. If that store was running PostgreSQL, the customer canceling their nested kitchen transaction could continue shopping while the blue tiles would immediately become available to other customers in the kitchen area. With CockroachDB, the other customers must wait for the first customer to check out their entire shopping cart before they, in turn, can order blue tiles. This current difference in behavior may thus impact the design of commerce-oriented client applications or any kind of OLTP system really. We acknowledge this; this divergence from the SQL standard  may be lifted in a future version of CockroachDB. Canceling schema changes Another limitation is that ROLLBACK TO SAVEPOINT does not yet know how to rewind over DDL statements in certain cases. For example, in other SQL engines, it is possible to rewind over e.g. CREATE TABLE or DROP INDEX using ROLLBACK TO SAVEPOINT and let the transaction proceed as if the DDL had not taken place at all. In CockroachDB, attempting to rewind a nested transaction that has executed some DDL statement(s) will likely encounter the error \"ROLLBACK TO SAVEPOINT not yet supported after DDL statements\". This limitation will likely be lifted in a subsequent version. Meanwhile, as in previous versions, CockroachDB v20.1 can cancel the effect of DDL using ABORT/ROLLBACK on the outer “regular” transaction. The specific reason for this limitation is that CockroachDB caches metadata about tables accessed by statements inside a transaction. When rewinding a nested transaction, it would need to invalidate these caches but only for the subset of the tables altered inside the nested transaction. This additional complexity in the management of these caches is not yet implemented. The particulars of this limitation also hint at when ROLLBACK TO SAVEPOINT actually can cancel DDL: if there was no query using a SQL table (any table!) prior to the nested transaction. This is not a particularly interesting case for client apps that need nested transactions due to component-based design, but happens to be common for CockroachDB clients for an unrelated reason; see the next sections. Recovering from serializability conflicts, a.k.a. “retry errors” Reminder: under various circumstances, a data read/write operation or transaction commit can encounter a conflict with a concurrent transaction. This occurs when the current transaction and the other transaction happen to operate on the same data and one of the two is writing. CockroachDB tries very hard to  handle this conflict internally and hide it from the SQL client application. However, in some cases, internal resolution is impossible and the conflict is reported to the client as a SQL error, with SQLSTATE 40001 and an error message containing a variant of the string \"restart transaction\". This general situation is common to all SQL engines implementing the SERIALIZABLE isolation level, including CockroachDB since v1.0 and PostgreSQL. As of CockroachDB v20.1, ROLLBACK TO SAVEPOINT cannot cancel the error state that occurs from a serializability conflict except in a very particular scenario: when the nested transaction being rewound is a direct child of the outermost transaction, and its write set coincides exactly with it. In other words, it is only possible if the outer transaction did not write anything before the nested transaction was created; this includes hidden writes such as that performed when the transaction accesses a stored SQL table. For example: BEGIN;\n  INSERT;\n  SAVEPOINT foo; -- nested transaction starts here\n    INSERT; -- encounters serializability failure\n  ROLLBACK TO SAVEPOINT foo;\n   -- rollback itself causes error 40001 \n   -- \"restart transaction: cannot rollback to savepoint\" The error is encountered because there was a write in the outer transaction (the first INSERT) before the nested transaction was created. In contrast, the following works: BEGIN;\n  SAVEPOINT foo; -- nested transaction starts here\n    INSERT; -- encounters conflict and error 40001\n  ROLLBACK TO SAVEPOINT foo; -- ok\n    INSERT; -- something else\n  RELEASE SAVEPOINT foo;\nCOMMIT; -- end of txn The following also works, and also illustrates the case where ROLLBACK TO SAVEPOINT can rewind over DDL: BEGIN;\n  SAVEPOINT foo;\n    CREATE TABLE t(x INT); -- CREATE in nested txn\n    INSERT INTO t(x) VALUES( 1);\n    INSERT INTO other ... ; -- encounters conflict and error 40001\n  ROLLBACK TO SAVEPOINT foo; -- ok, also cancels the CREATE TABLE\n    CREATE TABLE t(x INT); -- client retries the CREATE in nested txn\n    INSERT INTO t(x) VALUES( 2);\n  RELEASE SAVEPOINT foo; -- end of nested txn\nCOMMIT; -- end of txn What this means for programmers of client apps is that they must continue to handle retry errors, and they can only do so in the top level transaction—the one held by the coordinating component for the database transaction. It is not (yet) possible to handle retry errors at the level of individual components in a multi-component app using nested transactions. A special case: the restart protocol As another artifact of SERIALIZABLE isolation shared with PostgreSQL, CockroachDB can encounter a serializability conflict while committing a transaction. If the transaction is committed as a side effect of the client issuing a COMMIT statement, there is little the client can do to recover from the error: by that time, all the data structures and other state in the client code used during the transaction is (usually) already destroyed. (Remember that COMMIT is often issued in the destructor of transaction objects in client frameworks, as part of an RAII protocol .) The recommended way forward is thus to propagate the error in the application to a level in the source code able to restart the SQL transaction from scratch. Due to peculiarities in the design of CockroachDB v1.0, it was not desirable then to let clients restart from scratch upon serializability conflicts. Instead, CockroachDB v1.0 would perform better if the client reused the original SQL transaction somehow after a serializability conflict. To achieve this, CockroachDB has historically provided its own bespoke restart protocol , triggered by a “magic word”: the special statement \"SAVEPOINT cockroach_restart\" (note the specific name \"cockroach_restart\"). That magic word, since v1.0 and up to and including v20.1, is wholly unrelated to nested transactions as discussed so far. It happens to use a common SQL keyword (\"SAVEPOINT\") but triggers quite unrelated logic and behavior inside CockroachDB. Therefore, it is useful to think of it as a separate feature. When the magic word is encountered, CockroachDB upgrades the transaction to a different mode, where subsequent statements are interpreted differently. In that alternate mode, the words \"RELEASE SAVEPOINT cockroach_restart\" become equivalent to the COMMIT statement in the simple/normal mode. Then, the COMMIT statement becomes (more-or-less) a no-op. In other words, in the alternate transaction mode started with \"SAVEPOINT cockroach_restart\", the transaction commits when \"RELEASE SAVEPOINT cockroach_restart\" is sent by the client. After that statement, the transaction is already committed even before the SQL COMMIT statement is issued. Because it is committed, no other statements are allowed. This can be represented by the following state diagram: The alternate mode can only be enabled immediately after the BEGIN statement. The magic word is rejected with an error if it is encountered later. For backward compatibility with previous CockroachDB versions, v20.1 preserves this feature and the associated complexity, even though the motivation for it does not exist any more .  Indeed, since v1.0, it has become less important to reuse the same SQL transaction to effectively recover from serializability errors at commit time. In fact, since v19.1 we do not actively recommend clients to implement the special restart protocol any more. One reason is that CockroachDB has become better at handling serializability conflicts internally without participation from the client app (i.e. the retry errors with code 40001 have become rarer). Additionally, CockroachDB has become better at letting clients recover from a serializability error by retrying the transaction from scratch, as in other SQL engines. The preservation of this special restart protocol in v20.1 was thus done mainly out of care for backward compatibility. The restart protocol may be phased out in a later version. Beware: don't (over) use nested transactions Even though nested transactions are part of the SQL standard, PostgreSQL supports them and many of our users have demanded their inclusion in CockroachDB, we do not recommend their use in new applications. The reality is that nested transactions are a product of the early days of software engineering, in the 1990s, back when systems were tightly coupled and the Internet and the Cloud were not yet very relevant. They only make sense in the context of apps a) whose design is component-based but tightly coupled and b) whose architecture has promoted database transactions spanning multiple components. Only this combination makes it possible to pass an outer transaction from one component to the next to build nested transactions upon, and thus implicitly design one component with hidden, implicit knowledge of global state held by another component via its transaction handle. Nowadays, such tight coupling has a bad rep. This is because two additional decades of software engineering have taught us that implicit global state really, really does not play well with distributed services where some components may fail even as part of normal load. It makes it hard to fail over a component gracefully by transferring its responsibility to a hot spare. It makes it hard to reason about the current state of a component and troubleshoot it by just looking at the log of its inputs and outputs. Additionally, nested transactions can amplify performance anomalies. The row locks established by a large transaction whose latency extends across multiple components/services are more likely to incur transaction conflicts which, at best, increase the average latency of queries and, at worse, increase the error rate quadratically with the current query load. Finally, nested transactions can run afoul of correctness in distributed apps. In fact, the idea of multi-component transactions in client code really evokes the idea of a bull in a china shop. As long as all is well and the transaction is due to commit, the idea somewhat makes sense. However, what happens when the database (and not the client) decides the transaction is un-committable and must be aborted, for example because of a serializability conflict or a node restarting for maintenance? It is not just the database state that must be rolled back; all the possible side effects performed by the components holding the transaction must also be rolled back. Think about payment transactions. Think about email notifications. For one, the programmer is unlikely to think about this properly before the app is deployed. Also, as the number of components involved grows, the chance they are collectively performing unabortable external side effects (logging, payments, notification emails or SMS, REST queries to external HTTP APIs, etc) increases dramatically. So, in truth, CockroachDB supports nested transactions to ease adoption by users who are already pulling a weight of legacy software. We want to help them, too, bring their legacy code to the Cloud era and offer them the reliability and scalability of CockroachDB. But we do not wish more of their problems to appear elsewhere. If you have a choice in the matter, keep your database transactions as small as possible. Do not let them span multiple components. Do not interleave database transactions with application-level side effects or external calls to APIs. Favor append-only event logs where it makes sense. Partition the data to ensure that each locale of data access is isolated from other locales and cannot cause transaction conflicts. Reduce the scope of transactions to minimize the number of rows they ever access and may lock. In that world of best practices, there is little need for nested transactions, and that is a good thing. Summary CockroachDB v20.1 has introduced support for SQL nested transactions. Nested transactions are a legacy feature of certain SQL engines that facilitates component-based design in certain client applications. Although we do not encourage the use of nested transactions, because the concept is antithetical to good cloud application design, we provide the feature for compatibility with existing PostgreSQL client apps and ease adoption of CockroachDB by legacy code. Support for nested transactions is provided in a way compatible with PostgreSQL's dialect, using the SAVEPOINT, RELEASE SAVEPOINT and ROLLBACK TO SAVEPOINT statements. When learning, understanding and using this feature, the user should be careful to distinguish nested transactions , a standard SQL feature newly supported in v20.1, from CockroachDB's bespoke transaction restart protocol triggered by the magic SQL phrase \"SAVEPOINT cockroach_restart\", introduced back in CockroachDB v1.0. This magic phrase happens to use a common keyword (\"SAVEPOINT\") but triggers a behavior and mechanisms quite unrelated to nested transactions.  It is thus essential to distinguish the two features using the terms “nested transactions” and “restart protocol” and avoid the word “savepoint” which has become ambiguous. At the time of this writing, there are some limitations in CockroachDB's support for nested transactions. In particular, a nested transaction cannot be rolled back if it contains DDL statements (schema changes) or if it encounters a serializability conflict which CockroachDB cannot handle internally, unless the nested transaction was created immediately after the outer transaction. Rolling back a nested transaction also does not release row locks, unlike in other SQL engines, but these limitations may be lifted in a later version. If reading about serializability and transactions is your cup of tea, we're hiring !", "date": "2020-06-15"},
{"website": "CockroachLabs", "title": "When and Why to Use SELECT FOR UPDATE in CockroachDB", "author": ["John Kendall", "Nathan VanBenschoten"], "link": "https://www.cockroachlabs.com/blog/when-and-why-to-use-select-for-update-in-cockroachdb/", "abstract": "We didn’t implement SELECT FOR UPDATE to ensure consistency. Unlike Amazon Aurora, CockroachDB already guarantees serializable isolation, the highest isolation level provided by the ANSI SQL standard. Contention happens. And application developers shouldn’t need to risk the integrity of their data when transactions contend. To combat this, CockroachDB must occasionally return errors prompting applications to retry transactions that would risk anomalies, such as write skews. This means that just like with PostgreSQL in serializable isolation , developers need to implement retry-loops for contended transactions . For developers accustomed to relational databases with lower isolation levels, this can be an unfamiliar pattern. In CockroachDB 20.1, we simplified the handling of client-side transaction retry errors by enabling SELECT FOR UPDATE. This feature allows applications to take explicit control of row-level locking on a per-statement basis. By locking more aggressively while reading early on in a transaction, applications can avoid situations that lead to client-side transaction retry errors. In this post, we’ll explore transaction retry errors and how SELECT FOR UPDATE comes into play. To wrap up, we'll share a series of tests, using highly contended workloads comparing performance with and without SELECT FOR UPDATE (between v19.2 and v20.1), to evidence performance boosts and fewer client-side transaction retry errors. SELECT FOR UPDATE Demo In the following gif, we demonstrate SELECT FOR UPDATE avoiding a client-side transaction retry error: This example reveals the most common form of retry error--a transaction reads a row without acquiring a lock, it makes a modification to that row in the client, and then it attempts to write the modification back to the same row in the database. If a second concurrent transaction also modifies that row and commits between the first transaction’s read and write, the first transaction must retry. If it did not, it would risk writing the wrong updated value back to the row. For example, imagine two increments on the same integer counter. If both increment transactions read the value 12 from the counter and both are later allowed to update it to 13, then the two writes will have “skewed” and the effect of one of the transactions will be lost. A database that provides serializable isolation cannot allow this to happen! So on its own, the retry error is better than data corruption, but what if we don’t want to implement a client-side retry loop. That’s where SELECT FOR UPDATE comes in. By selecting the initial state of the row using SELECT FOR UPDATE in each transaction, we acquire an exclusive lock on the row earlier. If two concurrent transactions attempt to SELECT FOR UPDATE the same row, one will have to wait. This prevents us from ever getting into a situation where we have returned out-of-date information to the client about the value of the row, so we never need to revoke that information using a retry error. Benchmarking SELECT FOR UPDATE with the Yahoo! Cloud Serving Benchmark YCSB ( Yahoo! Cloud Serving Benchmark ) is an industry standard. It simulates “realistic” internet-style workloads that cause hotspots and create contention. To validate performance with SELECT FOR UPDATE, we used v19.2 and v20.1 on a 3-node cluster of c5d.9xlarge machines (AWS) and applied workloads A and F of the YCSB benchmark suite. For both workloads A and F, we tested with and without the YCSB column families optimization. This optimization uses a column family per column to avoid contention between writes to different columns within the same row. Workload A tests implicit SELECT FOR UPDATE statements. When sql.defaults.implicit_select_for_update.enabled cluster setting is enabled (it defaults to true), implicit SELECT FOR UPDATE uses the read-locking subsystem internally during UPDATE statements. An UPDATE statement is translated into a read and a write in CockroachDB. Implicit SELECT FOR UPDATE means that we use FOR UPDATE locking during the read portion of the UPDATE statement. This means your applications reap the benefits of implicit SELECT FOR UPDATE without any changes. YCSB Workload A issues 50% SELECT statements and 50% UPDATE statements along this skewed distribution. This allowed us to test the implicit SELECT FOR UPDATE functionality because the UPDATE statements uses the read-locking subsystem to improve performance. Workload F tests explicit SELECT .. FOR UPDATE statements. YCSB Workload F issues 50% SELECT statements and 50% read-modify-write transactions i.e. BEGIN ; SELECT ; UPDATE ; COMMIT ;. This allowed us to test the explicit select for update functionality, as we can experiment with adding FOR UPDATE to the SELECT statement in the read-modify-write transaction. Before and After CockroachDB 20.1 Results We ran YCSB on CockroachDB versions 19.2 and 20.1 to see the difference. Throughput Between v19.2 and v20.1, YCSB throughput (operations/second) improved on all cases: workload A, workload F, with and without column families. The greater queueing responsiveness and reduced thrashing, due to transaction retries, and improved system throughput. Tail Latency Between v19.2 and v20.1, the maximum transaction latency dropped across all workloads and with and without column families. This demonstrates that the improved fairness characteristics of SELECT FOR UPDATE queueing translates to reduced tail latencies and more predictable performance. Number of transaction errors We returned to the inspiration for implementing SELECT FOR UPDATE by measuring the number of client-side transaction retries while running YCSB. YCSB’s Workload F performs multi-statement transactions ( BEGIN ; SELECT ; UPDATE ; COMMIT ;) and is therefore susceptible to client-side retries. The Admin UI reports the number of client-side transaction retries in the sql.restart_savepoint.rollback.count report: The test started by running the Workload F against Cockroach v19.2. Just after 01:24, Cockroach was upgraded to v20.1 to enable our implementation of SELECT FOR UPDATE - the moment of truth. The load generator started adding a FOR UPDATE suffix to the SELECT statement in the multi-statement transaction. Client-side retries disappeared. All retry conditions hit on the first statement in the transaction, allowing them to be retried transparently on the server. Conclusion In CockroachDB 20.1, tests demonstrate enabling SELECT FOR UPDATE improved CockroachDB in three ways: Throughput Tail latency Number of transaction retry errors For implicit UPDATE statements, this new feature is enabled by default in CockroachDB 20.1. But for explicit SELECT FOR UPDATE, you'll need to explicitly modify your queries to reap the benefits. To get started with SELECT FOR UPDATE, dive into our SELECT FOR UDPATE documentation .", "date": "2020-06-22"},
{"website": "CockroachLabs", "title": "How to get zero-downtime scaling from single-region to multi-region applications", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/zero-downtime-scaling/", "abstract": "CockroachDB offers a number of features aimed at making it easy to support your application in multiple regions. In CockroachDB 20.1, we introduced a new feature Online Primary Key Changes that allows you to upgrade your application from a single-region to multi-region with zero downtime. We wrote a bit recently about the technical challenges involved in building online primary key changes and in this blog post, we'll walk through some of the use cases and benefits the feature can lead to. A Quick Example First, download CockroachDB 20.1 using your preferred method including Linux, Mac, or Windows binaries, Docker images, or building directly from source. This example will focus on the fictional ride-sharing company MovR that we’ve previously written about when introducing lateral joins . MovR is a ride sharing company that operates in multiple cities around the globe. We have explored its schema in great detail in this multi-region blog post . You can also use the new CockroachDB demo feature that we recently blogged about to try this out in less than five minutes. To use CockroachDB demo (and the more complex rides table with pre-populated data) enter the following command: ./cockroach demo This command will automatically load the MovR tables and data: show tables;\n\ntable_name\n+----------------------------+\n  promo_codes\n  rides\n  user_promo_codes\n  users\n  vehicle_location_histories\n  vehicles\n(6 rows) You can see that this rides table has already been well set up for multi-region as it has a compound primary key that places the region in front of id: show create table rides;\n  table_name |                                                        create_statement\n-------------+----------------------------------------------------------------------------------------------------------------------------------\n  rides      | CREATE TABLE rides (\n             |     id UUID NOT NULL,\n             |     city VARCHAR NOT NULL,\n             |     vehicle_city VARCHAR NULL,\n             |     rider_id UUID NULL,\n             |     vehicle_id UUID NULL,\n             |     start_address VARCHAR NULL,\n             |     end_address VARCHAR NULL,\n             |     start_time TIMESTAMP NULL,\n             |     end_time TIMESTAMP NULL,\n             |     revenue DECIMAL(10,2) NULL,\n             |     CONSTRAINT \"primary\" PRIMARY KEY (city ASC, id ASC),\n             |     CONSTRAINT fk_city_ref_users FOREIGN KEY (city, rider_id) REFERENCES users(city, id),\n             |     CONSTRAINT fk_vehicle_city_ref_vehicles FOREIGN KEY (vehicle_city, vehicle_id) REFERENCES vehicles(city, id),\n             |     INDEX rides_auto_index_fk_city_ref_users (city ASC, rider_id ASC),\n             |     INDEX rides_auto_index_fk_vehicle_city_ref_vehicles (vehicle_city ASC, vehicle_id ASC),\n             |     FAMILY \"primary\" (id, city, vehicle_city, rider_id, vehicle_id, start_address, end_address, start_time, end_time, revenue),\n             |     CONSTRAINT check_vehicle_city_city CHECK (vehicle_city = city)\n             | )\n(1 row) Since we want to show the journey of going from single-region to multi-region, I’ve modified the rides table to remove city from the primary key (note: you could also do this with ALTER TABLE rides ALTER PRIMARY KEY USING COLUMNS (id ASC); if you want to use the existing MovR rides table). To follow along at home, we will create and use a new database and then create a new rides table as follows: CREATE DATABASE movr_blog;\n\nUSE movr_blog;\n\nCREATE TABLE rides (\n\tid\n\t\tUUID NOT NULL,\n\tcity\n\t\tVARCHAR NOT NULL,\n\tvehicle_city\n\t\tVARCHAR NULL,\n\trider_id\n\t\tUUID NULL,\n\tvehicle_id\n\t\tUUID NULL,\n\tstart_address\n\t\tVARCHAR NULL,\n\tend_address\n\t\tVARCHAR NULL,\n\tstart_time\n\t\tTIMESTAMP NULL,\n\tend_time\n\t\tTIMESTAMP NULL,\n\trevenue\n\t\tDECIMAL(10,2) NULL\n); Now, we want to change this primary key so we need to use the following alter table statement: ALTER TABLE rides ALTER PRIMARY KEY USING COLUMNS (city ASC, id ASC);\n\nNOTICE: primary key changes are finalized asynchronously; further schema changes on this table may be restricted until the job completes\nALTER TABLE\n\nTime: 189.061ms Now in this example, we don’t have a load pointed at the cluster or a large amount of data to change so it is a relatively fast operation. As we will see below, online schema changes are designed to have minimal impact on foreground activity by not locking the table out for use by your customers. A bit more involved MovR example Let’s show a more involved example that considers the entire MovR database. We make the entire MovR workload available in this GitHub repository . To set up this example, make sure that you have docker installed and running. brew install docker You'll also need to install CockroachDB 20.1 . brew install cockroachdb Then, start a cockroachdb node on your laptop: ./cockroach start-single-node --insecure --host localhost --background Next, create the MovR database: cockroach sql --insecure --host localhost -e \"create database movr;\" Now, let’s generate some fakedata for the MovR database: docker run -it --rm cockroachdb/movr:movr-20.1-beta.20.4.1 --url \"postgres://root@docker.for.mac.localhost:26257/movr?sslmode=disable\" load --num-users 100 --num-rides 100 --num-vehicles 10 We can open the webui and see what’s going on by visiting the localhost directly in the browser: http://localhost:8080/#/overview/list We can even see what the existing tables look like, for example, the rides table: http://localhost:8080/#/database/movr/table/rides Now let’s point load at it: docker run -it --rm cockroachdb/movr:movr-20.1-beta.20.4.2 --app-name \"movr-loadgen\" --url \"postgres://root@docker.for.mac.localhost:26257/movr?sslmode=disable\" run --multi-region We can also see this load in the webui in the metrics tab: http://localhost:8080/#/metrics/overview/cluster Now, let’s upgrade this to a multi-region cluster. Normally, if you were doing this by hand, you’d need to run the following commands see through Docker: docker run -it --rm cockroachdb/movr:movr-20.1-beta.20.4.1 --app-name \"movr-loadgen\" --url \"postgres://root@docker.for.mac.localhost:26257/movr?sslmode=disable\" configure-multi-region --preview-queries\n\n[INFO] (MainThread) connected to movr database @ postgres://root@docker.for.mac.localhost:26257/movr?sslmode=disable\nDDL to convert a single region database to multi-region\n===primary key alters===\nALTER TABLE users ALTER PRIMARY KEY USING COLUMNS (city, id);\nALTER TABLE rides ALTER PRIMARY KEY USING COLUMNS (city, id);\nALTER TABLE vehicle_location_histories ALTER PRIMARY KEY USING COLUMNS (city, ride_id, timestamp);\nALTER TABLE vehicles ALTER PRIMARY KEY USING COLUMNS (city, id);\nALTER TABLE user_promo_codes ALTER PRIMARY KEY USING COLUMNS (city, user_id, code);\n===foreign key alters===\nDROP INDEX users_city_idx;\nALTER TABLE vehicles DROP CONSTRAINT fk_owner_id_ref_users;\nCREATE INDEX ON vehicles (city, owner_id);\nDROP INDEX vehicles_auto_index_fk_owner_id_ref_users;\nDROP INDEX vehicles_city_idx;\nALTER TABLE vehicles ADD CONSTRAINT fk_owner_id_ref_users_mr FOREIGN KEY (city, owner_id) REFERENCES users (city,id);\nALTER TABLE rides DROP CONSTRAINT fk_rider_id_ref_users;\nCREATE INDEX ON rides (city, rider_id);\nALTER TABLE rides ADD CONSTRAINT fk_rider_id_ref_users_mr FOREIGN KEY (city, rider_id) REFERENCES users (city,id);\nALTER TABLE rides DROP CONSTRAINT fk_vehicle_id_ref_vehicles;\nCREATE INDEX ON rides (city, vehicle_id);\nALTER TABLE rides ADD CONSTRAINT fk_vehicle_id_ref_vehicles_mr FOREIGN KEY (city, vehicle_id) REFERENCES vehicles (city,id);\nDROP INDEX rides_auto_index_fk_rider_id_ref_users;\nDROP INDEX rides_auto_index_fk_vehicle_id_ref_vehicles;\nALTER TABLE user_promo_codes DROP CONSTRAINT fk_user_id_ref_users;\nALTER TABLE user_promo_codes ADD CONSTRAINT fk_user_id_ref_users_mr FOREIGN KEY (city, user_id) REFERENCES users (city,id); We’ve scripted it to complete these commands for you so you can remove the --preview-queries to see this execute: docker run -it --rm cockroachdb/movr:movr-20.1-beta.20.4.1 --app-name \"movr-loadgen\" --url \"postgres://root@docker.for.mac.localhost:26257/movr?sslmode=disable\" configure-multi-region You can see these run in the jobs section of the webui or by running show jobs http://localhost:8080/#/jobs We can also verify that this occurred in the webui or by running show create table rides ; How did this affect the cluster? Back in the metrics table we can see that p99 latency rose only slightly while the schema changes took effect but we had no dropoff in throughput: You can increase the cluster size, increase the workload, and otherwise try out CockroachDB using MovR ! Future-proof your application Looking for other ways CockroachDB makes multi-region deployments easy? We previously covered How to Leverage Geo-partitioning , Reducing Multi-Region Latency with Follower Reads , and most recently How to Build a Multi-Region Application on CockroachDB . CockroachDB provides developers with powerful tools to create multi-region applications. But your application doesn’t need to be designed for multi-region from inception, we also provide you the tools to grow your application from single-region to multi-region. Try CockroachCloud for free for 30 days.", "date": "2020-06-11"},
{"website": "CockroachLabs", "title": "Time and Related Data Types in PostgreSQL", "author": ["Oliver Tan"], "link": "https://www.cockroachlabs.com/blog/time-data-types-postgresql/", "abstract": "We here at Cockroach Labs have been busy getting CockroachDB (CRDB) more in line with PostgreSQL for v20.1 . This allows you to be able to use Cockroach as a drop-in replacement for PostgreSQL (whilst using your favourite existing ORM or driver), but with the powerful, scalable backend we provide. Time , Timestamp and TimestampTZ have been three data types Cockroach has supported, but were not well matched with PostgreSQL functionality. We were also missing the TimeTZ datatype , as well as precision for time and interval types. Even worse, our results did not match PostgreSQL in some cases for the existing data types we do support. We've spent a lot of the v20.1 release mastering time data types, fixing what was broken through ORM tests and community reports whilst adding new features to bridge that gap. A significant amount of time was spent scratching our heads as we figured out why existing time-related ORM tests were failing against CRDB. We've found a lot of devils in the details, with knowledge over these edge cases sparsely located over old SQL mailing lists and StackOverflow. In this blog post, we'll explore some of the intricacies of time in PostgreSQL and look at how we reproduce these features using Go and its ecosystem. We'll share our recommendations for using time along the way. Are you excited to learn about time in the PostgreSQL world? Join us as a companion in our TARDIS ( T ime a nd R elated D ata Types In S QL...blog post) and hop into an adventure into the world of time. Allons-y! Session Time Zones When you open a connection to Postgres/CRDB, you will have a session between you (the client) and the database (the server). The session will contain a time zone, which you can set with the SET TIME ZONE command for both PostgreSQL and CRDB . When connecting using the shell, the PostgreSQL shell (psql) will try to connect and set the session to the local time zone as defined by your computer settings, whereas CRDB will default to UTC. We can observe this with CURRENT_TIMESTAMP , which returns the current timestamp in the default session time zone: otan=# SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------- 2020-03-23 16:57:39.499456-07 (1 row) root@127.0.0.1:61879/defaultdb> SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------------ 2020-03-23 23:57:59.751337+00:00 (1 row) psql shell CRDB shell You can specify your time zone to be a location (which is daylight savings aware), or a UTC offset. This will change the CURRENT_TIMESTAMP to be in the time zone your session is set to: otan=#  SET TIME ZONE 'Australia/Sydney'; SET otan=# SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------- 2020-03-24 11:06:33.338712+11 (1 row) otan=#  SET TIME ZONE '-11'; SET otan=#  select CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------- 2020-03-23 13:47:26.980411-11 (1 row) psql shell root@127.0.0.1:61929/defaultdb> SET TIME ZONE 'Australia/Sydney'; SET root@127.0.0.1:61929/defaultdb> SELECT CURRENT_TIMESTAMP(); CURRENT_TIMESTAMP ------------------------------------ 2020-03-24 11:05:30.756214+11:00 (1 row) root@127.0.0.1:61929/defaultdb> SET TIME ZONE '-11'; SET root@127.0.0.1:62124/defaultdb> SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------------ 2020-03-23 13:47:27.739213-11:00 (1 row) psql shell CRDB shell In the above example, CRDB and psql now output the same data as SET TIME ZONE is explicitly set to the same things. Note your ORM or driver may have different default behaviour than the psql shell or the CRDB shell, but should allow you to set a default time zone. Setting your session time zone will affect some time operations in some fun and exciting ways (which is not necessarily what you want as a developer) which we'll explore soon. POSIX Time Offsets In the above examples, we have used SET TIME ZONE with an integer offset (-11) or a location (Australia/Sydney). However, Postgres also supports the syntax for SET TIME ZONE to have GMT or UTC at the front, e.g. SET TIME ZONE 'UTC+3' . Let's see how it behaves: otan=# SET TIME ZONE 'UTC+3'; SET otan=# SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------- 2020-03-24 01:19:40.947013-03 (1 row) root@127.0.0.1:63094/defaultdb> SET TIME ZONE 'UTC+3'; SET root@127.0.0.1:63094/defaultdb> SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------------ 2020-03-24 01:20:53.696285-03:00 (1 row) psql shell CRDB shell Hold on a second, why does it have a time zone of -3 when setting the time zone to UTC+3 ? It turns out that this is because having UTC or GMT in front uses the POSIX definition for time zones , which defines zones to be hours west of the GMT line. In essence, the sign is reversed from the time zone structure we know and love, where positive integers represent that the timestamp is east of the GMT line (also known as ISO8601 standard). Note that the POSIX standard also applies if you just add colon separators to the offset in SET TIME ZONE: otan=#  SET TIME ZONE '+3:00'; SET otan=# SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ------------------------------- 2020-03-24 19:23:42.539356-03 (1 row) root@127.0.0.1:51387/defaultdb> SET TIME ZONE '+3:00'; SET root@127.0.0.1:51387/defaultdb> SELECT CURRENT_TIMESTAMP; CURRENT_TIMESTAMP ----------------------------------- 2020-03-24 19:23:08.83621-03:00 (1 row) psql shell CRDB shell The takeaway here is that integers are treated as ISO8601 format, locations do what you expect and anything else is POSIX standard. Thinking this is a little weird? We've got another surprise with this coming later! TIMESTAMP and TIMESTAMPTZ The TIMESTAMP (also known as TIMESTAMP WITHOUT TIME ZONE ) and TIMESTAMPTZ (also known as TIMESTAMP WITH TIME ZONE ) types stored as a 64-bit integer as a microsecond offset since 1970-01-01 in CRDB and as a 64-bit integer microsecond offset since 2000-01-01 in PostgreSQL (by default). As both data types are stored using only 64-bit integers, it is important to note that neither store any time zone information.  So where does TIMESTAMPTZ get the time zone from? That's right — the session time zone. As a corollary, that means storing the TIMESTAMPTZ and fetching the results from a different session time zone results in a different printed result, with the same equivalent UTC offset underneath. Let's compare storing a TIMESTAMP / TIMESTAMPTZ and fetching it from a different time zone: otan=# CREATE TABLE time_comparison(t TIMESTAMP, ttz TIMESTAMPTZ); CREATE TABLE otan=# INSERT INTO time_comparison values (CURRENT_TIMESTAMP, CURRENT_TIMESTAMP); INSERT 0 1 otan=# SELECT t FROM time_comparison; t ---------------------------- 2020-05-13 14:05:23.801845 (1 row) otan=# SELECT ttz FROM time_comparison; ttz ------------------------------- 2020-05-13 14:05:23.801845-07 (1 row) otan=# SET TIME ZONE '-3'; SET otan=# SELECT t FROM time_comparison; t ---------------------------- 2020-05-13 14:05:23.801845 (1 row) otan=# SELECT ttz FROM time_comparison; ttz ------------------------------- 2020-05-13 18:05:23.801845-03 (1 row) root@127.0.0.1:56384/defaultdb> CREATE TABLE time_comparison(t TIMESTAMP, ttz TIMESTAMPTZ); CREATE TABLE root@127.0.0.1:56384/defaultdb> INSERT INTO time_comparison values (CURRENT_TIMESTAMP, CURRENT_TIMESTAMP); INSERT 1 root@127.0.0.1:56384/defaultdb> SELECT t FROM time_comparison; t ------------------------------------ 2020-05-13 21:07:00.462552+00:00 (1 row) root@127.0.0.1:56384/defaultdb> SELECT ttz FROM time_comparison; ttz ------------------------------------ 2020-05-13 21:07:00.462552+00:00 (1 row) root@127.0.0.1:56384/defaultdb> SET TIME ZONE '-3'; SET root@127.0.0.1:56384/defaultdb> SELECT t FROM time_comparison; t ------------------------------------ 2020-05-13 21:07:00.462552+00:00 (1 row) root@127.0.0.1:56384/defaultdb> root@127.0.0.1:56384/defaultdb> SELECT ttz FROM time_comparison; ttz ------------------------------------ 2020-05-13 18:07:00.462552-03:00 (1 row) psql shell CRDB shell In the psql shell, the time we've inserted these entries as 2020-05-13 14:05:23.801845 with offset -07 . The TIMESTAMP column does not regard the time zone, so drops that information. When we switched time zones to UTC-3 , only the ttz column will change its result by transforming the time into the +3 offset, which is 10 hours ahead, hence now displaying 2020-05-13 18:05:23.801845-03 . In the CRDB shell, things look largely the same. However, the TIMESTAMP column has an extraneous +00:00 at the front. This is a UX quirk of using the Go driver lib/pq (which is not related to the C library libpq ) that powers the CRDB shell — it always displays a time zone when representing any time object since it uses the same format string for all time related types. We're looking to fix this UX issue in a future release — but have comfort that it represents the same value underneath. If you're still confused, don't worry — it's confusing to us too. Here is a summary: TIMESTAMP is an absolute value time offset. It does not store time zones, or change based on the session time zone. Another way to think about it is that it is always in UTC. TIMESTAMPTZ is also an absolute value time offset with no time zone metadata set, but it displays timestamps and performs operations in the session time zone. You've seen the \"displays timestamp\" bit already — we'll get to the \"performs operations\" component later. We'll omit CRDB shell output for the remainder of this section to avoid duplicate information — but it is worth noting we've had bugs in all aspects in areas below which is how we got to explaining it. Parsing Let's parse the opening ceremony of the Sydney Olympics in PostgreSQL with a fresh new psql shell in California: otan=# SELECT '2000-09-15 19:00'::TIMESTAMP, '2000-09-15 19:00'::TIMESTAMPTZ; timestamp      |      timestamptz ---------------------+------------------------ 2000-09-15 19:00:00 | 2000-09-15 19:00:00-07 (1 row) psql shell So parsing a timestamp string without any time zone information in the string will automatically default to the current time zone for TIMESTAMPTZ. Let's append Sydney's time zone in September (+11:00) into the string before we cast: otan=# SELECT '2000-09-15 19:00+11:00'::TIMESTAMP, '2000-09-15 19:00+11:00'::TIMESTAMPTZ; timestamp      |      timestamptz ---------------------+------------------------ 2000-09-15 19:00:00 | 2000-09-15 01:00:00-07 (1 row) psql shell As we see above: For TIMESTAMPs, we've got 19:00 — same as the input but we've ignored the time zone offset. This can be significant for us as 19:00-07 is different to the value it is stored as — 19:00+00. For TIMESTAMPTZ, we've got 01:00, which looks way off. Remember — TIMESTAMPTZ is a UTC offset, which displays in the session time zone. We've parsed TIMESTAMPTZ with +11:00 (which is parsed as ISO8601 format unlike SET TIME ZONE ) but we are displaying TIMESTAMPTZ in the time zone -07:00 as we are in California. With an 18 hour time difference, that places the opening ceremony at 1am local time in California, as you can see from the output. American readers, youse must have been tired watching the Olympics in 2000. Of course, explicitly changing the session time zone with the same strings will change the output for TIMESTAMPTZ: otan=# SET TIME ZONE 'Asia/Tokyo'; SET otan=# SELECT '2000-09-15 19:00+11:00'::TIMESTAMP, '2000-09-15 19:00+11:00'::TIMESTAMPTZ; timestamp      |      timestamptz ---------------------+------------------------ 2000-09-15 19:00:00 | 2000-09-15 17:00:00+09 (1 row) psql shell Again, the time zone when parsing TIMESTAMP is completely ignored, but for Tokyo's time zone of +09:00, we've had to rewind the clock from 19:00 to 17:00 to display correctly in the session time zone. Casting Let's look at a few casts between TIMESTAMP and TIMESTAMPTZ with the time one of the greatest cricket test match finishes of all time : otan=# SET TIME ZONE 'Australia/Adelaide'; SET otan=# SELECT '2006-12-05 17:00'::TIMESTAMP::TIMESTAMPTZ; -- case 1: timestamp -> timestamptz timestamptz --------------------------- 2006-12-05 17:00:00+10:30 (1 row) otan=# SELECT '2006-12-05 17:00'::TIMESTAMPTZ::TIMESTAMP; -- case 2: timestamptz -> timestamp timestamp --------------------- 2006-12-05 17:00:00 (1 row) psql shell When we cast from a TIMESTAMP to a TIMESTAMPTZ in Case 1, we have to set the time zone of +10:30. However, this actually changes the UTC time, as \"displaying it correctly\" in the current session time zone involves subtracting 10:30 from the underlying offset value. Conversely, when we convert from TIMESTAMPTZ to TIMESTAMP in Case 2, TIMESTAMP represents an absolute UTC value but without the time zone information. As such, we would have to add 10:30 to the underlying offset value for the correct equivalent value to make this equal to the value 17:00. This behaviour gets tricky, especially when the data is stored and fetched in a session with another session time zone whilst expecting the cast to TIMESTAMP to give you the same result: otan=# SET TIME ZONE 'Australia/Adelaide'; SET otan=# CREATE TEMP TABLE timestamptz_table (val timestamptz); CREATE TABLE otan # SET TIME ZONE +10:30; otan=# INSERT INTO timestamptz_table values ('2006-12-05 17:00'::TIMESTAMP); INSERT 0 1 otan=# SELECT * from timestamptz_table; val --------------------------- 2006-12-05 17:00:00+10:30 (1 row) otan=# SELECT val::TIMESTAMP from timestamptz_table; val --------------------- 2006-12-05 17:00:00 (1 row) otan=# SET TIME ZONE 'America/Chicago'; SET otan=# SELECT val::TIMESTAMP from timestamptz_table; val --------------------- 2006-12-05 00:30:00 (1 row) psql shell In the above example, we changed the time zone above from +10:30 to Chicago's -06:00, so we have a 16:30 time difference. Since we cast to TIMESTAMP using the TIMESTAMPTZ data type, we evaluate it as of the session time zone at the point of evaluation, hence getting 2006-12-05 00:30:00 when casting it in America/Chicago. Note if you want a result to always evaluate to the same TIMESTAMP no matter the session time zone, use the AT TIME ZONE syntax discussed below. For many of these operations, we need to move timestamps to a different time zone but whilst keeping the same timestamp underneath. Go does not natively do this, as time.In only changes the location, but has the same UTC offset underneath. This means that something at 10:00 cast to a +3 time zone  without any offset changes would report as 13:00+3 instead of 10:00+3. This was a source of quite a few of our time bugs! To do this correctly, we have to do an awkward dance: read the second argument from Zone to get the zone offset in seconds from the timezones before and after and then use time.Add to subtract that duration offset to the new time value. Example code: func KeepTimeOffsetInNewZone(t time.Time, loc *time.Location) time.Time {\n\tafterTime := t.In(loc)\n\t_, beforeOffsetSecs := t.Zone()\n\t_, afterOffsetSecs := afterTime.Zone()\n\ttimeDifference := afterOffsetSecs — beforeOffsetSecs\n\treturn afterTime.Add(-time.Duration(timeDifference) * time.Second)\n} Precision TIMESTAMP and TIMESTAMPTZ supports microsecond precision. However, there is an option for \"rounding\" of fractional digits for the seconds component. To do this, we can specify a number between 0 and 6 inclusive in parenthesis after TIMESTAMP and TIMESTAMPTZ, e.g. TIMESTAMP(3) for TIMESTAMP in millisecond precision allowing 3 fractional digits, or TIMESTAMPTZ(0) for TIMESTAMPTZ with no fractional digits. This will get the datums rounded to the specified precision. Let's look at examples using the time when Super Over rules were unfair : otan=# SELECT '2019-07-14 17:00:00.545454'::TIMESTAMP(0); -- rounded up timestamp --------------------- 2019-07-14 17:00:01 (1 row) otan=# SELECT '2019-07-14 17:00:00.545454'::TIMESTAMP(1); -- rounded down timestamp ----------------------- 2019-07-14 17:00:00.5 (1 row) otan=# SELECT '2019-07-14 17:00:00.545454'::TIMESTAMP(3); -- rounded up timestamp ------------------------- 2019-07-14 17:00:00.545 (1 row) otan=# SELECT '2019-07-14 17:00:00.545454'::TIMESTAMP(6); -- maximum precision and is default timestamp ---------------------------- 2019-07-14 17:00:00.545454 (1 row) psql shell The equivalent functionality in Go is available using time.Round in the time library. Functions and Operators Functions (e.g. extract , date_trunc ) and Operators (e.g. = , + , - , > ) are fairly easy to comprehend with TIMESTAMP. However they are a little nuanced with TIMESTAMPTZ, causing a variety of bugs internally in CRDB. As we alluded to earlier, it's important to remember that TIMESTAMPTZ performs operations in the session time zone . Let's look at some time operators, with timestamps around the release of the movie Crocodile Dundee : otan=# SET TIME ZONE 'America/New_York'; SET otan=# SELECT '1986-09-26 10:00'::TIMESTAMP = '1986-09-26 10:00-04'::TIMESTAMPTZ; -- Case 1 ?column? ---------- t (1 row) otan=# SELECT '1986-09-26 10:00'::TIMESTAMP = '1986-09-26 09:00-05'::TIMESTAMPTZ; -- Case 2 ?column? ---------- t (1 row) otan=# SELECT '1986-09-26 10:00'::TIMESTAMP < '1986-09-26 10:00-05'::TIMESTAMPTZ; -- Case 3 ?column? ---------- t (1 row) root@127.0.0.1:64900/defaultdb> SELECT '1986-09-26 10:00'::TIMESTAMPTZ + '1 day'::interval; -- Case 4 ?column? ----------------------------- 1986-09-27 10:00:00+10:00 (1 row) psql shell Remember — in all cases, the TIMESTAMP is converted to the TIMESTAMPTZ of the current session time zone, which is '1986-09-26 10:00-04' in New York. Taking a look at each case: Case 1: the added -04 for TIMESTAMP when converting to TIMESTAMPTZ means this result is true. Case 2: they are both internally the same time offset, and hence they are equal. Case 3: 10:00-05 can be thought of as 11:00-04, which is strictly higher, hence returning true. Case 4: adding an interval of 1 day changes the date to be incremented by 1. For builtins, let's look at extract : otan=# SET TIME ZONE 'America/New_York'; SET otan=# SELECT extract('hour' from '1986-09-26 10:00'::TIMESTAMP); -- Case 1 date_part ----------- 10 (1 row) otan=# SELECT extract('hour' from '1986-09-26 10:00-04'::TIMESTAMPTZ); -- Case 2 date_part ----------- 10 (1 row) otan=# SELECT extract('hour' from '1986-09-26 10:00-06'::TIMESTAMPTZ); -- Case 3 date_part ----------- 12 (1 row) psql shell From the above: Case 1: The hour can be extracted as \"10\" directly. Case 2: As the timestamp provided is in the same time zone as the session time zone, the calculation does not need any time zone conversions, but the underlying UTC offset has a different hour. Thankfully, Go's time.Hour operator (and Minute, Second, Month, etc.) takes into account what location the time is in, so extracting the hour is straightforward. Case 3: What happened here? Remember — TIMESTAMPTZ performs operations in the session time zone. If we were to run SELECT '1986-09-26 10:00-06'::TIMESTAMPTZ , we'd see 1986-09-26 12:00-04 as -04 is the session time zone.  As such, when moving it to from -02 to -04, the time becomes 12:00 — and since we perform the operation in the session time zone — extract will hence return 12. AT TIME ZONE AT TIME ZONE will convert a TIMESTAMPTZ to a TIMESTAMP at a given time zone, or a TIMESTAMP to a TIMESTAMPTZ at a given time zone (which will be transposed into the session time zone). It is worth noting that TIMESTAMP AT TIME ZONE <zone> and TIMESTAMPTZ AT TIME ZONE <zone> are inverses of each other. This can be useful if you expect users to cast from TIMESTAMPTZ to TIMESTAMP or vice versa from different session time zones but you need consistent offsets from the epoch time (see \"Casting\" for an example for how this could be a problem). Confusing? Let's look at the real examples using the release date of the song Friday : otan=# SET TIME ZONE 'Australia/Sydney'; SET otan=# SELECT '2011-03-14 10:00:00'::TIMESTAMPTZ AT TIME ZONE 'Asia/Tokyo'; -- Case 1 timezone --------------------- 2011-03-14 08:00:00 (1 row) otan=# SELECT '2011-03-14 10:00:00'::TIMESTAMP AT TIME ZONE 'Australia/Sydney'; -- Case 2 timezone --------------------- 2011-03-14 10:00:00+11 (1 row) otan=# SELECT '2011-03-14 10:00:00'::TIMESTAMP AT TIME ZONE 'Asia/Tokyo'; -- Case 3 timezone ------------------------ 2011-03-14 12:00:00+11 (1 row) psql shell From the above: Case 1: we are switching from Australia/Sydney time to Asia/Tokyo time, which is 2 hours behind. This involves moving from 10am to 8am, but underneath removing the UTC time zone offset to the absolute \"TIMESTAMP\" value of 8am. Case 2: we have a TIMESTAMP which we wish to convert to Sydney time. This is straightforward -- add the UTC offset to the time, which when we display in the session time zone of 'Australia/Sydney' will still be 10am. Case 3: Case 3 gets interesting. We've added the Asia/Tokyo time zone offset to the underlying offset, but remember we display this offset at the session time zone. With the two hour time difference, that means we see 12pm in the afternoon when displaying this operation with a session time zone of Australia/Sydney. POSIX Standard strikes again Remember the surprise earlier with the POSIX standard being used for strings when using SET TIME ZONE? In AT TIME ZONE, omitting the UTC/GMT prefix and having just a bare integer offset (e.g. +3 , -3 , 3 ) also behaves as POSIX timestamp (unlike SET TIME ZONE where integers were special and behave as ISO8601): otan=# SET TIME ZONE '+3'; SET otan=#  select '2011-03-14 10:00:00'::TIMESTAMP AT TIME ZONE '+3'; timezone ------------------------ 2011-03-14 16:00:00+03 (1 row) psql shell We would expect the above case to be 2011-03-14 10:00:00+03:00 if it were ISO8601 when using AT TIME ZONE. However, as +3 is POSIX for AT TIME ZONE, it really means \"at time zone 3 hours west of GMT\", to be displayed as \"3 hours east of UTC\", hence adding 6 hours to the result. Daylight Savings Now you may be wondering when to use a location versus when to use an absolute offset. Offsets already seem tricky given how it uses POSIX style offsets. This may help you decide — locations can infer changing time zone information. Let's look at a dates which traverse time zones in Chicago: root@127.0.0.1:64900/defaultdb> SET TIME ZONE 'America/Chicago'; SET root@127.0.0.1:64900/defaultdb> SELECT '2010-11-06 23:59:00'::TIMESTAMPTZ; timestamptz ----------------------------- 2010-11-06 23:59:00-05:00 (1 row) root@127.0.0.1:64900/defaultdb> SELECT '2010-11-07 23:59:00'::TIMESTAMPTZ; timestamptz ----------------------------- 2010-11-07 23:59:00-06:00 (1 row) psql shell With the daylight savings boundary change, we can see that the time zone offset changes. That's neat, isn't it? You may now be wondering — how are these time zone changes encoded? IANA maintains a database of daylight savings changes for each time zone (some of which date back a very long time). But which version of this database do we use? Well: CRDB will use a copy of this database that is installed within your computer. This is the default behaviour of Go. PostgreSQL ships out its own copy of tzdata every release. This means that time and daylight savings behaviours can change between computers when using CRDB if a newer version of the IANA database is on your system. This is currently tracked for a fix , and is one of the reasons we recommend always using UTC as your session time zone. Interval Math with Daylight Savings Let's look at how daylight savings impacts interval math: otan=# SET TIME ZONE 'America/Chicago'; SET otan=# SELECT '2010-11-06 23:59:00'::TIMESTAMPTZ + '24 hours'::interval; -- case 1 ?column? ------------------------ 2010-11-07 22:59:00-06 (1 row) otan=# SELECT '2010-11-06 23:59:00'::TIMESTAMPTZ + '1 day'::interval; -- case 2 ?column? ------------------------ 2010-11-07 23:59:00-06 (1 row) otan=# SELECT '2010-11-06 23:59:00'::TIMESTAMPTZ + '1 month'::interval; -- case 3 ?column? ------------------------ 2010-12-06 23:59:00-06 (1 row) psql shell Huh — are there not 24 hours in a day? Intervals in Postgres are represented as \"months\", \"days\" and \"seconds\", which plays a role in what we see here. Let's see how this applies to the cases above: Case 1: When adding \"seconds\" to TIMESTAMPs (which is represented by units that are not days, i.e. 24 hours still uses seconds until it overflows), we add real world seconds. This is done in Go by using time.Add . Case 2: When adding \"days\", we're just putting any math straight into the date fields, preserving the same time even as we cross daylight savings barriers. This is handled in Go by time.AddDate . Case 3: Similar to case 2, but we add months instead of days. The Y2K38 Problem Does 2038-01-19 03:14:07 spark any Y2K vibes? This is the time is 2147483647 (max int32) seconds after the unix offset of 1970-01-01, known as the Y2K38 date. This was an issue in Go when handling tzdata past 2038 which has only just been resolved. As Go 1.13 does not understand the \"extended\" format of tzdata which handles time zones past Y2K38, our handling of daylight savings is broken after the Y2K38 date in v20.1 (which ships with Go 1.13). We cannot easily parse it ourselves either, as the Go Location struct is not an interface, and the relevant variables to change are private and inaccessible without forking Go's time library. As such, if we pass Y2K38, the time zone in CRDB will be the same as the daylight savings in your current time zone, as that is what it is evaluated to in Go. This can be a problem if using a session time zone that has daylight savings: otan=# SET TIME ZONE 'America/Chicago'; SET otan=#  select '2037-09-06 04:15:30.746999-06:00'::TIMESTAMPTZ; timestamptz ------------------------------- 2037-09-06 05:15:30.746999-05 (1 row) otan=# SELECT '2038-09-06 04:15:30.746999-06:00'::TIMESTAMPTZ; timestamptz ------------------------------- 2038-09-06 05:15:30.746999-05 (1 row) root@127.0.0.1:57882/defaultdb> SET TIME ZONE 'America/Chicago'; SET root@127.0.0.1:57882/defaultdb> SELECT '2037-09-06 04:15:30.746999-06:00'::TIMESTAMPTZ; timestamptz ------------------------------------ 2037-09-06 05:15:30.746999-05:00 (1 row) root@127.0.0.1:57882/defaultdb> SELECT '2038-09-06 04:15:30.746999-06:00'::TIMESTAMPTZ; -- this should be time zone -05 timestamptz ------------------------------------ 2038-09-06 04:15:30.746999-06:00 (1 row) psql shell CRDB shell This is tracked for a future fix . As a side note — if you're curious about tzdata, it makes for an interesting read. Check out some of the weird offsets such as America/Chicago pre-1900 with a time offset of -5:50:36 . Time Twister Feeling confident about time? Feeling you may be turning half human, half time lord? Let's see — can you explain the following behaviour: otan=#  SET TIME ZONE '-9'; SET otan=# SELECT '1947-12-13 13:00+11'::TIMESTAMPTZ AT TIME ZONE 'UTC+3'; timezone --------------------- 1947-12-12 23:00:00 (1 row) psql shell The answer is near the end of this blog post. What do we recommend? Like many others, CRDB recommends usage of TIMESTAMPTZ as encoding time zone data is valuable. However, we recommend always setting the session time zone to UTC. This allows the user to not worry about not losing time zone information whilst parsing, whilst allaying concerns that if a user decides to use session time zones that they perform with intended daylight-savings aware behaviour. TIME and TIMETZ TIME (also known as TIME WITHOUT TIME ZONE) and TIMETZ (also known as TIME WITH TIME ZONE) both only store the time of day component of a TIMESTAMP. But: TIME is still encoded with 8 bytes, representing microseconds since midnight. TIMETZ is encoded with 12 bytes, with 8 bytes representing microseconds since midnight and 4 bytes for storing the time zone offset in seconds west of UTC (again, the opposite of what we're used to when talking time zones). Unlike TIMESTAMPTZ, the current session time zone is not taken into account (except for parsing) and since it only stores time zone offsets and not locations, it does not encode daylight savings information. Let's look at using CURRENT_TIME (the equivalent of CURRENT_TIMESTAMP) in psql (with California time as default) and CRDB: otan=# CREATE TABLE timetz_example (t time, ttz timetz); CREATE TABLE otan=# INSERT INTO timetz_example VALUES (CURRENT_TIME, CURRENT_TIME); INSERT 0 1 otan=# SELECT t from timetz_example; t ----------------- 14:32:36.681805 (1 row) otan=# SELECT ttz from timetz_example; ttz -------------------- 14:32:36.681805-07 (1 row) otan=# SET TIME ZONE 'Australia/Sydney'; SET otan=# select t from timetz_example; t ----------------- 14:32:36.681805 (1 row) otan=# select ttz from timetz_example; ttz -------------------- 14:32:36.681805-07 (1 row) root@127.0.0.1:56478/defaultdb> CREATE TABLE timetz_example (t time, ttz timetz); CREATE TABLE root@127.0.0.1:56478/defaultdb> INSERT INTO timetz_example VALUES (CURRENT_TIME, CURRENT_TIME); INSERT 1 root@127.0.0.1:56478/defaultdb> SELECT t from timetz_example; t ------------------------------------ 0000-01-01 21:34:05.393742+00:00 (1 row) root@127.0.0.1:56478/defaultdb> SELECT ttz from timetz_example; ttz ------------------------------------ 0000-01-01 21:34:05.393742+00:00 (1 row) root@127.0.0.1:56478/defaultdb> SET TIME ZONE 'Australia/Sydney'; SET root@127.0.0.1:56478/defaultdb> SELECT t from timetz_example; t ------------------------------------ 0000-01-01 21:34:05.393742+00:00 (1 row) root@127.0.0.1:56478/defaultdb> SELECT ttz from timetz_example; ttz ------------------------------------ 0000-01-01 21:34:05.393742+00:00 (1 row) psql shell CRDB shell As you can see, changing the time zone does not affect table results. Since TimeTZ stores the offset, they stay the same between session time zones shifts. It is worth noting here that CRDB outputs an extra \"0000-01-01\" for time types, as well as an extraneous \"+00:00\" for the time type. This data has no meaning and is the way our driver lib/pq for the CRDB shell displays this data. This is tracked for a future fix. When performing interval math with time, times past 23:59:59.999999, automatically overflows back to 00:00:00 as there is no \"date\" component. otan=# select '10:00'::time + '14 hours'::interval; ?column? ---------- 00:00:00 (1 row) otan=# select '10:00+03'::timetz + '14 hours'::interval; ?column? ------------- 00:00:00+03 (1 row) psql shell Parsing Parsing TIME and TIMETZ largely behaves the same as the parsing for TIMESTAMP and TIMESTAMPTZ: With TIME, any time zone offsets are ignored. With TIMETZ, the current session time zone appended to TIMETZ if none is specified (which changes based on daylight savings).  However, If a time zone offset is specified for TIMETZ, it will use that instead. The time zone offsets use the familiar ISO8601 standard. otan=# SET TIME ZONE 'Australia/Sydney'; SET otan=# SELECT '07:00'::time, '07:00'::timetz, '07:00-03'::time, '07:00-03'::timetz; time   |   timetz    |   time   |   timetz ----------+-------------+----------+------------- 07:00:00 | 07:00:00+11 | 07:00:00 | 07:00:00-03 (1 row) psql shell Precision Similar to TIMESTAMP/TIMESTAMPTZ, you can specify precision in parenthesis for TIME/TIMETZ types, which round to specified precision of fractional digits for the seconds component: otan=#  select '17:00:00.545454'::time(0), '17:00:00.545454+03'::timetz(0); -- rounded up time   |   timetz ----------+------------- 17:00:01 | 17:00:01+03 (1 row) otan=#  select '17:00:00.545454'::time(1), '17:00:00.545454+03'::timetz(1); -- rounded down time    |    timetz ------------+--------------- 17:00:00.5 | 17:00:00.5+03 (1 row) psql shell Casting When casting TIME to TIMETZ, the TIME will get promoted to the time zone of your current session. However, when casting TIMETZ to TIME, we will lose time zone offset: otan=# SET TIME ZONE 'Australia/Sydney'; SET otan=# SELECT '10:00'::time::timetz, '10:00+03'::timetz::time; timetz    |   time -------------+---------- 10:00:00+11 | 10:00:00 (1 row) psql shell Losing the time zone offset and reinterpreting it in the session time zone can be surprising, as casting that TIME back to TIMETZ will give you a different result. In other words, casting the inverse of the inverse does not yield the identity. You can see an example of this below: otan=# SET TIME ZONE 'Australia/Sydney'; SET otan=# SELECT '10:00+03'::timetz::time::timetz; timetz ------------- 10:00:00+11 (1 row) psql shell Here, our +03 in our TIMETZ has been reinterpreted in the session time zone of +11 after casting it to TIME, which represents a wholly different result. If you need inverses to match, AT TIME ZONE between TIME and TIMETZ will yield the desired effects. Comparators and Ordering of TIMETZ Consider the comparison of these two equivalent times (10:00+03 and 11:00+04), one in the same time zone and one in a different time zone: otan=# SELECT '10:00+03'::timetz = '10:00+03'::timetz; -- Case 1 ?column? ---------- t (1 row) otan=# SELECT '10:00+03'::timetz = '11:00+04'::timetz; -- Case 2 ?column? ---------- f (1 row) otan=# SELECT '10:00+03'::timetz > '11:00+04'::timetz; -- Case 3 ?column? ---------- t (1 row) otan=# SELECT '10:00+03:00'::timetz < '11:01+04:00'::timetz; -- Case 4 ?column? ---------- t (1 row) psql shell That's interesting — they're the same time in the real world, aren't they? Well not in the realm of TimeTZ. Recall that TimeTZ stores both microsecond offset AND offset representing seconds west of UTC. If the microsecond offset is the same, then we compare seconds WEST of UTC (i.e. the negative of the offset we see above). As such: Case 1 demonstrates that the same UTC offset AND time zone offset being equal means the result is equal, as expected. Case 2 demonstrates that despite having the same UTC offset, the results are NOT equal since the offset TIME ZONE is not the same Case 3 demonstrates that timezones \"more west\" (has a higher POSIX offset) have precedence. Case 4 demonstrates that UTC offsets take precedence, since 11:01+04 is one minute higher relative to UTC compared to 10:00+03. 24:00 in TIME/TIMETZ An interesting feature that is only supported for TIME and TIMETZ is 24:00:00 time. This is a time that can be parsed, but you cannot use arithmetic to achieve the value. Adding to 24:00:00 overflows the value back to 00:00:00. otan=# SELECT '24:00'::time; -- 24:00 time can be parsed as such time ---------- 24:00:00 (1 row) otan=# SELECT '23:59'::time + '1 minute'::interval; -- but trying to reach 24:00 via arithmetic overflows to 00:00. ?column? ---------- 00:00:00 (1 row) otan=# SELECT '24:00'::time + '1 second'::interval; — and adding to 24:00 time will overflow it to 00:00. ?column? ---------- 00:00:01 (1 row) otan=#  SELECT '24:00'::time + '0 second'::interval; -- even adding 0 seconds will overflow. ?column? ---------- 00:00:00 (1 row) psql shell Unfortunately, Go's time.Parse does not handle 24:00 — so our TIME string parsers all have wrappers to regex match and handle the 24:00 case. Parsing and displaying 24:00 time with the Go time library is also a challenge, as can be demonstrated in the lib/pq driver. Unfortunately, the lib/pq driver we use displays the 24:00 with the exact same representation as 00:00: root@127.0.0.1:57021/defaultdb> SELECT '24:00'::time, '00:00'::time; time            |           time ----------------------------+---------------------------- 0000-01-01 00:00:00+00:00 | 0000-01-01 00:00:00+00:00 (1 row) psql shell We have submitted a PR as a fix , but until then, 24:00 time is broken in the lib/pq driver, even against PostgreSQL. What do we recommend? Whilst TIME and TIMETZ store only the time component, TIME takes the same amount of space — and even more for TimeTZ. Furthermore, TIMETZ also does not keep track of location with time offsets. 24:00 time is an interesting case that is handled but may have limited practical uses. It is worth also noting that PostgreSQL advises against using the TIMETZ data type. TIMETZ was originally implemented to follow the SQL standard. See the note just above section 8.1 in PostgreSQL's own documentation . So our recommendation is to use … neither! If you need time, you are most likely better off using TIMESTAMPTZ which takes care of these shortfalls in all these cases. If time zone information is required, use a separate column to encode that information. Time Twister — Answer The session time zone is set at -9. This means 1947-12-13 13:00+11 would be translated to 1947-12-12 17:00:00-09 . Now UTC+3 is POSIX standard, meaning it really means 3 hours west of UTC (\"-3\" in ISO8601). This is six hours ahead of the session time zone of \"-9\", hence translating to 23:00:00. Since AT TIME ZONE translates a TIMESTAMPTZ to a TIMESTAMP, the time zone data is gone. (And if you are curious, 1947-12-13 is the date of the infamous Mankad incident.) Conclusion Each of the time types have interesting nuances: TIMESTAMP is a type with date and time that has no time zone information and is absolute. It disregards time zones when parsing, which can be unexpected. Alternatively, think of it as \"always UTC time\". TIMESTAMPTZ is a type with date and time that has time zone behaviour dependent on your session time zone and is conscious of daylight savings. If you want to be safe, always use UTC with it. TIME is a type with time of day info only. TIMETZ is a type with time of day and fixed time zone offsets. It does not depend on the session time zone for computation, and is not daylight savings aware. As mentioned above, our advice is to always use TIMESTAMPTZ with your session time zone set to UTC. If you need time zone information, use a separate column to store this information. At the end of the day, make sure what you pick works for you. Whew, that was confusing. Maybe I should write a strongly worded letter. In any case, working with time data types is indeed interesting — and we haven't even touched intervals and leap seconds ! Did you enjoy our adventures and deep dive into the internals and nuances of databases? As always, we're hiring!", "date": "2020-05-13"},
{"website": "CockroachLabs", "title": "How Distributed SQL Databases Solve Scale in the Healthcare Industry", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/healthcare-case-study/", "abstract": "Healthcare industry companies count on their database solutions to protect sensitive data and to deliver consistent performance. They also need the database to streamline communication between a complicated backend full of disparate systems. For most of the last 30 years this meant relying on Oracle because Oracle delivers the consistency required in an industry where lives are at stake and data must always be correct. The rise of cloud-native, distributed SQL database technology that offers a high level of consistency and horizontal scale is giving the healthcare industry a more flexible option. One healthcare company's migration from Oracle to distributed SQL Oracle delivers the transactional consistency needed to ensure that patients receive optimal outcomes from healthcare providers. But Oracle does not scale horizontally. Until recently, this wasn’t worth complaining about because there weren’t any distributed SQL (or NewSQL) databases that could guarantee a high level of consistency. And using something scalable like a NoSQL database was not an option because eventual consistency is out of the question. For the past two years, one forward-thinking healthcare provider has been migrating workloads off of Oracle and onto CockroachDB. While testing CockroachDB, they realized that the distributed SQL database shares a lot of qualities with Oracle: it speaks SQL, it delivers a high level of consistency, and it’s compatible with Postgres. The primary difference is that CockroachDB delivers these qualities in a cloud-native distributed environment. What this means for the healthcare industry is that there is now a proven database solution that can meet the consistency requirements, and can support the complicated backend systems that all need to talk to one another, while also giving healthcare companies the freedom to scale with ease. They can begin a migration to the cloud. They can easily expand services into new regions. Most importantly, they can have the flexibility to control when they scale, without having to worry about the burden of manual sharding. Read about how one innovative healthcare provider is moving workloads from Oracle and Redis over to CockroachDB .", "date": "2020-06-29"},
{"website": "CockroachLabs", "title": "Why TuneGO Chose CockroachDB over PostgreSQL", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/tunego-customer/", "abstract": "Every startup has a choice: Begin building with something familiar (Postgres, MySQL…) or begin building with something new (CockroachDB). Using a familiar database means engineers can hit the ground running and begin building on a trustworthy platform. Using a new database will require research, testing, and a period of time for acclimation. On the surface this looks like a simple decision: start building faster with a familiar database. Peel the onion back one more layer and it becomes clear why so many growth-stage companies are deciding to start with CockroachDB rather than Postgres or MySQL. Start with Scale in Mind Engineers at nearly every startup have experience with Postgres or MySQL. This means they’re familiar with the difficulty of scaling a legacy database. A database that doesn’t scale easily is a problem for growth-stage companies whose goals often include becoming the next Uber, the next Snapchat, or the next Airbnb. The choice that startups have to make is whether or not they want to build for scale and resilience from the beginning, or if they want to endure a migration on the road to becoming the next Uber. Theoretically, they could stay on PostgreSQL and try to scale it but the operational complexity of sharding is difficult and will become less and less tolerable with the proliferation of databases that shard automatically. Horizontal scale is what drove a lot of adoption for NoSQL databases. Now it’s doing the same for Distributed SQL databases like CockroachDB that can offer the same horizontal scale as a NoSQL database but also deliver transactional consistency and, well, SQL . Postgres Compatibility: The Unfamiliar is Familiar The familiarity of Postgres and MySQL is often too tempting for startups that want to start building fast. They’re willing to build on a legacy database knowing that they’ll eventually need to migrate to something more scalable. What these startups may not realize is that CockroachDB utilizes the Postgres wire protocol and is compatible with developer-friendly tools like jOOQ , Django , and others that allow startups to build fast despite being on a new database. This familiarity is pleasing to developers who are relieved to be able to speak SQL while also getting to explore the unique compatibilities CockroachDB has with Kubernetes and other cloud-native services. To learn more about why startups are choosing CockroachDB, read the case study about TuneGO , a music industry startup that chose CockroachDB for scale, resilience, and Kubernetes compatibility.", "date": "2020-07-07"},
{"website": "CockroachLabs", "title": "Disk Spilling in a Vectorized Execution Engine", "author": ["Alfonso Subiotto Marques"], "link": "https://www.cockroachlabs.com/blog/disk-spilling-vectorized/", "abstract": "Late last year, we shipped v1 of our vectorized execution engine . It enables column-based query execution and speeds up complex joins and aggregations, improving analytical capabilities in CockroachDB (which is first and foremost optimized for OLTP workloads). v1 of the engine didn’t support disk spilling, which meant it couldn’t execute certain memory-intensive queries if there was not enough memory available. Starting in CockroachDB v20.1 , these queries fall back to disk (also known as “spilling” to disk). In this post, we provide a top-down explanation of how we added disk spilling to the vectorized execution engine, starting with a description of on-disk algorithms for different types of queries, and ending with a description of the single building block that all of these algorithms use. Note that disk spilling is an existing feature of the default row-at-a-time execution engine. This post specifically covers the recent addition of disk spilling to the column-at-a-time vectorized execution engine. To learn more about why and how we created the vectorized engine, see our How We Built a Vectorized Execution Engine blog post from October 2019. Disk Spilling Operators Sorts Let’s start by covering sorts and their memory usage. A sort operator is planned when a query with the ORDER BY keyword is issued. As input, the operator takes a set of tuples, in any order, and a list of column indices to order by. It then outputs the tuples, ordered by the list of column indices. Note that the operator must buffer the entire input before emitting a tuple, as the tuple that sorts first could be at the end of the input. As a result, the size of the input that must be buffered can exceed the amount of memory an operator is allowed to use. This memory, also called “work” memory, is limited to 64MB in CockroachDB, by default. When the work memory limit is reached, a sort must be able to spill to disk in order to sort the input fully. To solve this problem, we took a divide-and-conquer approach, employing an external merge-sort algorithm that uses disk memory when inputs cannot be fully buffered in memory. The algorithm is broken down into two stages: sorting and merging. In the sorting stage, the operator buffers as much input data as it can in memory (the aforementioned “work” memory), performs a column-by-column, in-memory sort, and then writes the sorted partition to disk. This stage is repeated until there are no more tuples to process. Once the sorting stage is over, there will be N sorted partitions on disk. These partitions are then merged to emit the sorted output. GRACE Hash Join A hash join is a type of join algorithm that joins two input streams based on a set of equality columns. It uses a hash table to store the smaller of the streams, and then probes the table with the larger stream. For example, suppose a user were to issue SELECT * FROM customers, orders WHERE orders.cust_id = customers.id to get a result where each row contains customer data and an order they issued. During the execution of this query, the hash join operator builds an in-memory hash table of the customers table (it is the smaller one), where the key is the customer ID. It then performs lookups using the orders table, and emits the results. The memory usage in this example grows as the customers table grows because the entire table needs to be stored in-memory. In order to respect the 64MB limit on work memory, hash joins also use a divide-and-conquer approach when spilling to disk. This type of hash join is known as a GRACE hash join. In a GRACE hash join, all tuples in both the orders and customers tables can be assigned to one of N on-disk partitions by hashing each tuple based on the customer ID. Because of this, all order and customer tuples with the same customer ID will end up in the same partition. Partitions can then be read from disk and joined using the original in-memory algorithm to produce the same output. This divides the original problem into N subproblems. Note that a GRACE hash join only works if the size of a single partition does not exceed the operator’s work memory, since the partitions must be read fully into memory. To work around this limitation, the algorithm can simply apply the same divide-and-conquer approach to the large partition if it gets too large (i.e., repartition). In edge cases, it is possible that tuples with the same join column exist, making it impossible for a partition to decrease in size, regardless of the number of repartition attempts. In this case, a partition is sorted and a merge join is used. Merge Join A merge join outputs the same results as a hash join, but is only used when the inputs are already sorted by the equality columns. As was done with the customers in the hash join example, the merge join avoids the need to construct a hash table with one side of the input, making the operator more efficient. The merge join operator can simply advance both input streams until the tuples match on the equality columns, output the result of joining these tuples, and then move on to the next set of tuples that match on the equality columns. The merge join operator is generally considered a streaming algorithm, since not much state needs to be buffered during a merge join. However, in the case where both input streams have many tuples with the same equality column value, the operator needs to buffer all of these tuples on at least one side, since the result will be a cross product of both sets of tuples. In this case, spilling to disk is very simple, as the only thing that is needed is an append-only log that will be replayed multiple times. The Building Block All the algorithms covered so far have a common disk usage access pattern: they append data to on-disk queues (also known as an append-only log) and read from that queue sequentially (and possibly more than once). At a high level, a caller can enqueue and dequeue columnar batches of tuples. It can also reset the queue to go back to dequeue from the front of the queue. Under the hood, batches are serialized, compressed, and appended to a file. If the file exceeds a certain size, the queue rolls over to a new file. An in-memory cursor is maintained and incremented as the caller reads from these files. This design, including alternatives, is covered more in depth in this RFC. Batches are written to disk using the Apache Arrow IPC file format , which is a specification for how to serialize columnar data. Although we don’t use Arrow batches to represent physical data in the vectorized execution engine directly, we use a very similar representation that can be easily and efficiently converted into Arrow batches and serialized as such. For example, suppose we have a batch of strings that is represented using a flat bytes representation, composed of three buffers: A null bitmap to represent any nulls. A single bytes buffer that represents all the strings. An accompanying offsets buffer that represents the start and end indices of the individual strings in the bytes buffer. These three buffers are cast to bytes, treated as Arrow buffers, and then serialized using the same flatbuffer spec, which generally consists of some metadata that points to these buffers and the buffers themselves. Using this physical representation avoids a copy by using an O(1) cast to bytes because the data is already contiguous in memory. If strings were represented as a two-dimensional array, the data would need to be prepared for serialization by allocating a new buffer and then iterating and copying each element into it. Conclusion In this post, we covered how we used a single building block to implement a variety of on-disk algorithms in the vectorized execution engine in CockroachDB v20.1. Queries that could previously use an unbounded amount of memory now use up to a constant amount of work memory and spill to disk if this amount is not enough. With the addition of disk spilling, we renamed the experimental_on vectorize mode to on , since we now consider the vectorized execution engine ready for production use although it is not yet fully enabled by default. As a reminder, only queries that use streaming (non-buffering) operators and that are likely to read more rows than the vectorize_row_count_threshold setting (which defaults to 1,000) are run by default in v20.1. By running SET vectorize=on in a session or SET CLUSTER SETTING sql.defaults.vectorize=on , all supported queries including ones that spill to disk will be run through the vectorized execution engine. I hope you enjoyed learning about how we added disk spilling to the vectorized execution engine, and I urge you to try enabling it to speed up any complex joins or aggregations. And if you're interested in working on similar projects, we've got good news: Cockroach Labs is hiring !", "date": "2020-06-30"},
{"website": "CockroachLabs", "title": "What's New in CockroachDB’s Cost-Based Query Optimizer", "author": ["Radu Berinde"], "link": "https://www.cockroachlabs.com/blog/cost-based-optimizer-20-1/", "abstract": "In 2018, CockroachDB implemented a cost-based query optimizer from scratch , which has been steadily improved in each release. The query optimizer is the part of the system that understands the semantics of SQL queries and decides how to execute them; execution plans can vary wildly in terms of execution time, so choosing a good plan is important. In this post we go over some of the optimizer-related improvements in CockroachDB v20.1 . A large chunk of the changes we’ve made are around adding transformation rules to improve various types of queries and would be too extensive to discuss in detail. Throughout this release, we’ve added 15 new transformation rules (for a total of 224). There are however a few significant areas of effort that are worth going over: optimizer-driven foreign key checks; propagation of limit “hints”; WITH RECURSIVE clauses; and a new way to extract information about query planning and execution. Optimizer-driven foreign key checks CockroachDB has supported foreign keys since the first version, with foreign key actions added in v2.0. Much has changed in the SQL layer since then - most notably, we now have an optimizer with important capabilities, like locality awareness . However, the foreign key infrastructure remained largely unchanged and still used the original design from a pre-optimizer world. We have been working on reimplementing foreign keys so that they leverage the optimizer and its ever-increasing intelligence, and in v20.1 foreign key checks are planned by the optimizer by default. In this section we go over some of the details of how they worked before and how they work now. A foreign key describes a relationship between two tables; in the context of a particular relationship, we call one the \"child\" table and the other the \"parent\" table. A foreign key restricts the possible values on a certain column in the child table to the set of values that exist in the parent table (again, on a certain column). A simple example is to consider a parent table of customers and a child table of orders. The orders table has a \"customer\" column; by declaring a foreign key constraint we are saying that orders can only refer to valid customers. It is the responsibility of the database to enforce the constraint. This means that if we insert an order, we must check that the customer is valid (exists in the customer table). Conversely, when we delete a customer, we must make sure that there are no remaining orders for that customer which would become \"orphaned\". This brings up the problem of how exactly do we perform these FK (foreign key) checks? Our original implementation solved this by requiring both the child and the parent table to have indexes on the foreign key column. Then the relationship is effectively a mapping between these two indexes, and the indexes would always be used to perform the FK checks. The problem with this approach is that it is restrictive in terms of schema. The extra indexes entail overhead (in terms of storage, and extra work during insertions) and in some cases they don't help speed up any important aspect of the workload. In a multi-region deployment, the fact that a specific index is always used prevents effective use of the duplicate indexes pattern where each region has its own locally accessible index. The new implementation removes the shackles and divorces the foreign key relationship from indexes. Now the optimizer builds the FK checks as generic \"subqueries\" and plans them together with relevant mutations. This allows the optimizer to use its usual logic in determining the best way to execute these checks (including locality-sensitive index selection). Unfortunately, there is a catch. Foreign key actions can be used for more complex behaviors, like automatically cascading a deletion in the parent table into a deletion in the child table. We haven't yet reimplemented cascades, so we must fall back to the existing code implementing these actions (which still relies on the dual-index scheme). Because the existing code needs to continue to function, we weren't able to remove the dual-index requirement for v20.1. We are however working on this as a high priority item for v20.2. Despite this caveat, this work does present tangible benefits in v20.1 - most importantly locality-sensitive index selection for the foreign key checks. Soft limits The optimizer had various rules for the Limit operator since its inception. In particular, we try to “push down” limits as much as possible (“push down” is a fancy way of saying that we try to apply limits as early as we can during execution). But the situations which allow pushing down the Limit operator are pretty restrictive. This is because the operator is a “hard” limit, meaning that we can only use it when we absolutely know for sure that a certain number of rows is sufficient to get a correct query result. There are many situations where we can guess (estimate) a limit, without hard guarantees. For example, consider a simple query like SELECT * FROM t WHERE status=’done’ LIMIT 10 . The execution plan for this query (assuming we don’t have an index on status ) is to perform a full table scan and retain only the first 10 rows that pass the filter. In the worst case we may need to read the entire table; but in practice, depending on how many rows pass the condition, we will probably need to scan many fewer rows. During execution of this plan, once 10 rows are produced, execution will stop early so we won’t actually scan the entire table. However, the optimizer didn't take this into consideration when estimating the cost of the plan, which means it might have chosen another plan that it mistakenly thinks is lower cost. In addition to helping choose the right plan, an estimate of how many rows we will actually need to scan can help make things more efficient: we can configure the internal “batch” size of the scan (we retrieve 10,000 keys by default!). In v20.1 we added infrastructure for “soft” limits. A soft limit is a property inside the optimizer that is treated as a hint, defined roughly as “no promises, but execution will likely complete early, after this many rows are produced by this operator”. In the example above, we use table statistics to estimate the selectivity of the status='done' condition and calculate the soft limit for the scan (which sizes its initial batch of keys according to the soft limit). If the statistics show that (for example) about half of the rows have status='done' , the soft limit would be 20. An important aspect of this work is that the optimizer is now able to estimate the cost of expressions more accurately by taking into account early execution completion. The corrected costing can make a dramatic difference in more complicated cases, in particular when lookup joins are involved (an example is nicely described here ). Acknowledgement: Thanks to Celine O’Neil who did the soft limit work during her internship. Recursive CTEs A new feature we implemented in this version is support for the WITH RECURSIVE syntax which defines a recursive common-table expression . This improves our coverage of SQL in general and compatibility with PostgreSQL in particular. It’s interesting to point out that the addition of recursive CTEs makes SQL a Turing-complete language : you can in principle use SQL to perform any computation (like a full-fledged programming language); see this presentation . In practice, this means that you can do cool stuff, like fractals . Sierpinski Triangle generated in SQL using recursive CTEs. Source: Michael Malis Statement diagnostics bundle The 20.1 release also added a statement diagnostics bundle to the optimizer. CockroachDB provides a few ways to get insight about the plan for a query and its execution: the AdminUI shows logical query plans on the statements page ; flavors of the EXPLAIN statement show query plans with varying levels of detail, as well as execution statistics; tracing the execution of queries provides a lot of low-level detail (but is not easily consumable by most users). Unfortunately, there wasn’t a single action a user could take to extract all the relevant information when reporting an issue to us. In many cases this leads to a time-consuming back and forth between the user and our engineers when investigating issues. The problem of unifying these pieces of information into something easily consumable by users is complex and we are working on attacking it. In the short term, we wanted to at least simplify the collection of information. The solution was, of course , to introduce yet another variant of EXPLAIN: in v20.1 EXPLAIN ANALYZE (DEBUG) is similar to EXPLAIN ANALYZE but creates a statement diagnostics bundle: a .zip file that contains all the information from many “flavors” of EXPLAIN , as well as complete table statistics and query trace data. The bundle can be downloaded from the AdminUI. We also introduce a new mechanism for triggering the collection of this information: from the AdminUI statements page, users can select a statement fingerprint and activate diagnostics. The next time a query with this fingerprint runs, a diagnostics bundle will be collected automatically. Conclusion Building a cost-based SQL optimizer from scratch is no small endeavor, and we’ll continue to add more improvements and functionality in future releases. To get started with the new features in this release, download CockroachDB v20.1 . And as always, if building a SQL optimizer is your jam, Cockroach Labs is hiring .", "date": "2020-07-16"},
{"website": "CockroachLabs", "title": "Improving Unordered Distinct Efficiency in the Vectorized SQL Engine", "author": ["Archer Zhang"], "link": "https://www.cockroachlabs.com/blog/unordered-distinct-vectorized-engine/", "abstract": "For the past four months, I’ve worked as a backend engineering intern with the incredibly talented engineers of the Cockroach Labs SQL Execution team to unlock the full potential of the vectorized engine ahead of the CockroachDB 20.1 release. During this time, I focused on improving the algorithm behind some of CockroachDB’s SQL operators, to reduce memory usage and speed up query execution. In this blog post, I will go over the improvements that I made to the algorithm behind the unordered distinct operator, as well as new algorithms that address some existing inefficiencies. In CockroachDB’s 20.1 release , we removed the experimental flag on the vectorized execution engine . If the estimated number of rows scanned is greater than a certain threshold (1000 rows, by default), CockroachDB runs a subset of queries through the vectorized engine. Users can also use the vectorized engine to execute all supported queries using the following command in their SQL shell: set vectorize= on ; With this setting, CockroachDB will use the vectorized engine by default and only fall back to the old engine if it encounters unsupported queries. The Vectorized Execution Engine The vectorized execution engine is an interesting piece of technology that significantly speeds up SQL queries. In our early benchmarks, the new engine can be up to 40x faster than our existing row-at-a-time SQL engine. You can read more about how we built the vectorized engine in How We Built a Vectorized Execution Engine . When the vectorized engine finishes fetching all tuples into memory, it stores all of them in a columnar memory layout. This enables us to build SQL operators that are friendly to modern CPU caching architectures and branch predictors, and can greatly outperform our existing row-at-a time engine. However, these improvements have come at a cost. Columnar memory layout is great when it comes to speeding up operations, but sometimes it makes it difficult to implement complex operations, like SQL joins or groupings . Also, we need to be careful with our implementation such that we don't take any shortcuts, which can cause the vectorized engine to essentially perform like the old row-at-a-time engine. For example, if the logic within a tight loop is very complicated or involves expensive operations, such as having too much branching and on-the-fly memory allocation, it can offset the benefits of having a columnar memory layout. In order to understand the concepts presented in this blog, we recommend you read our previous blog post, 40x faster hash joiner with vectorized execution . Unordered Distinct Unordered distinct is a SQL operator that performs distinct operations on the given columns of the input. It removes the tuples that have duplicate values on the distinct columns from the query result. As its name suggests, it is designed to work with inputs where the distinct columns are unordered. The unordered distinct operator needs to keep track of all distinct tuples that it has come across throughout its lifetime. Naively, the operator can store all unique tuples inside an in-memory key-value dictionary. For all subsequent tuples it encounters, the operator only needs to perform a simple lookup in the dictionary to determine if this tuple has already been seen. This is because within each iteration of the loop, the operator needs to perform multiple steps of computation: Hash the key columns of the input tuples. Perform a dictionary lookup using the tuple's hash. If the same hash key exists, perform an actual comparison between the new tuple and the tuple with the same hash key in the dictionary to check if it is a hash collision. Perform a dictionary insertion if the new tuple is actually unique. Each iteration involves a hash computation, multiple branching and value comparisons (which can be very expensive, depending on the SQL data types), and insertion (which can lead to memory allocation and copying). The point of the vectorized engine is to reduce the one big loop into multiple small loops, with very simple operations inside the loop bodies. This can improve the program's cache friendliness and reduce branch misprediction inside the CPU. As of CockroachDB 19.2, the vectorized unordered distinct operator was implemented using our own vectorized hash table. Its implementation was based on Marcin Zukowski's paper \" Balancing Vectorized Query Execution with Bandwidth-Optimized Storage \". The vectorized hash table was the key to our implementation of an efficient vectorized hash joiner . Because the original vectorized hash table was designed for the hash join operator, it stores a lot of extra information that is not necessary for the unordered distinct operator. For example, in order to implement a hash join, the hash table needs to store all tuples, not just distinct tuples. In addition to storing all tuples, the hash table needs to build a linked list for each bucket to efficiently traverse through collisions. In the unordered distinct operator, none of those auxiliary data structures and computation are required. The only thing we care about is the first distinct tuple we encounter. As a result, we can discard all subsequent tuples that are identical to the first one, significantly improving the performance of the unordered distinct operator. The Vectorized Hash Table CockroachDB separates the construction of a hash table into multiple stages. In the first stage, it consumes all of its input and buffers all tuples in memory. It then hashes each buffered tuple, and groups tuples that have the same hash values into buckets. For each bucket, the hash table constructs a linked list in order to traverse through all of the tuples. Let’s call this linked list a hash chain . After hash chains for each bucket are constructed, the hash table traverses through each bucket to check if there are any hash collisions. For each bucket, the hash table creates a “same” linked list for each distinct value inside. The idea of this new linked list is similar to the hash chain. The only difference is the hash chain enables us to traverse through all tuples in each bucket that have the same hash value, and the “same” linked list allows us to traverse through all identical tuples. At this point, the construction of the hash table is complete. To better illustrate the algorithm, let’s look at an example. Consider an input table with three columns: a, b and c. Suppose that each input tuple is hashed to either 1 or 2, using some hash function, and a hash collision occurs. Input:                          hash buffer:\n+---+----+-----+\t        +-------+------+\n| a |  b |  c  |             \t| index | hash |\n+---+----+-----+             \t+-------+------+\n| 2 |null|\"2.0\"|             \t|   0   |  0   |  <- hash collision \\\n+---+----+-----+             \t+-------+------+                     *\n| 2 |1.0 |\"1.0\"|             \t|   1   |  1   |                     |\n+---+----+-----+             \t+-------+------+                     |\n| 2 |null|\"2.0\"|             \t|   2   |  0   |  <- hash collision  | \n+---+----+-----+.   ----->     \t+-------+------+                     *\n| 2 |2.0 |\"2.0\"|             \t|   3   |  0   |  <- hash collision  |\n+---+----+-----+             \t+-------+------+                     *\n| 2 |null|\"2.0\"|             \t|   4   |  0   |  <- hash collision /\n+---+----+-----+             \t+-------+------+\n| 2 |1.0 |\"1.0\"|             \t|   5   |  1   |\n+---+----+-----+             \t+-------+------+ The hash buffer is then used to construct the hash chains for each bucket. We represent all hash chains in the hash table using two arrays,“first” and “next”. Each entry in the “first” array maps from a hash value to an entry in the “next” array. This entry in the “next” array represents the beginning of the hash chain linked list. We use a value of 0 to represent the end of the linked list. first:                  next:\n+------+-------+        +-------+------+\n| hash | keyID |    \t| keyID | next |\n+------+-------+    \t+-------+------+\n|  0   |   5   |    \t|   0   | N/A  |  <- not used, reserved\n+------+-------+    \t+-------+------+\n|  1   |   6   |    \t|   1   |  0   |  <- tail of hash chain of\n+------+-------+    \t+-------+------+     hash value 0\n                    \t|   2   |  0   |  <- tail of hash chain of\n                    \t+-------+------+     hash value 1\n                     \t|   3   |  1   |\n                     \t+-------+------+\n                       \t|   4   |  3   |\n                      \t+-------+------+\n                       \t|   5   |  4   |  <- head of hash chain of\n                       \t+-------+------+     hash value 0\n                       \t|   6   |  2   |  <- head of hash chain of\n                       \t+-------+------+     hash value 1 The zeroth element of the “next” array is not used. As a result, we introduce a “keyID” column computed from the index. To get the value for “keyID”, we shift the index values of each entry by one. So, keyID = index + 1 . The value of the “next” array entry simply points to the key ID of the next entry in the same hash chain linked list. If we rewrite the result from the “first” and “next” array in a more readable form, we have this: +------------+---------------------------------------+\n| hash value | keyIDs for the hash chain linked list |\n+------------+---------------------------------------+\n| \t0    |        \t5 -> 4 -> 3 -> 1       \t     |\n+------------+---------------------------------------+\n| \t1    |        \t6 -> 2                 \t     |\n+------------+---------------------------------------+ Note that for each hash chain, the key ID values are in a monotonically decreasing order. This property is not important for the hash joiner, but it can cause some trouble for unordered distinct. We will revisit this later in the blog. The next step of the algorithm is to probe through each hash chain we have just built, and resolve any hash collisions. The hash table goes through every single tuple that it has buffered so far. For each tuple, it calculates its hash value and looks up the hash chain corresponding to this hash value. Once the corresponding hash chain is found, each tuple compares itself with the head of the hash chain. If their values are different, we have encountered a hash collision, since different values are hashed to the same hash value. For more details on how we implemented this step, see the blog post on hash joiners . The result from the hash collision detection stage is also a set of linked lists similar to the ones we used to store hash chains. Conceptually, this looks like the following: +-------------------------------+---------------------------------------+\n| keyID of the linked list head | linked list of keyIDs with same value |\n+-------------------------------+---------------------------------------+\n|      5                        |      5 -> 4 -> 1       \t        |\n+-------------------------------+---------------------------------------+\n|      3                        |      3       \t                        |\n+-------------------------------+---------------------------------------+\n|      6                        |      6 -> 2       \t                |\n+-------------------------------+---------------------------------------+ Note that this is different from the hash chain we mentioned earlier. In hash chains, we use linked lists to point to all tuples that have the same hash value. Now, the linked list contains tuples whose values are equal. While this information is important for the hash joiner in order to ensure the correctness of the output, it isn’t very useful for unordered distinct. As a result, when we find the head of the linked list, we can skip building out the linked list. This simple optimization actually yields about 35 ~ 40% of increase of performance for the unordered distinct operator. name                                     old speed  \tnew speed     delta numCols=1/nulls=false/numBatches=1-8 \t 128MB/s\t181MB/s       +41.41%\nnumCols=1/nulls=false/numBatches=16-8\t 107MB/s\t145MB/s       +35.51%\nnumCols=1/nulls=false/numBatches=256-8   45.1MB/s       82.6MB/s      +83.15%\nnumCols=1/nulls=true/numBatches=1-8 \t 71.5MB/s       112.1MB/s     +56.78%\nnumCols=1/nulls=true/numBatches=16-8\t 65.6MB/s       98.9MB/s      +50.76%\nnumCols=1/nulls=true/numBatches=256-8    22.8MB/s       41.5MB/s      +82.02%\nnumCols=2/nulls=false/numBatches=16-8\t 146MB/s\t190MB/s       +30.14%\nnumCols=2/nulls=false/numBatches=256-8   50.5MB/s       103.1MB/s    +104.16%\nnumCols=2/nulls=true/numBatches=1-8 \t 97.2MB/s       131.6MB/s     +35.39%\nnumCols=2/nulls=true/numBatches=16-8\t 82.0MB/s       111.2MB/s     +35.61% Can we do even better than this? Of course! Recall that in the initial stage of building the vectorized hash table, the engine buffers all tuples into memory first, and then proceeds to perform subsequent computations. This was designed so that the hash table has all the information it needs for the hash joiner. However, the unordered distinct operator only needs the distinct tuples. This is key to our second optimization: we only need to store the distinct tuples in the hash table! This optimization strategy also limits the memory consumption of unordered distinct to be proportional to the number of distinct tuples in the input instead of the total number of tuples in the input. Also, since we only buffer distinct tuples into memory, we reduce the total number of comparisons to perform throughout the entire algorithm. Vectorized Hash Table Distinct Mode After the unordered distinct operator finishes constructing the hash table, the operator goes through every single “same” linked list and copies the head of each linked list into its internal buffer. When the operator finishes processing all “same” linked lists, it outputs all the buffered tuples it has copied from the hash table. Observe that as the operator goes through each “same” linked list and copies only the head of each linked list, the memory access pattern is random instead of sequential. This access pattern is quite unfriendly to the cache inside the CPU, meaning the operator will have a large number of cache misses as it goes through each “same” linked list. We can improve this by adding a distinct mode to the hash table to avoid this random access pattern. This means that the output of the hash table should only contain the distinct tuples and the operator can simply sequentially copy it to its internal buffer. In order to fully implement the distinct mode in the vectorized hash table, we need to modify the algorithm that builds the hash table. First, since the main goal is to eliminate the need to buffer the entire input into memory, we shouldn't consume the entire input and then buffer all tuples in memory in the beginning. Instead, we can build the hash table incrementally. This is quite simple to implement. When we fetch a new batch of tuples, we first perform deduplication within that batch. After all duplicate tuples are removed from the batch, we check the remaining tuples within that batch against the already-buffered tuples to see if there are any additional duplicate tuples. Since we only insert batches that strictly contain unique tuples, we can be sure that there are no duplicate tuples within the buffered tuples. When the second duplication check is completed, we can simply append all tuples remaining in the new batch directly into the hash table, and the uniqueness invariant will still hold. This approach is reflected in the following high-level code snippet: func build() hashTable {\n    ht := hashtable{}\n    for {\n   \t batch := input.Next(ctx)\n   \t if batch.Length() == 0 {\n   \t\t break\n   \t }\n    \n       buildHashChains(batch)\n       // Removes the duplicate tuples within the new batch.\n   \t removeDuplicatedTuples(batch, batch)\n\n       // Removes the tuples from the new batch that already have duplicates\n       // in the hash table.\n   \t removeDuplicatedTuples(batch, ht.bufferedTuples)\n\n   \t ht.bufferedTuples.Append(batch)\n    }\n    return ht\n} Now, if you are familiar with the hash collision handling scheme we implemented in our previous blog post , you will realize that the process of de-duplication and checking for hash collision is quite similar. In fact, if you squint hard enough at the code , they are almost identical, with some slight differences. There are two phases to how we perform deduplication. In the first phase, we remove duplicate tuples within the batch itself. For each tuple, we traverse through the hash chain that has been built previously. As soon as it finds the first identical tuple in the hash chain, we stop the linked list traversal. The algorithm itself is quite straightforward. However, if you look closely, you will notice that this optimization cannot completely guarantee the correctness of the algorithm. We mentioned previously that once we build the hash chain, the values of the keyIDs inside the hash chain will be in a monotonically decreasing order. In other words, the head of the hash chain linked list is going to be the tuple with the largest keyID. The deduplication algorithm immediately stops the hash chain traversal when it encounters a tuple that has the identical value. That tuple is emitted as the distinct tuple. Because the order of the hash chain is reversed, the head (i.e., the last occurrence of the tuple inside a batch) will be the one that gets emitted. Recall the previous example: first:                  next:\n+------+-------+        +-------+------+\n| hash | keyID |    \t| keyID | next |\n+------+-------+    \t+-------+------+\n|  0   |   5   |    \t|   0   | N/A  |  <- not use, reserved\n+------+-------+    \t+-------+------+\n|  1   |   6   |    \t|   1   |  0   |  <- tail of hash chain of\n+------+-------+    \t+-------+------+     hash value 0\n                    \t|   2   |  0   |  <- tail of hash chain of\n                    \t+-------+------+     hash value 1\n                     \t|   3   |  1   |\n                     \t+-------+------+\n                       \t|   4   |  3   |\n                      \t+-------+------+\n                       \t|   5   |  4   |  <- head of hash chain of\n                       \t+-------+------+     hash value 0\n                       \t|   6   |  2   |  <- head of hash chain of\n                       \t+-------+------+     hash value 1 In this case, because  we start probing from keyID=5 and keyID=6, we will eventually emit the following keyIDs: 5, 6 and 4. However, inside CockroachDB's SQL engine, different SQL operators can be used as building blocks for more complicated queries. Making the unordered distinct operator deterministically output the first distinct tuple it encounters makes things even easier for SQL operators and the SQL planner. The solution to this problem is essentially to reverse the order of the linked list. Since the deduplication process starts with the head of the hash chain, if we can ensure the value of keyIDs within the hash chain is in monotonically increasing order, we can guarantee that the hash table will emit the first tuple it encountered. As the result, we have the following result: first:                  next:\n+------+-------+        +-------+------+\n| hash | keyID |    \t| keyID | next |\n+------+-------+    \t+-------+------+\n|  0   |   1   |    \t|   0   | N/A  |  <- not use, reserved\n+------+-------+    \t+-------+------+\n|  1   |   2   |    \t|   1   |  3   |  <- head of hash chain of\n+------+-------+    \t+-------+------+     hash value 0\n                    \t|   2   |  6   |  <- head of hash chain of\n                    \t+-------+------+     hash value 1\n                     \t|   3   |  4   |\n                     \t+-------+------+ Here's the benchmark for the unordered distinct operator with updated algorithm: name                                           old speed  new speed  delta newTupleProbability=0.001/rows=4096/cols=2\t154MB/s   156MB/s    ~\nnewTupleProbability=0.001/rows=4096/cols=4\t219MB/s   236MB/s    ~\nnewTupleProbability=0.001/rows=65536/cols=2     136MB/s   384MB/s    +181.65%\nnewTupleProbability=0.001/rows=65536/cols=4     245MB/s   518MB/s    +111.55%\nnewTupleProbability=0.010/rows=4096/cols=2\t154MB/s   155MB/s    ~\nnewTupleProbability=0.010/rows=4096/cols=4\t214MB/s   249MB/s    +16.29%\nnewTupleProbability=0.010/rows=65536/cols=2     181MB/s   373MB/s    +106.24%\nnewTupleProbability=0.010/rows=65536/cols=4     225MB/s   504MB/s    +123.53%\nnewTupleProbability=0.100/rows=4096/cols=2\t150MB/s   147MB/s    ~\nnewTupleProbability=0.100/rows=4096/cols=4\t207MB/s   204MB/s    ~\nnewTupleProbability=0.100/rows=65536/cols=2     165MB/s   301MB/s    +82.26%\nnewTupleProbability=0.100/rows=65536/cols=4     201MB/s   398MB/s    +97.83%\nnewTupleProbability=0.001/rows=4096/cols=2 \t113MB/s   122MB/s    +8.02%\nnewTupleProbability=0.001/rows=4096/cols=4 \t141MB/s   168MB/s    +19.43%\nnewTupleProbability=0.001/rows=65536/cols=2\t138MB/s   236MB/s    +71.44%\nnewTupleProbability=0.001/rows=65536/cols=4\t156MB/s   282MB/s    +80.65%\nnewTupleProbability=0.010/rows=4096/cols=2 \t109MB/s   117MB/s    +6.74%\nnewTupleProbability=0.010/rows=4096/cols=4 \t143MB/s   164MB/s    +14.59%\nnewTupleProbability=0.010/rows=65536/cols=2\t100MB/s   226MB/s    +125.29%\nnewTupleProbability=0.010/rows=65536/cols=4\t142MB/s   272MB/s    +91.53%\nnewTupleProbability=0.100/rows=4096/cols=2 \t108MB/s   112MB/s    +3.43%\nnewTupleProbability=0.100/rows=4096/cols=4 \t132MB/s   150MB/s    +13.62%\nnewTupleProbability=0.100/rows=65536/cols=2\t107MB/s   191MB/s    +78.89%\nnewTupleProbability=0.100/rows=65536/cols=4\t125MB/s   237MB/s    +89.61% We can see that depending on the characteristics of the input data, we can have a performance increase of up to 181% on top of our previous performance boost of ~35% to ~40%. Also, we see a drastic drop in the  memory footprint of the unordered distinct operator: name                                        old alloc/op new alloc/op delta newTupleProbability=0.001/rows=65536/cols=2  8.17MB       1.17MB     -85.68%\nnewTupleProbability=0.001/rows=65536/cols=4  14.0MB       1.2MB      -91.34%\nnewTupleProbability=0.010/rows=65536/cols=2  8.17MB       1.20MB     -85.29%\nnewTupleProbability=0.010/rows=65536/cols=4  14.0MB       1.3MB      -90.92%\nnewTupleProbability=0.100/rows=65536/cols=2  8.17MB       1.77MB     -78.27%\nnewTupleProbability=0.100/rows=65536/cols=4  4.0MB        2.2MB      -83.99% I hope you enjoyed learning how our vectorized hash table works under the hood and how we continuously evolve our algorithms and implementations. What I presented in this blog post is only the tip of the iceberg of what we have worked on for the vectorized engine in the CockroachDB 20.1 release . I am very grateful for the opportunity to work with the absolute brilliant engineers at Cockroach Labs and I want to really thank my mentor Alfonso, my manager Jordan and my teammate Yahor for this amazing internship. Thank you Cockroach Labs, it has been a great journey. If you're interested in working at Cockroach Labs, we have both internships and full-time positions available .", "date": "2020-07-09"},
{"website": "CockroachLabs", "title": "Using Lateral Joins for Business Analytics in CockroachDB 20.1", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/using-lateral-joins-in-the-cockroachdb-20-1-alpha/", "abstract": "CockroachDB’s 20.1 release supports lateral joins, which can dramatically reduce the lines of code needed to arrive at key business analytics. Today, I’m going to walk you through a demo of lateral joins and showcase how you might use them to run analytic queries directly in CockroachDB. What’s a lateral join? A lateral join is a type of correlated subquery that allows data sources to reference columns provided by previous data sources in the same query. In practice, this can be used to iterate over each row of a result, similar to a nested foreach loop. We will walk through an example below using CockroachDB 20.1 and our fictional ride-sharing company, MovR. Cluster Setup First, let’s set up CockroachDB 20.1 using your preferred method including Linux, Mac, or Windows binaries, Docker images, or building directly from source. This example will focus on the MovR rides table that is defined as: CREATE TABLE rides (\n                  id UUID NOT NULL,\n                  city VARCHAR NOT NULL,\n                  vehicle_city VARCHAR NULL,\n                  rider_id UUID NULL,\n                  vehicle_id UUID NULL,\n                  start_address VARCHAR NULL,\n                  end_address VARCHAR NULL,\n                  start_time TIMESTAMP NULL,\n                  end_time TIMESTAMP NULL,\n                  revenue DECIMAL(10,2) NULL                   \n                      ) Note, I’ve simplified this table a bit in case you want to follow along and copy-paste this table and the subsequent queries into CockroachDB. You can also use the new CockroachDB demo feature that we recently blogged about to try this out in less than five minutes. To use CockroachDB demo (and the more complex rides table with pre-populated data) enter the following command: ./cockroach demo This command will automatically load the MovR tables and data: show tables;\n\ntable_name\n+----------------------------+\n  promo_codes\n  rides\n  user_promo_codes\n  users\n  vehicle_location_histories\n  vehicles\n(6 rows) MovR OKRS: A Lateral Join Example The executive team at MovR has set a number of important Objectives and Key Results ( OKRs ) for MovR. The top objective for this year is to increase riders who use MovR. To do that, the executive team determined that the most important key result is to lower the average time between customers’ first ride and their second ride. This is an important metric because it helps customers build a habit of using MovR and increases the lifetime value of the customer, a key input into profitability. MovR uses CockroachDB as its OLTP system of record and it keeps track of every ride for each user. While we can export any data from CockroachDB via change data capture (CDC) into an analytics oriented database, in this case, we don’t need to as we can derive this answer directly in CockroachDB using SQL’s Lateral Join syntax. Eventually, we will have a fairly complex 25 line query. Before we get to the full query, let’s start in the middle and work our way out. First, we need to calculate the first time a user took a ride with MovR: SELECT\n    rider_id, \n    1 AS first_ride,\n    min(start_time) AS first_ride_time\n  FROM rides\n  GROUP BY rider_id This query finds the minimum start time of a ride for a given rider_id and alias it to the variable first_ride_time. It also keeps track of whether or not a user has completed a first ride by entering either 1 or null into a first_ride column. Next, we need to determine if the customer took as second ride, and, if so, determine how long after the first ride that the customer took their second ride: SELECT\n  1 AS second_ride,\n  start_time AS second_ride_time\nFROM rides\nWHERE\n  rider_id = r1.rider_id AND\n  start_time > first_ride_time \nORDER BY start_time\nLIMIT 1 Similarly to above, we are calculating a second_ride_time variable based on start_time. However, this time we require the start time to be greater than the variable we previously defined as first_ride_time. This ensures that we get our second ride time. Altogether, we can pair these queries into our larger query: SELECT \n  rider_id,\n  first_ride,\n  first_ride_time,\n  second_ride,\n  second_ride_time\nFROM (\n    --get the first ride time\n SELECT\n    rider_id,\n    1 AS first_ride,\n    min(start_time) AS first_ride_time\n  FROM rides\n  GROUP BY rider_id\n) r1 LEFT JOIN LATERAL (\n    -- Get the second ride time \nSELECT\n  1 AS second_ride,\n  start_time AS second_ride_time\nFROM rides\nWHERE\n  rider_id = r1.rider_id AND\n  start_time > first_ride_time \nORDER BY start_time\nLIMIT 1\n) r2 ON true This will produce a table that looks like: rider_id    | first_ride | first_ride_time                  | second_ride | second_ride_time\n+--------------------------------------+------------+----------------------------------+-------------+----------------------------------+\n  04553af9-dc34-461a-95fa-8c9e7123a613 |          1 | 2019-11-13 16:10:00.627167+00:00 |           1 | 2019-11-13 16:12:59.356821+00:00\n  77cacc32-57dc-4f52-83a4-d50a9a65b8bd |          1 | 2019-11-13 16:06:33.618505+00:00 |           1 | 2019-11-13 16:06:37.002664+00:00\n  c2c25f39-d2a0-480f-a5bb-36ec5436e34f |          1 | 2019-11-13 16:35:06.696277+00:00 |        NULL | NULL\n  418c9765-4a04-4728-9f5d-981d3a6e9c5d |          1 | 2019-11-13 16:06:28.37234+00:00  |           1 | 2019-11-13 16:19:33.451015+00:00\n  4be831d4-9115-412b-b2c4-dceb92c81e10 |          1 | 2019-11-13 16:15:32.557127+00:00 |           1 | 2019-11-13 16:20:33.258865+00:00 This is great, but we can do even better by directly making the comparison within CockroachDB using the avg comparison on the two defined variables: first_ride_time and second_ride_time. SELECT \n  avg(second_ride_time::DECIMAL - first_ride_time::DECIMAL) as average_time_to_second_ride\nFROM (\n    --get the first ride time\n SELECT\n    rider_id,\n    1 AS first_ride,\n    min(start_time) AS first_ride_time\n  FROM rides\n  GROUP BY rider_id\n) r1 LEFT JOIN LATERAL (\n    -- Get the second ride time \nSELECT\n  1 AS second_ride,\n  start_time AS second_ride_time\nFROM rides\nWHERE\n  rider_id = r1.rider_id AND\n  start_time > first_ride_time \nORDER BY start_time\nLIMIT 1\n) r2 ON true\n\nThis will produce a result like: \n  average_time_to_second_ride\n+-----------------------------+\n        38690.915436623268698\n(1 row)\n \nTime: 16.576ms *Note, if you are following along at home you may see a different final result because we generate random data in the MovR application. It this case it takes about 10 hours for our riders to complete their second ride. Now we have actionable data as a baseline for our OKR. We can measure the success of our efforts to lower the time to second ride as a key result moving forward. It may also begin to clue us in on a specific pattern--MovR customers may be using MovR to commute to and from work, a time of approximately 10 hours on average. Try Lateral Joins in CockroachDB 20.1 CockroachDB’s 20.1 release now supports lateral joins, which can dramatically reduce the lines of code needed to arrive at key business analytics. It opens up a whole new world of possibilities for application developers to enhance the power of correlated subqueries. As we show above, a LEFT JOIN to a LATERAL subquery can provide the source rows in the result even if the LATERAL subquery produces no rows for them. To try out lateral joins for yourself, test out CockroachDB 20.1 !", "date": "2020-07-16"},
{"website": "CockroachLabs", "title": "Build a Python App with SQLAlchemy + CockroachDB", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/sqlalchemy-support/", "abstract": "One of the great things about CockroachDB’s support for SQL is the ecosystem of frameworks, ORMs, and other tools for working with SQL data. SQLAlchemy is just one of the ORMs in that ecosystem, and one we use quite a bit for Python projects. You’ll find it referenced in some of our sample application docs, and our upcoming “ CockroachDB for Python Developers ” course has students use SQLAlchemy and Flask in projects. Given the prevalence of SQLAlchemy at Cockroach Labs and in the Python and Flask communities, we’re excited to announce a few updates that make our compatibility with SQLAlchemy even more seamless. Updates include new support for SAVEPOINT , and improved transaction retries (you can download the updated code and the sqlalchemy-cockroachdb Python package here ). Our docs team has written a start-from-scratch tutorial on how to build a `Hello World` app with SQLAlchemy and CockroachDB. So if you’re a Python developer looking to get started with an easy SQL database, check out the tutorial here . Or if videos are more your thing, you can preview many of the lessons from the upcoming “CockroachDB for Python Developers” course on our YouTube channel . As always, if you have any comments or questions about CockroachDB and SQLAlchemy , stop by our community Slack . Happy querying!", "date": "2020-07-28"},
{"website": "CockroachLabs", "title": "Distributed SQL: a Cloud-Native, Scalable Postgres Alternative", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/distributed-sql-webinar/", "abstract": "Gartner predicts that by 2022, 75% of all databases will be deployed in the cloud. The findings shouldn’t come as a surprise: on-premise data centers are slowly being phased out as companies realize the many benefits to running a database in the cloud. It’s a paradigm shift our customers at Cockroach Labs are experiencing first-hand, and, without Distributed SQL , it’s a hard one to meet. In a recent webinar, Our Vice President of Product Marketing Jim Walker sat down with Chris Casano, one of Cockroach Lab’s Solutions Engineers, to discuss the ins and outs of Distributed SQL, and how it delivers capabilities that traditional databases simply can’t. Legacy Databases Fail to Meet the Promises of the Cloud The cloud has had a fundamental impact on the way that we think about systems and the way that we design applications. There is a lot of value in the approach, but it’s introduced challenges as well. Minimizing costs by breaking down applications into microservices works fine at first, until you hit a certain scale. Transactional inconsistency and speed then become issues as dozens of microservices request information from--and write to--the database. Organizations without distributed capabilities often wind up paying a high cost for this down the road, when they’re inevitably forced to manually shard Postgres instances. When faced with this compromise, some companies turned to NoSQL stores to try to meet these requirements. These alternatives can typically meet the scale requirements, but they fall short as a transactional database because they were not designed to guarantee transactional consistency. Recently, some NoSQL solutions have offered transaction guarantees that are full of caveats and fail to deliver the isolation levels necessary for mission-critical workloads like financial ledgers, inventory controls, and identity management. A conversation about Distributed SQL This webinar addressed some of these concerns with some hands-on keyboard demos and a healthy conversation between our presenters, Jim and Chris. They spoke about and demonstrated how CockroachDB can survive multiple node failures, and scaling seamlessly to meet changing workloads. The replay of the webinar is here and a transcription of the question and answer session follows. [Watch the full Distributed SQL webinar] Q: Do you provision Cockroach on a virtual machine or a physical host, do you use a stateful container? How do you deploy a node Cockroach? A: CockroachDB needs no specialized hardware. It's hardware agnostic. You can run it on a VM, you can run it in Kubernetes , you can run it on bare metal. Q: If I have four nodes, how do you typically deal with four instances of an application or maybe five different applications all hitting the same database? How do they choose which node is it to just do load balancing and that sort of thing? A: You could just use a single network load balancer and that will distribute the traffic round robin across all the different nodes. CockroachDB has heuristics built into the database. So as nodes have more requests that are happening for the same piece of data, it will eventually move those ranges into those nodes. That way you don't have to incur all that network traffic. Q: Is this going to run like Kubernetes? A: Yes. Some of the core design principles that were used for Kubernetes in that designed to be shared nothing, so that each atomic unit is the same across all your instances. But what's really important is that each instant, CockroachDB doesn’t have different types of nodes. Every node is the same and contains the security constructs and how you integrate with everything else. Q: Does Cockroach labs have an operator? A: CockroachDB doesn’t need an operator to deploy or do rolling upgrades. But CockroachDB does have an operator for Kubernetes. Q: How do you optimize or how do you choose a primary key for naming your shards? A: It comes back to how you want to survive things and what kind of latency you want to have on that data. Those two questions which are critical in the design of the database itself that you've got to ask upfront. But if you wanted to change it, you can, and you don't have to have downtime. So if you have a primary key that you're sharding on, you actually want to change the way things are sharded so that, you can spin up a node, do an alter table command, and the database is going to start moving that to the new location. Q: How does this work with Daemon Sets, and how do we do rolling upgrades? A: The same way you would do a rolling upgrade in Kubernetes is the same way you're going to run here. You're just going to take nodes down or put pods down or up. You’re going to use stateful sets typically, but you can use demon sets to deploy CockroachDB as well.", "date": "2020-07-23"},
{"website": "CockroachLabs", "title": "CockroachDB & Spring Tutorials for Spring Data JDBC, JPA, & MyBatis", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/spring-data-access-tutorials/", "abstract": "From our enterprise customers to developer shops, a lot of folks use CockroachDB to back their Spring applications. It’s no surprise; Java is the most widely used programming language in the world, running in over 3 billion devices. And no matter what kind of application you’re running, the Spring ecosystem provides different data access patterns depending on your needs. We’ve recently added tutorials for a number of Spring data access patterns: Spring Data JDBC, Spring Data JPA, and MyBatis. Check them out below. CockroachDB and Spring Data JDBC Spring Data JDBC is a framework that lets developers easily implement applications on top of the Java Database Connectivity (JDBC) API. Compared to an ORM or other tools, JDBC is a low-level way to access data where you’re writing raw SQL commands. Since CockroachDB is Postgres wire compatible, we use the Postgres flavor of JDBC (also known as PgJDBC). Read the CockroachDB and Spring Data JDBC Tutorial → CockroachDB and Spring Data JPA Spring Data JPA provides support for creating Java Persistence API (JPA) repositories by extending the Spring Data repository interface. This handles support for JPA-based data access layers, making it much easier to build Spring-powered applications that use data access technologies. Read the CockroachDB and Spring Data JPA tutorial → CockroachDB, Spring, and MyBatis MyBatis is an open-source, lightweight data access framework on top of the JDBC Postgres driver. Compared to other persistence frameworks, MyBatis emphasizes the use of SQL. It executes the SQL you write and converts it to object representations in Java. Read the CockroachDB and MyBatis tutorial →", "date": "2020-08-10"},
{"website": "CockroachLabs", "title": "EU Privacy Shield No Match for the Cockroach", "author": ["Jim Walker"], "link": "https://www.cockroachlabs.com/blog/privacy-shield-data-compliance/", "abstract": "On July 16, 2020, the European Court of Justice got rid of the four-year-old Privacy Shield agreement struck between the U.S. and the EU that had exposed Europeans to possible U.S. surveillance. The agreement had also allowed U.S. companies like Facebook and Google to store data about European residents outside of the region. This move is yet another great example of the EU doing “right” by their constituents and holding tech companies responsible for their users' data privacy. The news also builds on the EU’s General Data Protection Regulation (GDPR) leadership, extending its consumer protections and providing a model for the rest of the world to work from as global data privacy policies continue to evolve. The end of Privacy Shield is just a part of evolving data compliance regulations This legislation presents significant compliance complications for organizations of all sizes. In this modern environment, nearly every organization conducts business across international lines, and very few employ data centers local to each of these jurisdictions. This change may require orgs to consider this as a deployment model. But that's just one part of the problem. Even if each organization setup data centers in every jurisdiction they would still struggle to comply with this legislation as most rely on legacy database technology that requires intense operational overhead, domain knowledge, and manual intervention/process to segment data into separate repositories. And compliance is critical as these laws have teeth. Regulatory change has proven especially prickly as data regulations vary by region and non-compliance can be quite expensive. This is evidenced by the €50M fine France issued to Google back in January for failing to comply with its GDPR obligations. A paradigm shift from siloed, monolithic data to distributed systems/data has allowed many companies to gain global scale and conduct business wherever they like. But when it comes time to move data domiciled in one location to another, the task is so difficult that many may just ignore compliance in favor of eating the fines and moving on. As the issue of consumer privacy moves to the fore, this strategy of curing the symptoms rather than curing the cause will expose those who neglect to address the foundation of the problem: the database. The cloud-native advantage to data regulation compliance The global pandemic has accelerated the transition of organizations to become completely cloud-native, digital-centric businesses. Digital is no longer a goal, rather it is current state. It is requisite. And those companies that have not only transitioned to the cloud but have adopted a cloud-native approach for the infrastructure, apps, and data have an advantage right now.  They're more well-prepared for the increased reliance on the scale and globally distributed nature of this new world and are more likely to survive. How to comply with data compliance regulations? Geo-partition your data. Geo-partitioning , a foundational capability of CockroachDB has broad application to these global jurisdictional challenges. It was originally developed to meet latency requirements in globally dispersed environments, as it allows data to be tied to a location and lets the database tie data to a user location. However, this capability leads a double life and has become incredibly important for adhering to the privacy obligations as outlined in this recent legislation. Legacy technologies such as Oracle have created band-aid-like features to mask their inability to do this efficiently, but this retrofit does not allow orgs to efficiently automate and adapt to these compliance requirements. Geo-partitioning delivers a level of automation that not only allows the data to decide where it should physically reside but also gives the administrator options to modify these requirements in production. This means as you expand your business into new geographies, you won’t incur downtime. You simply alter a configuration and the database physically moves the data to where it needs to be. When you open up a new data center in a new jurisdiction, you simply ALTER TABLE to instruct the database to move the right data to this new location.  CockroachDB automates the rest.  No application changes.  No downtime. Just compliance. CockroachDB delivers geo-partitioning so you can both adhere to regional compliance obligations and preserve low latency experiences without operational complexity or inordinate costs. Physically the database would have nodes in both the US and the EU, but to applications, it would present logically as a monolithic database. While CockroachDB has had the ability since version 1.0 to replicate databases and tables with geographic constraints, geo-partitioning significantly extends this by enabling row-level control. Organizations that employ CockroachDB are simply better prepared to meet the requirements placed on data that were presented today and most likely will allow them to meet many of the modifications in jurisdiction that are still yet to come.", "date": "2020-07-17"},
{"website": "CockroachLabs", "title": "How 5G and Public Clouds Will Shape the Future of Global Applications", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/5g-global-applications/", "abstract": "This past February, before the pandemic was deemed a pandemic and New York City was still humming along, as usual, I sat down at Cockroach Labs HQ to talk about the future of data in application development. It’s a broad topic and one that’s about to undergo a fundamental and immutable shift. We’ll get to why in just a minute. In the three months since that conversation, a lot has changed in the world. COVID-19 has rendered remote work a necessity. It’s led to unprecedented levels of internet usage. And the lasting changes it will bring to the economy and tech landscape are yet to come. But instead of re-routing or altering the course of the future of data, COVID-19 is merely acting as an accelerant to the transformation that was already underway. Before the pandemic, two technological tectonic plates were moving inexorably toward one another. The first: the continued improvement and accessibility of public clouds . The second: the impending rollout of 5G. These two capabilities are about to unlock latency levels that were previously only available to Fortune 500 companies. As these latency levels become more prevalent, users will come to expect them. In a pre-pandemic world, these latency levels were a nice-to-have. But in a post-pandemic world, they’ll be a must-have. Here’s what the transformation will look like, and what application developers — and application users — can anticipate in a 5G world. Global Applications and the 100 Millisecond Rule Today people consider the utilization of public clouds as a means of replicating data across different data centers and availability zones to enhance resilience. But this strategy does not necessarily solve the problem of latency. It’s no secret that latency is starting to matter more and more. It’s a huge problem that you can’t deliver adequate latency for a user in Australia if their request has to hop all the way to Virginia in order to talk to the database, then go all the way back to the user in Australia. In fact, you can’t do that in less than several hundred milliseconds. Which brings us to the 100 Millisecond Rule. The first mention of the 100 Millisecond Rule that I encountered came from the Department of Defense. They did a study on command and control systems in which they asked the question: what’s the maximum latency from the moment a user makes some sort of action, to when that action has a visible effect? For example, when a user hits a key on a keyboard, when should you guarantee that a letter appears on the screen? The answer, according to the Department of Defense, is 100 milliseconds. When you go beyond 100 milliseconds, you’ve introduced a delay that’s perceptible to a human being. That delay can befuddle an application experience that should feel instantaneous and non-virtual. Today, companies are trying to serve a global audience with applications deployed primarily in a single availability zone. They send everyone on the planet, regardless of location, to that one place. This is the old model and it’s incapable of satisfying the 100 Millisecond Rule for a global (or even bi-coastal) user base. A global application is an application that delivers a local experience to a global audience. To understand global applications you just need to look at the companies that have built them: Google, Facebook, Netflix, Apple, and HBO. Those kinds of companies have figured out global applications. But if we were to measure the number of hours engineers have spent building that data architecture, it would add up to centuries or maybe even millennia of engineering hours. That is what it took to build global applications. And what they achieved in those centuries, if you want to boil it down, is a global customer base that has what feels like a local experience. Local Experiences Are The Next Big Opportunity Right now, it’s not uncommon when you’re clicking on your mobile device to wait seconds before the thing happens that you’re expecting to happen. And we’re used to that. At the same time, we’re ubiquitously addicted to these mobile devices. In Manhattan, everyone is bumping into each other because they’re walking around with their heads down, looking at Slack, Instagram, Twitter, Youtube, TikTok, or some other new hotness I’m too old to know about. The incredible amount of time that an average person now spends in these virtual realities, enduring a latency-laced experience, is an unbelievable opportunity that’s ripe for the next generation of application developers. People always wonder, “what’s going to be the big next platform shift?” “What’s the next iPhone?” “Is it gonna be an Apple Watch?” (Nope!) It’s the out-of-the-box infrastructure that enables developers to deliver local experiences to global audiences — a platform for the people, with the global capabilities that Netflix, Facebook, Twitter and HBO spent cumulative centuries of engineering hours building. 5G Is a Historic Leap in Network Latency Picture yourself speaking in front of a crowded room or, more prescient to today, in front of a Zoom call with hundreds of people tuned in. Think about the ways you read the faces of the people listening. Think of all the subtle cues that you use to evaluate the experience. “She yawned, she’s bored, I’m terrible.” “Oh, he’s nodding, I must be killing it — or is he just being nice?” Now imagine building that kind of interpersonal nuance into application interactions. Can you reimagine Facebook or Twitter when the interactions happen in something approaching real-time? The next generation of applications will harness the capabilities of public cloud resources and 5G to deliver incredibly low latencies and ultimately nearly real-time experiences for those using them. New applications will utterly alter the user experiences that people spend half their waking hours consuming. 5G may sound like an incremental step forward for cellular networking. But the integer jump from 4 to 5G is, in my view, a historic leap. It’s rare in telecommunication networks for there to be a drop in latency like the one that’s expected here. And it will have a tremendous impact on the ways that people re-imagine the multi-billion dollar user platforms of today. The last time we saw such a shift in speed was when we went from dial-up to DSL. For anybody who was around at that time, it was amazing. The applications that came out of that shift were tremendous. We went from text-based games to multi-user interactive applications. And now we’re at Fortnite. But there is something beyond Fortnite. And 5G will help us get there. New Consumption Models Will Empower Developers What’s more, this new generation of applications isn’t going to come exclusively from large companies with millions in VC dollars. That’s because the lessons learned at places like Facebook and Google are quickly being boiled down and packaged into general-purpose systems. It’s a formidable challenge to take on this work. But it’s happening. We’ve seen this ourselves at Cockroach Labs. Many of the startups using CockroachDB are run by ex-Google, ex-Uber, and ex-Facebook engineers. These engineers have arrived at their next project with a very different attitude than they had when they joined Google, or Facebook or Uber five or ten years ago. This generation of engineers has been through the paradigm shift. They understand distributed systems. They’re using public clouds, microservices, and modern database technology. Engineers are not going to start with infrastructure that’s going to have to be re-platformed again and again. They’re going to demand the capabilities they’ve seen at Google, Facebook, and Uber. And companies are rushing to fill that gap by providing managed service software and infrastructure-as-a-service. The Future of Application Development Application development was already on the precipice of a massive shift. And then COVID-19 hit. Consumer demand for low latency enabled technologies — and the transportive experiences they enable — is higher than ever before. Appetites for Zoom hangouts that feel like real life, for MMORPG games that are truly immersive, and for new use cases we haven’t even dreamt of yet, are running high. And the perfect storm of 5G, the improved public cloud, and managed service software is going to make that a reality. It’s a brave new world on the horizon. One in which previously unthinkable applications will be developed, not by the Facebooks and the Googles of the world, but by individual developers and small teams. The coronavirus pandemic is making our world more distributed than ever before — and the impending changes to application development will make us ready to meet that reality. This piece originally ran on The New Stack .", "date": "2020-08-03"},
{"website": "CockroachLabs", "title": "Can I Run a Database In Kubernetes?", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/kubernetes-databases/", "abstract": "About 75 percent of container orchestration is done in Kubernetes . But the popularity of the k8s platform doesn’t mean it’s easy to use in all scenarios. Kubernetes can have a steep learning curve , especially when it comes to managing state and storage. In a recent episode of “The Cockroach Hour”, Director of Product Marketing Jim Walker sat down with Keith McClellan, one of Cockroach’s Solutions Engineers, to chat about the storage and data challenges that you’ll encounter deploying Kubernetes, and how running it with CockroachDB can simplify those challenges. This blog post is a recap of their conversation about all things state and storage in Kubernetes . The full video is available to view here . Why Kubernetes Dominates Container Orchestration Keith used to work at D2iQ –back when it was still called Mesosphere–and kicked off the conversation with a discussion of how Kubernetes came to be, and why it beat out most other container orchestration platforms. D2iQ’s container orchestration scheduling platform is based on Google’s Borg structure, which predates Kubernetes. But in 2015, D2iQ started contributing to the Kubernetes project before it was released as an open source platform, which is where Keith’s experience with the project kicked off. In that time, Kubernetes quickly emerged as the top container orchestration platform. Other tools on the market were mostly operator-focused, but Kubernetes won out by making it easy for developers to deploy their applications to a cluster of machines. The other platforms are suited for operators taking their containerized apps and deploying them to a cluster of machines. That’s a fine distinction, but it matters a lot when you’re talking about how long it’s going to take you to get an application up and out and in front of people. Shifting from ETL to Kubernetes Moving to Kubernetes from one of the other platforms required a paradigm shift into distributed systems. For Keith, who started as an ETL developer and had run into scheduling issues with the ETL cluster before Kubernetes even existed, it was great to see tools to assign workloads to cluster machines and make them production-supportable. But the challenges working with large distributed database environments and data pipelining environments led Keith to the infrastructure orchestration space. When Keith was at D2iQ, one of its products used CockroachDB as the metadata layer. He could see that the next thing that needed to be fixed in the distributed computing landscape was the ability to do system of record workloads in this type of environment. Maintaining State in Kubernetes Storage in a pod a la Kubernetes is ephemeral storage – basically, a temporary file system. When you provision storage to a disk that’s local to that machine, to that pod, it lives and dies with the pod. It sticks around long enough that if something really bad happened and you need to get that state back, you have the opportunity to do that, but there’s no way to actively reattach that state to a new pod. One of the biggest challenges of Kubernetes has come around maintaining state and then maintaining consistency of that state. Arguably this is prior to Kubernetes 1.13, but persistent volumes and StatefulSets were available back to 1.8 in an alpha beta type stage. But prior to that, if your pod had some ephemeral storage, if you didn’t attach external storage to that pod, you didn’t really have state that persisted across the pod restart. As a result, there were a lot of things that needed to come to the Kubernetes ecosystem. One of the best parts of Kubernetes is one of the worst parts of Kubernetes, because it was designed to be super easy for developers. It’s a little harder to operate than some of the alternatives, and one of those things that was harder to operate was bringing in this stateful storage layer. In the 1.8 to 1.13 timeframe, they brought this concept of persistent volumes to Kubernetes, which was this idea of having durable storage available to the local node that the pod was running on, be available to the container or the pod without you having to do any kind of manual manipulations to mount file systems into the pod or any of that kind of stuff. That allows you to have more stateful applications. However, the big challenge with the persistent volumes is that they have to be manually provisioned ahead of time. They have to be an available resource that an operator has set up inside of Kubernetes. Each node would have to have a set of persistent volumes set up, and they’d all have to be mounted on that machine, and available to the Kubernetes daemons that were running on that machine, and then they could be reserved. Storage classes were designed to make this a little bit easier. Instead of directly attaching to a persistent volume, it lets nodes request storage with particular types of mounts, e.g., read once/write once or persistent volumes. Then the systems figure out how to mount it in. But there still are challenges with running persistent stores on top of a Kubernetes based ecosystem. The vast majority of persistent volumes are some type of direct attached storage, which isn’t portable. This means that once a pod with a particular persistent volume is started on a node, you can’t really move that pod without an operator intervention. They’re remote storage options that allow you to have some more storage portability. But those aren’t necessarily things you’re going to be guaranteed to have in every Kubernetes cluster. If a pod fails, and there’s a persistent volume, then that volume won’t be garbage collected unless an administrator goes in and deletes the PV. By default, the timeout for ephemeral storage is either 12 or 24 hours, depending on the distribution in Kubernetes you’re running. But for a stateful workload, you wouldn’t use that. That’s used for collecting logs from pods that are cyclically failing, but not really what we would use storage for. What StatefulSets Can Do for State A Kubernetes StatefulSet allows you to bind a persistent volume claim to a kind of a clone of a pod. Rather than every time that a pod restarts, having to manually reclaim that persistent volume, the StatefulSet maintains the claim even if the pod restarts, and manages the process of automatically remounting it into that pod. So if we’re doing a rolling restart to do a software upgrade, we don’t lose our storage. We don’t lose the persistent volume during a pod restart, to change the binary out, or do a security patch, or restart a server because we had to do an OS patch. StatefulSets are used beyond storage. It’s largely the configuration that binds the state to the pod. Anything that needs that kind of criteria gets recorded in the metadata of the StatefulSet. As far as exactly which pieces of the system are currently leveraging that infrastructure, it changes every Kubernetes release. Every time there’s a new bit of state that needs to be attached, they just add it to the StatefulSet.\nWithout StatefulSets, you would see more ephemeral patterns. For example, if you have a microservice that needs a certificate, you’d need to use something like Vault. Now, Kubernetes has its own built in certificate management, so it gets easier to manage. But figuring out where that metadata needs to be stored in Kubernetes becomes a little bit more complicated. DaemonSets and StatefulSets are different. A StatefulSet will run an arbitrary number of pods across an arbitrary number of servers. A Daemon set will run a clone of a particular pod on every server in a Kubernetes cluster. And unless you have set, what in the Kubernetes ecosystem is called a taint, unless you’ve tagged a node to not get a copy of that daemon, you’re going to run a copy of this pod on every node in your cluster. Largely it’s used for things like security agents, and sometimes you use it for east, west load balancing. Sometimes you use it for other types of admin tools, like if you have a log stash agent that needs to run on every single node, that’s what you use it for. It is an option for running stateful applications. The daemon set has the same underlying constructs as a StatefulSet, as far as maintaining an attaching state. It works a little bit differently than a StatefulSet, but you can attach a persistent volume claim as a part of the daemon set, which allows you to do a lot of the same things you can do with a StatefulSet. You can use DaemonSets with CockroachDB. There are good reasons to do so; a lot of times it depends on the type of infrastructure that you’re running your Kubernetes cluster on. So, because we can scale both horizontally and vertically, so we can either scale our individual pods up to consumer resources, or we can scale out more pods to increase our resource utilization, it depends on your workload how you’d choose to be more distributed. If you’re doing a lot of data pipelining, having a local gateway to your database on every single one of your nodes can be a really valuable thing. But Cockroach supports both patterns depending on your use case. Unless you’re running two separate CockroachDB clusters, you don’t need to run a StatefulSet and a DaemonSet at the same time. You might want to do that, if you’re sharing production and pre-production on the same Kubernetes cluster. In that case, you might want to run your production environment on a daemon set, and you might want to run a pre-production or a test environment on a smaller subset of the nodes. But in a general purpose sense, that would be a very advanced kind of edge case for this. The only thing a developer really needs to be concerned about is whether or not they have state that needs to persist. If they have state that needs to persist, then they need to care about StatefulSets, or some other mechanism for preserving that state across pod restarts. It’s mostly there to make it easy to operate these types of workloads in production. From a developer perspective, it’s 10 lines of YAML max, in your pod configuration. On the backend, there’s configuring storage classes, and or configuring persistent volumes, and managing claims and all that kind of stuff. But from a developer perspective, it’s a very easy pattern to use. Why CockroachDB on Kubernetes? It’s actually easier for the most part to operate CockroachDB on Kubernetes]( https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html ) than it is on bare metal or on statically provisioned VMs, largely because CockroachDB is a single executable that offers a common gateway to the database through every node. Every node is exactly the same. The only difference is which kind of segments of the data that that particular node happens to be managing. So if you have an intermittent failure or you’re doing a rolling upgrade, you really want that infrastructure to come back up as quickly as possible. If you’re on bare metal you’ve got to write a script or system service to make sure that the database restarts. There’s a lot of operations work that just goes away with Kubernetes. Yet there are cases where a bare metal or a virtual VM based install is recommended for a customer over a Kubernetes based install, and there are reasons why Kubernetes might be the absolute best platform for somebody. But there are even cases where customers are running across three or more data centers, which is something that’s very common, to run a single database cluster across multiple different environments. Having the common operating layer of Kubernetes makes it much easier to manage the fact that you might not have the exact same infrastructure under the covers. There’s also a tradeoff between this resiliency that you get with Kubernetes and the performance you get with bare metal. In this scenario, best case performance is going to be better on bare metal than it is going to be on Kubernetes, even though it requires more operational overhead to maintain the database on bare metal. Then it becomes a question of meeting transactional requirements in a Kubernetes based environment. In that case, CockroachDB can drastically reduce operational overhead by being in Kubernetes]( https://www.cockroachlabs.com/guides/kubernetes-statefulsets/) . Q&A Q: How do you choose when to mount something, versus when do you use local storage? A: Generally speaking, I worry about mounting persistent storage if they’re is state that I know needs to survive a pod restart of some sort. If I don’t care about state surviving a pod restart, then I’m going to use ephemeral storage, because that’s going to allow me the maximum pod portability. As soon as I start doing persistent volume claims and whatnot, that changes the mobility of those pods a little bit, and so you’ve got to pay more attention as an operator. If I can get away with not mounting storage, I’m going to not mount storage. Q: Do you have any tips and tricks on editing and managing YAML files? A: Make sure you use VI and you don’t try to use TextEdit on your Mac, because it’s going to mess up all of your spacing. There’s a tool that you can get on your Mac or in Linux called YAMLlint . Q: Why federate the operations of the cluster? Why not just federate the data? And so, could CockroachDB be used as that tool to federate workloads across multiple clusters? A: There are some exceptions to this, but if your application is largely stateless and it uses a database to maintain state, then CockroachDB could do that. Then you don’t have to connect your control planes, Kubernetes being your control plane. You still do have to do network pairing, which is not the easiest thing to do in Kubernetes. Service mesh is not necessarily the best because it’s not designed for point to point communications. It’s designed to load balance incoming requests across a number of clones of a particular pod. So you might use Istio for managing the stateless side of your application. Some other form of network pairing may need to be used because every node in the database needs to be able to talk to every other node in the database. Not because this will happen all that much, but in a failure scenario sometimes the database can’t just talk to the next nearest neighbor that happens to be hosting a copy of the data to get to a consensus.", "date": "2020-07-02"},
{"website": "CockroachLabs", "title": "How Cockroach Labs Supports Work-From-Home During COVID-19", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/remote-work/", "abstract": "As a startup, our policies and processes constantly evolve. All content in this post is true at the time of publication. For the most up-to-date information, please visit our careers page or reach out to recruiting@cockroachlabs.com Adjusting to a completely remote lifestyle to weather a pandemic has been no easy feat. For many of us, maintaining productivity has been even harder. Cockroach Labs is working hard to ensure that our people have what they need to take care of themselves and work effectively, no matter where they are. Our company was founded with a belief in establishing balance and flexibility at the core of our organizational philosophy. We say so proudly on our careers website : “We care about productivity, not face time.” This principle has driven how we put best practices in place surrounding remote work, asynchronous communication, and relocating throughout your career with us. We’re particularly proud of how this commitment has smoothed the quick transition to company-wide remote work at the height of COVID-19’s spread. Our foremost goal at the start of the pandemic was (and remains) to make sure that our team is safe and practicing vital social distancing measures. From our founding in 2015, most of our early team was based in New York City. Over time, as we began to hire Roachers in metro regions throughout the US, we spun up co-working spaces in those new cities. Later, we did the same in Toronto to bring our past interns from Canada on board! Opening these offices meant welcoming more talent from more parts of the world and we’re happy to do so. As we’ve expanded our sales team, we’ve started hiring for revenue roles in the UK, too. We will continue to evaluate new regions as we grow, but we’ve confirmed an important hypothesis: a scaling workforce can still build the world’s best distributed database. How Do We Support a Scaling Workforce? Thankfully, the commonness of remote work here or teams living across time-zones at our company means we’ve built out a ton of resources to make Roachers effective from home. Here are some of the ways we do it: Workshops and Lunch and Learns: We’re big believers that you can never stop learning. That’s why we regularly facilitate workshops on subjects such as asynchronous (or non-instant) communication, effectively leading remote meetings, and more. Individual Coaching: One of the best ways to learn is from each other. Our staff is welcome to schedule time with a member of the People team, their manager, or a remote teammate to discuss how to stay productive at home. Remote Work Knowledge Share: We regularly ask our team to share what they’ve learned about how to best work remotely and communicate digitally. Here are some examples: We ask our team to specify their usual working hours on their calendars Conducted weekly Ask Me Anything sessions with members of the People team during the initial adjustment period Building “ How to Work With Me ” guidelines to be shared on each teammate’s Slack profile Following this checklist for optimal video call quality (courtesy of Raphael Poss): Connecting your computer to a power source for full CPU performance Using a wired internet connection Closing all non-essential applications during your calls Where possible, using separate audio and video equipment to enhance call quality Technology: We see both hardware and software as an investment in making all of us better at what we do and better at collaboration. Not only can every Roacher request the tech with which they work best when they onboard, but we can help set up remote offices if you’re going to be working from home long-term! Here’s what we’re doing to support our team further during the COVID-19 pandemic: Providing a stipend to all of our staff to purchase necessities for their home office (or NODES, as we’ve taken to calling them) Reimbursing home internet costs per month while employees work from home Onboarding all new hires virtually by sending materials and new technology directly to their homes Culture and Engagement: We’ve moved all of our social hours and team-wide meetings online, including: Netflix Party Watch Night Virtual Trivia Night Volunteer home office tours (yes, even if it’s a nook in your bedroom) Slack channels: setting up a #watercooler slack channel and Google Hangout, a channel for parents to share their tips and tricks for working and keeping their children in a routine, and more! Hiring: We’re moving forward on hiring with some key adjustments, including: Fully remote interviewing, including virtual coffee chats in lieu of lunches Gathering detailed feedback on remote interviewing experiences to quickly improve Walking candidates through what remote onboarding looks like What’s Next? We’ve set up a structure to make working outside of our offices not only possible, but easy.  Since we have been scaling this practice up since day one, we’re excited to say that when this pandemic ends, our infrastructure for flexible working arrangements will be stronger than ever. We know, nonetheless, that as our staff increases in size, that won’t be without its challenges. We know that flexible policies are only as good as the support you put behind them. That’s why continued learning, knowledge sharing, and investments in resources that make our team a better one will remain cornerstones of our collaboration strategy. We know that there’s no one-size-fits-all solution for each employee and we will chart a path forward with that in mind. While the future of work and the workplace are in flux, our commitment to doing what’s best for our team remains. At Cockroach Labs, we make sure to deliver on the core values to which we’ve committed. That’s how you put people first. Want to work with the team building a better workplace? Check out our open roles !", "date": "2020-08-12"},
{"website": "CockroachLabs", "title": "Alter Column Types Without Taking Tables Offline", "author": ["Richard Cai"], "link": "https://www.cockroachlabs.com/blog/alter-column-type/", "abstract": "There are many reasons you might want to alter the schema of your database but in many databases , this process typically requires downtime. In CockroachDB, we have supported online schema changes since our first stable release,  and in v20.1, we added the ability to alter primary keys while in production without downtime. The elegant design of alter primary keys eliminates the reliance on locks so that you can continue to use tables even while they undergo schema changes. For more details, read our docs page on online schema changes . During my internship at Cockroach Labs, I tackled the problem of enabling alter column types without taking tables offline. The need to change the type of a column (alter column type) is a fairly common use case . Database schema design is difficult and requirements evolve which lead to columns potentially having to change types. Requiring downtime or having to do a roundabout, manual process whenever you want to change a column can be very annoying, especially since you’ll have to keep an eye on the process to know when each step finishes so you can proceed on with the next. This blog post covers how we implemented alter column type and challenges we overcame in the process. Early Versions of Alter Column Type Previously, we only supported ALTER COLUMN TYPE when it would not require rewriting data on disk. For example, increasing the precision of a column from INT2 to INT4 does not require any data to be rewritten. Whereas when converting INT4 to INT2, we do need a rewrite since we have to \"truncate\" data. Altering the column types of data that required an on-disk rewrite without any downtime was a really interesting challenge to tackle. Without the alter column type feature, if you wanted to change the type of your column, the workaround would be to create a new column with the new data type you wanted as a computed column of the original column.  Basically, a new column is created and once backfilled, you drop the old column and rename the new column to the name of the old column. However this is fairly tedious and the process of creating the new column can be slow depending on how much data it contains. For example, if we had table t with a column id of type INT and we wanted to change it to STRING, we would have to execute the following statements: ALTER TABLE t ADD COLUMN c_string INT; ALTER TABLE t ADD COLUMN c_string STRING AS (c::string) STORED; This creates a new column and populates the values by converting the values in column c to string. This step may take a long time depending on how many values you have in the column. ALTER TABLE t DROP COLUMN c; ALTER TABLE t RENAME COLUMN c_string to c; This early version of ALTER COLUMN TYPE was  manual and tedious, and required the developer to closely monitor the process to know when each step finishes before proceeding to the next. Implementation of Alter Column Type in CockroachDB The implementation of the alter column type online schema change closely follows the manual process described in the previous section. Here are the steps for converting column c to type t. This example is the simplest case, where the column is not part of any indexes or has any constraints. Create a new non-public column c' that has the computed expression CAST(c AS t) or c::t for short. Wait until column c is backfilled. Once backfilled, we perform the “column swap” which involves steps 3–8. These steps must happen atomically. Swap the names of c and c'. Make c a non-public column and c' a public column This is so only one of the two columns is visible to the user until the old column is dropped. Update column c’ to use computed expression c::t’ where t’ is the type of column c’. This seems strange: why do we add a computed expression to the original column that references the new column? This will be covered in this challenges section. Remove the computed expression from c. Replace the c’ in the tables column list with c. Update the new column's LogicalColumnID to the old column's ID. The LogicalColumnID represents the ordering of the columns in the table. Enqueue drop for c’. The old column is now dropped asynchronously. To get a better understanding of these steps, we can follow the stages for altering a column type from INT to STRING. In this example, we start with table t with one column id of type INT. To change column id to type STRING, we execute ALTER TABLE t ALTER COLUMN id TYPE STRING . Initial layout for table t: column_name type computed expression public logical column id id INT true 1 We create a new column id’ with computed expression id::STRING. Note that it is not public and cannot yet be seen by users. column_name type computed expression public logical column id id INT true 1 id’ STRING id::STRING false 0 Swap the names of the two columns. column_name type computed expression public logical column id id’ INT true 1 id STRING id::STRING false 0 The original column, now named id’, becomes non-public and the newly created column, now named id, becomes public. column_name type computed expression public logical column id id’ INT false 1 id STRING id::STRING true 0 Update id’ to use computed expression id::INT and remove the computed expression from id. column_name type computed expression public logical column id id’ INT id::INT false 1 id STRING true 0 Swap the positions of the two columns and update the LogicalColumnIDs. column_name type computed expression public logical column id id STRING true 1 id’ INT id::INT false 0 Lastly, the old column is dropped asynchronously and we arrive at the desired state where column id has type STRING. column_name type computed expression public logical column id id STRING true 1 Challenges for Alter Column Type: Two-Version Invariant Object Descriptors In CockroachDB, we have a concept known as the two-version invariant for object descriptors.  Descriptors are what contain the information about a particular object such as a table or database. This implies that after any schema change, there are two valid versions of a descriptor. A particular node can be using the version before the schema change or after the schema change. This case was particularly tricky to handle when it came to the implementation of alter column type. Reading from either Table Descriptor is valid and not a problem, but the challenges appeared when it came to writes. Writing with the original version of the Table Descriptor is the more straightforward of the two cases here. Any inserts into the original column will be reflected in the new column due the fact that the new column has a computed expression referencing the old one. There are still cases where the conversion is invalid because the computed expression cannot convert the old data type into the new one. One simple case is when converting STRING to INT and the STRING value is not castable to INT, e.g. \"hello\". When an insert happens using the new version of the Table Descriptor, we still have to ensure that any writes are visible to nodes reading from the previous version of the Table Descriptor. This is the reason for adding the computed expression that references the new column to the original column during the column swap. Any inserts into the new column will also be reflected in the original column due to the computed expression. In this case, if the inserted value into the new table cannot be cast back to the original type, the insert will be rejected until the schema change is fully completed. One caveat is that if the user performs an alter column type and provides an expression to use for the conversion, we cannot generally invert the provided expression in order to update the old column. In this case, all inserts into the new column are disallowed until the schema change is finished. The two-version descriptor concept is fairly confusing, so let’s illustrate it with an example. Again we’ll start with a table t with one column id of type INT. Suppose we just finished the column swap and node 1 has the version of the Table Descriptor before the swap: column_name type computed expression public logical column id id INT true 1 id’ STRING id::STRING false 0 Whereas node 2 has the pre-swap version of the Table Descriptor. column_name type computed expression public logical column id id STRING true 1 id’ INT id::INT false 0 Inserts into the id column for node 1 will insert into the original column of type INT. In this case, all inserts will succeed since we can always cast INT to STRING. Inserts into the id column for node 2 will insert into the new column of type STRING. Some inserts may fail since we cannot always cast STRING to INT. Inserting “hello” into the column will fail in this case. This insert will fail until the node moves into the next stage of the schema change, where id’ is dropped. Note that because of the two-version invariant, once any node is on the Table Descriptor version where id’ is dropped, the Table Descriptor version with the column id as type INT is no longer valid, thus inserts into the new column no longer have to worry about reflecting in the old column. Current Limitations and Future Work Currently, alter column type only works on columns that are not indexed and do not have constraints. This is because our schema changer does not currently support more complex schema changes which update multiple columns and indexes at the same time. When the target column is indexed, we would also have to update the index with the new column. The proposed way to do this is to simultaneously create a new index along with the new column and perform an index swap similarly to the column swap. We plan to support this in the future. If simplifying schema changes is your cup of tea, we've got good news: Cockroach Labs is hiring !", "date": "2020-08-20"},
{"website": "CockroachLabs", "title": "Full Text Indexing and Search in CockroachDB", "author": ["Michael Goddard"], "link": "https://www.cockroachlabs.com/blog/full-text-indexing-search/", "abstract": "In this post, I’ll skim the surface of a very common pattern in application development: full text indexing and search.  I’ll start with a bit of motivation, what prompted me to explore this using CockroachDB.  Next, I’ll introduce the initial pass at a solution, followed by a deeper explanation of how that was done, and I’ll then improve on that result by adding a “score”.  Finally, I’ll discuss the limitations of this simplistic approach, within the context of information retrieval, ending with my answer to “So, why’d you do it?”  Let’s get started. The Experiment: Build Full Text Indexing with CockroachDB Yesterday, after becoming intrigued by the idea of “follower reads” in CockroachDB , I used Google’s search to help me find material on this topic.  In the ranked search results, shown below, note the order: the “Follower Reads ...” one is first (after the ad), then there is a blog post and, finally, the one beginning “Follower Reads Topology”. Naturally, my mind drifted and I speculated about how I might attempt to implement a text search feature using the very technology which was the subject of my search: CockroachDB. As we know, CockroachDB is a horizontally scalable, geographically distributed, ACID compliant database optimized for OLTP workloads ; still, I was curious to see how it would handle this analytic use case. Well, as it turns out, it’s a tractable problem, more so if you skip building the nice UI and other amenities.  The image below depicts my initial foray into this realm.  It’s a screenshot of my DbVisualizer window, where I ran the equivalent of the Google search shown above, though here my data set was restricted to the docs for v20.2 of CockroachDB .  What I find encouraging about this is (1) the top two results match those from the Google search and (2) the query returned results in under 40 ms. I should probably explain what’s going on within that DbVisualizer window.  Overall, this is a SQL query which blends the results of two separate queries, using a common table expression (CTE), ultimately yielding a sorted list of search results.  In the top query, on lines 2 - 4, the goal is to find the rows of the “docs” table where the content column contains the given three terms: “crdb_docs”, “follow”, and “read”.  That is achieved is through the use of the “@>” operator, which ensures that the query optimizer chooses the inverted index (aka Generalized Inverted Index, or “GIN”) on the content column, thus speeding things up.  All by itself, this initial query (labeled “d” here) provides an unordered list of search results.  Not too shabby.  Here’s what this initial query looks like: SELECT idx_name, uri, n_words\nFROM docs\nWHERE content @> '{crdb_docs, follow, read}'; Let me back up just a bit.  To get to this point, I had to spin up a local CockroachDB cluster , create two tables, index certain columns within these tables, then load my HTML data into them.  Here's a link on how to spin up CockroachDB cluster , and I’ll include my DDL, here: CREATE TABLE docs\n(\n  idx_name TEXT\n  , uri TEXT\n  , content TEXT[]\n  , n_words INT\n  , PRIMARY KEY (idx_name, uri)\n);\n\nCREATE TABLE words\n(\n  idx_name TEXT\n  , uri TEXT\n  , word TEXT\n  , cnt INT\n);\n\n-- Secondary indexes\nCREATE INDEX ON docs USING GIN(content);\nCREATE INDEX ON words (idx_name, word) STORING (cnt); As thrilled as I was with these initial results, based solely on the “d” query, the novelty wore off pretty quickly.  Missing from this was some kind of ranking mechanism, to order the results in descending order of relevance.  That’s where the query marked “w” comes in.  This one runs against a “words” table that tracks the total number of occurrences of each word, in each of the documents (and, in each “index”, since I anticipated there being more than one index).  This query computes an aggregate, by combining SUM with GROUP BY .  On line 11, elements from both queries are combined to generate a normalized score for each of the search results.  This is where the CTE comes in handy, since it enables me to compute that aggregate in query “w” and later refer to it in the score computation.  Finally, on line 13, the search results are sorted in descending order, by score.  Here’s the final query, featuring the CTE: WITH d AS (\n  SELECT idx_name, uri, n_words\n  FROM docs\n  WHERE content @> '{crdb_docs, follow, read}'\n), w AS (\n  SELECT idx_name, uri, SUM(cnt) n\n  FROM words\n  WHERE idx_name = 'crdb_docs' AND word IN ('follow', 'read')\n  GROUP BY (idx_name, uri)\n)\nSELECT w.uri, (100.0 * n/n_words)::NUMERIC(9, 3) score FROM w\nJOIN d ON d.idx_name = w.idx_name AND d.uri = w.uri\nORDER BY score DESC\nLIMIT 12; Now for the inevitable “limitations and disclaimers”.  If you glance at the code in the GitHub repository for this little experiment, you’ll notice it’s a pretty simplistic approach to full text indexing.  This is sort of the classic “bag of words” approach to information retrieval.  And, it does stemming and case folding (you may have noticed the terms “follow” and “read” in the SQL query).  Though this code does incorporate “term frequency” (TF), it ignores the closely related concept of “inverse document frequency” (IDF).  Mitigating that is the use of a “stop word” list to help identify and remove frequently encountered words which contribute little to the meaning of the text (“the”, “a”, “an”, etc.).  Since I hoped to keep the total number of words in the vocabulary reasonable, I used a list of English words and only indexed terms appearing in this list.  The impact of this is that a search for “CDC” would return zero results, though a search for “ change data capture ” would work fine.  Another shortcoming is that the “term offsets” of each of the words aren’t stored, so it’s impossible to do a phrase query.  Oh, search result highlighting and/or gisting, to show the user the context within the document that triggered the “hit” in the search -- also missing here.  The last thing is that this is restricted to English language texts, though it’s not hard to imagine extending it to handle other languages. Building a successful full text indexing and search engine is quite a task.  I’ve used Elasticsearch (ES) and Solr in the past, as well as Lucene, which forms the underpinnings of both ES and Solr.  Using ES or Solr, you get a text index and search experience out of the box that provides all that I mentioned up above, and much more.  If your goal is search, I highly recommend them; you can even use CockroachDB’s change data capture (CDC) feature to set up a change feed which will keep your Elasticsearch or Solr indexes sync’d to your CockroachDB tables. Why Build Full Text Indexing in CockroachDB? Having said all that, the inevitable question is “So, why’d you do it?”  Two reasons: (1) I’m into text search; (2) I’d just finished some experiments with geospatial data, which uses the GIN indexes, so I thought about using them again.  Also, when you have in your hands an amazing cloud-native, distributed SQL DB like CockroachDB, you’re tempted to stretch a little and see how it fares when applied to some more niche data problems.  I imagine I’ll not be able to resist further experimentation with this, and I’ll update that GitHub repo as I go.  Please feel free to fork or clone that and indulge your own text search yearnings.  Finally, thank you very much for sticking with me on this brief foray into the topic of text search.  I hope you enjoyed it. Additional References SQL and Python code for this experiment (GitHub repo) Spin up a local CockroachDB Google’s stance on using images of its search product in documents Lucene’s discussion of TF*IDF for relevance scoring Inverted (GIN) indexes in CockroachDB NLTK Snowball stemmer Apache Lucene on Wikipedia", "date": "2020-08-27"},
{"website": "CockroachLabs", "title": "What is Distributed SQL? An Evolution of the Database", "author": ["Jim Walker"], "link": "https://www.cockroachlabs.com/blog/what-is-distributed-sql/", "abstract": "As organizations transition to the cloud, they eventually find that the legacy relational databases that are behind some of their most critical applications simply do not take advantage of the promise of the cloud and are difficult to scale. It is the database that is limiting the speed and effectiveness of this transition. To address this, organizations want the reliability of a tested relational data store, such as Oracle, SQL Server, Postgres, and MySQL, but with the benefits of scale and global coverage that comes with the cloud. Some have turned to NoSQL stores to try to meet these requirements. These alternatives can typically meet the scale requirements but then fall short as a transactional database because they were not designed from the ground up to provide true consistency. Recently, some of the NoSQL solutions have offered “ACID transactions” but they’re full of caveats and fail at delivering isolation levels necessary for mission-critical workloads like a financial ledger, inventory control, and identity management. Distributed SQL is a new breed of database Some of the most successful companies that function at global scale have actually sorted out this problem and purpose-built databases to handle this. The most public example of this is Google Cloud Spanner. In 2012, Google published a paper on Spanner that demonstrated a new way of looking at databases, one that was rooted in distributed systems and global scale. “Spanner is Google’s scalable, multi-version, globally distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions.” – Spanner: Google’s Globally-Distributed Database There is a lot wrapped up in the description and also a 14-page paper that goes into explicit detail of how they were able to build a consistent AND scalable database. The paper is pure genius and outlines the foundation of the next evolution of the database: Distributed SQL. Distributed SQL is a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and bulletproof resilience. What comprises a Distributed SQL database? Several attempts have been made to deliver truly scalable SQL in a distributed environment. Some have tried to retrofit existing databases to meet their needs but this ultimately does not deliver on the promise of a truly Distributed SQL database. So then, what makes up a Distributed SQL database? The requirements can be summarized into the five core conditions: 1. Scale A distributed SQL database must seamlessly scale in order to mirror the capabilities of cloud environments without introducing operational complexity. Just as we can scale up compute without heavy lifting, the database should be able to scale as well. This includes an ability to evenly distribute data across multiple distributed participants in the database. 2. Consistency A distributed SQL database must deliver a high level of isolation in a distributed environment. In a cloud-based world with distributed systems and microservices are the default architectures, transactional consistency becomes difficult as multiple operators may be trying to work on the same data. The database should mediate contention and deliver the same level of isolation of transactions as we expect in a single instance database. 3. Resiliency A distributed SQL database must naturally deliver the highest level of resiliency without any need of external tooling to accomplish this. The cloud presents an always-on environment for our workloads and the database should have the same properties. With a distributed database we can reduce the time it takes to recover from a failure down to near zero and replicate data naturally without any external configuration. 4. Geo-replication A distributed SQL database should allow for distribution of data throughout a complex, widely dispersed geographic environment. The cloud presents an ability to reach every corner of the globe with an acceptable quality of service and the database should not restrict your applications from doing so. It should perform to meet your expectations 5. SQL And while these four technical requirements are paramount, there is one key prerequisite above all. The database must speak SQL. It is the language of data and the default for all application logic. We should not have to retrain developers to use the database. They should be able to use the SQL dialect they are already familiar with. There are a few databases that meet these requirements. The list includes Spanner, of course, but you could also consider Amazon Aurora, Yugabyte, FaunaDB and CockroachDB as members of this new category. All of these members meet the requirements in some form, some better than others. Noticeably missing from this list are Oracle, Postgres, MySQL and all of the NoSQL options. While each may meet some of the requirements, none of them meet all of the requirements and cannot be considered alternatives. The devil is six: Data Locality Once you live in a distributed world, it becomes apparent that the database itself could actually take care of domiciling data. With participants located in various regions or data centers, it becomes possible to understand the location of each and then tie the data that it stores to a location. Some application architects have implemented this as part of an application but this approach is error-prone and brittle. Using the database to geo-partition data based on some field in a table is a new requirement for Distributed SQL. This allows you to use the database to address data sovereignty concerns. It can also be used to have data follow a user so you can ensure low latency access to their information or to tie data to an explicit cloud so you can minimize egress charges. And god is seven: Multi-cloud A unique trait of a Distributed SQL database is that it has semi-autonomous units that all participate in a larger system. Each unit should be able to be deployed by itself and then join the larger system, the CockroachDB cluster. This is an inherent trait that fuels the first five requirements listed above. However, this can also be used to extend the database to be truly multi-cloud. The database should not rely on a single network to accomplish distribution. It should be divorced from these limits so that a participant can be located anywhere, from any public cloud, a private cloud and even a single on-premise instance. This requirement is important for the future of compute where we live in a distributed hybrid and multi-cloud world. Foundational Requirements for Distributed SQL While the aforementioned seven requirements are unique to Distributed SQL (well, all of them except the SQL thing), it is important to note that it is still a database and, of course, is required to deliver on the baseline requirements to be a database. There are a set of expectations around the following: Administration: You should be able to easily install and configure the database with a set of command line and graphical based tools. This includes the abilities to control the environment and control data lifecycle for backup/restore. As well as the ability to create tables, define and implement schema, set indexes/partitions and recreate the DDL. Optimization: The database should allow a DBA to gain insight into the performance of queries and the ability to optimize their execution. This includes such advanced features as a cost-based optimizer, which in a distributed world becomes a tricky and new concept. Security: As with any enterprise software package, security is critical and the database should provide the key AAA capabilities of authentication, authorization, and accountability. It should not stand on its own and should integrate with a central source of truth for identity management and governance so that consistent policy can be set for the data (at the table, row and column levels) it contains. Integration: A database does not function alone and should integrate with your existing applications using well tested or known drivers. It should integrate well with existing ORMs and also provide the ability for bulk ingest or export. It should also provide key capabilities that allow it to work with ETL tools and change data capture capabilities to integrate with more advanced services such as streaming analytics or cloud storage. These “foundational” requirements are critical and signal a more mature, enterprise-ready database. They may not be the most exciting features but are critical for adoption and success within any project. How to evaluate Distributed SQL databases CockroachDB is a wonderful option for your cloud-native Distributed SQL database. It has helped hundreds of organizations transition both the very most mundane workloads and some of the most mission-critical to the cloud. It has been the foundation of a cloud-native strategy within more advanced orchestrated environments. We are quite proud of what we have built. We are also advocates of this emerging category and believe that Distributed SQL is a proper evolution of the database and the future of the way we manage data in the cloud. To this end, we feel strongly that our solution and every other should be held to the highest regard when it comes to these core requirements: Scale Consistency Resilience Geo-replication SQL Locality Multi-cloud Administration Optimization Security Integration A database that meets all of these requirements has matured to be trusted for the mission-critical (and not so mission-critical) applications in the cloud. And some of these requirements are not simple. These are advanced topics that take time to get right. When you discuss these items with your vendor, we encourage you to go deep on concepts like consistency and locality. While everyone has read the same papers, ultimately it comes down to implementation and more importantly, production use. As with any new category, this becomes a paramount concern as it is only in production where the complex corner cases and issues can be identified and fixed. So, I guess then, the final (and 12th) requirement is maturity .", "date": "2020-08-18"},
{"website": "CockroachLabs", "title": "Riding the Roachermobile: Cockroach Labs’ Internal Mobility Program", "author": ["Dave Delaney"], "link": "https://www.cockroachlabs.com/blog/internal-mobility-program/", "abstract": "As a startup, our policies and processes constantly evolve. All content in this post is true at the time of publication. For the most up-to-date information, please visit our careers page or reach out to recruiting@cockroachlabs.com. At Cockroach Labs, we like to embrace our namesake and dub programs after our favorite insect. So far, the dictionary consists of  Roacher (a Cockroach Labs employee), Roachmate (your onboarding partner), and Remoacher (remote employees). We even MOLT , which is our onboarding process— going from a nymph to an adult Cockroach. It was only natural that when we decided to launch an internal mobility program, we incorporated cockroaches into the mix. Cue the Roachermobile - where Roachers have the opportunity to scurry on over to a new role at the company. Before getting into the why, let's step back and first discuss the what. What’s an internal mobility program? An internal mobility program is a system that provides opportunities for current employees to grow their career within the company, but outside of their current team. This is distinct from a promotion, the mobility most folks first think of, which is the basis for the “career ladder” that’s so often referenced. Where a promotion can be thought of simply as moving up a rung on the ladder, internal mobility would be more akin to hopping onto another ladder and traversing a unique path. A promotion will generally mean increased responsibilities in the same discipline whereas internal mobility might mean the same level of responsibility in a new focus area. It’s a less linear style of career growth and development. Why We Developed an Internal Mobility Program Successful, high-growth companies attract high-caliber, ambitious employees who are generally on the fast track to advancing their careers. However, two key blockers can cause a hiccup in that upward momentum. First, as you get higher up on the path, there tend to be fewer available opportunities to fill. This isn’t a universal rule, but searches for senior candidates are often highly specialized, especially in leadership. Second, employees might realize that they want to go in a different direction or that they’d like to learn something new. Perhaps they were exposed to a project outside of their standard responsibilities, or they’d like to build upon existing experience by developing complementary skills. For example, a Software Engineer might become a Sales Engineer to leverage their technical background while developing client-facing skills and business acumen (you can hear Bram's story here ). Every company wants (or should want!) to retain top-performing employees. Still, if those people don’t see opportunities for growth, they might move on. Internal mobility provides an alternative avenue for them. Internal mobility isn’t just about retention. It can also be an effective resource for companies that are looking to diversify their workforce. Diversity in the workforce can mean a variety of things, from gender, to racial, to educational / experiential diversity. Beyond creating a more inclusive, welcoming, and representative company, countless studies have shown that diverse workforces outperform those that are more homogenous. This isn’t a post purely focused on diversity, so I won’t go too deep into the research, but the McKinsey article “ Why Diversity Matters ” provides a solid high-level overview of this topic and  a more in-depth insight into the subject. Building a more diverse workforce has challenges if you view it from the traditional recruiting lens. For example, women and BIPOC are underrepresented in computer science programs and in the industry. Merely trying to recruit via traditional pipeline building may not be sufficient, particularly for smaller companies without FAANG-esque companies’ resources. As a result, creating an environment centered around development, career growth, and opportunity can help companies like ours drive inclusion and belonging in the workplace. How Our Internal Mobility Program Works At Cockroach Labs, we pride ourselves on supporting the intellectual curiosity and career development of our Roachers. We offer some fantastic perks in this regard, including a Learning and Development budget of $2500 per year for each employee, free Udemy memberships, flexible work, and we encourage informal mentorship and shadowing colleagues. The next logical step was to create a path for Roachers who have developed new skills and interests and want to branch out. In the past, we did not have a structure or set of expectations in place for Roachers to transfer internally. We realized that this works for some individuals who are more vocal about their desire to make a move, but doesn’t support our entire team or create enough awareness around our support for this kind of movement.  It became obvious that we needed something formal in place. In partnership with our People Operations team, we crafted our internal mobility policy with a key principle in mind: our goal is always to hire the best candidate for every role. Though we are often looking externally to fill roles, it’s important to recognize that people on our team could be well-equipped for them. Staff who perform well and meet expectations in their current roles for at least one year qualify for transfer, though the role change is not guaranteed. This is where the Roachermobile comes in— it’s also the name for the internal job board our employees can use to apply for open roles. Roachers must then complete a full interview process assessing the same experience and skill requirements as external candidates to ensure a fair hiring process and meet the goal of hiring the best candidate for all open roles. As a company of roughly 150 people, we don’t anticipate a massive uptick in transfer requests to start.  However, we believe that it’s essential to have policies like internal mobility in place to let our team know that we care about their growth and support it, too. As we continue to grow, I’m excited to see us better retain brilliant Roachers looking for something new by hitching a ride on the Roachermobile to another stage on their career journey. Want to take the next step in your career with us? Visit our careers page for opportunities!", "date": "2020-08-24"},
{"website": "CockroachLabs", "title": "Database Consistency Models and Isolation Levels", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/database-consistency/", "abstract": "``` The other week, we devoted an episode of The Cockroach Hour to talk about the dirty secret of isolation levels . When host Jim Walker was titling the webinar, I’m pretty sure he had one particular “secret” in mind: dirty reads. But the conversation kicked off with discussion of an even more pervasive secret: most developers don’t pay attention to isolation levels. Guests Tim Veil and John Sheaffer, both Solutions Engineers at Cockroach Labs, have decades of experience developing against and working with--and for--all sorts of databases. And both of them admitted that for years, they weren’t paying enough attention to what was happening with their applications' consistency models and isolation levels. ``` I’m not saying this to throw Tim and John under the bus. I’m saying it to illustrate that even very senior devs don’t always understand this topic, because for the most part, it’s easy to ignore. As Tim says in the webinar, most developers think “ If the database is up and running, why should I care about isolation levels?” In this recap of the consistency models webinar , we’ll explore what common database isolation levels mean, and why you should care. It’s an easily and widely misunderstood topic, but one that’s hugely important to security, performance, and most of all: data correctness. Pop Quiz: ACID Transactions The conversation started out with a review of ACID semantics, and why they matter. When a database checks off the four components of ACID--atomicity, consistency, isolation, and durability, it’s said to be ACID compliant. ACID compliance guarantees valid database transactions even in the face of unexpected errors, like network errors, hardware failures, and other outages. At a first look, this characterization can make it sound like all ACID compliant databases are created equal. As they discuss in the webinar, this couldn’t be further from the truth. We’ll dive into the conversation around that in a bit, but first, let’s review what each of the components of ACID means: A: Transactions are Atomic Atomicity guarantees that transactions complete (or fail) as atomic units. If a given statement that comprises a transaction fails to complete, the entire transaction will fail. C: Transactions are Consistent Consistency requires that all database transactions only affect the data in previously agreed upon rules. These rules vary a lot based on the database’s consistency model (more on this in a bit). I: Transactions are Isolated In ACID semantics, isolation is about concurrency control. Depending on the isolation model, it makes different contracts about what happens to your data, and when any changes made by one operation become “visible” to other transactions. D: Transactions are Durable Durability guarantees that any updates last. They become permanent and stored in memory. All four elements need to work in concert. There’s not a lot of wiggle room with the A, C, and D components of ACID. As Tim says in the conversation, you can more or less think of those components as binaries. A transaction is durable, or it’s not. A transaction is isolated, or it’s not. A transaction is atomic, or it’s not. But when it comes to consistency? That’s a different story. Consistency, unlike the other ACID semantics, is a somewhat hand-wavey spectrum, filled with increasingly more consistency models that it’s really important for developers to understand. What are Database Isolation Levels? Database isolation levels are a contract by the database about what side effects and anomalies you might see from any given transaction. Some common ones (starting with the strictest level) include serializability , read committed, read uncommitted, and repeatable reads. These have different implications for your data. For example, a repeatable read isolation level states that once a transaction has read some data, reading it again within the same transaction yields the same results. Concurrent transactions modifying that data have to somehow not affect the execution of our reading transaction. However, this isolation level allows the Phantom Read anomaly . Basically, if a transaction performs a query asking for rows matching a condition twice, the second execution might return more rows than the first. These anomalies have serious implications. Recent research at Stanford has explored the degree to which weak isolation leads to real-world bugs. CockroachDB’s consistency model is somewhere between serializable and linearizable. If you’re new to this topic and want to go a bit deeper, Wikipedia offers a great overview of database isolation levels , and Jepsen’s consistency models page is another good resource. Isolation levels map. Source: Jepsen.io To learn more about how to choose an isolation level that makes sense for your application, watch the full webinar.", "date": "2020-09-03"},
{"website": "CockroachLabs", "title": "How We Built a CockroachDB Dialect for Hibernate", "author": ["Jordan Lewis", "Nathan VanBenschoten", "Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/building-support-for-java-orm-hibernate-in-cockroachdb/", "abstract": "This post was originally published in 2017, upon announcing that CockroachDB’s support for Hibernate was in beta. Today, we’re excited to announce some big news: the CockroachDB dialect for Hibernate is officially available! 🥳 Hibernate now offers first-class support for CockroachDB. You can read more about the dialect, and our journey to get there, in this blog post. We’re working hard to make CockroachDB scalable and robust, but simple to use. One way we’ve approached this is by adding support for existing object-relational mappers (ORMs), which make it fast and easy to develop applications that interact with CockroachDB. To determine which ORMs to support first, we asked customers and users of our community Slack , conducted developer surveys , and performed other research. Hibernate came up most often, likely because of Java’s large developer community and Hibernate’s popularity within that world. This post is primarily about our Hibernate compatibility, but a bunch of the SQL compatibility work we initially did helped pave the path toward other ORM compatibility as well . Phase 1 (2017): Bootstrapping ORM Support When we first started working on this project, we had an ambitious goal of getting support for five ORMs off the ground. Hibernate was at the top of our list, and there were two potential paths forward: Implement the SQL features needed for Hibernate to function. This was feasible, because Cockroach supports the Postgres wire protocol and the majority of its syntax. Because many ORMs support PostgreSQL, this option involved adding SQL features that ORMs need but CockroachDB was lacking. Create a CockroachDB adapter for each ORM. Though SQL has a set of standards, each commonly used database has significant deviations from standards and/or proprietary SQL features. This requires ORMs to have an adapter for each database it connects to. So, option 2 was to create an extension that performed ORM-specific tasks using features CockroachDB already supports. In 2017, we wanted to release Hibernate support quickly. Plus, after some investigation, we discovered significant overlap in the SQL features other ORMs need. So, we went with option #1 at first. At the time, we decided that leveraging prebuilt adapters by fitting the interface they expected would be less effort in the long run than creating a custom adapter for each individual ORM or tool. So, what are those features? ORMs issue two basic types of queries. The first set consists of CRUD (CREATE, READ, UPDATE, DELETE) queries. This was relatively straightforward given our existing implementations of those SQL statements. Minor issues existed, such as incompatible date and time formats, but were simple to fix. The vast majority of the work involved supporting schema synchronization. Schema Synchronization One of the important responsibilities of an ORM is making sure that the tables, columns, and indices in the database are consistent with the models as defined by the code. For example, consider this Hibernate ORM model: private static final SessionFactory sessionFactory =\n            new Configuration()\n                    .configure(\"hibernate.cfg.xml\")\n                    .addAnnotatedClass(Account.class)\n                    .buildSessionFactory();\n\n@Entity\n@Table(name=\"accounts\")\npublic static class Account {\n    @Id\n    @Column(name=\"id\")\n    public long id;\n\n    @Column(name=\"balance\")\n    public long balance;\n} During startup, Hibernate performs the following actions: Creates the accounts table if it doesn’t exist. Hibernate “knows” about the table because of the function call addAnnotatedClass(Account.class) . Adds the id column to accounts if it doesn’t exist and makes it the primary key for accounts . Adds the balance column to accounts if it doesn’t exist. Hibernate knows how it wants the accounts* table to be structured based on the @Table , @Column , and @Id annotations. Now, it needs to know if accounts already exists and, if it does, what its current schema is. Schema Synchronization: pg_catalog vs. information_schema Schema synchronization makes the database schema consistent with the models (e.g. Account in the above example). To do this, Hibernate must retrieve the database schema through the mechanisms provided by the database. information_schema is a standard set of views that SQL defines to expose the structure of a database for introspection by clients. Unfortunately, the ORMs we’ve tested do not use this when connected to PostgreSQL. Instead, they use the relations in the PostgreSQL-specific pg_catalog schema. So, we’ve implemented the subset of pg_catalog views needed for Hibernate. This allows a Hibernate app to connect to CockroachDB as though it were PostgreSQL. This process was relatively straightforward, but involved significant changes to SQL execution , the type system , and a variety of other code . It also exercised parts of our SQL engine that were previously underused. For example, consider this query that Hibernate issues while running a sample app: SELECT NULL::text AS PKTABLE_CAT,\n       pkn.nspname AS PKTABLE_SCHEM,\n       [15 lines omitted]\n       CASE con.confdeltype\n           WHEN 'c' THEN 0\n           WHEN 'n' THEN 2\n           WHEN 'd' THEN 4\n           WHEN 'r' THEN 1\n           WHEN 'a' THEN 3\n           ELSE NULL\n       END AS DELETE_RULE,\n       [8 lines omitted]\nFROM pg_catalog.pg_namespace pkn,\n     pg_catalog.pg_class pkc,\n     pg_catalog.pg_attribute pka,\n     pg_catalog.pg_namespace fkn,\n     pg_catalog.pg_class fkc,\n     pg_catalog.pg_attribute fka,\n     pg_catalog.pg_constraint con,\n     pg_catalog.generate_series(1, 32) pos(n),\n     pg_catalog.pg_depend dep,\n     pg_catalog.pg_class pkic\nWHERE pkn.oid = pkc.relnamespace\n  AND pkc.oid = pka.attrelid\n  AND pka.attnum = con.confkey[pos.n]\n  AND con.confrelid = pkc.oid\n  AND fkn.oid = fkc.relnamespace\n  AND fkc.oid = fka.attrelid\n  AND fka.attnum = con.conkey[pos.n]\n  AND con.conrelid = fkc.oid\n  AND con.contype = 'f'\n  AND con.oid = dep.objid\n  AND pkic.oid = dep.refobjid\n  AND pkic.relkind = 'i'\n  AND dep.classid = 'pg_constraint'::regclass::oid\n  AND dep.refclassid = 'pg_class'::regclass::oid\n  AND fkn.nspname = 'public'\n  AND fkc.relname = 'orders'\nORDER BY pkn.nspname,\n         pkc.relname,\n         con.conname,\n         pos.n It’s reasonable for you to have looked at this query for a few minutes and still not understand what it’s trying to do. This query retrieves the foreign keys for the orders table in the public schema, using the relations in pg_catalog. Hibernate is not unique in using pg_catalog instead of information_schema. Several other ORMs, including the other four we prioritized in our 2017 round of ORM compatibility use pg_catalog as well. Fortunately, we also confirmed that these ORMs mostly use the same parts of pg_catalog that Hibernate does. Phase 2 (2020): Building a Cockroach/Hibernate Dialect We’ve improved CockroachDB’s Postgres compatibility a lot since that initial beta support for Hibernate. Even so, we wanted to ensure that Hibernate has first-class support for CockroachDB, and that we could work around some of the lesser-used Postgres compatibility issues that aren’t coming up on our roadmap. That’s why, in parallel to our SQL compatibility improvements, we decided to create our own Hibernate dialect. A CockroachDB Hibernate dialect allows us to provide a better developer experience by introducing CockroachDB-specific semantics. Compared to the Postgres dialect, the CockroachDB dialect has automatic handling for missing BLOB storage and Identity column support. We worked with the Hibernate maintainers, and the CockroachDB dialect is part of the official Hibernate release as of v 5.4.19 . Going forward, our team will also have the opportunity to further extend the CockroachDB dialect with CockroachDB specific features. And because the dialect is in the main Hibernate repo, it’ll be maintained in perpetuity, and the integration into Hibernate’s main release helps us provide better ongoing support. Thank you to the Hibernate team for all their hard work on helping us get this merged! Next Steps for ORM Compatibility In addition to Hibernate, we’ve also implemented support in CockroachDB for ActiveRecord (Ruby) , SQLAlchemy (Python) , GORM (Go) , and TypeORM (Typescript). A full list of ORMs and other third-party database tools that CockroachDB supports is available here . Want to use another ORM? We may already be working on it. You can first search for the ORM in our issue tracker . If you find a relevant issue, please voice your support with a GitHub reaction . Pull requests to add ORM-related features are greatly appreciated. CockroachDB Core is open-source and benefits significantly from community pull requests. If that’s not feasible, please file an issue that includes a test script or unit test that uses the ORM. It’s easier to prioritize support if we don’t first have to invest in understanding it and its incompatibilities. If you want to build an application with Hibernate ORM and CockroachDB, get started today by building a Java app with Hibernate .", "date": "2020-09-17"},
{"website": "CockroachLabs", "title": "How to Run a Software as a Service on Kubernetes", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/kubernetes-saas-implementation/", "abstract": "The first version of CockroachCloud, our database-as-a-service product, had our users fill out a Google doc with their cloud deployment preferences. We’ve come a long way since that initial proof of concept (including, yes, building a UI). More importantly, we’ve put implementation patterns in place that make something complicated (like configuring a cloud deployment) scalable. As we’ve written about before, choosing to run CockroachCloud on Kubernetes is a huge part of that implementation pattern. In a recent episode of The Cockroach Hour, CockroachCloud SREs Juan Leon and Josh Imhoff sat down to talk about some of the tools and processes we use to make that happen. While some of the information is specific to CockroachCloud, many of the implementation patterns covered in the webinar are applicable to anyone considering running their SaaS product on Kubernetes. In this blog post, we’ll recap some of those lessons, from the unexpected benefits of running on Kubernetes to the provisioning and certification platforms we use alongside it. Why We Use Kubernetes for our SaaS Product About a year ago, Josh wrote a blog about why the SRE team started running CockroachCloud on Kubernetes . There are a lot of different benefits to Kubernetes. Early on, we really cared primarily about the automation and orchestration abilities (more on that, below). But as time goes on, we’ve found more and more reasons to love K8s. Automation and Orchestration Primitives CockroachCloud is a managed database as a service, and we needed our customers to be able to scale their CockroachDB clusters up and down at the drop of a hat. Therefore, the powerful automation primitives inherent in Kubernetes was really top of mind for us. When we were just starting CockroachCloud, we did this workflow with Terraform. Terraform spun up the VMs for us, and then CockroachDB on those VMs with supervisor scripts and a touch of homegrown automation. And the benefits of that was it was very simple. You just have the VM and then just Cockroach process running. That's it, which the SRE team liked. There's not tons of moving pieces they got to understand. But the cost was high: it was rather hard to do orchestration tasks. But we knew we needed to automate these tasks--while maintaining a high level of reliability--and we didn't want to build that ourselves. Kubernetes was the best way to do that, and does an excellent job. Bin Packing Another main benefit to deploying our software as a service on Kubernetes is its bin packing capabilities. Kubernetes lets you pack a bunch of containers on a VM in a way that improves resource utilization. This is one of the motivating reasons for Google's Borg (a tool both Josh and Juan had used previously). Google realized how much resources they could save by bin packing containers through virtual machines. When we were initially building CockroachCloud, we didn't care about bin packing at all. We just wanted to run one Cockroach node per VM, because we wanted the whole VM for CockroachDB. We didn't even need that benefit of Kubernetes. And there's a lot of complexity that comes from those two requirements being implemented by Kubernetes and we were taking on that complexity in order to get only some of the benefits of Kubernetes almost. Kubernetes Provides a Common Interface Another big benefit we didn't totally expect when first using Kubernetes is the simplicity a common interface offers. Right now, CockroachCloud runs on GCP and AWS, and we have plans to expand. Kubernetes offers a consistent way of running production across clouds. And that's powerful. Other Tools We Use for CockroachCloud For Kubernetes monitoring: Prometheus and Grafana As far as monitoring goes, we run Prometheus clusters, Prometheus Alertmanager and Grafana in each customer cluster on Kubernetes. So in a given Kubernetes cluster we have dedicated to customer A, we're running Prometheus Alert Manager and Grafana. If it's a multi-region cluster , each region's running Prometheus and they're scraping all the regions, so it's replicated. Then we have a meta-monitoring Prometheus instance that’s just sitting outside of the cluster. It's going to page if the problems for the customer abound. For Certificates: Vault We also needed to automate certs when scaling CockroachCloud. We use HashiCorp’s Vault (and Kubernetes) for that. Vault is a really powerful secret store. If you have a secret that you want to just store in a specific place, just write a key to Vault, and Vault will keep it and handle encryption for you. It also serves as our certificate authenticator, and manages and distributes project owner tokens. We run Vault in a control plane that's used to manage all the clusters. That way, Vault's not in the serving path. Rhwn, we use secrets within each instance to manage the certificate. Alternatively, we could have had CockroachDB talk directly to Vault, but that would introduce security issues. Because then the customer cluster has access to this thing that has keys for all the customers. Vault writes things to Kubernetes, but the customer’s cluster--since it’s accessible by the customer--cannot talk to Vault. For Provisioning Hardware: Pulumi We were initially depending on Terraform to provision hardware. But again, we had this future in our heads of service that creates clusters and adds regions to existing clusters and all of that. And we didn't really want our code to be generating Terraform configs, that then get executed by Terraform. We just thought that was a little messy. So we looked at Pulumi, which is very core to Terraform, except that you can configure it with Go code. We’re a Go shop, which made it a great fit for us. Conclusion Your implementation patterns will no doubt vary from CockroachDB’s. But the lessons Josh, Juan, and Jim cover in this webinar are relevant to anyone running--or considering running--their software-as-a-service on Kubernetes. For more details on gotchas and lessons learned (plus a quick conversation about what Kubernetes can do it a serverless world), watch the full webinar here.", "date": "2020-09-14"},
{"website": "CockroachLabs", "title": "NoSQL vs. NewSQL vs. Distributed SQL: DZone's 2020 Trend Report", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/dzone-sql-trend/", "abstract": "Databases are evolving. For the past decade, we’ve read thinkpiece after thinkpiece taking firm stances on the “SQL vs. NoSQL” debate--some of which declaratively pronounce the death of SQL or the death of NoSQL. In DZone’s annual report on SQL v NoSQL database usage , we’re excited to see that we’re moving beyond that paradigm. What we’re seeing in this report is more nuanced, and a lot more exciting: the death of the “SQL vs. NoSQL” binary altogether. This is happening for a couple reasons. For one, more and more companies are using a combination of both for their business needs. They do not have one monolithic database--they run specific applications on the tools that best suit that workload. 58% of the companies surveyed in the report are using a combination of both. DZone's report finds that the use of SQL and NoSQL are neck and neck in the world of big data. But it is no longer strictly a winner-take-all competition. Instead, the research indicated that we’re moving toward a world in which the hybrid use of both database types is becoming the norm: nearly 60% of respondents said their organizations use both SQL and NoSQL to support big data efforts. In the report, interviewees at Oracle, Cockroach Labs, and more back up this research with what they’re seeing on the ground with customers. Another key insight: many organizations rate their own usage of NoSQL as “Bad” or “Very Bad,” especially ones who started using NoSQL when it first came to market . It’s indicative of what CIOs state in the report's interviews: companies need to align on a core set of use cases and business goals in deciding their tooling, rather than just pick the trendiest new technology. A New Paradigm: Distributed SQL But there’s another reason for this massive shift. Whereas companies used to need to make important--and difficult--tradeoffs in choosing between a SQL or NoSQL database, there are new products in the distributed SQL category that don’t require these tradeoffs. Distributed SQL databases, like CockroachDB, are delivering the benefits of both SQL and NoSQL. They deliver the ease of use of SQL--no more “Very Bad” ratings coming in--alongside the scalability and resilience capabilities that come hand in hand with NoSQL. An important caveat to note, when you’re shopping for a distributed SQL database: not all technologies labeling themselves as such actually meet the definition of a distributed SQL database . About the DZone Evolution of the Database Report DZone’s annual report is an attempt to answer the age-old “SQL or NoSQL” question. To do so, they compile original research from the DZone team as well as thought leadership from executives at Oracle, Cockroach Labs, and more. [ Read the Report ]", "date": "2020-09-09"},
{"website": "CockroachLabs", "title": "Introducing Pebble: A RocksDB Inspired Key-Value Store Written in Go", "author": ["Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/pebble-rocksdb-kv-store/", "abstract": "Since its inception, CockroachDB has relied on RocksDB as its key-value storage engine. The choice of RocksDB has served us well. RocksDB is battle tested, highly performant, and comes with a rich feature set. We’re big fans of RocksDB and we frequently sing its praises when asked why we didn’t choose another storage engine. Today we’re introducing Pebble : a RocksDB inspired and RocksDB compatible key-value store focused on the needs of CockroachDB. Pebble brings better performance and stability, avoids the challenges of traversing the Cgo boundary , and gives us more control over future enhancements tailored for CockroachDB’s needs. In our upcoming 20.2 release this fall, Pebble will replace RocksDB as the default storage engine. This is the story of why we’ve written Pebble, and how we changed such a foundational component of CockroachDB. Motivation The storage engine is a critical component of a database, providing the foundation for performance and stability. Traditional SQL and NoSQL databases have often been built with their own proprietary storage engines. MySQL uses InnoDB, Postgres comes with internal B-tree, hash and heap storage systems, Cassandra comes with an LSM tree implementation. Recently, some of these databases have added RocksDB backends (e.g. MyRocks and Rocksandra ). From a distance, this gives the perception that RocksDB is eating the low-level storage ecosystem. A closer inspection reveals the RocksDB backends for these existing systems come with significant caveats . When building any complex piece of software, it is impossible to build every component from scratch. Reusing existing components enables faster time to market, and often a better product as domain experts have taken the time to craft and tune the individual components. This was certainly true of our choice to use RocksDB, yet over time the calculation changed. RocksDB is used by many different systems. This wide usage implies significant testing and performance tuning, but it also means RocksDB is serving many masters. We can see the effect of this in RocksDB’s very large feature set and configuration surface area. The RocksDB code base has sprawled over time, growing from LevelDB’s original 30k lines of code to a current state of 350k+ lines of code. Lines of code is an inadequate metric, but these sizes do provide a rough feel for the relative complexities. RocksDB has been a solid foundation for CockroachDB to build upon. Unfortunately, as CockroachDB has matured we’ve encountered serious bugs in RocksDB. For example, RocksDB had a bug in compaction related code that led to an infinite cycle of compactions for a particular sstable, starving other parts of the LSM tree from being compacted. While the absolute number of bugs we’ve encountered in RocksDB is modest, their severity is often high, and the urgency to fix them is frequently House Is On Fire. This has required Cockroach Labs engineers to dive deep into the RocksDB code base as part of bug investigations. Navigating 350k+ lines of foreign C++ code is doable (we’ve done it), yet hardly what could be described as a good time. CockroachDB is primarily a Go code base, and the Cockroach Labs engineers have developed broad expertise in Go. C++ expertise is much sparser, and the barrier between Go and C++ is psychologically real. The barrier prevents usage of the native Go profile tools from introspecting C++, or from seeing C++ stack traces. We’ve had to write significant amounts of logic in C++ in order to avoid the performance overhead of frequent crossings from Go to C++, at times duplicating logic that already existed in Go. RocksDB is generally highly performant, but we’ve also encountered significant performance problems. CockroachDB was an early adopter of range deletions , but we were also early discoverers of some performance deficiencies in the first implementation. We upstreamed performance fixes for range deletions and aided in the design of the v2 implementation . RocksDB is full featured, but sometimes the features have deficiencies. At times we have chosen to work around those deficiencies in CockroachDB code rather than fix them in RocksDB. These decisions were not necessarily made consciously (see above regarding the psychological barrier between Go and C++). An example of such a workaround is the CockroachDB Compactor . The Compactor is used to force compaction of a portion of the data in RocksDB which has recently been deleted via a DeleteRange operation. This allows the disk space to be recovered more quickly than if we did nothing. The need for the Compactor stems from RocksDB not taking range deletion operations into consideration in its compaction decisions. Stepping back from the low-level details, the takeaway is that the storage engine has a critical impact on the functionality and behavior of CockroachDB. Owning the storage layer allows CockroachDB more direct control of its destiny. A critical reader may point out that several of the points above do not lead to the conclusion of reimplementing RocksDB. We could have instead chosen to build out internal expertise. We could have chosen to fork RocksDB, strip away the parts that we don’t need, and make enhancements tailored to the needs of CockroachDB. This latter approach was given serious consideration, but ultimately we came down in favor of reimplementing in Go as we believe removing the Go / C++ barrier will enable faster development long term. A final alternative would be to use another storage engine, such as Badger or BoltDB (if we wanted to stick with Go). This alternative was not seriously considered for several reasons. These storage engines do not provide all the features we require, so we would have needed to make significant enhancements to them. The migration story for CockroachDB clusters running RocksDB would have become vastly more complex, making it likely that we’d need to support both storage engines for a considerable amount of time. Supporting multiple storage engines is itself a large endeavor: it dramatically increases the testing surface area, and the alternative storage engines often come with significant caveats (e.g. MyRocks does not support SAVEPOINTs ). Lastly, various RocksDB-isms have slipped into the CockroachDB code base, such as the use of the sstable format for sending snapshots of data between nodes. Removing these RocksDB-isms, or providing adapters, would either be a large engineering effort, or impose unacceptable performance overhead. Building Pebble Replacing a component as large as RocksDB is a daunting task. We did have a few advantageous factors: We understood CockroachDB’s usage of RocksDB intimately. Pebble does not aim to be a complete replacement for RocksDB, but only a replacement for the functionality in RocksDB used by CockroachDB. A ballpark estimate is that this reduces the scope of the replacement task by at least 50%. The Pebble code base currently weighs in at a bit over 45k lines of code and another 45k lines of tests. This is a fraction of the RocksDB code size, and a big reason for that is that we’re not replicating all of the RocksDB functionality. We were not starting from scratch. A Go port of LevelDB was started a few years ago, but never completed. Very little of this starting point remains in Pebble, yet it did lay out the initial skeleton and provide the early code for reading and writing the low-level file formats. We can refer to RocksDB’s code as an implementation template. For example, while the low-level RocksDB file formats are not formally specified, the RocksDB code provides more than adequate documentation of these formats. Reusing the RocksDB file formats removes a degree of freedom from the Pebble design, but this is not an onerous constraint. This point is about more than just file formats, though. We can take inspiration and ideas from all parts of the RocksDB code. The API and internal structures of Pebble resemble RocksDB. Pebble is an LSM key-value store which provides Set , Merge , Delete , and DeleteRange operations. Operations can be grouped into atomic batches. Records can be read individually via Get , or iterated over in key order using an Iterator . Lightweight point in time read-only Snapshots provide a stable view of the DB. Internally, the data in Pebble is stored in a combination of Write Ahead Logs (WALs) and Sorted String Table (sstables). Recently written data is buffered in memory in a series of Memtables which are implemented under the hood by an arena-backed concurrent Skiplist. Memtables are flushed to disk to create sstables. Sstables are periodically compacted in the background. Both the compaction mechanics and heuristics in Pebble are similar to those present in RocksDB (at least for the configuration used by CockroachDB). Anyone familiar with RocksDB internals will see many similarities in the Pebble code. There are also many differences. We’ve documented some of the bigger ones . For example, the range deletion implementation is quite different from the one in RocksDB which enables more optimizations to skip over swaths of deleted keys during iteration. The handling of indexed batches is completely different which enables the Pebble implementation to support indexing of all mutation operations, while RocksDB currently does not (e.g. RocksDB does not support indexing of range deletions in batches). These examples are not meant as a critique of RocksDB. We fully expect some of the good ideas in Pebble to be picked up by RocksDB, just as we’ll continue to pluck good ideas from RocksDB. Functionality Pebble implements the subset of RocksDB functionality used by CockroachDB. We have no aspirations to eventually include every feature in RocksDB. In fact, quite the opposite is true. We intend to filter every feature addition and performance improvement through the criteria of whether it will be useful to CockroachDB. This is a harsh filter for a general purpose key-value storage engine, but that is not Pebble’s goal. So what functionality does Pebble include? Basic operations: Set, Get, Merge, Delete, Single Delete, Range Delete Batches Indexed batches Write-only batches Block-based sstables Table-level bloom filters Prefix bloom filters Checkpoints Iterators Iterator options (lower/upper bound, table filter) Prefix iteration Reverse iteration Level-based compaction Concurrent compactions Manual compaction Intra-L0 compaction SSTable ingestion Snapshots RocksDB functionality Pebble does not include: Backups Column families Delete files in range FIFO compaction style Forward iterator / tailing iterator Hash table format Memtable bloom filter Persistent cache Pin iterator key / value Plain table format SSTable ingest-behind Sub-compactions Transactions Universal compaction style Some of the items above might cause raised eyebrows. How does Pebble not include support for Backups or Transactions given that CockroachDB provides support for both? CockroachDB’s implementation of Backups and Transactions have never used the Backup and Transaction facilities in RocksDB. Transactions on a local key-value store are not needed to implement distributed transactions. Rather, CockroachDB uses Batches, which provide atomicity for a set of operations, as the base upon which to build distributed transactions. Bidirectional Compatibility We decided early on for Pebble to target bidirectional compatibility with RocksDB for the initial release of Pebble. More precisely, Pebble is currently bidirectionally compatible with RocksDB 6.2.1 (the version of RocksDB currently used by CockroachDB) for the subset of RocksDB functionality used by CockroachDB. Bidirectional compatibility means that Pebble can read a RocksDB generated DB, and RocksDB can read a Pebble generated DB. Compatibility with RocksDB enables a seamless migration to Pebble, simply requiring a Cockroach node to be restarted with a new command line flag: --storage-engine=pebble . Bidirectional compatibility enables an additional level of safety: if a problem is encountered when using Pebble, we can switch back to using RocksDB. Bidirectional compatibility also enables an additional level of strictness in testing which is discussed more in the Testing section. Note that bidirectional compatibility with RocksDB will disappear at some point. Maintaining such compatibility forever is at odds with our desire to enhance Pebble in the service of CockroachDB. Maintaining compatibility with new RocksDB functionality would be an enormous ongoing burden. Testing The storage engine is the component of a database that is tasked with durably writing data to disk. Bugs in the storage engine tend to be severe, such as data corruption, and data unavailability. Testing of the storage engine needs to be robust. Testing of Pebble would best be described as layered. The current testing layers are: Pebble unit tests Randomized tests (a.k.a metamorphic tests) Bidirectional compatibility tests CockroachDB unit tests CockroachDB nightly tests (a.k.a. roachtests) Unit Tests The first layer of testing is a large number of Pebble unit tests. These unit tests aim to test all of the normal cases and the corner cases. Listing out all of the corner cases is a challenging exercise. Even a diligent engineer can miss a corner case. Even more problematic is that small changes to the code can introduce new corner cases. It would be nice to believe we’d identify those new corner cases when making any change, but our experience suggests otherwise. Randomized Testing Randomized testing is a solution to the corner case problem that has been embraced in recent years. Fuzz testing is an example of randomized testing that is often used to check parsers and protocol decoders. For Pebble, rather than trying to explicitly enumerate all of the corner cases, we can instead write a test which randomly generates operations. The natural question arises: how do we know if the results of the operations are correct? With fuzz testing we simply look for program crashes. This is also the first line of checks in Pebble’s randomized testing which we further enhance with invariant checks for certain critical internal data structures. Simply looking for crashes and invariant violations is a bit unsatisfying. We’d like to know if the results of the operations are actually correct. Maintaining a separate model for the expected result of the operations is a daunting task as the data model implemented by Pebble is much more than just an ordered map of keys and values due to the presence of snapshots (both implicit and explicit) and range deletions. The solution is metamorphic testing . We randomly generate a series of operations, and then execute those operations multiple times against different configurations of Pebble. The output of the different runs is compared and any differences are a cause for concern. The Pebble configuration knobs that we tweak include the size of the block cache, the size of the memtable, and the target size of sstables. Changing these configuration operations causes different internal code paths inside Pebble to be executed. For example, changing the target size of sstables causes different scenarios in the handling of range deletions. At the time of writing, each instance of the metamorphic test is run against 19 predefined configurations and 10 randomly generated configurations. We’ve actually implemented two different versions of metamorphic tests. The first operates purely on Pebble APIs and only tests Pebble against itself. You might be thinking: why not also test against RocksDB? We had that same thought. Unfortunately, the Pebble API’s have some slight differences and generalizations in comparison to RocksDB that made this challenging. Instead, we implemented a second metamorphic test that works at the integration layer of Pebble/RocksDB within CockroachDB. This second metamorphic test verifies not only that Pebble and RocksDB produce identical results, but also that the Pebble and RocksDB specific glue code inside CockroachDB produces identical results. The metamorphic tests have proved incredibly useful in finding existing bugs, and quickly catching regressions when new functionality has been introduced. Crash Testing A key attribute of a storage engine is to durably write data to disk. In order to provide a useful foundation for higher levels to build on, Pebble and RocksDB allow a write operation to be “synced” to disk, and when the operation completes the caller can know that the data will be present even if the process or machine crashes. Testing crash recovery is an interesting challenge. In Pebble, we’ve integrated crash testing with the metamorphic test. The random series of operations also includes a “restart” operation. When a “restart” operation is encountered, any data that has been written to the OS but not “synced” is discarded. Achieving this discard behavior was relatively straightforward because all filesystem operations in Pebble are performed through a filesystem interface . We merely had to add a new implementation of this interface which buffered unsynced data and discarded this buffered data when a “restart” occurred. Bidirectional Compatibility Testing As discussed earlier, Pebble targets bidirectional compatibility with RocksDB. In order to test this compatibility, the metamorphic test was again extended. The “restart” operation was changed to randomly switch between Pebble and RocksDB. This testing has caught several incompatibilities between Pebble and RocksDB, such as Pebble incorrectly setting a property on sstables that caused RocksDB to interpret those sstables differently from Pebble. In addition to compatibility testing in the metamorphic test, we also implemented a CockroachDB-level integration test which mimics what a user might do to verify bidirectional compatibility. This test starts up a CockroachDB cluster, and then randomly kills and restarts nodes in the cluster, switching the storage engine being used. The types of bugs discovered in this testing varied from trivial differences to the most serious types of data corruption. An example of the latter was an extremely subtle difference in the hash function used by the bloom filter code: extending a signed 8-bit integer to 32-bits results in a different value than extending an unsigned 8-bit integer to 32-bits. This caused Pebble’s bloom filter hash function to produce different values than RocksDB’s bloom filter hash function for a subset of keys (i.e. keys containing a byte with the high-bit set). The origin of this bug is itself interesting. Pebble’s bloom filter hash function was inherited from go-leveldb which was inherited from LevelDB . The original implementation of LevelDB’s hash function had behavior that was dependent on whether the C char type was signed or unsigned (which is controllable via a flag for gcc/clang). That subtle dependency was fixed years ago in both LevelDB and RocksDB, but the dependency slipped back in somewhere in the translation to Go. Leveraging CockroachDB Testing The final layers of Pebble testing leverages the existing CockroachDB unit tests and nightly tests. We added an environment variable ( COCKROACH_STORAGE_ENGINE ) that controls whether CockroachDB unit tests use Pebble or RocksDB. We also implemented another storage engine for an additional level of testing. The Tee storage engine does as its name implies: it tees all write operations to both Pebble and RocksDB. Read operations are directed to both underlying storage engines and compared to ensure the same results are returned. CockroachDB runs a suite of nightly integration tests known as roachtests. A roachtest spins up clusters on AWS or GCP and performs cluster-level testing. The same COCKROACH_STORAGE_ENGINE environment variable was used to allow running these tests on Pebble. Performance No announcement of a new storage engine would be complete without a nod to performance. Replacing Pebble with RocksDB would be a non-starter if performance was significantly impacted. RocksDB is highly performant, and we had to spend significant effort to match or exceed its performance. The performance surface area of a storage engine is vast, and this post can only touch on a tiny fraction of it. Performance is not just about raw throughput and latency, but also resource consumption, such as CPU and memory usage. At the end of the day, what we care about most is the performance of Pebble vs RocksDB on CockroachDB level workloads. YCSB is a standard benchmark for examining storage engine performance. It runs six workloads:workload A is a mix of 50% reads and 50% updates. Workload B is a mix of 95% reads and 5% updates. Workload C is 100% reads. Workload D is 95% reads and 5% inserts. Workload E is 95% scans and 5% inserts. Workload F is 50% reads and 50% read-modify-writes. Pebble and RocksDB were configured with similar options (identical where there was overlap). The dataset sizes for all of the workloads fit in memory, though we’ve also performed testing of workloads with datasets that do not fit in cache. Pebble meets or exceeds RocksDB on the 6 standard YCSB workloads. CockroachDB performance has bottlenecks outside of the storage engine. For a more direct comparison of the storage engine performance, we implemented a subset of the YCSB workloads directly on top of Pebble and RocksDB. Note that workload F was not implemented in this storage engine only benchmark tool. The large delta seen on workload C is due to better concurrency in Pebble’s block and table cache structures. As can be seen from the CockroachDB-level comparison, the effect of this better concurrency becomes muted when the entire system is considered. Conclusion and Future Work The 20.1 release of CockroachDB last May introduced Pebble as an alternative storage engine to RocksDB. We were cautious in this introduction, not publicizing it widely and requiring users to specifically opt-in to using Pebble. We began testing Pebble on CockroachCloud clusters, first with internal test clusters, and recently with production clusters. We’re now confident in the stability and performance of Pebble. With the release of 20.2 this fall, Pebble will become CockroachDB’s default storage engine. RocksDB remains as an alternative storage engine in 20.2, but its days are numbered and we plan to fully remove it in a subsequent release. The 20.2 release will also bring enhancements to Pebble. We’ve made improvements to the compaction heuristics and mechanics that significantly speed up IMPORT and RESTORE workloads which were bottlenecked by the storage engine. We’ve incorporated range deletions in the compaction heuristics which have allowed us to get rid of the Compactor workaround in CockroachDB mentioned earlier. These are only the tip of the iceberg for where we ultimately want to evolve Pebble. The storage engine is the foundation of performance and stability in CockroachDB and we plan to continue enhancing Pebble in pursuit of ever greater performance and stability.", "date": "2020-09-15"},
{"website": "CockroachLabs", "title": "Cloud-Native Java-Persistence Layer using CockroachDB & Hibernate", "author": ["Robin de Silva Jayasinghe"], "link": "https://www.cockroachlabs.com/blog/java-persistence-layer-hibernate/", "abstract": "This blog is written by guest authors Robin de Silva Jayasinghe , Thomas Pötzsch , and Joachim Mathes. Robin de Silva Jayasinghe, is a Sr. software engineer based in Germany working at synyx GmbH & Co. KG. synyx is an agile software provider in Karlsruhe, Germany that works together with different companies to find the best possible IT-solutions for their challenges. Thomas and Joachim are working as software and systems engineers at Contargo, one of the leading container hinterland logistics in Europe. We are currently performing a shift from a classic VM based environment to a cloud-native setup, consisting of multiple Kubernetes clusters and more modern architecture and development setups. At the end of last year, some colleagues from synyx and our long-term partner Contargo formed a new team to build an application from scratch-- an application for the collection and management of booking data for container logistics (think shipping containers). The motivation for the application is to supersede several legacy booking applications and to provide better UX, performance, and maintainability. The new application will enable internal and external customers to integrate via user interfaces and APIs. As we were starting a so-called \"green field\"-project that would need to integrate with our cloud-native setup, we focused on using well-known setups and tools in a modern fashion to further drive our DevOps-oriented development process. In retrospect, this allowed us to reap the fruits of modern development setups and processes while maintaining velocity by not overwhelming developers. Delaying the Persistence Layer Decision A common approach is to set up a persistence layer on the first day of the project, which can lead to feature development being slowed down by the consideration of the database and its integration. Further, it can result in opinionated decision-making if features are bound to database changes/rely on (mostly relational) database structures. Thus, in this project, we chose to postpone all database-related decisions. Instead, we waited for a point in time where we had a thorough understanding of the domain, the underlying model, and the use cases of the application. How to Implement Persistence Inevitably, we had to start persisting our data -- which meant we needed to form a concept for our persistence layer. First, we had to decide what type of database would fit our solution. Possible options were: a NoSQL document store a graph database a relational database We picked the relational approach for the following reasons: We had a good feeling that the domain model had stabilized and we could build a relational schema from that knowledge. Due to the perceived stabilization, we deemed the risk of major changes in our data structures to be considerably low. The system would perform concurrent write transactions on the entries in the database and the ACID consistency model of a relational database would help us with achieving consistency among concurrent actors in the system. Our team is very familiar with using relational databases in combination with Spring Boot and Spring Data JPA. As a next step we had to choose the concrete relational database system. There were three requirements that needed to be satisfied by the potential database system: Cloud native: As already mentioned in the introduction, we needed a database that could run smoothly in a (private) cloud scenario. Thus, builtin support for cloud-native monitoring, alerting and distributed logging was an important factor. Scalability: Providing high-availability was a strong requirement for our application. Furthermore, the need for horizontal scaling due to high load was on the mid-term schedule. We knew that especially with API use cases but also with seasonal load there would be a need for horizontal scaling without additional operational effort. Schema flexibility: Although a good amount of our domain had been modeled and regarded as stable, we required a certain flexibility in the approach to data storage. This would allow us to be prepared for potential changes: There still were parts of the domain where we were not totally sure of the semantics or where we had document-like structure that we did not need or want to put in a relational schema. Support for key/value storage while maintaining high consistency would help to fulfill these requirements. In contrast to MariaDB and PostgreSQL, CockroachDB excels in all three sections. Good support for monitoring, alerting and logging allows for seamless integration in a cloud scenario, severely simplifying the management and surveillance of nodes and clusters. As we experienced in another project, CockroachDB offers multiple built-in features that allow scaling by adding and removing nodes with zero downtime. This can be achieved with other databases like MariaDB or MySQL but requires additional products like Galera Cluster or Percona that come at their own cost and add complexity. The flexibility of schemas is guaranteed when working with CockroachDB, as CockroachDB's storage engine RocksDB is a key/value store mapping strings to strings (CockroachDB just started using a new storage engine called Pebble that was inspired by RocksDB).  The technical boundaries to implement simple key/value use cases on top of that are low to non-existent. Additionally, CockroachDB, just like PostgreSQL, has support for the JSONB datatype. Thus, JSON values can not only be stored but also used for filtering expressions or in a SQL result list. * CockroachDB scored in all 3 sections and we prepared to continue the applications development. Hello CockroachDB! After deciding to use CockroachDB we started a new branch of our application. First of all, we needed a CockroachDB cluster for our developer machines. Docker-compose is a very handy tool for launching up dependencies during application development: version: \"3\"\n\nservices:\n  db-management:\n    # this instance exposes the management UI and other instances use it to join the cluster\n    image: cockroachdb/cockroach:v20.1.0\n    command: start --insecure --advertise-addr=db-management\n    volumes:\n      - /cockroach/cockroach-data\n    expose:\n      - \"8080\"\n      - \"26257\"\n    ports:\n      - \"26257:26257\"\n      - \"8180:8080\"\n    healthcheck:\n      test: [\"CMD\", \"/cockroach/cockroach\", \"node\", \"status\", \"--insecure\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  db-node-1:\n    image: cockroachdb/cockroach:v20.1.0\n    command: start --insecure --join=db-management --advertise-addr=db-node-1\n    volumes:\n      - /cockroach/cockroach-data\n    depends_on:\n      - db-management\n\n  db-node-2:\n    image: cockroachdb/cockroach:v20.1.0\n    command: start --insecure --join=db-management --advertise-addr=db-node-2\n    volumes:\n      - /cockroach/cockroach-data\n    depends_on:\n      - db-management\n\n  db-init:\n    image: cockroachdb/cockroach:v20.1.0\n    volumes:\n      - ./scripts/init-cockroachdb.sh:/init.sh\n    entrypoint: \"/bin/bash\"\n    command: \"/init.sh\"\n    depends_on:\n      - db-management The setup describes four containers: db-management the database node exposing the Cockroach DB admin console and acting as the initial join-node for the other nodes. db-node-1, db-node-2 further database nodes joining the initial node. Can be extended with more nores like db-node-3, db-node-4, ... db-init is a one-shot container running a shell script that creates the inital schema and user with the cockroach CLI tool. After we had the correct docker-compose definition we distributed the setup to our developer machines. Starting the DB cluster with docker-compose was just a matter of downloading the docker images. :) Creating the Tables Once the database was up we were ready to design the tables needed for our application. In our previous Spring Boot projects we created with a Liquibase - a pretty popular tool for table lifecycle management. Flyway would have been an alternative but we preferred Liquibase as we had a good amount of experience with that tool. (Heads up: As of now, only Flyway is officially supported by CockroachDB .) Using the existing domain model as an orientation we created roughly 50 tables and 1 sequence. Database access from Java One big selling point of CockroachDB is the compatibility with PostgreSQL. So, any tool that runs on PostgreSQL should be able to run on CockroachDB, too. Technically, CockroachDB speaks the PostgreSQL wire-protocol and covers it's SQL dialect. Consequently, we configured our Spring Boot application to use the PostgreSQL JDBC driver: Added the dependency to the build descriptor (pom.xml in our case): < dependency >\n    < groupId >org.postgresql</ groupId >\n    < artifactId >postgresql</ artifactId >\n    < scope >runtime</ scope >\n</ dependency > The database access is configured the application.yml: spring : datasource : url : jdbc:postgresql://localhost:26257/application-schema username : username password : password With this configuration in place, Spring Boot is ready to connect to the CockroachDB cluster. Find the right data access mechanism With the tables in place, we had to decide on the database access mechanism. Again we had several options to chose from: Plain JDBC / Spring JDBC template: SQL queries and result set mapping have to be implemented by the developer. jOOQ: jOOQ is a framework that provides API-generated DB access from the database metadata of the underlying schema. Spring Data JPA on top of Hibernate: JPA is the standard API for object-relational mapping (ORM). Hibernate is the most popular implementation of JPA. Spring Data adds another layer of abstraction above different data access mechanisms and makes the most common data access use-cases much easier. Spring Data JDBC: While following the Spring Data idioms Spring Data JDBC is more simplistic than it's JPA-based sibling. It offers some basic translation from objects into SQL statements as well as basic relationships (1:n, 1:1) between objects. It lacks, however, the caching and state-management that happens inside a JPA implementation. Although it would have been a valid option in hindsight, we abandoned the plain JDBC solution pretty quickly. Given the number of tables and our usage of inheritance structures, the size of the potentially resulting handwritten database access code would have been huge. jOOQ would have helped a lot with generated access code and its fluent data-access APIs. However, the team had no prior experience with jOOQ, and the tight schedule of the project did not leave much space for experimenting with new APIs. In the end, we decided on the well-known JPA/Hibernate combination. Following the DDD approach, we kept the core structures and behavior in the domain model and used the persistence part just as a data sink and source. Data coming in from the application layer is converted into the persistence structures (entities and embeddables) and saved to the database using the Spring Data repositories. Reading data works the other way around. By using this approach we decoupled the domain model and business logic from the used persistence technology and gained a good amount of flexibility. It would have been a more traditional approach (propagated in a lot of Spring-related documentation, blogs and tutorials) to model the domain directly in JPA entities and use them in the business logic. This however limits and binds the domain model and logic to the capabilities and characteristics of JPA (or any other persistence technology). If it turns out that JPA does not work well or if we want to extend the persistence with some aspects of CockroachDB that are not supported by JPA (e.g. key/value storage), we can easily implement them in the persistence layer without influencing the domain model. *Note - since working on this project, and writing this blog, CockroachDB officially announced a CockroachDB dialect for Hibernate . Optimizing Performance After completing the schema, the JPA entities and connecting the persistence layer to the domain layer we tested the application end-2-end. While doing that we observed slow response times in certain areas of the application. We had already seen the Cockroach console UI in other projects and used it to identify the slow queries. The statement overview section of the console offers the option to activate statement diagnostics. Once the statement was executed at least once with activated diagnostics we could download a very detailed archive containing information collected during the compilation and execution of the statement. This helped us a lot to spot table indexes that we forgot to create for some of the implicit foreign key relationships. ;) Automatically Retry Aborted Transactions Once we put some load on our new persistence layer we started to observe transaction errors in the application's logs. A quick search on the Internet showed that the observed errors were so called transient serialization errors . If the transaction contention cannot be resolved without serialization anomalies the conflicting transactions are aborted. This behavior is a consequence of CockroachDB's default isolation level: SERIALIZABLE . There are different means to reduce such errors but the application code must always expect that such errors occur. For such cases it's considered best practice to implement retry logic at the client (application) side. There is a pretty detailed guide on how to do this for Spring Boot and JPA . Although this caused some extra work and understanding for the development team, the behavior is correct and I prefer it to potentially inconsistent data in the application. JPA and Hibernate built and designed for relational databases with weaker default isolation levels. It's reasonable that some extensions needed to be done. Automated Integration Tests When you run integration tests you want to validate use-cases of your component without replacing its dependencies with fake implementations (e.g. mocks). For a database driven component this would mean that you have a running database at hand during your integration tests. Until like 2 years ago this was a common pattern to spin up an embedded in memory-database. In most of the cases this was H2 . As a consequence your application had to be designed in a way that it supported different database implementations. In Java a major abstraction to support different (relational) databases is JDBC. Most of the tools add an abstraction on top of JDBC to abstract from the concrete database and its SQL dialect. It turned out, however, that abstracting from the SQL dialect does not give you a compatibility warranty. From time to time, you run into corner cases where the embedded database behaves differently from the real one and you can only detect this in the central systems. That's why we wanted to use the same database in our integration tests as in our central systems. To achieve this, we start the database as a container along with the integration test. Testcontainrs is a fantastic tool that can be used with your test framework to manage the lifecycle of its dependent containers. CockroachDB ran smoothly, and none of the integration tests we wrote in our persistence branch caused any issues. You have to keep a close eye on the lifecycle of your database container and the schema, however. Delete/Rebuild everything does not scale well with the number of tests. ;) Going into more details here would need a separate blog, though. Making it DevOps In our project there is close cooperation and a lively exchange between devs and ops. Fundamental to the further development of our platform towards a cloud-native solution is the ability of the developers to run and build applications by themselves (true to the motto: \"You build it, you run it!\"). Enabling developers to run their own applications has ensured that developers have been able to integrate review apps into their applications. This was a first step towards the cloud capability of the application. To do this, we enabled review apps in GitLab by configuring our pipelines and set up a dedicated namespace in our K8s cluster, where we deploy a new version for each merge request. Parallel to the development of the persistence layer in the application, we already started to integrate CockroachDB into the review apps. For this purpose, we set up a separate CockroachDB cluster within the namespace of the review apps. Due to resource utilization, the database server is shared by all review apps for merge requests. This way, we were able to test the persistence merge request and all subsequent merge requests end to end (including the database). During this process, CockroachDB and its community were able to convince with good, complete documentation, ready-to-use images and helm charts . The developers were thus able to work together with the ops and later alone to operate and expand the CockroachDB cluster. This included scaling, monitoring, updating and backup management of the database. Road to Production One of the last steps before finishing the development of persistence was to equip the currently existing stages with a CockroachDB cluster. Since our stages still exist in our classic VM environment, it was necessary to find a setup for this as well. We agreed on setting up a multi-node CockroachDB cluster accompanied by an HAProxy to shuffle requests across the different nodes. After the setup was done, we prepared all application configurations with the database coordinates and credentials. Although CockroachDB-clusters typically do not reside in these habitats, it acclimatized well. After the merge of our new persistence solution, all applications were up and running. Without bigger issues. OK, nearly without -- for the production system we used the wrong hostname, but this was corrected quickly. This was a very smooth transition for such a big (architectural) change. Based on the experiences we made integrating CockroachDB, we expect a rather seamless migration to our cloud-native platform (K8s). The developers and ops that took part feel well enabled and confident to carry it out and drive our platform further. Closing comments By choosing CockroachDB as the underlying database of our new persistence layer, we made the right decision. By focusing on a cloud-native database we laid the ground stone for a modern persistence layer in our application, and departed from a conservative mindset. Instead of introducing a well-known (possibly outdated) solution into an environment that would shift towards a cloud-native setup, we broke out of our comfort zone. We were ready to face the challenges of choosing a modern solution. And it was worth it. Integrating CockroachDB in our developer setups was a pretty comfortable ride. Close cooperation between devs and ops made it easy to set up, operate, and use the database on a variety of platforms. CockroachDB's rich documentation and community resources helped out greatly during the process. CockroachDB clusters are very easy to handle. We did not encounter any major issues managing them -- neither on our developer machines (Linux and MacOS) nor on our central installations with K8s and Puppet-managed virtual machines. Thanks to the PostgreSQL compatibility we could handle CockroachDB just as if it was PostgreSQL, allowing for seamless integration with technologies we already use and know. The platform shift to Kubernetes will take place in the near future, and we are ready to take it on with a cloud-native database. With its features, ecosystem, and flexibility, CockroachDB gives us a lot of confidence in regards to upcoming challenges. CockroachDB is compatible with PostgreSQL via the pgwire protocol. Meaning that you could use any PostgreSQL database driver and PostgreSQL SQL dialect to access CockroachDB. However, this does not imply full compatibility since the underlying database and its behavior are not PostgreSQL. ;) So, when you decide to use a certain tool or framework you should check with the extensive documentation before you decide. And of course, it's also a good idea to just try out the tool you want to use. Maybe it's not fully supported but good enough for your use case (like our usage of Liquibase). Nevertheless, if I had to build another relational persistence for a cloud-native application I'd definitely choose CockroachDB.", "date": "2020-09-18"},
{"website": "CockroachLabs", "title": "New Cockroach University Course: CockroachDB for Python Developers", "author": ["Will Cross", "Crossman Wilkins"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-for-python-course/", "abstract": "Today, we are excited to introduce our second Cockroach University course: CockroachDB for Python Developers . This course is designed for application developers looking to advance their education with CockroachDB and learn how to build scalable, resilient applications and services. Similar to our inaugural Getting Started course, this free course includes a series of videos, exercises, and a final exam. However, this course is much more hands-on. You will build a full-stack vehicle-sharing application called MovR in Python using the popular SQLAlchemy ORM. Our goal is for students to walk away from this course feeling confident in building fully-functional applications with CockroachDB. Our hard-working training team spent many months building and testing this course to ensure it meets developers’ needs. In this post, we’ll dive into some of the details of what you can expect to encounter in CockroachDB for Python Developers . We hope you find this course valuable and always welcome your feedback. Who is this course for? This course is for Python developers. Whether you are looking to build a new application on CockroachDB, learn more about its internals, or find out if CockroachDB is a fit for your use case, this course is for you. If you are a manager or architect looking to introduce CockroachDB to your organization, this free course is useful to you as well. Have your developers complete this self-paced course on their own and save the time and money it typically costs to introduce a new technology. And since CockroachDB speaks SQL and is compatible with Postgres ORMs & drivers, it already has a familiar look and feel to many developers. What can I expect? If you've already taken the Getting Started course, you will be familiar with the course format. More specifically, the CockroachDB for Python Developers course includes: 3 robust chapters of content 19 educational videos 12 hands-on labs 1 final exam During the course, you will use free CockroachCloud clusters to build a full-stack vehicle-sharing application (called MovR) in Python. Using the popular SQLAlchemy ORM , the course begins with a simple application that handles scooters and their current positions. Next, you add features one by one as the course progresses until you have built a fully functional application that is ready to track multiple vehicle types and users. Along the way, you will learn everything you need to develop an application with CockroachDB including how to: Perform a schema migration Work with CockroachDB's rich data types Make good use of indexes for filtering and sorting your data At the end of the course, there will be a final exam to test your new skills. If you pass, you will receive a Certification of Completion. At that point, you can post your Certificate of Completion on LinkedIn (tag @cockroach-labs ) or Twitter (tag @cockroachDB ) to be entered to win a custom CockroachDB backpack. How can I get started? You can get started today for free here. If you are new to CockroachDB or not familiar with our unique architecture, we recommend taking the Getting Started course prior to CockroachDB for Python Developers . (Reminder: we will send you a free CockroachDB swag bag after passing the Getting Started final exam!) If you have questions along the way, we have a dedicated channel in our CockroachDB Community Slack called #cockroach-university, where our team is available to answer questions. What’s next? Over the next few months, we will be introducing this course in  three additional programming languages: Java, Go, and Node.js. Follow us on social media ( LinkedIn , Twitter ) for more updates on Cockroach University and new courses. We look forward to hearing your feedback and are glad you are interested in furthering your education with CockroachDB!", "date": "2020-09-29"},
{"website": "CockroachLabs", "title": "How to Calculate Your True Database Costs", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/true-cost-cloud-database/", "abstract": "Before the dominance of the cloud, calculating the cost of a database was a pretty simple equation: software costs + hardware costs = database costs. If you chose an open source product, the software cost might melt away. While the cloud has fundamentally changed how we consume and deploy software, too many people are still using this outdated calculation. The truth is, there are a lot more things to consider when pricing out the total cost of a database. The hardware and software costs are still there, but you also need to think about the price of scaling the database, of integrating with your existing and future systems, and of planned--or unplanned--downtime. When pricing out the cost of a cloud database, it’s crucial to ask these questions upfront. In a recent episode of The Cockroach Hour , VP of Product Marketing Jim Walker presented a complete rubric of all the questions to ask when calculating the true cost of a database. Costs fall into three main categories: hard costs, operational costs, and soft costs. In this recap of the webinar, we’ll walk through the line items in the rubric, and follow-up with an in-depth template you can use in your own calculations. Hard Database Costs On the surface, this portion of the equation hasn’t changed much since the pre-cloud days. The hard costs for a database fall into two categories: the software license and the hardware. However, the cloud introduces a few new questions to ask of your hardware and software costs. Cloud Database Software Costs There are lots of software licenses on the market, which can be grouped into three categories: Traditional Enterprise: This has been a model for thirty-odd years in enterprise software, before the cloud. You pay a large upfront fee for the enterprise software license (usually in the hundreds of thousands of dollars, plus a support and maintenance fee). After that purchase, you pay additional costs for added functionality and upgrades. Full Open Source: A fully free, Apache license. The costs associated with a free software license aren’t non-existent, though. You need to pay to maintain it, support it, and de-risk it. We’ll go into these costs in a bit. Commercial Open Source: This model came about 10-15 years ago as a viable solution to solve some of the issues companies saw with full open source licenses, like indemnification and support. Cloud Database Hardware Costs Hardware costs look different today than they did 30 years ago. But those hardware costs haven’t gone away just because there’s not a giant box humming in the server room. Sure, there are differences: you might negotiate price with a vendor, or take advantage of the economies of scale (don’t mean to disparage this), but it’s a cost you need to consider. You also still need to manage and operate all these. The operational costs don't just melt  away. There is considerable time spent in the interfaces of these cloud providers to actually manage these things and understanding from an operations point of view, so ease of use is important. Operational Database Costs On top of hardware and software costs, there are day-to-day costs incurred by running the database. These vary a lot based on the vendor you choose (and their pricing structure), but stem from the same question: what happens when we want to do X task in the future? Over the course of using a database, you’re going to have to deal with disaster recovery, with scale, with integrating the system with other tools. It’s very easy to think “We’ll cross that bridge when we get to it.” But when calculating the true cost of a database, you need to think about these inevitabilities and price out what it will cost when you need to cross these bridges. Disaster recovery: What’s the cost of failure? No matter how many layers of abstraction you place over your hardware, at the end of the day, we’re dealing with mechanical devices. And those devices will fail. The cost of a temporary or catastrophic challenge needs to be considered when choosing your database to deploy your application. While there are many reasons an application might fail, the database is a main cause of many outages. Old versions, write bottlenecks, memory issues, locked transactions, misconfigurations, hardware failure. You need to prepare for these inevitabilities, because they will happen at some point. Both planned and unplanned downtime can result in significant cost and while this spend may not be easily calculated, it should still be considered as part of your database choice. Each business is different and the impact of downtime is different for each, but these are a few items to consider when calculating the cost of downtime for your business: Loss of revenue: Missed opportunity to conduct online commerce or engage a prospect Reputation impact: Consumers may just go on to the next offer, your competitor Client satisfaction: Loss of trust in your product or service due to observed issue Regulatory costs: Jurisdictional regulations sometimes fine organizations for data issues Legal liability: In extreme cases, lawsuits may be filed associated with data loss There are some pretty simple and more straightforward technical reasons why data loss and downtime can be an issue.  Often we find ourselves dealing with an extended RPO/RTO period and will endure the high technical costs. Scale: What’s the cost of sharding? For most cloud databases, scale is accomplished by increasing the size of the instance. However, with this approach, you’re limited with the max size available to you. What happens when you want to scale beyond this, or need global scale? Some databases, like AWS Aurora, allow you to expand beyond a single instance (RDS) and allow for multiple instances. However, this is for reads only and limits the amount of transaction volume you can handle as there is no capability to scale write nodes. You still face the size limit. Further, this single write node configuration limits your availability to scale access across broad geographies as you will always encounter physical latencies for write access. If you choose to scale an open-source database like PostgreSQL or MySQL, you will eventually need to shard the database. There are massive costs associated with this approach. For one, you will need to modify your application, which introduces risk. You will also need to configure a new instance and cut over to this new configuration at some point, typically in the middle of the night. You will have extra costs associated with the hardware and the pain it causes for the team… and this is all the best case scenario. If something goes wrong in this process, you wind up with unplanned downtime. Also, management costs of a sharded database increase exponentially with each new shard. In the True Cost of a Database webinar , Jim dives into the true costs of sharding a database in depth, and how painful it can be for teams. Integration: What’s the cost of using other tools? Your database does not exist in a vacuum. It's going to be integrated into other parts of your IT platform. You aren't going to run OLAP in an OLTP database. There's a reason why data warehouses came about. And so integration between your database and other tools, like a data warehouse, is important. And depending on your database, it can be costly. Do your use cases require a Kafka or some sort of stream processing? Are you using a data lake? Evaluate Hidden Database Costs Before Buying Beyond operational costs, there are even harder-to-quantify questions to ask that constitute the hidden cost of a cloud database. These include the compliance costs, the competitive risk, potential for vendor lock-in, and your ability to attract talent. To learn how to factor hidden costs into your calculation, watch the full webinar on the True Cost of a Cloud Database . [ Watch the Webinar ]", "date": "2020-09-23"},
{"website": "CockroachLabs", "title": "Tutorial: How to Simulate a Multi-Region CockroachDB Cluster on localhost with Docker", "author": ["Fabio Ghirardello"], "link": "https://www.cockroachlabs.com/blog/simulate-cockroachdb-cluster-localhost-docker/", "abstract": "Simulating a CockroachDB cluster can be an incredibly useful tool for testing, training, and development work. It's an easy process that you can perform directly on localhost using Docker. Today's blog post is a tutorial by Cockroach Labs enterprise architect Fabio Ghirardello on how to simulate a multi-region CockroachDB cluster on localhost. Here are the instructions to simulate the deployment of a 9 nodes CockroachDB cluster across 3 regions on localhost using Docker. The instructions assume you are running Linux or macOS, although it should work on Windows using Cygwin , and have Docker installed. Below is the high level architecture diagram. Each region will host 3 nodes: region us-west2 hosts nodes roach-seattle-1|2|3 ; region us-east4 hosts nodes roach-newyork-1|2|3 region eu-west2 hosts nodes roach-london-1|2|3 . Configure Your Simulated Multi-Region Cockroach Setup with Docker First, you'll need to create the required networks. We'll create 1 network for each region, plus 1 network for each inter-regional connection. # region networks docker network create --driver = bridge --subnet = 172.27.0.0/16 --ip-range = 172.27.0.0/24 --gateway = 172.27.0.1 us-west2-net\ndocker network create --driver = bridge --subnet = 172.28.0.0/16 --ip-range = 172.28.0.0/24 --gateway = 172.28.0.1 us-east4-net\ndocker network create --driver = bridge --subnet = 172.29.0.0/16 --ip-range = 172.29.0.0/24 --gateway = 172.29.0.1 eu-west2-net # inter-regional networks docker network create --driver = bridge --subnet = 172.30.0.0/16 --ip-range = 172.30.0.0/24 --gateway = 172.30.0.1 uswest-useast-net\ndocker network create --driver = bridge --subnet = 172.31.0.0/16 --ip-range = 172.31.0.0/24 --gateway = 172.31.0.1 useast-euwest-net\ndocker network create --driver = bridge --subnet = 172.32.0.0/16 --ip-range = 172.32.0.0/24 --gateway = 172.32.0.1 uswest-euwest-net Each node is associated to its own region network, which will attach to the docker instance eth0 NIC. We also specify the node IP address with the --ip flag and the IP addresses of all nodes in its region using the --add-host flag. This will create an entry in the docker instance /etc/hosts file, which has precedence over DNS lookups (this will become very important in a bit!) Create the haproxy.cfg files for the HAProxy in each region. # us-east4 mkdir -p data/us-east4 cat - > data/us-east4/haproxy.cfg << EOF global\n  maxconn 4096\n\ndefaults\n    mode                tcp\n    # Timeout values should be configured for your specific use.\n    # See: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-timeout%20connect\n    timeout connect     10s\n    timeout client      1m\n    timeout server      1m\n    # TCP keep-alive on client side. Server already enables them.\n    option              clitcpka\n\nlisten psql\n    bind :26257\n    mode tcp\n    balance roundrobin\n    option httpchk GET /health?ready=1\n    server cockroach1 roach-newyork-1:26257 check port 8080\n    server cockroach2 roach-newyork-3:26257 check port 8080\n    server cockroach3 roach-newyork-2:26257 check port 8080 EOF # us-west2 mkdir data/us-west2 cat - > data/us-west2/haproxy.cfg << EOF global\n  maxconn 4096\n\ndefaults\n    mode                tcp\n    # Timeout values should be configured for your specific use.\n    # See: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-timeout%20connect\n    timeout connect     10s\n    timeout client      1m\n    timeout server      1m\n    # TCP keep-alive on client side. Server already enables them.\n    option              clitcpka\n\nlisten psql\n    bind :26257\n    mode tcp\n    balance roundrobin\n    option httpchk GET /health?ready=1\n    server cockroach4 roach-seattle-1:26257 check port 8080\n    server cockroach5 roach-seattle-2:26257 check port 8080\n    server cockroach6 roach-seattle-3:26257 check port 8080 EOF # eu-west2 mkdir data/eu-west2 cat - > data/eu-west2/haproxy.cfg << EOF global\n  maxconn 4096\n\ndefaults\n    mode                tcp\n    # Timeout values should be configured for your specific use.\n    # See: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-timeout%20connect\n    timeout connect     10s\n    timeout client      1m\n    timeout server      1m\n    # TCP keep-alive on client side. Server already enables them.\n    option              clitcpka\n\nlisten psql\n    bind :26257\n    mode tcp\n    balance roundrobin\n    option httpchk GET /health?ready=1\n    server cockroach7 roach-london-1:26257 check port 8080\n    server cockroach8 roach-london-2:26257 check port 8080\n    server cockroach9 roach-london-3:26257 check port 8080 EOF Next, create the Docker containers: # Seattle docker run -d --name = roach-seattle-1 --hostname = roach-seattle-1 --ip = 172.27.0.11 --cap-add NET_ADMIN --net = us-west2-net --add-host = roach-seattle-1:172.27.0.11 --add-host = roach-seattle-2:172.27.0.12 --add-host = roach-seattle-3:172.27.0.13 -p 8080:8080 -v \"roach-seattle-1-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-west2,zone = a\ndocker run -d --name = roach-seattle-2 --hostname = roach-seattle-2 --ip = 172.27.0.12 --cap-add NET_ADMIN --net = us-west2-net --add-host = roach-seattle-1:172.27.0.11 --add-host = roach-seattle-2:172.27.0.12 --add-host = roach-seattle-3:172.27.0.13 -p 8081:8080 -v \"roach-seattle-2-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-west2,zone = b\ndocker run -d --name = roach-seattle-3 --hostname = roach-seattle-3 --ip = 172.27.0.13 --cap-add NET_ADMIN --net = us-west2-net --add-host = roach-seattle-1:172.27.0.11 --add-host = roach-seattle-2:172.27.0.12 --add-host = roach-seattle-3:172.27.0.13 -p 8082:8080 -v \"roach-seattle-3-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-west2,zone = c # Seattle HAProxy docker run -d --name haproxy-seattle --ip = 172.27.0.10 -p 26257:26257 --net = us-west2-net -v ` pwd ` /data/us-west2/:/usr/local/etc/haproxy:ro haproxy:1.7 # New York docker run -d --name = roach-newyork-1 --hostname = roach-newyork-1 --ip = 172.28.0.11 --cap-add NET_ADMIN --net = us-east4-net --add-host = roach-newyork-1:172.28.0.11 --add-host = roach-newyork-2:172.28.0.12 --add-host = roach-newyork-3:172.28.0.13 -p 8180:8080 -v \"roach-newyork-1-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-east4,zone = a\ndocker run -d --name = roach-newyork-2 --hostname = roach-newyork-2 --ip = 172.28.0.12 --cap-add NET_ADMIN --net = us-east4-net --add-host = roach-newyork-1:172.28.0.11 --add-host = roach-newyork-2:172.28.0.12 --add-host = roach-newyork-3:172.28.0.13 -p 8181:8080 -v \"roach-newyork-2-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-east4,zone = b\ndocker run -d --name = roach-newyork-3 --hostname = roach-newyork-3 --ip = 172.28.0.13 --cap-add NET_ADMIN --net = us-east4-net --add-host = roach-newyork-1:172.28.0.11 --add-host = roach-newyork-2:172.28.0.12 --add-host = roach-newyork-3:172.28.0.13 -p 8182:8080 -v \"roach-newyork-3-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = us-east4,zone = c # New York HAProxy docker run -d --name haproxy-newyork --ip = 172.28.0.10 -p 26258:26257 --net = us-east4-net -v ` pwd ` /data/us-east4/:/usr/local/etc/haproxy:ro haproxy:1.7 # London docker run -d --name = roach-london-1 --hostname = roach-london-1 --ip = 172.29.0.11 --cap-add NET_ADMIN --net = eu-west2-net --add-host = roach-london-1:172.29.0.11 --add-host = roach-london-2:172.29.0.12 --add-host = roach-london-3:172.29.0.13 -p 8280:8080 -v \"roach-london-1-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = eu-west2,zone = a\ndocker run -d --name = roach-london-2 --hostname = roach-london-2 --ip = 172.29.0.12 --cap-add NET_ADMIN --net = eu-west2-net --add-host = roach-london-1:172.29.0.11 --add-host = roach-london-2:172.29.0.12 --add-host = roach-london-3:172.29.0.13 -p 8281:8080 -v \"roach-london-2-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = eu-west2,zone = b\ndocker run -d --name = roach-london-3 --hostname = roach-london-3 --ip = 172.29.0.13 --cap-add NET_ADMIN --net = eu-west2-net --add-host = roach-london-1:172.29.0.11 --add-host = roach-london-2:172.29.0.12 --add-host = roach-london-3:172.29.0.13 -p 8282:8080 -v \"roach-london-3-data:/cockroach/cockroach-data\" cockroachdb/cockroach:v20.1.5 start --insecure --join = roach-seattle-1,roach-newyork-1,roach-london-1 --locality = region = eu-west2,zone = c # London HAProxy docker run -d --name haproxy-london --ip = 172.29.0.10 -p 26259:26257 --net = eu-west2-net -v ` pwd ` /data/eu-west2/:/usr/local/etc/haproxy:ro haproxy:1.7 Now, we'll initialize the cluster: docker exec -it roach-newyork-1 ./cockroach init --insecure We then attach each node to the inter-regional networks. These networks will attach to new NICs, eth1 and eth2 . We then use tc qdisc to add an arbitrary latency to each new NIC. Connectivity between nodes in the same region will go through the region network, over eth0 , and connectivity among nodes in different regions via the inter-regional network, over eth1 and eth2 . Note: with the connection to the inter-regional networks, the Docker instance internal DNS gets sometimes scrambled up: issuing, say, nslookup roach-seattle-1 from host roach-seattle-2 will resolve to either an IP address from the in-region network or from the inter-regional networks. If the hostname does not resolve to the in-region network IP, traffic will go through eth1 or eth2 which has the latency applied, causing in-region connectivity to look very slow. To resolve such a problem we use static IP addresses added to each node's /etc/hosts file. This makes sure that in-region hostnames resolve to the region IP addresses, forcing the connection to go over eth0 instead of eth1 or eth2 . # Seattle for j in 1 2 3 do docker network connect uswest-useast-net roach-seattle- $j docker network connect uswest-euwest-net roach-seattle- $j docker exec roach-seattle- $j bash -c \"apt-get update && apt-get install -y iproute2 iputils-ping dnsutils\" docker exec roach-seattle- $j tc qdisc add dev eth1 root netem delay 30ms\n    docker exec roach-seattle- $j tc qdisc add dev eth2 root netem delay 90ms done # New York for j in 1 2 3 do docker network connect uswest-useast-net roach-newyork- $j docker network connect useast-euwest-net roach-newyork- $j docker exec roach-newyork- $j bash -c \"apt-get update && apt-get install -y iproute2 iputils-ping dnsutils\" docker exec roach-newyork- $j tc qdisc add dev eth1 root netem delay 32ms\n    docker exec roach-newyork- $j tc qdisc add dev eth2 root netem delay 60ms done # London for j in 1 2 3 do docker network connect useast-euwest-net roach-london- $j docker network connect uswest-euwest-net roach-london- $j docker exec roach-london- $j bash -c \"apt-get update && apt-get install -y iproute2 iputils-ping dnsutils\" docker exec roach-london- $j tc qdisc add dev eth1 root netem delay 62ms\n    docker exec roach-london- $j tc qdisc add dev eth2 root netem delay 88ms done How to license and configure your Cockroach Cluster You will require an Enterprise license to unlock some of the features described below, like the Map view. You can request a Trial license or, alternatively, just skip the license registration step - the deployment will still succeed. Open a SQL shell. You can download the cockroachdb binary which includes a built in SQL client or, thanks to CockroachDB's compliance with the PostgreSQL wire protocol, you can use the psql client. # ---------------------------- # ports mapping: # 26257: haproxy-seattle # 26258: haproxy-newyork # 26259: haproxy-london # ---------------------------- # use cockroach sql, defaults to localhost:26257 cockroach sql --insecure # or use the --url param for another host: cockroach sql --url \"postgresql://localhost:26258/defaultdb?sslmode=disable\" # or use psql psql -h localhost -p 26257 -U root defaultdb Run the below SQL statements: -- let the map know the location of the regions UPSERT into system . locations VALUES ( 'region' , 'us-east4' , 37 . 478397 , - 76 . 453077 ), ( 'region' , 'us-west2' , 43 . 804133 , - 120 . 554201 ), ( 'region' , 'eu-west2' , 51 . 5073509 , - 0 . 1277583 ); SET CLUSTER SETTING cluster . organization = \"Cockroach Labs - Production Testing\" ; -- skip below if you don't have a Trial or Enterprise license SET CLUSTER SETTING enterprise . license = \"xxxx-yyyy-zzzz\" ; At this point you should be able to view the CockroachDB Admin UI at http://localhost:8080 . Check the map and the latency table: Congratulations, you are now ready to start your dev work on a simulated CockroachDB multi-region deployment! References CockroachDB Docs CockroachDB docker image Docker Network Overview HAProxy Docs HAProxy docker image Clean up When you're done, here's how to stop and remove containers, delete the data volumes, and delete the network bridges: for i in seattle newyork london do\n    for j in 1 2 3 do docker stop roach- $i - $j docker rm roach- $i - $j docker volume rm roach- $i - $j -data done\ndone docker network rm us-east4-net us-west2-net eu-west2-net uswest-useast-net useast-euwest-net uswest-euwest-net", "date": "2020-10-02"},
{"website": "CockroachLabs", "title": "Build a Go App with upper/db and CockroachDB", "author": ["José Nieto"], "link": "https://www.cockroachlabs.com/blog/upperdb-cockroachdb/", "abstract": "Today's blog is a guest post by José Nieto, creator of upper/db. upper/db gives you tools for the most common operations with databases, and is now compatible with CockroachDB. upper/db is a data access layer written in Go with ORM-like features. It's compatible with PostgreSQL, MySQL, SQLite, MongoDB, and now, CockroachDB! I am very pleased to announce that our CockroachDB adapter is now in beta and ready to be used! Get the adapter the usual way: go get github. com /upper/db/v4/adapter/cockroachdb CockroachDB has two installation modes : secure and insecure . Secure mode has good defaults for production usage whilst insecure mode is more suited for local development and testing. Here's an example on how to connect to a local insecure node using the cockroachdb adapter: package main import ( \"fmt\" \"log\" \"github.com/upper/db/v4/adapter/cockroachdb\" ) var settings = cockroachdb.ConnectionURL{\n  Host: \"localhost\" ,\n  Database: \"bank\" ,\n  User: \"maxroach\" ,\n  Options: map [ string ] string { \"sslmode\" : \"disable\" ,\n  },\n} func main () {\n  sess, err := cockroachdb.Open(settings) if err != nil {\n    log.Fatal( \"cockroachdb.Open: \" , err)\n  } defer sess.Close()\n\n  fmt.Printf( \"connected to database %q\\n\" , sess.Name())\n} Transactions Transactions are essential when working with SQL databases as they allow you to represent several database-altering operations as a single unit. upper/db comes with client-side retry handling logic , whenever a transaction fails with a retryable error, upper/db will wait a few milliseconds and try again until succeeding: err = sess.Tx( func (tx db.Session) error { // If a retryable error happens here, the transaction will be retried a few // times (with exponential back-off). return nil }) if err != nil {\n    log.Fatal( \"Could not commit transaction: \" , err)\n} If you want to configure the maximum number of transaction retries you can use the SetMaxTransactionRetries method: sess.SetMaxTransactionRetries( 10 ) Build a Go App: Hello World Example See the official Hello World repo for a tutorial on how to build a Go app with CockroachDB and upper/db, or check out all methods for building Go apps with CockroachDB.", "date": "2020-09-29"},
{"website": "CockroachLabs", "title": "Demo: Reduce Latency 10x by Geo-Partitioning Your Data", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/geo-partition-data-reduce-latency/", "abstract": "Running a geographically distributed database has a lot of benefits. We see enterprise companies, startups, and students choose distributed dbs for reliability, scalability, and even security. But many distributed databases come at a serious cost: latency. Distributing nodes across the globe means your data will need to travel from one node to the other. By its very definition, distribution creates latency. Geo-partitioning your data in CockroachDB makes it easy to minimize that latency. We believe you shouldn’t have to sacrifice the benefits of a distributed database to achieve impressive throughput and low latency. With geo-partitioning, we can minimize latency by minimizing the distance between where SQL queries are issued and where the data to satisfy those queries resides. In a recent episode of The Cockroach Hour , CockroachDB experts walked viewers through this exact dilemma, and reduced a 9-node geo-distributed CockroachDB cluster from experiencing 400ms of latency to well under 10ms (in real time). In today’s blog post, we’ll walk through the demo, and the mechanics that made the latency reduction possible. Understanding Database Latency When examining your application latency (in CockroachDB and other analytic platforms) you’ll see two important terms: P99 latency: The 99th latency percentile. This means 99% of requests will be faster than the given latency number. Put differently, only 1% of the requests will be slower than your P99 latency. P90 latency: The 90th latency percentile. This time, 90% of requests will be faster than the given latency number, and 10% of the requests are allowed to be slower than your P99 latency. Providing a low latency experience for users fosters retention, encourages continued engagement, and most importantly, increases conversions. The question becomes, what are my target P99 and P90 latencies? This is highly dependent on what kind of application you’re running--and what the user expectations are--but there’s one important thing to keep in mind: the 100ms Rule. The 100ms rule states that every interaction should be faster than 100ms. Why? 100ms is the threshold where interactions feel instantaneous. The 100ms Rule was coined by Gmail developer Paul Buchheit . It isn’t hard if you have a single-region application with nearby users. But as mentioned, distributed databases often pay a latency toll in order to get benefits like high availability and resilience. That’s where geo-partitioning comes into play. Another important factor to note, when examining latency: there are lots of different types of latency, each with their respective impacts on performance. Database latency can be a big part of that, but you’ll also need to look into your asset download time, data processing time, and initial loading time to get a full picture, like the one below: Sample observed latency breakdown In our blog post Reducing Multi-Region Latency with Follower Reads , we go into great detail about how to use Uptrend’s free website speed test to break down where your latency is coming from. What is geo-partitioning? Geo-partitioning is the ability to control the location of data at the row level. By default, CockroachDB lets you control which tables are replicated to which nodes. But with geo-partitioning, you can control which nodes house data with row-level granularity. This allows you to keep customer data close to the user, which reduces the distance it needs to travel, thereby reducing latency and improving user experience. How Geo-Partitioning Data Reduces Latency When your application is running across multiple regions, and you have customers all over the world, it’s important to think about where your data lives. If a user in California is requesting something from Europe, you need to account for the time it takes to wire the data across the globe. Geo-partitioning data lets you mitigate that. In the demo below, CockroachDB expert Keith McClellan runs our sample app, MovR, to illustrate a 9-node cluster distributed across the United States, with users in New York, LA, and Chicago. He applies geo-partitioning rules to build secondary indexes in his database. This is all done during an online schema change--he’s re-distributing data while the cluster is still live. The change moves records to be primarily domiciled in the right regions, and optimizes range partitions to make it easier to make changes further downstream. The initial result: P99 latency drops from 400ms to under well under 40ms. It’s a 10x improvement, and, more importantly, well under the 100ms rule. Later in the demo, Keith goes even further, partitioning indexes such that the P99 latency is reduced to around 2ms. He identifies one table that’s used in all geographies: the promo codes table. It’s reference data that will be used by the app’s customers whether they’re in US east, central, or west. Keith wants it to be locally, consistently available, and not have to reach across regions for the authority to act on that data. The solution: create some partitioned indexes that allow us to respond from the local replica of the data, while still guaranteeing consistency. P99 latency goes down even further because now--for the promo code reference data--all three locations have local access to that data. To run Keith’s tutorial yourself, check out our docs on the subject, which contain the sample code. How CockroachDB Reduces Latency with Geo-partitioning CockroachDB is a distributed OLTP engine that, like many distributed systems, stores data across many nodes. But CockroachDB is the only distributed database to take this one step further: you have control of where data lives. As your cluster grows, and you begin to have nodes contributing from all across the world, it becomes increasingly important (and advantageous) to take advantage of that control. To see real-world examples of geo-partitioning in action, you can read about how an electronic lock manufacturer and multi-national bank are using the Geo-Partitioned Replicas topology in production for improved performance and regulatory compliance. More resources: Life of a Distributed Transaction | CockroachDB Docs How to Build a Multi-Region Application on CockroachDB Demo: Low Latency Multi-Region Deployment on CockroachDB Geo-partitioning case study: How an Electronic lock manufacturer uses geo-partitioned replicas for improved performance.", "date": "2020-10-07"},
{"website": "CockroachLabs", "title": "Raft Is So Fetch: The Raft Consensus Algorithm Explained Through Mean Girls", "author": ["Mikael Austin"], "link": "https://www.cockroachlabs.com/blog/raft-is-so-fetch/", "abstract": "Raise your hand if you’ve ever been personally victimized by the Raft Consensus Algorithm. Image Credit Understanding Raft can be tough. Raft is a consensus algorithm used in distributed systems to ensure that data is replicated safely and consistently. That sentence alone can be confusing. Hopefully the analogy in this post can help people understand how it works. In honor of national Mean Girls day (“on October 3rd he asked me what day it was”), I present the Raft Consensus Algorithm as explained through the movie Mean Girls. (For a great, more technical overview of Raft, we recommend The Secret Lives of Data ). Image Credit Raft consensus can be explained using cliques in high school. In the beginning of the movie, Cady is a “home-schooled jungle freak” and thus is not a member of a clique. She is a lone piece of data with no replicas. If she were to be hit by a big yellow school bus, her thoughts on army pants and flip flops would die with her and would never trend. The Plastics however, are part of a cluster. If Regina is hit by a school bus, the information she had wouldn’t die with her, since she had already shared it with Karen and Gretchen. If someone were looking for the Burn Book, they could find it by asking one of the remaining two members, even while Regina was recovering in the hospital. If she hadn’t replicated that knowledge, nobody would ever be able to locate the book. Every cluster of replicas needs to have a Raft leader, or a Queen Bee. Of course, this would be Regina George. Regina is the leader of the Plastics, a group comprised of Gretchen Wieners (her father invented Toaster Strudel and her hair is full of secrets) and Karen Smith (she’s not the brightest bulb and she has weather forecasting superpowers). Gretchen and Karen are the follower replicas. This dynamic is similar to Raft in that if there isn’t consensus among replicas, no action can be taken. I mean, you wouldn’t buy a skirt without asking your friends if it looks good on you first, right? Exactly, that’s why you need consensus, or the majority vote. If Regina is shopping and wants to buy a skirt, she can’t do so unless either Gretchen or Karen have signed off on the purchase. Let’s say Regina tells Gretchen and Karen that on Wednesdays they wear pink. Gretchen eagerly approves first. Now that Regina has Gretchen’s confirmation, the majority of the Plastics (⅔)  are in favor of wearing pink on Wednesdays, and consensus has been reached. Now it’s official. Image Credit Understanding Quorum in Raft The high school environment of Mean Girls is comprised of many different cliques. Typically these cliques each sit together at lunch, with no intermingling between tables. Let’s think of the space between tables as a deliberate schism between the Plastics and the “Art Freaks,” also known as “the Greatest People You Will Ever Meet.” Let’s make numbers easy and think of the Plastics as having 3 people and the Art Freaks as having 2 people, Damien and Janice. Let’s say a client delivered a message to the Plastics at the same time as another client delivered a message to the Art Freaks. ‘4 for Glenn Coco’ was sent to the Plastics (through Regina, the Raft leader ), and ‘0 for Gretchen Wieners’ was written to the Art Freaks (through Janice, the Raft leader). Since the Art Freaks are made up of only two people, Janice and Damien, they are not able to achieve a quorum, since the clique needs more than two members in order to resolve a tie when voting. Since they can’t achieve a quorum, the commit can’t even be made. However, because her clique has greater than two members (3), Regina was able to secure a majority and commit the change ‘4 for Glenn Coco.’ Image Credit Leader Election: Who Gets to Be the Raft Leader? When Regina shows up to lunch wearing sweatpants on a Monday she is dramatically booted from her role as leader of the Plastics. At given intervals, a leader must send out a sort of heartbeat to maintain their leadership status. This is their way of saying “hi, I’m still here.” Similarly, any deserving Queen Bee needs to send out cues of their dominance at regular intervals, and when Regina can no longer assert her status, she is no longer the Queen Bee. The Plastics need a new Raft  leader, obviously. Luckily, Cady Heron steps up as the candidate replica, and Gretchen and Karen each reply with their vote to ensure Cady is the new Queen Bee. Now, the Plastics can’t do anything without Cady’s direction first. When Cady dresses in army pants and flip flops, she only needs one other member of the Plastics to agree that it’s cool to achieve a quorum (with 2 out of 3 votes), and now her style is accepted by all. The state of their high school has become “army pants and flip flops.” Image Credit Since Cady is now the Queen Bee, or Raft leader, requests from the client will go to Cady first. Let’s say Damien is the client speaking to Cady when the entire class of junior girls is in the gymnasium. Damien shouts “she doesn’t even go here!” Because Cady is the Raft leader, she should be the one to receive this piece of information, and relay it to the rest of the Plastics. Once she has confirmation from at least one of the other Plastics, the information has been committed. Does she even go here? No, she just has a lot of feelings. Karen knows she doesn’t go here, Gretchen knows she doesn’t go here, Ms. Norbury knows she doesn’t go here, Janice Ian knows she doesn’t even go here. Everyone knows she doesn’t even go here. Image Credit Hopefully this makes the Raft consensus algorithm more relatable. With this level of consistency in CockroachDB, the limit does not exist! How fetch is that? Image Credit", "date": "2020-10-03"},
{"website": "CockroachLabs", "title": "How MyMahi Built a Scalable, Serverless Backend using CockroachDB and AWS Lambda", "author": ["Rafi Shamim", "Vy Ton", "Stefan Charsley"], "link": "https://www.cockroachlabs.com/blog/mymahi-serverless-lambda/", "abstract": "Even before the stay-at-home orders spiked the demand for digital learning platforms, MyMahi was architecting for scale. MyMahi, a New Zealand-based digital education company, built their student platform, which helps 8,000+ monthly active students track their learning journeys, with CockroachDB Core embedded in a technology stack that includes spot instances (e.g., AWS Fargate), serverless functions (e.g., AWS Lambda), and GraphQL (e.g., GraphQL.js, GraphQL Tools). Starting in 2018, MyMahi designed their new application to take advantage of technologies that allow them to scale out seamlessly in response to student activity. In this blog post, we'll highlight MyMahi’s application architecture and discuss how different technologies like AWS Lambda interact with CockroachDB to provide a scalable, serverless backend for their application. MyMahi Chose CockroachDB for Scalability, ACID transactions & Ease of Use Across their entire tech stack, MyMahi wants to embrace cutting edge technology that sets their platform up for long term success and mirrors the innovation that their platform brings to the education system. While evaluating databases, MyMahi looked for a solution that was scalable and easy to maintain, could deliver ACID transaction semantics, and utilized the familiar PostgreSQL dialect. MyMahi’s engineering team is small and intends to keep headcount down during this stage of their growth which means each engineer needs to spend as much time as possible adding value to the product. This is why they need a database that does not require engineers to spend time dealing with infrastructure. In addition, MyMahi wants their architecture to be able to handle their expected user growth without exploding in costs at the same time. On a more fine-grained level, the nature of MyMahi’s education product means that traffic is bursty -- during a single day there may be long periods of low usage activity followed by sudden spikes. They need a database that easily scales up and down with the ebb and flow of student traffic. MyMahi chose the open source version of CockroachDB as their database because it meets each of those requirements. Application architecture with AWS Lambda, AWS Fargate & CockroachDB With a database chosen, MyMahi still had to decide how to design the rest of their architecture. This diagram shows the major components they use: The platform is built in the AWS cloud. A user request comes in and is handled by Route53 DNS resolution. The request is sent to Cloudfront, which is the AWS content delivery network (CDN). Static content like image assets are delivered from an S3 bucket, while other requests are handled by API Gateway. Each API Gateway request is handled by Lambda, which is the serverless platform in AWS that can run stateless application code and take away the burden of having to think of where the code is running. These Lambdas have a 30-second timeout, so longer requests are sent to an SQS queue for processing by async Lambdas, which have a 15-minute timeout. Shorter requests are handled synchronously by fetching data and performing business logic. Some user content is in another S3 bucket, while other data is stored in a CockroachDB cluster. Initially, MyMahi deployed CockroachDB onto dedicated EC2 instances. In order to lower costs for their database cluster, MyMahi switched to using AWS Fargate spot instances, which offers AWS spare capacity at a significant discount. Fargate is another serverless platform, except instead of just running small functions, it runs long-lived containers. Today, MyMahi has 12 Fargate spot instances with 4 CPUS and 24GB RAM each. CockroachDB’s resiliency to node failure allows MyMahi to use spot instances which AWS can reclaim (i.e destroy) with only 2 minute notification. This approach carries a performance and availability penalty - if a node is reclaimed, the data that was stored on that node must be re-replicated in the rest of the cluster. During this period of replication if AWS reclaims another node, there is a risk of data loss and unavailability depending on the replication factor. With a replication factor of 3, the loss of two nodes causes irreversible damage and would require a full cluster restore from backup, and with a replication factor of 5, which is what MyMahi uses in production, this number is increased to three nodes. In their experience, MyMahi has only had their spot instances reclaimed a few times. Each component of this architecture can respond to increases in traffic, and because they use usage-based pricing, costs can remain low. Using AWS Lambda with Node.js for handling bursts of traffic We’ll start with the Lambda functions, as they are the part of the architecture that has all the code for connecting to and querying CockroachDB. MyMahi implements their Lambda functions using Node.js. To talk to CockroachDB, the first step is for the Lambda to open a connection. Here’s a small snippet from the MyMahi code that shows how they use the node-postgres driver to do this. Since CockroachDB is a cluster of multiple machines, the connection configuration below is actually pointing to an AWS network load balancer (NLB) that distributes requests across all the nodes in the cluster. import { Client, Pool, PoolConfig } from '#/pg'; const poolConfig: PoolConfig = { database: 'mydb', host: env.DB_HOST, port: 26257, ssl: Environment.IS_DEVELOPMENT ? undefined : { ca: env.DB_CA_CRT, cert: env.DB_USER_CRT, key: env.DB_USER_KEY, rejectUnauthorized: false }, user: 'appuser', application_name: 'demo-api (GraphQL)' }; const pool = new Pool(poolConfig); const client = await pool.connect(); One detail to take note of in this code is that although it creates a connection pool, which would normally be used to keep long-lived connections, in practice the connection pool usually only exists for as long as a single invocation of the Lambda. The way Lambda works is that each time the API Gateway receives a request, it takes care of provisioning or finding the compute resources needed to run the code. AWS may sometimes decide to reuse previously completed Lambdas, as described in their FAQ , but in general Lambdas must be stateless, and one cannot rely on the connection pool to exist across different invocations of the Lambda. The advantage of this is that this architecture keeps costs down when there are few incoming requests, and can adapt quickly when a sudden burst of student traffic arrives. MyMahi has not yet had any problems with many short-lived connections being created in bursts, though finding a way to make the connections last longer may be a future optimization. Using GraphQL to fetch exact data To power their different UI components, MyMahi was attracted to using GraphQL APIs over the traditional REST APIs in order to fetch the exact data needed from CockroachDB. MyMahi built their GraphQL API server using both graphql and graphql-tools libraries. Within a Lambda function, the GraphQL API server handles queries and mutations (i.e writes) to CockroachDB. Below is a sample GraphQL schema for a student’s Goal . Within the type Query map, GraphQL allows MyMahi to add additional queries to fetch a subset of a Goal fields without having to define REST endpoints on the server side. There are two things worth highlighting in the code sample below: The ! indicates that a field cannot be null A user can define custom types such as CreateGoalPayload to use as return values. type Query { goals(goalId: UUID): [Goal!]! // additional queries can be defined here } type Mutation { createGoal(input: CreateGoalInput!): CreateGoalPayload! updateGoal(input: UpdateGoalInput!): UpdateGoalPayload! deleteGoal(input: DeleteGoalInput!): DeleteGoalPayload! } type Goal { id: UUID! goalType: GoalType! title: String! information: String! complete: Boolean! dueAt: DateTime color: String tasks: [GoalTask!]! notes: [GoalNote!]! updatedAt: DateTime! createdAt: DateTime! } enum GoalType { DEVELOPING_ME HEALTH_AND_WELLBEING ACADEMIC EXTRA_CURRICULAR CAREER } type GoalTask { id: UUID! information: String! complete: Boolean! updatedAt: DateTime! createdAt: DateTime! } type GoalNote { id: UUID! information: String! updatedAt: DateTime! createdAt: DateTime! } For each field of every type such as goals , a resolver function defines the logic for fetching data. MyMahi uses the node-postgres driver to query CockroachDB inside model.getGoals() , model.getTasksForGoal() , and model.getNotesForGoal functions. { Query: <ISchemaLevelResolvers>{ goals: async (_, { goalId }, context) => { const model = context.container.get<GoalModel>(GoalModel); return model.getGoals(goalId); } }, Goal: <IObjectTypeResolvers<IDBGoal>>{ tasks: async (goal, {}, context) => { const model = context.container.get<GoalModel>(GoalModel); return model.getTasksForGoal(goal.id); }, notes: async (goal, {}, context) => { const model = context.container.get<GoalModel>(GoalModel); return model.getNotesForGoal(goal.id); } } } Here is the getGoals() function which uses a SQL builder library to construct the CockroachDB query. public async getGoals(goalId?: string): Promise<IDBGoal[]> { if (goalId != null) { const query = DBGoal.select(DBGoal.star()).from(DBGoal).where(DBGoal.id.equals(goalId)).toQuery(); const goal = await executeTracedQuery<IDBGoal>(this.client, query).firstOrNull(); assertResult(goal, goalId); return [goal]; } else { const query = DBGoal.select(DBGoal.star()).from(DBGoal).order(DBGoal.createdAt.desc).toQuery(); return executeTracedQuery<IDBGoal>(this.client, query).results(); } } Retry Handling in CockroachDB CockroachDB is an ACID-compliant database , meaning the GraphQL resolvers can use transactions that run concurrently as if they were isolated from each other -- that is, one transaction cannot see the operations of another concurrent transaction. CockroachDB runs at SERIALIZABLE isolation, which is the safest level of isolation and protects against data anomalies that can happen at lower isolation levels. One example is write skew , which happens when a transaction reads a value, then conditionally writes based on that value while that value has already changed. The extra bookkeeping of SERIALIZABLE isolation can lead to greater contention in the database. This means that when the database is unable to find a serializable ordering of transactions, it will pick a transaction to abort and ask the application to retry it. In order to make it easy to make changes to their GraphQL schema and resolvers, MyMahi abstracted this transaction retry logic into a single function, shown below. export async function transactionWrap<T>(client: pg.ClientBase, operation: () => Promise<T>) { // Used for keeping track of operation completion let complete: boolean = false; // Used for keeping track of operation result let result: T; try { // Begin transaction with savepoint await client.query('BEGIN; SAVEPOINT cockroach_restart'); // Loop until operation complete (rethrown errors break out of loop) while (!complete) { try { // Try operation result = await operation(); // Release savepoint await client.query('RELEASE SAVEPOINT cockroach_restart'); // Mark operation as complete complete = true; } catch (e) { // Error code 40001 means the transaction is retryable if (e.code === '40001') { // Rollback to savepoint at start of transaction await client.query('ROLLBACK TO SAVEPOINT cockroach_restart'); } else { // No retry so rethrow error throw e; } } } // If we reach here then we have a result (rethrown error would bypass this code) return result!; } finally { // If operation is complete then commit transaction, otherwise rollback transaction entirely if (complete) { await client.query('COMMIT'); } else { await client.query('ROLLBACK'); } } } This function makes use of the special cockroach_restart savepoint which is specifically meant to be used for transaction retry logic. Another option for dealing with transaction retries is to use a loop that begins a new transaction . Conclusion When MyMahi set out to design their new application architecture, they experimented and embraced new technologies that would allow them to scale out their student platform with a small engineering team. CockroachDB was the scalable, transactional database that supported their product goals and integrated well with spot instances, AWS Lambda, and GraphQL. With plans to expand to Australia, MyMahi feels confident that their technology stack is designed for the future.", "date": "2020-10-08"},
{"website": "CockroachLabs", "title": "Cockroach Labs + Hacktoberfest: A Celebration of Open Source", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/hacktoberfest-2020-open-source/", "abstract": "Cockroach Labs is proud to participate in DigitalOcean’s annual Hacktoberfest , a celebration of open source and community innovation. The month-long event encourages everyone from experienced developers to students and code newbies to make positive contributions to an ever-growing community. All backgrounds and skill levels are encouraged to complete the challenge. Once a hacker submits four high-quality pull requests, they receive a custom t-shirt from DigitalOcean. We’ve already seen contributors improve our docs and are thankful for this incredible community. Open Source is a function of good Cockroach Labs supports Hacktoberfest’s investment in open source. As long-standing open source community members, we were unfortunately unsurprised by the spam that flooded repositories last week (our repositories included). Our founders and team members have decades of experience in open source, and along the way, have seen the good and the “not so good” that may come from it. While the spammy PRs may have seemed like a harmless act for a small gain, these activities put undue strain on maintainers of open source repos. In general, open source builds active communities of like-minded individuals who are drawn together to build freely available software for other people to use. These communities are typically welcoming, warm groups of people who interact in code and often form lifelong friendships based in mutual respect. Sometimes, however, an open source initiative can be taken advantage of. Their open nature opens them to this very “un”open activity.  For instance a repo can be forked and used for profit, without giving anything back. In 2019, we took steps to protect our growing community from this sort of activity by changing our license from Apache to BSL --we wanted to give the community all the benefits of open source, while still protecting our IP from bad actors. These bad actors are in the minority, but can still wreak havoc on open source companies and communities. In order to keep open source alive and well, we need to keep evolving how we think about contributions, licensing, and the rules around community participation. We applaud DigitalOcean’s quick response , and rule change, to keep Hacktoberfest a great event for all. How to Contribute to CockroachDB this Hacktoberfest In the spirit of Hacktoberfest, we invite you to contribute to CockroachDB regardless of your database or open source experience. We’ve labeled the CockroachDB repository as open for PRs, and are readily accepting Hacktoberfest submissions. You can work on code or documentation issues, or contribute apps to our sample apps repository. Whether you’re a new or experienced developer or tech writer, a seasoned contributor or new to open source, there’s something for everyone: Suitable for Project Resources For new developers Create a to-do app using CockroachDB and a language/ORM of your choice How to contribute to the to-do apps repository For Go developers Work on CockroachDB code: List of good first issues Your first CockroachDB PR For tech writers and docs enthusiasts Help improve CockroachDB docs: List of good first issues Docs contribution guide More instructions on how to contribute can be found in our Cockroach Labs Hacktoberfest repository . And if you need help, have questions, or want to chat, we invite you to join the #contributors channel in our Cockroach Labs Community Slack . Good luck, and happy Hacktober! Additional Hacktoberfest Resources An Absolute Beginner's Checklist for Hacktoberfest Open Source Contributor Summit: How to Make High-Quality PRs Hacktoberfest FAQs How to contribute to open source without compromising your mental health and work-life balance CockroachDB Community Slack", "date": "2020-10-06"},
{"website": "CockroachLabs", "title": "GigaOm Radar Report: How Enterprise Companies Compare Cloud Databases", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/gigaom-enterprise-cloud-databases-comparison-dbaas/", "abstract": "There’s a new paradigm in the database industry: Databases-as-a-Service. With their inherent advantages in elasticity, cost of ownership, and provisioning flexibility, the Database-as-a-Service (DBaaS) model is quickly becoming the easy choice for small companies and startups. But in enterprise environments, navigating a paradigm shift like this is a different story. In enterprise environments, it may be imprudent to break completely from the on-premises past and present. As is always the case, diligent enterprise organizations must survey the market and take a holistic approach that deals with their data needs in totality. The question becomes: are enterprises adopting the DBaaS model? And which DBaaS vendors best suit enterprise workloads? These are the questions independent research firm GigaOm set out to answer in the GigaOm Radar Report for Cloud Databases . The report analyzes both how enterprise companies are approaching the DBaaS market, along with a detailed breakdown of top players, including Cockroach Cloud, MongoDB Atlas, Couchbase, Amazon DynamoDB, Amazon Aurora, and Google Cloud Spanner. Get the Report → GigaOm’s Methodology: Evaluating Enterprise DBaaS Readiness In order to produce a thorough analysis, GigaOm first identified the key criteria enterprise companies are using to compare DBaaS platforms, and then metrics by which to evaluate these criteria. Per their findings, the DBaaS characteristics producing the most excitement among enterprise companies include: The adoption of multi-model support Automation, leading to operational and administrative simplicity Easy geo-replication Elasticity Performance enhancements over traditional platforms The potential for TCO improvements Support for large volumes of distributed data Flexibility of deployment on and across public, private, and hybrid clouds. GigaOm narrowed these characteristics down into the top three main criteria enterprises should use to evaluate different DBaaS platforms: 1. Support for Relational Model: Does the database speak SQL? This criteria gauges support for the traditional relational database model and the SQL language, either through native support for relational tables or as SQL-like layers placed on top of other database models for language compatibility. 2. Multi-Model Support: Does the database support more than one data model? This refers to database platforms that have native support for multiple database models. This typically means two or more of the following: the document model, the key-value model, the graph database model, the geo-spatial model, and the column-family/wide-column store model. 3. Managed Services Automation Offerings: How Easy is it to Use? This criteria addresses capabilities like auto-provisioning, autoscaling, auto-healing, geo-replication, and serverless operations. Taken together, these qualities streamline platform operations, and ease headaches for DBAs. A strong DBaaS offering should handle many of these common operations automatically. DBaaS Enterprise-Readiness Analysis: GigaOm’s Findings GigaOm’s report offers thorough analysis of Amazon Aurora, Amazon DynamoDB, Cockroach Cloud, Couchbase, DataStax, Google Cloud Spanner, Google Cloud BigTable, Microsoft Azure CosmosDB, Microsoft Azure SQL HyperScale, and MongoDB Atlas. The below chart indicates how different vendors perform across key criteria and evaluation metrics: The full report offers in-depth analyses of each vendor built on the key criteria report,  reports to assess a company's engagement within a technology sector. This analysis includes strengths and weaknesses of each platform, and forward-looking guidance around both strategy and product. GigaOm's findings also include their signature Radar report: A forward-looking analysis that plots the relative value and progression of vendor solutions along multiple axes based on strategy and execution. The Radar report includes a breakdown of each vendor's offering in the sector, and indicates GigaOm’s predictions for how each product will fare in the years to come. GigaOm’s Summary: DBaaS is on the Rise Key takeaways from GigaOm’s DBaas analysis include: Many organizations are already using the platforms extensively and usage is accelerating. Building on cloud-native architectures enables significant flexibility in deployment of the platforms across public and private clouds as well as on-premises. The platforms enable key scenarios around elasticity, performance, data distribution, automation, and data replication that are very difficult to get right in an on-premises environment. This is due to the underlying cloud architecture and is expected to lead to even more capabilities down the line. SQL continues to be the dominant querying language in most cases; however, extensive support for both multi-model databases and the NoSQL model is widely observed. The trends we see in the market are very clear. Cloud-native database platforms are the natural evolution of the battle-tested, on-premises workhorse databases of the past, and we expect their inherent architectural and operational advantages to continue to attract vast interest. Get the report → About GigaOm GigaOm is an unbiased third party firm providing technical, operational, and business advice for IT’s strategic digital enterprise and business initiatives. GigaOm’s vendor-neutral advice empowers enterprises to successfully compete in an increasingly complicated business atmosphere that requires a solid understanding of constantly changing customer demands. GigaOm authors Yiannis Antoniou and Andrew Brust GigaOm work directly with enterprises both inside and outside of the IT organization to apply proven research and methodologies designed to avoid pitfalls and roadblocks while balancing risk and innovation. Research methodologies include but are not limited to adoption and benchmarking surveys, use cases, interviews, ROI/TCO, market landscapes, strategic trends, and technical benchmarks. Analysts possess 20+ years of experience advising a spectrum of clients from early adopters to mainstream enterprises. GigaOm’s perspective is that of the unbiased enterprise practitioner. Through this perspective, GigaOm connects with engaged and loyal subscribers on a deep and meaningful level.", "date": "2020-10-15"},
{"website": "CockroachLabs", "title": "The Architecture of a Distributed SQL Database Pt 1: Converting SQL to a KV Store", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/distributed-sql-key-value-store/", "abstract": "CockroachDB was designed to be the open source database our founders wanted to use. It delivers consistent, scalable SQL in a distributed environment. Developers often have questions about how we've achieved this, and our short answer is this: Distributed SQL . Our long answer requires a deeper understanding of CockroachDB’s unique architecture . You definitely don’t need to understand the underlying architecture of CockroachDB in order to use CockroachDB. This blog series on the architecture of CockroachDB gives serious users and database enthusiasts a high-level framework to explain what's happening under the hood. Today, we’re exploring the lowest layers of CockroachDB: the key value (KV) store. Why CockroachDB Converts SQL to a Distributed Key Value Store At the highest level, CockroachDB converts clients' SQL statements into key-value data, which is distributed among nodes and written to disk. Our architecture is the process by which we accomplish that, which is manifested as a number of layers that interact with those directly above and below it as relatively opaque services. The bottom layer of that architecture is a key-value store. More specifically, it’s a distributed, replicated, transactional key-value store, and we’ll go over all those terms in this post. But at the top layer, ultimately, CockroachDB is exposed as a relational database. It speaks SQL, the lingua franca of data, and is wire compatible with Postgres. (To better understand which standard SQL features CockroachDB supports, you can check out our documentation on CockroachDB’s SQL Feature Support. ) So: how do we get from SQL to a KV store? And why? Is CockroachDB a Distributed SQL Database or a Distributed Key Value Store? CockroachDB is a distributed SQL database that’s enabled by a distributed, replicated, transactional key value store. The key value layer is only available internally, because we want to be able to tailor it to the SQL layer that sits on top, and focus our energies on making the SQL experience exceptional. In fact, the CockroachDB project began as a key-value store , and SQL layer came after the fact.  We knew when starting that a distributed key-value API was not the endpoint we wanted to provide. We wanted a higher level structured data API that would support tables and indexes. After lots of soul searching, we embraced the inevitable and moved forward full-speed with SQL as the core of our structured data layer in 2015. Here’s what the underlying key-value store enables: It facilitates efficient distribution of data within the database It bakes in natural separation for distributed transactions Atomic columns enable dynamic schema change It’s extendable, which lets us add functionality like geo-partitioning Ultimately, this allows CockroachDB to retain the efficiency of a KV store but gain the natural ability to distribute data, and still speak SQL. A Tour of CockroachDB’s Key-value Store As mentioned, the bottom layer of CockroachDB is a distributed, replicated transactional key-value store. Let’s go over that, term by term. A key-value store means that it contains keys and values. Keys and values are arbitrary strings. This means when you store data in a Cockroach table, CockroachDB stores that data as a key and a value. The key is the thing we want to sort things on, and then the value is each of the columns that we want to store for that particular key. Let’s go over an example. Let’s say we want to create a DOGS table of all the office dogs at Cockroach Labs. In a traditional database, when we want to create a table, the SQL and subsequent table entry might look like this: Our table entries have an ID--some random arbitrary unique ID--the dog’s name, their weight, and any other information we might want to store. In CockroachDB, this is still what it looks like to the end user. But under the hood, it’s doing something very different from a traditional SQL database. It’s storing tabular data in a monolithic sorted map of KV pairs, where every table has a primary key, there is one key/value pair per column, and keys and values are all strings: Ultimately, much deeper encoding of the keys occurs within the database to: Create massive efficiencies for access of data through sorting Ensure against NULL values Maintain integrity We use multi-version concurrency control to process concurrent requests and guarantee consistency, which means that the keys and values are never updated in place. Instead, in order to update a value, you write a newer value and that shadows the older versions. Tombstone values are used to delete values. What this provides is a snapshot view of the system for transactions. We describe CockroachDB as having a model with the key space; that means there aren't separate key spaces used by different tables, it uses one big key space and we just do tricks with how we structure the keys. Here’s a depiction of what the monolithic key-space looks like, divided into ranges: It's a monolithic key-space ordered by key and divided into 64-megabyte ranges. Sixty-four megabytes was chosen as an in-between size; it’s small enough for ranges to be moved around and split fairly quickly, but large enough to amortize an indexing overhead. As a side note, ranges don't occupy a fixed amount of space; they only occupy the space that they're consuming and they grow and they shrink as data is added to them and deleted from them. Efficient Range Scans Let’s say I want to do a range scan for the keys between muddy and stella in our DOGS table. Because the keys are ordered, we can do a really efficient range scan. You might notice that this diagram looks very much like a B-tree . That’s part of what enables the speedy scans. You could imagine this range, the second range, and the third range are all living on different nodes within the cluster. If I want to do a range scan, I would have to go to every single node across the database. That's not very efficient. But with this architecture, we can actually do it just across these small ranges. Because everything is lexicographically ordered, we can do some very efficient range scans within CockroachDB. Range Splitting: Automated Sharding Transactions are used to insert and delete data into ranges. We’ll go into transaction details in another post, but for now, let’s use a simple example to show how they affect ranges. Let’s say a co-worker adopts a new dog named Sunny, and we need to add sunny to DOGS . If the Raft leader of the range indicates that there’s space for sunny , it’ll insert as expected. We go into the indexing structure, we see this corresponds to range three. We go to range three and we can insert the data, and then the insert is done. But what if there’s not space in the 64 MB range? What if someone else adopts a dog, and there’s no space in the range for the new dog, rudy ? If this were a Postgres instance, we’d need to shard the database to solve this problem. But because Cockroach is a distributed SQL database built on a distributed key value store, it shards for you, creating a fourth range: Splitting the range involves printing a new replica, a new range, moving approximately half the data from the old range into the new range, and then updating the indexing structure. The way this update is performed, it's using the exact same distributed transaction mechanism that we were using to insert data into range itself. Where does key value data live in CockroachDB? Another important thing to note about key-value data in CockroachDB: it's stored down on a local key-value store. We use Pebble for that purpose. The 20.1 release of CockroachDB last May introduced Pebble as an alternative storage engine to RocksDB. With the release of 20.2 this fall, Pebble will become CockroachDB’s default storage engine. We’ll keep working our way up through the levels of CockroachDB in this blog series, but if you’d like to learn more about the architecture of CockroachDB, you can read our docs or watch the full webinar on the subject here . Watch the Full webinar: The Architecture of a Distributed SQL Database", "date": "2020-10-14"},
{"website": "CockroachLabs", "title": "Intro to Multi-Region Distributed SQL Topologies", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/multi-region-topology-patterns/", "abstract": "CockroachDB is a Distributed SQL database. That means for the person doing the querying, everything looks like the SQL they’ve known for years. But under the hood, there’s a lot more going on. In order to achieve the bulletproof resiliency and elastic scalability CockroachDB delivers, data is replicated and geographically distributed across nodes of a cluster, and read and write requests are automatically routed between nodes as appropriate. Replication and geo-distribution mean that your nodes have a shape--a topology--and how you design that topology allows you to make important tradeoffs regarding latency and resilience. In a recent episode of The Cockroach Hour , database experts Tim Veil and Jim Walker met with Jesse Seldess, VP of Education at Cockroach Labs, to walk through why topology patterns matter in CockroachDB , and how to select the one that works best for your use case. Watch the webinar → Unlike with many other database platforms, you’re offered a lot of choice with CockroachDB. Do you want to optimize for low latency? Or do you want to optimize for resilience? It's important to review and choose the topology patterns that best meet your latency and resiliency requirements. In this recap of The Cockroach Hour, we’ll cover common topology patterns in CockroachDB, what the default topology pattern is, and how to choose the right one. What’s a Data Topology Pattern? Data typology patterns map the shape of your cluster. Are you deployed in a single region across multiple availability zones? Are you deployed across multiple regions? And where is data located across that cluster, across those nodes? Since CockroachDB can control data locality at the database, table, and even row level, you have a lot of options here, depending on how you want your database to perform. Typology patterns are especially crucial for multi-region deployments. In a single-region cluster, this behavior doesn't affect performance because network latency between nodes is sub-millisecond. But in a cluster spread across multiple geographic regions, the distribution of data can become a key performance bottleneck. For that reason, it’s important to think about the latency requirements of each table and then use the appropriate data topologies to locate data for optimal performance. When you're ready to run CockroachDB in production in a single region, it's important to deploy at least 3 CockroachDB nodes to take advantage of CockroachDB's automatic replication, distribution, rebalancing, and resiliency capabilities. Here’s what that might look like in a single-region deployment: Because each range is balanced across availability zones (AZs), one AZ can fail without interrupting access to any data. However, if an additional AZ fails at the same time, the ranges that lose consensus become unavailable for reads and writes. This is where multi-region topologies can help. Each multi-region topology assumes the below setup: But they’re very customizable from here. Because, as mentioned, CockroachDB can control data location at the database, table, and row level, we have a lot of choice regarding multi-region topologies. How to Choose a Multi-Region Topology Pattern In the database topology webinar , CockroachDB expert Tim Veil says there are two questions he asks every customer he works with, when setting up topology patterns: Latency: How quickly do you want to access the data? Resilience: What do you want to survive? Cockroach is a Distributed SQL database . When we say distributed, what we mean is that the database can span a wide physical geography. It can span multiple what we call failure domains, or things that can fail. This is an incredibly important value proposition. And so one of the questions that we have that we can ask that others can't is, when you're building this database topology, when you're building a solution based on Cockroach, is: What do you want your database to survive? Survival, for CockroachDB, isn't some meager existence. Survival means, what do you want to happen to your database and continue to serve reads and writes without interruption? That's survival, that's real survival for us. So with that in mind, what are the different ways you can configure Cockroach so that you can continue to serve reads and writes even or while significant failures are occurring, either planned or unplanned? All of our topology patterns are available on our docs , or you can watch the full webinar . In the rest of this blog, we'll dive into three common topology patterns. Topology Pattern #1: Follow the Workload (Default Pattern) When there’s high latency between nodes (for example, cross-datacenter deployments), CockroachDB uses locality to move range leases closer to the current workload. This is known as the \"follow-the-workload\" pattern, and it reduces network round trips and improves read performance. What can Follow the Workload Patterns Survive? Because this pattern balances the replicas for the table across regions, one entire region can fail without interrupting access to the table: Follow-the-workload is the default pattern for tables that use no other pattern. In general, this default pattern is a good choice only for tables with the following requirements: The table is active mostly in one region at a time, e.g., following the sun. In the active region, read latency must be low, but write latency can be higher. In non-active regions, both read and write latency can be higher. Table data must remain available during a region failure. Topology Pattern #2: Geo-Partitioned Replicas Using this pattern, you design your table schema to allow for partitioning , with a column identifying geography as the first column in the table's compound primary key (e.g., city/id). You tell CockroachDB to partition the table and all of its secondary indexes by that geography column, each partition becoming its own range of 3 replicas. You then tell CockroachDB to pin each partition (all of its replicas) to the relevant region (e.g., LA partitions in us-west , NY partitions in us-east ). This means that reads and writes in each region will always have access to the relevant replicas and, therefore, will have low, intra-region latencies. What can Geo-Partitioned Replica Topology Patterns Survive? This pattern optimizes for low latency over resilience. Because each partition is constrained to the relevant region and balanced across the 3 AZs in the region, one AZ can fail per region without interrupting access to the partitions in that region: However, if an entire region fails, the partitions in that region become unavailable for reads and writes, even if your load balancer can redirect requests to a different region: The geo-partitioned replicas topology is a good choice for tables with the following requirements: Read and write latency must be low. Rows in the table, and all latency-sensitive queries, can be tied to specific geographies, e.g., city, state, region. Regional data must remain available during an AZ failure, but it's OK for regional data to become unavailable during a region-wide failure. See it in action: Read about how an electronic lock manufacturer and multi-national bank are using the Geo-Partitioned Replicas topology in production for improved performance and regulatory compliance. Topology Pattern #3: Geo-Partitioned Leaseholders Using this pattern, you design your table schema to allow for partitioning, with a column identifying geography as the first column in the table's compound primary key (e.g., city/id). You tell CockroachDB to partition the table and all of its secondary indexes by that geography column, each partition becoming its own range of 3 replicas. You then tell CockroachDB to put the leaseholder for each partition in the relevant region (e.g., LA partitions in us-west, NY partitions in us-east). The other replicas of a partition remain balanced across the other regions. This means that reads in each region will access local leaseholders and, therefore, will have low, intra-region latencies. Writes, however, will leave the region to get consensus and, therefore, will have higher, cross-region latencies. What Can Geo-Partitioned Leaseholder Patterns Fail? Because this pattern balances the replicas for each partition across regions, one entire region can fail without interrupting access to any partitions. In this case, if any range loses its leaseholder in the region-wide outage, CockroachDB makes one of the range's other replicas the leaseholder: See it in action: Read about how a large telecom provider with millions of customers across the United States is using the Geo-Partitioned Leaseholders topology in production for strong resiliency and performance. Additional Resources and Next Steps You can learn more about different topology patterns (like duplicate indexes) in the webinar, or in our docs. For a production checklist and suggested next steps, here’s what Tim tells all CockroachDB customers when selecting a topology pattern: Review how data is replicated and distributed across a cluster, and how this affects performance. It is especially important to understand the concept of the \"leaseholder\". For a summary, see Reads and Writes in CockroachDB . For a deeper dive, see the CockroachDB Architecture documentation. Review the concept of locality , which makes CockroachDB aware of the location of nodes and able to intelligently place and balance data based on how you define replication controls . Review the recommendations and requirements in our Production Checklist . These high-level topology overviews don't account for hardware specifications, so be sure to follow our hardware recommendations and perform a POC to size hardware for your use case. Adopt relevant SQL Best Practices to ensure optimal performance. Want to learn about all our topology patterns? Watch the webinar -->", "date": "2020-10-21"},
{"website": "CockroachLabs", "title": "5 Reasons to Build Multi-Region Application Architecture", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/5-reasons-to-build-multi-region-application-architecture/", "abstract": "TL;DR - Multi-region application architecture makes applications more resilient and improves end-user experiences by keeping latencies low for a distributed user base. This blog will dive into each of the top five reasons why multi-region application architecture is worth building. #1. Fault Tolerance in Multi-Region Application Architecture Fault tolerance is the ability of a system to endure a failure of some kind and continue to operate properly. The unfortunate reality is that failures happen . A fire, a flood, or an extreme storm can take out a data center, an availability zone, or an entire region. But that doesn’t have to result in an outage. Multi-region applications are resilient in a way that single region and single availability zone applications are not. Single region architecture can survive machine failures. Single availability zone architecture can survive an AZ failure. But only a multi-region application architecture can survive a region failure. If your application has high-availability requirements then multi-region architecture is important. #2. Low Latency for geographically dispersed users How do you get low latency when users are spread out? Multi-Region application architecture helps deliver low latency by making it possible to keep data close to users even when those users are distributed all over the world. Consider an example: Imagine your business has users in New York City and Sydney, Australia. If you just deploy your application in a data center near New York, then users in Sydney will experience high latencies. They have to wait while their requests travel all the way to New York and back. But if you deploy your applications in multiple regions, and you put another data center closer to Sydney, then the client request time will dramatically improve. A good rule of thumb is to keep latencies under 100 milliseconds . That is the maximum threshold for an experience to feel instantaneous. When a user base is spread out all over the world it’s challenging to keep latencies under 100 ms because the data is limited by the speed of light. The only way to achieve low latency, and great user experiences, is to keep the data close to the users with multi-region application architecture. #3. Grow a user base Some organizations prefer to gain a meaningful quantity of users in a new region before adding a database in that region. There is a risk to this strategy. If your only data centers are in US-East then you’re most likely serving users in Sydney a high latency experience. If those users are having bad experiences then your application is unlikely to grow. Other companies take the opposite approach. A good example would be the online sports gambling companies from Europe that chose to add data centers in the United States as soon as gambling became legal in some US states. These businesses saw the potential to gain new users and then scaled their application architecture to that new region to make sure the new users would have a good experience. #4. Application architecture flexibility Once multi-region application architecture is in place, it becomes easier to scale applications up or down. There are three primary reasons why this is the case: Database schema When you design a schema for multi-region architecture, you need to partition your database, which requires identifying a column to partition on, actually partitioning the database on that column, and then setting up zone constraints. If you do that once, it’s a much lower lift to just add new regions with ALTER statements. Cloud deployments You have your database and your application deployed in multiple servers, across multiple regions. Most cloud providers make it easy to just add another regional server. Then you just need to deploy the database and app to that server. It’s also likely the case that you have used docker and a kubernetes engine to deploy the database and the app, which means you’ve written some docker and kubernetes deployment configuration files (i.e. a Dockerfile and a Kubernetes manifest file). With these already written, you simply use them to deploy to a new region. This is much easier than writing the files from scratch. Application logic You’ve already written the application to filter queries on partitions, and possibly to be aware of user locations. This means you don’t have to edit the application very much in an expansion to a new region. To return to the example above, it would be much easier for the online sports betting company to expand into the US if they already had a multi-region deployment in Europe. #5. Comply with Data Privacy Laws & Regulations There are now over 200 data privacy and data storage regulations around the world, the most famous probably being GDPR. And all the signs indicate that there will be more of these regulations every year. These regulations often prohibit storing certain data outside of certain boundaries . With the right multi-region deployment, you can keep data where it legally needs to be, by pinning data within the boundaries of a physical location. Additional resources and next steps If you’re building an application with the hope or the expectation that your user base will extend beyond a single region then it makes sense to research ways to build multi-region application architecture. Here are a few links to get you started: Video Series: How to Build a Multi-Region Application Blog: Multi-Region Distributed SQL topologies Blog: The Rise of Continuous Resilience Docs: Low Latency Reads and Writes in a Multi-Region Cluster", "date": "2020-10-27"},
{"website": "CockroachLabs", "title": "Faster Bulk-Data Loading in CockroachDB", "author": ["Bilal Akhtar", "Yevgeniy Miretskiy", "David Taylor", "Sumeer Bhola"], "link": "https://www.cockroachlabs.com/blog/bulk-data-import/", "abstract": "Last year the BulkIO team at Cockroach Labs replaced the implementation of our IMPORT bulk-loading feature with a simpler and faster data ingestion pipeline. In most of our tests, it looked like a major improvement: the release notes for CockroachDB v19.2 touted \"4x faster\" IMPORT. Many a 🎉 reaction was clicked, and the team moved on to new projects. But over the following months, it became clear we had celebrated too soon: we started to get reports of some IMPORTs that, instead of being faster, were much slower or even getting stuck. Armed with a test that could reproduce such a case, we started to dig. What followed was a search to find out what had happened. We spent weeks digging through the code, reading logs, and running experiments, and building new debugging tools. When we finally figured out what was going wrong, we realized fixing it would require changes at the very lowest layers of our storage system. In the end, the solution hinged upon our recent switch from RocksDB as our key-value store to Pebble , where we were able to add a new algorithmic approach to organizing files that led to a massive improvement, measured at an 80+% reduction in ingestion time time on the standard TPC-C benchmark dataset. Note: a firm understanding of how RocksDB or other LSM-based storage engines operate will be required for much of this to make sense. For those that want to pause here and brush up on the subject, this is a great introduction to LSM trees . In this post, we want to walk through our initial rewrite of IMPORT, how we isolated the cause of the slow-downs and, most importantly, how we fixed it. But to start, let's cover some basics. What Is IMPORT in CockroachDB? We built IMPORT to make it easier and faster to get large amounts of data into CockroachDB. CockroachDB's SQL layer is built on top of a distributed transactional key-value store. When a row is written via SQL in a normal INSERT statement, the SQL layer encodes the row as bytes. The encoded columns in the table's primary key become the key in that key-value pair and the rest of the columns are encoded in the value. For each row written the SQL layer sends those KVs in transactional Put-requests to the KV layer. The IMPORT process starts by running the format-specific input reader, like a CSV reader or Avro codec, to extract logical rows from the input data. It then uses the schema to encode those rows into the same byte KV pairs the SQL layer would produce if those rows were sent as INSERTs. But instead of sending the KVs as individual transactional Puts, IMPORT instead assembles them into large SSTable (\"SST\") files. SSTs are the native storage format used by CockroachDB's on-disk storage engine, so IMPORT can send them to the storage layer to be directly ingested, with minimal per-key processing compared to a Put'ing each key. When sending these files, we take every possible shortcut through various layers of the stack, skipping nearly the entirety of the transaction layer and using clever tricks to avoid making copies when doing things like replicating in the Raft log or writing to the write-ahead log. The result is a process that can bulk-load orders of magnitude faster than running regular INSERTs. Two-Pass IMPORT When we first built IMPORT, we used a two-pass approach. Since CockroachDB is a distributed system, we want to distribute the data we're importing across the cluster. For example, if you were importing a file of rows whose keys were uniformly distributed between 1 and 100 into three nodes, you might want to say keys 1 through 33 to go to node 1, keys 34 through 66 to node 2, etc. But when we start an IMPORT of an arbitrary file, we have no idea what the distribution of the keys it will produce is, and thus have no way to specify such a partitioning. This is what motivated the two-pass approach: we ask every node to read its assigned input files and convert them to KVs twice . In the first pass, they are told to discard all but a small sample of the resulting KVs, which then establishes the distribution of the KVs and partitions them roughly evenly. All of the readers then read and convert their assigned input a second time, this time using that partitioning to route streams of the produced KVs to destination nodes, which can assemble them into SSTables to ingest. One complication though is that SSTables are sorted but the incoming KVs could be in any order depending on the input, so at the destination node, we buffer all the produced KVs to a temporary RocksDB instance on disk until the input is fully read before reading the now sorted buffer back to produce SSTables and send those to be added to the storage layer. This is how IMPORT worked from its introduction until v19.2. While it was much faster for bulk-loading than running SQL INSERTs, we thought we could do better. One-Pass IMPORT In the two-pass IMPORT, we were writing every produced KV to a buffer on disk before reading it back to produce SSTs. But if our goal is to write all the keys to disk, why do it all to the buffer just to then read it back and do it again? Could we just write it directly to our storage instead? And, as long as we were getting rid of the extra write-pass, could we eliminate the extra read-pass too? Unlike the statically partitioned temporary buffers, our actual KV storage layer automatically and dynamically determines when a given node is over-full and splits some of its key range off to rebalance to another node. If we just directly sent out-of-order data, as it was produced, to the KV layer it should spread it around on its own, right? And thus we set about switching to \"direct\" IMPORT. In this implementation, we would only do one pass, in which the keys emitted from the frontend would be buffered in each reader but only up to some memory-based limit, then locally sorted and batched into SSTables that could be sent directly to the KV storage layer. This approach cut out the distributed shuffle and sort phase we had been doing prior to adding to storage. Instead, when the readers finished their first read pass and had flushed their buffers, the IMPORT was done. This approach cut out two of three read passes -- the sampling read of the input and reading the buffer -- and one of two write passes -- writing to the buffer -- so it had to be faster, right? Of course we knew it wasn't quite that simple. LSMs like RocksDB store data in order. Adding data to RocksDB out of order would mean it would need to do more work to get it in-order. But we were already adding out-of-order to our temporary RocksDB, we reasoned, so it couldn't be worse to just do the same to the real one instead. Plus, in many cases the data is dumped by its source system in- order, making buffering to sort entirely wasted work. Armed with this reasoning, we forged ahead with \"direct\" IMPORT, and our early benchmarks seemed to show what we expected -- eliminating the sampling phase and the extra write pass made most of our IMPORTs 4x faster. Giving a feature a big speed boost while deleting most of its code seemed like a win/win, and we celebrated shipping it in our v19.2 release. Examining IMPORT Slowdowns with LSM Tree Visualizations After v19.2 was released, most feedback on IMPORT was positive -- but we also got reports that some IMPORTs were slowing to a crawl or \"getting stuck\". We soon realized it was mostly in larger tables that needed to ingest large volumes of out-of-order KVs. This could be when the rows themselves were out-of-order relative to their KV encoding, or was more commonly seen with tables that included secondary indexes. Indexes almost always produce out-of-order KVs. If you consider a \"user\" table, while user ID 1 and user ID 2 may appear in-order, ID1 might have email xyz@example.com and ID2 might have abc@example.com , so an index on that email column would be in a completely different order than the source rows. In every case we looked at, we saw that the IMPORT process was blocked sending an SSTable to the storage layer. When we inspected those requests, we found they were being intentionally delayed by a backpressure system built to protect the storage engine and ensure its background maintenance work kept up with incoming data. The signals that were causing us to delay were either that we had too many files in the least-sorted part of the LSM (L0) or that we had too many bytes that required compaction. Our unsorted ingest had a known edge-case where it could produce many \"small\" SSTs: if a given reader read a fixed-size buffer of rows that were uniformly distributed over the key-range of a massive table that had been split into many ranges to spread it around a cluster, the subset of that buffer that would be assigned to the SST sent to any one range could be very small. Throwing thousands of tiny files at the LSM is actually much worse for bulk-loading than the old individual Puts we use for normal writes. This is because LSMs have a built-in mechanism for batching up lots of puts into a single file: the memtable. Thus the first optimization we added was a heuristic to fall-back to Puts to the memtable when ingesting an SST that was \"too small\" to be worth adding to the LSM as a separate file, i.e. where the overhead of a file to compact would outweigh the write-amplification of adding to the memtable and WAL (write-ahead log) and then flushing. This helped alleviate the cases where the number of files grew too large due to the \"tiny SST after sorting problem\", but we still saw IMPORTs that, after starting off fast, would suddenly slow to crawl. We noticed that this was usually happening when, for inexplicable reasons, the storage engine compactions would suddenly no longer keep up with the ingestion load and the backpressure would kick in. Why was compaction falling behind so suddenly and why didn't it recover once the backpressure gave it some time to do so? We were stumped at this point, and brought in the help of our colleagues on our storage engine team so we could stare at our logs and metrics together. The \"Inverted\" LSM Tree One thing common to all the cases where the IMPORT slowed to a halt was that the node triggering the backpressure had an \"inverted\" LSM tree. Typically, an LSM wants the majority of its data in its \"lowest\" levels. As new data comes in, if a file contains data that overlaps with an existing file, it has to be placed \"above\" that other file and then the compactor has to do some work to combine it and move it down. In RocksDB, levels are numbered 0-6, with L0 being the highest/newest, and L6 being the lowest/oldest. The highest, non-L0, non-empty level is called LBase, and typically the LSM has files in L0 as well as LBase - L6. If too much data piles up in an upper-level while you keep adding more data, that additional data will continue to pile up above that level, and you may end up with a top-heavy \"inverted\" LSM, where new data that might have ingested to a lower level in a well-shaped LSM just keeping piling up at the top, making the problem worse. This is why those back-pressure systems were added in the first place: to give the compactor a chance to resolve such a situation before it got worse. But in our case they clearly were not working: around the time of the inversion we often saw RocksDB embark on a giant compaction from L0 to LBase which could take it minutes, sometimes ten or more, to complete. During that time, it cannot start other compactions from L0, so once the limit is reached, to prevent the problem getting any worse, the backpressure would kick in and bring the entire job to a standstill. But we still didn't understand why it suddenly got so bad. What caused this sudden giant compaction? It was clear the out-of-order ingestion was adding lots of overlapping SSTs and that that put more load on the compactor. But we were writing everything out-of-order to the temporary buffer RocksDB before and it never had this problem. Something didn't add up but we were struggling to see what it was. LSM Tree Visualization Our co-founder and resident RocksDB expert started work on a small LSM tree visualizer tool that could process the RocksDB manifest and produce a visual timeline of the files and how they overlapped. The visualizer tool though made it much more obvious, as we scrolled through the timeline of one these tests, that right before one of these giant compactions started, we saw an unusual file added to L0. Specifically, it was a very small, in total size file but it was exceptionally \"wide\" in terms of the key-span it overlapped. Why is that an issue? Consider the example LSM tree below, where each horizontal line is an SSTable occupying a part of the keyspace: Figure 1: A simple example of an LSM tree with two levels, and with SSTables visualized as horizontal lines occupying a slice of the keyspace. For simplicity of explanation, we have defined LBase as L6, so the only non-empty levels of the tree are L0 and L6. Here, L6 has 4 non-overlapping sstables; all levels outside of L0 are required to not have any overlapping SSTables within themselves. L0 also has 4 sstables, with some overlap between them as well as with L6. Consider a “flush” happens from memory to L0, adding the red SSTable below: Figure 2: Same LSM example as above, now with a newly added SSTable to L0 that occupies a wide slice of the keyspace, overlapping with all the older files The wide SSTable itself doesn’t need to be very large in disk size (in our observations, we saw very wide SStables that were only a couple kilobytes large). But it overlaps with all the other files in L0 and L6. If we wanted to move it “out” of L0 and into Lbase, we’d have to run a compaction including all those files (to maintain LSM tree invariants). Many of those overlapping files pulled in from L0 and L6 could be large. Even if the overlapping files weren’t large, the mere presence of these wide files would block other incoming SSTables in that wide key range from being ingested at lower levels, resulting in more and more bytes accumulating above them. There’s a high likelihood that we’d eventually have to do a mega-compaction that would take a while and use up valuable disk bandwidth. All the overlapping files in L6 would have to be rewritten. If that mega-compaction with the very wide SSTable gets started, while more L0 SSTables continue getting added to the LSM, the LSM would end up looking like this: Figure 3: A compaction chosen to compact away the wide L0 file, that ends up picking all overlapping older files in L0 and L6. The newly added L0 files at the top cannot be added to a running compaction (it’s already running), and they cannot be compacted into L6 as they conflict with older, overlapping, already-compacting files. So at this stage, RocksDB would schedule an intra-L0 compaction to join those files into one L0 file: Figure 4: A concurrent intra-L0 compaction being chosen while the mega L0 -> L6 compaction is running Assuming the smaller intra-L0 compaction finishes before the mega L0->L6 compaction, the LSM would look like this, with the newly-produced file in red: Figure 5: The intra-L0 compaction finished and produced another wide L0 SSTable We can see how this intra-L0 compaction was a highly unproductive exercise; it ended up producing another wide SSTable out of what were originally narrow ones. This new wide SSTable will continue to require expensive large L0 -> LBase compactions that rewrite most of LBase, and will block any newer files from easily getting compacted out of L0 too. This begs the question: why were wide SSTables being generated to begin with, especially if they didn’t have many bytes in them? And again, why was this only a problem in directly ingested IMPORTs and not in the previous version, ingesting the same keys? Sharing a (Key)space Can Be Challenging Once our visualizer tool made the problematic \"wide\" file easy to spot, we examined its key bounds and had a breakthrough: its span was the upper-bound of our total addressable logical keyspace, aka \"KeyMax\". Of course that would overlap with many other files: it overlaps every possible key above its start key! But why did we end up with that as the upper-bound for an SST? Obviously our CSV can't produce a row with that key, as its keys would all be within its assigned table span prefix. This was our first indication that it wasn't just our CSV's produced keys we needed to think about. A little digging revealed that that key was being produced by an unrelated write being done by one of our KV layer's internal housekeeping routines, not our IMPORT. Finally we understood what was different from the old two-pass IMPORT: two-pass IMPORT did its sorting in its own temporary rocksdb, and where the KV's housekeeping writes didn't get mixed into the same files. We finally had our smoking gun. And happily it looked like a quick fix to alter the KV routine to use a more narrowly scoped upper-bound key. We thought we were as good as done. We excitedly merged the change to the KV's housekeeping routine and re-ran our tests... only to observe the same compaction death spiral. But we had fixed it, hadn't we? Going back to the LSM visualizer, it immediately highlighted another \"wide\" SST being added and triggering the giant compaction of death. Looking at this file's key-span bounds again revealed that it was IMPORT mixing its data keys with the system's internal keys that was to blame. This time it was internal keys used to store metadata, which are kept under a prefix that sorts at the minimum end of the keyspace, below all table data. When we send a write to a range, the range writes the associated key to its storage but also updates its own internal range metadata for things like number of keys, their size, etc. The range periodically writes its metadata to its metadata key and if both the metadata write and the data write are sent to RocksDB and end up in its memtable together, when that memtable flushes to a file, that file's span will be at least from that metadata key all the way up to that data key, which means it overlaps all other data keys up to the written key . The visualizer made this very apparent: it showed a tiny file appear in L0 overlapped nearly the entire keyspace, and thus immediately triggered our giant compaction of death. Unlike the previous case of KeyMax however, there was no easy way to avoid the writes to the metadata keys this time. Those familiar with RocksDB might immediately suggest using its \"Column Family\" feature  which provides nearly entirely separate LSMs for logically separate key-spaces. If we stored our metadata keys and table-data keys in separate column families, we'd be all set, right? However changing to do so now would pose a very tricky migration challenge. Alternatively, a less drastic separation could be provided by a feature that manually partitioned one key-space in place, such as has been explored in academic projects like PebblesDB's \"guards\" . While manual partitioning approaches like those could address our metadata vs table-key spans, the same issue would arise even in a pure table data setting: Our input CSV could have rows in any order, including an order of rows that could generate a similarly \"wide\" span, e.g. if it had row 0 followed by row 1,000,000,000. Introducing a New KV-Store: Pebble During this time, we were building a new storage engine in-house to replace RocksDB. It’s called Pebble (no relation to the above-mentioned PebblesDB), and has been in development since late 2018. Pebble is designed to be compatible with RocksDB in its on-disk format, making bidirectional migrations very straightforward. Written in Go, it’s designed to efficiently implement just the select set of RocksDB features that CockroachDB relies heavily on. We discuss Pebble in this blog post , and it is already the default storage engine for CockroachDB v20.2. Owning our own storage engine lets us incorporate CockroachDB-specific features and improvements down in the storage engine itself. As part of our import speedup efforts, we reorganized L0 of the LSM into dynamic “sublevels”, where each sublevel can be seen as yet another non-L0 level of the LSM. These sublevels would be ordered by key age, such that keys in newer sublevels will always shadow keys in older sublevels, just like with regular levels. And each sublevel would also contain a set of non-overlapping SSTables, just like other levels. The sublevel organization is a function only of the key age and key span of files, so it is backwards compatible with RocksDB. How does the switch from RocksDB to Pebble affect our slowdown issue? For one, instead of having to read and merge every L0 file independently for every read and compaction, we can reduce the effort down to only the number of sublevels that are being read;  only one sstable would need to be read in every sublevel at a time. Our read amplification gets significantly reduced. Going back to our earlier example, where L0 files are ordered by age only: Figure 6: Example from above, with many L0 SSTables, some wide and some narrow. Here, we have 8 SSTables in L0. Not all of them overlap with all the other files; so reading and merging 8 files every time we do a key read or compaction in L0 is very inefficient. However, if we move files into sublevels while still ensuring that newer keys always remain “above” older keys, the LSM tree ends up looking like this: Figure 7: Same example, but with L0 organized into sublevels based on key overlaps and file age. By organizing L0 into dynamic sublevels while still respecting all existing LSM invariants, we’ve managed to reduce read amplification in our example down from 8 to 4. This is already a significant enough improvement. But the extra-wide SSTable in L0.2 is still preventing us from optimizing this further. Organizing SSTables this way has another benefit: it allows us to easily calculate overlaps across sublevels, as files in each sublevel are sorted by key ordering. Previously, all files in L0 were just sorted by age, making overlaps harder to identify. Once these overlaps are known, we can calculate “split points” to reduce excessive overlapping even further. Any future flushed SSTables would be split at these split points to prevent excessively wide SSTables. In our example, our algorithm would split the L0.2 SSTable into three SSTables: Figure 8: Same sublevel example, but with wide L0 files split at calculated flush points This can be more efficiently organized as: Figure 9: Same example, after “moving down” non-overlapping fragments The right half of the LSM is only 3 sublevels tall now! A nice improvement, but nothing too groundbreaking. However, when choosing compactions, this gives us three main benefits: Increased L0 -> LBase compaction concurrency : As there are no super-wide files overlapping all other files, we can schedule more concurrent L0 -> LBase compactions, each of which can operate on a subset of the keyspace without having to rewrite all of LBase or read most files in L0. Reduced need for intra-L0 compactions : Due to increased concurrency of L0 -> LBase compactions, and because read amplification is now equal to the number of sublevels (and not number of L0 files), there is a significantly reduced need to run intra-L0 compactions. More efficient compaction picking : The sublevel structure itself tells us all the information we need to know to pick efficient compactions. We know that our goal should be to reduce read amplification in L0 to allow for more writes into it in the future; so we can pick the files that cause the “tallest” peak in the sublevel structure, and prioritize that first. For our running example, these would be the 3 L0 -> LBase compactions we can pick with compaction #1 being of the highest priority as it would reduce the sublevel height by 1: Figure 10: Lack of wide files allows for shorter, more concurrent L0 -> LBase compactions Since compaction #2 and compaction #3 conflict on one LBase file, both of them cannot be scheduled concurrently. However each of compaction #2, #3 does not need to include more than 3 files from L0. This is critical in reducing the size of compactions. Smaller compactions can result in a healthier LSM because they don’t block other compactions for long durations. And compaction #1 can run independently of the others, concurrently. So in this example two compactions can concurrently run from L0 to L6. This is an improvement over RocksDB, which is limited to running one compaction at a time out of L0. Additionally, if needed, compaction #1 and compaction #2 can proceed concurrently with an intra-L0 compaction, as shown in the following diagram: Figure 11: Intra-L0 compactions are also unlikely to produce wide L0 files Since such intra-L0 compactions are sub-level aware, they do not result in wide files, and therefore maintain sub-level health. In practice, with sublevel-based compactions and flush splits, we see that throughput to move bytes out of L0 is high enough to mitigate the need for intra-L0 compactions altogether. This keeps the write path as fast as possible. Algorithmic aspects of sub-level compactions Sub-level compactions are algorithmically more complex than regular compactions for the following reasons: A compaction can span multiple sub-levels (not just 2 levels like normal compactions). For backwards compatibility, the sub-level assignment of a file is not stored and is purely a function of the age of data in the file and the key span of that file. This also means that the sub-level assignment for files needs to be (efficiently) recalculated as other files are added and removed from L0. Consider the sub-levels and files in the following diagram, where the file numbering corresponds to the age of the data (higher number is younger data). Figure 12: An LSM example with 9 files in L0 of varying widths As expected, for files that overlap, as we go to higher sublevels the numbering increases. However, it is legal for file F3 to be in a lower sublevel than F2. Since F1 is part of the key span that has a depth of 4, it can be a preferred file to build a compaction candidate. To build the input files for this compaction, one walks up the sublevels looking for overlapping files. This is to reduce future write amplification since compacting only F1 down to L6 means that that data will get rewritten when later compacting F2, F6, F9. Say we include file F2 and then F6. Since F6 overlaps with F4, we need to include F4 and then transitively include F3. Not doing so will violate the LSM invariant by moving younger data for a key to L6 while leaving older data behind in L0. The set {F1, F2, F6, F4, F3} is a valid compaction candidate and this compaction will include the two files depicted in L6. If the total bytes in this candidate compaction are not above a large compaction threshold, we may also include F9 since it reduces write amplification in the future. After the compaction, that included files {F1, F2, F6, F4, F3} from L0, finishes, the LSM would look like the following (assuming no more data was added while the compaction was running). Figure 13: Same LSM example, after compacting files F1, F2, F3, F4, F6 to LBase Note that file F9 has moved from sublevel 3 to sublevel 0, because there are no older files overlapping its key space. And sublevel 3 no longer exists, since there are no files that need to be in that sublevel. The Result: 80% Time Reduction in TPC-C Bulk Import We have illustrated the sub-level approach using toy examples. Here we show a snapshot of the LSM visualization of a real LSM in the middle of a heavy import. Note that the LSM looks inverted in terms of L0 bytes being very large. This will slowly clear itself when there is spare write bandwidth after the import finishes. But this LSM is healthy in that the read and write amplification have not increased much. Instead of wasting write bandwidth on intra-L0 compactions that produce wide files and leave data in L0, the compactions seen in this import were all moving data from a higher level to a lower level. Figure 14: Real-world LSM visualization, before sublevels. Note the high count of sstables in L0 (847). Here’s the exact same LSM revision, except with L0 broken down into sublevels: Figure 15: Same example, but with sublevels enabled. The same 847 L0 SSTables can be organized into just 13 sublevels. There are 13 non-empty sublevels in the above example, so the contribution of L0 towards read amplification at this point is 13. Without the sublevels work, L0 would have contributed 847 to read amplification, as that’s the count of all files in L0. This shows how much of an improvement sublevels are, even in situations where the LSM is temporarily in a less-than-ideal shape. Our real-world-like import example consisted of about 5TB of data to import, using the stock table in our TPC-C workload generator, with warehouse count set to 50,000. Before making any of the improvements outlined in this blog post, a CockroachDB cluster running on 10 c5d.4xlarge nodes on AWS on ephemeral SSDs would take 6.5-10h for the import job to finish, after which it would be another 3-6h for compaction activity to quieten down. After making the above improvements, we were able to bring it down to a much more reliable 1h15m-1h30m for the import job, and an additional 30 mins for compactions and range merge activity. That’s a time reduction of more than 80%! Conclusion This project started with a change at one layer of the system that had unexpected and unintended effects on another. This led us on a journey of spelunking into the lowest layers of CockroachDB, RocksDB and eventually Pebble. It was frustrating at times and hit some dead-ends along the way, but ultimately proved fruitful. We came away with not only a faster IMPORT but also a storage layer better equipped to better handle more varied write distributions, which could occur in other workloads as well. We’re excited to make the IMPORT process a better experience for our users, and will continue to do so in future releases of CockroachDB. And if visualizing LSM trees is your cup of tea, we’ve got good news: Cockroach Labs is hiring .", "date": "2020-10-13"},
{"website": "CockroachLabs", "title": "Database Security Capabilities of CockroachDB", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/database-security/", "abstract": "In last week's episode of The Cockroach Hour, Jim Walker chatted with Cockroach Labs PMs Piyush Singh and Tommy Truongchau, along with security lead Aaron Blum, to talk about security features in CockroachDB. The full video is available here , and the transcript is below. Jim Walker: Hello, everybody. Good morning. Good, evening. Good, afternoon. No matter where you're at on this glorious planet today. It's beautiful and sunny here in Denver, Colorado. Just wanted to welcome everybody to this week's Cockroach Hour. Security is an important topic, I think, for all organizations. It's something that's near and dear to my heart. I started my career as a developer coding security constructs and RBAC controls and object libraries and whatnot. I love this topic. I always think of it as one of these big topics that usually causes a stir and typically something that people are very stringent about. You have to be good at it, otherwise it fails. Right? Because the weakest link in your security is only as strong as you are, right? We spend a fair amount of time here at Cockroach Labs thinking about security and weaving it into Cockroach database all over the place. Now, that said, this is the first time actually in one of these Cockroach Hours where we're doing just a outright navel-gazing and looking inside CockroachDB and talking about security and the context of a database, and more importantly, a distributed database like Cockroach. Thank you all for joining. That's what's on the hook for today. But real quick, before I get started, a little bit of housekeeping. There is a QA panel. Please do ask questions in there. Some of our sales engineers and some of our technical staff are in there. Sometimes we get some really, really great chat going on back and forth. Before you ask, yes, the recording will be available after the event. I know my friend, Dan, gets it up on our YouTube channel within, oh gosh, I don't want to give an SLA here, but it's pretty quick. But we will make everything available for you all. Today's session is intermediate. I say it's intermediate to advanced, and we aren't going to get into code, but security is never simple. I don't think it'll ever be a basic conversation. People ask us, \"What kind of session is this going to be?\" Just straight up, we are going to get into VPC peering, we're going to get into certificates, we're going to get into data encryption and how we do that in Cockroach database. I'm sorry, we're not going to talk about geo-partitioning, latency and compliance, or code today. I'm sorry, those are the wrong bullets on the slide. My bad. But we will be giving away coffee mugs to the best questions. So please do ask your questions, and at the end of the session, I think JP will go through and choose some of the best questions, and we'll interact with you and get coffee mugs out to you. I think they're great, actually. I have two of them. They're awesome. Jim Walker: Without further ado, I wanted to thank my distinguished panel for joining us. Gentlemen, if you want to come on video, that'd be great. I want to see your faces. There's Aaron. There's Tommy. There's Piyush. Awesome. Thank you, each of you, for joining us. I'm honored to have this group today. This does make up a large portion of the security brain here at Cockroach Labs. There's some really intense stuff going on, especially as you secure data. Databases are not a simple thing to take care of. I guess, if each of you could just introduce yourself, and then I think... I told everybody I love security because that's what I started coding. I love it. I think it's cool. It's a real geeky topic, but I think it's amazing actually because there's somebody always trying to break it. Right? But why is security important to you? Who you are, what your role is here at Cockroach Lab, and why is security important to you, I think would be a good ice breaker. Aaron, do you want to start us off? Aaron Blum: Sure. I'm the lead security engineer, so I get to touch lots of different parts of data process. That's build pipeline, that's database internals. I get to help work on the SaaS strategy to make sure that we're building something that's going to be consistently secure. I even get roped in to help customers with how to deploy this in a secure fashion and what choices they have as far as making that work. Jim Walker: Why is security important to you, Aaron? Aaron Blum: It's actually just a core passion. It's something that I've been doing for quite a while. One of the fun parts about Cockroach is you get to see that play out in ways that you wouldn't otherwise in a traditional application because of its scalability. Jim Walker: Piyush, I know you've been thinking about the security areas of CockroachDB for quite some time as well. Want to introduce yourself? Piyush Singh: Sure. I am the lead product manager for our operator experience group. We think about all of the operational aspects of your database. Security is obviously a huge part of that. In terms of why security is important to me, I just know how much our customers are trusting us to deliver a product that actually manages their data, keeps it not only alive, but also secure. This is all about delivering the thing that our customers need. That's why anyone gets into product management, right? It's all about like, \"Hey, how can we get, not only get you to trust, build something that you will trust with your data.\" That's fundamentally why this is super important to me. Jim Walker: Thank you, and thanks for doing this. I think it's one of those things. We've got to work the way that they work it. If we don't secure their data, problem. You aren't going to implement a database unless it's secure, right? Piyush Singh: Exactly. Jim Walker: Like Aaron said, it's not easy. I saved the best for last. My favorite product manager in the company. Sorry, Piyush, I'll just flat out just do it. I was just going to say it out here, dude. Tommy, do you want to just give a quick introduction as well? Tommy Truongchau: Yeah, sure. Hi, everyone. I'm Tommy, product manager. I joined Cockroach Labs around six months ago. I got the honor to work with Piyush and Aaron on the security side. Wanted to talk about why security is important from our perspective, right? Jim Walker: Yeah. Tommy Truongchau: Cool. Alluding to what Piyush was saying, as a PM, you hear a lot about impact and how do you get the most, how do you help your customers be successful and things like that. I find that, at least in this space, security is one way to unlock a lot of adoption and to get customers to trust and have confidence in using CockroachDB and CockroachCloud to secure and protect their data, manage their data, their workflows, and to run their workloads. That's exciting, because this space is just moving into this new world, it's cloud native, it's distributed. How do we actually make that secure is an interesting challenge and it's an interesting problem. So I find it really fun. Jim Walker: I think it's a really good point, Tommy. Working off a couple of different things, especially what Aaron said, and you said for that matter, doing security in a database is something that's complex and not to be shorted in terms of the intense focus that has to be put into that. It's funny, when everybody moved to the cloud, they were like, \"Oh, move to the cloud because they have the best practice in security,\" but then the cloud actually creates issues. You know what I mean? It creates whole new things. It's funny how people get in the cloud and then they realize, \"Oh, my God, I have a whole new range of things that I have to worry about.\" Let's just talk about just generally as a database. Piyush, as lead product manager of this area, how do you break it down when we start thinking about the concepts of security that are important in CockroachDB? Piyush Singh: Sure. There's quite a few different areas that we keep in mind as we're building out our product roadmap and making improvements. A few things that I can talk about, it starts with, let's say, encrypting your data at rest in Cockroach . Encryption at rest is obviously a major feature that we offer to secure your data once it's in Cockroach. We also want to secure all of that data while it's in flight. Connecting to your database securely, connecting new nodes to your cluster and properly authenticating them to join the cluster, encrypting the traffic that's flowing in between nodes. There's also role-based access controls . I know, Jim, that's something that you mentioned you've worked on. That's something that we're building out. We have to consider things like, what's our story with compatibility with Postgres? What types of controls are we offering? Are we making privileges that are fine-grained enough that our users aren't giving way too many permissions when they need to get someone to do something in their database? Let's see, what else? There's also encrypted backup and restore . How is the data that's in your cluster being securely stored elsewhere so that, if the worst happens, you're able to recover. There's definitely a ton of topics, and I know Aaron and Tommy probably have even more that they could mention. I'm sure I missed a few there, but those are just a few of the ones that I can speak to, at least that we've worked on recently. Jim Walker: I think, as you move to the cloud, Tommy, you and I have talked a lot about there's even added things that come with a SaaS implementation, right? Tommy Truongchau: Yeah. Right. If you're referring to CockroachCloud , our managed offering, that's something we're trying to push along, get it out to market, and help make that easy for customers out there. We talk about things like... Oh, sorry, Jim. You were going to say something. Jim Walker: No, go on. No, no, you were just going there. Tommy Truongchau: No, I was going to talk about, for example, one of the new things we are including in CockroachCloud is, how do you securely connect to your clusters? We have new offerings around VPC peering for GCP clusters. Down the line, we're going to do AWS PrivateLink to connect to your AWS apps as well. These are a few examples of things that we're pushing the needle on. Jim Walker: We'll talk about all those today. I think it's going to be really interesting, y'all, when we get to multi-tenant as well, seeing a multi-tenant database deployed in the cloud, serverless. What does the world of security mean in this circle, serverless? Aaron, I want to actually turn it to you because I think your background in terms of your deep knowledge of security, you've been in the security game for really quite some time. What we were just talking about, the concept of distributed systems adds a ripple to the challenge of what it means to be a secure database. What do you feel like the biggest challenge is that distributed systems adds to this security stuff? Aaron Blum: I think the main thing with a distributed system is you want it to be available and you want to make sure everything can communicate as it needs to. Often, security is making sure that unauthorized parties are not communicating or accessing things, so there's a fundamental push and pull there. I think our solution to that has been quite good in that we've worked to make sure the nodes have very strong trust between them. We have MTLS. That means that not only is the node sure that it's communicating with other nodes that are part of that trusted hierarchy, but itself can actually use that same trust anchor to validate itself to those nodes. It's a web of trust that's just as resilient as the underlying data structures. Jim Walker: Yeah. It's almost like you got to think about it in a distributed department, it's a web. It's not synchronous communication between two endpoints. What you're talking about is a many to many relationship. I've been in the Kubernetes space too for quite a while, Aaron. It seems like security gets in the way, and it is people, right? What do you feel like? How do you make it simple? What do you do? I only ask because, look, yeah, we're talking about Cockroach, but I think there's a lot of distributed systems people on the phone as well. Right? What's the best practice to think about that, right? Aaron Blum: Today we're looking at a couple of ways of doing it for Kubernetes. Internal to CockroachCloud, we orchestrate all the certificate management and all the trust primitives. We have our own system that handles that, but realize that might not be satisfying. We do have ways for you to do it. You can treat them all as independent nodes and then store things within Kubernetes Secrets. Actually, last night, I spent a fair bit of time working back and forth with Ben trying to refine where we're going on. We're looking at an approach that will allow nodes to behave as first tenants within the Kubernetes sets, so they'll come up, you'll pass them in an initialization and just say, \"Hey, you're a CockroachDB node set.\" They'll be able to pair with each other, share a secret, and then come online. Any resources like user CAs and things that you want to anchor to will be provisioned to them, and it'll figure out the rest as it goes. You have to orchestrate it to you turn it on and it pretty much just works and behaves in an internally consistent and secure fashion. Jim Walker: Is that done natively in Cockroach, or is that something that's been... like we use an operator to do that. What are you thinking? How does that get implemented? Again, we're doing it for a database. I hope there's people on the phone who are doing it for whatever application they're building it, and there's a best practice pattern. Right? Aaron Blum: We're trying to put as much of that into the application layer as possible to relieve the load from Kubernetes because anything that you've put into an operator starts to become a little bit more unusual or you have steps, and you wind up with, especially in a distributed system, the issue of, well, who comes up first and who initializes things if you have dozens or even thousands of nodes coming up? Who's in charge, and why do you trust that node, and what happens if that node fails? We really wanted to take an approach where the database itself has a resiliency to build the security structures in a similar fashion as the way it builds the other distributed components. Jim Walker: I think some of the same concepts we use within the database itself in terms of how it actually communicates. I mean, we're using things like gossip and that sort of stuff. I mean, it's basically we're just living on top of the already-great communication we're doing between those, correct? Aaron Blum: Yeah, absolutely. That's actually one of the things we're going to do for provisioning or we're looking at doing where after the node is established that they all belong to the same group, they'll be able to share all the rest of the configuration information underneath secure comms patterns using the underlying data structs. Jim Walker: All right, so I think the Kubernetes thing is going to be interesting. I think one of the other challenges within Kubernetes, and Tommy has touched on some of the stuff you were talking about as well and certificates and all this stuff. How do we deal with certificate management with Kubernetes and CockroachDB? How do you see customers doing this? I don't know. Aaron, you were out there, Piyush, I don't know. Who wants to jump in and answer that? Aaron Blum: I can take that one. Today, we support really robust security controls around certs. You can use different trust anchors or CAs for the nodes communicating with each other. You can use different ones for authenticating users coming into the system, and you can also put your own certificates or allow the system to use self-signed ones for communicating to such ports. Depending on your deployment strategy, you may need to use an external CA to mince certificates for all your internal services. That's fine. We support that. If you're just doing a dev environment and you need to mince your own certs, you can that too, so it provides that sort of granular access control. Jim Walker: Right. But I mean, typically, these things are involved. It's not simple to actually set up, get configured, deal with it. What have we done to basically simplify that too, or are we just going down that path? I know we work with that stuff. I mean, I don't think we would've been able to survive without working with it. What have we done to simplify it well? Aaron Blum: Today, we are pretty well-supported if you have the ability to orchestrate your own certificate story. Getting started is a little bit more friction, and we're actively working on improving that. Again, going back to the Kubernetes thing we talked about before where we're looking at being able to bring the nodes up and have them automatically communicate and gossip the other trust primitives, we're looking at doing the same thing for manual or scripted deployed solutions. The notes would actually be able to generate an internode communication web with their own CA. It would not be externally exposed. It's opaque, so the notes will be using strong TLS, which the user doesn't have to manage at all anymore. For everything else, if you want to set a certificate that you've signed or that you trust, you can do that. That's just a matter of putting the right certificates with the right names in the right config files or directories. Jim Walker: Right. How does that work, Aaron? I'm sorry to... I'm kind of intrigued by this actually. I'm trying to ask the question to get into it a little bit deeper. If you think about a TLS connection, there's some sort of certificate exchange between two nodes that actually have that conversation. How does that work? I mean, is it a public key that then we use this metric key to do that? I guess I'm asking how TLS works a little bit. Aaron Blum: No, not at all. As it works today, each node gets a certificate to its own host name that's issued by a common CA. The public key for that CA is also installed in the certificates directory for each node, so the nodes can validate each other's certificates and represent themselves with signed certificates to be able to establish mTLS and mutually authenticate across the web. Jim Walker: Right. So then basically we're using public private key infrastructure which has been around for a long time and proven to establish secure communication between the nodes. I guess that ticks the first part off, secure data in motion. It wasn't that one of your key things, Piyush, right? Is there anything else we have to do to secure data in motion? Aaron Blum: I've got the auth picture. We need to make sure that we're talking to the right people. Jim Walker: Yeah. Yeah. Piyush Singh: Exactly. That's actually like a pretty large area. It's deceptively large because there's all sorts of external integrations that people want supported. There's authentication and authorizations that every user will have when you're talking to your database. There's existing systems, like Active Directory and Kerberos and all of these other tools that people want to use because they centralize authentication throughout their entire company, especially large companies, large enterprises will want to have a single central place where they manage all the permissions for their users, and if someone joins or leaves the company, they just have to work in that one place instead of going through and provisioning accounts for them and every single internal service that they provide. That's something that we're actually working on. We do, for example, support Kerberos integrations for authentication. We're kind of laying the groundwork, doing a lot of the security work to enable authorizations as well, authorizations meaning what privileges users actually have inside of the database. I mean, that's on the database side. We also have this wonderful packaged avenue that ships with our database, and there, we're working on things like single sign-on that's actually coming in our 20.2 release. Piyush Singh: Again, it's that story of you don't want to have to create a username and password for every single user that's accessing your database. You just want to use some central OAuth provider to allow people to access their Admin UI and see what's happening inside of your cluster, so properly authenticating and authorizing users is definitely a huge area that security covers like Cockroach. Jim Walker: There's other types of connections though. Again, well, we'll just round out this whole section with VPC peering. Tommy, we added VPC peering over the summer I think with AWS, now with VPC, so just explain to me what it is and what's the state of that project now? Tommy Truongchau: Yeah. Yeah, for sure. What we tend to find is that a lot of our customers, they tend to run their apps in their own virtual private networks, and one of the challenges that we had with Cockroach cloud in the beginning was the fact that how do you connect to those other VPCs in your cloud provider? We didn't really have a way to do that. What we ended up seeing were a lot of customers allowlisting the public internet to allow traffic between Cockroach's clusters and their applications. That works. Seamless UX, you can say, but it's not actually secure from their perspective. It's not actually desirable from what they need. We heard this come up a lot and a lot from all of our CockroachCloud customers. Over the summer, we were able to bake in support for VPC peering over GCP. You do the same thing with AWS clusters, but doing it using their PrivateLink end point stack. That's currently in a process right now. Actually, just today, we enabled VPC peering for all Cockroach cloud customers. That's available for all folks using Cockroach cloud today. The private link, self-service UI, that's going to be coming down later this year, but we're excited to get that out soon for people. Jim Walker: Awesome. Congratulations. I know that was a bit of a labor of love to get that out there, but it's basically, it's just configurable via the Admin UI in Cockroach cloud now, correct? Tommy Truongchau: I'm sorry? Jim Walker: Is it configurable just in the Admin UI now in CockroachCloud, right? Tommy Truongchau: Yeah, VPC Peering is available in the CockroachCloud admin UI. Jim Walker: So- Piyush Singh: Really, the big point to hit there too is this is kind of a challenge that people have, especially with modern cloud deployments where IP white listing is, or IP allowlisting I should say is challenging because you are no longer running your application on something that has a fixed IP address. If you're orchestrating your application, you're killing pods, bringing them up based on the volume of requests that are coming into your application, you're going to have new IP addresses coming in and out of existence, and trying to connect to your Cockroach cluster is like how do you know which ones you should allow to connect to your cluster? It's kind of that new modern infrastructure that's kind of driving this need for peering, which I think is kind of interesting to see. Jim Walker: Yeah. Yeah. I think it's a unique challenge. Well, it's actually not even a challenge for us as a database. I think lots of applications are dealing with this and this complexity, and this is kind of one of those new things like, yeah, moving the cloud, and all of a sudden, I get there, and what else... What? I've got a bunch of other stuff that's taken care of now, so it's a pretty good example of that. I actually want to go back a little bit, Piyush, there was a question in the chat about audit as well, so there's the whole triple A of security, right? There's authentication, authorization. Is it auditability, accounting, whatever, right? When we talk about RBAC and role-based access control, we talked a little bit about authenticating user, but the authorization part of that thing, we put that into CockroachDB Core in the spring. What led you to that decision here because I like it when features get into our... We do run an open core business model, so what got you there? Piyush Singh: For sure. Yeah, and to set some context, we kind of have a rough rule of thumb that we follow for which features we think should fall into the open core part of our product versus the commercially licensed part of our product, which is do we think it's useful to startups or do we think it's something that's more useful to sophisticated enterprise customers? Historically, we kind of thought that role-based access controls were more of the letter, like you need very sophisticated access controls when you have an organization that's like hundreds of thousands of people, but I think what we ended up finding was actually there's a huge Postgres compatibility story here. Actually, the way Postgres treats roles and users, it actually treats them interchangeably. In order to support that concept of making users and roles interchangeable to mash Postgres syntax to support external tools that rely on Postgres syntax and different application frameworks, we actually ended up realizing, \"Okay, well, we have to essentially make this role-based access control stuff fall into the open core part of our product, because, otherwise, it's just going to break a lot of these integrations.\" In order to make the user experience better for basically everyone who's connecting into the database, we decided it just made more sense to put this into the open core part of the product. Jim Walker: Honestly, I think security is a baseline. You can't build an app without some sort of foundational security. I don't care if you're building I don't care if you're building a simple birthday app. You know what I mean? Piyush Singh: True. Jim Walker: Maybe it's just because I've been in the security space for so long, at least tangentially related. I'm no Aaron, but I at least care about this thing. I think that, making it a part of our core product was something that we talked about at the last release, and I don't think it was very well understood. It's like, \"It's not just for the enterprise, it's for every single company.\" I want to shift a little bit from the data in motion, which I think that was where we're at. Let's talk a little bit about data at rest, as well. There's two sides of this, right? So, encrypting data at rest, and then there's the backup, and restore. So let's talk about the encryption at rest. How does that work today,? I know we can do that, at what level can we do that? Who wants to pick that one off? Aaron? I see Aaron going for the mute button. Aaron Blum: I had the mute button already off. I can't speak too deeply about this. I know that we actually just shifted our storage engine. We went from RocksDB to Pebble , and I remember the PR for that, because we basically evicted a bunch of the old C++ code that enabled RocksDB to do encryption at rest, and replaced it with our own pebble engine. I can't go deep into the weeds, what I can say though, is all the nodes, if configured to use encryption at rest, will write encrypted data, and only encrypted data, to the data stores on disk. Jim Walker: And it's all configurable, right? I mean, at what layer can you actually configure encryption at rest? Is it the whole database? Is it a table? Is it a row? Is it a column? Piyush Singh: As far as I know, it's at the entire cluster level today. We've heard requests for things like row level, or column level security, and controls around that, and that's definitely something that we're looking at long-term, but it's definitely the whole cluster today. Jim Walker: What about when we do integrations with change data capture (CDC) ? Is it something that CDC capabilities that we have that are encrypting, or is it basically whatever we're feeding into CDC, is encrypted? Piyush Singh: That's a great question, actually. Aaron, do you know, I know we've done a little bit of work on that recently, I don't know if you're familiar? Aaron Blum: I don't know the current state, and I don't want to misrepresent it. Jim Walker: Fair enough. I think there was a question in the chat about recovery and master keys. Let's talk about backup and restore. Backup and restore on distributed systems is not simple, and then encrypted backup and restore also nuts, as it's compounding. Compounding complexities. Piyush Singh: I know one thing that we've been thinking about recently is how we can integrate with external secret managers. You probably don't want to be in the business of managing the complexity of dealing with all of these keys, and rotating, and doing all of that by hand, or scripting it. So, there's just external tools that are built to handle all of that stuff. Now it's on Cockroach, like, \"Okay, we see our customers are using these things, which is the security best practice. How can we support them to make sure that they're successful?\" Piyush Singh: So actually, we have started laying the groundwork for support of AWS KMS, right? So that all of the security backup stuff happens without you having to get way into the weeds of it, and just making it way more hands-off, and easier to use. That's the thing that's top of mind right now. Obviously we do support encrypted backups, and restoring from those backups, so I think that was something we added relatively recently. I don't remember the exact release off the top of my head. Jim Walker: We have an enterprise feature, but that's not incorporated, because we're moving some of our backup restore features into Core, this release, correct? Piyush Singh: That is correct. I don't think that is part of the piece that's moving to Core. But that is something that's super exciting for the 20.2 release too. We've definitely heard a ton of requests for like, \"Hey, can we get the distributed backup into the Core part of their product?\" So, we're really excited to be making that change. Jim Walker: And distributed backup, it's one of these things that I guess I didn't really get it. I thought we just did it naturally, but it's actually not simple to do. Because in our database, we can actually do something called geo partitioning, which is tied into locations. If you're going to do a backup, and you have some sort of policy about, that you're meeting some compliance, or regulation thing because of GDPR in Germany. Whatever that is, customer data, it needs to reside in certain places. If you just did a backup and restore the entire cluster, and that went to one central repository, you've just violated all that policy. Piyush Singh: Yep. The trick is restoring as well, right? Jim Walker: Exactly. Piyush Singh: Like, \"Hey, if something happens, I need to also restore this in such a way that the data never leaves the specific geographic region.\" That's super tricky. Jim Walker: It's just not simple. It's one of these concepts of the distributed systems where you just think, \"Oh, wait, oh, wait. That's actually...\" and then you get into it. Aaron Blum: Yeah. That was actually the biggest snag we hit when we first started exploring the AWS KMS, because we're like, \"Okay, well we'll just generate keys. Oh, wait, we're going to need different keys for the different regions, because we don't want one reason to be able to decrypt the backups. Okay, we're going to need to send these to different regions, and they're going need to have different keys, but we keep a master key for that?\" It was a really interesting problem, and I don't know where it currently is, but watch this space, because we're definitely working on making sure that we can preserve the integrity of that data, as well as the privacy. Jim Walker: Yeah. To me, it's just one of these things you guys had, like an even distributed system. Everything old is new again, I guess, eventually. We have solved these things in Postgres, and MySQL, and Oracle, and everybody doing it distributed, it just adds a layer of complexity. I think it's actually a pretty important point. One of the other questions that came up, and why we're still talking about encryption, \"When will you be able to mask data in Cockroach?\" I don't think we can do it today, correct me if I'm wrong, but is that something that's on the roadmap, Piyush? Piyush Singh: It's something we've heard requests for. We're definitely looking at it, but we don't have any immediate plans to support data massive yet. So, we're still in the early requirements gathering phase for that. Jim Walker: I think right now, Aaron, what we're seeing is organizations, they do it around the deployment architecture, what they like, and it's basically the entirety of the database, correct? Aaron Blum: Yeah, for now. Jim Walker: So, thank you, that's really good guys. So, there was another question, and actually it was something that I wanted to talk about as well. You usually talk about integrating with authorization frameworks, and these things. There's also learning and monitoring when it comes to security, and there's a whole suite of tools out there that allow you to understand what's going on. I mean, from Splunk, to, I don't know, somebody in the observability space? What does Cockroach implement on that side of the world? Let's just start with alerting, we'll come back to logging. Piyush Singh: Okay. Yeah, we're actually starting to build out some partnerships in that, in that space. So, we are looking at what external vendors we want to support. In terms of pure monitoring, obviously we're starting to build out metric based monitoring support, with companies like Elastics Kibana, Datadog. So, they will be able to scrape our API endpoint--just metrics--and then we can configure alerts on top of that. In terms of security alerting, I would expect most of that to be through logging. So, we are looking at companies like Splunk. How can we better integrate with them, how can we--this is starting to get into logs--how can we format our logs so that they're easy to adjust there? Jim Walker: It's all related, right? Piyush Singh: It is, it is. Yeah, it's that question of like, \"Okay, how can we feed these tools with the proper data?\" So, to that end, we're starting to generate these trails, these audit logs of all the different actions users are taking in the database. Things like, \"When are users provisioning new accounts? When are they granting those accounts permissions? When are people connecting and authenticating to the database? When are they connecting and failing to authenticate?\" All of those events, we actually track and store into audit logs, or we have an event log that tracks changes, things like that. Then ideally, you would feed that into some external tool, which would then be able to monitor out on top of these events. Jim Walker: Yeah. Then, doing it in real time, Piyush, is this something that's scraping the Prometheus endpoint? I mean, I think that's what people are also interested in. So, this is the real-time thing, too. How does that work right now? Piyush Singh: Yeah. So, this is veering into intrusion detection systems, and I know this is something Aaron is actually very passionate about, so I will actually kick this over to him. Jim Walker: Go on, Aaron. Aaron Blum: So, today we have a number of logs, and you can configure what goes into the audit logging. We found that, that's fine for customers that know exactly what they want to audit, but we're actually working on aligning things to a security specific log sync, so that you can have a security log that's admitted and monitored directly. Then you'll get that at the right time, so the log will continue to just spill to disk today. We're looking at ways to actually feed that to a network sync, or something else, but once that's done, you'll have a fairly high fidelity feed of all the security events from the cluster. Jim Walker: Do all databases do it like this, Aaron? Aaron Blum: Not in my experience. It's a grab bag, depending on who wrote what, where. Whether you're going to get a pure security log, or whether they're going to have to go pick through a lot of other noisy events and try to isolate the things that actually matter to you. So, we're trying to make it very, very digestible. Jim Walker: That's cool, I think I've gotten through all the questions that we had here. Then, I think we actually talked about the last one, too, Tommy. We've talked a fair amount about, \"What have we learned in CockroachCloud,\" but beyond VPC peering, is there anything else that we've learned in deploying Cockroach as a service ourselves, that might be a best practice for people to think about as they deploy their distributed applications? I don't know, Tommy or Aaron? Tommy Truongchau: I'm noodling through right now, but Aaron, if you want to jump in, if you're free to. Aaron Blum: All right, go for it. Tommy Truongchau: Yeah. I mean, I guess I would say if you have VPCs try to connect with that. We highly discourage you from connecting your traffic through open internet. But, on that topic, there is the conversation of, there's a free tier of Cockroach Cloud that's coming up, that's being worked on by the team. And one of our requirements there is, let's simplify that UX. If CockroachCloud customers aren't able to connect to their VPCs right now, but we want to remove that ability for them to... Or to require them to allow this IPs in order to connect, how can we improve the security posture of CockroachCloud, by default, to help them connect there entirely? So there are things that are happening behind the scenes to enable that. I don't know, Aaron, if you wanted to hone in on some of the details there, if we're allowed to talk about it. Aaron Blum: Well, I'll talk about it from a logging standpoint, because that's actually one of the places that really highlighted... As I looked at trying to secure CockroachCloud and make sure that we were protecting our customer's data, I realized that picking through all of the omitted log messages was very, very high volume. And anybody that's worked with an enterprise team knows that they tend to charge you either by volume or by lines. And I didn't want to adjust all the lines from the database. I want to adjust the security events. So, going back to that, I started getting requirements around, and development work around, getting just that security feed, which we can then build models of what's normal and what's not. And so, instead of forcing customers to use an allow list, or asking them to take specific VPC actions, we're going to put controls in place that would allow us to identify bad actors that are probing the infrastructure and reject them or limit them, and allow customers to continue to interact, without feeling the pain of this additional secure controls. Jim Walker: Don't get in their way. Just make it work, right? Aaron Blum: That's right. It should just work. Jim Walker: It should just work. And I think that's the trick with security. When it just works, nobody even notices it. It goes underappreciated, but it's actually really awesome. So kudos to you in the team, Aaron, because I think it's some really kind of elegant work beyond that. And you had referenced Ben earlier, Ben Darnell, one of our founders, probably one of the single best engineers I've ever met in my life. He's brilliant. And so these are not easy things to solve and to do it kind of elegantly. So what other questions? So do we have any plans to integrate with Linux security groups and users, Piyush? Tommy Truongchau: That's a good question. I actually haven't heard that request to date. So we don't have any plans yet, but actually, if you're willing to file a Github issue, I would happily follow up there. Jim Walker: That's right. And we are open. And then, I guess, we use asymmetric encryption. What does that mean? Who wants to take that? Aaron Blum: I'll talk a shot at answering this one. I don't know the exact context that's being quoted from, but all the PKI work that we do is asymmetric crypto. So if you want to authenticate to the cluster using certificates, you're not sharing a symmetric key that can be compromised in both places. It's, you have one end and that authenticates you, you have your private key and then through a standard key exchange, you'll establish secure communications using TLS. Jim Walker: Yep. And I think, just looking up how PK infrastructure works, helps people understand how asymmetric encryption works. I think it's truly awesome. So I'll go way back, you guys. Again, I got to give a little shout out to my history, man. We had implemented SHA1 MD5 and embedded 128 bit symmetric key into the bios of the machine, all within about 25K of code. So that you can actually take that symmetric key, which you were guaranteed what that was, and actually implement public key infrastructure from the bios up into the OS, which I thought was pretty damn cool, but that was some, again, remarkable engineering, but that was symmetric, but it allowed us to do asymmetric eventually. There's some really cool stuff and a lot of good reading out there, for the person who asked the question. This is a wonderful topic to go down. Because it's super interesting and really, really cool. Well y'all, I think we covered everything. We covered all the concepts that you should kind of outline at the beginning. I think we hit all the questions. There was one question that was like, \"Can we install the admin UI separate from our cloud cluster or whatever that is? I think the person who asked this question, Dietrich, the Admin UI with every node of Cockroach. Every node of Cockroach is one atomic unit. There is no different types of nodes. There's no admin node, a storage node, and a transaction node. A node is a node is a node. It comes with all of this security, it comes with all of our UI, and it comes with all on the CLI, it comes with all of the CD... Jim Walker: The binary is the binary, you can connect to any node, and you can go back to the admin UI, which is just awesome. And talk about key concepts in distributed systems. And Beam, living up to those primitives, that single atomic unit, being the full context of our software, single binary, is really what allows us to scale very easily, just at the drop of a hat, and I pointed out the cluster, as long as you have a TLS connection, you're good. Aaron Blum: That was actually one of the challenges on the Kubernetes side, because since all the nodes are the same, it's not like you bring up a master and then you bring up a bunch of auxiliary nodes. It's like, all the notes come up, they're all the same. There's nothing to differentiate them, which when you're trying to establish the trust primitives that build the entire Kubernetes set, how do you pick one? Or do you pick one? Jim Walker: We solved that, Aaron, right?So I think that's one of those things. And again, it comes back to these, as you're building out distributed systems, as you're going down these paths trying to figure that out in your own applications, this is one of things about being an open source company, and contributing back to the community. And in putting our code into the core product, where people can actually go and investigate this stuff and see that's practice, which to me, I get these conversations about open source all the time, like, \"License, license...\" Well, there's also a whole bunch of code and incredible software engineering that's out there. And I think this is one of those areas that, this is Cockroach giving back to the world as well. And some great minds in our team. And again, now there's always going to the doc. So with that, let's see, there's... Let's see here. There's one more question: Is it possible to disable the root user inside Cockroach and have different admins connect to the cluster? Aaron Blum: I can take that one. Jim Walker: Yeah, can you Aaron? That'd be great. So just paraphrase the question really quick. Yeah. Aaron Blum: Yeah. So CockroachDB has a special root user that is used for doing certain cluster administration events. It is a very special user, and by default, does not have a password set. You can only use a certificate to authenticate a set. It's basically a very special maintenance account. At some point in the future, I would like to see that disappear and become a named admin account that can be anything that you want, but today, it is a special account. We do not recommend using it for anything outside of cluster administration. And we have a new slew of permissions. I think landing in 20.2, they'll actually make it easier to administer clusters with named users that are not that special user. Tommy Truongchau: It's also worth mentioning that we do have a dedicated advocate role that does give you a select slew of admin privileges, so you can grant the admin role to a user that you want to run a lot of these administrative commands, and then not have to use the user. Jim Walker: And all pretty well-documented. Yeah. Very, very well documented. Jesse on the docs team, again, just does a great job. Aaron Blum: I think you can also potentially put a line in for the host based off to prevent remote access to the root roll. So you would have to be local to the device with the certificate to be able to authenticate. Jim Walker: One last thing, I guess, Aaron, you talked a little bit about this before. Are there different levels of auditing within Cockroach? I think we're kind of building that in now, right? Aaron Blum: Absolutely. So you have everything from the security auditing that we were talking about earlier, all the way down to every query and every transaction is audited, so depending on your level of granularity. Your mileage might vary if you turn on audit everything that the database does, because that's going to be a lot of IO, but it's available to you and it's tunable. Jim Walker: Okay. And then actually, this is Alex Robinson's shout out: If you're running two clusters in OpenShift and you wanted to connect them, how do you do that? This just shipped out OpenShift for Kubernetes, right? Yeah, multiple confederated clusters, right, Aaron? This is not a simple thing to do. And how do you connect nodes within Cockroach, which is actually a different layer, right? We're not talking about a federated cluster, we're essentially federating data now at the database layer. How does that work? I know we have a pretty good video about this I think Alex Robinson had done for us a while ago. Do you know how that works, Aaron? Aaron Blum: No, I can't speak to that. Tommy Truongchau: I will say, one super nice thing is actually, we just had a Kubernetes operator that was OpenShift certified, that will be available on... Actually, I think is available now on the Red Hat marketplace. And so one super simple answer is, user operator. Jim Walker: Well, yeah, user operator to deploy in OpenShift. And I think that the complexities of doing multi-region applications on top of Kubernetes is not simple. We're doing it in our SRE team today. So I think the easiest way to get that done is to actually go use CockroachCloud. All right, you guys, so listen, thank you very much, all three of you, for taking the time today. That was really, really helpful. Actually, I learned a fair amount today. I miss our lunches together, you guys. I miss sitting around and having these conversations, because you learn more about the company that way than in... But great work, and thank you all for joining. And thank everybody for joining us today. And there was a lot of really good questions. We really appreciate it. We hope it was helpful for everybody. Again, the recording will be up and available. There is a survey after this event as well that JP will put in front of everybody. Please do complete that. It really helps us get better, and any and all of that feedback is just really wonderful. Aaron Blum: Bye Jim. Jim Walker: Tommy, see you Tommy. Piyush, we'll see you later, buddy. Tommy Truongchau: See you Jim. Jim Walker: Thanks everybody, and have a great day. Watch the webinar -->", "date": "2020-11-04"},
{"website": "CockroachLabs", "title": "JPMorgan Chase Honors Cockroach Labs for Innovation & Partnership", "author": ["Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/jpmc/", "abstract": "Like many industries, the financial services business is undergoing dramatic change. In response to the whole world going digital and the explosion of data available to drive decision making and customer experiences, banks are pursuing aggressive innovation in new applications and cloud infrastructure. With this in mind, we are very proud to share that we have been inducted into the JPMorgan Chase Hall of Innovation for our work helping the financial services leader build next-generation applications and infrastructure. Cockroach Labs joins past winners including Tableau and Confluent, companies we are honored to stand alongside. In the last decade, only 25 companies have been inducted into the JP Morgan Chase Hall of Innovation, which recognizes companies for going above and beyond in their partnership to deliver innovative and disruptive technology. Our work with JPMorgan Chase includes helping them to modernize their global data centers and building next-generation applications that ensure they continue delivering world-class service to their tens of millions of customers. Cockroach Labs is actively working to earn our place as the system of record for modern financial services applications. This award from JPMorgan Chase is a sign that we are making great progress towards that goal. “At JPMorgan Chase, we believe that working with entrepreneurial companies and their innovative technologies is critical to our success. CockroachDB provides us with a modern database platform that has accelerated our momentum toward cloud-first, resilient infrastructure and a new class of modern financial applications,” said George Sherman, Global Technology Infrastructure CIO at JPMorgan Chase. This award and our ongoing partnership with JPMorgan Chase reinforces the growing demand for a new generation of operational data infrastructure, as organizations across the globe accelerate the development of cloud-native, distributed applications and services. In 2020, companies including SpaceX, eBay, and LaunchDarkly adopted CockroachDB to power a new class of applications that can scale fast, survive anything, and thrive everywhere.", "date": "2020-11-05"},
{"website": "CockroachLabs", "title": "DASH: Four Properties of Kubernetes-Native Databases", "author": ["Nate Stewart"], "link": "https://www.cockroachlabs.com/blog/dash-kubernetes-native-database/", "abstract": "``` Cloud-native application architectures help developers deliver amazing experiences to their customers around the world. They do this by taking advantage of billions in cloud provider investments, which provide nearly unlimited and on-demand resources spread across hundreds of data centers globally. ``` Kubernetes – the Google-built open source container orchestration system – is quickly becoming a ubiquitous tool for deploying, running, and scaling these applications. Kubernetes simplifies and supercharges application delivery if (and this is a big “if”) those applications are architected to take advantage of the resources available in cloud environments. Certain certain issues remain that can make managing stateful applications difficult. Simply put: storage on Kubernetes is not a solved problem. This is highly dependent, however, on what kind of database you use alongside Kubernetes, and whether you select a database that is Kubernetes-native. Traditional relational databases that give life — or state — to any application were not architected to take full advantage of the resources available in the cloud. On the other side, NoSQL databases fail to thrive in Kubernetes-orchestrated environments. A new breed of relational databases -- Distributed SQL databases -- have emerged over the past few years and are able to take full advantage of the cloud and the operational benefits provided by Kubernetes. Unless your database is compatible with Kubernetes, you will not be able to take full advantage of the orchestration magic it renders possible. In this post, I will introduce the DASH Framework to help you answer the question: what makes a database Kubernetes-native? We will walk through basic architectural principles of Kubernetes, examine how they interact with the database, and cover the four principles you need to expect from a Kubernetes-native database: Disposability, API Symmetry, Shared-Nothing, and Horizontal Scaling. Along the way, we will evaluate NoSQL, traditional relational databases, and Distributed SQL solutions against this new rubric, and provide a framework to evaluate which are the best fit for cloud-native architectures. What is the DASH Framework & How to Use it to Evaluate a Kubernetes-Native Database DASH is a framework to think about Kubernetes-native operations, and help evaluate how Kubernetes-native any database is. DASH stands for the four Kubernetes-native operations a distributed database must provide in order to truly work with Kubernetes: Disposability, API Symmetry, Shared Nothing, and Horizontal Scaling. The rest of this guide will uncover what these principles mean, and whether or not they work with traditional relational databases, NoSQL databases, and Distributed SQL databases. DASH Principles Disposability: Losing things should be a non-event. Because a Kubernetes-native database can handle failures, any disruptions are entirely hidden from the client. API Symmetry: Every server in the network provides the same answer to the same question. Though they are distributed, Kubernetes-native databases act as a single logical database with the consistency guarantees of a single-machine system. Shared-Nothing: True Kubernetes-native database can operate without any centralized coordinator or single point of failure. Horizontal Scaling: Scale out, not up. To make these concepts concrete, I’ll use examples of my favorite DASH database, CockroachDB (Google Evangelist Kelsey Hightower also thinks it is a great example ), but you should keep in mind that these concepts extend, at varying degrees, to other Distributed SQL databases. D : Disposability: Failure Must Be a Non-Event Everything fails eventually. No matter how much abstraction you layer on top of it, we’re working with machines at the end of the day, and machines crash. Disposability is the ability of a database to handle failures, or processes stopping, starting, or crashing with little-to-no notice. Disposability enables a database to be resilient, able to to survive the failure of any piece of hardware yet still provide access to the database with limited or no impact on query performance. It is bulletproof, always on and always available, and will avoid any single point of failure. This is particularly important in a Kubernetes environment, because Kubernetes pods are mortal by design. The Kubernetes Scheduler plays an important role in managing disruption incidents. What is the Kubernetes Scheduler The Kubernetes scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, the scheduler finds the best Node for that Pod to run on, based on configurable scheduling principles . The Kubernetes Scheduler uses a set of rules for determining which pods (small groupings of containers that are always scheduled as a unit) run on which machines. Once pods are scheduled, they remain on those machines until some sort of disruption occurs due to voluntary (i.e. scaling in or upgrading) or involuntary (e.g. hardware failure or operating system kernel panic) factors. When disruptions occur, Kubernetes reschedules pods to more suitable nodes. In NoSQL databases like MongoDB and Distributed SQL databases like CockroachDB, this is a non-event. But in traditional relational databases, a Kubernetes pod rescheduling itself can result in inconsistent data, because of the way the database handles failover. Disruptions like these are a significant problem for legacy relational databases, because they typically have a single machine powering them at any given time. For production deployments these databases may send updates asynchronously to a second instance that will take the lead in the event the primary machine goes down. However, in practice the process of actually failing over is difficult to do well. Github recently described the perils of executing a MySQL failover during a major outage , which resulted in out of date and inconsistent data. If the systems powering legacy relational databases fail, users will likely know about it. Both Distributed SQL and NoSQL databases typically have the Disposability property, as they were designed to thrive in ephemeral cloud environments where virtual machines could be restarted or rescheduled at a moment’s notice. Databases with this factor should be able to survive node failures with no data loss and no downtime. For example, CockroachDB is able to survive machine loss by maintaining three consistent replicas of any piece of data among the nodes in a cluster; writes are confirmed to clients as soon as the majority of replicas acknowledge the action. In the event that a machine containing a given replica is lost, CockroachDB can still serve consistent reads and writes, while simultaneously creating a third replica of that data elsewhere in the cluster to ensure it can survive future machine failures. Keep in mind that disposability isn’t just about surviving individual machine failures. DASH databases should be able to extend the concept of disposability to entire data centers or even data center regions. This type of real-world failure should be a non-event; these capabilities would help avoid issues like the ones encountered by Wells Fargo, where “smoke in the data center” resulted in a global outage . A : API Symmetry: Every Server Should Provide the Same Answer to the Same Question In a distributed system, each node serves as a point of entry to the data stored within. But depending on your database’s consistency model , those nodes do not always give the same answer. The API Symmetry Principle demands that when given the same request, every server in the system returns the same answer. A Kubernetes-native database must implement and enforce serializable isolation so that all transactions are guaranteed consistent. This is especially important in Kubernetes because of the way Kubernetes Services are configured. Kubernetes uses Services to allow clients to address a group of identical processes as a whole through a convenient DNS entry. This way applications don’t need to know about the many instances that power a frontend; there could be one backing server, or there could be hundreds. Services are essentially defined as rules that say any pods with a given collection of labels should receive requests sent to this service (assuming their health checks and readiness probes say it is OK to send traffic). KUBERNETES SERVICES In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). By using Services, you don't need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them. Why does Kubernetes take this approach? By decoupling the pod from the address of the service with which it is associated, we can scale without disrupting existing application instances. This is possible because of API symmetry, meaning the pods included in the group all have the same API and provide consistent responses, regardless of which instance is chosen by the Kubernetes Service. For stateless services, this is simple: The logic in each service is the same, so sending a request to any service at a given time will always yield the same result. Note that for a database to have API symmetry, the underlying data must also be strongly consistent . If you get different answers depending on which node you are routed to, the abstraction provided by Kubernetes is broken, and that leads to bug-causing complexity for application developers. This is actually an area where traditional relational databases perform better than NoSQL systems. When all queries are being sent to one machine, you get the same result every time. In NoSQL systems, you need to make a tradeoff between Disposability and API Symmetry. [This scenario closely mirrors tradeoffs inherent in the CAP theorem : that a database must choose between Consistency (API Symmetry) and Disposability (Availability).] When asynchronous replication comes into play (either for high availability or to support performance improvements like read replicas), the API symmetry is violated, since the master becomes the source of truth where the replicas would be slightly out of sync. With NoSQL systems, different machines might give different responses to the same query. In the Distributed SQL world, techniques like consensus replication allow databases to provide API symmetry without sacrificing Disposability. By using API symmetry alongside Kubernetes Service Objects, you can create a single logical database with the consistency guarantees of a single machine database (even if there are actually dozens or hundreds of nodes at work behind the scenes powering the database). Let’s go back to our CockroachDB example: any CockroachDB node can handle any request. If the node that receives a load-balanced request happens to have the data it needs locally, it will respond immediately. If not, rather than sending an error back to the client (or providing a stale result), that node will become a gateway and will forward the request to the appropriate nodes behind the scenes to get the correct answer. From the requester’s point of view, every node is exactly the same. S : Shared-Nothing Architecture: Eliminate Single Points of Failure The cloud should not have a maintenance window. True cloud-native services should have the capability to be always on. This means removing any single points of failure. The Shared-Nothing property dictates that a database should be able to operate without any centralized coordinator or single point of failure. In the stateless world, this goes hand in hand with disposability. When state is involved, this concept becomes an additional consideration. Traditional relational databases are notorious for having single points of failure. This extends even to modern RDBMS systems, like Amazon Aurora . Even some Distributed SQL databases rely on special coordinators to keep track of all the bookkeeping required to build a globally-distributed system. What this means is that you can have architectures that can survive certain workers being disposed, but if you take down the coordinator process, if the entire system doesn’t go offline in many cases the configuration state will be frozen and certain types of critical operations – potentially those required to debug the issue – will fail. Cloud-native databases should be able to survive in a world where any node can fail, not just “any node except for our special master node that coordinates everything.” For example, CockroachDB has no master process— this is what gives it its eponymous survivability characteristics. Although each CockroachDB database server is stateful, it only relies on the state for which it is responsible (though it does cache some knowledge that it has gleaned about the cluster through communicating with other nodes via a peer-to-peer network). CockroachDB nodes do not rely on any authoritative source to say what they should be doing at any given point in time. Shared-nothing architectures for stateful systems allow both ultra-high availability and ease of operations. \\[Author note: Interestingly enough, Kubernetes itself is not a shared-nothing system. It has a single-region control plane that, if destroyed, will compromise the cluster. Operators can create Distributed SQL database clusters that survive more than Kubernetes’ control plane nodes by spanning Kubernetes clusters across regions or even cloud providers.] H : Horizontal Scaling: Scale-Out Rather Than Up The last factor required of a Kubernetes-Native database is horizontal scalability . Similar to the way this term was used in the Twelve-Factor App, this means if you want more throughput, you simply add more processes. Kubernetes Controllers and Schedulers combine to make horizontal scaling an easy, declarative process : The cloud promises infinite scale and a Kubernetes-native database needs to simplify utilizations of these resources without causing any additional operational overhead. It should automate and deliver effortless scale. While horizontal scaling is a fundamental benefit of many NoSQL systems, traditional relational databases do not do this well. They rely instead on sharding—with or without the help of systems like Vitess —to accomplish this use case. What this means for traditional RDMS systems in cloud-native environments is that if you need more power, you have to buy a more expensive machine and incur downtime, or you have to dramatically increase your operational overhead by splitting your database into many pieces that cannot easily talk to each other. For teams that rely on vertical scaling, this means there is a natural limit to how powerful a relational database can get—at the end of the day, it is limited to what can be powered by a single server. Distributed SQL databases take a page from NoSQL systems and scale by adding more machines. For example, a single Kubernetes command can scale out CockroachDB by provisioning new resources and spinning up additional pods with no downtime; the Kubernetes load balancer will recognize the new database capacity and automatically start routing requests among the new instances. Each node can independently process requests while also taking part in helping other nodes when it comes to completing tasks like processing complex queries by breaking them up into smaller bits of work that can be completed in parallel. Kubernetes has features to help scale out stateful services by doing things like providing pods with predictable network identities to facilitate service discovery among the cluster instances. A bridge to the cloud for relational databases DASH provides a necessary framework for evaluating whether a database delivers a truly Kubernetes-native architecture. Disposability enables resilience, ensuring your stateful systems can survive when ephemeral cloud resources cease to exist. API symmetry enables consistency, allowing distributed databases to always provide the up-to-date answer, no matter which process is handling the client request. Shared nothing properties enable your database to make forward progress without any centralized master or coordinator. H orizontal scalability allows the database to take advantage of the unlimited and on-demand resources available in the cloud. Kubernetes-native databases give IT teams an automated database that operates as an always-on, elastic data layer that adds the missing cloud-native foundation to their stacks. Distributed SQL: DASH-Complete, Kubernetes-Native Database Architecture NoSQL, Distributed SQL, and traditional relational databases make different architectural tradeoffs. But when it comes to Kubernetes compatibility, there’s a clear winner. Distributed SQL databases like CockroachDB were built from the ground up to work out-of-the box with Kubernetes and other microservices. Running a traditional relational database on Kubernetes is a challenge and typically most organizations will just run it alongside the platform to simplify operations. However, this often creates a bottleneck or worse, a single point of failure for the application -- a violation of the DASH framework. Running a NoSQL database is better aligned, but can still cause data consistency issues. Both traditional relational and NoSQL databases require complex operators to help manage these databases in the environment as they simply were not built with the same architectural primitives. A Distributed SQL database like CockroachDB allows you to deploy a relational database seamlessly on top of Kubernetes so you can gain the advantage of all its benefits across your entire application. Originally published at The New Stack.", "date": "2020-10-21"},
{"website": "CockroachLabs", "title": "Distributed BACKUP and RESTORE Added to Free CockroachDB Core", "author": ["Michael Wang"], "link": "https://www.cockroachlabs.com/blog/distributed-backup-restore/", "abstract": "We are constantly looking for ways to help small teams make a big impact with CockroachDB. We heard from our community that DUMP, the free, single-machine Disaster Recovery feature we provided in our open source option, CockroachDB Core, didn’t quite go far enough for supporting the types of data-intensive apps that startups were building in 2020. So in CockroachDB v20.2, basic distributed BACKUPs , along with the entire suite of RESTORE functionality, are now a part of our free, open source option CockroachDB Core, to provide reliable, valuable, usable Disaster Recovery for all CockroachDB customers and applications. The Evolution of BACKUP & RESTORE in CockroachDB When we detailed the decision to move to an Open Core model with a commercial license in this 2017 blog post , our founders Peter, Ben and Spencer said: “Features necessary for a startup to succeed will be &mldr; part of the open core [CockroachDB Core]; a feature which is primarily useful only to an already successful company will be CCL, and part of the [CockroachDB] enterprise product” [Editor’s note: We’ve since moved from the APL to BSL ] They also detailed the decision for making Backup and Restore a part of the Enterprise offering: “The first [of the Enterprise features] is a fully-distributed, incremental capability for quickly and consistently backing up and restoring large databases using configurable storage sinks (e.g. S3 or GCS). The same functionality, but non-distributed, will be available for free to all users.” That was written three years ago, and more recently, we began to hear from CockroachDB Core users that DUMP, the free, single-machine Disaster Recovery feature we provided in CockroachDB Core, didn’t quite go far enough for supporting the types of data-intensive apps that startups were building in 2020. “But Michael, CockroachDB is designed to deliver bulletproof resilience. Why do people still need backup and recovery features?” BACKUP & RESTORE: Because Even the Safest Boats Carry Life Jackets CockroachDB is, in fact, designed to deliver bulletproof resilience . Built with fault tolerance in mind, isolated issues like small-scale node failures don’t require manual intervention or complex scripting. But even the world’s safest boat needs to carry life jackets. The same principle applies to your data. This is why CockroachDB includes built-in distributed backup and recovery functionality , to help customers: recover from mistakes (such as a forgotten `where` clause changing everyone’s name to “test”) stay in regulatory compliance through backup archival add an extra layer of protection for their customers’ data. Prior to v20.2, CockroachDB offered two different methods for backing up data: BACKUP for CockroachDB Enterprise and Cockroach Cloud users and DUMP for CockroachDB Core users. BACKUP is distributed, can write to a number of different storage options, and captures native binary data with very high reliability and reproducibility. BACKUP is paired with RESTORE , which is also distributed and restores data from files created by BACKUP. DUMP, like the “dump” command in other databases, is a standalone client program which reads all the data from all the tables and prints it out as SQL statements that can be replayed in another CockroachDB cluster. This text-based detour leaves room for the restoring cluster to interpret that text differently, plus converting to/from text makes both sides slower and more expensive. But the applications of 2020 are more data-intensive than the apps of yesteryear and community members reached out through our public GitHub repo , our Community Slack and Forum feedback channels that DUMP and its trade offs just weren’t cutting it anymore. They were right. We confirmed these frustrations through some very light testing and found that running DUMP from a laptop on a 10GiB table from a 3-node cluster on c5d.4xlarge AWS machines took around three hours. On that same cluster under load, using BACKUP to back the whole cluster up to s3 (100GiB) took around three minutes. (Note: These are not apples to apples comparisons nor are they meant to be benchmarks–for example the dump had to transfer data over the internet whereas backup was sent to S3 is the same region. They’re provided as examples of the types of results we encountered in making this product decision. We advise customers to benchmark themselves as Backup speeds will vary based on setup.) -- Our customers told us the product wasn’t fitting their needs. We tested the performance ourselves, which only strengthened their case. As a result, we decided to make a change. In our v20.2 release, basic distributed BACKUPs , along with the entire suite of RESTORE functionality, are now a part of CockroachDB Core to provide a reliable, valuable, usable Disaster Recovery option for applications and companies running on Core. In doing so, users of CockroachDB Core now have access to: Distributed, scalable, performant binary backups Corruption checks to ensure restored data is the same as what was backed up Backup scheduling with the same resilience as the underlying cluster via our built-in backup scheduling functionality (new in v20.2 – no more cron jobs for backups!) Other BACKUP functionality will remain a part of our CockroachDB Enterprise and CockroachCloud. Incremental BACKUPs Encrypted BACKUPs and KMS integration Revision History BACKUPs (to enable Point-in-time-Restore) Locality Aware BACKUPs In addition, we deprecated DUMP in v20.2 and DUMP will be removed in a future version. For data portability, where dump’s text-based encoding was actually desirable, we are working to ensure that schemas can be easily exported and made our distributed export to CSV feature EXPORT available in CockroachDB Core as well. As we mentioned before, Cockroach Labs is constantly looking for ways to help small teams make a big impact with CockroachDB. This is the first of many such changes to come. As technologies emerge, best-practices evolve and data volumes grow, we’ll continue to evolve as well. (Thanks to our customers and community members for providing feedback that’s making our product better. Your input helps us keep tabs on the market shifts and evolving needs. If you’re not already part of our community Slack, join the conversation here !)", "date": "2020-11-12"},
{"website": "CockroachLabs", "title": "A Brief History of Databases", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/history-of-databases-distributed-sql/", "abstract": "The concept of a database existed before there were computers. Some of you are even old enough to remember the filing cabinets in which your parents kept health records, tax documents, and old family recipes. The first computer database was built in the 1960s, but the history of databases as we know them, really begins in 1970. The birth of the relational database In June of 1970, a computer scientist from IBM named Edgar F. Codd published an academic paper titled, A Relational Model of Data for Large Shared Banks . That paper introduced a new way to model data. It elaborated a way of building a bunch of cross-linked tables that would allow you to store any piece of data just once. A database with this structure could answer any question, so long as the answer was stored somewhere in it. Disk space would be used efficiently, at a time when storage was expensive. This paper launched databases into the future. Oracle brought the first commercial relational database to market in 1979 followed by DB2 , SAP Sysbase ASE, and Informix . In the 1980s and ’90s, relational databases grew increasingly dominant, delivering rich indexes to make any query efficient. Table joins, the term for read operations that pull together separate records into one, and Transactions, which means a combination of reads and especially writes spread across the database, were essential. SQL, the Structured Query Language, became the language of data, and software developers learned to use it to ask for what they wanted, and let the database decide how to deliver it. Strict guarantees were engineered into the database to prevent surprises. The arrival of the NoSQL database Relational databases were architected around the assumption that they would be run on a single machine. Also, they were architected before the internet gained massive popularity. The volume of data that can be created by millions, or billions, of networked humans and devices, is far more than any single server can handle. When the workload grows so heavy that no single computer can bear the load, and the most expensive hardware on the market would be brought to its knees by the weight of an application, the only path is to move from a single database server to a cluster of database nodes working in concert. For a traditional SQL database, architected to run on a single server, this is a painful process. It requires massive investments of time and often involves tradeoffs that sacrifice many of the features that brought developers to these databases in the first place. By the late 2000’s, SQL databases were still extremely popular, but for those who needed scale, NoSQL had become a viable option. Google BigTable , HDFS , and Cassandra are a few examples. These NoSQL data stores are built to scale easily and to tolerate node failures with minimal disruption. But they come with compromises in functionality, typically a lack of joins and transactions, or limited indexes. These are shortcomings that developers have to engineer their way around. NoSQL can scale beautifully, but relational guarantees are elusive. Distributed SQL is the next evolution of the database Traditional SQL databases have tried to solve their scale problem (and hold onto their market share) by bolting on features to help reduce the pain of sharding. At the same time, NoSQL systems have been building out a subset of their missing SQL functionality. But neither class of database was architected from the ground up to deliver the transactional guarantees of  relational databases and the scale of NoSQL databases. In 2012 Google Research published what has come to be known as the Spanner paper in which they introduce Google Cloud Spanner , a database that was architected to distribute data at global scale and support consistent transactions. This new breed of database is known as Distributed SQL . There are five conditions that must be met in order for a database to fall into the distributed SQL category: scale, consistency, resiliency, SQL, and geo-replication. The presence of each of these capabilities means that a mission-critical workload that is run in multiple regions of the world, can be accessed as a single data store, and can be scaled by simply adding nodes to a cluster. As more IT departments adopt a cloud-centric philosophy the popularity of a database that can deliver the scale and distributed transactions will continue to rise...until the database evolves again.", "date": "2020-10-27"},
{"website": "CockroachLabs", "title": "How to Use Containers, OpenShift, and Kubernetes with Red Hat", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/openshift-kubernetes-red-hat/", "abstract": "In last week's episode of The Cockroach Hour, Jim Walker chatted with Red Hat principal product manager Scott McCarty to talk about everything from the what the future of serverless and distroless are, to what happens when you run Oracle on Kubernetes. The full video is available here , and the transcript is below. Jim Walker : Welcome, everybody. Welcome to our event today, The Cockroach Hour. Today, we have a special guest from Red Hat joining us, so we're going to talk about OpenShift. We're going to talk about containers, we're going to talk about Kubernetes, all this goodness. And I'm going to presume we're going to end up talking a little bit about open source too, after meeting our presenter here. But let me just give a little quick guidance before we get started here. We do have a QA panel, there is a chat panel as well. Please engage however you like, we enjoy questions on The Cockroach Hour. So with that, thank you all for joining and today's session is... We've been asked before, can you tell if these things are basic, intermediate or advanced? I think of this session as more of a basic conversation. We're not going to get into any command line. We're not going to get too deep into distributed transactions like we have in the past. This is a little bit more higher level stuff, workloads, Kubernetes, definitions of these things, where people are getting tripped up on these things and talking through those sort of things, right? But please do ask questions, we will send mugs out to people. I have a mug around here somewhere, I model it. So with that, let's bring the cameras on. So Tim and Scott, if you guys want to join me. There you are Tim and then Scott. Scott McCarty: Hi, Jim. Jim Walker: So everybody in the crowd, it's guys in flannel shirts today. Scott McCarty: Flannel shirt Wednesday. Jim Walker: Our outfits are boring but I promise that the conversation will be lively and we'll keep it to some really interesting topics here. But first and foremost, thank you, Scott, for joining us. Scott McCarty is a principal product manager in charge of containers at Red Hat. So, Scott, what do you work on at Red Hat? A title is what? Four words. Scott McCarty: Yeah. Jim Walker: What are you working on at Red Hat? Scott McCarty: But it's all about the syllables. If you're a principal product manager, that's like what? Seven, eight syllables. Jim Walker: And then you throw containers in there, there's another couple there. Scott McCarty: Yeah. No, three syllables, you're good to go. Yeah, I used to have a title that was 16 syllables. I thought that was way cooler but I used to make fun of myself much more. So in this role, I live in what's called... So there's two main platforms at Red Hat, if you will. There's OpenShift and there's REL and I live in this nexus in between the two, where our team builds technology like CRI-O, Podman, Buildah, Skopeo. We work on runC, we work on all these low level bits. I also manage the roadmap for Red Hat Universal Base Image. So all these primitives are what I call them. They're like the basic flour, sugar, eggs and water of containers that we've basically built for Red Hat. Jim Walker: I should have worn my Rocket T-shirt today, knowing what you work on. So anyway, we'll come back to that though, if anybody- Scott McCarty: Yeah. We consider Podman and the spiritual successor of Rockets. Jim Walker: It is, it is, it is. I know but the little Rocket logo is so great and everything. Scott, how long have you been at Red Hat? Scott McCarty: I have been at Red Hat nine years. Jim Walker: Yeah, that's awesome. Scott McCarty: Yeah, it's been a while. Nine and a half. Jim Walker: It's amazing when I meet Red Hat employees. The tenure is long, I've met people, a dozen... Well, over a dozen years. What is it? Red Hat's been around for how long, 20 years now? Scott McCarty: On our company call today, Mike Evans passed 25 years. So we've been around since what? Over 25 years. Jim Walker: The start date. Scott McCarty: I don't remember the start date, '93. I want to say '93 was the start date. Jim Walker: So I can finally say, there's some people who have been there a couple dozen years. But to me, one of the most important companies of my generation and all of our generations in terms of, I think all three of us here are open source advocates and zealots and all those. I don't have a neck beard but I definitely love open source. So thank you, Scott, for joining us. And then Tim Vale, if you want to introduce yourself. Tim Vail: Well hey, everybody. Tim Vail here, head of solutions engineering at Cockroach Labs, been here about two and a half years. And as Jim mentioned, spent a lot of time in open source prior to that. So glad to be here, glad to be talking about all this fun stuff. Jim Walker: And Tim brings in an angle of engagement with customers every day, both large and small. Smaller companies using CockroachCloud, larger companies trying to deploy CockroachDB on Kubernetes or in these distributed environments, in a single cloud, multicloud, lots of everything. We're going to talk about, hopefully, all those things today. My name is Jim Walker. I'm in product marketing here at Cockroach Labs. I am just a week short of two years at this company. Man, Scott, you're just way beyond me in any sort of tenure but that's okay. I was at CoreOS before this, so I almost could have made it to Red Hat but that's okay. Jim Walker: But welcome, everybody. Like we usually start, good morning, good afternoon and good evening. Please do use chat and we'll come back to this. Not thank you, we're not done yet. So I'm going to stop sharing so that it's us on here. So Scott, your title is principal product manager of containers. So when you have to describe what a container is to somebody and let's just say it's the most highest level, what is it? Scott McCarty: Yeah, so it depends on who I'm talking to. For this audience, I'd say it's probably more technical audience, so I'd describe it as... Even the simple technical definition I'd say, is it's two things. There's a runtime component and then there's an at rest component. So I jokingly say that containers are fancy files and they're fancy processes. Jim Walker: That's right. Scott McCarty: At rest, they're just fancy files on disk. They're tarballs that have some metadata wrapped around them that are defined by the Open Containers Initiative. And then at runtime, they're just fancy processes that have extra controls in place constraining them. Before Red Hat acquired CoreOS, I wrote an article that took a spin on their CEO's article about, can you run databases and he asked all these different questions that he gets that are... I don't want to say he said they're dumb but we get all these questions that are remedial. They're like, can I run a database in a container? And I'm like, replace the word container with process and then ask that question again. Jim Walker: Right, exactly. Scott McCarty: Can I run a database in a process? And you're like well, I don't know any other way that you can run a database. So if you use the right mindset, these questions, they're pretty easy to solve themselves, answer themselves. So I joke they're just fancy files and fancy processes. And then there's one other component, there's a registry server. It's just a fancy file server. And other than that, that's it. Fancy files, fancy processes, fancy file servers. Jim Walker: Yeah, nothing really new. There's just so many names spaces that have been around for a long time here, y'all. So this is just something, if anybody understands Linux, it's a pretty easy transition to do this. But Scott, if I think about containers... And your title, right? Is principal product manager for containers. And I know there's container platform and Kubernetes Engine and there's OpenShift. Is there anything to innovate in containers? Or are we at steady state and basically, the OCI has defined this and Docker did a great job of pushing everything? Are we at steady state with containers? Scott McCarty : Well for me, no. It's funny because my day in and day out, I'll admit, I'm down in the engine room. I jokingly say, most of the time, these are things that a CIO won't understand what we're doing. They have to just trust that we're constantly adding new features and making it better but they don't really necessarily understand what that is, nor should they need to. That's why you buy something like OpenShift. But that said, no, my days are fairly intense. Jim Walker: I know. Scott McCarty: There's a ton of features that we're still adding in CRI-O and Podman in particular. I'll give you an example, what we're working on right now is image mounts. So something Docker never thought about and nor should they have needed to, it was new. But you're like oh, I want to fire up one container and then I want to modify another container with that. So can I bind mount in this container and then scan it, analyze it or maybe even modify it? And so we have these two features that we're working on in Podman called overlay mounts and then also image mounts. And so you can imagine, this is the flour and sugar and eggs and water of the cake that is containers. If you really look at the way a container works, it's just a bunch of overlay file system layers and all of them are read only except for the top one. And the top one you write to and then when you save the container, you just add one more layer. Scott McCarty: It's actually not nearly as complex as people think. And I have diagrams and graphs, I go into these Container Internals Labs. If you Google Container Internals Labs, you'll find all kinds of deep dive information I give. But in a nutshell, once you learn that that's how containers work, you realize you can do it inside of a container as well. So we're looking at, how can we bind mount a container image into a running image, into a running container and then maybe do a read write layer that's a temp FS? And then when this container goes away, this one goes away too but we can scan it and add things. Scott McCarty: There's all these basic use cases where you want two containers to be able to talk to each other in a really deep way through a POS-X file system mount, essentially. And so that's one of the things we're doing right now. We need this for, for example, Anchor and and Twistlock and Aqua Sec and all these security scanners. They want a way to bind mount in an image, analyze it read only, so you can't muck it up. But know what's going on it and then save all that data back to a database somewhere. That's the perfect use case for that. Those kinds of features, we do all the time. Jim Walker: And I think it's just one of those things, as we become much more mature with distributed systems. Look, really, who was just doing distributed systems five years ago? Okay, really a handful maybe of companies. And some of them probably weren't doing it very well. You walk around QCon today and wow, there's a lot of people interested in this stuff and actually pushing the bounds of these things. And I think Polvi and what the team did at CoreOS to actually help drive some of these core initiatives... They had Rocket, which was their container runtime and I know there's been competing versions of this but to think of what an operating system needs to look like in a container. I think to me, that's the interesting piece, right? Yeah, these things are going to run on an operating... People don't realize, in that container is an OS, right? Scott McCarty: An OS, yeah. Jim Walker: Let's shrink the size of your container, right? And so I presume just that Linux level of expertise is where you're living, right Scott? Scott McCarty: Yeah, it is and it's weird because there's two competing narratives. One narrative is that OS doesn't matter anymore. But then when you dig into the covers, you're like wait a minute, actually, it matters more in a lot of ways than it ever has. So that's a strange part, you go, how did CoreOS get sold for $250 million if the OS doesn't matter? And how did Red Hat get sold for 34 billion if the OS doesn't matter? There's definitely two competing narratives depending on what your interests are and goals and desires are in the Universe. Jim Walker: Yeah. The OS always matters, I don't care what anybody says. Scott McCarty: Yeah, even with functions as a service, I don't see how it doesn't matter. Jim Walker: Yeah, right? Scott McCarty: It's still going to matter. Jim Walker: Yeah. Really, serverless, it's still servers and there's still an OS running on those things. And I tell you what, there's a whole lot of optimizations that happen and it has to happen at that layer to actually make serverless a reality, in my opinion. I think we are only scratching the surface of what that world is going to be. And what is it a truly serverless environment? To me, it ends up being something Kubernetes like, if you will, almost, Scott. Where you are communicating at that layer for all those things to happen, right? Scott McCarty: Yeah, absolutely. Yeah, there's two sides in my world. There's the container images side and the container host side, they're the two main things that we're going to... Now at the container images side, we're always looking at things like serverless and distroless and things like that. And both of these words bother me, serverless and distroless. Jim Walker: I know. Scott McCarty: Neither of them are actually without servers or without distros, just because you don't put a package manager in a container image, does not mean that there's not a cadre of human beings going out, that are subject matter experts in all these different pieces of software and packaging them in a way that is consumable. That's still the same business requirement, even if you don't use RPMs or maybe you don't use a manager but somebody has to still go do that work for you. And you need to be able to consume I easily. Jim Walker: Serverless is just somebody else's servers. That's what the cloud is, right? It's just somebody else's servers, right? Ultimately. Scott McCarty: And I joke, distroless, it's just somebody else's distro. Even if you look at the Google server list, a lot of the different build languages rely on Debian packages. They don't include the package manager and you can't rebuild them but they're still pulling from that dependency tree. So dependency trees still matter, even in 2020. Jim Walker: So one other thing that's related to this, I think some people who are newer to containers or aren't familiar with the complexity of... You and I talked a little bit before this about the supply chain and how do we get applications to production? There's also things like Ansible or at CoreOS, we had Quay, right? Can you just describe to me what those tools play in this whole supply chain? Because I think they're actually pretty important for people to understand, right? Because the point is not easy. Scott McCarty: Yeah, absolutely. So this is probably a good segue into describing what OpenShift 4 is versus 3. If you look at OpenShift 3, we basically had a Kubernetes distribution, OpenShift. We had Ansible deploying that Kubernetes distribution on RHEL, Red Hat Enterprise Linux, which is a Linux distro. And so you can understand all of the primitives there, right? You're like oh, it's a configuration management system, a Kubernetes distro, a Linux distro and wire it all together, you do upgrades. That's pretty easy to understand, that's probably how the whole world did things. You go back to 2005, I was using a homegrown CF engine that we had written at American Greetings, doing e-cards and I left there in 2005. But I called that Web 1.0. That was the tail end of Web 1.0. E-cards were still making more money than you think, even in 2005. Jim Walker: For sure. Scott McCarty: We did do distributed systems with CF Engine. We were doing with, a CF Engine equivalent, that we were basically pushing out to a thousand Linux web servers. The business problem there is the same, it's running 75 different services across a 1000 different nodes. If you look at OpenShift 3, we're kind of doing the same as we'd always done, except that we were deploying the Kubernetes distro. So you had one workload that you needed to deploy to all of these with a config management system. And then all the other workloads where the application workloads were deployed with Kubernetes Yaml. You could think you'd hit that Kube API and deploy all of your stuff there. But then when you go to OpenShift 4, we actually kind of extended the API down a layer to the cluster itself. I'll admit, I don't think we, even Red Hat have done a good job of explaining what OpenShift 4 is, but it is a profoundly different way of thinking about Kubernetes. Scott McCarty: It treats the cluster itself as ... you treat every object in the cluster, including the nodes as objects in the Kubernetes cluster. It's essentially extending that, ... wait, let me back up and say, config management works on the concept of defined outcome. I want this thing to have user added or whatever, make it an item potent, but do it, don't do it 20 times, but add it, make sure it's there. Kubernetes works on a very different paradigm where it's defined state, actual state and it's constantly ... it's got a timer going, a controller that's basically constantly looking for ... looking at the defined state, looking at the actual state, comparing the two and trying to make them look the same. Scott McCarty: So that's a very different paradigm because, config management might run on a deploy and then maybe if you were super brave, you'd run it once a day or once an hour or something like that. Scott McCarty: Kubernetes is doing it all the time. All the time at any given times per second or minute, it's looking at the state. Managing the cluster itself as part of that defined state is what OpenShift 4 does. So you manage the nodes, you manage the container engine, you manage everything. So if you look there's this thing called the MCO, is what we call it. And it basically is an operator and an operator is basically just an extension of Kubernetes that takes that defined state, actual state and applies it to other things. So we're basically taking this MCO and applying it to the node and the container engine that's running on that node, which are the two main things you need to configure. Historically we did that with Ansible, but now we're moving it into the cloud. So now it's a single API. You literally hit the Kubernetes API, this rest API, and you can manage everything, from the node to the engine, to the containers that are running on it to the state of config files on the cluster. You're treating the cluster as a single computer now through a single API endpoint. Scott McCarty: Those two paradigms are very, very different. So then that changes, not to be super complex, but to answer your question, what roles do these things play? They're different in each of those worlds. Jim Walker: That's right. That's right. It all collectively comes out as OpenShift. And the value in OpenShift is this greater simplification is abstraction up away from Kubernetes basically and these kind of crazy configurations and, you know, I don't like to write Yaml. I avoided COBOL early on in my life. I don't like to count spaces. I think it's one of these things that why deal with any of that? I want command line, but I don't, maybe a bit of a UI to manage and to do all these different things, but for me what y'all doing with OpenShift is truly tremendous and it's all in open. It's all open source. Jim Walker: So there's kind of this boundary though, between Kubernetes and what Red Hat is doing an OpenShift. It's a very tightly integrated platform of what's happening here. How do you guys choose what happens in say, the Kubernetes community and the stuff that Clayton and that whole team is driving coding away on, and then what the OpenShift is seeing. You're a product manager, so you sit between the two sides. How does that work internally with you all? Scott McCarty: This is actually touches on this concept of open source as a supply chain. So I think there are fundamentally two ways that people look at open source. There's sort of the ... let's go three, there's three ways. There's companies that look at technology as a closed core, and I'm borrowing Aquaint's and mine's terminology very specifically there, Joseph Jacks calls it “closed core.” It's the idea that we're going to build everything proprietary and we're going to deliver all value to the customer through this proprietary means. Then there's debate about what does open core mean? I'll fully admit there's an open debate about what does that mean. Scott McCarty: Red Hat is at the other end of that spectrum where essentially the vast, vast, vast, vast, 99.9% of our supply chain is open source. I leave room for a long tail there of there's little pieces of glue code and things that exist in our environment, like our build systems and there's things behind the scenes that aren't necessarily public, but you don't need them to run OpenShift at runtime or anything. It's just you need some glue code here and there to make your specific environment work. Scott McCarty: So I'd say this becomes a supply chain choice. If you look at OpenShift, it's made up of basically, there's two major pieces of the supply chain. Well, actually Kubernetes is a huge chunk of the supply chain for it. I would say that's like the motor in a car, let's make a car comparison. The motor is Kubernetes. You need an electric motor or gasoline or diesel motor. You have to decide this if you're a product manager. You have to decide which motor meets the needs of my customer. You go my customer wants to pull trailers. So maybe we only have diesel and electric as an option, maybe gas isn't good enough. These are the kinds of decisions that the PM makes. So, then you look at OpenShift, I'd say Kubernetes is the motor. And then you look at all the things that run on top of Kubernetes. First let's start with what's below it, Red Hat Core OS, below it. You look at the supply chain, it's Fedora, then RHEL and RHEL Core OS is basically just a different snapshot and version of the exact same bits are in RHEL. Scott McCarty: So the supply chain is the Linux is the motor for that. And there's a whole bunch of thousands of other projects around it, for the door handles and everything else. And then you come down into a distribution like Fedora then down into RHEL and RHEL Core OS. You could start to imagine in your mind, a map of this crazy deep web of all these different projects that go into OpenShift. So the supply chain for OpenShift is basically Kubernetes to Linux upstream, major motors and then downstream you go to OKD. OKD is again, an open source freely usable. You don't have to pay a subscription to use it, sort of interim project. Think of it as almost like a distro like Fedora, where it brings it all together. Scott McCarty: And there's OpenShift. So how do we decide what goes where? That's the biggest question, right? Jim Walker: Yeah. Scott McCarty: This is where I try to talk about it in my article about what is open source product management? Your upstream suppliers should do something different than what you do. If your upstream supplier sells fuel injectors and you sell fuel injectors, you're going to have a problem. You guys are never going to be able to differentiate each other, and you're never going to make money. If the upstream supplier sells a fuel injector and you sell a car, it's a lot easier to differentiate. So you have to look at the use case. The use case for OpenShift is it's a, I always joke, it's a dump truck that can go 200 miles an hour and it carries 10 tons of dirt. That's not something that everybody needs, but if you need a dump truck that goes 200 miles an hour and handles pretty well and carries 10 tons of dirt, that's an interesting use case. It's an advanced use case that basically people that don't want to have to manage that entire supply chain themselves handle, and so we decide based on what the need of the user is essentially. Jim Walker: Yeah. Yeah. Let's abstract that exact point up Scott. And so, yes, there's the internal workings of the car and fuel injectors and the engine and the drivetrain and the chassis and all these things, and I think those things are important. I think what OpenShift is doing is pulling all that together. So you get a car. Scott McCarty: Yeah. Exactly. Jim Walker: Or I'm sorry, in this case, a pickup truck. I'm new to these things, you say it's a pickup truck, how do I work this into my architecture today as an enterprise organization. Let's abstract it up to a whole other layer and what am I using it for? What's the value I'm getting out of OpenShift and Kubernetes, at that layer? Without getting too deep into the weeds, what are the workloads people are using, we'll come to that as the next thing, but what is it? Why do I want to buy this thing ultimately? Scott McCarty: To be fair, it's not that the workloads are necessarily different, although they can be. The user of OpenShift wants something that just works out of the box and we'll upgrade for 10 years, whatever, seven years in a row. You're not going to get that value out of the upstream. If you're upgrading Kubernetes every six months, you're going to have a lot of work on your hands. The bottom line is that's just a lot of work for you to do. And that costs money, and that costs engineers and engineers are hard to hire. It's pretty natural where the fit is. We want to make the experience lifecycle, FIPs compliance, security, so government organizations, large enough enterprises that they, they would try to tackle this themselves and realize that they don't want to do it because it costs a lot of money and these are very hard engineers to get a hold of. Scott McCarty: It's more based on a business understanding of do I really want to tackle all this myself? Do I want to buy a bucket of parts and build a dump truck, or do I just want to buy a dump truck, take it to Ford and have them service it? Jim Walker: Right. And I think that's one of the things that people struggle with with Kubernetes. There's a lot of pieces. It's complex. I have to deal with storage and networking and security and all these different things. And I think what OpenShift does is simplify a lot of that for the end user. Scott McCarty: It's a prebuilt solution. Jim Walker: Exactly. And having a packaged solution around those things. And we used to do this with, with Hadoop. I mean, could somebody roll their own Hadoop distribution eight years ago or whatever? Yeah sure, but why would you do that? And then you need indemnification, all these things. One of the other areas that you just touched on very clearly and people are asking why is this important? Back in Core OS, this whole concept of tectonic was how did we do rolling upgrades and how do we automate upgrades of software? Scott McCarty: There you go. Upgrades is probably one of the biggest challenges of Kubernetes. Jim Walker: That's right. Yeah. Each pod has to be updated and there's different layers that have to be updated in each pod. Then the control plan itself needs to be updated. And I think that's one of those core benefits. Talking about the operational complexity of Kubernetes, well, just understanding of the damn thing took me awhile to figure out. Running it in production is difficult. I think that's where the value for ... then again, I'm not the product manager. See I'm on product marketing. See, this is how we work together buddy. Scott McCarty: The funny part is you're like, you would never find a business in the Fortune 2000 that would build their own Linux distro. I mean, maybe, if their business is embedded systems where they're doing some very, very specific things, but you're not going to find anybody building their own Linux distro to run SAP on it. That just doesn't even make any sense whatsoever. I think it's a little bit easier to see with a technology that's a little bit older and more mature because you start to realize you're like, \"Oh yeah.\" The perfect car for a family would be like, mom and dad build the car specifically for the family, with the exact size seats for every family members, the exact safety required. Nobody does that. That's irrational and not efficient. And so the market provides an approximate solution that's pretty good. That's worth paying for essentially. Mom and dad would rather buy a minivan than go build a minivan that's perfect for their family. This is so obvious with cars. It's so obvious with Linux, but it's still not completely obvious with Kubernetes because it's new and people look at it and they go \"Oh, this is strategic to our business. So we want to build it and then add all these things to it, and then maintain over time.\" And one to three years into that, they go, \"Yeah, this was not necessarily what we actually wanted to do. We would actually be better if we were building stuff on top of Kubernetes that actually met our business needs as opposed to building Kubernetes.\" Jim Walker: Right. Exactly. I think that's where people are struggling right now. They get too deep in the reeds. Tim, you see people out there using Kubernetes all the time. What are the workloads you're seeing kind of people shift into Kubernetes? Is it all of them and what does that journey look like for them? Tim: Yeah. It's funny. I think Kubernetes still for the folks that we talk to is a bit aspirational. It's on everybody's roadmap and certainly those who are doing it, are doing it at the app layer. Customers are certainly talking about, \"Well I'm moving my application workloads, my microservices to Kubernetes type architecture.\" But, obviously here at Cockroach Labs, what are we selling? We're selling a database and we want to, and can run, CockroachDB on Kubernetes. And so we're starting to have the conversation with people not only using Kubernetes to deploy application servers, but to start to deploy a storage layer. That kind of really, at times, makes people's head kind of explode. \"What do you mean I can run a database as we were talking about containers?\" \"Well, sure. Of course.\" \"Yeah. I can run a database and Kubernetes?\" \"Absolutely, for all the same reasons that you'd want to run your application in Kubernetes.\" Not everybody's there yet, but they're getting there. They're definitely getting there. Jim Walker: And I think it's one of these things we're going through a paradigm shift. Literally. It's going to take all of us time and, Scott, you're in it. When you're deep in it, you're just there. Whereas if there's a mass majority of people just don't understand, what is the result of distributed systems for you? Let me shift a little bit into the data and what that actually means in these systems. The first time I ever saw CockroachDB, and here I am now at this company a few years later, Polvi from CoreOS was up on Brian... I forget his last name. The guy who runs OpenStack. I forget the guy's name. He was the president of the OpenStack consortium or whatever. I forget what it was, but we're at OpenStack Summit. Yeah. They were at OpenStack Summit and they wanted to show off Kubernetes and how you couldn't kill this thing, and what was the distributed application? Well, it was the database. Like, holy cow, I can have this distributed database. Right? And so I think we're seeing people think about Kubernetes. When it first came out, people were like, \"Oh, it's for stateless workloads.\" I didn't even know what that means. You know what I mean? Are you still hearing a lot of those same kind of concerns from customers, Scott? Like, where am I using this thing? How am I going to use it? Like, that's great. I trust you Red Hat and this OpenShift thing's awesome. I'm sold. Are customers struggling with the workloads and how to instrument and how to make it all happen? I presume that's one of the reasons why OpenShift is here in the marketplace and all that. Right? So... Scott McCarty: Absolutely. So one of the most popular talks I've given over the last five, six years has been my migration talks. If you look, in a generation we've gone from bare metal to virtual machines. I mean, we can go back as far as you want, mainframes to mini computers, to Unix boxes, to Linux, to virtual machines, but there's always been three options. Right? There's lift and shift. Can I just take this thing I have and run it on this new thing? And does it give me some benefit? Right? And then there's augment. You're like, well, we'll move part of it and then we'll add some things to it to maybe make it work better. And then there's the, let's just write all this crap from scratch. And obviously, it gets more expensive. You go from lift and shift as the cheapest to rewrite everything. The CIO does not want to hear that you're rewriting everything. So this is constantly like, there's always going to be a business challenge here. Right? And so, yeah, absolutely not. One of my number one conversations is on what can we put in OpenShift or what will work well? Jim Walker: Right, exactly. Scott McCarty: What do we run on RHEL and what will work well there? And what should we just leave alone because it's not worth moving in? And these are like, yeah, absolutely. And they're really complex questions depending on the environment they require. A lot of times it didn't result in consulting going in and analyzing a whole portfolio of applications, looking at them and all that. And you're like, \"These 32 run here, these 72 run there,\" you know? And it's not an easy question to answer. But at a minimum, I mean, the thing I try to guide people is if you could separate the code, the configuration, and the data, whatever that means. With MySQL, you can separate the code, configuration, and data. But the problem is the data is one thing, unless you're using Galera or using something like Cockroach. If you have a single piece of data that's one chunk that's not set up to be distributed essentially to have multiple replicas and things like that. If it's not cloud native, which we tried to teach people with OpenStack, but I think we just rammed through that wall and just people kept going, like, \"I can run it like a virtual machine.\" Now with containers, I think it's finally we're at a hard place where if you really want to run this in Kubernetes, you need to be able to imagine a couple of things. One, this thing could run anywhere, because Kubernetes can decide where it can run it wherever it wants at any given moment. And then two, it might restart it a thousand times. So you've got to be able to handle two things, right? Can it move anywhere at any time and restart a thousand times? Apache can handle being restarted a thousand times. An Oracle database, not so much like. You restart an Oracle database a thousand times on a thousand different nodes, that's probably going to create a problem. Jim Walker: Good luck. Scott McCarty: So something like Cockroach that's designed for that is going to handle that a lot better than something that's not designed to do that. Jim Walker: Oh, no. And I think it comes back to the core principles of distributed systems. Right? Build it as a small single atomic unit so it can be deployed the same way everywhere. Right? They're built for scale so they can easily scale them, so they communicate in that way. They will survive at all times. Right? Jim Walker: And I think those core principles come back. You mentioned something, Scott, that I found interesting. It was like, look at, I can lift and shift. I'll take the thing  and I'll dump it into a container and run it. Right? I can make some small changes to it, augment, augment, and okay, maybe it's distributed at that point, whatever that means. Scott McCarty: Yeah. Or maybe we run part of it outside the cluster, part of it in the cluster. That's- Jim Walker: Right. Exactly. Or do you just re-architect from the ground up? And here at Cockroach Labs, we chose to re-architect from the ground up. And you just set some off in my mind. And I realized why I joined this company is because if you're going to run a database on Kubernetes, you have to re-envision it. It's not as simple as just running it, mounting a persistent volume storage class and figuring that out. If you try to augment or you try to lift and shift, it's not built for that world. Right? And so we realized that with the database, is this some of these things, like if I'm a consumer of OpenShift or I'm going down the path of Kubernetes, maybe my application is just not built for this new world? Is that a question that organizations should be asking themselves? Scott McCarty: Oh, yeah. Absolutely. Absolutely. Because if you force a round peg in a square hole, it's not going to work well, and you're going to end up with more nightmares. Like I said, just cram an Oracle database on Kubernetes and see what happens. It'll work great for a few days, months, years, until one day Kubernetes runs into some problem where it can't schedule it and it restarts it 200 times and then the database gets corrupted. And you just end up with these crazy problems. Right? Because it wasn't designed to run in a distributed way. Jim Walker: And then somebody was asking a question was like, what are the challenges to moving a database into Kubernetes? And I think that's exactly it, Scott, right? Like if you're going to run a legacy database, like you want to run MySQL or Postgres in a pod? Okay, great. But you need to understand that pod is ephemeral and it may die. And so even with stateful sets, which is a key concept in Kubernetes, if somebody doesn't know what that is, I would check out stateful sets. It's a very interesting way of actually maintaining some sort of state when a pod dies. Right? The simplest explanation, right? Scott McCarty: Yeah. Jim Walker: Without that, it was a complete lost cause, but even with that, it's still a problem. Right? Because it- Scott McCarty: It still won't handle a hundred restarts. That's the other problem I tried to explain. It's not just about where it lands. It's also about how many times it lands there. Jim Walker: Right. Scott McCarty: There's time and space. Like I always try to explain to people, there's time and space in Kubernetes, and you have to decide can this thing handle being moved anywhere in the universe a thousand times per second? That's a different problem set. It has to be designed for that. Scott McCarty: Now there's things like Apache that can handle that pretty well. You're right. Maybe you have a very small website, a blog. It can probably handle that with just a MySQL database. But you do that at scale, that's probably going to fall down. That's where it starts falling down. Jim Walker: Yeah. And really it's choosing the right workloads to run in these environments and where it makes sense. Where are you going to get the efficiencies, and why do you need that efficiency, I think is the biggest question, right? And I think that's where I've always had these conversations about workloads and Kubernetes and OpenShift and all these things. It really comes back to that. Jim Walker: Going back, though, and just talking about applications, moving them over and this sort of thing, there's this concept of an operator and a Kubernetes. I remember Brandon Phillips and I having a conversation about operators. Actually, I think Brandon wanted to call them “controller operators.” And Rob was like “controllers.” I'm like, \"No, man, just operators.\" Should people be thinking of that? Is that the conduit to make your app work in Kubernetes, or is that just basically something that the software engineering teams that are building things that are going to be deployed, they should be thinking about? Right? You know what I mean? So is this something that's consumer grade or is it more like the industrial grade supply chain vendors have to think about that? Scott McCarty: I think it's both. I think both groups of people have to think about it. I mean, that's probably, again, one of the biggest conversations I've had over the last few years is explaining to people what operators are. The way I've broken it down, and I bumped this off Rob to see if he was cool with it, but if you think about traditional operating environments, operational excellence, what did it take to achieve operational excellence? You deployed an app on a server, and then you had a human being also called a sysadmin, that would make sure those two things work together. And that's how you achieved operational excellence, right? Scott McCarty: In this Kubernetes world, you really need a robot sysadmin called an operator that now you deploy the app on the platform, and now it's a cluster treated as, again, an OpenShift for paradigm is that you could treat the cluster as a single computer basically, but you're spanning across a bunch of nodes. So it's the platform, the app, and then a robot sysadmin that you deploy side by side with the app that takes care of the app. So the operator can do things like back it up, restore it, check on it if it's broken. I talk about if it restarts 200 times and the tables get corrupt, the operator will know what to do, right? The operator's what helps handle, coddle these sort of, I call them cloud immigrants. They might not be cloud native, but they've immigrated into the cloud. Jim Walker: That's right. Scott McCarty: And so coddling that. That's what the robot sysadmin/operator does. And so the knowledge now moves from, we used to have that in a runbook in a Wiki, and instead of in a Wiki, we now have moved it into the operator. Right? Jim Walker: Well, yeah, and for a while there- Scott McCarty: Somebody has to do that work now. Jim Walker: That's right. And for a while there, we moved it into the SRE's head. Right? And so the SRE was basically going off and building scripts. And SRE being one of these artifacts of Google and their whole approach, again, it's like, is this GIFE, which is Google infrastructure for everybody? Right? Like, yeah, kind of. I think the operator was codifying- Scott McCarty: I think so, too. Jim Walker: Some of the things that the SRE was doing, because operating these things at scale are incredibly difficult. How do you actually manage certificates in a distributed environment across every pod and make sure that everything's going to be sorted out? Okay, Vault's going to probably solve one problem for you, but you still need to distribute and make it... These things are complex. Scott McCarty: It's about velocity in my mind. So if you looked at the velocity of the standard application, you could fix it once a day, once a month, once a year. I had a server that we booted up, put RHEL, I forget what it was, six or seven on it, and it ran, probably five or six, sorry, and it ran for 10,000 days or whatever it ran, and then we shut it off. It was ridiculous. It booted once and it shut down once. So we never had to reboot it. There has never been a remotely exploitable kernel bug, so we never had to reboot it. This is what people don't understand. Scott McCarty: That kind of velocity is very different than the velocity of, I might not need to do this multiple times per second or minute. And so when you looked at the way config management works in a traditional environment, if, for example, your monitoring system notices a web server goes down, your config manager could go out or you literally could have Nagios talk to Ansible to go out and restart the web server. Right? This would happen once a month, once a week. That's fine. That feedback loop is slow enough, and there's enough slack. It's kind of like a 1960s car. It's like the gear, you find a gear and you can feel it grab. And you're like this car can still go- Jim Walker: But that's what makes it fun, man. That's fun. Right? You feel it. Scott McCarty: It is fun. But when you're talking about inside the Kubernetes cluster, this defined state, actual state methodology, it's an OODA loop that's basically happening all the time. And so the idea is you're deploying the operator in this tight OODA loop. You're now in the reactor, right? It needs to build a happen and react at the state of the Kubernetes database, so at the speed of the Kubernetes database state change. Right? And so that's what that operator does. Scott McCarty: You deploy the automation inside the cluster and it has access to the state data, unlike you. So historically, you had a monitoring system, a fault monitoring system, a config management system, and then the underlying platform and all of these different disparate systems had to talk to each other, and they didn't share a database. But if you deploy all this in Kubernetes, it's all saved in the SED database. And I think in my mind that velocity that that can achieve of restarting a container within seconds of it failing is a different animal than doing it. Oh, well it took us two, three, four minutes to get this thing back up. Okay. No problem back in the day. But at scale with capacity, that doesn't work. Jim Walker: Yeah, and it's interesting. You talk about state and how you use that in the context of the operator is funny. When I first joined Cockroach Labs, we got to build an operator. And I remember guys on our team that understand Kubernetes, he's like, \"What are you talking about? We don't need an operator.\" And actually, because we were designed as a distributed system that can actually natively survive a failure of a pod or region or whatever, because you can do that, you don't need a lot of those core operator things that some applications may need. Because you got to think about scale or resilience, right? Scale is how do you deploy and actually get all these things to work, right? Or rolling upgrades. These sorts of things are not always easy to deal with. How do you apply patches across the huge one? Scott McCarty: Yeah, rolling upgrades is a huge one. Jim Walker: Yeah. How do you apply patches? And I think those are the core patterns that I see in operators. And we have on operator. I know we're retainer certified and then our operator is now certified by Red Hat as well. I'm probably pushing out some news before Cube, but what the heck? The people on the thing will know. Right? Jim Walker: But yeah, we did it because, well, there's certain things or certain deployment patterns that actually are difficult to do, and it goes beyond just being a distributed system. And I think that's the kind of stuff that I think people- Scott McCarty: And upgrades- Jim Walker: Yeah, sorry. Scott McCarty: Are a perfect example. Jim Walker: Yeah. Scott McCarty: I was going to say, you really struck me when you say upgrades. Upgrades is probably the hardest thing. Again, go back to a traditional environment. Imagine upgrading a MySQL database from, I don't know, a major version, three to four, whatever. And you go, \"That's a lot of interaction.\" You've got to shut the database down, back it, up, run some kind of up, install the new version of the software before you do that. Scott McCarty: How do you do that? Which order do you do these things in? You have to shut it down, export the database then shut it down, then remove the software, add the new version, or maybe install them side by side. Then you've got to run some upgrades script that changes a schema. Then you've got to fire the new one up. Oh, it didn't work. Oh, shit. Now how do we get the schema back to the old version to restart this? That's not easy. Jim Walker: Scott, that's one instance. That's a stovepipe database. Scott McCarty: That's one instance, yeah. Jim Walker: Start talking about I have 50 containers all running this software- Scott McCarty: That's why you had maintenance windows that were eight hours long. And SSH into each one of the servers and start figuring this out and do it. So the upgrade process, applying patches, which patches are so incredibly important today. I mean, security problems are just massively important. Like, you can imagine historically this has happened with go and fiddle-fart with all that figured out. Then we got to automation where it had like an Ansible, or Chef, or CF Engine. We could kind of codify those rules but it still took... I mean, dude Ansible upgraded with something, it still takes hours sometimes to upgrade things. You don't have hours, right? You've got to go figure all that out at the factory and then deploy it in production. I jokingly say containers are about building at the factory, not at the dock, right? Historically you go back to the 1800's, we'd take all the lamps, and pianos, and barrels, and boxes and crates, and literally on carts with horses. We take them down to the dock and load them on and it took freaking three, four or five days. It would take months to load a ship. People don't understand this, like a month. Then we ended up getting the containers and you're like, we loaded the same amount of stuff that's just as complex because we did it at the factory where it was air sealed and clean... and we did it in saltwater with open air. Just put it on there, turn some twist ties, lock the thing down. Jim Walker: Right. Scott McCarty: That's the idea of an operator. The operator is able to handle just these final last mile operations, with access to the state data of what's going on in the entire system at the same time. But since you have that unified state data, it's like a robot sysadmin that has access to the- Jim Walker: That's right. Scott McCarty: ...it's like I just hooked my brain into a distributed system and like, \"Oh, I can feel the state of all the things.\" It's the matrix. Right? You didn't have that kind of power with a regular configuration management system, but you do with an operator. Jim Walker: Right. Scott McCarty: Because you can check all these things in real time and figure out what's going on. Jim Walker: Right. Honestly, if I think back about a couple of years ago it wasn't that Kubernetes couldn't do this, it's just that Kubernetes is its own layer. It has to do what it does, which is basically let me manage state. Ultimately Kubernetes is one thing, it listens to etcd and says, \"This is the state that I need.\" That's it. Literally if you're going to bring it down to the most simple thing etcd is just \\[crosstalk 00:46:57] it says, \"Here's my state.\" As you said, Scott, \"I need to make sure that the state looks like this.\" It means five instances of this, six instances of that, 120 instances of this please, and that's all it does. It's really all it does ultimately in the end right? Scott McCarty: Yep. Jim Walker: There's a lot of complexity and there's networking and storage and all these things that have to happen. Right? Scott McCarty: It handles defining the state really well, not- Jim Walker: managing the state- Scott McCarty: ... changing the state. Jim Walker: Right. Exactly. Scott McCarty: When there's things you got to do in between the state change. Jim Walker: That's right. Scott McCarty: Yeah. Jim Walker: If people think about operators and where they fit, that's how they fit. There's the thing that's basically controlling and making... it is the Uber robot. then you got a robot controlling the robot. Right? It's robot SRE is what this is, so. Scott McCarty: That's what it is, it's a robot SRE. Then OpenShift 4, is basically about having robot SREs for both the applications, which is what regular operators do. Then we've also written system operators that manage the host and the engine, and then all the other software that runs an OpenShift. Jim Walker: Let me do my job and make my robot overlords happy as well. Let's talk about the Red Hat Marketplace and what you guys are doing there. I think people are asking a lot of questions about like complexity of applications. Are we just basically building things because engineers have free cycles? Or what is the value of the marketplace? We are definitely in the marketplace and we're seeing adoption through there,. What is the vision and what's going on with OpenShift from that point of view? Scott McCarty: So in my opinion, the marketplace is like the final culmination of what we've wanted for 20 years. If you go back, when I first started doing this like '98, there wasn't really a concept of cloud, but automation was already like... Even in 2000, 2001, we knew what automation... Unit sysadmins knew what automation was. Right? Jim Walker: Right. Scott McCarty: But the problem is everybody built automation for their specific environment everywhere. Right? So if you really think about, if you had 1,000 different companies building automation, there was 1,000 different automations they were building. So it was 1,000 times the amount of work that you need to do. Jim Walker: Yeah. Scott McCarty: Then I would say by like 2005 to '10, even '11, '12, we had OpenStack and we started to see the writing on the wall. They're like, \"Wait a minute. I can interact with the API. Then I've got to build the automation once and I can deploy the automation at the 1,000 sites, but write it once.\" We could see it, but it didn't quite go to the point where you could select the automation that you want and deploy. There's a chef in the kitchen that was still choosing the ingredients and figuring out how to make the food. But there wasn't a menu, right? There was no menu. You couldn't just order- Jim Walker: That's right. Scott McCarty: .... it was like cooking at your house. I saw a lot of projects fail because I was actually a solutions architect at the time for Red Hat. I remember adding one customer in particular. They had this wildly ambitious plan. They used ServiceNow. All this crazy homegrown automation with like Ansible, and Chef, and Puppet or whatever they were using. They had like all the moving parts of what's in the marketplace today, but there was no standard way to do the automation. It was still an open system that didn't have constraints placed on it. I'd argued now for the first time, since we've standardized on Kubernetes, we've kind of standardized on this operator framework. Which really is a framework of like how you do the automation. You now have the ability to standardize a marketplace. This is like options trading. Options are standardized, there's a set price and a set date. So you can trade them in Chicago, nobody knew that in 1776. Nobody was like, \"Oh, we'll just trade these options. I'll buy something at a future time.\" We realized that if you set the standards, you can actually trade these things on the market. Jim Walker: Okay. Scott McCarty: That's what we finally done. We finally figured out how to get software into a market and trade it essentially for dollars. Jim Walker: Right. Scott McCarty: Because all the automation is standard. So now, if you go there and you look at CockroachDB, you click on a button, it'll install. You know it will install because it was built on specific versions of OpenShift. Jim Walker: That's right. Scott McCarty: So you know that will work. We're really close to the dream, if you will, the dream state. Jim Walker: Yeah. Scott McCarty: We're finally to the electricity where you just buy the electricity and it actually works. Jim Walker: That's right. That's right. I think it's just taken time. I've contended over the past five to 10 years, what we've done is we've basically abstracted out all the components of the software delivery supply chain. From the fingertips of the, of the developer all the way through to the mouse click of the user and everything that has to happen in between. We finally have codified that- Scott McCarty: Finally. Jim Walker: ... into its constituent parts and it's all API based so that, that entire supply chain... Just as we've done in the past with a physical supply chain, like a wool jacket, like you're talking about. Right? The time it takes and all the components from the raw materials to packaging, to getting it in the store, putting a price in a skew and all that. We basically created a very well greased automated supply chain for software. I think Kubernetes is basically now expanding that to even deeper depths of global deployments. I think doing that has made huge changes. It wouldn't have been done without the likes of the companies like the Chefs and Puppet. Those were huge, massive steps. Everybody the CloudBees team and what they're doing in CI/CD a huge piece of this. Jim Walker: I think about LaunchDarkly and what they're doing with feature flagging. How cool is that, that we have that in the supply chain? I feel like OpenShift Scott, the culmination of a lot of those things. It's Ford versus the one-off car, right? That's where we're headed, right? It's like the full end to end production facility. Correct? Scott McCarty: Yeah. I liken it to like Swatch. People don't know, but in 2010 ish, they created the first automatic watch that was touchless. From the plastic to the time it rolled off the assembly line and you literally put in a box and everything. You just went in the truck and it went to the store. It was historically for like 500 years, we've been making watches or whatever it is, there always had to be a watchmaker. The closest they'd gotten it like into the 2000s was having a watchmaker fine tune it at the very end and they'd- Jim Walker: Right. Scott McCarty: But they figured out a laser way to do it now that it's like completely touchless manufacturing. What we've done is touchless manufacturing, but instead of just for one company for one product line, we've done it for an ecosystem of products. Jim Walker: That's right. Scott McCarty: Like now, CockroachDB, and even your competitors, and your friends, and allies, and all these different pieces of software, they get all built on the same essentially assembly line that basically allows them to have this touchless manufacturing. Jim Walker: the trick is, it's all going to end up in different stores and in different countries and all these different things. So I'll ask you a bit of a directive question. I'm a believer in this thing, but do we live in a multi-cloud world? Is that basically the future? I think there's some people who say, \"No way, it's never going to happen. It's too complex.\" I'm a believer in it personally. Scott McCarty: Here's what nobody realizes, just like the cloud... Do you remember the narrative seven years ago when we were like, \"You're already using Amazon and you don't know it.\" Jim Walker: Yeah, right. Scott McCarty: Well, you're already using Azure and Amazon and you don't know it like. Jim Walker: Yeah. Scott McCarty: The CIO doesn't know, but he has developers in this group doing this thing and developers in another group. I've seen CIO do stuff as crazy as not allow... Literally working with a credit card company to limit where people spend money. Because, they can't get a handle on it. They can't control where everybody's going. I would say, I don't know, 80% of the fortune 500 is probably already have multi-cloud and they don't know it. So now it's going to be well, five years from now when we finally realize and wake up, \"Oh, well, you actually are using... wait, we're using how many? Seven different clouds.\" Let's figure out how to make all this stuff work together, right? Jim Walker: Right. Scott McCarty: We definitely need to get a handle on this and then control the costs and blah, blah, blah. Jim Walker: Well, I asked the right question. You just almost threw your microphone right off the table, dude. I mean, like Scott- Scott McCarty: I'm passionate about this one. Jim Walker: I'm super passionate about this one. In fact, we created a multi-cloud conference last year called Escape. I know the Red Hat team was extremely supportive of it because I think they shared the vision. Personally, I've been an OpenSource person for a while. Jim Whitehurst and Paul Cormier, the leaders of Red Hat have done such a phenomenal job, actually understanding that the world is not homogenous, right? The world is heterogeneity and it's always going to be that way. By the way, the world is also open. Scott McCarty: Yeah. Exactly. Jim Walker: I think that's a lot of the trick. I commend you guys all y'all at Red Hat. The delivery of OpenShift and what I'm seeing an OpenShift 4, and I'm sorry, but I'm an old CoreOS person. I’m really proud to see this really coming to light. To see the marketplace in its reality. Looking back to my conversation with Brandon Phillips a long time ago, \"Like, man, it'd be great if you could just push a button and deploy.\" By the way, not even worry about upgrades, because that's just all automated and background. Scott McCarty: Yep. Jim Walker: To me the- Scott McCarty: It's the app store of B2B apps- Jim Walker: That's right. Scott McCarty: ... as opposed to just a B2C app on your phone. Jim Walker: That's right. Kudos to the team on everything that's been going on. But we're at the top of the hour. Scott, thank you so much. That was a great conversation. I had a hard time getting a word in edgewise, but that's good. It was really good. Sorry, Tim- Tim: Hey, I can just as easily be quiet on these things.I liked it too. It was great. It was awesome. So thank you. Jim Walker: Yeah. Super insightful. So, but seriously, thank you. Thanks for joining us, Scott. It was really, really a lot of fun. Say, Hey to Rob from me over there. Scott McCarty: I will. Thank you for having me. I appreciate it. Jim Walker: Alrighty. Everybody, yeah. Recording this will be available. There are no slides for you to send. But I hope it was useful for everybody. I think I believe we, we still send out a survey, so please do fill out the survey. Let us know how we did. We're always trying to improve these things as always, it's constantly evolving. So for on behalf of Three Guys with Flannel Shirts, thank you everybody for joining us today. So thank you everybody for joining us and I hope you have an enjoyable rest of the day. Bye now.", "date": "2020-10-30"},
{"website": "CockroachLabs", "title": "Why I Left IBM to Work on CockroachDB", "author": ["Adam Storm"], "link": "https://www.cockroachlabs.com/blog/why-i-left-ibm/", "abstract": "I’m a database nerd. Or, to be more precise, a DBMS nerd. What I love most about them is that while they’re everywhere, and modern society could not function without them, they’re incredibly difficult to build well. Part of this difficulty stems from the fact that databases are complex, and their construction borrows from nearly all fields of Computer Science. My love of databases, however, wasn’t always this strong. Back in university, I avoided the Databases course at all costs, inferring (quite incorrectly) that there wasn’t much new to learn in the domain, and that all of its hardest problems had already been solved. This avoidance came to bite me quite quickly when, upon graduating, I was recruited by the local DBMS development team at IBM, and I decided to join them. Joining IBM - The early days of self-managing databases When I first joined IBM, I worked on the Autonomic Computing team, a team whose mission was to make life easier for database administrators by finding ways to automate tasks that were difficult, or error prone, when performed manually (e.g. physical database design, statistics collection, memory management). Specifically, I was asked to lead a team whose goal was to automate the tuning of the DBMS’s various memory configuration parameters in a way that would optimize performance. While the work was exhausting (on more than one occasion I fell asleep with my laptop beside me), it was rewarding to produce a novel solution to a prickly problem&mldr; Our work ended up being enabled by default in the product, and is still in use at thousands of customers today. At the time we started working on automating memory tuning, all of the academic approaches available suffered from constraints which prevented them from being implemented in an industrial product. For instance, there were several approaches available for tuning buffer caches, but many of these required dividing queries into classes and specifying response time goals. This query division and goal specification could often be as difficult as the underlying memory tuning. Furthermore, while there were papers written about tuning individual memory consumers (like caches, working memory, locking memory, etc), there was no work which explored how one could unify the approaches to create a mechanism for automating tuning for a grouping of memory consumers. As part of our investigation, we also uncovered several complications which made memory tuning difficult on a complex DBMS. For example, one of the largest consumers of query memory in a DBMS is the memory required for sorting. Every time a query contains an ORDER BY clause, and there is no corresponding index to leverage, the system must sort the data. This sorting is most efficiently performed in memory. If however, the sort is too large to be contained in memory, most systems allow the sorted rows to “spill” to disk by writing them to a temporary table. The temporary table then is cached by the system (in a write back cache) before possibly being written to disk (in cases where the cache is not large enough to hold the entire temporary table). The fact that there exists this interplay between sort memory and caching memory, makes tuning of the memory difficult. Ideally, the system would optimize the amount of sort memory to prevent spills (which are costly, even if they never spill all the way to disk), but adding additional caching memory can give the illusion of tuning progress as it can accelerate the act of sort spilling. A second complication was that most academic approaches assumed that individual tuning decisions were free. In actuality, this is far from the case. When decreasing the size of a write-back cache for instance, any dirty pages that exist in the section of memory being freed, have to be written to disk. As a result, frequent cache size reductions can increase the burden on the I/O subsystem. Additionally, since contiguous blocks of memory must be freed to be of use to other memory consumers, the writing of pages to disk must wait on any pages that are currently in use by concurrent transactions. In practice we found that it was necessary to model the cost of these memory decreases, and employ a Control Theory approach to reduce frequent oscillations in consumer sizes. These challenges combined to make the project very demanding, especially since I was so inexperienced. Not only did I need to wrap my head around the complex way in which memory management was performed in the DBMS, but I had to do so while at the same time learning about Control Theory, commercial software development, and leading a team for the first time. Thankfully, I had brilliant partners in IBM Research, an amazing manager and mentor, and a very forgiving team. While the work was exhausting (on more than one occasion I fell asleep with my laptop beside me), it was rewarding to produce a novel solution to a prickly problem (resulting in several academic research papers ), and grow so much professionally at the same time. Our work ended up being enabled by default in the product, and is still in use at thousands of customers today - something which fills me with tremendous pride. Distributed transactions and commodity hardware Once we shipped the first wave of Autonomic Computing features, our team was split up to help work on other more pressing projects. As part of this split, I ended up working on a team that was determining the feasibility of bringing some technology which had been very successful on the mainframe - Db2 Data Sharing - to commodity hardware. When Db2 Data Sharing was introduced in 1994 (along with the mainframe’s Parallel Sysplex), it allowed databases which were typically limited to a single machine, to scale to multiple machines. This made it one of the first examples of a distributed relational database, and was a huge relief for customers struggling to scale their databases to meet workload demands. When Oracle followed suit with their Real Application Clusters (RAC) in 2001, there was pressure at IBM to bring a competing technology to the non-mainframe market. I was asked to lead the transaction management team. I was joining a team composed of transaction management experts. While this could have been intimidating, it actually freed me from having to make daily decisions around the technology, and instead allowed me to focus on leading (which the team needed). It was here where I grew my skills in software development project management&mldr; The problem with doing so however, was that mainframes had some technological advantages over UNIX-based servers. For example, mainframes had both high-speed interconnects, and hardware-based clock synchronization, both of which made distributing a database over multiple servers much more feasible. Luckily for us, when we started this work in 2006, Infiniband connections were gaining widespread adoption, and combined with Remote Direct Memory Access (RDMA), allowed for extremely low latency interactions between nodes of the cluster. RDMA, combined with an internal implementation of a Lamport clock, allowed us to deliver similar technology that existed on the mainframe, to our existing non-mainframe customers. This new role was my first introduction to core DBMS development, and is where I learned about things like ARIES for the first time (fortunately enough, from the paper’s original author). The first year was a tough slog, as there was so much to learn, but it was also exhilarating to be learning something completely new, especially after being deep in the bowels of memory management for so long. Once the team proved out the prototype (to which I had very lightly contributed on account of all the learning), we were green-lit to build it into our product as what would eventually be called Db2 pureScale. At that point we needed to scale the team out dramatically, and since I’d been learning about the technical details for several months, I was asked to lead the transaction management team. This was very different from my experience in Autonomic Computing (where everyone on the team was new to the area), as I was now joining a team composed of transaction management experts. While this could have been intimidating, it actually freed me from having to make daily decisions around the technology, and instead allowed me to focus on leading (which the team needed). It was here where I grew my skills in software development project management, while at the same time picking up enough of the technical details to be able to make a significant contribution, especially at the tail end of the project. Building a column store into an existing relational database In 2009, when we shipped Db2 pureScale, I went off in a completely different direction, but was reunited with two old friends. My old manager (and his manager), from the Autonomic Computing days, were starting up a team to investigate the possibility of incorporating recent research on Column Stores into our product. The difficulty in the task was what drew me to it. That, along with the fact that it was uncharted space, which would allow me to both lead the team, and design the technological solution. Doing both at the same time wasn’t always easy, and the pull of project management would often distract me from what I really wanted to do, which was code. Since the early days of relational databases, the prevailing wisdom was that rows needed to be stored contiguously on disk to maximize performance. This made sense for transactional workloads, where rows are inserted and updated one at a time (and therefore, having them in a single place on disk is most efficient), and queries are often accelerated by indexes. For analytical databases however, rows are typically modified as part of batch jobs, and it’s common for queries on very wide tables to select only a handful of columns and, in the absence of indexes, scan large portions of the table(s). As a result, it can be more efficient to store column values for consecutive rows together, as queries can read only the column values requested by the query, and save the I/O required for all unread columns. Column storage also lends itself well to vector processing, where column values are stored consecutively in memory and processed in batches as opposed to individually. As an added bonus, column storage also helps with data compression, as a given page of data (or even an entire file on disk) will contain a single column whose values are often drawn from a limited set of possibilities (e.g. State, area code, gender, or age). In my initial introduction to column stores I was asked if I’d join a team along with some people from IBM Research to see if we could build a rapid prototype for integration into Db2. As we were partnering with the hardware division on this prototype, I was asked to take on the work to add SIMD instructions to our new vectorized processing engine. This was like nothing I’d ever done before, and getting right down to the machine instruction level was both exciting and daunting. When we successfully completed the prototype and the work to build it into the product began, I lobbied to lead the team which would be building the column store’s insert/update/delete capabilities. The initial prototype we built was insert only (well, to be fair, bulk load only) and ensuring that we could allow fine-grained data modification was no easy task, as rows were split into multiple files on disk, and we were required to match insert performance of the existing row store, at least for large batches of inserts and updates. The difficulty in the task was what drew me to it. That, along with the fact that it was uncharted space, which would allow me to both lead the team, and design the technological solution. Doing both at the same time wasn’t always easy, and the pull of project management would often distract me from what I really wanted to do, which was code. In the end though, getting to something that would delight our customers was my ultimate goal, and if that meant more technical troubleshooting than actual coding, I was happy to do that. I also happened to be very lucky that the team assembled around me was very strong, and well balanced. They were a pleasure to work with, and some of them continue to be close personal friends today. In 2013 we shipped our solution (named, BLU Acceleration), and published a research paper about its novel design. My experiment with Hybrid Transaction/Analytical Processing (HTAP) After we shipped BLU Acceleration, the researchers responsible for its inception decided to turn their attention to Hybrid Transaction/Analytical Processing - HTAP. Database workloads are broadly divided into two classes: transactional and analytical. The goal of HTAP is to build a system which is optimized for both transactional and analytical processing, using a single data copy. This is no trivial feat, since as mentioned earlier, transaction processing benefits from storing rows together on disk, while analytical processing is often faster using column organized storage. I started collaborating with IBM Research on the HTAP effort, as the sole developer. The work was interesting, and we made good progress, but ultimately, the company was not interested in pursuing HTAP at the time (at least, not in the deeply integrated manner which we were pursuing). In all situations, it’s helpful to be at the right place at the right time. While it’s not clear to me if deeply integrated HTAP is the right “place” (it certainly has its advantages, but it’s very difficult to get right), we didn’t have the timing right. At the time the biggest influence on the database space was the emerging shift to the cloud, and HTAP alone did not directly address the challenges brought about by this shift. As a result, while the work we were doing was interesting, and likely would have borne fruit, it was not the organization’s most pressing challenge at the time, which is why it failed. Building a new DBMS for IoT workloads When the HTAP work was shut down, I put together a proposal to build a cloud-native analytical database from the ground up. As with transactional databases, traditionally analytical databases were designed to run on a single machine. In the 80’s and 90’s however, database designers realized that long running analytical queries could be processed much faster if they were split over many machines and run in parallel. This “splitting” involved partitioning the database and having each machine own a portion of the database. The fact that machines wouldn’t be sharing data at all (each machine would own a distinct portion of the database) led to this approach being called the shared nothing architecture . For the next several decades, shared nothing would dominate the analytical database landscape. Then came the shift to the cloud, or more specifically, the shift to cloud-native storage. The work was also invigorating, as we were unencumbered by the difficulty of retrofitting an existing system, and could instead focus on innovating. In the cloud, the most cost-effective way to store large data sets, and analytical databases typically contain very large data sets, is through object storage (AWS S3, Azure Blob Storage, Google Cloud Storage). This not only makes storage much less expensive, but also places data on storage which is visible to all nodes of the cluster, opening up a whole new range of possibilities. For example, with data on shared storage, partition ownership can be reassigned to nodes almost instantaneously, without having to physically move data on disk (as is the case in traditional shared nothing deployments). Instantaneous partition reassignment not only allows the clusters to expand and contract on a per-query basis, but it also dramatically simplifies high availability, since partitions owned by failed nodes can be easily reassigned to surviving nodes. Recognizing that a shared nothing architecture on top of cloud storage was the wave of the future, I proposed that we build such a system and first focus on IoT workloads. To accomplish this, the organization asked me to assemble a team outside of the database organization, so that we would be free to innovate without restrictions. This was refreshing, but also challenging, as the team needed to build up all of the infrastructure we’d taken for granted over the years. Fortunately, it also allowed us to innovate, and we leaned heavily on newer technologies like Docker, Jenkins and Kubernetes, which weren’t being used in the database organization at the time. The work was also invigorating, as we were unencumbered by the difficulty of retrofitting an existing system, and could instead focus on innovating. Unfortunately however, the team dynamics necessitated that I spend most of my energies on maintaining harmony, and less on technological decisions. This was only possible because I had such a strong technical team, which I trusted implicitly. While the road to success wasn’t direct, we eventually shipped our IoT-optimized cloud-native database (named Db2 Event Store) in 2017, and recently published a research paper on its initial design and evolution. Moving away from technical work - My time as an executive As recognition for my work on Db2 Event Store I was promoted. In my new role I no longer led focused technical teams but rather, worked more closely with the executive team on our division’s strategy. This included more time working with customers, providing architectural oversight to several large and successful products, and planning for our future. The work was rewarding, and I really enjoyed working with my fellow executives (especially my boss), but ultimately it took me away from the deep technical work that I enjoyed so much. It was in part because I missed the deep technical work that I was receptive to new opportunities, when people came knocking. This culminated in me leaving IBM and joining Cockroach Labs earlier this year. What’s some advice you’d give to people just starting out in the database industry? One of the more enjoyable aspects of being a leader is the impact you can have on the careers of others. I’ve been fortunate enough to mentor many people over the years and am frequently asked for my opinion on how to succeed in our industry, and the database domain specifically. Here are a few things I mention to people who ask: #1: Be a generalist The database space is thrilling with its enormous breadth and depth of technical challenges. As a result, it’s possible to be incredibly broad or incredibly deep. Some people have a natural tendency to get deeper and deeper in one area (say, query optimization, developer interfaces, or transaction management), and to be fair, going deep is required for the industry to evolve and solve increasingly complex problems. When building a career as a DBMS developer however, I’ve found that there are benefits to being broad. Specifically, it allows you to identify problems that exist across the system, and work to address them more holistically. When asked about this specifically, I generally advise people to learn as much as they can, about as many areas of the database world as possible, early on in their careers. If you take this approach, it will allow you to identify problems and propose solutions outside of the space in which you’re currently focused, and drive more value to the organization as a whole. #2: Avoid the money trap It’s often the case that people just starting out in our industry will chase the offer with the highest payday. I understand the temptation. When you’re early on in your career, there are pressing financial considerations (saving for a house, marriage, starting planning for retirement) and that bigger paycheque can seem like the right way to rank offers. The problem however, is that in many cases maximizing your return early can work against you later in your career. When you’re just starting out, you should instead be trying to maximize learning opportunities, even if that means lower compensation. Becoming a vicious learner early, will ensure that when you’re in your maximum earning years (your 40s and 50s), you’re most desirable and impactful. To that end, if you’re just starting out your career, find the opportunity which will allow you to make the most impact to the company in which you work, learn from experts around you, and become a generalist by building your skills in a bunch of different areas. #3: Make your own luck Anyone who’s had a successful career (or an unsuccessful one, for that matter) would be remiss to mention the extent to which luck has played a part. Being good helps a lot, but a significant helping of luck can make the difference between being mediocre and exceptional. So much of life is about being in the right place at the right time. To a large extent, luck is not something you can change. You have no influence on the family into which you’re born, the language(s) you grow up speaking, or the geo-political situation you find yourself in as a child. That being said, as you get older, you have the ability to influence your luck, and you should take advantage of that ability. When I finished my undergraduate degree and was looking for a place for graduate studies, I had several offers to consider. One of them was by far the most lucrative, but in the end I decided to go to the University of Waterloo because I felt that doing so would position me best down the road. Two years later when I was recruited from there to join IBM it was partly by luck (I graduated right around the time that IBM was recruiting - it was the right time) but also, the database group at IBM was only recruiting from University of Waterloo at that time (I was in the right place). If I hadn’t gone to University of Waterloo, it’s likely I never would have ended up at IBM. From there the luck kept snowballing. At IBM I had the tremendous opportunity to study under some of the founders of the database industry - members of the original team that built System R, as well as their successors. The depth of knowledge they possessed about database systems is something that still humbles me today. I was unequivocally lucky to have been recruited into such a tremendous learning environment, but the luck I created for myself by going to Waterloo started everything. When deciding where to work, which team to be a part of, or how to spend your energies outside of work, it’s always helpful to remember that we have the ability to influence our own luck. While you can’t always control the “right time”, you may have a better sense for the “right place”. Why I left IBM to join the team building CockroachDB When switching jobs, especially after so many years, there’s rarely a single reason why. After having many conversations about the move with friends, family and colleagues, here’s the distilled list of reasons why I’m so excited to be at Cockroach Labs: Awesome tech: I’m sure you’ve realized by now that I’m a techy at heart. As a result I’d find it very difficult to work at a place where I don’t believe in the technology I’m building. Luckily at Cockroach Labs, that’s not a problem. CockroachDB was designed from the ground up for infinite scale, the ability to survive anything, and strongly consistent transactions. With the database market moving so heavily to the cloud , it’s the database architecture that the world needs now, and that need will get only more intense in the future. Maturity of the product: CockroachDB is in a sweet-spot, much like Db2 was when I joined IBM. While it’s feature and function rich, and has been recognized for its innovation by sophisticated customers , there are still many meaty problems to solve. Getting a chance to solve some of these problems is what excites me to come to work every day. Scaling an already strong team: Cockroach Labs has built a very strong team to develop CockroachDB. Everyone I’ve met during the interview process, and since joining the team, has been warm, welcoming, and very strong technically. At the same time, we’ll need to grow our team dramatically to accomplish all of our lofty goals. I’m excited that I’ll get to be part of scaling the team and working on all of the new challenges that scaling brings. 🇨🇦 DB The North: It would be crazy for me to omit location from this list. I’m a proud Torontonian, and love living and working in the 6ix. The fact that Cockroach Labs recently opened up an office here was one of the keys to me making the move. And it’s a great move for the company too. Toronto has been a hub for database development for years, with several large database vendors setting up shop here, and some of the country’s most talented database researchers close by. The database talent in Toronto is intense, and we’ll only be adding to it with our strong team. Sometimes it’s just time for a change: All of the excitement above doesn’t directly explain why I chose to leave my old job. I could easily write another TL;DR post on this answer, but to net it out, it came down to wanting a change, and an opportunity to see a different part of our vibrant industry. For me, life is too short to retire having only seen a small portion of what’s out there. If you share my passion for solving tough technological problems, making an impact on a rapidly growing segment of the market, and working on a strong welcoming team, we’d love to hear from you .", "date": "2020-11-17"},
{"website": "CockroachLabs", "title": "First-time Open Source Contributors Making a Difference at Hacktoberfest", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/hacktoberfest-2020/", "abstract": "With 29 CockroachDB apps created and 33 Docs issues closed, it’s officially a wrap on CockroachDB’s Hacktoberfest 2020 project . While planning for Hacktoberfest, we asked our community: what holds them back from contributing to open source and how we could help them overcome those obstacles? The unanimous response holding developers back from contributing to open source was that they wanted us to help curate “beginner-friendly issues.” So that’s what we did. We combed through our code and docs repos to identify beginner-friendly first issues. We also created a s ample applications repository for folks to contribute CockroachDB apps using a language/framework of their choice. The goal here was to allow contributors to use a technology they were comfortable with while learning a new technology (CockroachDB), and helping our open source project build out our sample apps repo. Win-win! The project served its purpose: It enabled folks to make their first-ever open source contributions: Mentored by our Education, SQL, and App Dev teams, the new contributors were able to take their first giant leap into the world of open source, and created some very cool PRs! Some contributors chose to contribute all 4 PRs required to win the Hacktoberfest to CockroachDB projects: In addition to the open source projects, we also published An Absolute Beginner’s Checklist for Hacktoberfest which proved to be a community favorite. We had a blast participating in Hacktoberfest and welcoming new contributors into our open source community. Thank you DigitalOcean, Intel, and DEV for organizing the event. If you wanted to contribute to CockroachDB projects but missed out on Hacktoberfest, you can participate in the next MLH Hackathon and compete for the ‘Best Use of CockroachDB’ prize . In the meantime, don’t forget to join the fun on our CockroachDB community slack channel .", "date": "2020-11-13"},
{"website": "CockroachLabs", "title": "Thanking CockroachDB's Open Source Contributors", "author": ["Oliver Tan", "Rafi Shamim", "Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/thanking-cockroachdbs-external-contributors/", "abstract": "CockroachDB was conceived as open source software, and we are proud that CockroachDB remains open source to this day. Throughout our journey, our community has made valuable contributions to our product. Over the course of our existence, we have had over 1590 commits from over 320 external contributors across all our open source repositories. Today, we want to thank all our wonderful external contributors. In this blog post, we celebrate open source contributions across the CockroachDB repositories, with a glimpse of how we manage contributions to our own repo. We take a closer look at the impact of external contributions on the spatial offering for v20.2 and CockroachDB ORMs . We also take a look at how we encouraged first-time open source contributors during Hacktoberfest 2020. Finally, we recognize all our external contributors and celebrate their work. Open Source Contributions to the Cockroach Repo The CockroachDB repo contains the core database product, where most of our contributors show initial interest. In this section, we will dive into how we encourage external contributions for the database as well as how we manage the community and reviews. Labeling good first issues We follow GitHub’s recommendation of regularly adding the good first issue label to issues to solicit contributions. Generally, the issues we affix with the “good first issue” label are small bug fixes or feature improvements that can be approached with minimal context. These good first issues generally contain a small set of reproduction steps. We have found that external contributors find it easier to resolve issues that include pointers to code snippets and tests to look at in the codebase. Our team’s extra effort to include relevant pointers pays off since these issues are usually picked up quickly by the community. Out of the 85 “good first issues” closed in the past year, 30 had been commented on or merged by an external contributor within 14 days of being tagged “good first issue,” and 50 within 30 days. Assigning reviewers for external contributions Members who are not part of your GitHub organization cannot request reviewers - and they may not know whom to talk to! It is important to us that all external contributions are looked at, and we do our best to ensure they get merged. A simple remedy that any repository can use is to add a CODEOWNERS file in the GitHub repository that will automatically tag a team from Cockroach Labs. However, this is only effective if owners were declared for each part of the repository, which may not be true for bigger projects like CockroachDB. To make sure no external contributions go unnoticed, we created a GitHub bot named blathers , which looks through issues mentioned in the PR description and tags authors or issue commenters from the Cockroach Labs organization (see example ). Blathers can also automatically detect common anomalies found in PRs and suggest fixes to them (see example ). Failing that, blathers will provide steps for finding help with reviewing the contribution (see example ). Supporting our open source contributors We have ample resources for getting started with contributions, including a customary CONTRIBUTING.md file that links to the more detailed Contributing to CockroachDB public wiki page.\nOur community Slack channel also contains a #contributors slack channel , allowing external contributors to engage, ask questions, and follow up.\nFinally, we aim to actively maintain “good first issue” tickets and respond to our contributors' questions to unblock them quickly. Strong follow-up both here and on Slack generally leads to a successful external contribution. Celebrating our contributors over the past year Currently, we have had 333 commits from 96 open source contributors in 2020. These contributions - no matter how big or how small - have all incrementally made CockroachDB a better database. We are grateful to our contributors and would like to thank them here today: 23 Contributions: Erik Grinaker 18 Contributions: Aniket Bharsakale 16 Contributions: Dima 15 Contributions: Artem Barger, Jaewan Park 13 Contributions: Abhishek Gupta, Shinwanee, Max Neverov 10-11 Contributions: Vaibhav, Martin Besada 6-9 Contributions: Rohan Suri, Joël Gähwiler, dkorwar, Adzim Zul Fahmi, Marcin Knychała, Neeral Dodhia, Irkan 4-5 Contributions: Eugene Kalinin, Ini, Mnovelodu, Vincent Xiao, Yongyang Lai, Cyrus Javan 2-3 Contributions: Arun Ranganathan, Jay, Alina, Chanyoung Park, Girish Ramnani, Anthony Huang, glorv, James, Vipul Gupta, Hugo, Lars Lehtonen, Jason Brown, Joshua Shanks, Cyrus Javan, Amit Sadaphule, Cholerae Hu, Oleg Kovalov, Georgy Savva, Hiep Doan, Himanshu Chawla, Kumar Akshay, Matthew Rathbone, Xuhui Lu, Yanxin 1 Contribution: Arun Mittal, BurtonQin, Dominique Rau, Sergei Gorjunov, Terry Wong, Nitesh Koushik S, Richard Tweed, Tayo, UdokaVrede, Adrian Popescu, Antoine Grondin, Latern4ever, Ben McCann, Clucle, Damien Hollis, Deven Bhooshan, Andrew Grosser, Ian Cowley, George Papadrosou, Gabriel Jaldon, Gábor Liptak, Harsh Seth, heidawei, Joshua M. Clulow, jieniu$, Jake Rote, Jaime Soriano Pastor, Juan Carlos, Katie, Kirk True, Lance Rutkin, Masroor Hasan, Michael Meng, Omar Bahareth, Petr Jediný, Poh Zi How, Pratik Sethi, gary rong, Ruixin Bao, SaintMalik, Sam Ward, Shaker Islam, Tancredo Souza, Teiva Harsanyi, Themis Papavasileiou, Tom Milligan, Vincent Thiery, TAKAHASHI Yuto, zhiqiangxu, Zain Malik Impact of Open Source Contributions to CockroachDB’s v20.2 Spatial Offering Open source contributors have been instrumental in the large number of spatial functions we were able to make available in CockroachDB’s spatial data offering . So far, we’ve had 59 commits from 15 contributors covering over 70 spatial functions out of the 249 we have available. This section will dive deep into how we corralled these contributions and celebrate the contributors. Maintaining Issues for Spatial Functions For our spatial offering, our aim was for CockroachDB to be able to be a drop-in replacement for PostGIS , an extension of PostgreSQL providing spatial functionality. This is in line with our goals for CockroachDB to be a drop-in replacement for PostgreSQL. We knew this would be a daunting task. In spatial functions alone, PostGIS has over 300 functions for vector geometry, covering anything from encoding/decoding to coordinate clustering. We were understaffed for this lofty endeavor and would have had to cut scope for our first spatial release. However, we realized that spatial functions were easily parallelizable once we had a foundation for manipulating spatial data. We also knew that we had a keen community eager to contribute! Once we implemented a few basic functions, we compiled a spreadsheet of all functions to be implemented. Then we created a tool that converts the spreadsheet entries into GitHub issues based on an issue template . The template contains detailed instructions on where to begin, what changes need to be made, where to add and run the tests. The tool also updates the spreadsheet after an issue is closed. We decided to tag issues we considered suitable for external contributors as “E-Easy.” As over 140 issues were deemed suitable, it would have polluted the good first issues page for contributors looking to contribute something outside of spatial. However, we did create a meta-issue with the good first issue tag to make the spatial issues discoverable. With the practices highlighted in the earlier section, we ensured that all contributions were reviewed and issues and inquiries were responded to promptly. The results - 59 commits implementing over 70 functions from 15 contributors - speak for themselves. Altogether in v20.2, we managed to implement 229 functions, a fair way towards the 300+ functions PostGIS has today. Our efforts in creating the spreadsheet, syncing with GitHub, and providing implementation details paid off. Celebrating our spatial contributors We are extremely grateful to all our contributors on these important spatial data features. They are, by order of contributions: 19 Contributions: Erik Grinaker 6 Contributions: Jaewan Park, Arthem Barger 5 Contributions: abhishek20123g, Vincent Xiao 4 Contributions: Marcin Knychała, Azdim Zul Fahmi 3 Contributions: Cyrus Javan 1 Contribution: manhhiep92, himanshuchawla009, Themis Papavasileiou, Michael Meng, Deven Bhooshan, Arun Ranganathan, ArjunM98 CockroachDB’s spatial offering would also not have been possible without Tom Payne . We found that Tom’s open source go-geom library provided the best solution for manipulating spatial features in Go. Tom’s quick reviews for our patches and guidance for features were instrumental for our spatial offering. Thank you! Our spatial functionality today would not have been available so early without the quality contributions from the individuals mentioned above. Thank you for your contributions! The Open Source Community’s Impact on CockroachDB’s ORMs Having a great database is one thing, but just as important is being able to connect to it and use it within an application. The open source community has been instrumental in getting existing object relational mapping (ORM) software and other tools to work with CockroachDB. We highlight and thank them here. Cockroach Labs maintains several open source projects built internally by engineers to integrate with ORMs. These “dialects” (which sometimes are also known as “adapters”) tell the ORM how to create SQL queries that are compatible with CockroachDB and can also provide other helpful functionality like automatic transaction retry logic. We maintain dialects or helpers for Django , SQLAlchemy , Sequelize , ActiveRecord , and various Go libraries . It would be impossible for us to stay on top of all of these on our own, so we are immensely grateful for our community of developers who report issues with these projects or submit bug fixes and improvements. Thanks to the following contributors! 16 contributions: Christian Köhn 6 contributions: adamgee 3 contributions: Zac Pullar-Strecker 2 contributions: Georgy Savva 1 contribution: Christian Kulpa, Dominique Rau, Terry Chun Wong, Danilo Cabello, Garvit Juniwal, Masroor Hasan, Neeral Dodhia, Ronan Cherrueau, Paul Colomiets, Xuhui Lu In addition to projects that we maintain, we’ve also benefited tremendously from all the hard work done on tools that are supported by others over the past year. The teams at Hibernate , Flyway , and Liquibase have added first-class CockroachDB support into their projects, and our developer community has already contributed directly to those projects as well. A few other tools with maintainers dedicated to supporting CockroachDB in their projects (and who work directly with us to identify fixes and improvements within CockroachDB) are TypeORM , .NET EF Core , Elixir/Ecto , knex , and Beekeeper Studio . Having an ecosystem of tools like these is key to our success so far, and just as importantly improves the lives of developers who work with CockroachDB. Hacktoberfest 2020 Encouraged First Time Open Source Contributors While we get amazing contributions from experienced developers, we know that the CockroachDB repo can be overwhelming and intimidating to new contributors. To lower the barrier to entry and welcome first-time contributors, we participated in Hacktoberfest 2020 and designed beginner-friendly projects for the participants. We identified and tagged beginner-friendly issues in our code and docs repos and also created a sample applications repository for folks to contribute CockroachDB apps using a language/framework of their choice. The goal was to allow contributors to use a technology they were comfortable with while learning a new technology (CockroachDB) and helping our open source project build out our sample apps repo. The result: 29 CockroachDB apps created and 33 Docs issues closed ! Celebrating our External Contributors Going Forward We acknowledge all CockroachDB external contributions in the release notes for each release - but this does not cover contributions to other repositories. To recognize our external contributors in a more general fashion, we have set up a Hall of Fame with statistics on all external contributions to CockroachDB. The page will list all our external contributors over the course of our history and will be updated regularly. Contribute Today! At Cockroach Labs, we are proud of our external contributors and always keen to welcome new contributors.\nFor keen contributors out there - join the fun!\nTo get started: For contributing to CockroachDB: Follow our contribution guide , which contains instructions for getting CockroachDB set up, and look for issues labeled “good first issue” . For our documentation, check out our Docs Contribution Guide . For our ORMs, check out the READMEs in Django , SQLAlchemy , Sequelize , ActiveRecord , and various Go repositories. If you are ever stuck, feel free to contact us on the #contributors channel on our community Slack - we’re always happy to help! And if you are interested in becoming a full-time CockroachDB contributor, we are hiring !", "date": "2020-11-23"},
{"website": "CockroachLabs", "title": "What’s So Special About Spatial Data?", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/spatial-data-types/", "abstract": "How is Lyft able to tell you how far away your driver is? How does DoorDash give accurate estimates for the food you just ordered? Both of these satisfying user experiences are possible because of spatial data. In this blog we’ll cover the basics of spatial data and then show some examples of common applications and use cases that use spatial data. What is Spatial Data? Spatial Data, often referred to as geospatial data, is any data that contains information about a specific location. In layman’s terms, spatial data is data about location. You may not realize it, but you’re already familiar with spatial data because you interact with it whenever you open your map application to look for the nearest gas station or to see the full landscape of cafes nearby. The quantity of data that relates to location is overwhelming, but becomes easier to digest when broken down into the two most commonly used spatial data types. Spatial Data Types The two primary spatial data types are Geometric and Geographic data. Geographic data is data that can be mapped to a sphere (the sphere in question is usually planet earth).  Geographic data typically refers to longitude and latitude related to the location of an object on earth. GPS data is a good example of geographic data. Geometric data is data that can be mapped to a two-dimensional flat surface. A good example of geometric data would be the floor plan of a building. Think about the last time you were running low on gas in an unfamiliar area. You probably used a map application to pull up all the gas stations nearby. That application contains geometric data that captures all the data related to the roads nearby. That application can tell you how long it will take you to get to the gas station by applying driving speed pattern data, with traffic pattern data, and the geometric data. The map application is the simplest of spatial data examples. There are literally thousands of other examples. Examples of Spatial Data Applications, Use Cases, and Workloads IoT Applications & Spatial Data The Internet of Things (IoT) refers to networks of objects that are embedded with sensors (think Bird scooter or Citi Bike ) that make it possible to send data from the “thing” to a database. IoT workloads often employ spatial data. Here are a few examples: IoT Platform Analytics: Spatial data is the data that shows where users are signing into an application or, to reference the Bird example, accessing a scooter. Real-Time Sensor Detection: This is spatial data on your Fitbit tracking how far you ran (or didn’t run) today. Personalized Views: Google maps knows where you are, and it knows you like Thai food and Mexican food. It can serve you an ad based on your location and the fact that you have a crippling affection for Taco Bell. Transportation/Logistics Applications & Spatial Data Transportation and logistics companies deal with the movement of people and products. These include companies such as airlines, trucking, railroads, shipping, and logistics firms, as well as companies that provide transportation infrastructure. Here are a few examples of spatial data workloads from that industry: Operations Research: Spatial data knows which machine type will work best in a certain location based on the topographical characteristics of a location as well as the surrounding conditions like temperature and traffic. Supply Chain Management: Spatial data can estimate how fast a person or a product will get from point A to point B. Real-Time Analytics: Spatial data can tell you where in the world your latest shipment of olive oil is. Environmental Technology Applications & Spatial Data Environmental technology is the use of electronic devices to monitor a natural environment. The output of environmental technology could be measurements of tidal patterns, temperature patterns, or the status of a forest fire. The most relatable spatial data workloads have to do with using environmental technology to protect the environment and ourselves. Flood Risk Analysis: Spatial data keeps track of areas that are particularly susceptible to flooding by combining the geographic data with weather data and historical data. Real-Time Natural Disaster Detection: Spatial data can help predict where a wildfire will spread by capturing its current location and then factoring in the wind speed/direction as well as the objects that lay in its path. Farming/Irrigation Management: Spatial data can record which areas of a farm have been serviced or not serviced by the machines used for planting, harvesting, pruning, or irrigating. These are just a few examples of how spatial data is used in the real-world. There are tons of other use cases for spatial data that relate to urban planning, or fraud detection, geomarketing, civil engineering, and more. How to build with Spatial Data When you talk spatial data you have to talk PostGIS. PostGIS is the spatial database extension for PostgreSQL. It has over 300 different built-ins and functions to make it easier to work with spatial data. PostGIS has helped launch apps like Instagram and FourSquare and is included in the tech stack for countless other applications. Until very recently Spatial Data workloads could not reliably be built on any of the truly distributed databases. But that isn’t the case anymore. CockroachDB, the cloud-native distributed SQL database, now supports spatial data types . Rather than reinvent the wheel, CockroachDB uses the same PostGIS compatible SQL syntax . So you can build applications that leverage spatial data on a database that’s always on, easy to scale, and simple to use. Deploying spatial data workloads on a distributed database, rather than a monolithic database means that reads and writes aren’t routed through a single node and you don’t have to do asynchronous replication across multiple sites. And when it’s time to scale to different regions you don’t have to worry about manual scaling responsibilities. If you have questions about spatial data you can ask them in the CockroachDB community slack . If you want to learn more about deploying distributed spatial data workloads you can look at our spatial features support documentation .", "date": "2020-11-18"},
{"website": "CockroachLabs", "title": "Highly Available Spatial Data: Finding Pubs in London", "author": ["Michael Goddard"], "link": "https://www.cockroachlabs.com/blog/highly-available-spatial-data-demo/", "abstract": "Spatial Data & High Availability Imagine you’re driving a rental car in Rome and the satnav (or GPS) on your phone stops working. This happened to me two years ago when I was commuting by car each day from an Airbnb in Trastevere to an office on Via Amsterdam . In that moment I remember thinking two things: I wish I had paid more attention to landmarks on previous drives and spatial data applications need to be highly available. Fast forward to today and the world is different. We’re in the midst of a global pandemic that has inflicted much suffering and has kept us, for the most part, at home. After being home for the past eight months, I find myself longing to get out and travel. Since that’s not going to happen in reality, I’ve opted to take an imaginary trip to visit some places I’ve enjoyed in the past, and the vehicle for this is a simple demo of the new spatial data functionality in CockroachDB v20.2 . Spatial Data Demo The highly available spatial data demo is a single page web app which, when the page loads, shows a “tourist” on a map in a location selected randomly from a small set, surrounded by the nearest pubs, cafes, restaurants, or bars (the type of amenity is also randomly selected). As you pan around the map, the nearby amenities are updated. When any of these is clicked, its name is displayed, along with its distance, in meters, from where the tourist is. The ingredients for this spatial data demo are: Leaflet , Python, Flask , Mapbox , the CockroachDB K8s operator , and data from OpenStreetMap . Because CockroachDB provides highly available map data, there’s no need to worry about stranding the tourist the way that I was stranded 2 years ago when my map application stopped working. The video below describes all of this in more detail, and the demo’s GitHub repository contains a detailed README along with everything necessary to reproduce the demo (except for infrastructure and data, though a link is provided to the data set).  I hope you enjoy the video ! Spatial Data Demo Resources Data set: https://storage.googleapis.com/crl-goddard-gis/osm_475k_eu.txt.gz GitHub repository: https://github.com/cockroachlabs-field/crdb-geo-tourist Spatial Data Reference Material https://www.cockroachlabs.com/docs/v20.2/spatial-data.html https://www.cockroachlabs.com/docs/v20.2/spatial-indexes https://leafletjs.com/examples/quick-start/ https://leafletjs.com/examples/custom-icons/ https://wiki.openstreetmap.org/wiki/Category:Amenity_icons", "date": "2020-12-02"},
{"website": "CockroachLabs", "title": "Why CockroachDB and PostgreSQL Are Compatible", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/why-postgres/", "abstract": "CockroachDB is built to be largely compatible with PostgreSQL , meaning that software written to use PostgreSQL can often be used with CockroachDB without changes. Why is CockroachDB compatible with PostgreSQL? Perhaps surprisingly, at the time of writing (early May 2018), the answer to this question had not been documented publicly on the CockroachDB web site, nor in the CockroachDB docs, nor in CockroachDB-related articles in third party sources. I was lucky to sit next to Cockroach Labs Chief Architect and Co-Founder Ben Darnell when Lakshmi Kannan, now the general manager of CockroachCloud , asked us the very same question: Why was CockroachDB designed to be compatible with PostgreSQL? What follows is an extended rewording of the resulting discussion. Disclaimer: this is a personal recollection written without notes and after a week had passed. Any inaccuracy in the ideas, arguments, timelines, statements, facts or opinions recollected here is entirely mine. Why is CockroachDB compatible with anything at all? CockroachDB could very well have been created with its own SQL dialect and/or network protocol. This is the way RethinkDB did it. Although Google's Spanner 's SQL dialect is inspired by MySQL , it is not fully compatible with it either. Compatibility with anything is a choice, it is not inherently necessary when building a SQL RDBMS . Avoiding compatibility can even be a way to commercial success, by enabling vendor lock-in . The primary reason we chose PostgreSQL compatibility was to make sure our users would not have to learn too many new things just to use CockroachDB. When designing a custom network protocol, the database provider must also provide client drivers. Considering the multitude of platform and programming languages in use today, a custom protocol yields a hard choice: either focus on a few platform/language combinations and only provide drivers for that, or divert a lot of engineering resources on client drivers. The former choice yields a smaller potential user base. The latter choice takes resources away from the implementation of a better RDBMS. By adopting an established network protocol, especially one for which drivers exist on many platforms and for many programming languages, a new RDBMS makes itself immediately available to a larger ecosystem of existing client code. This is why CockroachDB chose to be compatible with Postres, an established RDBMS. There is one more thing to be said here about multiple layers of compatibility. I will come back to this in a moment. How does a product like CockroachDB choose which systems to be compatible with? There are plenty of well-known and widely used RDBMSs out there. After deciding a new project should be compatible with anything, how does one choose what to be compatible with? Here, two first-order constraints came into play for CockroachDB. The first is that CockroachDB was open source from the get-go. It was thus essential to choose compatibility with other RDBMSs that have open source drivers and applications. More specifically, it was legally necessary to choose compatibility with other RDBMSs whose open source drivers are legally allowed to connect to another product than what they were designed for. An open source driver for MSSQL, for example, is no good, because the MSSQL network protocol is likely patent-encumbered and no \"MSSQL compatible\" database can be built without onerous agreements with Microsoft (if at all). Substitute \"MSSQL\" with \"DB2\" or \"Oracle\" and the argument remains. The second constraint is that CockroachDB aimed to appeal to open source developers. What do open source developers like? Open source client drivers are one thing, but the ecosystem around the technology matters much more. There must exist welcoming communities, public discussion forums, a free market of third party software, free and open learning resources, etc. In contrast, many proprietary RDBMSs restrict discussion around their tech to semi-private forums. They often have complex third party publishing restrictions. Training to learn and use the technology is usually restricted and expensive. So for CockroachDB, proprietary RDBMS were out of the compatibility story from the get-go. What remains? CockroachDB looked at: long-established projects for which the client ecosystem is already mature, again in the interest of reducing the amount of work needed to spend on client code. widely-used projects to maximize the potential user base. open source, so that their implementation could be freely inspected, to simplify the implementation of compatibility. Looking for long-established and widely-used open source RDBMS that are commonly known to serve as compatibility anchor for Distributed SQL (or NewSQL) databases , the search narrows down to just two: MySQL and PostgreSQL. Why isn't CockroachDB compatible with MySQL? Truth be told, as Ben Darnell recollected, MySQL was an appealing choice initially. Google's Spanner, developed with similar ideas as CockroachDB and with a similar audience in mind, aimed for some amount of MySQL compatibility. Other recent Distributed SQL RDBMSs, like MemSQL , opted for MySQL compatibility, too. Ben Darnell, like the rest of the early team at Cockroach Labs, even had more personal experience with MySQL than with PostgreSQL. So what happened? Initially, CockroachDB toyed with the idea of compatibility with MySQL. What tipped the balance in PostgreSQL's favor was a combination of multiple factors. There was initially a clear impression that PostgreSQL's documentation of its network protocol was clearer, more detailed and overall more supportive of a third party implementation than MySQL's documentation of its own protocol . Meanwhile, the PostgreSQL License is compatible with CockroachDB's own Apache License , which enables reuse of (some of) PostgreSQL's own source code in CockroachDB unchanged. In contrast, MySQL (and its successor MariaDB ) is released under the GNU GPL , which prevents direct reuse of MySQL code in CockroachDB. Also, it became clear that MySQL had grown organically over time in a somewhat less principled manner than PostgreSQL. Throughout the documentation and source code, MySQL seemed to have more exceptions and special cases to care about. Proper SQL transactions came much later to MySQL than to PostgreSQL, and there remains significant cruft in the MySQL documentation and the MySQL ecosystem as a fallout of weak transaction isolation. As I understood the argument, the CockroachDB team did not like the perspective of sharing the same culture and ecosystem as one that usually only sees transaction isolation as a complicated, opt-in, \"advanced\" feature. These factors alone caused CockroachDB to slowly but surely focus on PostgreSQL compatibility. But as time went on, some other aspects came up and confirmed this was a good choice. More on this below. How compatible is CockroachDB with PostgreSQL? Today, CockroachDB supports PostgreSQL's network protocol, called \" pgwire \" in the CockroachDB source code, very well. It also supports most of PostgreSQL's SQL syntax, by virtue of being able to reuse PostgreSQL's syntax parser virtually as-is (with extensions). The compatibility story is however currently more murky at the level of the dialect's semantics, simply because there is much more work to achieve adequate compatibility at that level. Protocols for communication between things on a network are typically layered . On the Internet, for example, the IP layer is at a lower level, TCP higher than IP (IP can function independently from TCP, and TCP is built upon it), and HTTP is higher than TCP (TCP can function independently from HTTP, and HTTP is built upon it). For RDBMSs the communication is organized like this: the SQL network protocol is at a similar abstraction level as TCP or HTTP on the Internet. the SQL syntax is at one level above that. the SQL dialect semantics is at one level even higher. SQL RDBMSs do not typically explain their communication protocols like this in their docs, but application developers see this structure clearly in their code: when a driver can connect successfully to a database (the network protocol) there is still work to do, because they can still get errors if the app sends invalid SQL punctuation (the SQL syntax). Even when the punctuation is fine, there can still be errors when the app uses SQL functions not currently supported on the server (the SQL semantics). These practical stumbles during the development process of apps are the visible artifacts of a layered communication protocol. What happened regarding compatibility with the PostgreSQL dialect? What are the plans? Initially (back in 2015 and 2016), the idea was that CockroachDB would be compatible with PostgreSQL's network protocol, so as to enable reuse of client drivers, but that it would provide its own, potentially PostgreSQL-incompatible SQL syntax and/or dialect semantics. In particular, it was envisioned that CockroachDB would provide sufficiently many specific SQL extensions that a custom SQL dialect would be necessary (or at least warranted). And so early (non-released) versions of CockroachDB had a hybrid MySQL/Spanner/PostgreSQL dialect, available to clients over the PostgreSQL network protocol. The assumption underlying this strategy was that users would want to reuse PostgreSQL drivers (which talk the network protocol) but would accept using CockroachDB-specific SQL queries with their drivers in exchange for CockroachDB-specific benefits. As the team learned the hard way in the ramp-up to CockroachDB 1.0, many developers in the ecosystems that CockroachDB wants to enter do not write their own SQL queries any more —as opposed to, e.g., ten or twenty years ago. By early 2016, it became clear that the CockroachDB team had to do more than just make client drivers compatible to stimulate adoption; it also had to make the higher level frameworks compatible. This in turn meant that the initial idea to restrict compatibility to the network protocol was insufficient, so the PostgreSQL compatibility mandate was extended throughout CockroachDB's entire SQL layer. That, or invest engineering work to extend existing PostgreSQL frameworks to make them work with CockroachDB. This is now well-understood and, in fact, both directions are being heavily invested in. The priority is now to increase compatibility with PostgreSQL's semantics out-of-the-box, for example by providing more and more of PostgreSQL's built-in functions and operators, and by exposing more compatible data via the information_schema And pg_catalog introspection tables. This work is ongoing and future versions of CockroachDB aim to become increasingly more compatible in this way. Meanwhile, there exist a few features where CockroachDB will likely never be fully compatible with PostgreSQL due to a fundamental difference in database architecture. For example, [PostgreSQL's system columns (e.g. xmin/xmax)](https://www.postgresql.org/docs/13/ddl-system-columns.html) cannot be implemented efficiently in CockroachDB at all, because CockroachDB's transaction model is so different. When existing client frameworks for PostgreSQL require such features, Cockroach Labs will instead invest into providing custom CockroachDB-specific versions of the framework, either by building them in-house or supporting third parties to do the work. As CockroachDB becomes more popular, the developers of the frameworks might even choose to implement CockroachDB-specific alternatives themselves. For the most up-to-date information about CockroachDB's compatibility with PostgreSQL go to this docs page . In retrospect, was Postgres compatibility the right choice? As it turns out, making CockroachDB compatible with PostgreSQL is actually a lot of work. Would have it been any different if CockroachDB had chosen another route instead? The first alternative, to provide a custom network protocol and SQL dialect, forcing the creation of a new application ecosystem from scratch, would not have been the right option. CockroachDB aims to create a huge community of users. Without compatibility with an established software ecosystem, there is a huge bootstrapping problem which the team had no experience in solving. Furthermore, adopting compatibility has a huge benefit on the development process: it removes open questions from the design discussions. Open questions in the client/server interface of a networked software are a huge time sink and usually becomes source of costly disagreements between sub-teams or long-winded design iterations. By adopting compatibility with something, anything really, the CockroachDB team was able to utilize a definite source of answers for a large number of design decisions. This has accelerated the development by focusing efforts on more important matters, for example resilience and operational simplicity. Another alternative route would have been to prioritize early compatibility with MySQL instead. The MySQL protocol and dialect, albeit less well documented and less principled as discussed above, is arguably simpler to implement—especially for the semantics aspects beyond the network protocol. MySQL has fewer SQL data types, fewer built-in functions, fewer introspection facilities, fewer configuration options. All this could have translated to less work for CockroachDB. At this time, the CockroachDB team is still pretty comfortable with the choice of prioritizing PostgreSQL compatibility first. The arguments discussed above still hold, especially the code license and the programmer culture. In addition, over time, several other aspects in favor of PostgreSQL became clear. For one, the acquisition of MySQL AB, the company behind MySQL, by Sun Microsystems (now Oracle) in 2008 has fragmented the MySQL ecosystem. MariaDB, while initially fully compatible, now provides its own feature set. There is no unified steering governance body behind MySQL's future any more. PostgreSQL can be considered more future-proof in that regard. Also, Oracle now owns the bulk of MySQL's intellectual property, and a company building a product too closely related to MySQL and appealing to a similar enterprise audience might land a bit too close to Oracle's anti-competition radar for comfort. However, the MySQL community is massive, and many are showing interest in porting applications to CockroachDB. For that reason, the CockroachDB team is actively developing MySQL migration tools, which are available in the latest alpha release . Separately, PostgreSQL's reference documentation is extremely clean, well-written, well-presented and developer-friendly, moreso than MySQL's. It is an effective complement to CockroachDB's own documentation, and users have reported that they are happy to use both together. Finally, as years go by the specific compatibility choice ends up mattering less and less. By the time a project becomes well-known and enough users have adopted it in their tech stack, (social) network effects and the quality of provided support resources largely overshadow the initial ramp-up benefits of (tech) network or dialect compatibility. Hopefully, CockroachDB will find itself in this position soon enough. So yes, in retrospect, probably still the right choice. If you have additional questions about our PostgreSQL compatibility please join the CockroachDB Community Slack channel to chat with CockroachDB users and engineers. Copyright © 2018 Raphael ‘kena’ Poss. Permission is granted to distribute, reuse and modify this document according to the terms of the Creative Commons Attribution-ShareAlike 4.0 International License. This work, “Why PostgreSQL in CockroachDB” by Raphael ‘kena’ Poss, is a derivative of “ The PostgreSQL in CockroachDB - Why? ” used under CC BY SA . To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/ .", "date": "2020-12-01"},
{"website": "CockroachLabs", "title": "What is VPC Peering and When Should You Use It?", "author": ["Tommy Truongchau"], "link": "https://www.cockroachlabs.com/blog/what-is-vpc-peering/", "abstract": "If you’re building and managing applications in public cloud providers like GCP or AWS, chances are you’ve heard of VPC peering. This blog post explains what VPC peering is, why you’d want to use it, and, if you’re using CockroachCloud today, how you can get started with our new VPC peering functionality. What is a VPC and what is VPC peering? First thing’s first - a virtual private cloud (VPC) is a logically isolated, virtual network within a cloud provider. A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IP addresses. VPC peering allows you to deploy cloud resources in a virtual network that you have defined. Instances in either VPC can communicate with each other as if they were within the same network. Data can be transferred across these resources with more security. What are the benefits of using VPC peering? Improve security. VPC peering comes with the major benefit of improving security by enabling private connectivity between two or more VPC networks, isolating traffic from the public Internet. Because your traffic never leaves the cloud provider’s network, you reduce a whole class of risks for your stack. Save money on network costs. With VPC peering, you save on network transit costs and benefit from improved network latency. Because peering traffic does not leave your cloud provider’s network, that reduces public IP latency. And since peered networks use internal IPs to communicate, transferring data over the cloud provider’s network is cheaper than over the public Internet. Get more flexibility for services that don’t need to connect to the Internet. Another reason to use VPC peering is when your instances do not require a public IP address or a network address translation (NAT) configuration to the public Internet. This can be desirable for backend services, where a user wants to block all egress traffic to the public Internet from their instances. VPC peering in CockroachCloud Since releasing CockroachCloud, our database-as-a-service , a year ago, we’ve continued to add new features and functionality. With the most recent update, CockroachCloud now provides you with the ability to directly peer your GCP VPCs with your CockroachCloud cluster’s VPC. Before, CockroachCloud required you to authorize networks that could access the cluster, typically including your application server’s network in a production environment, as well as your local machine’s network in a development environment. This process was clunky and also had some limitations when running applications in Kubernetes. Now, VPC peering gives you a faster and smoother user experience. You can sidestep the old requirement of allowlisting IP addresses within CockroachCloud before you’re able to connect (though that IP allowlist option is always available if the need arises). Follow these three steps to set up VPC peering in GCP: At the moment of creating a cluster, you can specify an IP address range (in CIDR notation) for your CockroachCloud cluster’s network. That range should not overlap with the IP ranges in your application’s network. Once your cluster is configured, you kickoff the VPC peering request by adding your GCP project ID and GCP VPC network name. CockroachCloud will provide you a handy gcloud command line to run which will allow GCP to accept the new VPC peering request. What about VPC peering in AWS? If you are running on AWS, hang tight. The ability to securely connect AWS VPCs with CockroachCloud via AWS PrivateLink is coming soon. Learn more about VPC peering in CockroachCloud With VPC peering, you can securely connect your GCP applications with your CockroachCloud clusters. You can learn more in our VPC peering documentation and sign up here to get started. And if you have any questions, feel free to swing by our community Slack Channel . We’re excited to see what you build with CockroachCloud!", "date": "2020-12-11"},
{"website": "CockroachLabs", "title": "CockroachDB 20.2 Performs 40% Better on TPC-C Benchmark, Passes 140k Warehouses", "author": ["Aayush Shah", "Vy Ton", "Aditya-Maru"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-performance-20-2/", "abstract": "One of the main reasons our customers choose CockroachDB is the easy horizontal scalability it offers, while maintaining data consistency with serializable isolation. This combination lets customers run critical OLTP workloads, like financial ledgers and e-commerce shopping carts, at large scale without the hassle of legacy sharding.  With every release, we make significant investments in improving CockroachDB’s performance and scale. We measure CockroachDB’s performance through many diverse tests, including the industry-standard TPC-C benchmark to track our progress across releases. Our latest version, CockroachDB 20.2, passed 140K warehouses with a maximum throughput of 1.7M transactions per minute (tpmC) on TPC-C. This represents a 40% improvement with the same resources as compared to the results previously reported with CockroachDB 19.2 in this post . Additionally, CockroachDB 20.2 was able to load TPC-C 140K in less than 3 hours compared to the ~20 hours it took to load TPC-C 100K in 19.2. This improvement was the result of faster bulk-data loading , which built upon work in Pebble, CockroachDB’s new storage engine . If this were an official TPC-C run, it would be the 20th largest run on the TPC-C leaderboard . Not only has CockroachDB 20.2 reached a higher maximum warehouse on TPC-C than in any previous release, but CockroachDB is also more efficient with the same cluster resources for workloads of varying sizes. This allows our customers to handle larger workloads on their clusters, saving costs. What is the TPC-C benchmark? TPC-C is the industry-standard OLTP benchmark which simulates an e-commerce or retail company. Created in 1992, TPC-C has withstood the test of time and remains the most mature industry benchmark for OLTP workloads, and the only objective comparison for evaluating OLTP performance. In its own words, TPC-C: “…involves a mix of five concurrent transactions of different types and complexity either executed on-line or queued for deferred execution. The database is comprised of nine types of tables with a wide range of record and population sizes. While the benchmark portrays the activity of a wholesale supplier, TPC-C is not limited to the activity of any particular business segment, but, rather represents any industry that must manage, sell, or distribute a product or service.” As a result, TPC-C includes create, read, update, and delete (e.g., CRUD) queries, basic joins, and other SQL statements used to administer mission-critical transactional workloads. It includes detailed specifications for concurrency and workload contention. TPC-C 140,000: CockroachDB 20.2 creates greater efficiency with the same cluster size On commodity AWS hardware, CockroachDB 20.2 allows you to do more with the same cluster resources as is evident by the higher max warehouses metric that TPC-C measures. In 20.2, more performant foreign key checks and lookup joins contribute to greater efficiency. For foreign keys, indexes are no longer required on the referencing column . Therefore, writes during the TPC-C workload won’t incur the performance overhead of updating indexes every time. Reproduce these results We’ve updated our performance page with the steps to reproduce these results. You can visit this page to learn more about how CockroachDB provides predictable scaling, throughput, and latency. If you fail to achieve similar performance profiles, there is likely a problem in either the hardware, workload, or test design. TPC-C is one of several benchmarks that we continuously run. Looking forward, these benchmarks help us identify areas for improving CockroachDB in order to provide great performance all while delivering resilience and data consistency.", "date": "2020-11-19"},
{"website": "CockroachLabs", "title": "Why a Major Cable Co. Switched from Amazon Aurora to CockroachDB", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/the-multi-region-cockroachdb-deployment-powering-a-telecom-providers-24-7-support-platform/", "abstract": "With millions of customers to serve, a major cable company needed to build a virtual customer support agent to scale their customer requests. The app had to provide 24/7 help to users across the United States, and store metadata about customer conversations. The first version of the application was built on Amazon Aurora, in a single cloud region on the east coast of the United States. However, this deployment was vulnerable to failures, and when a networking failure in an AWS region knocked the entire service offline, the team realized that Aurora’s single-master architecture was not sufficient to attain the always-on customer experience they wanted. They needed to explore other options in hopes of migrating the app. >Here’s what they were looking for in a new database: Always-on availability, because a 24/7 virtual agent can’t have downtime, even in the event of a failure. Strong consistency, meaning t he data should always be up to date, no matter what machine or data center handles a request Low latency reads, from anywhere in the US They turned to CockroachDB for its strong resiliency, consistency, and low-latency reads. The end result is a multi-region deployment that can survive datacenter failures: Head to the full case study for a graphic of the implementation pattern they’re using today, complete with how they’ve configured their cluster to ensure both local reads and ultimate survivability.", "date": "2020-12-22"},
{"website": "CockroachLabs", "title": "Cockroach Labs Raises $160M on $2B Valuation, Reflecting Explosive Growth and Product Innovation", "author": ["Peter Guagenti"], "link": "https://www.cockroachlabs.com/blog/explosive-growth-2021/", "abstract": "We are proud to announce a new round of funding at a milestone valuation, which recognizes our company’s rapid growth, happy customers, and our emerging role as a leading cloud-native SQL database. The events of 2020 delivered more change in the past few months for businesses than many of us have experienced in the last 10 years. The impact of stay-at-home orders forced organizations to rapidly adapt to changing consumer behaviors, remote workforces, and explosive growth of digital services. As companies rushed to evolve their businesses and adapt their applications to support the rapid shift to digital-everything, many discovered that they needed more resilient, more easily scalable, and more flexible data infrastructure. These circumstances led many companies – from start-ups seeing massive numbers of new users , to traditional businesses needing to change how they worked – to choose CockroachDB. It’s on the back of this exceptional growth that we are now proud to announce our latest round of funding; a series E that raises $160 million at a valuation of $2 billion. This latest financing was led by Altimeter Capital with participation from new investors Greenoaks and Lone Pine, and existing investors Benchmark , BOND , FirstMark , GV , Index Ventures , and Tiger Global . In the past year we have more than doubled our revenues and customer count, with more than half of our customers choosing to run on our recently-released cloud database-as-a-service, CockroachCloud . We have also seen significant growth of our developer community, reaching more than 19,000 Github stars and over 300 external contributors . Cockroach Labs has also seen validation of its business through awards, including induction into the JPMorgan Chase Hall of Innovation and being named as Crain’s #1 Best Places to Work in New York City for large companies. More important than any of the accolades are how we are helping you, our customers and open source users. Teams around the globe are using CockroachDB to build scalable, resilient, and innovative apps and services that they simply could not build before. For example, we have a digital banking customer using CockroachDB to keep pace with its rapid growth of 13M+ users with a reliable, always-on infrastructure; a gaming customer using CockroachDB to achieve low latencies so that their users can play in-real-time from anywhere in the world; and a global retail customer using CockroachDB to comply with local data regulations and optimize inventory/order management. We now see our software in use across every major industry, in companies based all around the world, and supporting everyone from the smallest individual users to the category leaders in retail, banking, and streaming media. What we will do with the funds So what does this mean for the future of Cockroach Labs and our customers? We are on a long-term mission to be the database of choice for your most important applications, and we are committed to creating a company that you would be proud to do business with. With this latest round, we will continue to double-down on product development. We created CockroachDB to be the most highly evolved, cloud-native, distributed SQL database on the planet. Our promise to you is to make scale so simple, you don’t have to think about it. To make data so resilient, it becomes impossible to destroy. And to make deployment so seamless, your apps effortlessly run anywhere in the world. Significant engineering effort and innovation is required to continually deliver on that promise, and this funding supports our ongoing efforts. Cockroach Labs’ latest innovation, a forever-free version of CockroachCloud for development and education, will soon be released in beta. Sign up here to be the first to try it . Join our team In addition, this financing allows us to expand our staff across every department to best support our expanding, global customer base. We encourage you to consider joining us on our mission to build the next great database company. Explore open roles at cockroachlabs.com/careers and hear directly from the folks building CockroachDB on Youtube . This significant milestone for our company would not be possible without the support from our customers, our partners, and our open source community. Thank you all for believing in Cockroach Labs, and supporting us through this journey.", "date": "2021-01-12"},
{"website": "CockroachLabs", "title": "Bose is Building Databases on Demand with Elastic, CockroachDB, and Kubernetes", "author": ["Charlotte Dillon", "Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/bose-elastic-cockroach-meetup/", "abstract": "Back in the bygone era of meeting in person, shaking hands, and clapping enthusiastically for the brave souls who are willing to stand in front of their peers to do live demos, Elasticsearch held a meetup in Boston at Bose headquarters. Chris Chambers, Cloud Engineer at Bose, spoke to the crowd about how his team built a platform as a service using a number of open source tools, including CockroachDB, Elasticsearch, and Kubernetes . Bose really requires no introduction but it bears mentioning that Chris works on a specific team within Bose called Galapagos. The mission of Galapagos is to bring Bose’s software into the 21st century - In the past, Bose was focused on embedded software (software that runs on speakers and headphones) and now that they’ve mastered the art, they have started to modernize so that they can bring cloud-connected experiences to all our devices. Data Stores for Microservice Architecture Adoption of microservices architecture has led to a sprawl of databases, Chris explained. And microservice developers need to be able to quickly, repeatedly spin up data stores behind those services–something a number of Meetup attendees had experienced in their professional lives or side projects. To ease that pain, Bose wanted to extend Kubernetes to allow any software development team to get easy access to Elasticsearch, CockroachDB and other popular data stores with a powerful, cost-effective, scalable, reliable solution. Here is a short video in which Dylan O’Mahoney, a Principal Cloud Architect from Bose, explains their use case with CockroachDB: Modern Application Tech Stack Bose has been using CockroachDB to power their cloud-connected platform unit for two years now, in large part due to its resiliency and horizontal scalability. But CockroachDB is just one ingredient of this modernization project. The full tech stack includes Elasticsearch, Kafka/MongoDB/Postgres for when they need a low availability version of CockroachDB. And Redis for when they need to share memory with other applications. Underneath all that is Kubernetes, which hosts essentially all the compute that we do. They’re using the service catalog which is a kind of abstraction layer that allows external services or really any service to surface into Kubernetes in the same way that a pod or a deployment or a Statefulset would, but without exposing those details. So behind a service catalog instance, there might be an AWS RDS instance or a CockroachDB instance, running as a StatefulSet in the same Kubernetes cluster or in a different Kubernetes cluster, or even across clouds. Why build a database on demand? Essentially what Galapagos is trying to do at Bose is to create low friction ways for developers to work, and to use things which they may not be completely familiar with, in a way that’s quick and easy for them. Galapagos wanted this to be self-service, so it should be possible for teams to stand up a database in just a few minutes, without any help from DevOps. No developer time is wasted while they read along in the Wiki for instructions. And the process is declarative so that developers can repeat deployments in any environment, whether they’re doing a feature branch to quickly test something, or if they’re preparing to go to production. The declaration about what kind of database they need should be the same, and only the scale and other kinds of parameters should change. And that should all live right next to your application so that the code that uses a database is versioned alongside the declaration about what kind of database and the parameters that you use. If Galapagos project at Bose is relevant to your own work and you want to learn more about the tech stack and how you can implement some of the same practices you can check out our webinar with Chris on the same subject .", "date": "2021-01-13"},
{"website": "CockroachLabs", "title": "How We Built Scalable Spatial Data & Spatial Indexing in CockroachDB", "author": ["Sumeer Bhola"], "link": "https://www.cockroachlabs.com/blog/how-we-built-spatial-indexing/", "abstract": "Support for spatial data and spatial indexing is one of the most requested features in the history of CockroachDB. The first issue requesting spatial data in CockroachDB was opened in October 2017, and closed on November 12, 2020 with the release of spatial data storage in CockroachDB 20.2 . Spatial data, sometimes called geospatial data, is data that contains information about geographic (and geometric) features, with PostGIS being one of the most popular spatial data extensions in use. CockroachDB’s spatial data storage and processing features are compatible with PostGIS, while also providing the scale and resilience of CockroachDB. This blog post discusses how we built spatial indexing in a horizontally scalable, dynamically sharded database. It also covers why simply using PostGIS on top of CockroachDB was not an option, since the R-tree indexing that PostGIS relies on is not compatible with how CockroachDB achieves dynamic horizontal scaling. Background: Two Common Approaches for Spatial Indexes Current approaches to spatial indexes fall into two categories. One approach is to “divide the objects”. This works by inserting the objects into a balanced tree whose structure depends on the data being indexed. The other approach is to “divide the space”. This works by creating a decomposition of the space being indexed into buckets of various sizes. In either approach, when an object/shape is indexed, a “covering” shape(s) (e.g. a bounding box) is constructed that completely encompasses the indexed object. Index queries work by looking for containment/intersection between the covering shape(s) for the query object/shape and the indexed covering shapes. This retrieves false positives but no false negatives. For example, the following diagram shows three shapes A, B, C with the corresponding covering shapes in dotted lines. The coverings of A and B intersect with each other, but don’t intersect with the covering of C. An intersection computed using the index will produce a false positive that A and B intersect, which will be eliminated by exact intersection evaluation over the actual shapes. Spatial Index Approach #1: Divide the Objects PostGIS is a notable implementation of divide the objects. It maintains an “R tree” (rectangle tree) which is implemented as a Postgres “GiST” index. The covering shape used by PostGIS is a “bounding box” which is the minimal rectangle that encompasses the indexed shape. The following shows an example of an R tree where the red (solid) rectangles show the bounding boxes, and are the leaf nodes in the corresponding tree. The blue (dashed) rectangles are the intermediate nodes, and contain all the bounding boxes in their sub-tree. A search starting from the root can omit sub-trees that have no overlap. For example, consider a search for shapes that intersect with the yellow triangle labeled B, shown with its bounding box. The search at the root will omit R2. Then at the child, it will omit R3, and explore both R4 and R5. When exploring R5, neither R13, R14 are relevant, while in R4, only R11 is relevant. [ Image Credit: Radim Baca and Skinkie from Wikipedia ] Spatial Index Approach #2: Divide the Space The other  approach for spatial indexes is to “divide the space” into a quad-tree (or a set of quad-trees) with a set number of levels and a data-independent shape. Each node in the quad-tree (a “cell”) represents some part of the indexed space and is divided once horizontally and once vertically to produce 4 children in the next level. Each node in the quadtree has a unique numeric ID. Divide the space algorithms tend to use clever strategies for the unique numeric cell-IDs with important guarantees: The cell-IDs of all ancestors of a cell are enumerable. The cell-IDs of all descendants of a cell are a range query. The cells of nearby cell-IDs are spatially near. The S2 library from Google is an example of this approach for dividing the earth and assigns cell-IDs using a Hilbert curve . The following shows points, depicted as small circles, being indexed in a quad-tree , where the squares of various sizes are the quad-tree cells. [Image Credit: David Eppstein - self-made; originally for a talk at the 21st ACM Symp. on Computational Geometry, Pisa, June 2005 ] When indexing an object, a covering is computed, often using some number of the predefined cells. Ancestors and descendants can be retrieved by using the cell-ID properties above. The number of covering cells can vary per indexed object. There is an important tradeoff in the number of cells used to represent an object in the index: fewer cells use less space but create a looser covering. A looser covering retrieves more false positives from the index, which is expensive because the exact answer computation that’s run after the index query is expensive. However, the benefits of retrieving fewer false positives can be outweighed by how long it takes to scan a large index. The following shows both 4 and 8 cell coverings of the city of Paris constructed using the S2 library. Users can construct such a visualization using CockroachDB’s ST_S2Covering function and geojson.io as described here . Because the space is divided beforehand, it must have finite bounds. This means that Divide the Space works for GEOGRAPHY (spherical/spheroid geometry), and for GEOMETRY (planar geometry) when the part of the plane that will be used is bounded. We discuss later how we configure the bounds for GEOMETRY, and how we handle the case of a shape exceeding the bounds. Why We chose the “Divide the Space” Approach for CockroachDB CockroachDB is a dynamically sharded, horizontally scalable SQL database, that arranges all data into a lexicographic total order . As the workload increases, tables and indexes are split into ranges, consistent with this order, and ranges are moved between nodes to balance the load. Ranges can later be merged when the load decreases. The activities relating to range splitting/merging/rebalancing are localized to the affected ranges and do not affect other ranges, which is key to achieving horizontal scaling. A divide the objects approach is incompatible with this scheme since: The shape of an intermediate node is dependent on many of the indexed shapes, which creates a non-localized dependency. A general multi-dimensional structure cannot be directly represented in a lexicographic totally ordered space. For these reasons, CockroachDB uses a divide the space approach, leveraging the S2 library. The totally ordered cell-IDs are easily representable in CockroachDB’s lexicographic total order. This choice comes with some additional advantages: (a) bulk ingestion becomes simple, and (b) compactions in our log-structured merge tree ( LSM tree ) approach to organizing storage can proceed in a streaming manner across the input files, which minimizes memory consumption. In contrast, BKD trees , which are a divide the objects approach, also permit a log-structured storage organization, but, to the best of our understanding, compactions need to load all the input files to redivide the objects. Index Representation and Querying Spatial data is indexed as an inverted index that contains the cell-ID and the primary key of the table. In the example below, the primary key is the city name and shows the city from our earlier example indexed using 4 cell-IDs, where the cell-IDs are computed using the S2 library for cell covering and ID assignment. Cell ID City 5180842921145925632 Paris &mldr; &mldr; 5180843007045271552 Paris &mldr; &mldr; 5180843041405009920 Paris &mldr; &mldr; 5180843041405009920 Paris Note that a particular cell-ID can be used in the covering for multiple table rows, and each table row can have multiple cell-IDs in the covering, that is, it is a many-to-many relationship. Queries as Expression Evaluation over Sets Now we come to the most interesting part – how to evaluate queries using such an inverted index. Consider a query that is trying to compute what indexed shapes contain a given shape, or more generally trying to join two tables based on this containment relationship. For example, if we had two tables with cities and parks and their corresponding geometries, one could do the following to pair each park with the city it is in. SELECT parks.name, cities.name FROM parks JOIN cities ON ST_Contains(cities.geom, parks.geom) We can reduce this problem to: given an actual shape g, find the indexed shapes that contain g. In the join case, g will successively take on the value of each geometry on one side of the join. The following abstract example illustrates how this is evaluated using an index. In this example c[i] is a cell number. Consider g has the cell covering c[213], c[61], c[64] in a quad-tree rooted at c[0]. In the numbering here, the children of cell c[i] are numbered c[4*i+1]&mldr;c[4*i+4] (we are not using a Hilbert curve for numbering, for ease of exposition). The following depicts this covering as a tree with the leaf cells being the covering cells. Note that the paths from the leaves to the root are not the same length. All shapes containing g must have coverings that contain c[213], c[61], and c[64]. Assume a notation index(c), where c is a cell, that returns all the shapes that have an entry in the index for cell c. The indexed shapes that would satisfy the containment function are: (index(c[213]) ⋃ index(c[53]) ⋃ index(c[13]) ⋃ index(c[3]) ⋃ index(c[0])) ⋂\n(index(c61) ⋃ index(c15) ⋃ index(c3) ⋃ index(c0)) ⋂\n(index(c64) ⋃ index(c15) ⋃ index(c3) ⋃ index(c0)) One can factor out common subexpressions in the above, which increases efficiency of evaluation. To perform such set expression evaluation, we have developed new distributed query processors, which we discuss in the next section. New Distributed Query Processors We introduced two new distributed query processors, inverted filterer and inverted joiner, that apply to the spatial SELECT and JOIN queries. These can evaluate general set expressions derived from the expressions being evaluated. Both these operators can be distributed, for scalable evaluation. The sets are represented as ranges of cells to scan from the inverted index. These processors produce false positives because the coverings are not a precise representation of the original shape. These false positives are subsequently eliminated using a lookup join that retrieves the original shape and does a precise evaluation of the spatial expression. The use of these new processors, and the subsequent lookup join, is automatically planned by our cost-based query optimizer . The cost-based optimizer uses histograms over the cell-IDs to decide when to use the inverted filterer. The inverted join is currently planned using a heuristic instead of a cost-based approach. Over 25 of the spatial functions and function variants listed here are accelerated using spatial indexes. Complex expressions which include spatial and non-spatial functions can also be accelerated using spatial indexes. The following is the EXPLAIN output for our earlier query, which shows both the inverted join and the subsequent lookup join. > EXPLAIN SELECT parks.name, cities.name\nFROM cities JOIN parks\nON ST_Contains(cities.geom, parks.geom);\ntree        |         field         |       description\n---------------------+-----------------------+--------------------------\n                     | distribution          | full\n   lookup join       |                       |\n   │                 | table                 | parks@primary\n   │                 | equality              | (name) = (name)\n   │                 | equality cols are key |\n   │                 | pred                  | st_contains(geom, geom)\n   └── inverted join |                       |\n   │                 | table                 | parks@geom_index\n   └── scan          |                       |\n                     | table                 | cities@primary\n                     | spans                 | FULL SCAN Making queries fast for Real World Data So far we’ve considered the algorithmic aspects of spatial indexing. The real world data that a spatial database deals with can bring more challenges. Quality of Cell Covering Good cell coverings are important for performance. We faced two problems with the quality of cell coverings. First, polygons with line segments that are near collinear can confuse the covering generator to produce very wide coverings, for example a whole earth covering for the polygon of a city neighborhood. We’ve solved this with a heuristic that recognizes such poor coverings, and falls back to using the bounding box of the shape to generate the covering. Second, for shapes that are close to a rectangle, cell coverings can be worse than a “divide the objects” index that uses bounding boxes. Our earlier example with Paris is an unusually extreme representation of this problem, where the 4 cell covering is over 5x the area of the original shape. Note that there are also many shapes where a cell covering can be better than a bounding box. We’ve solved this by developing a scheme that gets us close to the best of both approaches. In our scheme, the inverted index stores both the cell-ID and the bounding box of the original shape. Before using a cell as part of the set expression computation, we use the bounding box to do a fast check of whether the original shape can satisfy the expression, and if not, ignore the key stored with the cell-ID. For certain workloads we’ve observed 3x reduction in false positives with this scheme. Indexing Bounded Space We earlier discussed how “divide the space” requires the bounded space to be specified up front. This is not a problem for the spherical/spheroid geometry used for the GEOGRAPHY type. However, this can be a challenge for the GEOMETRY type. We’ve adopted a two-pronged approach to maximize usability and performance for our users: Ease of Use When the GEOMETRY column being indexed has a known SRID that corresponds to an earth projection, we automatically infer the finite space for the index. CockroachDB understands over 6000 SRIDs, including all EPSG supported SRIDs. When the SRID is not known, we use reasonable defaults to reduce the probability of a shape not fitting into the finite space. Finally, if this is not sufficient, the user can specify the bounds when creating the index . Handling shapes that exceed the finite bounds We gracefully degrade indexing performance in this case. A shape that exceeds the finite space is clipped and indexed with both the cell-IDs corresponding to the part that falls in the finite space as well as a special overflow cell-ID. The specific geometry being used for the query is similarly handled – if that geometry does not exceed the finite space we do not need to query the overflow cell-ID. The Spatial Data & Indexing Road Ahead We plan to continue improving spatial indexing, based on feedback from our users. Examples of areas we are working on are: Geo-partitioned spatial indexes: A compelling feature of CockroachDB is the support for geo-partitioning , which reduces latency and handles compliance requirements. Inverted indexes, including spatial inverted indexes, currently cannot be geo-partitioned. We are actively working on geo-partitioned inverted indexes. Left join algorithmic improvements: The plans we generate for left outer/semi/anti joins involving spatial inverted indexes are not as efficient as we desire (unlike inner joins). We are actively working on a new scheme that addresses this problem for “non-covering” indexes. i.e., indexes that cannot fully evaluate the expression, either because they do not contain all the columns needed by the expression, or contain an imprecise column, like the cell covering in a spatial index. Query improvements for other types with inverted indexes: CockroachDB already supports inverted indexes over JSON and ARRAY types. Queries on these inverted indexes currently do not use the new distributed query processors described above. We are actively working on using these processors for JSON and ARRAY types. Pebble optimizations for spatial queries: Now that we have an in-house storage engine, Pebble , we have identified and made optimizations to better support inverted index queries, that will be available in the next release. Don’t hesitate to reach out if you have any feedback on features or capabilities you’d like to see with spatial data. You can post it in the #spatial or #product-feedback channels of the CockroachDB Community Slack .", "date": "2020-12-09"},
{"website": "CockroachLabs", "title": "How Retailers Can Exceed Customer Expectations on Cyber Monday (& Beyond)", "author": ["Cassie McAllister"], "link": "https://www.cockroachlabs.com/blog/retailers-survive-cyber-monday/", "abstract": "As retailers gear up for Cyber Monday, they are preparing for unprecedented traffic given the new ways people are shopping due to COVID-19. It’s already predicted ecommerce sales will hit over $10 billion USD on Cyber Monday alone. And although retailer’s online strategies are shifting, customer’s expectations remain the same. They want it cheap, and they want it now. According to one report , 62% of consumers say that no amount of safety measures would persuade them to shop in person. So for the first time ever, in-person Black Friday shopping deals became a thing of the past with many retailers shutting their doors and encouraging online shopping instead. So not only do retailers have to factor in the influx of traffic to their ecommerce sites, but they also have to consider that certain items (i.e. PlayStation 5, iPhone 12, etc.) will be in high demand and shipping/distribution will be a major concern . Retailers are taking new approaches such as shifting to a “ Cyber Month ” in order to promote deals early to avoid logistical nightmares, and developing lottery systems (like Nugget ) to set expectations upfront. Some companies (such as Peloton ) are even saying they won’t have a sale at all. Regardless of your strategy, Cyber Monday and associated holiday peak shopping periods will put pressure on your infrastructure. How are you going to handle massive traffic to your ecommerce site and prevent an outage? Will you survive a region failure? Are you confident when giving customers accurate pricing and shipping information? How many transactions can you process in an hour? In this post, we will cover three strategies retailers can employ to exceed customer expectations this holiday shopping season and beyond. #1 Website: How to handle massive site traffic with no downtime for your customers We will start here because if your website isn’t up and running, we can’t get much further. The backend of your ecommerce site should be able to withstand fluctuating traffic patterns without incurring latency for customers. Because CockroachDB offers inherent high availability you can ensure your apps and services are always on and available with 3x data replication. CockroachDB is literally (and ironically) impossible to kill. It’s architected on a node based system and is geo-replicated, so if one node fails, it automatically distributes the data evenly between all nodes. It’s also important to note that CockroachDB can process and store 10s of millions of transactions per day and accommodate peak performance insert rates with ease. CockroachDB is built to withstand fluctuating workloads without incurring latency. Additionally, it provides automatic rolling upgrades which results in 0 downtime – making developers very happy. Several retail and ecommerce companies use CockroachDB as a Point-of-Sale (POS) system of record for their brick-and-mortar stores and ecommerce sites. Because CockroachDB guarantees consistent transactions (more on this below), there’s never a problem with inconsistent data. Retailers can rest easy knowing that their website will maintain stable performance with CockroachDB. #2 Shipping & pricing: Guaranteed inventory and pricing for your customers Today, it’s absolutely crucial to be able to quickly and easily access your inventory so you can provide accurate purchasing and shipping information. It’s also important that you provide accurate pricing that reflects the continuously fluctuating deals. CockroachDB delivers correct data with a durability you can trust in any environment at local or global scale. It’s not eventual consistency like some databases promise. It’s guaranteed. Your customers don’t eventually want their order at the best possible price, they want it when you promise them. CockroachDB has a built-in Cost-Based Optimizer that serves low-latency, consistent, and current reads by accessing the closest data. This feature is especially valuable to customers who are using CockroachDB as a system of record for their supply chain operations. Retailers can manage inventory, pricing, and deliveries for all locations across the globe and ensure customers receive accurate and timely information. #3 Customer privacy: global data protection/consumer identity Large-scale data breaches make headlines each year and often cause distrust between companies and their customers. However, retailers still need a way to sell to global audiences while keeping their customers' data safe. One of the most unique aspects of CockroachDB is that it offers the ability to partition data by its location, a capability we call geo-partitioning . This means that you can tie data to a physical location using row-level controls. For regulations like the GDPR, European customer data stays in Europe . So not only does this allow retailers to scale to a global audience with guaranteed consistent transactions, but they can also adhere to data privacy regulations regardless of where the customer is located. Because of this feature, many customers use CockroachDB to protect their customer’s data and ensure it is being stored in the right location. For example, if you want to authenticate global customers to your ecommerce application CockroachDB makes this easy by housing data close to customers even when they are traveling. Looking beyond 2020 It’s safe to say that everyone is ready to put this year in the past. But if you are in charge of your company’s infrastructure, you always have to think about the future of your applications and services and the technologies that you build them on. So after reading this post, are you feeling confident? Are you ready to tackle the 2020 holiday season and the new ways consumers will be shopping potentially forever ? No? You should get in touch with us. Or try CockroachCloud for free. We have dozens of happy retail customers who use CockroachDB as the backbone to their infrastructure. We will also be posting two more blogs about how you can better capture global revenue and service your customers (not your database). Stay tuned!", "date": "2020-11-30"},
{"website": "CockroachLabs", "title": "Sharing Screens: What's it Like to Be an Engineer at Cockroach Labs", "author": ["Chelsea Lee", "Isaac Wong"], "link": "https://www.cockroachlabs.com/blog/engineering-learning-at-cockroach-labs/", "abstract": "When it comes to learning, we have all benefited from social learning in the workplace. Social learning is an opportunity for people to learn from one another through programs that help us share knowledge such as peer mentorship, or attending a lunch and learn, where someone shares their expertise with the broader company. Though sometimes unconscious, we are able to learn and observe by example and then apply what we take away to our own work. Much of social learning is reliant on being in the office. We’re all familiar with someone approaching your desk with a “quick question” that soon turns into a 30-minute over-the-shoulder demo where you are both trying to solve a problem collaboratively. While  interactions like these can be disruptive, there is also tremendous value in time spent collaborating with a teammate to explore and find solutions together. With our team working from home these past months, we’ve had to think more creatively on how we can foster these learning opportunities on our Engineering team. Human connection is vital to how we welcome people onto our team, so we’re hard at work to strengthen it. There’s two moments in time that we wanted to try and simulate: when you first join the company and ongoing day-to-day learning from each other. Pair Programming in MOLTing MOLTing is what we call our onboarding period at Cockroach Labs. During this 30-day timeframe, we focus on getting each Roacher up to speed with company knowledge, product knowledge, and role knowledge. For Engineering, we hope by 30 days engineers feel confident submitting PRs and navigating the code base. The code base for CockroachDB is vast , so we organize at least three pair programming sessions for each new Roacher that joins the team. One is scheduled with their Roachmate , another with a member of their team, and a third with an engineer working on a completely different product area. This way, new Roachers have an opportunity to meet others working on the product that they wouldn’t necessarily interact with during a normal day. When developing any new program, I recommend always asking yourself: “has anyone done this already?” This way you can learn from peers in your field and evolve programs to work better for your team. A quick search led me to a blog post written by the team at Atlassian on how they’ve made remote pair programming work. It was reassuring to see their confidence in remote pair programming, and their pro-tips on how to best organize these sessions were extremely valuable. Based on feedback from pilots, we’ve set some guidelines that have been helpful for pair programming with new Roachers: The more tenured Roacher should start as the Driver. The pair can feel free to swap roles during the session, but it’s more likely the new Roacher will be asking questions as they are shown the code. Determine together what you’ll attempt to accomplish ahead of time. Each new roacher is interested in learning something different. For some, it’s completing a small bug fix on the database code, for others, it’s walking through their product area’s code or getting help completing their starter project. We recommend that people either speak beforehand or open the session with a discussion on what they want to explore. “Watch Me Work” Office Hours While the Pair Programming during MOLTing helped ramp engineers up on our code base and product, there was still a gap for those who missed working alongside their peers on complicated problems. This type of learning is organic— in the workplace, you might overhear your colleagues discussing a problem they are trying to solve, leading you to listen, ask questions, and learn something new! One of engineering managers, Jordan, runs his own Twitch stream where he showcases databases and database programming. Earlier this year, I heard that his team was streaming to each other in a smaller group setting and thought, “how can we encourage this company-wide?” and went about making these sessions more regular. From these streams, you’re able to learn from others how to shape your work, solve problems, investigate, code, test, and build well-structured solutions. We hope it also provides engineers exposure to systems and components outside of the ones they normally work on. We’ve recruited a couple of our senior engineers to schedule “Watch Me Work” Office Hours on a biweekly basis. Some of them stream their office hours on Twitch, while others use internal Google Meet links. Either way, Roachers are encouraged to drop by, stay as long as they want, and respectfully observe how other engineers write their code. We encourage the hosting engineer to explain their thought processes and share insight into the choices they are making to teach their peers along the way. We’re grateful for the engineers who’ve set aside time every month to keep the learning accessible and make the sessions feel as “organic” as possible. We’ve even had our CTO, Peter Mattis, use his CTO Office Hours to share his screen and show parts of the code that he’s been working in. ⸺ Each of these programs are learning experiences within themselves. We’re taking in feedback, tweaking the guidelines, and evolving them as we go. One thing we’ve considered as we’ve built programs is how things will scale as we grow our team, and how these programs will apply to whatever our future workplace looks like. Both of these programs, though started as a remote practice, will stay with us in and out of the office.", "date": "2021-01-13"},
{"website": "CockroachLabs", "title": "Log and Error Redaction in CockroachDB v20.2", "author": ["Raphael 'kena' Poss"], "link": "https://www.cockroachlabs.com/blog/log-and-error-redaction-in-cockroachdb-v20-2/", "abstract": "CockroachDB users trust us with their most sensitive data (see: healthcare , finance ). And the best way for us to maintain that trust is for Cockroach Labs to never see this data at all. In CockroachDB v20.2, our tooling is able to automatically redact users' sensitive data out of log files, so that Cockroach Labs never even receives it. We also do this always for crash report telemetry. Data sharing in CockroachDB deployments In the default configuration, CockroachDB automatically sends anonymized telemetry data at periodic intervals to Cockroach Labs (see here to turn off diagnostics reporting ). This telemetry data is documented online and devoid of details about the data stored in a user’s cluster. Even the SQL metadata, the schema of databases and tables, is anonymized to only keep information about the structural relationships between columns, indexes and other database objects. Moreover, users can completely opt out of this telemetry reporting if they so choose. This has been true ever since telemetry was first introduced, in 2016. Additionally, when a CockroachDB node crashes or encounters an expected error, details about the situation are reported automatically to Cockroach Labs. We currently use Sentry.io as a collector for crash and error events. As with telemetry data, error data has been heavily redacted and fully anonymized ever since it was introduced, in 2017, and users can also opt out of automated reporting. Finally, CockroachDB continuously prints out details about its behavior into log files. These log files are stored alongside a cluster’s data, by default in the store directory. Log messages are extremely descriptive and spell out the lifecycle of a cluster over time. Therefore, they are invaluable when troubleshooting issues. Naturally, log files are not automatically collected and users must choose to send them to us when asking for help. When some data is too little data The text of error messages in CockroachDB can contain bits and pieces from a user’s application, for example a failed STRING to INT conversion can reveal sensitive data in the STRING value. The assembly of an error message from variables and other dynamic state inside CockroachDB involves multiple parts of the code base. An error or crash payload can even contain data from multiple components or layers in the architecture, as it traverses a distributed cluster to emerge at a SQL client connection boundary. There is no single engineer or team responsible for reviewing and editing the composition of all errors. Neither would we want to build such a team, as it would likely become a bottleneck and create friction against future development and our ability to quickly improve and evolve CockroachDB. Moreover, all the pieces of data that compose an error are glued together as simple character strings. The use of Go’s string type inside error object is a pervasive feature of Go’s ecosystem, and most of the software dependencies that CockroachDB is built upon depend on this basic abstraction. The problem with simple strings is that they do not have any internal structure: we have no way to know, at the system boundary, whether a special word inside a string comes from CockroachDB’s own source code or from data entered by an application or stored into a SQL table. Therefore, the redaction code for crash and error reporting up to and including v20.1 was extremely conservative and was eliminating many useful pieces of information from payloads, to avoid the risk of exposing user data. In practice, we often found ourselves unable to investigate crash or error reports, because of the lack of context. This is why we wanted more data included in error and crash reports. When some data is too much data Log files are populated with log entries from all components throughout the CockroachDB code base. Naturally, they can contain details about pretty much anything—including configuration data, client details, SQL schema and values. As for error payloads, log messages are composed as strings. The payload of a single log event can be assembled from components across multiple areas in the source code. As for errors, it would be undesirable to channel the authoring of every log message through a strict review and editing process: the velocity of the team would be seriously impaired. And, as for errors, the idea to use simple character strings to represent log messages internally is extremely ingrained throughout the Go ecosystem, with many of our code dependencies using a Printf-like abstraction for logging to CockroachDB’s own logs. However, for log files we cannot afford to strip any of this data out before log files are stored on disk. When problems arise and a situation needs to be troubleshooted, a user absolutely needs to know “what happened” so they can successfully recover and move forward. Therefore, log files contain all the details collected by the CockroachDB code; no redaction takes place. This puts us in a bind when a user finds themselves unable to troubleshoot a situation on their own, and they approach us for help. Just as them, we do need details about their cluster’s lifecycle. We need the details that are present in their log files. But our users rightfully would prefer if we did not get our hands on the sensitive bits of their business: the data they have stored “inside” the SQL tables or the IP addresses of their private servers is often inconsequential when troubleshooting hard problems, yet is included in log files. This is why our users wanted less data included in the log files they send us for analysis. A need for semantic boundaries in amorphous strings The problem we set out to solve was to improve the sharing of data between CockroachDB users and Cockroach Labs, to maximize the amount of non-sensitive data that could be shared, while ensuring that no sensitive data would be shared. It’s interesting here to consider the asymmetry of the problem statement. Cockroach Labs can certainly work with incomplete details about clusters deployed by users. It is often possible to troubleshoot problems without a full picture of a situation. Cockroach Labs thus wanted more data, but certainly not all of it, and we were not picky about “how much” — just some more would already have been quite good. However, meanwhile, many users certainly care that none of their data ever, by any means, leaves their infrastructure. Just a single word out from a SQL table leaking out through logs or telemetry could constitute a severe breach of confidentiality and a catastrophic legal liability (imagine, the name of a celebrity found in telemetry data for a company that manufactures cancer drugs). To these users, there is no discussion that they can tolerate “some” amount of data leak for the greater good of troubleshootability. If asked to choose, they’d choose “none”; and when pressed, they would likely drop CockroachDB altogether. Meanwhile, the structure of data flow inside CockroachDB’s source code is rather promiscuous: variable values flow from one package to another, and there is not a great deal of boundary between variables that contain a copy of user data and those that don’t. Therefore, generally inside CockroachDB we are not yet able to confidently point out to variables and say “this one is sensitive” and needs to be handled with care. In fact, the Go language that CockroachDB is built with has an extremely poor type system, which makes it practically difficult to customize data types to separate “sensitive” from “non-sensitive” bits. The most used data type is string , which represents unstructured sequences of characters. They can represent anything, and are used for pretty much everything. Strings are largely amorphous and ubiquitous. The code that builds strings, especially errors and logging events, mixes and matches sensitive data with insensitive data into strings. To be conservative, a redaction algorithm that is handed mere strings can thus only consider that all of the characters inside are sensitive, lest the redaction run the risk of leaking some forbidden words. This, by the way, is what our error redaction code had to do. This is, incidentally, also why we have always categorically refused to use regular expressions or other means of pattern matching to extract safe information from log files or error reports. There is not enough structure inside CockroachDB strings to make pattern matching work: any pattern that a hopeful operator would design could accidentally include a string built by an application. Since string compositions inside CockroachDB evolve rapidly, as engineers change code when adding features or refactoring code, a pattern developed one day could stop working the next, or suddenly become more inclusive. Just a single accident where a pattern match would report a false positive could yield an unacceptable, trust-breaching data leak. What was needed instead was to introduce more structure inside those pesky amorphous strings inside the CockroachDB source code. We needed to create a semantic boundary between data that was definitely cool to report to Cockroach Labs, and data that maybe wasn’t. Finality of strings Technically, our problem was somewhat non-trivial: Go programmers rely on the ubiquitousness and promiscuity of strings; it makes them able to work fast. The Go compiler does not very well enforce abstract type boundaries between string-like data types, so that it remains very easy to convert back-and-forth. In particular, all the string formatting and composition libraries out there remove any type distinction when composing strings together, so that the final result is a simple, amorphous Go string . Go programmers think it’s a feature, not a bug: we could not readily change that, as going against a programming language’s idioms is a sure way to reduce programmer productivity and ease of hire and training of new staff. What we did instead was to recognize a unique property of error objects and log messages in the CockroachDB project: they are not just write-once (all Go strings are); they are final. “Final” here means that they are never used as input to compose more complex strings. This is easy to recognize with log messages: the log messages are immediately printed to the output log file, but they are not “read back” by CockroachDB to do other things. For error objects, the situation is a little subtler. A Go error object may store strings and other data. An error object can be chained with another error object to build a more complex error object. Then, at the end, an error object can be transformed into a string via its Error() method. The key observation here is that the composition of error objects does not cause the strings and other values inside them to be transformed. The chaining of errors with each other does not imply that there is composition of different strings into larger amorphous strings. The conversion of a Go error into its representation as a string only happens if the code that handles the error chooses to convert it into a string, after which it is not an error object any more. In its “object” form with the error type, the strings inside errors are final too. With this understanding, we could then choose to change the data type and encoding for “final strings”, to introduce a clear separation between sensitive and non-sensitive data. It did not matter that we did something unusual in Go to achieve that, because there was no other Go code inside CockroachDB that would “consume” this unusual data. Crudely said, nobody in the CockroachDB teams really cares about how such final data is represented or stored—they never get to see it or manipulate it in their day-to-day job, unlike the other strings in their code. Redactable strings at error and log boundaries This is how we introduced a new data type in our code base: final strings in log messages and error payloads are now implemented via a new data type, called “redactable string”, or RedactableString in Go. (We have implemented this data type and the various facilities around it in a public standalone library, for free reuse by the Go community: see https://github.com/cockroachdb/redact ) A redactable string largely behaves like a Go string, except that it delineates sensitive data with redaction markers ‹ … ›. These are unicode single brackets, with code points U+2039 and U+203A. We chose them because they are visually discreet and extremely unlikely to be encountered as safe data that is useful for troubleshooting. Then, redactable strings provide a method called “Redact” which automatically strips all marked sensitive data and replaces it with “‹×›”. We have added calls to this feature in various parts of CockroachDB, for example in the command cockroach debug zip which automatically collects log files. To make this abstraction robust, when composing a redactable string from regular (or other kinds of) strings, the composition automatically strips any occurrences of the redaction markers in the input data and replaces them with a generic question mark “?”. This ensures that data containing the markers does not accidentally break the string boundaries during composition. We do not mind the fact that this may cause partial information loss, because again these strings are “final” —geared towards external output for communication to humans during troubleshooting, not communication between systems or data preservation. Moreover, we chose to ensure that the default and easiest way to build redactable strings from other Go strings is to consider the input string as sensitive. This means that the easiest way to create a redactable string conservatively places the entirety of the resulting composition in-between redaction markers. This way, when we introduced redactable strings in the CockroachDB project, we did not need to revisit all the existing logging and error code: we could simply let it “do its thing” with the comfort and confidence that all the resulting strings would still be considered sensitive and not risk being leaked. This conservative approach may at first look like it did not buy us much, as it still looks like most strings would be redacted out conservatively. That was done on purpose: we were very deliberate by assuming that all this error and logging data would be treated as sensitive until  we had built definite confidence it was not. We then gradually opted certain strings out of this “sensitive” status. The main and most powerful mechanism we implemented was to consider every literal constant string inside CockroachDB’s source code as “safe”, i.e. non-sensitive. Literal constants are those things spelled out “as-is” inside the source code. Of course, the Go compiler does not help libraries distinguish literal from non-literal strings, so we achieved that in a roundabout way. We defined certain parts of our API by fiat to only take literal strings as arguments. Then, we added linter programs that enforce this rule via source code analysis during CI runs. This is the current idiomatic way to extend Go’s type system—by enforcing it “externally”. (We could perhaps choose to tweak the Go compiler instead, but for now we want to preserve the ability of our community to build their own CockroachDB binaries using the standard toolchain.) Another wide-impact choice we made was to mark certain Go data types as “always safe”. For example, we automatically consider simple integers and durations as “safe” during the composition of redactable strings. This ensures, for example, that variables that represent range IDs and timeouts can always be included in reported data as “safe”. This does not include, however, SQL values with e.g. INTEGER or INTERVAL types, as these are represented inside CockroachDB using different (non-simple) data types. After that step, we also explained throughout our team how redactable strings work, so that certain parts of CockroachDB could carefully and manually opt into the special rules around them. We have established some automation as well to ensure that any unusual use of redactable strings pop out clearly during code reviews. The mental overhead to think about this remains low, however, since all this work is occurring only for “final” strings in our logging and errors infrastructure. In fact, most of the associated complexity is fully abstracted behind CockroachDB’s util/log and errors packages. More details about the design, as well as a review of the alternatives we considered, can be found here: https://github.com/cockroachdb/cockroach/blob/master/docs/RFCS/20200427_log_file_redaction.md Look and feel in CockroachDB v20.2 The most visible impact of this work is the appearance of redaction markers inside CockroachDB log files, starting in v20.2. Users will notice that any piece of data in log files that comes from their configuration or their cluster’s stored values will be enclosed in-between redaction markers. They will also likely notice that redaction markers enclose things that most definitely look like they are not sensitive, but this is a by-product of our choice of being conservative: these are data items that were non-literal constants in the CockroachDB source code, and will still need to review the remainder of those to determine which are definitely not sensitive. Until we do this work, we do not assume they aren’t and so they get redaction markers, just like users' data. Separately, we have built a new command-line flag --redact-logs in the commands cockroach debug zip and cockroach debug merge-logs . These are commands that we document as instruments of data collection when submitting support cases. The new flag ensures that any data inside redaction markers, including all the users' sensitive data, is erased before it is sent to Cockroach Labs. (Note that cockroach debug zip at this time only knows how to redact sensitive bits out of log files. Sensitive bits in other non-log files are not edited out. Proceed with care. This is tracked as github issue # 52470 .) Finally, we also have rewritten our Sentry.io reporting code so as to use redactable strings from error payloads. As expected, the error redaction removes anything between redaction markers before it is sent to Sentry. This is not directly visible when operating CockroachDB normally, however a user can satisfy themselves this is true by proxying and inspecting the traffic between CockroachDB and errors.cockroachdb.com. Next steps for log and error redaction The work on log and error redaction that led to v20.2 was primarily concerned with the relationship between Cockroach Labs and CockroachDB users. In that relationship, any data that can identify a CockroachDB user, like their IP addresses or hostnames, is just as sensitive as the data stored inside their clusters. From that perspective, we only needed a binary distinction between “non-sensitive data”, which is data confidently known to be safe to be seen by Cockroach Labs, and “sensitive data”, which would be everything else. Some of our customers have already approached us to explain that this distinction was too simplistic. Since we built this feature, we have been taught by our community that users care about another distinction: that between “operational” data and “application” data. This distinction is similar to the one we made already, but needs to occur entirely on the user’s side: it establishes a data boundary between the application developers and the DBAs and system administrators that operate a CockroachDB cluster. In that relationship, it is customary to prevent the DBA or support engineer at the user’s organization from seeing sensitive data produced by applications. For example, a bank’s DBA may need to see operational data that pertains to networking problems, disk access errors etc, but prevented from seeing the name of account holders and their statements. Today, CockroachDB’s redaction markers capture both operational and application data under a single label: “sensitive” data. Our users find this distinction too coarse, and instead wish for an additional distinction, between “operational” and “application” data in log files and error payloads. As we want to enable more CockroachDB deployments, in particular in larger Enterprise organizations with multiple departments and different data access policies, we will need to extend our mechanisms in this direction.", "date": "2021-01-18"},
{"website": "CockroachLabs", "title": "Announcing CockroachDB 20.2: Build more, deploy easier, innovate faster", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-20-2-release/", "abstract": "Here at Cockroach Labs, we want to arm you with tools you need to build better products, deliver better customer experiences, and maybe even create the next billion-dollar idea. Our goal with CockroachDB is to make it easier for any and every developer to deliver data-intensive applications, allowing them to easily take advantage of high availability and elastic scale. With our latest release, CockroachDB 20.2, we have added updates to make developers even more productive with a broader range of workloads. We’ve also continued to improve the security and management capabilities of the database and, as always, have made considerable improvements to the performance of CockroachDB. With this release we are also incredibly happy to note that the majority of these new capabilities are available in our free option, CockroachDB Core. With CockroachDB 20.2 you can now: Store and index spatial data using PostGIS-compatible SQL syntax Deploy and manage your cloud-native stack with greater ease, using our new CockroachDB on Kubernetes offering, which packages CockroachDB with an Operator Use core Backup and Restore capabilities (previously Enterprise-only) in our free, community option, CockroachDB Core Work more efficiently with easier debugging , added SQL functionality , and improved support for Java and Ruby Take advantage of improved performance —CockroachDB 20.2 passed TPC-C with 140k warehouses and a maximum throughput of 1.7M transactions per minute (tmpC), representing a 40% performance improvement over the past year Enjoy generally enhanced performance and stability with our new storage engine, Pebble Save time and improve security with new and updated management and security features As a general-purpose, distributed SQL database, CockroachDB is the right choice for any data-intensive application. The updates in 20.2 expand CockroachDB’s support to more workloads and give access to more developers. We’re excited to see what you build! Read on for more details, then head over to the 20.2 Docs for a full list of what’s new. Note that all features mentioned in this blog post are available for free in our open-source option. If you want to try out CockroachDB 20.2 yourself, you can download the release here or try it for free on CockroachCloud . Build more with CockroachDB 20.2 In 20.2, we focused on giving developers more tools, so they can build more types of applications and realize the full potential of those applications. Spatial data types and indexing in CockroachDB Spatial data powers some of the world’s most innovative apps and services, letting you answer questions like, “Where’s the nearest gas station?” and “How long will it take for my ride-sharing vehicle to arrive?” and even, “Where can I catch a Pokémon?” The only problem is, this data has been locked away in brittle legacy or separate specialized databases, making it difficult for developers to support large datasets in the cloud. With 20.2, we give spatial data the same first-class treatment as other data types, bringing it into the cloud age and making it easier to develop applications that use it. CockroachDB is the first SQL database to build this functionality from the ground up for a distributed environment. This means you can now effortlessly scale your spatial data and have the confidence it will survive outages. And you can serve all your customers with fast, always-on experiences no matter where they are on the globe. CockroachDB now supports the following, all of which are open-source, available for free, and accessible with PostGIS-compatible SQL: Geometry and geography types External formats (GeoJSON, Well-known Text (WKT), and Well-known Bytes (WKB)) Common spatial shapes (e.g., line, polygon, geometry collections) Spatial indexing More than 200 PostGIS-compatible built-ins and functions New transactions and sessions pages in the DB Console Our DB Console (formerly Admin UI) displays metrics like SQL performance, network traffic, and storage capacity and is critical for troubleshooting and debugging. CockroachDB 20.2 adds two new pages to help developers introspect and understand query performance: Sessions Page : See live database sessions, and cancel them easily from the DB Console. For a given session, you can see whether there’s a live transaction, which statements are currently running, and how long sessions have been running. Transactions Page : See historical SQL transactions and the statements that comprise them, so you can better understand application performance. This is in addition to the existing statements page, which lets you troubleshoot individual statements. Additional SQL functionality in CockroachDB 20.2 CockroachDB is wire compatible with PostgreSQL and delivers standard SQL syntax, so you can use our database as your next generation relational store. In 20.2, we’ve improved our SQL capabilities adding: User-Defined Schemas : Structure your data hierarchy with schemas, which are commonly used in relational databases including PostgreSQL. This update makes CockroachDB more familiar for developers, more compatible with PostgreSQL applications and tools, and more flexible in its support for different data isolation patterns such as microservices. Partial Indexes : Index only the subset of rows needed for fast reads. More precise indexing reduces the amount of data stored by your indexes and therefore the performance impact on writes to data that does not need to be indexed. Materialized Views : Reduce costs for frequently-run queries by caching query results in-memory and only updating when necessary. Enumerated types (ENUMs) : With this popular data type, you can restrict inputs to a defined set of values like a drop-down list. Improved performance of Foreign Keys : As a crucial component of relational databases, foreign keys protect data integrity by creating references between two tables to ensure the entry into one table is a valid entry into the other. In 20.2, performance improvements in foreign keys will let more customers use them. Better support for Java and Ruby in CockroachDB 20.2 CockroachDB supports a variety of popular data access tools, including ORMs, making it easier to develop in your preferred programming language. Specifically, for 20.2 we improved support for Java by adding better compatibility with Hibernate , MyBatis , and Spring Data JDBC ; and Ruby by adding compatibility with Active Record . We also built out an adaptor for the Go data access layer upper/db . Many thanks to all the community developers who collaborated with us on these projects. Don’t hesitate to let us know in Slack if there’s a tool you wish CockroachDB supported, or if you’d like to collaborate on building out support! Improved database performance in CockroachDB 20.2 As with every release we are committed to constantly improving CockroachDB’s performance and we’ve made significant advances with 20.2 . TPC-C: CockroachDB passes 140,000 warehouses at 1.7M transactions / minute : TPC-C is the industry standard transactional database benchmark, simulating an e-commerce environment. We’ve written a lot about TPC-C in the past as we think it is the best measure of OLTP database workloads. CockroachDB 20.2 passed TPC-C with a maximum volume of 140k warehouses (previously we reported 100K)and a maximum throughput of 1.7M transactions per minute (tmpC), which represents a 40% performance improvement over the past year. TPC-H: CockroachDB decreased query latencies on 20 of 22 queries : We also ran TPC-H, which extends our benchmarking work with complex analytic queries. While CockroachDB is primarily a transaction-oriented database, it can also perform complex joins and aggregations that are best measured through a benchmark like TPC-H. On the TPC-H benchmark, we saw a decrease in query latency for 20 out of the 22 queries with query 9 latency improving by 80x. Deploy easier with CockroachDB 20.2 Your database should make you more efficient, not slow you down and 20.2 introduces updates to let you more seamlessly deploy and manage CockroachDB—both with an Enterprise license and for free. Introducing CockroachDB on Kubernetes CockroachDB is already the easiest database to run with Kubernetes—indeed, it is the only database architected and built from the ground up to deliver on the core distributed principles of atomicity, scale and survival. This means you can manage your database in Kubernetes, rather than alongside it. And hundreds of our customers are doing just that. Today we’re introducing CockroachDB on Kubernetes , a version of our distributed database that packages up CockroachDB with our brand new, open-source Kubernetes Operator. We’ve learned a whole lot about Kubernetes over the past few years by using it for our own database-as-a-service, CockroachCloud and we’ve packaged many of these learnings into an open-source Kubernetes Operator. This offering makes CockroachDB even easier to deploy on Kubernetes, With CockroachDB on Kubernetes, you get a truly cloud-native database plus automated management and best practices with our new Operator: Deployment : Deploy with an operator that handles cluster securing (certs) and configuration (persistent volume size, number of CockroachDB nodes, and more). Management : Simply scale your cluster up and down on pods in Kubernetes without any manual manipulation of the data. Add a node (or remove a node) by spinning it up in a pod and the database will rebalance the data for you. Rolling upgrades : Execute rolling updates according to CockroachCloud’s best practices to perform upgrades and apply security patches. And the database naturally handles online schema modifications as well, even for primary keys so you can avoid any downtime. Resilience : Pods are ephemeral, but databases (nodes) are not; however, with CockroachDB, we use our core survivability capability combined with StatefulSets to elegantly recover from any pod failure. Basic distributed Backup and Restore are now in CockroachDB Core We want you to be able to build scalable production applications on our community option, CockroachDB Core. And with each release, we carefully review all of our capabilities to determine if any existing or new features should be placed into Core. We’ve outlined a set of guidelines to help us make these determinations and it seems we increasingly err on the side of Core these days. In our last major release (20.1), we added Role-Based Access Control (RBAC) to CockroachDB Core, giving community users more control over security. In this release, we’ve already spoken to the new Spatial capabilities that have been added to Core, and we’ve also added more advanced backup/restore capabilities to Core, including BACKUP , RESTORE , and EXPORT . We’ve been delighted to see CockroachDB Core clusters grow to support terabytes of data, and we recognize that scalable, distributed backups are crucial for these types of production applications. We hope these additions will let our community users achieve both effortless scale and peace of mind in production, with rock-solid disaster recovery plans: BACKUP : Captures native binary data with very high reliability and reproducibility, writes to a number of different storage options such as AWS S3, Google Cloud Storage, NFS, or HTTP storage, and distributes work across the nodes to maximize performance. RESTORE : Restores cluster databases or tables from a BACKUP. EXPORT : Exports tabular data or the results of arbitrary SELECT statements to CSV files. More advanced Backup features, such as Incremental Backups, Encrypted Backups, Locality-Aware Backups, and Revision History remain only in CockroachDB Enterprise and CockroachCloud. We’re incredibly grateful for the community feedback that challenges us to make our product better—for every discussion thread, Slack post, and email. Keep them coming! Automated, more seamless management To make CockroachDB as low-touch as possible, we’ve added more automated management capabilities and general improvements to operations. Native scheduled Backups : Schedule backups directly from your CockroachDB cluster, instead of having to run a separate backup scheduler. As they are native to the database, scheduled backups have the same resilience as your cluster. Userfile upload : Import data more easily with the new command `cockroach userfile upload,` which lets non-admins load files from their client into a cluster. User-scoped files protect from malicious actors and are a simple way to upload data from a laptop. Faster imports : Bulk imports of data into CockroachDB are now significantly faster , so you can minimize time spent waiting for data to load. Imports are accomplished using the SQL command IMPORT , and let you import CSV/TSV files, Postgres dump files, MySQL dump files, and more. Stronger security and compliance in CockroachDB 20.2 With 20.2, we’ve continued to improve CockroachDB’s security and compliance story to meet enterprise requirements and data privacy regulations. Certificate Revocation : Revoke a TLS client certificate before its expiration date, helping improve security in the case a certificate is compromised. With this update, CockroachDB now supports Online Certificate Status Protocol (OCSP). PII-redacted log files : Protect your sensitive data by redacting PII and other confidential information from logs. This helps you both maintain confidentiality and comply more closely with GDPR and other regulations. More granular Role Based Access Control (RBAC) : 20.2 introduces a list of new privileges and role options for CockroachDB’s RBAC, meaning finer-grained permissions for database users. CockroachDB’s new storage engine: Pebble With the goal of continuous improvement of CockroachDB, our team built a new storage engine from scratch, called Pebble. Previously, CockroachDB used RocksDB, and while it has served us well, we saw an opportunity to further enhance CockroachDB with a purpose-built storage engine. Pebble is an open-source key value store written in Go, and it bring a number of improvements to CockroachDB: Better performance and stability. Avoids the challenges of traversing the Cgo boundary. Gives us more control over future enhancements tailored for CockroachDB’s needs. Pebble is the default storage engine in CockroachDB 20.2, with the option to enable RocksDB if desired. Learn more about Pebble in this blog post. Innovate Faster with CockroachDB 20.2 With the updates in CockroachDB 20.2, we continue to move towards a world where the database automatically handles all rote operations, scale, and resilience. It gives you a flexible platform you don’t have to worry about. It works, and it opens up a world of possibilities. And all that means you can focus your energy and output on developing, creating, and innovating. CockroachDB 20.2 is the database for modern cloud applications—let’s see what you can build! Try CockroachDB 20.2 for Free This blog post covers just a sampling of the updates in 20.2. For a full list, head over to the 20.2 Docs . To try out these features yourself, just download CockroachDB Core or spin up an instant, free cluster on CockroachCloud , our CockroachDB-as-a-Service offering. Finally, we love feedback and would love to hear from you. Please join our Slack community and connect with us today! Try 20.2 for free", "date": "2020-11-10"},
{"website": "CockroachLabs", "title": "GDPR Compliance is Not Easy, But CockroachDB Can Help", "author": ["Sean Loiselle", "Spencer Kimball"], "link": "https://www.cockroachlabs.com/blog/gdpr-q-and-a/", "abstract": "Since January 28, 2020, the EU has issued $192 million (€158.5 million) in fines for GDPR (General Data Protection Regulation) violations ( DLA Piper , Engadget ). Although companies have had years to become fully GDPR compliant, compliance is not easy.\nAfter the GDPR took effect in 2018, we had a conversation with Cockroach Labs co-founder and CEO Spencer Kimball on the topic, who detailed the nuances of the law and what companies should do to comply within the context of their databases. This is a recap of that conversation from 2018. *** Editor's Note: Portions of the conversation below have been edited for accuracy and to better reflect the current state of the GDPR. ***\n*** --- *** Sean Loiselle: What does GDPR mean for the database itself? What are the implications or what are the technologies that GDPR affects? Spencer Kimball: Well, it's important to keep in mind that GDPR isn't so much about the technologies that are used as it is about penalizing companies that fail to assume greater responsibility for protecting their customers' data. It's concerned with incentivizing better data privacy and governance outcomes, not mandating any particular piece of technology or technological standards. This is really the only feasible approach that also has any hope of not stifling innovation, because the technological landscape is evolving so rapidly. The guiding principle behind GDPR is privacy by design. This means that security best practices must be implemented from the ground up, instead of tacked on as afterthoughts. In practice, this means that the infrastructure which supports a service must also have been designed with similar forethought for privacy concerns. GDPR judges outcomes, which are the product of end-to-end design, and a service composed of various layered technologies is only as secure as its weakest link. Databases naturally assume a heavy share of the burden, as they broker access to and maintain the permanent storage of the data. For example, any database which is worth its salt must encrypt data in flight from the database to the application server(s). However, a further step can be taken to eliminate other threat vectors, such as encrypting the data when it is stored to physical media, or at rest. Article 32 of the GDPR, which addresses security of processing”, never explicitly mandates encryption in flight or at rest, because it maintains a less specific, but more encompassing directive that the controller and processor shall implement appropriate technical … measures... to ensure a level of security appropriate to the risk. ” The type and amount of data, as well as the severity of the harm if it were to be inadvertently exposed, must be taken into consideration, as well as the likelihood of an array of risks leading to security breaches. In the end, each company must look to what the industry as a whole is doing and make sure that, at the very least, it is not inviting risks which are out of step with industry peers. Actors involved in GDPR compliance must ultimately rely on good judgement, and also expect that to evolve over time. CockroachDB encrypts data in flight and at rest with sophisticated key management mechanisms. While GDPR doesn't explicitly mandate that either of these encryption features be employed, you'd be on thin ice without both if, for example, you're planning to store significant customer financial data. Spencer Kimball: That's just a taste of what you must rely on from the database to comply with GDPR. There's plenty more about availability, access, integrity, testing, etc. But the most vexing aspects of GDPR go beyond what any database can hope to control because the compliance must cover the entire array of systems which a company uses to process customer data. GDPR aims not just to protect personal data, but to make it visible to its ultimate owners as well as make it disappear at their behest (the right to be forgotten”). To comply with these requirements, a company first has to figure out where all the data for a customer lives. It turns out that's a truly Herculean task for companies which have been processing data for decades. It could encompass hundreds of systems, thousands to hundreds of thousands of different data stores, including even random CSV files! That's the big data governance challenge. You've got data exhaust spread out over decades. The mind boggles. Sean Loiselle: Data exhaust. I've not heard that term before. Spencer Kimball: Think about exporting a SQL report into an Excel spreadsheet which then gets stored on some local hard drive somewhere. Over the years that gets copied into network storage and backed up in who knows how many places. That's just one simple data exhaust journey from a core system to the periphery. And core systems backup and migrate, upgrade, and splinter. This kind data is everywhere and that's a very big challenge to overcome. Unfortunately, CockroachDB isn't going to solve those kinds of problems. But that segues into an area where new database technologies like CockroachDB have a really bright future: data sovereignty and domiciling. The GDPR imposes restrictions on the transfer of personal data outside of the European Union or to countries which have been deemed to have data protection adequacy”. Interestingly, the United States has not been given the European Commission's adequacy imprimatur. Sean Loiselle: Is that true – the United States is not a trusted non-EU jurisdiction? Spencer Kimball: Correct. The United States is seemingly unconvinced that data privacy is a fundamental right. To adopt an adequacy decision, a country's regulations must be at least as good as the EU's. If they're not, personal data may still be transferred, but only through very specific legally permitted transfer mechanisms. One such transfer mechanism used for transfers to the United States called Privacy Shield was struck down in 2020 and that decision cast a great deal of doubt over the validity of using standard contractual clauses for those transfers, which most companies rely upon today. This is a big problem for anyone paying attention, not only do businesses need an adequate transfer mechanism, the ground is shifting underneath us by the day around which mechanisms are legally sufficient! Even if you have a way to transfer the data outside of the EU, you still need to tell your customers that you will be transferring their data and what transfer mechanism you will use. One of the things that GDPR goes above and beyond to make clear is that transparency to end users is very important and shouldn’t be buried five clicks deep on your website. End users need to be affirmatively told where their data is being sent. Sean Loiselle: Can you just provide the same kind of notifications you see on EU websites about using cookies, where someone has to accept it to continue using the site? Spencer Kimball: Exactly, that's what many companies will need to do. And that's not necessarily such a tall order... Add an interstitial to your website which pops up and lets the user know that their personal data will be transferred for processing and storage to the United States, for example, and to ask for consent where its needed. That's really where GDPR requirements end, and good business sense kicks in. Companies have to start thinking, \"Okay, but what do our customers want?\" This is a point that Kindred brings up. They're keeping financial data for their customers. Those users, whether they're in the EU or whether they're in Australia, naturally would prefer that personal data be stored within their respective legal jurisdictions. Let's consider the business decision which must be made by a fictional company which has traditionally stored its data in an Oracle RDBMS instance in a US-east datacenter. With GDPR, this company now must interrupt its European users to explain the company’s plans to transfer and store their personal data in the United States. If they have a regional competitor with a local service, there is every reason to believe they will lose customers who find this plan disagreeable, or even alarming. Make no mistake, these requirements are fundamentally warnings from the European Commission to consumers. Things become even more problematic for SaaS businesses, where personal data is processed and stored on behalf of other companies' customers. In these circumstances, adverse reactions can have a more dramatic impact on the bottom line because the level of concern is magnified. This is because it's shared both by the end consumer, who must still consent to or be informed about the transfers, and also by the SaaS customer who must assess the cost to their business of using a non-local provider for the same service. How does our fictional company solve this emerging problem? If they chose to keep their existing data architecture, they'd need to create two versions of their service: one for the United States, and a new siloed service for the European Union. The better solution is to use CockroachDB's geo-partitioning feature , which provides the ability to domicile data in close proximity to customers. Physically the database would have nodes in both the US and the EU, but to applications it would present logically as a monolithic database. While CockroachDB has had the ability since version 1.0 to replicate databases and tables with geographic constraints, geo-partitioning significantly extends this by allowing row-level control. Data domiciling is where we provide a differentiating capability for GDPR compliance. It's not compliance per se, but it unlocks the ability to create architectures where data can be processed and stored local to the customer, avoiding the costs associated with cross border data transfers. It's also worth mentioning that this can result in significantly reduced customer-perceived latencies! Sean Loiselle: Could you do something like this with other technologies on the market? For example, say you're using Cassandra, you're a small SaaS company, how would you approach data domiciling if you wanted to? Spencer Kimball: Cassandra does have some of these same capabilities if you squint really hard. In point of fact, with talented engineers, you could string together multiple instances of MySQL or Oracle and build a middleware layer which abstracts access from the application to hide the complexity of what's happening beneath the hood. But then you'd essentially have built another database, and it's yours to maintain. Ouch. What CockroachDB is bringing to this is a polished solution which otherwise looks just like a traditional monolithic RDBMS to application developers. The goal here is to make life easy for developers. Their energy is best spent iterating on the business use case, not struggling to write middleware, or dealing with the lack of transactionality or consistency. As GDPR compliance continues to take effect, it is important to consider which tools and technologies will allow you to work smarter, not harder, to adhere to the new rules. CockroachDB is one such tool which allows companies to comply simply by nature of the technology. Illustration by Christina Chung", "date": "2021-01-19"},
{"website": "CockroachLabs", "title": "GCP Outpaces Azure, AWS in the 2021 Cloud Report", "author": ["John Kendall", "Yevgeniy Miretskiy", "Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/2021-cloud-report/", "abstract": "The 2021 Cloud Report stands on benchmarks. Now in its third year , our report is more precise than ever, capturing an evaluation of Amazon Web Services (AWS), Microsoft Azure (Azure), and Google Cloud Platform (GCP) that tells realistic and universal performance stories on behalf of mission-critical OLTP applications. Our intention is to help our customers and any builder of OLTP applications understand the performance tradeoffs present within each cloud and within each cloud’s individual machines. Perhaps your current configuration isn’t the most cost effective. Or you are looking to build a net-new application and want to see which provider has the fastest network latency. Maybe storage has been an issue in the past and you are looking for new solutions. Regardless of your motivation, the report is designed to help you achieve your goals and develop the best architecture for your specific needs. The 2021 Cloud Report is developed by a team of dedicated engineers and industry experts at Cockroach Labs. It compares AWS, Azure, and GCP on micro and industry benchmarks that reflect critical OLTP applications and workloads. This year, we assessed 54 machines and conducted nearly 1,000 benchmark runs to measure: CPU Performance ( CoreMark ) Network Performance ( Netperf ) Storage I/O Performance ( FIO ) OLTP Performance (Cockroach Labs Derivative of TPC-C ) -- Highlights 1. On all things throughput, GCP is king GCP delivered the most throughput (i.e. the fastest processing rates) on 4/4 of the Cloud Report’s throughput benchmarks: network throughput, storage I/O read throughput, storage I/O write throughput, and maximum tpm throughput –  a measure of throughput-per-minute (tpm) as defined by the Cockroach Labs Derivative of TPC-C. For the first time in our three years of benchmarking the clouds, GCP delivered the highest amount of raw throughput (tpm) on the derivative TPC-C Benchmark, a simulated measure of the transactional throughput of a retail or ecommerce company. This win is made all the more exciting after GCP’s third place finish in 2020 . And for the third year in a row, GCP won the Network Throughput benchmark, delivering nearly triple the throughput of either AWS or Azure. Notably, GCP’s worst performing machine for Network Throughput outpaced both AWS and Azure’s best performing machines. 2. Competition for most powerful CPU processor heats up between Intel, AMD, and Amazon’s Graviton2 As we evaluated CPU across each of the machines, it became apparent that what’s inside each machine matters. We evaluated each cloud’s CPU performance using the CoreMark version 1.0 benchmark. CoreMark is open-source, cloud-agnostic, and tests against various real-world workloads like list sort and search. We reported the average number of iterations/second for both the single-core results as well as the 16-core results. On the single-core runs, Intel swept the board, with 3/3 winning machines running Intel processors. But when we looked at performance on the 16-core benchmark, none of the winning machines ran Intel processors. In fact, the AWS custom-built Graviton2 Processor, which uses a 64-bit ARM architecture, edged out GCP and Azure’s winning machines, both of which ran AMD processors. 3. AWS network latencies are unbeatable AWS has performed best in network latency for three years running. Their top-performing machine’s 99th percentile network latency was 28% and 37% lower than Azure and GCP, respectively. We do note that when it comes to placement policies, it is important to keep in mind possible randomness in the physical distance between instances and the correlated effects on network latency. 4. When more expensive “advanced” disks are worth the expense Each of the clouds offers what we designated an “advanced disk” – a more expensive disk for applications and workloads that require higher performance: AWS’s io2, Azure’s ultra disk, and GCP’s extreme-pd. Azure’s ultra-disks are worth the money: Azure’s ultra disk was a strong competitor this year, in use in each of Azure’s first-place finishes. In many of our benchmarks, machines running ultra disk demonstrated performance improvements commensurate to or better than their estimated price increase. AWS’s advanced io2 disks deliver low latency: Overall, AWS machines with the advanced io2 disks averaged 51% lower latency than the machines with general purpose disks, so if latency is of critical importance for your application or workload, we recommend evaluating io2 disks. GCP’s general purpose disk matched performance of advanced disk offerings from AWS and Azure: GCP’s extreme-pd disk was unavailable to us at the time of testing, so it was notable how well GCP’s general purpose (pd-ssd) disk performed against Azure and AWS’ advanced disks. In fact, GCP’s top-performing machine (n2-standard-16 / pd-ssd) for storage I/O read and write achieved only 5% fewer read IOPS than Azure and AWS’s top-performing machines. For overall OLTP performance, advanced disks may not be necessary: In our benchmark simulation of real-world OLTP workloads, cheaper machines with general purpose disks won for both AWS and GCP. The Cockroach Labs Derivative of TPC-C is a compute and memory intensive workload, and while it values storage I/O performance, we found the benchmark did not drive sufficient load at the storage I/O level to prove the value of io2 and ultra disks. As a result, memory- and compute-optimized machines thrived, and storage-optimized machines with advanced disks were underutilized. To improve OLTP performance, we found it is better to spend on more nodes, memory, and better CPUs. Machines with advanced disks could be more appropriate for heavier read or write tasks, and critical workloads with demanding latency requirements. Methodology & Open Source Reproduction Steps As in previous years, we have made everything required for reproduction — steps and resources, scripts, configurations, and instructions — available in an open source repository . Our goal is to ensure these resources will always be free and easy to access. We encourage you to review the specific steps used to generate the data in this report. Note: if you wish to provision nodes exactly the same as we do, use this link to access the source code for Roachprod, our open source provisioning system. Run the benchmarks Ask questions on HackerNews What else is in the report? The 2021 Cloud Report includes detailed results and analysis, an exhaustive list of individual machine performance, more details on OLTP performance, and the full results of the microbenchmarks for: CPU Performance Network Performance Storage I/O Performance OLTP Performance Each year there is an overall “winner” of the report based on the cloud’s ranking for each of the respective benchmarks. Download the report to see who was declared this year’s winner, and to better understand how each cloud and its machines performed, on each of the benchmarks. < Read the report >", "date": "2021-01-15"},
{"website": "CockroachLabs", "title": "How Retailers Can Achieve 2021 Revenue Resolutions with CockroachDB", "author": ["Cassie McAllister"], "link": "https://www.cockroachlabs.com/blog/retail-resolutions/", "abstract": "It’s 2021 and the in-store shopping experience is still on pause with many stores shutting their doors forever . Online shopping is no longer the future of retail, but the new reality of retail. On average, ecommerce sales grew 32.4% year-over-year with some retailers (Best Buy, Target) seeing over 100% growth . 2020 forced retailers to rethink the way they do business and the way they reach their customers. Smaller businesses were forced to take their sales online, while larger businesses needed to find new, creative ways to stay ahead of the growing ecommerce competition. Organizations that have the bandwidth and the money to build data-intensive applications are changing the customer experience. They offer extremely personalized applications and doubled down on targeted advertising. With so much competition online, if you don’t innovate your business to meet the status-quo, you will struggle to survive. Looking ahead, how will this new “digital shopping experience” affect your bottom line? Will you acquire as many customers this year without the physical experience? Will you be able to expand your online services to new audiences? In the era of COVID-19, to reach your revenue goals it’s important to have the ability to scale your business and reach new customers regardless of where they are located. In this post, we will cover three strategies retailers can take to ensure they meet their revenue goals this year. #1 Get the most out of your data & provide a great customer experience In this new digital shopping era, it’s all about meeting (and exceeding) customer expectations. The easiest way to lose your customers is to not deliver on your promises. From inaccurate pricing to unreliable delivery times to inconsistent inventory, all of these issues can impact your bottom line. In fact, in a recent study 50% of retailers complain that bad data hurts their business (i.e. poor operational efficiency, reduced developer productivity). In order to prevent bad data from infiltrating your organization and hindering your ability to provide a great customer experience, you must use a database that promises resiliency and consistency. Additionally, your database must be able to handle a high volume of data. CockroachDB is extremely reliable (it is a relational database after all) and was built to deliver consistent transactions at scale. However, once you add a distributed element, transactions can come from various directions and consistency can become complicated. To solve this issue, CockroachDB provides the highest level of isolation (serializable) in a distributed database. This means you can ensure your data is always available and always correct. Additionally, CockroachDB automatically distributes data to help address heavy workloads. Running critical, transaction-heavy workloads such as inventory and payments becomes easier in a distributed setting with CockroachDB. For example, you can use CockroachDB to optimize global inventory management . Since CockroachDB provides a single holistic view of your data, your team can be smarter about how you handle stock and get your product into the hands of customers on time. By investing in a reliable, consistent database you can stop worrying about bad data hurting your ability to create a great customer experience. #2 Grow your customer base by scaling to a larger audience Now that we’ve shifted to an online mindset when it comes to shopping, why not provide your product(s) to as many consumers as possible? This initiative may concern your database architect who will think about the overhead it will take to scale your current database and expand to new regions. And it might concern your CTO who has to comply with various international data restrictions. However, the ability to scale to a larger and more global audience can ultimately make or break your business. In order to reach new customers in new locations, you must rely on a distributed, cloud-native foundation. CockroachDB is built to scale across geographies and regions – you simply add new nodes and CockroachDB will automatically rebalance and replicate your data throughout the cluster. Additionally, CockroachDB is cloud-native meaning it allows you to build in a hybrid and/or multi-cloud environment. Not only is CockroachDB built to scale elastically across clouds, but it is the only database that offers geo-partitioning. This capability allows you to tie data to specific locations (i.e. close to your customers) to reduce global latencies and meet compliance regulations. Several of our retail customers leverage geo-partitioning to reach international audiences while remaining compliant. For example, if you are located in the UK you can use CockroachDB to authenticate customers to your website/application in North America. For many retailers, this capability is crucial to the success of their business. #3 Avoid costly data mistakes & eliminate unnecessary downtime Outages can be detrimental to your business and in today’s economy, they simply aren’t acceptable. However they are extremely common – at least 48 retailers experienced outages during Black Friday/Cyber Monday last year. Not only do outages cost millions of dollars, but they create distrust with your customers. Upgrades can also cause significant downtime that prevents your customers from accessing your site and they may never come back. Retailers should look to reduce these types of unnecessary risks wherever possible so they don’t impact their bottom line. Architected to withstand any outage, CockroachDB allows your apps and services to have continuous access to data. If a machine, zone or region goes down, CockroachDB automatically rebalances without compromising correctness or availability. Additionally, your applications will stay online as you roll out new features or perform rolling upgrades of the database/operating systems/machines. Not only can our customers discard expensive backup tools, but they can rest easy knowing that their applications and services will always be on and available. Several retailers use CockroachDB to reduce Recovery Point Objective (RPO) to zero which saves developer teams significant time. The future of retail Retailers who make it through this difficult time will come out even stronger. Especially if they choose technology and infrastructure like CockroachDB that can support their new resilient, data intensive applications. If you are interested in learning more about how CockroachDB can help you achieve your revenue goals this year get in touch with us. Or try CockroachCloud for free. Also check out part one of this retail series on how you can meet and exceed customer expectations and stay tuned for the third part on how you can better service your customers.", "date": "2021-01-25"},
{"website": "CockroachLabs", "title": "Connect and Contribute Days: Social Impact at Cockroach Labs", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/connect-and-contribute/", "abstract": "Companies like Cockroach Labs are first and foremost groups of people; people with passions and drives that go beyond what they do for work. This is, by and large, what drives organizations to consider how to best enable their staff to make an impact on their communities through volunteerism, donations, and action. As a company, we have emphasized doing good in our corporate giving initiatives : we match referral bonuses with an equivalent donation to a 501(c)(3) charity of our staff’s choice, run marketing campaigns geared towards amassing donations to groups such as Women Who Code and Black Girls Code, and establish an employee-led committee each year to decide where to donate a significant corporate gift. Still, many of our people have other ways of giving back. To give Roachers the time and space they need to be forces of good, we’ve established Connect and Contribute Days: company time dedicated for our people to learn, seek understanding, grow, volunteer, mobilize, or otherwise make a positive change. We know that people work differently and their strategies for supporting others are different, too. Connect and Contribute Days allow us to make room for everyone in community building, no matter where they are in their learning process. While some of us have been volunteering or mobilizing for years, others are still discovering which avenues work best for them-- both are necessary. What do Connect and Contribute Days Look Like? There are a few ways that Roachers can take advantage of Connect and Contribute (C&C) Days: Election days where we encourage employees to take time to partake in local elections. Company-allocated C&C days that could include a company-wide volunteer event or necessary engagement to reflect on the world. C&C days allocated by our CREWS (employee resource groups) that could include a speaker, workshop, or engagement. As such, we’ve held a number of Connect and Contribute Days in the last year to set aside time for both voter registration and voting. Empowering our CREWS to allocate days also ensures that we can use this time to support a wide array of vital causes. Moreover, Connect and Contribute Days are neither mandatory nor holidays, but they are encouraged. We ask managers to limit the number of meetings held on these days to ensure we can take full advantage of them and make the largest impact possible. Our company continues to grow by leaps and bounds. Each new teammate brings with them interests and commitments that can only make us better. We believe deeply that the collective power of nearly 200 people taking action to improve our world is the most meaningful contribution that we can make as a company. There is no cause too small or challenge too mighty for us to tackle together. Connect and Contribute Days are meant to give our people the resources and encouragement to join others in making a difference, because we believe in putting people first. To join us in building stronger communities, visit our careers page .", "date": "2021-01-27"},
{"website": "CockroachLabs", "title": "From Interns, With Love: CockroachDB Internship Projects", "author": ["Irfan Sharif"], "link": "https://www.cockroachlabs.com/blog/from-interns-with-love-cockroachdb-internship-projects/", "abstract": "While not exactly envious of our current crop of interns (because, you know, the whole work from home thing), I’ll admit I find myself reminiscing back to when I was one myself. I’m still surprised the engineering team let me anywhere near the stuff they did. When I first interned four years ago, we had declared a just code yellow to focus our energy towards stabilizing CRDB . Having joined the newly-formed distributed query execution 1 team, but now with its focus directed elsewhere, what this meant for me was free rein to flesh out distributed hash and merge joins 2 , few aggregation primitives (think SUM , COUNT , DISTINCT , etc.), and some sorting algorithms . That was more than enough to rope me back in for a second internship. This time I brought my dear friend Bilal along, who also went on to intern twice. I even managed to sneak my brother (a strictly worse engineer) in as a two-time intern. All of which is to say that I think internships here can be pretty great. CRDB is a cool system to work on, and we’re still at the point where we’re happy to let junior engineers take on work that elsewhere would only be accessible to someone more senior. This was true for me then, and I’d say the same applied for our most recent cohort. We have hosted several interns over the years (and hired plenty of them into full-time roles), all working on projects deserving of full-length blog posts. Today, however, we’ll highlight two projects from our most recent batch and give a briefer treatment for the remaining. Read-based compaction heuristics Aaditya Sondhi interned on our Storage team to work on Pebble , a storage engine based on log-structured merge trees 3 (abbrev. LSMs). Aaditya worked on introducing read-based compactions to Pebble, but before diving into what that means, we’ll first need to understand what read-amplification and compactions are. Compactions and read-amplification in LSMs In LSMs, keys and values are stored as sorted strings in immutable blobs called SSTs (sorted string tables). SSTs are stacked across multiple levels (L1, L2, &mldr;), don’t overlap within a level, and when searching for a key that overlaps with multiple SSTs (necessarily across multiple levels), the one found at the higher level is considered authoritative. This brings us to read-amplification: the amount of physical work done (bytes read, number of disk seeks, blocks decompressed, etc.) per logical operation. When reading a key k from a two-level LSM, we may have to trawl through both if it isn’t found in the first. That in turn brings us to compactions 4 . As data flows into higher level SSTs, LSMs maintain a “healthy” structure by compacting them into (fewer but larger) lower level SSTs. At one level (sorry) this lets LSMs reclaim storage (range deletion tombstones and newer revisions mask out older values), but also helps bound the read IOPS required to sustain a fixed workload. Like all things, this is counter-balanced 5 6 with the need to maintain sane {write,space}-amplification, which the rate of compactions directly play into. Figure 1. An SST compaction; the L1 SST overlaps with two L2 SSTs and is compacted into it. (Aside: there’s something to be said about how storage engines are characterized in terms of resource utilization 7 as opposed to unqualified “throughput” or “latency”. System-wide measures like $/tpmC are another example of the same. These feel comparatively easier to reason about, more useful for capacity planning, and easily verifiable.) Optimizing compactions for read-amplification Compacting LSMs based on reads isn’t a novel idea. It was originally implemented in google/leveldb , and later dropped in facebook/rocksdb . As for the Go re-implementation of it ( golang/leveldb , incidentally where we had forked Pebble from), it hasn’t ported over the heuristic yet. Part of the motivation for using a purpose-built storage engine was to let us pull on threads exactly like this. We hypothesized that by scheduling compactions for oft-read key ranges, we could lower read amplification for subsequent reads, thus lowering resource utilization and improving read performance. In implementing it , we borrowed from the ideas present in google/leveldb . For every positioning operation that returned a user key (think Next , Prev , Seek , etc.), we sampled the key range (mediated by tunable knobs). The sampling process checked for overlapping SSTs across the various levels in the LSM. If an oft-read SST was found to overlap with ones from lower levels, it was scored higher to prioritize its compaction. $ benchstat baseline-1024.txt read-compac.txt\nname            \told ops/sec  new ops/sec  delta\nycsb/C/values=1024\t605k ± 8%   1415k ± 5%  +133.93%  (p=0.008 n=5+5)\n\nname            \told r-amp\tnew r-amp\tdelta\nycsb/C/values=1024\t4.28 ± 1%\t1.24 ± 0%   -71.00%  (p=0.016 n=5+4)\n\nname            \told w-amp\tnew w-amp\tdelta\nycsb/C/values=1024\t0.00     \t0.00       \t~ \t(all equal)\n\n\n$ benchstat baseline-64.txt read-compac.txt\nname          \told ops/sec  new ops/sec  delta\nycsb/B/values=64\t981k ±11%   1178k ± 2%   +20.14%  (p=0.016 n=5+4)\n\nname          \told r-amp\tnew r-amp\tdelta\nycsb/B/values=64\t4.18 ± 0%\t3.53 ± 1%   -15.61%  (p=0.008 n=5+5)\n\nname          \told w-amp\tnew w-amp\tdelta\nycsb/B/values=64\t4.29 ± 1%   14.86 ± 3%  +246.80%  (p=0.008 n=5+5) Figure 2. Benchmarks showing the effect of read-based compactions on\nthroughput, read-amplification and write-amplification. As expected, we found that read-based compactions led to significant improvement in read heavy workloads. Our benchmarks running YCSB-C (100% reads) using 1KB writes saw read amplification reduced by ~71% and throughput increased by ~133%. With YCSB-B (95% reads) using small value reads/writes (64 bytes), we reduced read-amplification by ~15% which led to a throughput increase of ~20%. These benchmarks targeted Pebble directly, and there’s still a bit of legwork to be done around parameter tuning (we’re necessarily trading off some write-amplification in this process), but the results are encouraging. Query denylists (and our RFC process) Angela Wen interned on our SQL Experience team, which owns the frontier where SQL clients meet the database. During her internship Angela worked on introducing a mechanism to gate certain classes of queries from being run against the database. This was motivated by our Cloud SREs running large CRDB installations, and wanting the ability to deny queries(-of-death 8 ) when emergent situations call for it (think “circuit breakers” 9 ). Angela’s experience captures the kind of broad leeway accorded to interns that I’m arguing we do a bit better than elsewhere. A general purpose query denylist is a very open-ended problem, with many personas you could design it for, and one took deliberate effort to build consensus on. The process we use to structure these conversations are RFCs, and we ended up authoring one here as well. The RFC and the ensuing discussions clarified who the intended users were, the “must haves”/“nice-to-haves”, catalogued the various classes of deniable queries, and most importantly, outlined the actual mechanics of the denial itself. For all my gripes with RFCs, I find the process of actually writing one edifying. It can foster real agency over a component’s design and works decently well as a pedagogical tool (also I imagine it’s cool to have public design documents to share with friends similarly into query denylists). We ended up eschewing our original proposal to implement file-mounted regex-based denylists (the contentions here being around usability, deployment, etc.) in favor of cluster settings of the form: SET CLUSTER SETTING feature.changefeed.enabled = FALSE;\nSET CLUSTER SETTING feature.schema_change.enabled = TRUE; Configuration changes were made to disseminate cluster-wide by means of gossip 10 . Individual nodes listen in on these updates and use the deltas to keep an in-memory block-cache (sorry) up-to-date. This is later checked against during query execution to determine whether or not it’s an allowable operation. As mentioned earlier, we scrapped lots of alternate designs during this process, and were better off for it. We re-sized our scope to focus instead on certain classes of queries as opposed to more granularly matching specific ones. This came after observing that a vast majority of problematic queries during prior incidents were well understood, and could be structurally grouped/gated wholesale. That said, we modularized our work to make it simple to introduce new categories as needed. Observability, design tokens, data-loss repair, and more! We hosted a few other interns this semester, and there’s much to be said about their individual contributions. We typically structure our programs to have folks work on one or two “major” projects, building up to them with “starter” ones. Here we’ll briefly touch what these were. Query runtime statistics Figure 3. The query execution plan for a full table scan followed by an AVG . Cathy Wang interned on our SQL Execution team and worked on improving observability for running queries. We have some existing infrastructure in place to surface various execution statistics. Cathy built upon this to include details about network latencies (useful for debugging queries run within geo-distributed clusters), structured our traces to break down how much time is spent across various layers in the system, and tacked on memory utilization to our traces to surface exactly how much memory is in-use during any point mid-execution. This last bit is worth elaborating on: Go’s garbage collector doesn’t give us fine-grained control over allocations, and to that end a result we’ve had to design our own memory accounting/monitoring infrastructure to closely track usage during a query’s lifecycle. By exposing these internal statistics, we expect developers to better understand the memory footprint of individual queries and to tune them accordingly. Design tokens Pooja Maniar interned on the Cloud side of things, specifically on our Console team. One of the projects she worked on was consolidating and standardizing our “design tokens” . Think of these as abstractions over visual properties, variables to replace hardcoded color palettes, fonts, box shadows on pressed buttons, etc. The motivation here was to limit the number of design decisions developers had to make, whether it be choosing between specific hexcodes, UI components, etc. We wanted to create and hoist guidelines into a centralized, shared repo and then integrate it into our several console pages (accessible both through the database itself and our cloud offering). We were also partway through a brand-refresh at the time, and Pooja’s grand unification helped ensure brand consistency throughout. Quorum recovery Sam Huang interned on the KV team (they let me mentor this fellow), and one of the projects we worked on was to introduce a quorum recovery mechanism within CRDB. Because CRDB is built atop raft-replicated key-ranges, when a cluster permanently loses quorum for a given set of keys (think persistent node/disk failures), it’s unable to recover from it. This necessarily entails data-loss, but we still want the ability to paper over such keys and provide tooling for manual repair. Sam worked on introducing an out-of-band mechanism to “reset” the quorum for a given key-range, and somewhat cleanly, we were able to leverage existing Raft machinery to do so. This came from the observation that if we were to construct a synthetic snapshot (seeded using data from extant replicas, if any), and configured it to specify a new set of participants, we would essentially trick the underlying replication sub-system into recovering quorum for this key-range. Our synthetic snapshot incremented the relevant counters to “come after” the existing data, which also in-turn purged older replicas from the system. Metamorphic schema changes Jayant Shrivastava interned on our SQL Schemas team, and spent his time here ruggedizing our schemas infrastructure. CRDB makes use of several advanced testing strategies to ensure correctness and stability, including use of fuzzers , metamorphic and chaos testing, Jepsen 11 , and much more. Having observed some latent fragility in this area recently, Jayant fleshed out an equivalent test harness but focusing instead on schema changes . We constructed a workload generator to execute randomly generated DDL statements, executing within the confines of individual transactions. These statements generate and drop tables on the fly, do the same for columns with randomized types, and are executed concurrently with statements issued against those very tables/columns. We leveraged metamorphic methods here by asserting against the invariants of the system rather than specific outputs (things like “transactions that have read from a certain column should expect to always find it in subsequent reads”). Put together we were able to cover a large space of possible interleavings and uncovered several critical bugs in the process. Import compatibility Monica Xu took a brief hiatus from her aspiring music career to intern on our Bulk IO team. Her team’s broadly responsible for getting data in and out of CRDB as fast as possible (specifically import/export and backup/restore). Monica made several contributions in this area, including enabling progress tracking for dump files, supporting dry run imports , and improving pg_dump 12 compatibility. There were kinks to be work out with the latter seeing as how CRDB only supports a subset of Postgres syntax, which can be problematic when processing pg_dump files as is. The particular set of questions Monica helped address was what “reasonable behavior” is when chewing through potentially destructive import directives. Think DROP TABLE [IF EXISTS] , or CREATE VIEW , which is particularly tricky given it stores the results of the query it was constructed using, results subject to change during the import process. Monica engaged with our product teams when forming these judgements (we now simply defer to the user with instructive messaging), and helped significantly ease the onboarding experience for developers migrating off of their existing installations. Parting thoughts If you’re still here and interested, check out our careers page . And don’t let the database-speak throw you off, most of us didn’t know any of it coming in. Radu Berinde, Andrei Matei. 2016. Distributing SQL Queries in CockroachDB . ↩︎ Raphael Poss. 2017. On the Way to Better SQL Joins in CockroachDB ↩︎ Arjun Narayan, 2018. A Brief History of Log Structured Merge Trees . ↩︎ Siying Dong, [n.d.]. Leveled Compactions in RocksDB . ↩︎ Mark Callaghan, 2018. Read, Write & Space Amplification – Pick Two . ↩︎ Mark Callaghan, 2018. Name that Compaction Algorithm . ↩︎ Nelson Elhage, 2020. Performance as Hardware Utilization . ↩︎ Mike Ulrich, 2017. Site Reliability Engineering, Addressing Cascading Failures . ↩︎ Martin Fowler, 2014. Circuit Breakers . ↩︎ Abhinandan Das, Indranil Gupta, et. al. 2002. SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol ↩︎ Kyle Kingsbury, 2016. Jepsen Testing CockroachDB . ↩︎ PostgreSQL 9.6.20 Documentation, pg_dump ↩︎", "date": "2021-01-21"},
{"website": "CockroachLabs", "title": "Running CockroachDB on Kubernetes", "author": ["Alex Robinson", "Jim Walker"], "link": "https://www.cockroachlabs.com/blog/running-cockroachdb-on-kubernetes/", "abstract": "Since this post was originally published in 2017, StatefulSets have become common and allow a wide array of stateful workloads to run on Kubernetes. In this post,  we’ll quickly walk through the history of StatefulSets, and how they fit with CockroachDB and Kubernetes , before jumping into a tutorial for running CockroachDB on Kubernetes. -– Managing resilience, scale, and ease of operations in a containerized world is largely what Kubernetes is all about—and one of the reasons platform adoption has doubled since 2017.  And as container orchestration continues to become a dominant DevOps paradigm, the ecosystem has continued to mature with better tools for replication, management, and monitoring of our workloads. And as Kubernetes grows, so does CockroachDB as we’ve recently simplified some of the day 2 operations associated with our distributed database with our Kubernetes Operator.  Ultimately, however, our overall goal in the cloud-native community is singular: ease the deployment of stateful workloads on Kubernetes. Bringing State to Kubernetes CockroachDB helps solve for stateful, database-dependent applications through replication of data across independent database nodes in a way that will survive any failure (just in case our name didn’t make total sense).  CockroachDB, combined with Kubernetes’ built-in scale out, survivability and replication strategies, can give you the speed and simplicity of orchestration without sacrificing the high availability and correctness you expect from critical stateful databases. How do CockroachDB + Kubernetes = Retained State? While Kubernetes is fairly straightforward for use with stateless services, management and surviving state has been a challenge. Why?  You can’t simply swap out nodes as they depend on data in pod-mounted storage.  And rolling back doesn’t work for databases either. Some best practices have evolved to workaround the challenge of deploying data-driven apps on K8s: Run the database outside of Kubernetes: This creates lots of extra work, adds redundant tooling, and can actually invalidate the value you were looking to gain from K8s. Use a DBaaS: This, too, limits the value of scale and resilience of Kubernetes and limits your choice to only those options provided by your Cloud Service Provider To keep up with the demands of modern, data-driven apps, the Kubernetes community developed  a native way to manage state, via StatefulSets. StatefulSets assign a unique ID that keeps application and database containers connected through automation. Note: The use of “Unique ID” is a bit tricky here. Each resource in kubernetes gets UID to identify it but that UID will change whenever the resource is updated. StatefulSets assign pod identity which is persistent across pod generations but is separate from UIDs. StatefulSets are ideal for CockroachDB because the UID means it doesn’t get treated as a new node in a Kubernetes cluster, cutting way back on the amount of data replication required to keep data available.  This is key to efficiently supporting fast distributed transactions and our consensus protocol . For a real life example of a CockroachDB running on Kubernetes to retain state check out this Pure Storage case study . Step By Step Kubernetes Tutorial Step One: Building Your Kubernetes Cluster The year is 2021.  There are lots of ways to get your Kubernetes cluster up and running.  For this walkthrough we’ll use GKE .  If you’re interested in other paths, we have resources for: Running a cluster locally on Minikube Setting a cluster up on AWS With the Google Cloud CLI installed, create the cluster by running gcloud container clusters create cockroachdb-cluster Step Two: Spinning up CockroachDB Just like most Kubernetes deployment configuration work, CockroachDB config is managed by a YAML file like the one below.  We’ve added comments to help provide some context for what’s going on. Start by copying the file from our GitHub repository into a file named ‘cockroachdb-statefulset.yaml’. This file defines the resources to be created, in this case including the StatefulSet object that will spin up the CockroachDB containers and then attach them to persistent volumes. You’ll then need to create the resources shown below.  If you’re using Minikube, you may need to manually provision persistent volumes . You should soon see 3 replicas running in your cluster along with a couple of services. At first, only some of the replicas may show because they haven’t all yet started. This is normal, as StatefulSets create the replicas one-by-one, starting with the first. $ kubectl create -f cockroachdb-statefulset.yaml\n\nservice \"cockroachdb-public\" created\n\nservice \"cockroachdb\" created\n\npoddisruptionbudget \"cockroachdb-budget\" created\n\nstatefulset \"cockroachdb\" created\n\n$ kubectl get services\n\ncockroachdb          None         <none>        26257/TCP,8080/TCP   4s\n\ncockroachdb-public   10.0.0.85    <none>        26257/TCP,8080/TCP   4s\n\nkubernetes           10.0.0.1     <none>        443/TCP              1h\n\n$ kubectl get pods\n\nNAME            READY     STATUS    RESTARTS   AGE\n\ncockroachdb-0   1/1       Running   0          29s\n\ncockroachdb-1   0/1       Running   0          9s\n\n$ kubectl get pods\n\nNAME            READY     STATUS    RESTARTS   AGE\n\ncockroachdb-0   1/1       Running   0          1m\n\ncockroachdb-1   1/1       Running   0          41s\n\ncockroachdb-2   1/1       Running   0          21s Lifting the Hood If you’re curious to see what’s happening inside the cluster, check the logs for one of the pods by running kubectl logs cockroachdb-0 . Step Three: Using the CockroachDB cluster If all has gone to plan, you now have a cluster up and running.  Congratulations! To open a SQL shell within the Kubernetes cluster, you can run a one-off interactive pod like this, using the cockroachdb-public hostname to access the CockroachDB cluster. Kubernetes will then automatically load-balance connections to that hostname across the healthy CockroachDB instances. $ kubectl run cockroachdb -it --image=cockroachdb/cockroach --rm --restart=Never -- sql --insecure --host=cockroachdb-public\n\nWaiting for pod default/cockroachdb to be running, status is Pending, pod ready: false\n\n \n\nHit enter for command prompt\n\nroot@cockroachdb-public:26257> CREATE DATABASE bank;\n\nCREATE DATABASE\n\nroot@cockroachdb-public:26257> CREATE TABLE bank.accounts (id INT PRIMARY KEY, balance DECIMAL);\n\nCREATE TABLE\n\nroot@cockroachdb-public:26257> INSERT INTO bank.accounts VALUES (1234, 10000.50);\n\nINSERT 1\n\nroot@cockroachdb-public:26257> SELECT * FROM bank.accounts;\n\n+------+---------+\n|  id  | balance |\n+------+---------+\n| 1234 | 10000.5 |\n+------+---------+\n\n(1 row) Step Four: Accessing the CockroachDB Console To get more information into cluster behavior and health, you can pull up the CockroachDB Console by port-forwarding from your local machine to one of the pods as shown below: If you want to see information about how the cluster is doing, you can try pulling up the CockroachDB admin UI by port-forwarding from your local machine to one of the pods: kubectl port-forward cockroachdb-0 8080 You should now be able to access the admin UI by visiting http://localhost:8080/ in your web browser: Step Five: Simulating node failure We talked about DB survivability earlier.  Now you can test it for yourself.  What happens when a pod goes bad or gets deleted? To test the resiliency of the cluster, try killing some of the containers by running a command like kubectl delete pod cockroachdb-3 .  This must be done from a different terminal while you’re still accessing the cluster from your SQL shell. If you get a “bad connection” error from deleting the same instance your shell was communicating with, simply retry the query. The container will now be recreated for you by the StatefulSet controller, just as it would happen in the event of a real production failure. If you’re up for testing the durability of the cluster data, you can try deleting all the pods at once and ensuring they start up properly again from their persistent volumes. To do this, you can run kubectl delete pod –selector app=cockroachdb , which deletes all pods that have the label app=cockroachdb. This includes the pods from our StatefulSet. Just like during setup, it might take some time for them all to come back up again. But once they are up and running again, you’ll be able to get the same data back from the SQL queries you’re making in the shell. Step Six: Scaling the CockroachDB cluster Before removing nodes from your cluster, you must first tell CockroachDB to decommission them . (This lets nodes finish in-flight requests, rejects any new requests, and transfer all range replicas and range leases off the nodes. Now that the nodes are decommissioned you can scale your Kubernetes cluster by simply adding or subtracting replicas by resizing the StatefulSet as shown below: kubectl scale statefulset cockroachdb --replicas=4 Step Seven: Shutting the CockroachDB cluster down Once you’re done, a single command will clean up all the resources we’ve created during our oh-so-brief Kubernetes tutorial. The labels we added to the resources do all the work. kubectl delete statefulsets,pods,persistentvolumes,persistentvolumeclaims,services,poddisruptionbudget -l app=cockroachdb You can also shut down your entire Kubernetes cluster by running: gcloud container clusters delete cockroachdb-cluster Where to go from here Now that you’ve mastered the basics, what next? Writing applications that use the CockroachDB cluster via one of the many supported client libraries . Modifying how the cluster is initialized to use certificates for encryption between nodes . Setting up a cluster in a cloud or bare metal environment using a different kind of PersistentVolume, rather than on Container Engine. Setting up Prometheus to monitor CockroachDB within the cluster, taking advantage of the annotations we put on the CockroachDB StatefulSet. Contributing feature requests, issues, or improvements to CockroachDB , either for the Kubernetes documentation or for the core database itself! Check out more tutorials and tech talks about Kubernetes References More information and up-to-date configuration files for running CockroachDB on Kubernetes can be found in our documentation .", "date": "2021-03-03"},
{"website": "CockroachLabs", "title": "Use the Cockroach DB Console to Observe and Troubleshoot SQL", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/use-the-cockroach-db-console-to-observe-and-troubleshoot-sql/", "abstract": "Whether you’re a developer or operator, we want to give you the observability tools you need to monitor your CockroachDB cluster. This includes standard tools such as logs , metrics like Prometheus and health endpoints , and alerting . But modern observability requires more than just the standard tools. To give you as much insight into your database as possible, CockroachDB also provides SQL commands, tables, and a native DB Console packaged with every binary to provide observability for any workload. The DB Console gives operators and developers access to important cluster data including Metrics, Databases, Sessions (new in CockroachDB 20.2), Transactions (new in CockroachDB 20.2), Statements, and more. The purpose of this post is to give you an overview of several key ways you can monitor CockroachDB, and provide you with a place to learn more. Why do operators use the DB Console? If you’re an operator, you know that it’s not a matter of if things will go wrong with your environment, but when. We arm operators with access to critical cluster health information by evaluating node status (e.g., live, suspect, and dead nodes), replication health, as well as assess key metrics like CPU, memory, and disk usage. This allows you to prevent problems before they happen or quickly discover the root cause during an emergency. Why do developers use the DB Console? If you’re a developer, you know that change is the only constant for production workloads. New business requirements lead to schema migrations and new transactions with new performance goals. CockroachDB already provides zero downtime migrations to minimize the impact of modern development. The DB Console now provides you with a way to observe your workloads and the sessions, transactions, and statements that comprise them. Iterating is a way of life for developers, CockroachDB now provides a tool that makes it easy to assess how changes to schema and query design impact the workload. Let’s dive a bit deeper into how developers can use the DB Console. Sessions Page If you notice throughput has dropped or CPU usage is approaching 100%, you might suspect an errant statement is running loose somewhere in your database. You need to be able to find it and cancel it as quickly as possible. The Sessions page, introduced in CockroachDB 20.2, provides you with live access to all sessions (i.e., connections) and in-progress transactions and statements. You can sort by session age, transaction age, statement age, or memory usage to find any problematic active sessions. More importantly, CockroachDB now allows you to cancel problematic sessions and queries: You can click into any given session to learn more about the number of statements executed or retried, as well as learn about memory usage. Now with CockroachDB 20.2, developers have the flexibility to find and cancel sessions. Learn more about the sessions page via our documentation . Transactions Page It’s not always easy to understand the transactions that a complex, dynamic workload performs. Understanding which statements and tables make up each transaction, as well as the transaction’s end-to-end latency, are critical inputs into understanding and managing workload throughput and end-user latency.  CockroachDB 20.2 adds visibility into transaction fingerprints on the new transactions page in the DB Console. Like the statements page introduced in previous versions, the transactions page shows historical information about already executed transactions and statements. However, unlike with the statements page, you can now see all of the statement fingerprints that comprise a specific transaction: You can even drill down into the statements detail pages to learn more about that fingerprint. Learn more about the transactions page via our documentation . Prefer to use the SQL shell? No problem! We use SQL to power the sessions, transactions, and statements pages in the DB Console. If you’d rather access this information directly, you can use the following tables: DB Console Page CockroachDB Internal Table Sessions crdb_internal.node_sessions Transactions crdb_internal.node_transaction_statistics Statements crdb_internal.node_statement_statistics For example, here’s an example of a statement aggregation from the crdb_internal.node_statement_statistics virtual table: root@127.0.0.1:26257/movr> select * from crdb_internal.node_statement_statistics limit 1;\n-[ RECORD 1 ]\nnode_id             | 1\napplication_name    | $ cockroach demo\nflags               |\nkey                 | ALTER TABLE rides ADD FOREIGN KEY (city, rider_id) REFERENCES users (city, id)\nanonymized          | ALTER TABLE _ ADD FOREIGN KEY (_, _) REFERENCES _ (_, _)\ncount               | 1\nfirst_attempt_count | 1\nmax_retries         | 0\nlast_error          | NULL\nrows_avg            | 0\nrows_var            | NaN\nparse_lat_avg       | 2.1e-05\nparse_lat_var       | NaN\nplan_lat_avg        | 0.000362\nplan_lat_var        | NaN\nrun_lat_avg         | 0.002501\nrun_lat_var         | NaN\nservice_lat_avg     | 0.002923\nservice_lat_var     | NaN\noverhead_lat_avg    | 3.899999999999954e-05\noverhead_lat_var    | NaN\nbytes_read_avg      | 0\nbytes_read_var      | NaN\nrows_read_avg       | 0\nrows_read_var       | NaN\nimplicit_txn        | true\nTime: 1ms total (execution 0ms / network 0ms) You can also review in-progress sessions, transactions, and statements via the following SQL SHOW commands: SHOW SESSIONS; SHOW TRANSACTIONS; SHOW QUERIES; Try out the DB Console with CockroachDB and CockroachCloud Whether you’re a developer focused on using the database to iterate upon your application or an operator focused on keeping the entire cluster happy, CockroachDB 20.2 has the tools you need to observe and troubleshoot any problem. You can get started today with a free trial of CockroachCloud. Is there an observability feature that you wish we supported? We’d love to hear what it is. Drop us a line in the #product-feedback channel of our community Slack group .", "date": "2021-01-26"},
{"website": "CockroachLabs", "title": "Brazil's Nubank Uses CockroachDB for Resiliency & Scale", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/nubank/", "abstract": "Nubank migrated its credit card authorization service from in-memory storage to CockroachDB to achieve scalable, always-on infrastructure and more efficient operations Nubank, a leading Brazilian financial technology company that was just valued at $25 billion dollars, needed a scalable SQL database for several critical services. The bank initially stored data on-prem using an in-memory data store, but as the business rapidly grew (from 12 million customers in 2019 to 34 million customers in 2021), it needed a transactional solution for some of their applications. Nubank’s team chose CockroachDB due to its ultra-resilience, simple scalability, strong consistency, and easy maintenance. They set up on-prem CockroachDB deployments and migrated their credit card authorization application. As the company continues to expand into new markets, they may migrate some apps to Amazon Web Services (AWS) and create a hybrid cloud deployment that spans on-prem servers and cloud regions. Challenge: How to expand into new countries without creating an operational nightmare? Nubank is a Latin American digital bank with headquarters in Brazil. Founded in 2013, it quickly grew to become the highest valued private digital bank in the world, with 34 million customers. Nubank was built to address Brazil’s excessively high borrowing rates and poor banking customer experience. The company offers fee-free credit cards, digital savings accounts, and personal loans, which are all accessible from mobile devices. As Nubank’s business grew and its customer base expanded into Mexico, Colombia, and other countries, it needed to evolve its application architecture. The first iteration of infrastructure was built with Java Heap using an in-memory data store. However, this infrastructure was operationally complex and poorly scalable. For example, the bank’s credit card authorization app (the service that decides whether or not to approve transactions) was spread across four instances of Java Heap, and a Kafka message communicated data changes between the instances. The team found that Java Heap was not sufficient to store this kind of data. “We have a highly complex transaction environment where we are continually searching for simplicity, scalability, and robustness,” said Gabriel Melo, Engineering Manager at Nubank. The team decided they needed a transactional SQL database and began evaluating options. Their goal was to migrate one service at a time, starting with their authorization app. Requirements: An always on, never down experience for their customers Nubank’s team had several requirements for their new solution. Resiliency was important, as they need to provide always-on service for their customers. A horizontally scalable database that could meet a guaranteed level of consistency, in order to support their large volume of transactions. Nubank also demands simplicity, so the team wanted the new database to improve the efficiency of developer and infrastructure teams. To increase productivity, the database ideally would be a SQL database that supports standard drivers. The bank also sought a solution with minimal operational upkeep. Solution: A database that combines consistency, resiliency, and scale Nubank’s team evaluated multiple databases, including Cassandra and CouchDB. Ultimately, they decided that CockroachDB best met their requirements. CockroachDB’s combination of consistency and scalability provides a solid foundation for Nubank’s credit card authorization system and for their future expansion. “In our environment, CockroachDB brings us the confidence and simplicity to follow Nubank’s growth,” said Melos. CockroachDB was built to support transactional workloads like Nubank’s financial ledger . It achieves strong consistency and guarantees the highest level of isolation ( serializable isolation ). At the same time, CockroachDB scales automatically. Data is broken into 64-megabyte chunks called ranges that distribute and balance across nodes. If a user wants to add an additional node, he or she simply spins it up and points it at the cluster. Nubank has initially deployed CockroachDB on-prem in three locations, and if they choose to migrate to the cloud, CockroachDB’s flexibility and scalability will allow them to seamlessly transition. Resiliency is also built into CockroachDB, helping Nubank achieve the always-on service it requires. CockroachDB replicates all ranges three times and stores the replicas in a way that maximizes geo-diversity. The replicas are spread across nodes of a cluster, which together forms a single logical database. Depending on the cluster’s topology and how many machines and regions are involved, the replicas could be located across multiple regions, data centers, machines or all of the above. This replication ensures survival in the event of failure. In addition, CockroachDB increases the productivity of Nubank’s developer and infrastructure teams. Its standard SQL interface, which is PostgreSQL wire compatible, allows developers to communicate easily with the database. The infrastructure team has benefitted from CockroachDB’s automated backups, automated garbage collection , and rolling upgrades. These features give them more freedom to focus on tasks like maintaining the hardware of their on-prem servers. Results: CockroachDB is the mission-critical database backing Nubank’s credit card authorization system CockroachDB is now the mission-critical database backing Nubank’s credit card authorization system. It provides the availability, scale, consistency, and simplicity the digital bank needs. The company is evaluating additional apps to run on CockroachDB in the future. As the company continues to grow and expand internationally, they may migrate some data to AWS. In this case, they would run CockroachDB in a hybrid deployment across on-prem data centers and cloud regions supported by AWS or any other cloud vendor. Additional resources: Nubank case study A top US financial software company turns to CockroachDB to improve the application login experience JPMorgan Chase Honors Cockroach Labs for Innovation & Partnership Financial Data Firm Consolidates Legacy Oracle Databases into a Single Global CockroachDB Deployment", "date": "2021-01-28"},
{"website": "CockroachLabs", "title": "5 Career Tips from Women Crushing It in B2B Tech Sales", "author": ["Megan Mueller"], "link": "https://www.cockroachlabs.com/blog/5-tips-for-women-in-b2b-sales/", "abstract": "The women on the B2B technical sales team at Cockroach Labs routinely crush their sales targets. They shared with us some of their expert tips for entering into enterprise sales and finding success in the field. [ watch the interview here ] . Jen Murphy (far left in photo), Head of Channel Sales, led the discussion featuring Account Executives (from left to right) Charlotte Mastantuono, Mikael Austin, and Carolyn Parrish. 1. It’s not enough to be seen. Make sure you are heard. Mikael, an Account Executive, highlighted how women are often overlooked as subject matter experts in meetings: “if people don’t look at you, it can make you question yourself… they’ll ask questions to the man next to me even if he doesn’t have the answer.” She added, “At Cockroach, I actually feel very different… they listen to me.” 2. Your experience in other fields can make you a better seller – but find a mentor Charlotte studied Events and Business Management, but was inspired to enter the world of sales seeing her husband perform his work as an Account Executive. “I started as [a Sales Development Representative] back in the day, booking meetings,” she shared. “It’s important to have a mentor because there are a lot of best practices to be learned.” An audience member asked if women later in their careers have to start at entry level roles when transitioning; Jen’s advice was that “you can apply more advanced jobs,” but “you need to be willing to take a sidestep” on a different path to gain new skills. She also advised that entry-level roles such as SDR positions are great for learning sales fundamentals. 3. Have a family AND crush your sales numbers Jen recalled that when she started her career twenty years ago, women starting or growing their families during their career were frowned upon. Fast-forward to present: Charlotte joined us with a 17-month-old and noted that her decision to join us was made easier by learning about our culture of support, and the many on the Cockroach Labs team are parents themselves . “I felt like I was at a company that supported my personal goals. I recently announced that I’m having my second daughter and I want to inspire other women… you can have another baby, take that time out , and be closing those million dollar deals” when you get back. 4. Be selective. Ask probing questions when you’re interviewing at a new company Some of the advice panelists shared here included: make sure you meet the person who would be your manager, ask questions about the promotion plan and career path for your role, and, if you’re seeking mentorship, be sure to inquire about whether the team you might join is ready to help new people grow. 5. Pull up a chair for other women Everyone on the panel agreed that as you grow in your career, you can lift other women up while rising. “People here want to make you successful, we have an overabundance of that here.” To listen to the full interview, click here . There’s room for everyone on our sales team and we’re growing fast. If you want to join the team hard at work to close our next big customer, visit our careers page to learn more.", "date": "2021-03-08"},
{"website": "CockroachLabs", "title": "Distributed Tracing and Performance Monitoring in CockroachDB", "author": ["Charlotte Dillon"], "link": "https://www.cockroachlabs.com/blog/performance-monitoring/", "abstract": "When you’re working with distributed systems, data storage and retrieval aren’t as straightforward as they are in legacy monolithic databases. This comes with advantages like resilience and high availability , but it means that performance monitoring of a given transaction can be challenging. Query execution is often an extremely complex web of interactions. Following and analyzing performance bottlenecks in this environment can be difficult and sometimes frustrating to get to the root cause. For this reason we added distributed tracing to our UI and made an accompanying tutorial below. What is Distributed Tracing? Distributed tracing is a method of monitoring application performance by tracing the path of a query to identify issues that could be impacting application performance. In Kubernetes, this challenge has been solved using distributed tracing. Companies like Lightstep are leading the way to simplify observability analysis for these complex environments. At Cockroach Labs, we added distributed tracing for transactions as an option in CockroachDB 20.1. Distributed Tracing in CockroachDB vs Lightstep It’s important to distinguish between ‘distributed tracing’ as it’s understood in the field of Observability compared to the distributed tracing that you have access to in CockroachDB. CockroachDB implements the open tracing standard, so you can use tools like Jaeger , Zipkin , and Lightstep to trace transactions within CockroachDB. Companies in the Observability field instrument multiple layers of their stack to track a request as it’s shuttled through various services. This allows them to trace action from outside of the database to the action happening inside the database. Here’s a simple example that paints the picture of how distributed tracing works in CockroachDB: Let’s say people are waiting too long after clicking a button on your application. In CockroachDB you can use a trace to determine what happens when that button is clicked: What all is happening in the background? Where time is being spent? What happens in parallel vs what is blocked waiting on other things to finish.? First you’ll see that the database request is taking too long to return. Then you’ll isolate which query was the slow one, after which you’ll run a trace in CockroachDB for that specific query/transaction to see what made that specific request slow. Now you’re ready to fix the issue, and improve your application performance. Demo of Distributed Tracing In this demo video, Senior Product Manager Piyush Singh points a TPCC workload at a CockroachDB cluster and walks through how to troubleshoot queries directly in the Admin UI. You’ll see how he identifies the performance issue in the admin UI, opens up a diagnostic report which tells the system to trace the next query with a fingerprint that matches the slow query. In addition to the trace you’ll receive additional diagnostic information that you could need to troubleshoot query performance. To learn more about distributed tracing and how to troubleshoot your query, check out our docs on diagnostics reporting in CockroachDB . Also of interest will be this documentation around different ways of making your queries faster (one of which is to diagnose with a trace).", "date": "2021-02-01"},
{"website": "CockroachLabs", "title": "When (& Why) You Should Use Change Data Capture", "author": ["Michael Wang"], "link": "https://www.cockroachlabs.com/blog/why-use-change-data-capture/", "abstract": "Change Data Capture (CDC) can simplify and improve both your application and data architectures. The trick is to figure out the most effective use cases where employing CDC will have the desired impact. In this blog, I’m going to unpack two useful CDC use cases: The first is streaming data to your data warehouse and the second is event-driven architectures. By no means are these the only two use cases for change data capture, but they are excellent examples for demonstrating the ways that CDC can simplify application and data architecture. What is Change Data Capture (CDC)? Change data capture is a set of technologies that allow you to identify and capture data that has changed in your database, so that you can take action using the data at a later stage. Use CDC For Streaming Data to Your Data Warehouse Streaming data from your database into your data warehouse goes through a process called ETL or ELT. CDC can make this process more efficient. What are ETL & ELT? ETL stands for Extract Transform Load whereby you take the data from your primary database, extract it, do some data transformations on it (aggregations or joins) and then put those into your data warehouse for the purposes of analytics queries. ELT is a more common concept these days, where instead of transforming before you load, you actually load the raw data into your data warehouse -then do those aggregations and joins later. Batch Data vs Streaming Data Traditional ETL is based on the batch loading of data. You would achieve this by either doing a nightly job where you do one big query to extract all the data from your database to then refresh your data warehouse, or you poll your database on some periodic cadence, for instance every half hour or an hour, to get the new data and just load that new data into your data warehouse. Either way there are three big downsides to this process: Periodic spikes in load: These large queries impact the latency and ultimately the user experience, which is why a lot of companies tend to schedule spikes in low traffic periods. Network provisioning: Sending all that data puts a lot of strain on your network. And because you have big spikes in network costs and bytes that you’re sending over the network, you have to provision your network to be able to handle peak traffic and peak batch sending of data. Delayed business decisions: Business decisions based on the data are delayed by your polling frequency. So if you update your data every night that means you can’t query what happened yesterday until the next day. Using change data capture to stream data from your primary database to your data warehouse solves these three problems for the following reasons: CDC does not require that you execute high load queries on a periodic basis, so you don’t get really spiky behaviors in load. While changefeeds are not free, they are cheaper and they are spread out evenly throughout the day. Because the data is sent continuously and in much smaller batches, you don’t need to provision as much network in order to make that work, and you can save money on network costs. Because you’re continuously streaming data from your database to your data warehouse, the data in your warehouse is up-to-date, allowing you to create real-time insights, giving you a leg up on your competitors because you’re making business decisions on fresher data. Use CDC for Event-Driven Architectures In event-driven architectures , one of the hardest things to accomplish is to safely and consistently deliver data between service boundaries. Typically, an individual service within an event-driven architecture needs to commit changes to both that service’s local database, as well as to a messaging queue, so that any messages or pieces of data that need to be sent to another service can do so. But this is challenging. What happens if your message commits to your database but not to the messaging queue? What happens if the message gets sent to the services but it doesn’t actually commit in your database? To make this more concrete, let’s think about an imaginary (and currently impossible) social event app called MeetWithMeTomorrow. Users can go into the application, create an event, invite their friends, and then confirm that event. And when that event is confirmed, a push notification is sent to your friends so that they know where to meet you. In this mock architecture the data moves like this: The user creating the event will send the event to both the event tracking database as well as to a kafka messaging queue, That would then propagate that over to the notification service, Which would then send out the push notifications to each of the individuals that you invited. The problem with this architecture is that sometimes the kafka queue does not receive your message. When this happens, push notifications don’t get sent, but the event creator thinks they have been. That’s a confusing user experience. Conversely, if the event makes it to the kafka queue and the push notifications get sent to that user’s friends but it doesn’t get committed to the database, the user doesn’t know that those push notifications were sent. So the users friends will show up, but the user won’t. Another bad experience. There are workarounds that address this issue. You can add application logic to only consider an event created if it detects both that the message was committed to the event tracking database and that it was successfully published to your kafka queue. In this case, for instance, your application can be a subscriber to the kafka topic that you’re writing to, that way, it can know that the message properly propagated. The problem here is that you run into scenarios where you’re continually trying to write to your event tracking database, and the message successfully gets to the kafka queue. Eventually, that event will either be created or rolled back - but a notification has already been sent to that user’s friends even though the user sees this transaction as pending. Additionally, this adds a lot of complicated application logic that you really don’t need to achieve this goal. A cleaner solution is to use the Outbox Pattern in conjunction with change data capture technologies. The general concept of the Outbox Pattern is that in addition to writing to the other tables within your event tracking database in each transaction, you also write to a special outbox table. And instead of synchronously writing to your kafka queue, you wait for the database transaction to commit. You point change data capture to the outbox table and emit the changes to your outbox table and to your kafka queue asynchronously. In the MeeWithMeTomorrow example, instead of just writing to your events and your attendees tables in that same write transaction, you also will write to your outbox table. When that transaction commits, change data capture is listening to changes on that table and will emit this new event to your kafka queue which is then emitted to your notification service. Three benefits of using the Outbox Pattern with Change Data Capture: Life is short - avoid complicated application logic. By using an outbox you have a history of the events that should have been emitted. This allows you to audit what your application is doing, but also, if your kafka server is down, or you run into other issues, you can always replay those events. So you never lose any messages that should have been sent to your notification service. You can tailor the data stored in your outbox table that is ultimately sent downstream to your services in the format that is most conducive to the consumer. This also means that if you change the schema of your events or your attendees' table, you don’t necessarily have to change what is put into the outbox table - this reduces the system interface churn. More Use Cases for Change Data Capture Using change data capture in event-driven architectures or to stream data to a data warehouse are just two examples of use cases in which CDC can simplify and improve your application and data architectures. There are plenty of other use cases in which CDC delivers similar value. A few that come to mind are: Streaming updates of your search indexes Streaming analytics and anomaly detection Streaming updates to your online data stores for your production machine learning models Hopefully, the change data capture examples that I’ve provided here are helpful. If you have any further questions please reach out in the CockroachDB Community Slack . If you’d like to see an example of an existing CockroachDB user with an event-driven architecture check out this case study .", "date": "2021-03-10"},
{"website": "CockroachLabs", "title": "The Fastest Blockchain Transactions Belong To Bitski's NFT Marketplace", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/fast-blockchain-transactions/", "abstract": "We’ve been building for this for three years , said Naveen Molloy, COO of Bitski , a startup company based in San Francisco that has suddenly found itself at the center of the NFT craze. While most of us are just now realizing that fungible is not a microorganism, and that an NFT can sell for $6.6 million , Bitski has been ready and waiting for this moment. They observed the popularity of digital assets years ago in the virtual world gaming industry. They saw the challenges presented by blockchain transactions. And they built a platform to process blockchain transactions faster. CockroachDB Engineer Nathan Stilwell interviewed Bitski’s CTO Patrick Tescher and COO Naveen Malloy to discuss NFT basics, Ethereum, and the NFT marketplace, as well as the challenges of processing blockchain transactions. What is an NFT? Technically speaking, an NFT (non-fungible token) is an entry on the blockchain that is unique, finite, and distinguishable from every other NFT. Bitcoin, by contrast, is fungible. Meaning that it is indistinguishable from the other coins of its kind. When Molloy says that Bitski has been building for this tidal wave of NFT interest for the last three years he means that Bitksi has been building the infrastructure to address the challenge of buying and selling NFTs. A digital asset does not become an NFT until it has been minted on the blockchain. At that point it becomes unique, and can be owned. The challenges in this process are security and speed. Bitski addresses these challenges with architecture that emphasizes Swiss bank-level security and an innovative approach to processing blockchain transactions. Bitski’s NFT Marketplace is ‘Shopify for NFTs’ Originally, Bitksi’s intention was to build all the hard infrastructure of a digital wallet as well as an enterprise management layer for the blockchain. This hard infrastructure is what answers the very difficult question of “So you bought an NFT, now where is it?” It’s in a Bitksi wallet. And it’s safe. Bitski’s expectation was that somebody would come along and build an NFT marketplace on top of Bitksi. But nobody did. There was a clear need for a simple platform to create, sell, and transfer NFTs. So Bitski built a “creator portal” on top of their digital wallet infrastructure. The way they saw it, all the hard work had already been done. The creator portal is just a simple marketplace that allows people to sell their NFTs and use their digital wallets. Exactly the same way that Shopify or Etsy gave people a place to upload images of their work, and then process those transactions. The primary difference between these two endeavors is the blockchain. The Burden of Blockchain Transactions There are many existing NFT marketplaces currently. But they are all struggling with the famous challenge of slow blockchain transactions. It takes at least a minute for any blockchain transaction to go through. This was fine for the original blockchain workloads (like a bank transfer). But if it takes a minute to sell 100 NFTs it will take over an hour to get all those transactions processed. Which creates confusing and unpleasant delays for the end user. On NBA TopShot , the exclusive platform for buying and selling NBA NFTs, you might click to purchase an NFT and then wait 15 minutes or more while the transaction processes. During which time you’re likely assuming that the transaction didn’t work so you need to try again. Or leave. This is the challenge that Bitksi’s business is built around. They observed the way that the early NFT platforms struggled with transactions. So they embarked to build infrastructure that would speed up blockchain transactions and create better end user experiences. To do this, they built a “Blockchain Transaction Operator” that queues up transactions in batches, gathers all the information that each transaction is going to need, and then broadcasts that batch of transactions to the blockchain. Instead of sending one transaction each minute to the blockchain, Bitksi broadcasts 1,000 different transactions at once, which allows the transactions to process extremely fast. If you buy something on Bitski it’s delivered within a minute. The Architecture of a High Functioning NFT Marketplace Before building out their existing architecture, Bitski looked at how other companies were trying to solve the problem of blockchain transactions. They saw a lot of solutions using Kafka to get a rapid transaction queue in place but those solutions couldn’t keep track of all the details required in order to make the system work. Bitksi first built their solution using etcd which allowed them to keep track of some of the state for future transactions in a single place. Etcd created it’s own complexities and Bitski didn’t need it anymore when they migrated from Postgres over to CockroachDB . In CockroachDB they have a couple of indexes designed with everything that blockchain transactions need. Bitski dumps transactions into CockroachDB. In the process Bitski is able to validate whether or not the transactions are going to go through on the database level. So they can create tokens as fast as CockroachDB can create them, and then a task in the background relays created tokens to the blockchain. Bitski’s NFT marketplace built on CockroachDB has proven to be an order of magnitude faster than what everyone else is doing. For more detail about Bitski’s NFT marketplace architecture you can skip to this section of their interview with CockroachDB or you can read the formal Bitksi case study . Navigating traffic & transaction spikes during “Drops” In the NFT industry, assets are usually released together in what’s known as “a drop.” Buyers are given a notification that certain items will “drop” in 72 hours, or 15 minutes, or some other relatively imminent moment. Once the drop begins, every NFT in the drop will sell out in a matter of minutes or even seconds. What this means is that hundreds or thousands of transactions are running every second. For most platforms it can take two or three days for them to catch up to all the transactions in those tiny windows. Bitski is all caught up within an hour or two. In addition to the high blockchain transaction speed that Bitski’s architecture creates, it is also highly consistent and highly available. High availability is essential for Bitski because downtime during a “drop” would mean missed revenue for the NFT creators and a confusing experience for the NFT buyers. This is another reason Bitski chose CockroachDB - the database does not go down. Bitski initially built their application on top of Postgres where they experienced planned and unplanned downtime. Since switching to CockroachDB neither of those have been an issue. Database Performance Expectations: 100ms Writes CockroachDB serves as the general purpose database for Bitski. One of the most important workloads that it handles is the login experience. Logins should happen in under one second in order not to lose users before they even get onto the platform. Around the time of a “drop” there are thousands of queries happening in the login flow. The database needs to deliver low write latency because the entire login experience needs to happen in 100ms or less. 100ms is the cutoff for something to feel ‘instantaneous.’ This is often called the 100ms rule . Bitski uses Hydra for authentication - which supports CockroachDB natively. Hydra gives Bitski an access token, which gets queried a lot. When Bitski was setting up their second data center they had an issue with read query latency being too slow. They filed a support ticket with the CockroachDBSupport Team and got help setting up a Secondary Index , which lowered the latency from 70-millisecond read queries to 0.5 milliseconds. The Future of NFTs & Blockchain This surge of NFT popularity is familiar. It’s impossible not to associate this craze with the original buzz around Bitcoin. Bitcoin endured plenty of doubt and criticism, but has still reached extraordinary value. Will NFTs have the same kind of endurance? At Bistki, they’ve been planning for this for years. And they see the blockchain and NFTs as an inevitable evolution in the way that digital assets are bought and sold. Coinbase will be the platform for currency. And Bitksi will be the platform for NFTs. One of the primary blockers for this evolution has been the confusion around what blockchain is and how it works. Bitski thinks this blocker will dissolve when people start to go through the experience of making NFT purchases. In the words of Molloy, Nobody understands how Netflix works, but it doesn’t stop them from enjoying the streaming experience. When you buy a plane ticket you don’t need to calculate how much money is needed for gas for the flight. That shouldn’t be a requirement for transactions on the blockchain either. In a few years blockchain won’t be the topic anymore. People will just buy and sell and trade and not think about the underlying infrastructure.", "date": "2021-03-09"},
{"website": "CockroachLabs", "title": "Just How \"Global\" Is Amazon Aurora vs CockroachDB?", "author": ["Jim Walker", "Sean Loiselle"], "link": "https://www.cockroachlabs.com/blog/just-how-global-is-amazon-aurora/", "abstract": "Many databases -- including Amazon Aurora and CockroachDB -- claim to be \"global.\" While there is no official definition of the term \"global database\", it deserves to be unpacked. For those who prefer to watch or listen, we explore Aurora's architecture indepth in the webinar: \"CockroachDB vs Amazon Aurora: Battle of the Cloud Databases\" . What is a Global Database? A global database promises global capabilities. At a bare minimum, it should satisfy these three requirements: Global availability : Leverages geographic diversity of nodes to keep data available. It should be always on and resilient to facility failures and natural disasters. Global performance : Supports policies to replicate data close to where it is most often accessed (reads & writes) to achieve optimal global latencies. Global consistency : Maintains consistency even in a globally dispersed deployment. CockroachDB is a global database built from the ground up with global availability , global performance , and global consistency in mind, while others - like Amazon Aurora - have tried to shoehorn these similar capabilities into legacy technology. This post will walk you through a review of Amazon Aurora's approach to a global database compared to CockroachDB , with a focus on how they fare across these three requirements. While Aurora may deliver some global-like capabilities, the application of the word “global” is applied too loosely to conform to the definition. Here’s what that means for your deployment. What is Amazon Aurora? Amazon Aurora is a cloud-based relational database engine that combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora may call itself a \"global database\" in its marketing materials, but Amazon Aurora is a mostly an option for single region, deployments. It's optimized for read-heavy workloads when write scalability can be limited to a single master node in a single region. No matter how it's deployed, it will suffer latency issues with global writes as a write node (single or multi master) is always tied to a single region. Aurora uses the standard MySQL and PostgresSQL execution engines and layers in a distributed storage system that provides them with scale and resilience.. With a standard instance of Aurora you have a single write node that handles all writes into the database. You then deploy a number of read nodes to meet your read throughput requirements across multiple availability zones within the region. Aurora accomplishes this by writing to a custom, distributed storage layer, instead of locally to attached persistent storage. This layer is comprised of SSD storage devices across multiple AZs. It writes 6 copies of your data across, two copies in each of three availability zones. Each transaction is committed when 4 of the 6 replicas have been committed. While Aurora still uses the MySQL or PostgresSQL execution engines, this new approach reduces read latencies and also provides redundancies that make it more resilient than a traditional monolithic architecture with attached storage. Is Amazon Aurora Really a Global Database? In 2018, Amazon introduced Aurora Global Database, which takes its original single region architecture and allows you to extend read replicas from a single region to two regions. This allows expanded read coverage with reduced latencies across geographic distance to one additional region. It employs asynchronous replication at the storage layer to keep the two regions in sync. The primary instance contains the write node, while secondaries are comprised of only read nodes so you only scale reads to a new region. This configuration is constrained to duplicate the entire storage layer to the additional region, and looks like this: On failure, Aurora Global Database allows one of the read-only database nodes in the secondary region to be promoted to the primary write node, allowing business continuity in the event of a regional outage. However, because the replication between regions is asynchronous, there is the potential for data loss of up to a second (i.e., a non-zero recovery point objective – RPO), and up to a minute to upgrade the read node to primary write node (i.e., 1 minute for the recovery time objective – RTO). Both of the previous two configurations rely on a single write node for the database. This implies a single point of failure which introduces failover risk and a significant recovery time objective (RTO) in the event of failure. Additionally, this single endpoint has significant write scalability limitations and global applications must endure the network round trip to the region where the write node is located in order to execute an operation. These geographic latencies imply a significant performance impact on users outside the region where the write node resides.This is a huge issue for global performance. What is Amazon Aurora Multi-Master? Amazon Aurora Multi-Master's offering allows for a second write node, but disallows read replicas. It is available in eight regions. It’s only available for MySQL, and in its current incarnation, can only be deployed in a single region (an issue for global availability and performance). Multi-master doubles the maximum write throughput at the expense of significantly decreased maximum read throughput. In fact, reads can only be completed through these two nodes and it requires you to think through which endpoint you want to query within your application. This also means, there is no option to scale reads to an additional region. The diagram shows the max deployment for the current release. The AWS documentation points out additional limitations of their multi-master product, as well as use cases that this version of Aurora is good for. Aurora multi-master does not allow for SERIALIZABLE isolation. While Aurora continues to improve, its architecture is decades old and was not designed to support scalable inter-regional operation. Aurora’s architecture will likely limit multi-master to be single region forever, as it requires communicating with all write nodes during every read to avoid staleness. The cost of providing consistency increases with the addition of each additional write node.  Adding support for additional regions to the multi-master architecture would increase complexity and the introduction of geographic latencies would significantly impact performance. Does Amazon Aurora Provide Global Availability? \"Global Availability\" requires that the database leverage geographic diversity of nodes to keep data available. It means more than just two regions around the planet. Let's evaluate the set of Aurora product offerings in light of our three requirements for global operation defined above. First and foremost, Aurora is hardly “global” as it (currently) only allows you to extend the secondary read replicas into only one additional region, however they intend to open this to additional regions. Also note that when you spin up the second region, you incur charges for replicating data to this new region. So, with Aurora you have to carefully select which regions you deploy in to ensure coverage with reasonable latencies and watch your costs. This architecture also leaves room for extended RPO/RTO. When a write node fails, Aurora will promote a read node to be a write node and this takes about a few minutes to happen. Non-zero RPO means data loss. If that’s not acceptable for your workload, Aurora isn’t acceptable for your workload. Also, an RPO measured in minutes is acceptable for some workloads but not all. However, when a region fails, there is additional time added to the RPO clock. If an entire region fails (a rare occurrence, but possible), Aurora is smart enough to make sure a new write node is spun up in the secondary region and promote it to primary. This process takes minutes and during this time you have an unknown state of your data. For the time you're offline, you lose data. Once you're back online with the primary, you'll need to sync data and clear issues. Does Amazon Aurora Provide Global Performance? Two issues plague Aurora when it comes to providing global performance.  First, as noted above, they depend on a single write node for all global writes and second, they can only extend read replicas to one other region. (Multi-master is not acceptable to consider as it is only two endpoints and single region) This architecture stresses performance when it comes to physical latencies. Amazon Aurora performs well in a single region and is durable to survive failures of an availability zone. However, there can be latency issues with writes because of the dependence on a single write instance. The distance between the client and the write node will define the write latency as all writes are performed by this node. Let’s consider a deployment where you have an instance of Aurora running in Richmond. For users on the east coast they will experience low latency read/writes however across the country in LA, we can expect at a minimum up to a 70ms latency. With Aurora, we could extend it to another region so that we have better read performance across the two regions, however, writes will still be single threaded and incur the same performance expectations. It might be good enough, but what happens once you go global? If we locate the write instance in Richmond, can we cover users in Sydney with acceptable write latencies? Finally, in order to truly provide global performance, you will ultimately need your data to live closer to where it is needed. Even with a truly distributed database, this is incredibly important as you try to attain millisecond read AND write latencies. And having some mechanism in the database to anchor data to a particular location becomes important for complex regulatory and varied compliance requirements that are present in a global implementations. Aurora does not provide this capability. Does Amazon Aurora Provide Global Consistency? Global consistency requires that the database maintain consistency even in a globally dispersed deployment. Aurora drives consistency of data through their storage layer which ensures quorum by writing 4 of 6 replicas to disk. They also rely on isolation levels that can be set in the write node as part of standard MySQL and PostgresSQL execution layers they have commandeered. By default, they provide READ COMMITTED (PostgresSQL) or REPEATABLE READ (MySQL) isolation which could present issues with write skews and dirty reads. Further, replication to a secondary node is “typical latency of under a second”, so there is always risk of a dirty read regardless of the platform, even with serializable isolation on. As noted above, AWS has added a multi-master capability but if you choose that type of deployment, you lose all the other benefits of Aurora. Further, they do not offer serializable isolation within this deployment type and again, it will incur performance hurdles. Finally, What is the Cost and Performance of Amazon Aurora vs CockroachDB? Beyond scale, performance and consistency, it is also important to consider the cost of a global database . In order to get cost numbers, we have executed some benchmarks against various instance types of Aurora. Benchmarking is a tricky topic but one that we at Cockroach Labs take seriously. We go to great lengths to make sure that what we measure is reasonable and aligned with some public facing standard. Internally, we rely on a few benchmarks and the TPC-C benchmark is one of them. And while no benchmark is perfect, we hope this is at least directionally correct. We are always open to comment on these and highly suggest you benchmark these solutions for yourselves. There’s a low ceiling on what Amazon Aurora can handle for OLTP workloads.  We ran a TPC-C like workload on the smallest machines that could achieve 85% of the maximum TPC-C throughput, with these results:. TPC-C 1k Results Database Isolation Level Machine Type $/tpmC CockroachDB SERIALIZABLE 3x Google Compute Engine n1-highcpu-16 with attached Local SSDs $3.98 Aurora MySQL REPEATABLE READ db.r5.4xlarge $7.27 Aurora MySQL SERIALIZABLE db.r4.8xlarge $11.49 Aurora PostgreSQL READ COMMITTED db.r4.2xlarge $20.08 TL;DR Not only does CockroachDB offer the lowest price for OLTP workloads , it does so while offering the highest level of consistency. To get a better understanding of the numbers, check out this blog post about the price of running OLTP workloads in CockroachDB and Aurora. In short, the CockroachDB cost includes 3 machines, as well as a DBA's salary to monitor and maintain the cluster. For this comparison, we began by simply adapting our TPC-C suite to leverage both Postgres and MySQL compatible APIs, without modifying their isolation levels ( REPEATABLE READ for MySQL; READ COMMITTED for Postgres). Interestingly, Aurora's PostgreSQL compatible databases were able to run on the smallest machines, which lead us to assume they'd be the most cost efficient; however, when we observed that the Aurora billing was largely dominated by I/O costs. We believe the implementation of the Aurora Postgres interface doesn't seem to have received the same kind of optimizations that Aurora MySQL implementation has. In fact, 90% of the cost of running TPC-C on Aurora PostgreSQL was I/O related. So, despite Aurora MySQL requiring larger machines, at the default isolation level, the cost was much lower than PostgreSQL (although still much higher than CockroachDB). The previous examples relaxed isolation levels within Aurora. So, we ran our test suite against Aurora MySQL to complete TPC-C 1k at SERIALIZABLE isolation as this is the default level of isolation within CockroachDB. Some feel higher isolation levels will unequivocally destroy your performance. However, with Aurora, throwing more processors at the problem solves it. The downside to this, though, is that it dramatically increases the amount of I/O required. It seems the culprit is the Aurora MySQL deadlock mechanism which continually causes transactions to have to restart, which results in a substantial increase in activity going back to disk (i.e. consuming I/O). Ultimately, we are not Aurora experts, so the above should be consumed with caution, however, it does outline some of the key issues and we wanted to provide a tangible and reasonably pragmatic comparison of not only the architecture but also the costs of Aurora. Two Essential Constraints for a Global Database A true global database--that is, one that’s globally available, performant, and consistent--shouldn’t have to fight against 6-way replication. It should offer serializable isolation, low latency, and non-zero RTO. A true global database doesn’t battle against the constraints of its architecture. It battles against two facts of life: Everything will fail, and you need to prepare for that The speed of light is constrained. If you’re shopping for a ‘global’ database that’s sacrificing performance, availability, or consistency for anything but these two bullet points, you need to re-evaluate whether it’s really as ‘global’ as it claims to be. CockroachDB was conceived of as a global database from the very beginning and we have a very deliberate and sharp focus on these two challenges. Every node is a consistent gateway to the entire database and provides both read/write access while guaranteeing reliable performance and serializable isolation at any scale.", "date": "2021-03-17"},
{"website": "CockroachLabs", "title": "From Batch to Streaming Data: Real Time Monitoring with Snowflake, Looker, and CockroachDB", "author": ["Abbey Russell"], "link": "https://www.cockroachlabs.com/blog/from-batch-to-streaming-data-real-time-monitoring-with-snowflake-looker-and-cockroachdb/", "abstract": "Batch data sucks - it’s slow, manual, cumbersome, and often stale. We know because we’ve dealt with these problems ourselves. Any business needs to track metrics, from customer activity to the internal workings of the company. But how can we keep those metrics up to date to extract maximum business value? Here at Cockroach Labs we build CockroachDB, a distributed database meant to survive everything and thrive everywhere. CockroachDB is primarily optimized for transactional- “OLTP”- data, and sometimes it is advantageous to stream that data to an analytical warehouse to run frequent, large queries. That’s what we do with our “telemetry data” – the data we collect internally on product usage. Telemetry data, anonymized and captured only from those users who do not opt-out, helps us track feature usage and make product decisions. Telemetry helped inform the decision to move distributed backup and restore to CockroachDB Core. It has also informed our use of free trial codes for CockroachCloud dedicated clusters in places like our Cockroach University classes. We’re a fast-moving startup, and having this data up-to-date and easily available to stakeholders is key for us to make the best product decisions for our users. That’s why we moved from batch uploading our telemetry data to streaming this data using changefeeds . Our original approach: Weekly cron jobs, batch updates, and Snowflake We host telemetry data in an internal production system that runs on CockroachCloud , our managed database-as-a-service. To avoid borking the cluster every day by pulling the latest data from multi-TB sized tables, we previously stored an extra copy locally and batch uploaded it to Snowflake weekly. This approach had many shortcomings. For one, it relied on a weekly cron job or a nudge of “Hey Piyush, can you update the data for me?” to keep the data up to date. This process proved fragile, unreliable, and took more time and energy than it needed to. The data was also not kept as up-to-date as we would have liked. Frustrated with this process, we turned to using one of our native database features to give us faster, easier, and more reliable data updates: changefeeds. Streaming data out of CockroachCloud using change data capture (CDC) CockroachDB’s change data capture (CDC) process watches a table or set of tables, picks up on any changes to the underlying data, and emits those changes via a changefeed. The changefeed streams the updated rows in near real-time out of the database to an external sink (i.e., Kafka or cloud storage ). For example, our product team uses the table FEATURES to keep track of how often features are used. When a row is added to the FEATURES table logging usage of the SQL command INSERT, the changefeed will pick up on the added row and emit it to an external system. The message will look something like this: {\n\t{\n\t\t\"__crdb__\":{\n\t\t\t\"updated\":<timestamp 1>\n\t\t},\n\t\t\"feature\":\"sql.insert.count.internal\",\n\t\t\"internal\":false,\n\t\t\"node\":1,\n\t\t\"timestamp\":<timestamp 2>,\n\t\t\"uptime\":2106051,\n\t\t\"uses\":100,\n\t\t\"version\":\"v19.1.2\"\n\t}\n} This is a simple concept but allows us to build powerful applications. An application can take advantage of the transactional guarantees of CockroachDB while ensuring other applications that rely on the information are kept up-to-date in near real-time. Change data capture can be a key component of microservice architectures; it can also be used to keep auditable data logs. The changefeeds native in CockroachDB allow us to stream data automagically and in near real-time out of CockroachCloud to a cloud storage sink and then to Snowflake. By using changefeeds to stream data to analytical tools, we take advantage of purpose-built solutions for internal analytics while maintaining the high performance of the underlying database. Making the switch to using changefeeds has added robustness and reliability to our process: data streaming isn’t interrupted by vacation days, sick days, or home internet connectivity problems. It has allowed us to go from weekly to daily or near real-time metrics, allowing us to diagnose problems earlier. For example, on two occasions we noticed dips in our Net Promoter Score (NPS) due to stale versions of CockroachDB being published as the most recent version to tools like Homebrew or Docker. With streaming analytics, we can now alert on our NPS score and see version breakdowns immediately rather than a week later. We can resolve issues like this quickly. Ultimately, this change has freed up time so that we can focus on what really matters: creating value for our users. How we did it The journey of our data looks something like this: We first use an AWS S3 bucket as a general-purpose data dump. Cockroach supports S3 as a native sink for changefeeds. S3 buckets also support auto-ingesting of data into Snowflake, making S3 an ideal dump for the changefeed data in this case. We then load the data into Snowflake, using Snowpipe. Snowflake automatically picks up on the diffs of the data, and auto-ingests the changes. If you have workloads with a large amount of updates (vs append-only) you’ll need to do some preprocessing of the data before sending the data to Snowflake. Though we use Snowflake, you can use the OLAP database of your choice. Our Looker instance reads from Snowflake as a data source, aggregating the data and displaying it in dashboards. Looker is a purpose-built business intelligence tool that makes it easy to build beautiful dashboards out of complex data queries. Example Dashboard from Looker docs Chaining these tools helps us get the best out of each application and makes our overall data stack more powerful. Using changefeeds to stream data updates has led to more granular, up-to-date dashboards that we use to drive product decisions. As a startup, we need data to move fast and innovate. Additionally, the ability to quickly identify and fix problems is invaluable, since negative impressions can deal a fatal blow to a nascent company. When stale versions of CockroachDB were published to package management tools, it gave the impression that our product underperformed and was potentially less stable than expected. Mistakes will happen. Reducing the costs of mistakes like this is key to our long term success. Tutorials/Resources The best place to start to explore doing something similar with your CockroachDB cluster is our tutorial on streaming changefeeds to Snowflake . You will be taken step-by-step through the process of auto-ingesting database changes into Snowflake. From there you can connect Snowflake to Looker , Tableau , or your business intelligence software of choice. Note: This tutorial uses enterprise changefeeds to stream to a configurable cloud storage sink. Core changefeeds will not be able to natively connect to a cloud storage sink.", "date": "2021-03-16"},
{"website": "CockroachLabs", "title": "How to Create an Inclusive Environment at a SaaS Startup", "author": ["Evan Atkinson"], "link": "https://www.cockroachlabs.com/blog/crews-employee-support-groups/", "abstract": "During my first week at Cockroach Labs, our company held its annual Roacherness Awards, a ceremony to recognize and celebrate Roachers who embody our values. I was pleasantly surprised when my colleague Chelsea received an award for our value ‘Establishing Balance,’ celebrating the boundaries she sets between her work and home life through using benefits like our flexible Paid Time off policy to explore the world. This being my second job out of college, I was shocked. My experience post-college was one in which companies asked me to dive into my work with cult-like abandon, to define myself by the goals of the company first, and to let who I am come second. From my experience at Cockroach Labs, it's so apparent to me now that we should all be valued at work for more than what we do— we should also get to be who we are. Cockroach Labs has many of the markings of a tight-knit community - a culture of open communication, collaboration, and respect. Still, it is ultimately a company of incredibly talented people who show up each day to build something fantastic together, but who are defined by so much more. Even more surprising was that while we emphasize encouraging our people to establish balance in their lives, we are equally committed to providing every opportunity for Roachers to authentically represent themselves at work in a community that respects them for it. When I joined the team in December, we had about 130 Roachers (now over 165), and well before I arrived, we had established the CREWS program, which other companies call Employee Resource Groups. CREWS (Cockroach Employees Who Support) are groups intended to create an inclusive environment for Roachers from underrepresented backgrounds and help employees connect with their peers on a deeper level. Groups host events to support and advocate for their members, both internally and externally. These include talks with guest speakers, reading groups, member lunches, and events to provide the broader company with meaningful education and resources. While these groups are company-sponsored, they are run by employees for employees. Some of our current groups include: Women of Cockroach Labs - a community of women and allies at CRL that promotes professional and personal development Roacher Pride - a group dedicated to celebrating pride and educating the broader company on how to be an LGBTQ+ ally at work and in the community POC-at-CRL - a community of people of color at CRL that promotes professional development and belonging Infinite Colors - A group dedicated to raising awareness and sharing resources around neurodiversity in the workplace An effective Employee Resource Group allows employees to feel a sense of belonging and educates the company at large on perspectives and experiences that may differ from their own. These groups are spaces for collective learning and collective action and are a massive contributor to the culture at the core of Cockroach Labs. In June of this year, the CREWS program evolved to add executive sponsorship and expanded company support for group initiatives. Each group is assigned a member of our executive team to serve as an advocate at the executive level while also providing visibility and recognition for their group’s efforts. Additionally, an introduction to the program was added to our onboarding curriculum so that new Roachers can make closer connections at a growing company from day one. CREWS groups are also welcome to allocate Connect & Contribute (C&C) time for their groups to make space to engage in an event, workshop, or an issue within their communities. These are times when managers are encouraged to move or cancel meetings to allow group members to support their group’s mission while providing the rest of the company with resources and means to interact. As our team grows, our CREWS will grow with it. If you are thinking about joining Cockroach Labs (and you should, it’s fantastic over here) and don’t see yourself represented in our current set of CREWS groups, start your own! My job as Culture and Workplace Associate is to make sure that everyone here can bring their fullest selves to work, and almost ironically, remember that they are infinitely more than just this job. To join Cockroach Labs and our CREWS, visit our careers page .", "date": "2021-03-20"},
{"website": "CockroachLabs", "title": "A Vue.js, Firebase, & CockroachDB App that Makes Mentorship Accessible", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/a-vue-js-firebase-cockroachdb-app-that-makes-mentorship-accessible/", "abstract": "The current mentorship model is broken. It requires you to have the privilege of belonging to an established network (like a renowned university) or ‘cold call’ potential mentors on a platform like LinkedIn. Even after you find a mentor, it’s difficult to sustain a mentor-mentee connection. This is the challenge a group of students from Lassonde School of Engineering at York University set out to resolve through their Hack the North 2020++ app: mntr.tech mntr.tech is a CockroachCloud application built with Vue.js and Firebase that uses an advanced matching algorithm to get the best fit between mentors and mentees while letting them have the final say in the match. We invited the team for a live-streamed chat with Jordan Lewis, Engineering Manager at Cockroach Labs. Here’s what we learned. Meet the team The DevOps and backend for the app were developed by Allen Kaplan, a fourth-year engineering student, and a full-stack developer. Amuleek Simak (a second-year engineering student) worked on the backend with Allen, helped implement the endpoints, and created a working backend using the valid requests. Aman Chhina (a third-year engineering student) and Harpreet Janday (a first-year engineering student) built the frontend with Vue.js and Nuxt.js. The tech stack Frontend: Vue.js and Nuxt.js Backend: A Go program on Google Cloud Run Auth: Firebase Database: CockroachCloud See source code at https://github.com/hackthenorth2020 How it works The app’s front end is written in Vue.js on the Nuxt.js framework for routing and state management and is deployed on Firebase hosting. Firebase auth is used to manage user accounts. When a user interacts with the app, their token is sent in the request header as a JWT Bearer token. The backend Go program validates the incoming token using the Firebase Admin SDK and processes the business logic before sending requests to the CockroachCloud database. Why Vue.js, Nuxt.js, Go, and Firebase While selecting the tech stack, the team did an excellent job playing to their strengths and learning new tech. Since Allen was teaching the team from scratch, he wanted to go with something he was already comfortable with. He finds Vue.js intuitive and can easily put together a frontend thanks to the many Vue.js libraries. Aman had experience with React and found it easy to transition to Vue.js. Harpreet found it easy to pick up Vue.js as well. Harpreet and Aman ran through Vue and Nuxt examples to get familiar with the syntax and not get bogged down with syntax errors during the hackathon. They chose Nuxt.js for its state management features. They used server-side rendering for the SEO benefits it provides. Nuxt.js also has a Firebase package that integrates with Vuex right out of the box. The team chose Firebase for user authentication since it’s the easiest way to manage auth for beginners. Why CockroachCloud CockroachCloud is the managed database-as-a-service offering of CockroachDB. The newly announced plug-and-play forever free CockroachCloud clusters made it an attractive choice for the team. The team took the Cockroach University Getting Started course in preparation for the hackathon. As the team’s DevOps person, Allen found it quite easy to set up the CockroachCloud cluster and just share a link to the cluster with his team. To quote Allen, “Cockroach really solves a lot of those pain points, especially on a small project where you don’t want to waste configuring your database.” Things to look out for when using CockroachDB Make sure you are setting up certificates correctly Harpreet shared his experience of running into an auth issue with certificates. Editor’s note: Certificate setup can be a tripping point for folks new to CockroachCloud. A little trick here is to set sslmode=require in your connection string to avoid the certificate setup. For example, if your connection string is postgres://<username>@<hostname>:<port>/<dbname>?sslmode=verify-full&sslrootcert=<your_certs_directory>/cc-ca.crt&options=—<clustername> replace it with postgres://<username>@<hostname>:<port>/<dbname>?sslmode=require&options=—<clustername> Beware this makes your cluster susceptible to man-in-the-middle attacks. It might be okay for hackathon projects, but DO NOT use this option in production. Be careful when batch-inserting data Harpreet also shared the incident when he tried to insert 70,000 rows into a table and crashed CockroachDB. Editor’s note: Use multi-row INSERT statements for bulk inserts into existing tables Beware of the free tier’s limitations (for example, CSV Import isn’t supported yet) Tips for first-time SQL users For rapid SQL development, use tools like SQL Fiddle to test table creation, inserts, and updates. Editor’s note: You can use the interactive CockroachDB playground to test your schema and queries. Save the SQL Fiddle statements and queries in a separate file just in case you lose the SQL Fiddle ones. Run SHOW CREATE TABLE to understand what your table looks like, especially if you run ALTER TABLE statements to ensure the resultant table schema is as you expected. Allen prefers writing SQL without using ORMs. To quote Allen, “It definitely can help you with development speed but a lot of times at hackathons, what you’re doing isn’t complex enough to end up going through the difficulty of learning an ORM”. Get a head start at your next hackathon Before the hackathon, figure out what tech stack you want to learn and run through some examples to get familiar with the syntax to avoid getting bogged down with syntax errors during the hackathon. Since all hackathons are now virtual, pre-hackathon workshops are the norm. Make the most of these workshops to prep for the hackathon. Editor’s note: Before your next hackathon, complete the Cockroach University Getting Started course and earn some cool swag! Curate a good mix of experience and expertise in your team. Consider pair-programming and live-sharing for peer-to-peer mentorship. Editor’s note: Online hackathons are difficult (thanks, COVID). We know you want to push yourself and compete for the sponsor prizes, but remember to be kind to yourself. Hydrate, eat well, and get some sleep. We are rooting for you! Check out the full discussion here: https://youtu.be/kKqWgDAnPLY For more “ How We Built It ” streams, follow us on Twitch !", "date": "2021-03-19"},
{"website": "CockroachLabs", "title": "Aurora Builds Payment Acquiring Solution with CockroachDB on Kubernetes", "author": ["Derek Maxwell"], "link": "https://www.cockroachlabs.com/blog/payment-solution-architecture/", "abstract": "Imagine you show up at your favorite cafe and its credit card terminal is down. What happens? Most likely, you don’t have any cash on you. Instead, you go to the next cafe and order from them instead. We never want this to happen to one of our customers, so that’s why we chose to build our infrastructure on CockroachDB. At Aurora Payments , we make sure that small and medium sized businesses (SMBs) get paid quickly and with low fees. We integrate with their point of sale (POS) technology and business management software. We learn all the nuances of their business so that we can process and deliver payments exactly the way our customers need them. We know that all of our effort to deliver exceptional payment solutions would be meaningless if we didn’t have a database that is always on - and always available. Why Aurora Payments Chose CockroachDB In some sense, I didn’t really choose the payment solutions industry, it chose me. I was running a cloud software delivery business when my largest client, who just happened to be a payment processor, asked me to join their firm as CTO. That company was acquired by Aurora in 2018, giving us access to capital to build out a powerful technology ecosystem. When I got into the business, I realized that we had a chance to really make a positive impact on peoples’ lives. I’ve attended well over one hundred industry events and trade shows during my time here, allowing me to connect directly with the merchants that we serve.  I have never seen a group of customers who are so emotional about their payment partner, especially when they are bombarded by inbound sales calls from competitors. What I began to understand is not only how critical payment processing is for any business, but how a trusted relationship combined with proprietary technology integration can facilitate transformational change, even in a small family-owned business. I viewed this as a great technical opportunity and, frankly, a huge responsibility to do right by the people that chose to work with us. Merchants trust us with delivering their money, which can mean the difference between making payroll and not. Our sales partners trust us to deliver their residual income payments, to service their clients, and to provide powerful data in real time. However, in order to build a next generation payments solution for SMBs, we needed a database bringing strengths that aligned with the priorities of the payment solutions industry and with our vision for the future of Aurora’s service: • Always on and always available - No customer of ours will ever lose money because their payment solution was unavailable. • Strong consistency - We won’t ask our customers to deal with a confusing system of eventually consistent transactions. We’ll get it right the first time. • Kubernetes Compatibility - We were moving to container orchestration and wanted a database that fit well with k8s. • Horizontal scale - Our intention is to grow across in all of the United States. In my experience you’re never too small to pick a scalable solution early on because the cost of changing up later  down the road is greater than just choosing a scalable solution from the start. High Availability & Scale for Payment Service Workloads We evaluated other databases beginning with the database we were already using: Postgres. On a single server, I think the performance of Postgres and CockroachDB would be pretty comparable. But the magic of CockroachDB happens when you start to add nodes and distribute the data across those nodes automatically. The problem with Postgres is that it doesn’t scale well natively in a geo-redundant configuration with an active/passive HA setup. If you try to launch a Postgres instance in NYC and another in Los Angeles, you really have to choose which one is going to be active. The failover process can be somewhat smooth, but it won’t be a sub-second switchover. At Aurora we want to plan for the times when there’s a site outage that’s completely out of our control - because we know it’s going to happen . And when it does, we need to have a database that will allow us to continue processing payments without skipping a beat. SQL vs NoSQL in Financial Services Any database that offers eventual consistency is not an acceptable solution for us. We need an accurate status for every transaction. We can’t run someone’s credit card twice - that’s bad customer service. People have finite credit limits and finite sums of money in their bank accounts - so we can’t risk running duplicate transactions and causing problems for them and our vendors. In my mind, the only viable solutions for online transaction processing are the relational databases like a traditional Postgres on-prem solution or the distributed SQL databases like CockroachDB - which gives you the same consistency as Postgres, but also gives you the scalability and redundancy that we wanted. The consistency strength of CockroachDB is outstanding, as it guarantees serializable transactions, which is the highest isolation level guaranteed by the SQL standard. This is a new class of database that’s become known as NewSQL or Distributed SQL . Critical Data Stored in CockroachDB Currently, CockroachDB serves as our System of Record database (and will eventually become our general-purpose database) and for that reason it stores essentially all of the data required by our customers and our internal users. This includes critically private data which is subject to external audits and vendor risk assessments, all of which is housed in CockroachDB. Then there’s all the point-of-sale transactional data that you would expect. It gets interesting when you delve into the line level data or behavioral data. For example, if you order the double-tall nonfat latte at 8:00am on Tuesday and you tip 18% - we’re storing all that behavioral data. This has opened up new opportunities for us and our vendor partners. CockroachDB Helps Unlock New Revenue Streams Prior to my arrival at Aurora, the company was essentially a reseller of other companies' payment solutions. Now that we’ve begun building our own payment ecosystem, we have access to a wealth of transaction data. And data, as you know, is powerful. Now, in addition to our payment solution business, we are also building a customer loyalty business. Marketing and payment services are traditionally thought of as distinct from one another. But they absolutely fit well together. We can use the drink order, drink time, tip amount, and other behavioral data to help our customers send the right promotional email or text, at the right time, to the right people. And we can create a more seamless experience for the customer in the cafe by pre-loading their preferred tip amount and email preferences. It’s worth noting that having all this data at our fingertips also makes it much easier for us to understand our cash flow. We make money when our customers are processing payments. Conversely, if a customer is forced to close, or has a bad month, we feel the effects as well. Real time visibility into the data, particularly during the challenges of the past year, has been extremely valuable to us. Payment Solutions Application Architecture with CockroachDB A simple overview of our architecture and tech stack looks like this: • Front end is React • Back end is .Net Core C# • Database: CockroachDB (still migrating some workloads off of Postgres) Debian for backend and front-end webservers A combination of load balancers managed by Google and based on HAProxy • Kubernetes • Hybrid-Cloud: Google Cloud & two co-location facilities (We’ll never be 100% public cloud or 100% private). We have private interconnection capacity between each of our datacenter facilities and Google Cloud, which means our internal traffic never traverses the public Internet. How CockroachDB gives Aurora a Competitive Advantage My observation of our competitors is that most of them are running on legacy database systems like MySQL, standalone Postgres, or Microsoft SQL Server. One of our competitors that is many times our size, has built its service, reporting, and analytics platform on a SQL Server backend. We’ve noticed that there are times of the day when it can be almost completely unusable. This leads to a poor experience for the competitor’s sales partners, merchants, and employees. We are also aware of another player in the industry who has attempted to solve for the availability issue by building its database on a public cloud’s native mySQL offering, only to discover during one particularly disruptive cloud outage that not only is their cross-regional database not an immediate failover, but they are now heavily vendor-locked-in and are unable to leverage a multi-cloud strategy. By using CockroachDB, we’ve removed the database from being a bottleneck for performance or availability. The database should not be a bottleneck in the first place, but legacy systems were not built for modern applications. Choosing CockroachDB makes us more advanced than the vast majority of the financial services industry, even firms many times our size. Everyone is capable of spinning up thousands of front-end servers (and paying lots for it) or processes to deal with requests - but not very many people consider fixing the core problem: the database itself. They just use what they know - and deal with the bottlenecks. There’s a productivity advantage as well. Because CockroachDB is wire-compatible with Postgres , it speaks SQL and makes the migration process fairly easy. CockroachDB eliminates a lot of the manual labor that I’m accustomed to. The ability to spin up a non-production cluster with just a few lines of code is really impressive. Traditionally, that just isn’t how databases work. There’s supposed to be some kind of challenge or struggle that you have to conquer manually! My development team members had to acquaint themselves with the architecture of CockroachDB to fully understand its potential. Once they saw the ease of use and power of the database, they got comfortable with it. When we start to add nodes and distribute data, then we’ll really reap the competitive benefits of choosing CockroachDB. What’s next for Aurora Solutions & CockroachDB First, we need to get all of our transactional workloads off of Postgres and onto CockroachDB, and they are in the process of being moved now. We also will be migrating our initial bare metal clusters of CockroachDB fully onto Kubernetes because we were impressed with how easy it is to manage data that way. The next step will be to continue to build the RISE family of payment processing products, which are all workloads that will run on CockroachDB. At this point, CockroachDB will become our general-purpose database. To be frank, I’m very excited about it because I think it will help our customers a great deal and, to circle back to the beginning of this story, that’s what makes this work meaningful to me. We can make a positive impact on the lives of small and medium size business owners by not only saving them money, but also offering a trusted partnership for delivering payment solutions. CockroachDB helps us do that.", "date": "2021-03-23"},
{"website": "CockroachLabs", "title": "How Engineering Internships Work at Cockroach Labs", "author": ["Waverly Heurtelou"], "link": "https://www.cockroachlabs.com/blog/how-eng-internships-work/", "abstract": "When building out our internship program at Cockroach Labs, we wanted to be intentional about how we would support up-and-coming developers, bring in fresh ideas, and create a funnel of amazing new grads to Cockroach Labs. It’s been five years since we hosted our first interns at our headquarters in New York. As the company has grown, so has our engineering internship program. Our general philosophy around interns may be different from other technology companies: we assign real work that applies to the next releases of CockroachDB and CockroachCloud. Our interns understand that their work affects our company’s roadmap, gain a sense of what working at Cockroach is like, and build relationships with engineers on their team and leadership members. Planning for our Internship Program From our People team to Engineers to Leadership teams, many people and teams make our Engineering internships run smoothly. A few months before our interns start, our People Development and Engineering Operations team sync on available teams that our interns will be able to join and potential starter projects, as well as who their Roachmate will be. Check out this blog for some of our recent internship projects! Your First Month They have a specific project they work on, mini-projects along the way, attend weekly team meetings and 1:1’s with their roachmate and manager! This ensures that our interns are set up for success from their first day - meeting their manager and roachmate and having an in-depth introduction into the codebase. A roachmate is similar to what other companies would call a mentor, but we had to put our own Cockroach Labs spin on it! This person is paired up with them to help them with their transition to the company, introduce them to other teammates, and a go-to person as they work through their projects. To create a holistic view of CRL, how the business works, and to understand how the teams work together, they also go through company onboarding like all of our full-time employees. Transparency and Feedback We know that Roachers perform at their best when we communicate openly and honestly, so this is something we make sure to do with our interns throughout their internships, especially at their midpoint check-in and end of internship sync. At the midpoint of their internship, we have their roachmate submit feedback on how they are performing on their projects, their interaction with the team and include any areas to improve on throughout the rest of their internship. This is also an opportunity for our interns to give feedback to their Engineering teams and People team. End of Internship With code reviews, tea time, and Flex Friday projects, the internship flies by! A few weeks before the end of their internship, each of our interns does a final presentation to our Engineering team about their final project and about their time at the company. The projects our interns work on will impact our team’s day-to-day, upcoming releases, and it’s an opportunity to showcase all of the great work they’ve done the past few months. If you’re passionate about shaping the future of databases and working with some of the brightest minds in technology, keep an eye out for our Fall 2021 internships coming in April! Interested in checking out our jobs? Visit our careers page to see if there’s a role for you.", "date": "2021-03-29"},
{"website": "CockroachLabs", "title": "How to Migrate from Go Dep to Go Modules", "author": ["Oliver Tan"], "link": "https://www.cockroachlabs.com/blog/dep-go-modules/", "abstract": "Do you enjoy weird and strange build issues? Or do you think something we do in this blog post is fishy and you want to fix it? Want us to use bazel instead? Good news - we’ve got a role for you! We’re on the lookout for more engineers on our Developer Infrastructure team ! We’re looking to expand our sprawling development infrastructure as we grow to more people and a bigger codebase with new and exciting functionality and cloud management. If you want to help the engineers write the exciting next generation of databases by empowering their work environment, don’t delay - apply today! -– For a significant part of our history, CockroachDB used the dep vendoring tool for managing package dependencies. Go modules have been widely available since Go 1.11, and with Go 1.14 heavily recommending the migration as well as dependent libraries moving towards the Go module world, it was time CockroachDB followed suit. However, migrating to go.mod was no straightforward feat for CockroachDB. Everything that could possibly complicate the upgrade seemed to come our way. We faced issues such as being unable to import vendored protobuf files, recognizing implicit upgrades and downgrades, the infamous error writing go.mod errors and a whole lot more. And in a Software Engineer’s worst nightmare, it was very difficult to find and apply StackOverflow answers, Github issues and documentation from Go that were applicable in our scenario! Curious? Follow along our blog post as we explore migrating to Go modules, sroughly following the guidance from the “ Migrating to Go Modules ” page. The Initial Migration from dep to Go Modules Our first step was to run the migrate tool: go mod init github.com/cockroachdb/cockroach which migrated our Gopkg.lock file to their go.mod and go.sum equivalents. However, the go.mod file failed the goimports tool because of lines like these: replace github.com/grpc-ecosystem/grpc-gateway 52697fc4a24978380c5ad7b80adc795336d4dfd4 => github.com/cockroachdb/grpc-gateway v1.14.6-0.20200519165156-52697fc4a249 Which caused Go’s linter to alert us with the following failure: replace github.com/grpc-ecosystem/grpc-gateway: version \"52697fc4a24978380c5ad7b80adc795336d4dfd4\" invalid: must be of the form v1.2.3 Interesting. In our Gopkg.toml we had defined grpc-ecosystem/grpc-gateway to be attached to a specific branch in our fork: [[constraint]]\n  name = \"github.com/grpc-ecosystem/grpc-gateway\"\n  branch = \"v1.14.5-nowarning\"\n  source = \"https://github.com/cockroachdb/grpc-gateway\" But the error message gives us our clue – the 52697fc4a24978380c5ad7b80adc795336d4dfd4 component was in fact extraneous! Removing all of the third arguments in lines like these fixed that problem: replace github.com/grpc-ecosystem/grpc-gateway => github.com/cockroachdb/grpc-gateway v1.14.6-0.20200519165156-52697fc4a249 The go.mod file still had problems - namely that the module import was defined twice: once in the require go.mod stanza and once in the replace directive. Below is an example of the duplicate definition of “github.com/cockroachdb/grpc-gateway” in the require stanza: require (\n   ...\n   github.com/cockroachdb/grpc-gateway v1.14.6-0.20200519165156-52697fc4a249\n   ...\n) Which would error whenever we tried to build anything with Go: go: github.com/cockroachdb/grpc-gateway@v1.14.6-0.20200519165156-52697fc4a249: parsing go.mod:\n\tmodule declares its path as: github.com/grpc-ecosystem/grpc-gateway\n\t        but was required as: github.com/cockroachdb/grpc-gateway Deleting the above line in the require stanza (and our other packages that has replace ) fixed that issue. Huzzah! Now we get some more interesting output when attempting to go build . However, from this point on, there were no more migration guide steps we could follow to resolve our issues&mldr; Encountering error writing go.mod After that, we should be able to build CockroachDB packages using Go modules. However, we run into the following error: error writing go.mod: open /Users/otan/go/pkg/mod/github.com/lib/pq@v1.3.1-0.20200116171513-9eb3fc897d6f/go.mod298498081.tmp: permission denied These seem like an error commonly encountered (judging by the many different results from a quick Google search) and something that seems tempting to blindly use chmod to fix. But hold that temptation - it will not work on other people’s systems! We discovered these errors seem to be a result of code attempting to modify our $GOPATH/pkg/mod directory where all the go.mod modules are downloaded to. This is notably permissioned as read only to prevent corruption. This makes sense - you do not want another Go module modifying a cached shared module on your system. In our case, we found that go/loader was deprecated and should have been upgraded to go/packages - otherwise, it will load and change the files on the module. This involved changing a number of our linters, such as our returncheck linter . Using protobuf and C files defined in Go modules CockroachDB imports certain libraries that require protobufs, and also relies on protobuf definitions inside certain vendored directories. Our new build errors all seemed to point to the fact that we could not import protobuf files that do not appear to be importable from our previously defined vendor directory. We realized this was because the file generator for protobufs (protoc) in the current directory, or directories specified with the -I flag . This is more verbose with Go modules as the imports have a @sha/version suffix in the directory name. As an example, we would need to do -I$GOPATH/pkg/mod/github.com to import prometheus/client_model@v0.2.0/metrics.proto in the .proto file. When using dep, we used -I./vendor/github.com so we could simply import prometheus/client_model/metrics.proto , with no notion of versioning required. Needing to include the Go module suffix in protobuf paths can be a tedious change - especially during upgrades where it could be an easily forgotten update. This is one of the use cases for go mod vendor , which copies the module files from $GOPATH/pkg/mod into a “vendor” directory in the current Go repo. In theory, this should basically make Go modules very similar to where dep stored the vendored modules. In Go 1.13 or before, Go will look for the modules inside the vendor directory if any of the Go commands have -mod=vendor as a flag (e.g. go build -mod=vendor . or go test -mod=vendor . ). It’s painful to remember to include the flag everywhere, but this was resolved in Go 1.14 as the flag -mod=vendor is implicitly added to any Go command if a vendor directory is detected. However, another problem emerged - protobuf files did not seem to be copied into the vendor directory if there are no Go files in the same directory. In particular, we had problems with protobuf files missing in github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api , github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/rpc and github.com/prometheus/client_model . This is because Go does not detect any Go dependencies on these packages, and as such does not attempt to copy them over when running go mod vendor . As such we have adapted and then adopted the modvendor tool to copy these additional directories from the $GOPATH/pkg/mod directory to the vendor directory. Another use case for go mod vendor To support cross compiling CockroachDB, we add extra .zcgo_flags.go files in directories that contain C files to tell the Go compiler know where to link certain libraries based on the OS that is being compiled. For example, in knz/go-libedit , we generate the following (.gitignore’d) file in vendor/github.com/knz/go-libedit/unix/zcgo_flags.go when compiling on OSX: // GENERATED FILE DO NOT EDIT\n\n// +build !make\n\npackage libedit_unix\n\n// #cgo CPPFLAGS: -I/Users/otan/go/native/x86_64-apple-darwin19.5.0/jemalloc/include\n// #cgo LDFLAGS: -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/cryptopp -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/protobuf -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/jemalloc/lib -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/snappy -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/libedit/src/.libs -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/rocksdb -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/libroach -L/Users/otan/go/native/x86_64-apple-darwin19.5.0/proj/lib\nimport \"C\" Which tells the clang compiler to link libedit/src/.libs when compiling libedit. Since the $GOPATH/pkg/mod directory containing these modules is read only, the generated files had to be put into the vendor directory using go mod vendor . When doing this, we needed to ensure we compile the cockroach binary with -mod=vendor (for Go 1.13 or before) or else we get a link error. Reappearing and disappearing tool related imports At this point, we have everything building! It was time for the next step in the go.mod migration checklist: running go mod tidy . This cleans up any unused imports that are lingering in go.mod and go.sum. Unfortunately, go mod tidy made some packages disappear! In particular, the packages that disappeared seemed to be related to tools we run that we vendor and pin at a specific revision in our Gopkg.toml and Gopkg.lock files. Instead of importing these tools, we install and run these directly from our Makefile . Since we do not explicitly import them, go mod tidy happily cleans these entries up. Any subsequent go install after the modules are cleaned up would not be pinned to the version we have previously specified. Thus, in order to use a pinned, vendored tool, we had to make sure it does not disappear from go.mod. We created a file that imports the required tools with blank imports  to ensure that these dependencies are not removed. This looks like the following (see source ): // +build tools\n\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/client9/misspell/cmd/misspell\"\n\t\"github.com/cockroachdb/crlfmt\"\n\t\"github.com/cockroachdb/go-test-teamcity\"\n\t\"github.com/cockroachdb/gostdlib/cmd/gofmt\"\n\t\"github.com/cockroachdb/gostdlib/x/tools/cmd/goimports\"\n\t\"github.com/cockroachdb/stress\"\n\t\"github.com/goware/modvendor\"\n\t\"github.com/grpc-ecosystem/grpc-gateway/protoc-gen-grpc-gateway\"\n\t\"github.com/kevinburke/go-bindata/go-bindata\"\n\t\"github.com/kisielk/errcheck\"\n\t\"github.com/mattn/goveralls\"\n\t\"github.com/mibk/dupl\"\n\t\"github.com/mmatczuk/go_generics/cmd/go_generics\"\n\t\"github.com/wadey/gocovmerge\"\n\t\"golang.org/x/lint/golint\"\n\t\"golang.org/x/perf/cmd/benchstat\"\n\t\"golang.org/x/tools/cmd/goyacc\"\n\t\"golang.org/x/tools/cmd/stringer\"\n\t\"golang.org/x/tools/go/analysis/passes/shadow/cmd/shadow\"\n\t\"honnef.co/go/tools/cmd/staticcheck\"\n\n) Note that +build tools directive, which tells Go to only build the package if the tools tag is provided in a Go command (e.g. go build -tags 'tools' ). In practice, we never set the tools tag and thus the file is never built or included in any package. Implicit upgrades and downgrades When using go mod init to migrate from dep to Go modules, we found that some modules were implicitly upgraded and some modules were implicitly downgraded after running go build and go mod tidy . As an example, take github.com/linkedin/goavro from Gopkg.lock but not present in Gopkg.toml: [[projects]]\n  digest = \"1:6ff6c3cb744b42df69cb874c3f19d387ece7ba7998e7d4de811c9bf61a7b1a09\"\n  name = \"github.com/linkedin/goavro\"\n  packages = [\".\"]\n  pruneopts = \"UT\"\n  revision = \"af12b3c46392134a7db8c1a8b6c6a33419fab0ea\"\n  version = \"v2.7.2\" After running go mod init , this import disappeared completely from go.mod after our migration. When running go build , it detected that github.com/linkedin/goavro did not exist, and instead reappeared as an earlier version, coincidentally the one right before the repo migrated to Go modules: require (\n   ...\n   github.com/linkedin/goavro v2.1.0+incompatible\n   ...\n) This was an undesired downgrade in the library. This seemed to be a trend with a few Go modules packages. [[projects]]\n  digest = \"1:0981502f9816113c9c8c4ac301583841855c8cf4da8c72f696b3ebedf6d0e4e5\"\n  name = \"github.com/mattn/go-isatty\"\n  packages = [\".\"]\n  pruneopts = \"UT\"\n  revision = \"6ca4dbf54d38eea1a992b3c722a76a5d1c4cb25c\"\n  version = \"v0.0.4\" which gets respected in go.mod initially: require (\n   …\n   github.com/mattn/go-isatty v0.0.4\n   …\n) But somewhere whilst running a go build and go mod tidy , something changes the version of the module to v0.0.9: require (\n   …\n   github.com/mattn/go-isatty v0.0.9\n) This is seemingly because this import is used by several dependent packages, which preferred the latest version. We could have gone down the route of trying to pin everything down to avoid vendor changes on the old Gopkg.toml. However, we decided it was easier to wave through all the upgrades and downgrades, as even if we pinned everything down, module dependencies may still end up changing after moving from dep to Go modules as some packages still had to be upgraded to properly support Go modules (we found some packages at some versions caused issues with Go modules, in which a version upgrade was required). To ensure waving through these implicit upgrades and downgrades was safe, we wanted to ensure that all these upgrades and downgrades were audited in a systematic fashion. To do this, we wrote a script that compares the contents of SHAs and versions outlined in vendor/modules.txt against the old Gopkg.lock. We used vendor/modules.txt to detect package changes over go.sum and go.mod, since vendor/modules.txt seems to contain the exact version of what module we were using for each dependency. go.sum had too much data and go.mod had too little. Comparing the old vendor directory and new vendor directory was untenable as thousands of lines of code and files had changed. To illustrate the differences between go.mod, go.sum and vendor/modules.txt, let’s compare them. Here is the vendor/modules.txt entry for gogo/protobuf: # github.com/gogo/protobuf v1.3.1 => github.com/cockroachdb/gogoproto v1.2.1-0.20190102194534-ca10b809dba0 Compare this with the go.sum file, which contains a lot of entries for the same package. This is because our dependencies all have a dependency on a different version of the protobuf library: github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.2 h1:+Z5KGCizgyZCbGh1KZqA0fcLLkwbsjIzS4aV2v7wJX0=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI= Under go.mod, it may be unclear that some implicit import could be included by surprise into vendor/modules.txt. As an example, the import of github.com/russross/blackfriday is included in  vendor/modules.txt despite not being present in go.mod and is explicitly used by a main package. It is worth noting that in Go 1.14, explicit imports are marked in vendor/modules.txt with a ## explicit line right after the import to help with reasoning about implicit vs explicit imports as defined by being mentioned in go.mod: # github.com/rcrowley/go-metrics v0.0.0-20190826022208-cac0b30c2563\n## explicit\ngithub.com/rcrowley/go-metrics\ngithub.com/rcrowley/go-metrics/exp\n# github.com/russross/blackfriday v1.5.2\ngithub.com/russross/blackfriday The version changes dumped from the script were transformed into a checklist of issues on GitHub which engineers audited . With good integration tests and manual auditing, we found 4 unwanted behavior changes out of the 80 or so import changes caused by the implicit upgrades or downgrades which warranted further action. We are not sure how to reproduce the same list without doing go mod vendor and peering into the vendor/modules.txt file. Some of these involved updating the import definitions to the versioned Go module equivalent . In particular, we had to change all imports of “github.com/cockroachdb/apd” to “github.com/cockroachdb/apd/v2” and “github.com/linkedin/goavro” to “github.com/linkedin/goavro/v2”. This resulted in quite a large number of file changes in the PR which were mostly just import changes leading to a larger and less well-focused code review. Unfortunately, specifying the version number in the package import does not work pre-Go modules and such we could do this as a separate commit/PR. Why are we still seeing certain packages in go.sum despite never importing them? Despite upgrading “github.com/cockroachdb/apd” to “github.com/cockroachdb/apd/v2”, we still found “github.com/cockroachdb/apd” in go.sum despite not being in go.mod. This was confusing to us. However, go mod why tool told us why: $ go mod why github.com/cockroachdb/apd\n...\n\n# github.com/cockroachdb/apd\n\ngithub.com/cockroachdb/cockroach/pkg/ccl/changefeedccl/cdctest\ngithub.com/jackc/pgx\ngithub.com/jackc/pgx.test\ngithub.com/cockroachdb/apd It seems as though the unit tests of github.com/jackc/pgx rely on the deprecated package which pulls it into go.sum. However, this package does not show up in vendor/modules.txt or in the vendor directory at all as we do not require pgx tests to be compiled in any part of CockroachDB. Reproducible Builds and the Go module cache For dep, we stored our vendor directory in a repo called “ vendored ”, using a git submodule on the CockroachDB repo to import the directory. This allows for reproducible builds without needing to check all the vendor directory contents into the CockroachDB repo (and as a side effect making those import git contribution stats more meaningful ;)). Further rationale and instructions for maintaining this is available here . In the Go module world, this is achieved using the Go module cache, which copies module source files into $GOPATH/pkg/mod. In particular, all the versions stored in go.sum would download the exact same version from the Go module cache from any different source. However, we had concerns with this, such as: What happens if a Go module becomes unavailable or deleted? Does it stay in the cache or get deleted? How does one inspect cache history? What is the behavior of a module using the same version tag but changing SHAs. We don’t think this is possible with Github, but is it elsewhere? These questions were blockers to us as it was possible we would not get a completely reproducible build at any SHA at some point in the future. We could alleviate this concern by hosting and owning our own Go module cache such as goproxy/goproxy but that comes with its own operational complexity and extra work for our infrastructure. As such, we have opted to maintain the vendored repo, using the present day solution of using git / Github as our “go module cache”. This preserves reproducible builds in a way we were comfortable with. This has an added benefit of not having to run go mod vendor every time vendored packages change, which can take a while compared to simply pulling in an update to the submodule. However, maintaining a git repo with go mod vendor is tricky as it wipes the vendor directory before recreating it. As such, we moved the git objects in .git to a separate directory before moving it back after go mod vendor is complete. Thus, our vendor directory rebuild script looks like the following : #!/usr/bin/env bash\n\nset -Eeoux pipefail\n\nTMP_VENDOR_DIR=.vendor.tmp.$(date +\"%Y-%m-%d.%H%M%S\")\n\n# restore the vendor directory if any of the below steps fail\ntrap restore 0\nfunction restore() {\n  if [ -d $TMP_VENDOR_DIR ]; then\n    rm -rf vendor\n    mv $TMP_VENDOR_DIR vendor\n  fi\n}\n\nmv vendor $TMP_VENDOR_DIR\ngo mod vendor\nmodvendor -copy=\"**/*.c **/*.h **/*.proto\"  -include 'github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/api,github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis/google/rpc,github.com/prometheus/client_model'\nmv $TMP_VENDOR_DIR/.git vendor/.git\nrm -rf $TMP_VENDOR_DIR This script only needs to be run after a module change. More detailed instructions for our workflow when changing Go modules is available here. Summary of our dep to Go Module Migration Steps To migrate from dep to Go modules, we needed to perform the following: Run go mod init github.com/cockroachdb/cockroach . Remove the third argument for all replace directives. Remove the libraries we needed to replace from the require stanza. Change all usages of go/loader to go/packages. Run go mod vendor . Use modvendor to include protobuf files that are not copied over in the vendor directory. At this point, we could start building compiling binaries with go build -mod=vendor ... Add a file that imports all tools we need to pin at a specific version. Run go mod tidy . Audit all implicit downgrades and upgrades that occurred when running go mod init , re-upgrading and re-downgrading packages as necessary. Update packages which required import definition changes in the codebase to use the Go module’d version. (Go 1.13 only) Update all scripts to use -mod=vendor to make sure everything imports from the correct directory. Results As of cockroachdb/cockroach#49447 , we’ve moved to Go modules, with great fanfare from the Twitter community: Our general “weirdness” with handling vendored packages is mostly alleviated using go mod vendor . It is able to handle protobufs, we are able to inject extra files into it and we’re able to commit the vendor directory as a git submodule. Furthermore, we were also successfully able to fast follow and upgrade to Go 1.14 to alleviate the need to remember to add the -mod=vendor flag to compile and test our packages (but may need to downgrade due to a Go 1.14 timer issue ). Overall, Go modules usage is a lot smoother in terms of speed compared to using dep ensure , and the management of modules feels more intuitive. Being able to use different versions of the same module at the same time also makes migration of certain packages to newer modules easier. However, we found for those migrating that Go modules could be complicated and overwhelming - especially for large and complex code bases - as evident by our wordier-than-expected migration process. There were a few steps here which were not immediately obvious from the “ Migrating to Go Modules ” docs page. However, reading the Go mod reference page and the entire blog post on Go modules on golang.org was useful as a base step. Some of the build errors and required package upgrades were difficult to resolve without a deep understanding of the Go module system, which these texts offered insight on. We’re hiring for our Developer Infrastructure team! Do you enjoy weird and strange build issues? Or do you think something we do in this blog post is fishy and you want to fix it? Want us to use bazel instead? Good news - we’ve got a role for you! We’re on the lookout for more engineers on our Developer Infrastructure team ! We’re looking to expand our sprawling development infrastructure as we grow to more people and a bigger codebase with new and exciting functionality and cloud management. If you want to help the engineers write the exciting next generation of databases by empowering their work environment, don’t delay - apply today!", "date": "2021-03-31"},
{"website": "CockroachLabs", "title": "CockroachDB ❤️ Open Source", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-%EF%B8%8F-open-source/", "abstract": "Seven years ago, Spencer Kimball made the first commit to the CockroachDB — an open source project he had started with Ben Darnell and Peter Mattis: Over the years, the project grew in popularity and so did the number of contributors. In 2015, Spencer, Ben, and Peter cofounded a for-profit company around the project — Cockroach Labs. As a for-profit business, we consider it our responsibility to financially support the open source developers and maintainers who help us build a healthy ecosystem of tools and libraries that work with CockroachDB. Cockroach Labs proudly sponsors the following maintainers who help us make CockroachDB the most evolved database on the planet (presented in reverse alphabetical order to liven things up). Daniele Varrazzo Daniele is the maintainer of psycopg2 — the most widely used Postgres driver for Python. We collaborated with Daniele to add testing to the main psycopg2 repository to ensure that CockroachDB stays compatible with every change to the psycopg2 project. Tom Payne Tom’s go-geom library gave us a strong foundational start to our geospatial project. Although this wasn’t a paid collaboration, we want to acknowledge how critical his work is for CockroachDB’s geospatial capabilities. Dominik Honnef Dominik is the creator of staticcheck – the advanced Go linter we use to check the entirety of the CockroachDB codebase. We are proud to sponsor Dominik’s ongoing work that helps the Go community at large. Tim Graham Tim is a full-time Django Fellow and has been instrumental in building CockroachDB’s Django adapter. Along with keeping tabs on every CockroachDB and Django release and adding support and functionality to the CockroachDB Django adapter, he also helps us identify and track down CockroachDB performance issues. Keith Doggett Keith has been instrumental in helping us add support for CockroachDB’s spatial features to ActiveRecord and making CockroachDB work with the latest ActiveRecord release. Jack Christensen Jack is the primary developer and maintainer of the PGX project — a Postgres driver for Golang. Jack helped us add testing to the PGX project to ensure ongoing CockroachDB compatibility with the PGX project. We are grateful for the ongoing community collaborations and welcome new ways to give back to the community. If you’d like to get involved, check out our contributing guide !", "date": "2021-03-18"},
{"website": "CockroachLabs", "title": "For Compliance and Latency in Banking, Move the Data Closer to the Customer", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/compliance-and-latency-banking/", "abstract": "Driven by a new breed of fintech applications, financial services organizations look to the cloud for infrastructure that is always-on, resilient, and able to support the real-time processing of transactions. In this post, we investigate how these organizations can deliver peak performance while ensuring compliance with local data protection and privacy laws. As Facebook and Google, and new-age fintech apps like Betterment and Robinhood, have driven consumers to expect feature-rich applications, every business needs to be concerned with performance. While this remains a top priority for financial services organizations, another component unique to the banking industry needs to be top of mind – keeping latency low. Another significant benefit of cloud-native applications for banks is that they can support very high volumes of transactions with low latency. Traditionally, the main drivers for reducing latency have been to speed up transactions and increase revenue – or minimize potential lost revenue. In its Gospel of Speed from 2012, Google revealed that a 400ms delay in search page generation caused traffic a 0.44% drop in search volume. Now, with every Google search, you’ll see how many milliseconds it took to return results. Around the same timeframe, in a presentation by a staff engineer, Amazon revealed that every 100ms of latency cost it 1% in sales. There are countless examples of companies facing lost revenue due to delays or downtime. In financial services, low latency is most often associated with high-frequency financial trading, or trading that is entirely automated and optimized to take advantage of changing market prices. These applications make millions of decisions per millisecond, and receiving data a fraction of a second faster than a competitor’s systems can equate to millions of dollars. Most other industries do not have these demands, especially under strict compliance guidelines and regulations. But low latency should be a concern for every financial services organization. A recent study found that nearly 90% of business leaders need low latency of 10ms or less to ensure their applications’ success. Financial services organizations must also consider the effect of latency on new use cases like cryptocurrency, edge computing, artificial intelligence (AI), and machine learning (ML). By taking advantage of low latency, data scientists can make informed real-time business intelligence decisions, and banks can use AI for real-time fraud detection. Multi-region deployments and geo-partitioning The majority of data moves between elements in a distributed application over public networks. This means even a perfectly architected application can experience lag if it has to communicate with a database thousands of miles away. As most banks have operations spanning regions, nations, and even the globe, they need to make infrastructure decisions considering these dispersed applications or customers. The solution is simple in theory – put the data closer to the application or customer. One way to keep the data closer to users is through a multi-region deployment. For a U.S. financial software company seeking a new database solution for its customer identity access management (CIAM), a multi-region deployment was the solution for achieving high performance and consistency. The CIAM layer was initially built on Oracle with multi-region replication using GoldenGate. However, the company soon discovered this configuration did not provide the speed or the always-available login experience it needed. Customers would experience a lag in authentication after creating an account, resulting in a poor user experience. The team decided to deploy CockroachDB across three AWS regions in the U.S., which brought resiliency by replicating data and distributing the replicas to maximize geo-diversity. However, multi-region deployments can be complicated for organizations with distributed databases because managing state across a set of machines is never straightforward. Organizations need to determine if the benefits outweigh the costs since using a single-region deployment is detrimental to speed and availability. This is where geo-partitioning of data comes in. Geo-partitioning provides row-level replication control, meaning organizations can attach data to a specific location. A global financial data firm , for example, deployed CockroachDB to reduce latency across four GCP regions and two on-premise data centers by creating a single hybrid, geo-partitioned deployment. The firm had outgrown its expensive and dated Oracle database architecture. It chose CockroachDB to migrate its identity access management microservice because the geo-partitioning features provided a solution for authenticating entities even when strongly tied to specific geographic regions. Geo-partitioning can also work even if a customer moves or travels, which is crucial for payment applications. Don’t forget regulations Beyond the speed complications for banks operating in multiple regions, financial organizations need to consider data regulations. Data privacy is a hot-button issue, with new laws and regulations coming into effect every year . At the start of 2020, more than 120 countries had more than 200 legislations to protect data and consumer privacy. These regulations range from newer state-wide mandates, like the California Consumer Privacy Act of 2018 (CCPA), which gives consumers more control over the personal information businesses collect about them, to sweeping regulations like the European Union’s General Data Protection Regulation (GDPR), which covers everything from data collection and sharing to data storage, erasure, destruction, and even more. The most important aspect of these regulations for organizations to keep in mind when growing a broad regional or global customer base is that they often prohibit storing certain data outside of certain boundaries. For example, this could mean a U.S. bank with customers in Europe may need to store those customers’ data within the EU. Keeping data closer to the application or customer offers another important benefit of geo-partitioning. The ability to pin data to a specific location can help ensure compliance in countries or regions that require data to be stored within the borders. CockroachDB from Cockroach Labs is the only database solution that offers geo-partitioning for multi-region deployments . Using these capabilities, developers at banks and financial services organizations can designate where data should be stored at the database, table, and row-level. With this, organizations can deliver their applications with the lowest possible latency while keeping compliant with the latest data protection and privacy regulations. Download the eBook How Financial Service Companies Can Successfully Migrate Critical Applications to the Cloud to learn more.", "date": "2021-03-26"},
{"website": "CockroachLabs", "title": "New Course: Build a Full-Stack App in Java with Spring Boot & CockroachDB", "author": ["Diana Carroll"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-for-java-course/", "abstract": "We are excited to share that Fundamentals of CockroachDB for Java Developers is now available on Cockroach University . This course is designed for Java application developers who are looking to advance their education with CockroachDB and build scalable, resilient applications and services. This is our third course offering following the inaugural Getting Started course and Fundamentals of CockroachDB for Python Developers. Similar to the Python course, you will build a full-stack-ride sharing application, but in Java using Spring Boot with Spring Data JPA . Fundamentals of CockroachDB for Java Developers includes a series of videos and exercises as well as a final exam. You will also be using CockroachCloud Free , which is our forever free database-as-a-service offering. It’s ideal for development, testing out CockroachDB, and hobby applications. Who is the Java course for? Before taking this course, you should have some experience with Java and application development. If you are familiar with SQL, it will help you along the way, but it’s not required. If you are brand new to CockroachDB, you should consider taking the two-part introductory course, Getting Started with CockroachDB first. This course is a great training tool for managers to share with their developer teams. Developers complete this self-paced course on their own and save the time and money it would cost to introduce a new technology. And since CockroachDB is Postgres-wire compatible , it speaks SQL and has a familiar look and feel for many developers. What can I expect? This course will take you approximately 4 hours to complete [see the syllabus here ]. If you’ve taken the introductory courses, you will be familiar with the format. This course includes: Three chapters of content Nine videos Elevent hands-on labs A final exam You will build a full-stack ride-sharing application in Java using Spring Boot with Spring Data JPA. You will start with a simple version that manages a fleet of scooters and their current positions. You will add features one by one as the course progresses until you have a highly functional application that is ready to track vehicles, rides, and users. Along the way, you will learn how to: Deploy a free CockroachDB cluster on CockroachCloud Design your CockroachDB schema in SQL Model your data in JPA Perform transactions such as adding, removing, and updating rows At the end of the course, there will be a final exam to test your knowledge. When you pass, you will receive a Certificate of Completion. We encourage you to share your Certificate of Completion on LinkedIn (tag @cockroach-labs ) or Twitter (tag @cockroachDB ) to show off your new skills! For a limited time, once you pass the final exam you will be entered to win a raffle for an iPad Air. We will select winners on April 30. Remember to check your email for the Certificate of Completion and raffle entry form! How can I get started with Cockroach University? You can get started today for free here . Again, for anyone new to CockroachDB or not familiar with our unique architecture, we recommend taking the two-part introductory courses, Getting Started with CockroachDB . (Reminder: we will send you a free CockroachDB swag bag after passing the Getting Started final exam!) Along the way, our docs can be used for references for things like syntax, SQL examples, and additional tutorials. The example application, MovR, uses Spring Boot. For help, you can refer to: The Spring Boot Docs The Spring Data JPA Docs The Hibernate ORM Docs Where to get help? If you have questions or feedback, we encourage you to join the #cockroach-university channel in our CockroachDB Community Slack , where our team is available to chat. What’s next in Cockroach University? This spring, we are planning to launch CockroachDB Performance Basics for Developers which will be a follow up to this course and the Python course. Follow us on social media ( LinkedIn , Twitter ) for more updates on Cockroach University and new courses. We hope you enjoy this course and always welcome your feedback. Thanks for your interest in advancing your education with CockroachDB!", "date": "2021-03-30"},
{"website": "CockroachLabs", "title": "Infinite Colors: Supporting Neuro-Diverse Experiences in the Workplace", "author": ["The Infinite Colors Crew and Allies"], "link": "https://www.cockroachlabs.com/blog/infinite-colors-neurodiversity/", "abstract": "In summer 2019, the Infinite Colors employee resource group , or CREWS (Cockroach Employees Who Support), was launched at Cockroach Labs. It is an employee-driven initiative, and its mission statement is to raise awareness and share resources about the challenges and opportunities of neurodiversity in the workplace. By neurodiversity, we refer to the variations in the human brain regarding sociability, learning, attention, mood and other mental functions in a non-pathological sense. It encompasses at least, but is not limited to, traits over-represented in ADHD , autism , dyscalculia (\"numbers\"), dyslexia (\"words\"), dyspraxia/DCD (\"movement/coordination\"), mood disorders and Tourette's syndrome . Note that folk often experience neurodiversity without being formally diagnosed, sometimes without even being aware of the existence of possible labels. Neurodiversity is experienced by an individual as a degree of divergence from their local group on traits related to meta-cognition, sociability, learning, emotional regulation, attention, mood and other mental functions. Towards its awareness goal, the group members facilitate respectful conversation and lift taboos on neuro-atypical traits and behaviors. Team members share how they feel welcome and accepted , often without masking : Knowing that there is this type of space is very reassuring. It feels like a place where I can be open about who I am. (Senior Product Designer) This is the first place I have worked at where this is a topic that is OK to talk about. You know how companies typically make space for visible handicaps or differences. But we have invisible differences too! It was important to me to see the company acknowledge this explicitly by promoting the CREWS group. I felt this was the first time I can be accepted for who I am. (Senior Staff, Product Management) It provides me comfort. Knowing that there are others like me, sharing similar experiences and struggles. It is a community I did not expect. Knowing that other folk are different is cool. It makes me feel safer. (Engineering Tech Lead) The existence of the group's Slack channel, and the existence of the CREWS group signifies the acknowledgement by Cockroach Labs that neurodiverse individuals exist. It makes me feel more comfortable, when I express to my coworkers that I feel low on energy or forget to do something. It feels nice not to be held to standards that I cannot meet. I like the comfort it gives me that I can be myself without fear of reprisal. (Site Reliability Engineer) It is nice to have a space to talk about neurodiversity. I value the ability to have this type of conversation. It makes it easier to connect. I also liked how the group gave an opportunity for my coworkers to share something about them that we couldn't talk about otherwise. (Senior Staff, Product Management) It makes me feel happy, safe and grateful to be at an organization that allows for spaces like this. I am happy that our org acknowledges and recognizes the beauty in neurodiversity, without any sense of stigma. In my experience, in my particular role, I am expected to always be \"on\" and presenting, which can be wonderful as I love what I do. However, the flip side of that comes with knowing that I sometimes am required to put up a facade, as the concerns about stigma are still there. I think we’re growing as a society past this fact, but it’s something that is fairly unavoidable. With this group, I feel comfortable and supported , knowing everyone is kind, honest, sharing and caring. It reminds me of how lucky I am to be part of such an organization. (Account Executive, Sales) I got to know people I work with at a more personal level. This means a lot to me. (Technical Support Engineer) Image Credit: \"Art of Communication\" by somethingzenzn (Creative Commons BY-NC-ND 3.0 license) Meanwhile, Infinite Colors also maintains a corporate library of resources that includes guides and tips on how to optimize one's workplace, develop positive work relationships, advocate for one's diversity, avoid burnout, and guide peers and managers towards understanding and accommodation. Team members share how they see Infinite Colors fostering recognition and accommodation for different ways to be human on the workplace: It is a space where I can see different types of folks & appreciate those differences. (Senior Product Designer) From a business perspective, we are thereby enabling our co-workers to perform better. I am grateful about how the company makes onboarding materials available on video, which helps me learn better. I was impressed by the willingness to produce training videos without background music, thereby acknowledging how certain folk would feel over-stimulated otherwise. Making the CREWS’ resources available, and making all the staff aware of these issues, also makes us able to better serve neurodiverse customers! (Senior Staff, Product Management) It gives me somewhere I can participate and ask questions, where there would be stigmas having these conversations elsewhere. (Engineering Tech Lead) The articles shared by the group have broadened my mindset. (Senior Product Designer) I never got to meet my co-workers in person. Also, I am not very good at Slack communication. So it is hard for me to participate in activities that are not directly related to my work. However, I really like receiving all the links pasted in the groups' Slack channel. I read all of them. (Site Reliability Engineer) In tech, there is a stigma about engineering, that it's supposed to be hard to relate to engineers. What the CREWS group does for me is that it provides a different and more helpful framework for understanding what it means to work with different personalities. (Senior Technical Writer) With the rapid move to a remote-first workplace, it was hard to evaluate whether the communication interface we choose with coworkers makes sense for them. I liked how the CREWS group shows and explains how different folk communicate and relate to each other. It gives us guidance on how to have these conversations. For example, it really made me understand the value of asynchronous and/or silent meetings. It helped me appreciate that we need alternative meeting formats for folk who don't function well in large groups, or feel overwhelmed by mostly verbal discussions. To me, the CREWS group is all about making it easier to work with each other. (Senior Staff, Product Management) In our industry, it is the difference between folk that relates the most to who we are, the one that most defines our interface with each other and with the world. I can't remember ever having a conflict with someone due to their country of origin, race, gender, sexual preferences or skin color. But I did experience struggle due to differences in interpersonal dynamics. Understanding differences in thought patterns and communication styles is key to social success and engagement, and is under-appreciated. I think it is huge, and it often feels too ancillary relative to other things. I feel like building a taxonomy of these differences will be super beneficial to our community. (Software Engineer) I believe the entire group deeply cares about neurodiversity and mental health. This is important in my life. I feel safe, respected and supported, and I find this to be a unique work experience. I am glad to see, hear and read about other folk's experiences, as well as discuss various topics. One thing that made my day was the request last year to provide a quiet space during the yearly company event. I found it amazing to realize and learn how important this is, and that the company cared enough to accommodate the request. Nowadays, when I have guests, I am mindful to make spaces that work for them, and I am aware that there should be a space for everyone to feel comfortable. (Account Executive, Sales) Image Credit: Ice Forest Waterfall by boldfrontiers In fact, Infinite Colors also receives executive sponsorship from Peter Mattis, CTO of Cockroach Labs, and some of its members have senior roles in the organization. Through their support, Infinite Colors has been able to influence both the daily experience of the staff and the organization's processes. For example, resources are being developed for managers on how to best coach, support and supervise reports in the context of neurodiversity. Infinite Colors' mission and resources are now also introduced to every new hire. The corporate Diversity, Equity and Inclusion projects actively seek input and insights from its members. Teammates reflect on how they see Infinite Colors developing this organizational impact: Knowing that the company is promoting a blog post on this topic, and knowing that many co-workers are eager to share their experience, is a direct confirmation that this is not just one person's pet project. (Site Reliability Engineer) [When I started], I wasn't sure whether a company this size already had employee support groups. I think it's a good investment from leadership to support this type of diversity. (Software Engineering Intern) Image Credit: Autumn Welcome to Sherando Lake by boldfrontiers Infinite Colors already accompanies team members throughout their tenure inside Cockroach Labs. Eventually, we expect its mission and materials to become public, as we know that it has a general positive impact on our ability to recruit talented and diverse staff: When I applied at Cockroach Labs, I chose this position over other offers with better titles or better compensation, because I was impressed by the company's commitment to fairness and balance. I feel that sharing about Infinite Colors in public will help others like me appreciate CRL better and generally help with hiring. The existence of the CREWS group really helps to shine a light on and bring awareness to neurodiversity. (Senior Staff, Product Management) Personally, fostering bonds within our teams bumps my energy to another level. I find that having the CREWS group makes for a very exciting workspace, it makes us more open and curious, and thus able to know each other better. I am excited to be a part of that. (Senior Product Designer) It was an exciting experience to discover the group's Slack channel on my first day at work. The group caught my eyes on the corporate wiki, which I was browsing ahead of my first day. Overall, I found the existence of the group gratifying! It was a welcome addition to my first days at the company. (Software Engineering Intern) I was in the process of applying to Cockroach Labs; I remember a line in the company's materials that mentioned embracing neurodiversity. This struck me as very progressive, and it made a good impression very early in the application process. (Senior Technical Writer) Image Credit: Sam's Organic Universe by boldfrontiers On a day to day basis, Infinite Colors does not project itself socially like your usual workplace community: its members do not seek or organize regular meetups nor do they display their affiliation conspicuously. Yet, our feeling of purposefulness is real and we are excited to share when prompted: Sometimes I get anxious to share more personal things. But at the same time I think it's good to have this opportunity to talk. (Senior Product Designer) This group has a meaning for me that I cannot easily explain to others. I am a member of this group but I don't actively participate. I am still deciding how and what I would like to share. I think it is an inspiration to others that this group exists. It makes me proud. (Senior Staff, People Operations) I find that caring about these issues makes it possible for everyone to equally participate. In fact, I find that acknowledging and promoting this group elevates us as a team. Personally, fostering bonds within our teams bumps my energy to another level. I find that having the CREWS group makes for a very exciting workspace, it makes us more open and curious, and thus able to know each other better. I am excited to be a part of that. (Senior Product Designer) Just the existence of the CREWS group is good! At the same time, I like that we are taking stock of what folk experience from it. (Engineering Tech Lead) It's interesting to consider that when folk mention the other CREWS groups in informal conversations, Infinite Colors is not as often mentioned. Perhaps some more education is needed internally for understanding what neurodiversity is about. (Senior Technical Writer) Unfortunately due to societal norms we can be made to feel that we have to fit in a certain mold, when in reality we should be celebrating our diversity. We are all unique, smart, capable, and wonderful people - and great at our jobs, nonetheless! This makes us who we are at our workplace - better salespeople, better engineers, better marketers, better educators; our neurodiversity also makes us who we are outside of work - wonderful friends, siblings, children, parents, volunteers, citizens. We often need that reminder, which is why Infinite Colors is so great. (Account Executive, Sales) As a person who identifies as a cis-straight-male, I feel my privilege so I find it is my responsibility to learn about differences around me and help spread their message. Also personally I feel I have a right to help when my co-workers can use my help, at a humane level. I feel that is my responsibility too. So I was truly thrilled when I heard I had the opportunity to join the group. I found it fruitful. This may sound a bit selfish, but it gave me new ways to assume my responsibilities in a positive way. It makes me feel I can make a difference. (Technical Support Engineer) I like that we think about neurodiversity and inclusion. Culturally, we already seem comfortable approaching diversity from the perspective of external attributes like gender, race, skin color. However, I think we can only reach a true depth in inclusivity when we also consider the differences in how people think. It helps us better understand the world, and helps us build a better society. (Software Engineer) Image Credit: Golden Essence of Llanberis by boldfrontiers For most folk, the year 2020 has been straining. Neuro-diverse individuals have been particularly challenged, having to balance their differences with all the changes happening around them. We are still learning the new normal. Yet, in a way, these struggles also opened the door to new ways to connect: neuro-diverse folk were suddenly more able to support their teammates by teaching them about workable strategies to maintain balance, coping with change and mental health. They were also able to accompany and support journeys of self-discovery. This gave us new opportunities to learn more about each other and strengthen our bonds. Infinite Colors gave words to these conversations and gave them a context. Through this shared collective experience, we created a new way to foster belonging. We were able to make Cockroach Labs a more welcoming and inclusive workplace today than it was a year or two ago. We are proud of this achievement. We wish to further this mission though, and for this we also need to regularly meet new perspectives and new experiences of neurodiversity. We hope that you will share yours, and consider joining us. Colophon: We are also grateful to the numerous contributors who contributed to this blog post, including but not limited to: Angela Wen, Joe Lowinske, jordan ryan reuter, Piyush Singh, Raphael Poss. Header Image Credit: Color Me Rainbow by SubhadipKoley", "date": "2021-04-06"},
{"website": "CockroachLabs", "title": "Why do outages still happen in 2021?", "author": ["Peter Mattis"], "link": "https://www.cockroachlabs.com/blog/why-do-database-outages-happen/", "abstract": "Disasters happen. (Most) outages shouldn’t. When multi-billion dollar companies like Zoom , Slack , and Fanduel experience outages, the reaction from users tends to be anger and surprise. Anger, because people need to get their work done and set their fantasy football lineups. Surprise, because it’s 2021. Isn’t high availability the norm? Not all outages are created equal. Though they have a similar impact on end users, they happen for all kinds of reasons. And some of these reasons are preventable. While many outages look the same (applications down, murky company response about the cause), there’s a lot of nuance to what’s happening under the hood. If we take a closer look, we can see what is preventable today, and what (as an industry) we are still working on. The evolution (and limits) of high availability In order to understand why outages are still happening, it’s important to look at the history of high availability . The evolution of highly available systems–and looking at where companies currently sit on that spectrum–indicates a lot about what their capabilities are and what steps they can take to become more resilient. Highly available systems used to be systems with redundant, hot-swappable power supplies, disk drives, and even CPUs. Nowadays, we recognize that there are better pathways to high availability than making a single machine highly available. Instead, we turn to making services highly available by using large clusters of machines, where any node in the cluster can fail without bringing down the service. This means that a disaster that affects one node–like a fire in the datacenter, or the proverbial backhoe–doesn’t cause the entire service to go dark. Resilient, distributed systems that are built to failover in cloud-native ways are no longer prone to outages like this. So why do even highly available systems at tech companies like Facebook and Google fail? The nuance regarding these kinds of failures is that even with data storage systems that can survive datacenter failures, there can still be outages. Take, for example, a network configuration error with Google that took down half the internet in Japan . Human error isn’t going to be eliminated by the next innovation in highly available services. Natural disaster caused outages, on the other hand, can be a thing of the past. Resilient systems that can handle a datacenter outage of this kind already exist, and are in place at tech-forward companies. The future is here. But as William Gibson said, “it’s just not very evenly distributed.” Is your data resilient to disaster? After Wells Fargo’s outage in 2019 , the company spoke about routing to backup datacenters, and the failover datacenter failing. While we don’t know the details of their data storage configuration, that language is outdated, and indicates that they’re not using the cloud native systems that provide the resiliency we see at contemporary tech companies. This is largely because the data solutions sold to older enterprise companies–like Oracle’s solutions–aren’t cloud native, and therefore can’t provide the resiliency we’ve come to expect in 2019. Companies using this legacy tech are often locked into a long contract, or haven’t made the leap to the cloud yet because of the perceived cost of switching. In the meantime, they’re forced to focus on disaster recovery instead of disaster prevention. And they’re going to stay that way until they start truly building for resiliency, and getting off legacy tech. While some of the tech used at companies like Google and Netflix is internal to them, a lot of these solutions are available off the shelf. Here are three ways you can build for resiliency today: 1. Automate disaster testing Simulating disasters (and recovering from them) shouldn’t be a manual process. Companies that build this into their everyday processes, like Netflix and its Simian army , have a working model that makes emergency protocols not an edge case, but the norm. 2. Choose a self-healing database Choosing a database that self-organizes, self-heals, and automatically rebalances is an important component of resilience. By making component failure an expected event that a system can handle gracefully, you’ll be prepared when a datacenter goes down for whatever reason. 3. Replicate to have redundancy built into the system To survive disasters, you need to replicate your data and have redundancy built into the system to eliminate all points of failure. Prepare for disaster by having data transactionally replicated to multiple data centers as part of normal operation. The path towards resiliency already exists. But a lot of organizations need an initial push to get off the legacy tech they use and onto the cloud-native solutions they need. If you want to learn more about surviving database disasters and the differences between legacy database failover and distributed database failover watch this video: \\", "date": "2021-03-23"},
{"website": "CockroachLabs", "title": "How to Benchmark and Tune Google Cloud", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/benchmark-google-cloud/", "abstract": "Engineers on cloud performance teams can spend their entire workday tuning and optimizing cloud configurations. We caught up with Steve Dietz, a Google software engineer focused on Google Cloud performance, to get advice on how to performance tune, provision, and benchmark Google Cloud Platform (GCP) . The conversation led to some revealing insights into how to configure GCP to be most optimal for your workloads. We’ve gathered some of those insights below. To catch the whole conversation with Steve, head over here . #1: Use a Performance Benchmarking Tool on Your Cloud Performance benchmarking tools like the open source PerfKit Benchmarker (which is maintained by the Google Cloud team) allow anyone to measure the end-to-end time to provision resources in the cloud. PerfKit reports on standard peak performance metrics, including latency, throughput, time-to-complete, and Input/Output Operations Per Second (IOPS). A benchmarking tool should serve to provide an understanding about what’s happening in an environment, while including offering latency metrics between components in different regions. To that end, PerfKit offers a publicly available dashboard showing cross-region network latency results between all Google Cloud regions. Below are the results of Google’s own all-region to all-region round trip latency tests, using n1-standard-2 machine types and internal IP addresses. Anyone can reproduce the results themselves by running a snippet of code available on the PerfKit site. In addition to tools like PerfKit, there are a number of resources to help GCP users get the best performance out of their product. The blog post, “ Performance art: Making cloud network performance benchmarking faster and easier ,” and a follow-up report on measuring networking latency in the cloud, can help you get started with Google cloud benchmarking and data collection. #2: Read Benchmarking Research Cockroach Labs set out to better understand customer needs by conducting original research. This process first involved gauging how well CockroachDB performed while running in cloud environments from different providers. When the team discovered a significant difference in performance between AWS and GCP, it published its inaugural cloud report in 2018 to help customers make informed decisions when choosing a cloud provider. The 2021 version of the Cockroach Labs Cloud Report goes even further, using a series of microbenchmarks and typical customer workloads — such as  CPU, network, storage, and a derivative of TPC-C — to compare the performance of AWS, Azure and GCP. The Cloud Report benchmarks cloud providers against transactional (OLTP) workloads. As the researchers noted in the report and in the reproduction steps , all of the benchmarks were selected with transactional workloads in mind. A machine learning-focused workload may be better served by using a different set of benchmarks to compare cloud performance. #3: Evaluate Workloads Before Configuring GCP One of the most common questions when setting up a cloud deployment is: should I use the provider’s default configurations? When Cockroach Labs set out to benchmark AWS, Azure and GCP, it needed to have enough constant factors between the three providers to ensure accurate results. The team accomplished this by using each provider’s defaults, so that misconfigurations or configuration bias wouldn’t affect the testing outcomes. For users, default configurations may be ideal for some workloads. Before altering the default machine configurations, consider the types of machines (family, series, machine type, etc.) that are being offered — for example, N2 with Intel versus N2D with AMD — and evaluate whether one may be better suited for your workload. One of the discoveries in the 2021 Cloud Report , was that machines running Intel CPU processors performed exceptionally well on single-core tests, but machines running Amazon’s Graviton2 and AMD performed better on the multi-core tests. Learn More About Provisioning and Benchmarking Optimizing and benchmarking your cloud infrastructure involves a lot of nuance and finetuning. Our suggestions above offer a starting place. For more advice on benchmarking and provisioning GCP, listen to the full conversation between GCP and Cockroach Labs .", "date": "2021-04-01"},
{"website": "CockroachLabs", "title": "5 Beginner-Friendly Coding Livestreams to Learn Programming", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/5-coding-streams-for-beginners/", "abstract": "The past year has been a great year for code livestreams. Stuck at home, developers took to their OBS setups to share their coding projects with their peers. And viewers tuned in to watch a fellow developer struggle through a programming issue, get stuck, google things, and eventually figure things out, and found their coding communities in the process. But if you are a code newbie, you might find it daunting to follow along with the experienced developers' livestreams. Fear not, for you have your own peers learning new tech stacks live! Here are five of our favorite beginner-friendly streamers that you can watch to learn programming: Learn full-stack web development: Leon Noel ( https://www.twitch.tv/learnwithleon ) runs a free Web Dev Bootcamp live on Twitch every Tuesday and Thursday at 6:30 PM EST. The goal of the bootcamp is to help those affected by the pandemic as well as help underrepresented folks learn web development and get hired as software engineers. You can also bring your questions to the Office Hours on Saturdays at 12 PM EST. Learn Python with Pachi: Pachi Carlson is currently learning Python live on her Twitch channel ( https://www.twitch.tv/pachicodes ) on Mondays and Wednesdays at 10 AM EST. Learn Golang with Paul: Paul Kernfeld is currently learning Golang from scratch every Friday at 12 PM EST on Cockroach Labs' Twitch channel . Learn Rust with Prince: Prince Wilson is currently learning Rust with Exercism.io on his Twitch channel ( https://www.twitch.tv/maxcellw ) Learn Javascript with CJ: The creator of the popular coding YouTube channel (Coding Garden) also has an excellent Twitch channel ( https://www.twitch.tv/codinggarden ) for full-stack app projects, tutorials, and Q&A sessions (mostly featuring Javascript). Bonus streams: Follow along Jeseekia Vaughn’s awesome journey through 100 Days of Code: https://www.twitch.tv/metadevgirl Want to get some of your own coding done? Join Mayuko ( https://www.twitch.tv/hellomayuko ) for a chill co-working stream for pomodoro coding sprints.", "date": "2021-04-02"},
{"website": "CockroachLabs", "title": "Free Courses: Distributed SQL Tutorials & Labs for Beginners", "author": ["Crossman Wilkins", "Alex Yarosh"], "link": "https://www.cockroachlabs.com/blog/cockroachuniversity-intro-first-steps/", "abstract": "It’s been over a year since we released Cockroach University . In that time, we’ve launched two new developer courses, hired a full team of training professionals, and planned out a comprehensive roadmap for the future. Over 6,000 developers, architects, engineers, and other tech professionals have enrolled in one or more Cockroach University classes. We’ve received a lot of positive feedback so far but one comment we heard over and over again is that our students would appreciate shorter courses that focus on different skill sets. So, we took our students’ advice and split the inaugural Getting Started course into two parts: Introduction to CockroachDB and Distributed SQL and Practical First Steps with CockroachDB . Which course is right for me? We encourage beginners and anyone who is brand new to distributed SQL and CockroachDB to start with Introduction to CockroachDB and Distributed SQL . This course does not involve any coding and is ideal for non-technical roles who are interested in learning the concepts behind distributed databases and CockroachDB. In Introduction to CockroachDB and Distributed SQL , you learn: What differentiates distributed SQL from both legacy SQL & NoSQL databases How CockroachDB ensures consistent transactions without sacrificing scale & resilience The key functionality that brings people to CockroachDB & more! The second course, Practical First Steps with CockroachDB , is hands-on and will provide all the tools you need to get started with CockroachDB. If you are familiar with SQL, that will help you along the way, but it’s not required. Practical First Steps with CockroachDB teaches: How to spin up a cluster How to use the Admin UI to monitor cluster activity, and How to use SQL shell to solve a set of hands-on exercises & more! Ultimately, both courses are useful basics for developers and great training tools for managers to share with their teams. They each take around 1 hour - 90 minutes to complete and they are self-paced. Because CockroachDB is Postgres-wire compatible , it speaks SQL and has a familiar look and feel to many students. We also want to remind you that all Cockroach University courses are FREE! You can use Cockroach University to save the time and money it would cost to introduce a new technology. What happens when I complete the course(s)? After completing each course, you will receive a Certificate of Completion. We encourage you to share your Certificate of Completion on LinkedIn (tag @cockroach-labs ) or Twitter (tag @cockroachDB ) to show off your new skills! Once you pass the final exam for Practical First Steps with CockroachDB you will be eligible to win a free swag bag. Remember to check your email for the Certificate of Completion and swag form! What other courses are available? As we mentioned before, we also have two developer courses available: Fundamentals of CockroachDB for Java Developers and Fundamentals of CockroachDB for Python Developers. During both of these courses, you will build a full-stack ride sharing application either in Python or Java. As the course progresses, you will add features one by one until you have a highly functional application that is ready to track vehicles and users at scale. Before taking either of the developer courses, you should have experience with Java or Python  application development. It is also helpful to be familiar with SQL and CockroachDB. What’s next? Are you ready to get started with Cockroach University or try out one of our new courses? You can sign up or login to your account here . In the coming months, we have many more courses planned including a course for javascript/node.js developers and a course focused on schema design. Follow us on social media ( LinkedIn , Twitter ) for more updates on Cockroach University and new courses. We hope you enjoy your experience with Cockroach University and always welcome your feedback. If you have questions or thoughts you would like to share, we encourage you to join the #cockroach-university channel in our CockroachDB Community Slack , where our team is available to chat. < Learn Distributed SQL Fundamentals >", "date": "2021-04-08"},
{"website": "CockroachLabs", "title": "How LUSH Optimized Global Inventory Management with CockroachDB", "author": ["Dan Kelly"], "link": "https://www.cockroachlabs.com/blog/global-inventory-management/", "abstract": "Every day Lush processes hundreds of thousands of transactions in over 950 stores located across 49 different countries. Consistency is important. And inventory management can create extraordinary opportunities when the right tools are used to deliver real-time data insights. In this blog, we’ll cover what Lush’s inventory management architecture looked like before CockroachDB and what they’re able to accomplish with CockroachDB. Lush Architecture Prior to their Migration to CockroachDB If you haven’t already indulged in the fragrant self-care of a Lush bath bomb then you may not know that Lush Fresh Handmade Cosmetics is a global inventor, manufacturer and retailer of fresh handmade cosmetics - known for the quality of their ingredients and the environmental thoughtfulness of their packaging (or lack thereof). Prior to their migration to CockroachDB Lush’s database architecture was made up of MySQL instances in 20 of the countries in which the Lush Group operates. While this architecture was an integral part of the company as it grew globally, it created a lot of operational complexity. In particular, the disparate MySQL instances could not talk to each other, which made it difficult to optimize inventory or evaluate the company’s performance on a global scale. The Challenge of Disparate MySQL Instances In this disparate MySQL instance architecture there is no easy way to immediately assess the performance of Lush’s products at global scale. The data needs to be located in the same system to make it easy to interrogate that data. The emergence of data protection laws and data localization regulations adds more challenges. As an international retailer that does business all over the globe, Lush is required to domicile some user data in particular countries. This is the cost of doing business on a global scale, and it has been incredibly tricky to accomplish. Lush wanted to simplify and modernize their data architecture in order to make it easier to analyze the company’s global performance, comply with local data regulations, and reduce operational complexity. Their requirements included: High availability Cloud neutral Easy management of global user data Source available Standard SQL and simple insertion and extraction of data A managed offering Migrating off Google Cloud Spanner Before eventually landing on CockroachDB Lush first turned from MySQL to Google Cloud Spanner in their early efforts to modernize their database. They quickly ran into a couple of problems: Cloud lock-in: Using Spanner locked Lush into the Google Cloud ecosystem. Even tasks like exporting data to CSV required use of Google products like exports to buckets. Blackbox: The lack of visibility into Spanner made it difficult for the engineering team to troubleshoot. Lush scaled to 10 nodes without issue, but they had to make that decision blind. Whereas with CockroachDB, they’re starting with 3 nodes and they can see everything that’s happening. Custom programming language: Spanner speaks a custom query language, rather than the standard SQL. This would require Lush’s engineering team, already versed in SQL, to learn a new programming language in order to program against Spanner. Because of these challenges Lush ultimately determined that Spanner was not the right solution and began evaluating alternatives. Their research led them to CockroachDB. How Lush Evaluated CockroachDB Lush began evaluating CockroachDB in 2017. Right out of the box CockroachDB met their most pressing requirements. To start, CockroachDB is a source available database with 100% visibility into the code. It uses standard SQL, making it easy for the Lush team to get up and running quickly. Being able to spin up a CockroachDB node locally with Docker in a matter of seconds allows Lush developers to learn by doing, which isn’t possible or as easy with other database technologies. The combination of Postgres language support in Go with the ease of getting started made for a seamless integration into Lush’s developer ecosystem. Lush also had exceptionally high availability requirements. As a global retailer that processes 165,000 financial transactions per day (in the UK), it was critical that their database be ultra-resilient and not have issues with outages or downtime. CockroachDB delivers zero RPO (recovery point objective) and an average of 4.5 second RTO (recovery time objective), ensuring that data is never lost and applications are highly available. Finally, Lush was seeking a managed database so that their engineering team could focus on application development, not on maintaining third-party software. When they began their evaluation in 2017, Managed CockroachDB was in development, and Lush became a design partner while CockroachCloud was brought to market. CockroachDB Workloads at Lush Optimize Global Inventory and Order Management Lush plans to use CockroachDB for many different use cases, including as the system of record for all orders. The rollout will begin with all stores in the UK, of which there are over 100, and will expand to include all 950+ global stores. Lush will also be deploying CockroachDB as their stock management layer. This would give Lush a real-time view of global inventory, allowing them to see how much product they have in a particular store, in a particular country, and in the entire world – something they haven’t had visibility into before. Increased visibility into their inventory enables Lush to optimize the strategy with which they stock their stores. It allows them to avoid ever being understocked and to proactively take advantage of trends they see in particular regions. Analyze Global Business & Comply with Regulations CockroachDB gives Lush a global view of all their data. At the same time, Lush can partition data to a particular region without losing the ability to access and analyze that data. What became particularly attractive to Lush is that they can configure their database before they decide where they need to geo-partition. They can then retroactively pick and choose where data is stored, satisfying both data sovereignty concerns and the need to have a global view of data. This is really powerful for Lush because it allows them to be strategic about how they handle (a very large data set) of product stock. Relieve Operational Management Reducing operational complexity is addition by subtraction. With CockroachDB, Lush no longer manages dozens of MySQL shards. CockroachDB’s ability to auto-rebalance, auto-recover from failures, and scale without manually sharding data relieves Lush’s engineering team of huge management overhead. As early users and design partners on CockroachCloud, Lush was able to reduce their operational management even further. This allows their developers to focus completely on application development without worrying about the underlying issues that surround distributed systems of any kind. Lush developers get time back because of what CockroachDB automates. This liberates developers to evaluate and use data to make good decisions. To learn more about CockroachDB workloads at Lush and other CockroachDB use cases check out our customers page.", "date": "2021-04-08"},
{"website": "CockroachLabs", "title": "Build a Rails App with ActiveRecord and CockroachDB", "author": ["Ali Ibrahim, Test Double"], "link": "https://www.cockroachlabs.com/blog/app-rails-activerecord-cockroachdb/", "abstract": "*Guest post alert! Marla and Ali worked with the Cockroach Labs team to get the ActiveRecord CockroachDB Adapter ready for Rails 5.2 and beyond! Their work with Cockroach Labs is done, but the adapter lives on. This blog post was originally shared on their blog at Test Double .* -– When I work in Rails apps, I don’t find myself worrying too much about the database. Since Rails natively supports popular databases like MySQL and PostgreSQL, I usually only need to make a few config changes to get an application’s database up and running. I don’t find myself running into too many problems using databases that Rails doesn’t support either. Thanks to Rails’ well documented database interface and strong community support, I still only need a few config changes to use databases like Oracle and SQL Server . If you’re with me so far, then it should come as no surprise that it’s just as easy to use CockroachDB with Rails! 🎉 What’s a CockroachDB?? Glad you asked! CockroachDB, built by Cockroach Labs , is a database that’s designed to be scalable and highly available. It also uses the PostgreSQL wire protocol so you can use it almost anywhere you’d use PostgreSQL. Almost (more on that later). So how do we use CockroachDB with Rails? Since I like learning by example, let’s configure an existing Rails app to use CockroachDB. Using CockroachDB with Rails In this example, we’re going to change the CodeTriage Rails app so it uses CockroachDB instead of PostgreSQL. After following the CodeTriage contributing guide to get the app running locally, the app will be ready to talk to PostgreSQL. To switch to using CockroachDB, we’ll first need to install and configure CockroachDB. How do you install CockroachDB? First, install CockroachDB per the install guide . Next, we’ll use the cockroach demo command to create a single-node CockroachDB cluster. We’ll run the command with the --empty flag so the we don’t run into any conflicts loading the CodeTriage schema later. $ cockroach demo --empty\n#\n# Welcome to the CockroachDB demo database!\n#\n# You are connected to a temporary, in-memory CockroachDB cluster of 1 node.\n#\n# This demo session will attempt to enable enterprise features\n# by acquiring a temporary license from Cockroach Labs in the background.\n# To disable this behavior, set the environment variable\n# COCKROACH_SKIP_ENABLING_DIAGNOSTIC_REPORTING=true.\n#\n# Reminder: your changes to data stored in the demo session will not be saved!\n#\n# Connection parameters:\n#   (console) http://127.0.0.1:63115\n#   (sql)     postgres://root:admin@?host=%2Fvar%2Ffolders%2Fzj%2F41x2d76s4kq4vv8_c8qrl1z00000gn%2FT%2Fdemo900101820&port=26257\n#   (sql/tcp) postgres://root:admin@127.0.0.1:63117?sslmode=require\n#\n#\n# The user \"root\" with password \"admin\" has been created. Use it to access the Web UI!\n#\n# Server version: CockroachDB CCL v20.2.5 (x86_64-apple-darwin14, built 2021/02/16 12:57:34, go1.13.14) (same version as client)\n# Cluster ID: 83ec1cc1-4b7a-410f-b0b4-dea5ea562b9b\n#\n# Enter \\? for a brief introduction.\n#\nroot@127.0.0.1:63117/defaultdb> After the cockroach demo command creates the empty database, it opens an interactive SQL shell. The demo database only exists in memory while the shell is open, so we’ll keep it open until we’re done. The cockroach demo command also gives us some information on how to connect to it. # Connection parameters:\n#   (console) http://127.0.0.1:63115\n#   (sql)     postgres://root:admin@?host=%2Fvar%2Ffolders%2Fzj%2F41x2d76s4kq4vv8_c8qrl1z00000gn%2FT%2Fdemo900101820&port=26257\n#   (sql/tcp) postgres://root:admin@127.0.0.1:63117?sslmode=require From this information we can see we have a user named root with password admin the CockroachDB server is listenting at 127.0.0.1 (a.k.a. localhost ) at port 63117 and sslmode is set to require Most of these details will be the same when you run the cockroach demo command, but the port might be different. Take note of these connection details as we’ll need them later. Now that CockroachDB is up and running locally, we’re ready to make some config changes to CodeTriage. Add the ActiveRecord CockroachDB Adapter First, we’ll edit the Gemfile and replace the pg gem with the ActiveRecord CockroachDB Adapter gem. Since CodeTriage is currently running against Rails 6.1, we’ll install v6.1.0.beta1 of the ActiveRecord CockroachDB Adapter. --- a/Gemfile +++ b/Gemfile @@ -31,7 +31,7 @@ gem 'local_time', '2.1.0' gem 'maildown', '~> 3.1'\n gem 'omniauth', '~> 1.9.1'\n gem 'omniauth-github' -gem 'pg' +gem 'activerecord-cockroachdb-adapter', '6.1.0beta1' gem 'puma'\n gem 'rack-timeout'\n gem 'rrrretry' Then after installing the gem with bundle install , we’ll make some changes to config/database.yml . Configure CodeTriage to use the ActiveRecord CockroachDB Adapter First, we’ll change the adapter value from postgresql to cockroachdb . --- a/config/database.yml +++ b/config/database.yml @@ -1,5 +1,5 @@ defaults: &defaults -  adapter: postgresql +  adapter: cockroachdb encoding: utf8\n   pool: 5\n   host: localhost Next, we’ll grab the connection details we noted earlier from the CockroachDB interactive SQL shell we have a user named root with password admin the CockroachDB server is listenting at 127.0.0.1 (a.k.a. localhost ) at port 63117 and sslmode is set to require and set port , user , password , and requiressl . --- a/config/database.yml +++ b/config/database.yml @@ -3,7 +3,10 @@ defaults: &defaults encoding: utf8\n   pool: 5\n   host: localhost -  password: +  port: 63117 +  user: root +  password: admin +  requiressl: true Now CodeTriage should be ready to use CockroachDB! Let’s set up the database by running bin/rake db:create db:schema:load db:seed . $ bin/rake db:create db:schema:load db:seed\nCreated database 'triage_development' Created database 'triage_test' rake aborted!\nActiveRecord::StatementInvalid: PG::FeatureNotSupported: ERROR:  unimplemented: extension \"pg_stat_statements\" is not yet supported\nHINT:  You have attempted to use a feature that is not yet implemented.\nSee: https://go.crdb.dev/issue-v/54516/v20.2\n/Users/alimi/.rvm/gems/ruby-2.7.2/gems/activerecord-6.1.0/lib/active_record/connection_adapters/postgresql_adapter.rb:678:in ` exec_params ' /Users/alimi/.rvm/gems/ruby-2.7.2/gems/activerecord-6.1.0/lib/active_record/connection_adapters/postgresql_adapter.rb:678:in `block (2 levels) in exec_no_cache' /Users/alimi/.rvm/gems/ruby-2.7.2/gems/activesupport-6.1.0/lib/active_support/dependencies/interlock.rb:48:in ` block in permit_concurrent_loads ' Uhh…that doesn’t look good. 😅 CockroachDB quacks like PostgreSQL but it isn’t PostgreSQL If we take a look at that last command/error again, we can see the CodeTriage databases were created in CockroachDB. $ bin/rake db:create db:schema:load db:seed\nCreated database 'triage_development' Created database 'triage_test' But things went wrong when trying to load the database schema from db/schema.rb . rake aborted!\nActiveRecord::StatementInvalid: PG::FeatureNotSupported: ERROR:  unimplemented: extension \"pg_stat_statements\" is not yet supported\nHINT:  You have attempted to use a feature that is not yet implemented.\nSee: https://go.crdb.dev/issue-v/54516/v20.2 In db/schema.rb , CodeTriage is enabling the pg_stat_statements extension but as the error tells us CockroachDB doesn’t support it. Although CockroachDB uses the PostgreSQL wire protocol and acts a lot like PostgreSQL, it’s very important to remember CockroachDB ain’t PostgreSQL . You can use CockroachDB as if it were PostgreSQL in a lot of places which means you won’t have to learn a bunch of new stuff to use it. But you might run into small differences in behavior like this. For demonstrations purposes, we’ll change CodeTriage’s db/schema.rb so it no longer enables the pg_stat_statments extension (nor the plpgsql extension). --- a/db/schema.rb +++ b/db/schema.rb @@ -12,9 +12,6 @@ ActiveRecord::Schema.define(version: 2020_11_15_123025) do -  # These are extensions that must be enabled in order to support this database -  enable_extension \"pg_stat_statements\" -  enable_extension \"plpgsql\" create_table \"data_dumps\", id: :serial, force: :cascade do |t|\n     t.text \"data\" Now, let’s try loading the schema and seeds again. $ bin/rake db:schema:load db:seed\nsuccess\n....................................................................................................% OK, that looks a lot better. But can we really get away without having these extensions? CodeTriage will error wherever it’s expecting the PostgreSQL extensions to be installed and available. We don’t need to worry about this here because this is just a blog post, but it would give me pause if I was changing a production database. If this was a real migration, I’d review the compatibility doc and update the application so it no longer depends on PostgreSQL features. Now that we’ve done the config changes and set up the database, we should be able to talk to CockroachDB from CodeTriage. 🕺🏾 Connecting to the CockroachDB database from CodeTriage Let’s spin up a rails console and get some data! Since we ran bin/rake db:seed earlier, our database should have some seed data. $ bin/rails console\nLoading development environment (Rails 6.1.0)\n>> User.count\n   (67.7ms)  SELECT COUNT(*) FROM \"users\"\n=> 101 OK, we have 101 users. Let’s try fetching the first one. >> User.first\n  User Load (2.3ms)  SELECT \"users\".* FROM \"users\" ORDER BY \"users\".\"id\" ASC LIMIT $1  [[\"LIMIT\", 1]]\n  TRANSACTION (0.9ms)  BEGIN\n  User Update (3.9ms)  UPDATE \"users\" SET \"updated_at\" = $1, \"account_delete_token\" = $2 WHERE \"users\".\"id\" = $3  [[\"updated_at\", \"2021-03-04 01:00:45.821404\"], [\"account_delete_token\", \"874464f621a5930c859c5b99b9d1d26705386d61bd34caf00b1288e949dec48dc257459c6fcb297da12ef32dd16419ff64bc7289d94425a94c46fd94ffb89ce9\"], [\"id\", 637885482494296065]]\n  TRANSACTION (22.5ms)  COMMIT\n=> #<User id: 637885482494296065, email: \"\", created_at: \"2021-03-03 02:13:20.466321000 +0000\", updated_at: \"2021-03-04 01:00:45.821404000 +0000\", zip: nil, phone_number: nil, twitter: nil, github: \"schneems\", github_access_token: nil, admin: nil, avatar_url: \"http://gravatar.com/avatar/default\", name: nil, private: false, favorite_languages: nil, daily_issue_limit: 50, skip_issues_with_pr: false, account_delete_token: \"874464f621a5930c859c5b99b9d1d26705386d61bd34caf00b...\", last_clicked_at: \"2021-03-03 02:13:20.466267000 +0000\", email_frequency: \"daily\", email_time_of_day: nil, old_token: nil, raw_streak_count: 0, raw_emails_since_click: 0, last_email_at: nil> It works! 🙌🏾 You might notice this user has a really big id. CodeTriage specifies a Serial id for the users table so you might expect our first user to have an id of 1. CockroachDB recognizes Serial , but instead of assigning user id’s sequentially from 1 it will assign them based on the transaction timestamp and the node’s id . CockroachDB does this to ensure globally unique id’s are used across nodes in a performant manner. In case you forgot, CockroachDB is not the same as PostgreSQL! I can also spin up the application by running bin/rails server and watch the server output to see ActiveRecord make some queries to CockroachDB. $ bin/rails s\n=> Booting Puma\n=> Rails 6.1.0 application starting in development\n…\nStarted GET \"/\" for 127.0.0.1 at 2021-03-03 20:25:52 -0500\n   (0.7ms)  SHOW crdb_version\n   (3.5ms)  SELECT \"schema_migrations\".\"version\" FROM \"schema_migrations\" ORDER BY \"schema_migrations\".\"version\" ASC\nProcessing by PagesController#index as HTML\n   (1.8ms)  SELECT COUNT(*) FROM \"users\"\n  ↳ app/controllers/pages_controller.rb:59:in `block in description'\n   (1.1ms)  SELECT COUNT(*) FROM \"repos\"\n  ↳ app/controllers/pages_controller.rb:60:in `block in description'\n…\n  ↳ app/views/pages/_repos_with_pagination.html.slim:1\n  Repo Load (2.7ms)  SELECT \"repos\".\"id\", \"repos\".\"updated_at\", \"repos\".\"issues_count\", \"repos\".\"language\", \"repos\".\"full_name\", \"repos\".\"name\", \"repos\".\"description\" FROM \"repos\" WHERE (issues_count > 0) ORDER BY issues_count DESC LIMIT $1 OFFSET $2  [[\"LIMIT\", 50], [\"OFFSET\", 0]]\n…\nCompleted 200 OK in 661ms (Views: 387.9ms | ActiveRecord: 226.8ms | Allocations: 117408) The queries work! And the app loads!!! Use CockroachDB with Rails today Thanks to the ActiveRecord CockroachDB Adapter , we can use CockroachDB in Rails apps just like any other database. And since CockroachDB talks and acts a lot like PostgreSQL, it can almost be a drop in replacement for PostgreSQL ( almost 😉). Try using CockroachDB in your Rails apps today! Humblebrag: Marla and I had a lot of fun working with Cockroach Labs to get the ActiveRecord CockroachDB Adapter ready for Rails 5.2 and beyond! Our work with Cockroach Labs is done, but the adapter lives on. Follow Cockroach Labs’ progress on GitHub .", "date": "2021-04-12"},
{"website": "CockroachLabs", "title": "Retraction: My Go Executable Files Are Still Large (What's New in 2021)", "author": ["The Cockroach Labs Team"], "link": "https://www.cockroachlabs.com/blog/go-file-size-update/", "abstract": "In 2019, we published an exploration of the size and makeup of the executable files produced by the Go compiler. CockroachDB engineers, the Go team, and the greater Go community were intrigued by the analysis, and the results of it pushed our projects further. Two years passed, and both Go and CockroachDB had evolved significantly in that time. We wanted to explore: What might have changed? What could we learn? On April 14, 2021, we published our revisited exploration of the Go executable files. However, what we thought was bold, exciting language and analysis came across to readers and particularly to developers on the Go team as incendiary and accusatory. To make matters worse, we made mistakes in the analysis and came to incorrect conclusions, then defended those conclusions for too long, even while updates were already happening behind the scenes in response to community feedback. We expected the Go developers to provide all the answers, with little examination into where CockroachDB’s growth contributed to our findings (thank you to Jeff Wendling @ Storj for taking the time to do an analysis with the CockroachDB version pinned). The phrase “extraordinary claims require extraordinary evidence” comes to mind, and we did not deliver extraordinary evidence. Over the course of Friday and Saturday, we listened to the feedback from the community – and from Russ Cox specifically. Although we revised the article, we failed to make clear that we were revising it, we failed to publish our revisions in a timely manner, and we failed to explain what we had revised. The failure to adequately acknowledge our mistakes was disrespectful to the Go development team, as well as to the broader Go community. The original draft of the analysis we presented was reviewed internally, and yet still inaccuracies slipped through. We are a team of humans. We make mistakes. We take full responsibility for the invalid conclusions we published, and we are sorry. Open source software development relies on a community contributing ideas and asking hard questions that push the technology forward. We think this is a beautiful thing. Our community can exist only with mutual trust and respect. In this case, our community probed our own analysis, and uncovered mistakes and errors. We have removed the article from our site and leave, instead, this apology. We did not hold up our end of the agreement. We are grateful for the feedback, and will continue to listen and do better. -- The Cockroach Labs Team", "date": "2021-04-14"},
{"website": "CockroachLabs", "title": "How We Support Neuro-Diversity in the Workplace", "author": ["The Infinite Colors Crew and Allies"], "link": "https://www.cockroachlabs.com/blog/how-our-resource-group-for-neuro-diversity-empowers-everyone/", "abstract": "This past summer, we launched the Infinite Colors employee resource group , or CREWS (Cockroach Employees Who Support), at Cockroach Labs. What is Infinite Colors? Infinite Colors is an employee-driven initiative to raise awareness and share resources about the challenges and opportunities of neurodiversity in the workplace. By neurodiversity, we refer to the variations in the human brain regarding sociability, learning, attention, mood and other mental functions in a non-pathological sense. It encompasses at least, but is not limited to, traits over-represented in ADHD , autism , dyscalculia (\"numbers\"), dyslexia (\"words\"), dyspraxia/DCD (\"movement/coordination\"), mood disorders and Tourette's syndrome . Note that folks often experience neurodiversity without being formally diagnosed, sometimes without even being aware of the existence of possible labels. How does Infinite Colors support its mission? 1. Dialogue: Towards its awareness goal, our group members facilitate respectful conversation and lift taboos around neuro-atypical traits and behaviors. This allows us to better support our neuro-diverse members and colleagues. Our members agreed to share their perspectives on how having a community for neurodiversity has made them feel welcomed, accepted, and comfortable with masking less: Knowing that there is this type of space is very reassuring.\nIt feels like a place where I can be open about who I am. - Senior Product Designer This is the first place I have worked at where this is a topic that is OK to talk about. Companies typically make space for visible handicaps or differences, but we have invisible differences too! It was important to me to see the company acknowledge this explicitly by promoting the [Infinite Colors] CREWS group. I felt this was the first time I could be accepted for who I am. - Senior Staff, Product Management 2. Resources: Infinite Colors also maintains a corporate library of resources that includes guides and tips on how to optimize one's workplace, develop positive work relationships, advocate for one's diversity, avoid burnout, and guide peers and managers towards understanding and accommodation. We feel that the opportunity to develop and socialize these resources with the broader team fosters greater recognition of neurodiversity and vital support strategies. 3. Advocacy: Infinite Colors also receives executive sponsorship from Peter Mattis, CTO of Cockroach Labs, and some of our members have senior roles in the organization. Through their support, Infinite Colors has been able to make an impact on both the daily experience of the staff and the organization's processes. For example, resources are being developed for managers on how to best coach, support and supervise reports in the context of neurodiversity. Infinite Colors' mission and resources are now also introduced to every new hire. Our corporate Diversity, Equity and Inclusion project teams also actively seek input and insights from our members! What’s next? Infinite Colors already accompanies Roacher throughout their tenure at Cockroach Labs. Eventually, we expect to share our mission and materials with more people, as we want to encourage more neurodiverse folks to join us.", "date": "2021-04-12"},
{"website": "CockroachLabs", "title": "How Cockroach Labs Gives Back", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/charitable-corporate-giving/", "abstract": "Cockroach Labs was recently named on both Crain's and Built in NYC's \"Best Places to Work\" lists. In the published blurbs, they talk about the way our team treats employees: from the really important perks like great healthcare to the smaller ones like office snacks and Citibike memberships. I think what these lists are missing, however, is a greater sense of how Cockroach Labs views its relationship to the humans we interact with: our employees, our customers, and the world at large. One of the many ways this is manifested is in our company's corporate giving initiatives. Here are some of the charitable giving practices we've instituted, and how they contribute to making Cockroach Labs a great place to work. Matching Employee Referral Bonuses to Charity of Choice This past year, we launched an unusual kind of referral bonus. For all priority roles, employees who refer a new hire receive a $5,000 bonus, and an additional $5,000 to donate to a charity of their choice. The program is the brainchild of Senior Manager of Recruiting, Dave Delaney, who knew there was a way to spend hiring budget while still making a difference. As Dave puts it, “We’re very fortunate to be in the industry we’re in, for employee benefits, compensation, and general quality of life. I wanted to create a program that gave not only to employees, but to the world at large.” Last year, we made 20 donations from our referral program for a total of $55,000 given to 501(c)(3)s like RAICES Texas , Autism Speaks , Trans Lifeline , and the ASPCA , to name a few. A side-bonus of this referral program: Dave was a bit nervous to pitch it to our executive team, but upon their immediate buy-in, he knew that Cockroach Labs was a good value fit for him. Charitable Giving Through Marketing Work in tech long enough, and you’ll get a closet full of t-shirts you don’t wear. And probably some hoodies. As our events manager, Jp Sisneros spends a lot of time in convention centers staffing and perusing software company booths. This year, he took a step back to think about why we were giving away t-shirts and socks. What could we do that spoke more to who we are as a company, what we care about, and what our product does? In 2019, we started a campaign at two of the biggest tech trade shows in North America— we committed to donating to Women Who Code for each scan we received at KubeCon North American and AWS re:Invent. In 2020, we expanded this campaign. During KubeConEU, we partnered with Sysdig and Instana to donate $2 to Black Girls Code for every booth visit. Every time we offer swag to a member of the CockroachDB community, we also give them the opportunity to make a donation to charity, instead. Since we first started these campaigns for charity, we’ve given over $25,000 to Women Who Code, Black Girls Code, Code2040 , and StartOut. Our Recruiting team liked this idea so much that they chose to give donations to Black Girls Code throughout 2020 in lieu of swag at virtual events! Peer Acks Committee to Give Back One way we express gratitude towards one another at Cockroach Labs is through a tradition called peer acks . Throughout the week, acknowledgments come in with a storm of emoji reacts, and--if they’re written in poetry--get read out at team meetings. At the end of the year, each peer ack grants employees one entry into a raffle to join the Pay It Forward Committee, a team of five Roachers who are granted $10,000 to donate to 501(c)(3) charities doing good in the world.\nThis year, the Pay it Forward Committee organized their budget into five categories they cared about: education, disaster relief, health, social justice, and conservation. They did a bunch of research on where charity dollars do the most good, and they donated to the following organizations: Modest Needs School of St. Jude Green Worker, Inc Clean Ocean Action The committee also gave to the Cockroach Labs Black History Month fundraiser, which benefited organizations such as dev/color , Thurgood Marshall College Fund , Center for Black Equity , and the Loveland Foundation . What we like about this practice is that the end of year charitable giving isn’t the reason we give peer acks: the primary benefit is acknowledging one another. The charitable giving becomes an added benefit on top of an existing practice and an opportunity to “Pay it Forward.” Charitable Giving Back as a Company Perk The underlying commonality in these three programs is a belief that people are motivated to give; they’re motivated by giving to charity as much (or even more!) than they are by free swag. Over the past year, it’s a bet that’s paid off, both with our team and with the causes we care about. If these values resonate with you, check out our current job listings!", "date": "2021-04-21"},
{"website": "CockroachLabs", "title": "Kubernetes As Explained Through the 1997 Blockbuster Titanic", "author": ["Mikael Austin"], "link": "https://www.cockroachlabs.com/blog/titanic-explains-kubernetes/", "abstract": "Titanic is a 1997 Oscar-winning film starring Leonard DiCaprio and Kate Winslet (who will from now on be referred to as K8 Winslet) in a love story aboard the ill-fated Titanic ship. As we know, the Titanic was an “ unsinkable ship ” - the largest ship in the world at that time - yet it sank mere days into its maiden voyage, costing the lives of 1500 people on board when it hit an iceberg and sunk to the bottom of the North Atlantic on April 15, 1912. I want to reimagine the ending of the Titanic, both the ship’s ending and the film’s. We’re going to look at the story through the lens of Kubernetes and distributed database technology to see how the story could have ended happily. The Titanic: A Grand, Expensive, Single-Instance Database Think of the original Titanic as a single instance database. This can be a single large instance in a cloud, or a large on-prem Oracle setup. It is grand, expensive, and monolithic. When sailing is smooth, things are great! There are fancy dinners and rambunctious parties. Image Credit But if your single instance database were to crash into an iceberg, there’s no practical, immediate recovery. If there were an outage, it could take hours or days to recover all that was lost. Whether it’s due to a natural disaster, human error, or an iceberg, outages happen. Shouldn’t your database (and your ship) be prepared to handle worst case scenarios? Why Did the Titanic Sink? A Quick Diversion Into Poor Architecture Here are some of the mechanics behind why the Titanic – the “unsinkable ship” – sank. The Titanic had 16 compartments in the hull of the ship, separated by partitions called bulkheads. They were designed this way so if there were a hole in the ship, the bulkheads ensured only one compartment would fill with water. The issue was that the bulkheads did not reach the deck above. Instead, each bulkhead stopped 10 feet above the waterline. Think of this as a wall that almost reaches the ceiling above it. When the Titanic struck the iceberg, 5 of the 16 compartments were breached, which caused the bow to dip. This made the water flow into all the compartments. As soon as one filled up, water continued to rush over the bulkhead, and the ‘unsinkable’ ship sank within 3 hours. Once the hull of the ship was breached, there was nothing to stop the ship from sinking. What’s worse, there were not enough lifeboats to save everyone on board. Image Credit Disaster Recovery for Boats & Databases Disaster recovery in a single instance “Titanic database” is typically done through asynchronous replication, which slows down considerably when there are more transactions and a limited amount of throughput. Only companies with lots of extra resources are able to pay for a high availability version of their single instance database. In the case of the Titanic, there were too many people who needed to be saved (transactions), and not enough lifeboats (throughput). The only people who were able to be saved were the upper class, who paid much more money for their high-end experience. Ok, so what does any of this have to do with Kubernetes? Image Credit Let’s Reimagine the Titanic With Kubernetes & a Distributed Database Now let’s imagine the Titanic could be rebuilt to remove its single point of failure. What would it take to make it a highly available, disaster-recovering ship of our dreams? Distribute & Replicate: What if we had MORE Titanics? Instead of a single, monolithic ship, let’s distribute and replicate our Titanic to have two identical boats to ride alongside it. This divides the party across three boats instead of one, while also adding capacity in case one of the boats suffered a catastrophic issue. Even in the face of catastrophe, we can just add more replicas! No need to worry about running out of lifeboats. Think of these as your distributed database. Let’s Add a “Helm Chart” At the literal helm, let’s introduce a Helm chart. Helm gets our ships running in the water and helps us chart a course for success, but it can’t do much without someone to keep an eye on all the gauges. The Ship Captain Will Be Played By “The Operator” Now let’s imagine an upgraded ship captain, played by “the operator”  to control all onboard functioning. The operator gives the crew information about directions, outside conditions, and ship status. It allows our crew to upgrade parts of the ship while it’s moving. Imagine changing the engine without losing any time. It even comes with a security staff that lives aboard the ship. The operator makes running Kubernetes with a database significantly easier. With an operator, the Titanic would probably have never struck the iceberg in the first place. Our Reimagined Titanics On our reimagined Titanic – or really, Titanics because we will run three boats instead of one – we can now play out the love between Rose K8 Winslet and Jack, and rewrite the ending to be a happy one. Rose K8 Winslet: Constricted, Static, Stateless Rose K8 Winslet, our heroine, feels as constricted by her lifestyle as she is by her corset. K8 is a product of her environment–an upper class young woman in the early 20th century who was raised to play a very specific part in society. We could say that K8 was raised to be stateless. Boring, static, akin to an unchanging web page. Image Credit Jack Dawson: Lively, Dynamic, & Full of &mldr;State? Jack Dawson represents CockroachDB in the Kubernetes (K8) and CockroachDB love affair. Jack is lively, dynamic, and fearless. When K8 met Jack, her perspective changed. With Jack/CockroachDB, K8 could become stateful. K8 could finally be dynamic, interesting, and grow to her full potential. Image Credit On our reimagined distributed Titanics , if one of our ships hit an iceberg, one (Kubernetes) container might fill with water, but it would not affect any of the other containers. This means that the ship will not sink if one container in the hull is breached. In the event of an outage scenario in which one of our boats were to sink, there would be two other ships to transfer passengers to, as well as enough lifeboats for everyone aboard. Similarly, in a disaster recovery scenario with Kubernetes and CockroachDB, all data ranges on a destroyed node are able to safely transfer to another node. None of the passengers or crew members are lost. Database transactions can continue near, far, wherever you are, and go on and on without fail. Image Credit Rose K8 Winslet and Jack (CockroachDB) are a love story for the books, and if only the original Titanic had followed design patterns of distributed systems, nobody would have died in the end. The bulkheads would have been built properly, distributed ships would have ensured recovery in the face of disasters, and there are enough lifeboats for everyone. And in case anyone is wondering, hypothetically, of course there is enough room for both K8 and CockroachDB on that piece of wood! CockroachDB and Kubernetes were meant to be, and you can run mission critical operations on CockroachDB on Kubernetes. And that is something to “never let go” of! Image Credit", "date": "2021-04-15"},
{"website": "CockroachLabs", "title": "Distributed Spatial Data in Free, Open Source CockroachDB", "author": ["Andy Woods"], "link": "https://www.cockroachlabs.com/blog/spatial-data/", "abstract": "CockroachDB is a reliable, relational database, built to help you scale your transactional workloads in the cloud. Our focus on common relational data types has brought bulletproof resilience and effortless scalability to all sorts of customer applications, from customer service applications to global data stores for cloud-connected devices, to streaming video providers. Today, we’re excited to bring that same bulletproof resilience and effortless scale to new use cases: spatial data types in CockroachDB. The same open-source database you use to store and access common relational data types (e.g., INT, TEXT, UUID) can now be used with GEOMETRY and GEOGRAPHY spatial data types. And best of all, it’s free. All spatial data features and functions are included in the open-source license of CockroachDB 20.2. CockroachDB 20.2 provides access to spatial data types, indexing for fast reads, external formats (e.g., GeoJSON,  Well Known Text), common spatial shapes (e.g., linestrings, polygon, geometry collections), builtins (e.g., ST_DWithin, ST_Contains). We aim for PostGIS compatibility to allow you to seamlessly migrate existing applications as well as use the rich third-party ecosystem. Our initial release comes with more than 220 PostGIS compatible built-ins and functions –and this is only the beginning. We will continue to add to this catalog in future releases. What can you build with spatial data? With spatial data you can build applications that meet the needs of users that want information related to physical locations. Your customers increasingly expect both fast and accurate data from their applications to help them make informed decisions. They want to know how many cars are in their vicinity or how long they will wait for a ride-sharing vehicle to arrive. They want to use location data to minimize waste and reduce their carbon footprint. They want to know the exact value of a house based on recent comparable sales in the same geographic area. CockroachDB spatial data and indexing can help you store and query this data today. This spatial data tutorial from our docs is extremely thorough and includes an interactive element to help you visualize what the application development experience will be like on CockroachDB. Here’s an example of a sample spatial data application built on CockroachDB: Use CockroachDB to help your customers answer location data and mapping questions like: Where is the nearest gas station? Should I build a gas station in this location? Is the house I want to buy located in a flood plain? What is the risk of providing insurance to this homeowner? What is the congressional district for my home address? What are all the counties that my congressional district serves? Resiliency & Scale: What PostGIS & NoSQL can’t offer you out of the box CockroachDB provides a horizontally scalable implementation of spatial data that allows you the ability to store and query large amounts of data with its unique indexing strategy and advanced cost-based optimizer. Each query can be distributed across multiple nodes for efficient processing through the use of our distributed SQL execution engine. CockroachDB can do all this while providing the most accurate data possible with serializable isolation. CockroachDB provides all of the above benefits while ensuring that you never need to worry about losing access to your spatial production data via its highly available architecture and online schema migrations. CockroachDB uses a distributed consensus algorithm to ensure that your data is highly available in the event of a node, availability zone, or region failure. This same architectural design also provides you with increased development flexibility as it ensures that schema changes never lock you out of production data. No longer will you need to choose between NoSQL’s eventually consistent data accuracy or PostGIS’s lack of scalability. Today, you can have both accuracy, reliability, and scale in CockroachDB. Familiar Postgres and PostGIS-compatible SQL You can use familiar SQL access patterns with PostGIS-(and OGC -) compatible implementations of important spatial features like: SQL data types (e.g., GEOMETRY, GEOGRAPHY) External formats (e.g., GeoJSON,  Well Known Text) Popular shapes (e.g., POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON, GEOMETRYCOLLECTION) SQL built-ins (e.g., ST_Contains) Postgres, and by extension PostGIS, is one of the most popular databases in the world. Developers often raved about PostGIS’s improved usability over Postgres’s native spatial data types as PostGIS provided revolutionary quality of life improvements including more than 300 built-ins and functions. Rather than reinventing the wheel, and creating our own syntax, we’ve chosen to provide developers with familiar PostGIS compatible SQL syntax. This means that you can use the same APIs that you use with PostGIS today while future-proofing your business with CockroachDB’s scale-out architecture. PostGIS helped launch Instagram and FourSquare (before each company eventually ran into sharding and scale concerns). It’s also been a key part of the stack for thousands of other applications from plucky startups to Fortune-500 behemoths. We know that regardless of the size of your company, the size of the mission-critical data needed to run your application will only continue to grow in today’s always-on, always-connected, modern world. CockroachDB helps you scale with your mission-critical data. You won’t be forced away from your initial stack when building with CockroachDB because we can accommodate massive scale. Here is a quick overview of the familair spatial data types and how they are supported: Try out spatial data with CockroachDB CockroachCloud lets you avoid the hassle of managing both a database (e.g., Postgres) and an extension (e.g., PostGIS) as we deliver one logical binary and one simple connection string. To get started today you can: Sign up for a CockroachCloud account and start a free trial today Learn more in our spatial reference documentation Watch our video about the basics of Spatial Data . What’s so special about spatial data? Ask questions in our public community Slack group", "date": "2021-04-30"},
{"website": "CockroachLabs", "title": "How to Improve Query Performance of Your Apps [Free Course]", "author": ["Diana Carroll", "Alex Yarosh"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-query-performance-for-devs/", "abstract": "Once you build a sample application, the logical next step is to improve its query performance and functionality. This may include tasks like adding indexes, reviewing the query execution plan, and optimizing sorting performance. Well… we’ve designed a free Cockroach University course to help you do just that! CockroachDB Query Performance for Developers introduces you to key CockroachDB features that will help you improve your application and take your understanding of CockroachDB to the next level. Continue reading to learn more about this course! Who should take Query Performance for Developers ? As the name suggests, this course is designed for developers but will also be useful for database administrators, system architects, and others involved in the application development process. It’s helpful to have experience with application development and some familiarity with SQL prior to enrolling. Additionally, before taking this course, we recommend taking either of our fundamental developer courses, which walk you through how to build a full-stack vehicle-sharing application: Fundamentals of CockroachDB For Python Developers Fundamentals of CockroachDB for Java Developers . If you are brand new to CockroachDB and distributed SQL technology, check out the courses in our “Getting Started” series: Introduction to CockroachDB and Distributed SQL and Practical First Steps with CockroachDB . (Note: if you pass the Practical First Steps with CockroachDB final exam, we will mail you free Cockroach-branded swag!) What does the Query Performance for Developers course include? This course is self-paced and will take you approximately 2 hours to complete [see the syllabus here ]. Specifically, this course includes: Nine videos Elevent hands-on labs A final exam During the course, you will create your own CockroachDB cluster in the cloud using CockroachCloud Free, then connect to your cluster using the SQL shell installed on your desktop. Then, you will learn how to: Analyze a query execution plan Add indexes to avoid expensive full table scans Improve sorting performance Efficiently query fields in JSON records At the end of the course, there will be a final exam to test your knowledge. When you pass, you will receive a Certificate of Completion. We encourage you to share your Certificate of Completion on LinkedIn (tag @cockroach-labs ) or Twitter (tag @cockroachDB ) to show off your new skills! What if I have questions? If you have questions or feedback, we encourage you to join the #cockroach-university channel in our CockroachDB Community Slack , where our team is available to chat. Since CockroachDB is Postgres-wire compatible , it speaks SQL and has a familiar look and feel to many developers. However, we recommend taking a look at our docs pages for references on things like installing and using the CockroachDB SQL shell and SQL command syntax . What’s next? You can get started today for free here . We have several more courses planned for the rest of this year starting with a course on schema design. Follow us on social media ( LinkedIn , Twitter ) for more updates on Cockroach University and new courses. We hope you enjoy this course and always welcome your feedback. Thanks for your interest in advancing your education with CockroachDB!", "date": "2021-04-27"},
{"website": "CockroachLabs", "title": "How to Model JSON Data in a Go App With CockroachDB", "author": ["Jack Christensen"], "link": "https://www.cockroachlabs.com/blog/json-go-cockroachdb/", "abstract": "*Guest post alert! Jack is a core maintainer of pgx, a PostgreSQL driver and toolkit for Go. He helped build the testing integration for CockroachDB and pgx. Jack blogs at https://www.jackchristensen.com/ * -– Many applications benefit from representing some data in a relational structure and some data in a more flexible document structure like JSON. Taking advantage of the JSON functionality available in a relational database can reduce the need for a dedicated object database, minimize infrastructure and application complexity, and improve performance. Modern SQL databases such as PostgreSQL and CockroachDB support both models with native support for storing, building, and manipulating JSON documents. The Go language and the pgx database driver also include functionality for working with relational and document data. For this example, we will model JSON data in a simple Go app that uses CockroachDB as its datastore. We will use a simplified products table for a shopping application. Every product has a SKU and a name. But there are many other attributes of a product such as color, size, weight, format, and capacity that only apply to some products. We will use one column for all these extra attributes. create table products (\n\tsku text primary key ,\n\tname text not null ,\n\textra_attributes jsonb not null ); insert into products (sku, name, extra_attributes) values ( 'A1000-BK' , 'A1000 Battery' , '{\"color\": \"black\", \"capacity\": 4000}' ),\n\t( 'A1200-RD' , 'A1200 Battery' , '{\"color\": \"red\", \"capacity\": 5500}' ),\n\t( 'TBLTCASE-10' , 'Tablet Case' , '{\"size\": \"10 inch\", \"material\": \"Leather\"}' )\n; The following code snippets will assume a database connection is already established. See https://www.cockroachlabs.com/docs/v20.2/build-a-go-app-with-cockroachdb for more information on connection setup. We can use a string or a []byte to read or write a database jsonb type. Then we can use the encoding/json package to convert it to and from our application data type. However, with pgx that isn’t necessary. pgx can automatically marshal and unmarshal values into JSON. type Product struct { SKU string Name string ExtraAttributes map [ string ] interface {}\n\t} // ... var product Product err = conn . QueryRow ( context . Background (), \"select * from products where sku = $1\" , \"A1000-BK\" ,\n\t). Scan ( & product . SKU , & product . Name , & product . ExtraAttributes ) if err != nil { // handle error } fmt . Printf ( \"%#v\\n\" , product . ExtraAttributes ) // => map[string]interface {}{\"capacity\":4000, \"color\":\"black\"} A map[string]interface{} can handle any JSON object, but may be a bit awkward to work with. If we know the object structure ahead of time we can read directly into a Go struct. type BatteryAttributes struct { Color string `json:\"color\"` Capacity int32 `json:\"capacity\"` } type Battery struct { SKU string Name string ExtraAttributes BatteryAttributes } // ... err = conn . QueryRow ( context . Background (), \"select * from products where sku = $1\" , \"A1000-BK\" ,\n\t). Scan ( & battery . SKU , & battery . Name , & battery . ExtraAttributes ) if err != nil { // handle error } fmt . Printf ( \"%#v\\n\" , battery . ExtraAttributes ) // => BatteryAttributes{Color:\"black\", Capacity:4000} We can also build JSON documents from relational data directly in the database. For example we can use the jsonb_agg and jsonb_build_object functions to build a JSON document that lists all products. var buf [] byte err = conn . QueryRow ( context . Background (), \"select jsonb_agg(jsonb_build_object('sku', sku, 'name', name)) from products\" ,\n\t). Scan ( & buf ) if err != nil { // handle error } fmt . Println (string( buf )) // => [{\"name\": \"A1000 Battery\", \"sku\": \"A1000-BK\"}, {\"name\": \"A1200 Battery\", \"sku\": \"A1200-RD\"}, {\"name\": \"Tablet Case\", \"sku\": \"TBLTCASE-10\"}] This approach can both simplify the Go layer and improve performance. In summary, taking advantage of the JSON functionality available in a relational database like CockroachDB can reduce the need for a dedicated object database, minimize infrastructure and application complexity, and improve performance.", "date": "2021-05-04"},
{"website": "CockroachLabs", "title": "Inc Magazine Honors Cockroach Labs on Best Workplaces List 2021", "author": ["Lindsay Grenawalt"], "link": "https://www.cockroachlabs.com/blog/inc-magazine-best-workplaces-2021/", "abstract": "Cockroach Labs was named for resilience and survival. The last year has tested us all in unimaginable ways, and it was the strength and resilience of our team and our community that kept us afloat and propelled us forward. So it is an absolute honor that, after the year we’ve all had, we have been named to Inc. magazine’s annual list of the Best Workplaces for 2021 . The list is the result of a wide-ranging and comprehensive measurement of American companies that have created exceptional workplaces and company culture. Inc. collected data from thousands of companies, and every company took part in an employee survey that covered management effectiveness, perks, and fostering employee growth. The companies on Inc.’s Best Workplaces list set an example that we all can learn from. We are humbled to be named alongside them. “The definition of a positive workplace has changed drastically over the past year,” says Inc. magazine editor-in-chief Scott Omelianuk. “Stocked fridges and nap pods were no longer perks many companies could rely on once work went remote. So, this year’s list is even more important as it reveals organizations that continue to enrich the lives of its employees amid a pandemic.” From the very beginning, our goal was to build a human-forward business where our employees can balance their careers with their personal lives. Over the last several years, we have challenged the status quo and built programs for better recruiting , more flexibility, better support , more opportunities in and out of the office , and fostered an environment where diversity of thought exists and is celebrated. We will continue to strive for an innovative and inclusive workplace for all. If you are in the market for your next role, consider joining us on our mission to build the next great database company. We are expanding with open positions in almost all departments. Explore open roles at cockroachlabs.com/careers and hear directly from the folks building CockroachDB .", "date": "2021-05-12"},
{"website": "CockroachLabs", "title": "CockroachDB 21.1: The Most Powerful Global Database is Now the Easiest", "author": ["Meagan Goldman"], "link": "https://www.cockroachlabs.com/blog/cockroachdb-21-1-release/", "abstract": "Today we’re excited to announce the release of CockroachDB 21.1, the latest version of our distributed SQL database. For this release, we took a step back and asked how we can make even more development teams successful with multi-region clusters. Thousands of engineering hours and Github tickets later, the result of our efforts is a dramatically simpler and more accessible developer experience for managing the location of data . CockroachDB’s multi-region capabilities are a core set of features that let you manage data across multiple cloud availability zones and regions, to optimize latency and availability. For 21.1, our team has re-designed and re-architected these features, so you can now control your data’s latency and availability with just a few declarative SQL statements . This new abstraction brings multi-region capabilities to all developers at any experience level, so they can start small with a single region, grow as they need, and easily scale data across regions to deliver faster, more reliable experiences to users everywhere. We’ve also delivered a variety of other updates in CockroachDB 21.1, so you can now: More easily optimize and troubleshoot SQL queries with more details now available in EXPLAIN and EXPLAIN ANALYZE , and more details on the DB Console’s Statements , Transactions , and SQL Dashboard pages–including information on query contention. Store JSON and spatial data in multiple cloud regions with the addition of partitioned inverted indexes . Benefit from faster and more memory-efficient query execution , due to updates to our vectorized execution engine . Work more seamlessly with the developer tools ActiveRecord , Hibernate , Liquibase and Datagrip with updates in CockroachDB’s compatibility with these tools. More easily test and migrate applications that previously used Postgres with improvements to IMPORT PGDUMP so you can now ignore unsupported statements and log these statements in an outfile . Get more configuration control over logs , with newly modernized logging infrastructure that includes: the ability to organize events into channels, the ability to redirect events to monitoring tools, structured logging, and new formatting options (like JSON) to enable automatic processing with different tools. Stream data from your cluster to a greater variety of tools , with SASL/SCRAM encryption support now available for CockroachDB’s native Change Data Capture (CDC) capability. Monitor your cluster with Elastic’s Kibana , with our recommended CockroachDB dashboards now available directly in Kibana . And more! Additionally, we’ve recently made the following improvements to CockroachCloud, our fully managed cloud version of CockroachDB: Monitor your CockroachCloud cluster with new email alerts for when CPU, memory, storage, or IOPS exceed a threshold over time, and integrate more easily with incident response tools like PagerDuty. View SQL statements and database sessions right in the CockroachCloud Console , to optimize and troubleshoot performance. SOC 2 Type 2 Certification : we’ve completed our security certification for SOC 2 compliance for CockroachCloud. Read on for more details on what’s new, or head over to our Docs page for a deep-dive into the full list. And as always, we’d love to hear what you think on our Community Slack . CockroachDB 21.1 makes multi-region data dead-simple CockroachDB’s horizontal scale eliminates the pain developers experience with common techniques like manual sharding. Scaling up or down is as easy as adding or removing a node, and data automatically distributes across nodes without any manual intervention needed. This architecture, combined with 3x data replication, enables CockroachDB’s high availability. Your cluster can survive losing a node without any impact. In addition to these default settings, you can also use CockroachDB’s unique multi-region features to customize your data’s location, availability, and latency based on your application’s needs. These features let you scale CockroachDB across multiple availability zones and cloud regions, then anchor data to location by database, table, and row. [Image 1:] Scale one logical cluster across multiple cloud regions in CockroachDB. With CockroachDB’s multi-region features, developers can achieve data that survives entire region failures, serve low-latency reads and writes globally, and help comply with data regulations like GDPR. While these capabilities are incredibly valuable, the previous methods to configure them in CockroachDB were fairly complex. We realized there was room to make them much easier to use, so they could be even more impactful. In CockroachDB 21.1, you can now control your data’s availability and latency with only a few declarative SQL statements. Pick your survival and latency goals, and CockroachDB does everything for you under the covers. It’s really as simple as four steps (yes, that’s right, just four): Deploy your nodes to multiple regions Define which regions you want your databases to operate in using a few SQL statements Define survival goals for your databases using a few SQL statements Create or alter tables using one of three new table locality settings , again through a few SQL statements: Regional tables : fast reads and writes from one region Regional by row tables : fast reads and writes from one region, and different rows in the table can be optimized for access from different regions Global tables: fast reads from all regions, at the expense of slower writes [Image 2:] In CockroachDB, distribute data across multiple regions for fast writes with just an ALTER TABLE command. In addition to these usability updates, we’ve also added updates that: Make it simpler to expand from a single region to multiple regions, by just adding a new region to your database Enable your cluster to understand which region data was inserted into, and subsequently keep that data in that region, eliminating the need for complicated application logic We believe your aspirations shouldn’t be constrained by your infrastructure. With the updates in CockroachDB 21.1, everyone—not just massive teams and tech giants—can meet high demands for application performance and availability. And you no longer need to start your business on a platform that’s “good enough,” hamstringing yourself for when your business grows, and suffering the pain of a migration. Now, you can build from the start on a platform that will take you from one cloud region to a global footprint, from three people in a garage to a multi-million dollar business. To see these features for yourself, we’ve put together a quick demo video. Get started on CockroachDB, instantly and for free This blog post covers just a sampling of the updates in 21.1. For a full list, head over to the 21.1 Docs . To try out these features yourself, spin up a free cluster instantly on CockroachCloud , our CockroachDB-as-a-Service offering. Also, please join us on May 19th for a livestream, The Cockroach Hour: ALTER DATABASE SURVIVE REGION FAILURE . We’ll discuss the challenges of distributed services, why multi-region applications matter, and how our engineering and product teams partnered to make these concepts simple in 21.1. Finally, we love feedback and would love to hear from you. Please join our community and let us know your thoughts! < Get Started with CockroachDB for Free >", "date": "2021-05-18"},
{"website": "CockroachLabs", "title": "How Banks and Fintech Apps Innovate (And Compete) Around Cloud-Native", "author": ["Jessica Edwards"], "link": "https://www.cockroachlabs.com/blog/banks-fintech-cloud-native/", "abstract": "Modern fintech apps like Betterment and Robinhood push the limits of what consumers expect from banking apps. One of their biggest competitive advantages is that they can support real-time decision-making and transaction processing, providing users with instant decisions, personalized experiences, and feature-rich applications. To keep up, banks and financial services organizations need to embrace digital transformations that enable them to innovate around transaction services. These services can include dynamic pricing, hyper-personalized content, real-time business process optimization, and fraud detection. Financial services organizations have a wealth of transaction data. By finding an efficient way to gather, store, automate, and analyze this data, they can gain invaluable customer insights and build solutions that are miles ahead of the competition. This is where cloud-native infrastructures come in – making data more accessible and providing timely insights to enable automation and guide informed decision-making. Banks need to adopt these modern, cloud-native data platforms, replacing the likes of Oracle , IBM DB2, NoSQL, and other legacy systems . Cloud-native = containers + microservices Another key benefit of cloud-native architecture for financial services organizations is that applications are distributed and made up of loosely coupled microservices. This provides low latency, even when operating at high-volumes, by moving the data closer to the source. Distributed applications also provide better uptime since data stored in a centralized location is vulnerable to outages or attacks and makes it easier for banks to add new state-of-the-art capabilities and services. Cloud native architecture is designed to integrate seamlessly with one or more private, public, or hybrid cloud platforms. It uses modern application development principles and tools, like containers and microservices, to build portable, scalable applications. By their very definition, containers provide a simple way for processes and applications to be bundled and used throughout the entire application life cycle – from development to test to production. Kubernetes, the container orchestrator that is often synonymous with cloud-native deployments, has been a popular choice for fintech organizations like Nubank and Square. Even larger, more traditional banks, like Ant Financial, BlackRock, and Capital One, have moved off of or integrated their legacy software with Kubernetes and other open source tools in the cloud-native community. By building a cloud-native infrastructure, financial services organizations can create a dynamic environment composed of independent processes that all work together seamlessly and make it easy to add new features. Beyond transaction services, cloud-native architectures are driving innovation in areas like data science, IoT, and edge computing. What to look for in your cloud-native database: performance, flexibility, scale, and Kubernetes Banks and financial services organizations have found, however, that moving to a cloud-native architecture can introduce new challenges. Legacy data platforms were not designed to handle the complexity of distributed environments or large numbers of services and often break down when stretched. To fill this gap, financial services organizations should adopt a flexible database solution that provides easy access to the transaction data needed to add new innovative and personalized services. It should be for the cloud built from the ground up to take advantage of the scale and resiliency yet capable of being deployed on-premises or across hybrid or multi-cloud environments. It should also be easy to add new nodes for scale and automatically rebalance and replicate data throughout a distrusted system. Some of the critical database features that financial organizations should look for include: ACID compliance and consistent transactions: A database needs to guarantee ACID (atomicity, consistency, isolation, and durability) compliance to ensure the validity of transaction data . No single node should be responsible for handling all data affecting a transaction. Built for Kubernetes: A modern database should be architected to integrate with Kubernetes and other cloud-native tools and deliver scalability and resilience in a Kubernetes environment. Compatibility: A database should fit in with an organization’s existing SQL expertise and be compatible with its current applications. It should also align with the development approaches used by the organization. Support for multi-national customers: Geo-partitioning allows organizations to determine where distributed data resides, keeping it close to the source to reduce latency. It also helps ensure compliance with data protection and privacy laws. By combining these features, banks and financial services organizations can quickly and efficiently create new transaction services and personalized features. Finding a partner in Cockroach Labs One such database is CockroachDB , the cloud-native, distributed SQL database that provides next-level consistency, ultra-resilience, data locality, and massive scale to modern cloud applications. But organizations need to think beyond just features. As with other tools and technologies in the cloud-native space, financial services organizations can look for free and open source software (FOSS) solutions and build around them. These require a substantial time investment and a team with deep expertise to make them run in production in a high-stakes enterprise environment. The simpler solution is to partner with a solutions provider, like CockroachLabs, that brings both the technology and deep industry expertise. “At JPMorgan Chase&mldr; CockroachDB provides us with a modern database platform that has accelerated our momentum toward cloud-first, resilient infrastructure and a new class of modern financial applications,” said George Sherman, Global Technology Infrastructure CIO at JPMorgan Chase. Cockroach Labs was inducted into the JPMorgan Chase Hall of Innovation to help the financial services leader build next-generation applications and infrastructure. For a real-world example, an American financial data firm that was frustrated by its legacy Oracle database architecture turned to CockroachDB to consolidate its several databases worldwide into a single platform and migrate its applications to the cloud. Now, the database is capable of deploying across both on-premise machines and cloud data centers. The firm also took advantage of geo-partitioning to replicate data across its regions and minimize latency for its identity and access management system. It plans to migrate more and add brand new apps to its CockroachDB deployment. Additionally, one of the largest financial software companies in the U.S adopted CockroachDB when it needed a new database solution for its customer identity access management (CIAM). It decided to deploy CockroachDB to its AWS regions for consistency, resilience against outages, and high performance across multiple regions. The company was able to improve its customer login experience by moving off of Oracle GoldenGate. Now, because of its distributed nature, it only takes about 280 milliseconds to achieve login authorization no matter where the customer is located. CockroachDB delivers distributed SQL and combines the familiarity of relational data with elastic scale and bulletproof resilience. CockroachDB is: Simple to scale Architected to handle unpredictability and survive failures ACID-compliant Cloud-native and designed for Kubernetes The only database solution that offers geo-partitioning for multi-region deployments Many global banks, financial services organizations, and fintech companies have already adopted CockroachDB for their transaction-based workloads. Download the eBook ‘How Financial Service Companies Can Successfully Migrate Critical Applications to the Cloud’ to learn more.", "date": "2021-04-28"},
{"website": "CockroachLabs", "title": "Career Change: How I Became a Developer Advocate", "author": ["Amruta Ranade"], "link": "https://www.cockroachlabs.com/blog/career-change-how-i-became-a-developer-advocate/", "abstract": "Last year, I changed careers from Tech Writing to Developer Relations at Cockroach Labs. This blog post details how the Flex Friday, Learning is Good, and Roachermobile programs at Cockroach Labs helped me reinvent my career. The Backstory My first job out of college was in 2009 as an Electronics Engineer in India, testing electronic circuits and writing datasheets. I liked the documentation tasks more and switched to tech writing full-time. After 5 years as a tech writer in India, I realized I had run the course of being a self-taught tech writer and moved to the US to pursue a formal education in technical communication. By the time I completed my Master’s in Technical Communication degree, I landed the Senior Tech Writer position at Cockroach Labs. By August 2019, I was nearing my 10-year anniversary as a tech writer and itching for a new challenge. Enter — Developer Relations. Flex Fridays FTW At first glance, Developer Relations seemed like the perfect fit for me with the right mix of tech, content, and community. But moving to DevRel full-time was a career risk — leaving a secure position as a Staff Tech Writer to pursue the shiny new role everyone was talking about. I remembered trying out the documentation tasks while still working as an Electronics Engineer had helped me make the career change from engineering to tech writing. I decided to try out DevRel tasks before making an official move. Thankfully, we have the Flex Fridays program at Cockroach Labs that allows us to use Fridays to learn and experiment with projects outside our job descriptions. To make sure I was making an informed decision, I used my Flex Fridays to research what Developer Advocates do and designed a series of experiments to try out different aspects of the career. I called the project Prototyping a Career in Developer Relations and documented it on my YouTube channel. Learning is Good Flex Fridays gave me the time for my DevRel project, but some experiments required monetary support to travel to and participate in conferences. To fund my conference travel, I made use of another of my favorite perks at Cockroach Labs — the Learning is Good program — which gives each Roacher $2500 per year for professional development through courses, conferences, coaching, and so on. To the Roachermobile! The experiment helped me envision myself in the role of a Developer Advocate. Once I knew it was something I wanted to pursue seriously, I jumped on the Roachermobile — the aptly named internal mobility program at Cockroach Labs. As this blog post explains, \" An internal mobility program is a system that provides opportunities for current employees to grow their career within the company, but outside of their current team ... Where a promotion can be thought of simply as moving up a rung on the ladder, internal mobility would be more akin to hopping onto another ladder and traversing a unique path. \" The Roachermobile allows Roachers who perform well and meet expectations in their current roles for at least one year to qualify for an internal role change. The Roacher must then complete a full interview process assessing the same experience and skill requirements as external candidates to ensure a fair hiring process and meet the goal of hiring the best candidate for all open roles. As part of the Roachermobile program, I was given the opportunity to interview for the position of the first Developer Advocate at the company, and I must have done well on the interview because I got the job! Of Champions and Cheerleaders There were some hiccups along the way since the Roachermobile program as well as the DevRel function were completely new to Cockroach Labs at the time. My manager, Jesse Seldess, was my unwavering champion throughout the transition. There were times when I got frustrated and lost confidence in myself or the process. But Jesse, my team, and my coworkers from Engineering, Product, and Marketing cheered me on through the challenging moments. It's rare to find a tech company where you feel seen and heard. It's rarer to find a company that values you enough to make space not only for who you are but who you aspire to be. I’m proud of how far I’ve come and excited to grow into the new role! Curious about how you can grow your career with our team? We're hiring plenty! Check out our open roles here .", "date": "2021-05-03"},
{"website": "CockroachLabs", "title": "How Our Open Interview Process Helps You Land the Job", "author": ["Devonaire Ortiz"], "link": "https://www.cockroachlabs.com/blog/how-our-open-interview-process-helps-you-land-the-job/", "abstract": "At Cockroach Labs, we’ve long worked at reducing bias for better, more inclusive recruiting outcomes . We do this by removing resumes throughout the hiring process so that interviewers can focus on you, the candidate— not your background. We also emphasize exercise-based interviews that give people the opportunity to demonstrate that they can do the job instead of recounting past experience. These practices are rooted in a basic, human principle for hiring: as a company looking for amazing talent, we want to eliminate hurdles that might keep you from showing us your strengths in an interview, not create them. Four years ago, we saw another opportunity to help people put their best foot forward: we decided to make our interview prep materials openly available. Often, you’re asked to apply for a job with no insight into what the interview process looks like, how long it will take, what skills you’ll be evaluated on, and more. With our open-source interview repository , we give you a transparent picture of the entire recruiting cycle at Cockroach Labs whether or not you apply. We think that an equal ability to prepare for our interviews makes them fairer. Setting You Up for Success When we were a smaller company with less specialized roles, we updated our repository with the latest interview for each role across our team. As we grew and spun up dozens of new roles each quarter, that became harder to do. Instead, our interview guide now provides you with a comprehensive overview of the recruiting process, Cockroach Labs’ values, and each section of our repository features example interviews relevant to the skillsets our teams value. By reviewing these materials, you should be able to start interviewing here with confidence and familiarity. Throughout your interview process at Cockroach Labs, your recruiter will provide you with interview prep well in advance of your next exercise, whether it be through a pre-interview prep call or a document outlining what to expect during the meeting and what work you should do in advance. We know that exercise-based interviewing is a time investment, so we aim to make it as seamless as we can. No “Gotcha” Questions We’ve all heard of oddball interview questions in tech like “How would you get an elephant into a refrigerator?” They’re said to draw out what some consider unmeasurable qualities, but to put it simply, we don’t use them. These “gotcha” questions don’t prompt anyone to show their skills or be creative, but to improvise. Interviews should be challenging, but they shouldn’t be designed to trip you up. Making it Easier to Be Your Best Self We don’t want you to jump through hoops to join us: instead, our goal is to provide insight into what it’ll be like to interview here. That way, you can make an informed decision when applying and preparing for your interviews. When you sit down for an interview at Cockroach Labs, you can be sure that we’re focused on you and your performance, not your resume. We know that interviews are nerve-wracking, and our hope is that making it easier to come prepared will relieve some of the stress they come with. Our recruiting process is built for humans to be their best selves and share the strengths that will land them the job. We’re always looking for people to build great things with us— check out our careers page if this sounds like you.", "date": "2021-05-11"},
{"website": "CockroachLabs", "title": "What Parental Leave is Like at Cockroach Labs", "author": ["Lauren Weber"], "link": "https://www.cockroachlabs.com/blog/parental-leave/", "abstract": "Welcoming a child while working at a tech startup can seem like an overwhelming life change. In a world where everyone is already strapped for time, you’re going to take three months away from your role!? It sounds scary, but at Cockroach Labs, we’re working on ways to make this stressful time easier, and I’m excited to share more about our programs for Roachers who are starting or growing their families. For starters, if you think the technology industry is just startups full of recent college graduates, think again! At Cockroach, our average employee is 36 years old. 30% of our team has children, and we average 2 children per family with more Little Roachers on the way! All Parents Welcome As much as possible, our guides and programming are focused on using inclusive language. We prefer to talk about growing families rather than just childbirth specifically. Where we used to give out onesies upon a child’s arrival, we recently switched to blankets because they’re suitable for a broader age range of new children (and more long-lasting than the onesies anyways). All the programs mentioned here apply to any parent at Cockroach Labs, regardless of gender and how the new family member is arriving. We were never prouder than when our co-founder, Ben Darnell, announced he’d be unplugging for twelve weeks with the arrival of his first child. When asked what he was most excited about, another new dad said, “Bonding with my baby. Helping to equalize parental expectations and responsibilities.” How we create space for Little Roachers Thanks to our partnership with Cleo , we had experts help us start Parental Leave Guides for employees. Even before Roachers have announced that their family will be growing, they can access our Preparing for Parental Leave Guide with pointers on breaking the news to their manager and team. The guide continues with communicating time off needs in advance of the child’s arrival, documenting responsibilities for handoff, and tips for the actual transition to leave. Inevitably, this can be a time for nerves and our goal with the guides is to recognize and normalize those feelings. Dan Kelly in Marketing, who took 12 weeks of leave, said, “It doesn't feel good to abandon your team for 3 months. Before I took the leave, I kept asking to make sure it was truly okay, and everyone kept assuring me that it was.” Our Leave Benefits All parents who have worked at Cockroach Labs for 30 days or more are eligible for up to 12 weeks of fully paid parental leave. Together with our flexible time off policy, Roachers can take an additional two weeks of paid time off (PTO) for a cumulative parental leave of 14 weeks during the year following a child’s arrival. Our benefits are coordinated with state-mandated programs, as applicable, and our People Operations team works with each Roacher on a customized leave plan. We encourage parents to take leave time in minimum one-week increments, which eases the administrative burden for everyone involved and increases quality time for families. Of course, some parents decide to take one month at a time at various points throughout the first year of their child’s arrival. Other parents are back in action after a week or two away, and many exhaust every moment of 14 weeks all at once. We’re as flexible as possible with plans that feel comfortable for parents and have found that preparing and communicating plans with teams is key. Giving families support and options with the time away is our primary aim. We offer several perks in association with the arrival of a Little Roacher. Our welcome pack includes a bib, and we wrap two board books: a \"C is for Cockroach\" bug alphabet book and a first coding book. A $500 baby bucks bonus is a nod to the uptick in expenses that parents experience with a growing family. While these perks are fun, much of the positive feedback we get is for services and support through Cleo . Parents are most grateful for access to lactation and sleep consultants that help solve a few of the key concerns during the first year following birth -- both are provided through Cleo. Re-Roaching: Returning after Parental Leave Retaining the great people we’ve attracted to Cockroach Labs is a critical goal of our parental support strategy. An often-cited statistic from Harvard Business Review notes that 43% of women with children will voluntarily take time off from their careers at some point. We want to ensure the decision to off-ramp from a career is not impacted by a lack of support at Cockroach Labs. Accordingly, we partner with managers to develop a Return to Roach Guide for parents who have taken an extended leave. This guide contains logistics details about our Family Room which is prioritized as a lactation space (but includes a changing station and a few toys if a little Roacher swings by the office with their parent). Parents are prompted to consider if they’d like a modified return to work schedule. The simplest form of this is a schedule where Flexible Fridays convert to Family Fridays for an 80% schedule that is fully paid for up to 12 weeks. Our managers are used to developing starter projects for new hires, so returning Roachers have a 30-60-90 day plan with a work project focus accompanied by a framework for learning, performance, and personal goal-setting during the transition. Just as our onboarding program includes a Roachmate onboarding buddy, the Return to Roach includes a ReRoachmate. ReRoachmates are allies and parenting peers who help returning Roachers in many of the ways that new hires get help. After 3 months away, it can feel like you’re joining a new company, so ReRoachmates help us normalize the transition back to work. ReRoachmates are encouraged to swap stories about what happened at work and how it was at home. Celia La, an engineer who took 14 weeks of leave, said, “Be kind to yourself: It'll take 3 months after returning to work to feel back to pre-baby work-self. Lean on your fellow parents; sometimes, just talking about how you're feeling to others can be helpful.” ReRoachmates are our way of networking parents across the company to share perspectives and tips on working parent life. What’s Next for Family Benefits? Having built strong resources for Roacher parents, we plan next to develop leave guides for managers to mirror the guides we have for parents themselves. Proper expectation-setting is a big part of a successful parental leave experience, and we hear from parents that this can always be improved. At the same time, we strive to stay current with our benefits. Where we already offer a medical plan with up to $10,000 of advanced reproductive technology, we recognized that parents taking advantage of our leave benefits faced stressors in the process of using those benefits. To address that, we are excited to be bringing on a new partner, Cocoon , that specializes in leave management programming. Cocoon was created to help employees navigate the labyrinth of state and insurance benefits. They provide an improved leave experience by removing stressful and confusing processes around claims, and ensure employees are paid promptly. Additionally, they provide support for the transition of going out and returning from parental leave. Suffice to say, I’m proud of what we can offer Roachers with families today and expect that our programs will only be enriched in the future. If you’re a parent or considering parenthood in the long haul, be sure to check out our careers page !", "date": "2021-05-17"},
{"website": "CockroachLabs", "title": "Living Without Atomic Clocks", "author": ["Spencer Kimball", "Irfan Sharif"], "link": "https://www.cockroachlabs.com/blog/living-without-atomic-clocks/", "abstract": "The design of CockroachDB is based on Google’s Spanner data storage system. One of the most surprising and inspired facets of Spanner is its use of atomic clocks and GPS clocks to give participating nodes really accurate wall time synchronization. The designers of Spanner call this “TrueTime” , and it provides a tight bound on clock offset between any two nodes in the system. This lets them do pretty nifty things! We’ll elaborate on a few of these below, but chief among them is their ability to leverage tightly synchronized clocks to provide a high level of external consistency (we’ll explain what this is). If someone knows even a little about Spanner, one of the first questions they have is: “You can’t be using atomic clocks if you’re building an open source database; so how the heck does CockroachDB work?” It’s a very good question, and one we (try) to elaborate on here. As a Spanner-derived system, our challenges lie in providing similar guarantees of external consistency without having magical clocks at hand. CockroachDB was intended to be run on off-the-shelf commodity hardware, on any arbitrary collection of nodes. It’s “cloud neutral” in that it can very well span multiple public and/or private clouds using your flavor-of-the-month virtualization layer. It’d be a showstopper to require an external dependency on specialized hardware for clock synchronization. So what does CockroachDB do instead? Well, before answering that question, let’s dig a little deeper into why TrueTime was conceived for Spanner. The Importance of Time in Distributed Systems Time is a fickle thing. For readers unfamiliar with the complexities around time in distributed systems research, the thing to know about it all is this: each node in the system maintains its own view of time, powered by its own on-chip clock device. This clock device is rarely ever going to be perfectly in sync with other nodes in the system, and as such, there’s no “absolute” time to refer to. Existentialism aside, perfectly synchronized clocks are a holy grail of sorts for distributed systems research. They provide, in essence, a means to absolutely order events, regardless of which node an event originated at. This can be especially useful when performance is at stake, allowing subsets of nodes to make forward progress without regard to the rest of the cluster (seeing as every other node is seeing the same “absolute” time), while still maintaining global ordering guarantees. Our favorite Turing award winner has written a few words on the subject here . Linearizability By contrast, systems without perfectly synchronized clocks (read: every system) that wish to establish a complete global ordering must communicate with a single source of time on every operation. This was the motivation behind the “timestamp oracle” as used by Percolator . A system which orders transactions T1​ and T2​ in the order \\[T1​,T2​] provided that T2​ starts after T1​ finishes, regardless of observer, provides for the strongest guarantee of consistency called “external consistency” . To confuse things further, this is what folks interchangeably refer to as “linearizability” or “strict serializability”. Andrei has more words on this soup of consistency models . Serializability Let’s follow one more tangent and introduce the concept of “serializability”. Most database developers are familiar with serializability as the highest isolation level provided by the ANSI SQL standard. It guarantees that the constituent reads and writes within a transaction occur as though that transaction were given exclusive access to the database for the length of its execution, guaranteeing that no transactions interfere with each other. In other words, no concurrent transaction T2​ is able to read any partially-written state of transaction T1​ or perform writes causing transaction T1​ to read different values for the same key over the course of its execution. In a non-distributed database, serializability implies linearizability for transactions because a single node has a monotonically increasing clock (or should, anyway!). If transaction T1​ is committed before starting transaction T2​, then transaction T2​ can only commit at a later time. In a distributed database, things can get dicey. It’s easy to see how the ordering of causally-related transactions can be violated if nodes in the system have unsynchronized clocks. Assume there are two nodes, N1​ and N2​, and two transactions, T1​ and T2​, committing at N1​ and N2​ respectively. Because we’re not consulting a single, global source of time, transactions use the node-local clocks to generate commit timestamps. To illustrate the trickiness around this, let’s say N1​ has an accurate one but N2​ has a clock lagging by 100ms. We start with T1​, addressing N1​, which is able to commit at ts=150ms. An external observer sees T1​ commit and consequently starts T2​ (addressing N2​) 50ms later (at t=200ms). Since T2​ is annotated using the timestamp retrieved from N2​’s lagging clock, it commits “in the past”, at ts=100ms. Now, any observer reading keys across N1​ and N2​ will see the reversed ordering, T2​'s writes (at ts=100ms) will appear to have happened before T1​'s (at ts=150ms), despite the opposite being true. ¡No bueno! (Note that this can only happen when the two transactions access a disjoint set of keys.) Figure 1. Causally related transactions committing out of order due to unsynchronized clocks. The “anomaly” described above, and shown in Figure 1, is something we call “causal reverse”. While Spanner provides linearizability, CockroachDB only goes as far as to claim serializability, though with some features to help bridge the gap in practice. I’ll (lazily) defer to Andrei again, he really does cover a lot of ground here . How does TrueTime provide linearizability? OK, back to Spanner and TrueTime. It’s important to keep in mind that TrueTime does not guarantee perfectly synchronized clocks. Rather, TrueTime gives an upper bound for clock offsets between nodes in a cluster. Synchronization hardware helps minimize the upper bound. In Spanner’s case, Google mentions an upper bound of 7ms. That’s pretty tight; by contrast, using NTP for clock synchronization is likely to give somewhere between 100ms and 250ms. So how does Spanner use TrueTime to provide linearizability given that there are still inaccuracies between clocks? It’s actually surprisingly simple. It waits. Before a node is allowed to report that a transaction has committed, it must wait 7ms. Because all clocks in the system are within 7ms of each other, waiting 7ms means that no subsequent transaction may commit at an earlier timestamp, even if the earlier transaction was committed on a node with a clock which was fast by the maximum 7ms. Pretty clever. Careful readers will observe that the whole “wait out the uncertainty” idea is not predicated on having atomic clocks lying around. One could very well wait out the maximum clock offset in any system and achieve linearizability. It would of course be impractical to have to eat NTP offsets on every write, though perhaps recent research in this area may help bring that down to under a millisecond. Fun fact: early CockroachDB had a hidden --linearizable switch that would do essentially the above, so theoretically, if you did have some atomic clocks lying around (or generally an acceptable maximum clock offset), you’d get Spanner-like behavior out of the box. We’ve since removed it given how under-tested it was, but perhaps it would make sense to resurrect it as cloud providers trend towards exposing TrueTime-like APIs . Chip-scale atomic clocks are a reality; putting one on server motherboards would beat the pants off a quartz crystal oscillator. How important is linearizability? Stronger guarantees are a good thing, but some are more useful than others. The possibility of reordering commit timestamps for causally related transactions is likely a marginal problem in practice. What could happen is that examining the database at a historical timestamp might yield paradoxical situations where transaction T1​ is not yet visible while transaction T2​ is, even though transaction T1​ is known to have preceded T2​, as they’re causally related. However, this can only happen if (a) there’s no overlap between the keys read or written during the transactions, and (b) there’s an external low-latency communication channel between clients that could potentially impact activity on the DBMS. For situations where reordering could be problematic, CockroachDB makes use of a “causality token”, which is just the maximum timestamp encountered during a transaction. It’s passed from one actor to the next in a causal chain, and serves as a minimum timestamp for successive transactions to guarantee that each has a properly ordered commit timestamp. Of course, this mechanism doesn’t properly order independent causal chains, though imagining a use case where that’s a problem requires creativity. But there’s a more critical use for TrueTime than ordering transactions. When starting a transaction reading data from multiple nodes, a timestamp must be chosen which is guaranteed to be at least as large as the highest commit time across all nodes. If that’s not true, then the new transaction might fail to read already-committed data – an unacceptable breach of consistency. With TrueTime at your disposal, the solution is easy; simply choose the current TrueTime. Since every already-committed transaction must have committed at least 7ms ago, the current node’s wall clock must have a time greater than or equal to the most recently committed transaction. Wow, that’s easy and efficient. So what does CockroachDB do? How does CockroachDB choose transaction timestamps? The short answer? Something not as easy and not as efficient. The longer answer is that CockroachDB discovers an appropriate timestamp for the transaction as it proceeds, sometimes restarting it at a later timestamp if needed. As mentioned earlier, the timestamp we choose for the transaction must be greater than or equal to the maximum commit timestamp across all nodes we intend to read from. If we knew the nodes which would be read from in advance, we could send a parallel request for the maximum timestamp from each and use the latest. But this is a bit clumsy, since CockroachDB was designed to support conversational SQL where the read/write sets are indeterminate, we can’t know the nodes in advance. It’s also inefficient because we would have to wait for the slowest node to respond before even starting execution. Aside: readers may be interested in Calvin and SLOG , a family of research systems developed around declaring read/write sets upfront (though giving up conversational SQL) which consequently manages to avoid this class of problems. What CockroachDB does instead is actually surprisingly similar to what Spanner does, though with much looser clock synchronization requirements. Put simply: While Spanner always waits after writes, CockroachDB sometimes retries reads. When CockroachDB starts a transaction, it chooses a provisional commit timestamp based on the current node’s wall time. It also establishes an upper bound on the selected wall time by adding the maximum clock offset for the cluster \\[commit timestamp, commit timestamp + maximum clock offset] . This time interval represents the window of uncertainty. As the transaction reads data from various nodes, it proceeds without difficulty so long as it doesn’t encounter a key written within this interval. If the transaction encounters a value at a timestamp below its provisional commit timestamp, it trivially observes the value during reads and overwrites the value at the higher timestamp during writes. It’s only when a value is observed to be within the uncertainty interval that CockroachDB-specific machinery kicks in. The central issue here is that given the clock offsets, we can’t say for certain whether the encountered value was committed before our transaction started. In such cases, we simply make it so by performing an uncertainty restart , bumping the provisional commit timestamp just above the timestamp encountered. Crucially, the upper bound of the uncertainty interval does not change on restart, so the window of uncertainty shrinks. Transactions reading constantly updated data from many nodes may be forced to restart multiple times, though never for longer than the uncertainty interval, nor more than once per node. As mentioned above, the contrast between Spanner and CockroachDB is that Spanner always delays writes for a short interval, whereas CockroachDB sometimes delays reads. How long is that delay?  It depends primarily on how often the same row is being read and written at nearly the same time. Most of the time when this happens, the read is simply retried once, so a hypothetical 2ms read becomes 4ms. If it’s unlucky, the read may have to be retried more than once. There is an upper limit on how many retries can occur based on how clocks are synchronized. For NTP, this could be 250ms, so even the most unlucky transactions won’t have to retry for more than 250ms for clock-related reasons. Because CockroachDB relies on clock synchronization, nodes periodically compare clock offsets amongst themselves. If the configured maximum offset is exceeded by any node, it self-terminates. If you’re curious about what happens when maximum clock offsets are violated, we’ve thought about it a bit here . Concluding thoughts If you’ve made it this far, thanks for hanging in there. If you’re new to it, this is tricky stuff to grok. Even we occasionally need reminding about how it all fits together, and we built the damn thing. If unraveling the challenges of clock synchronization in the face (or absence) of atomic clocks is your cup of tea, then great news — we’re hiring! Check out our open positions here .", "date": "2021-04-25"},
{"website": "CockroachLabs", "title": "Fueling the Start-up Economy: Crowdfunding on a Transaction-Oriented System", "author": ["Cassie McAllister"], "link": "https://www.cockroachlabs.com/blog/birchal-funding-platform/", "abstract": "This is the story of how Australian-based Birchal created a crowdfunding platform on CockroachDB Early stage capital is one of the many financial services sectors experiencing digital disruption. Up and coming startup founders are increasingly turning to online equity crowdfunding platforms instead of pursuing traditional early-stage capital raising methods. In recent years, equity crowdfunding was established as a reliable and popular mechanism for startups and SMEs to raise capital. However, building an affordable, easy-to-use online platform for companies (and investors) to use is not an easy feat. There are many challenges associated with adhering to data regulations, reaching customers in different regions, and scaling to accommodate growth. Creating opportunity in the Land Down Under The Australian Crowd Source Funding (CSF) industry, which commenced in 2018, is nascent but accelerating with approximately 120 successful CSF offers completed, raising approximately $80m across the industry to date, the majority of which have been completed on Melbourne-based platform, Birchal . How did they get there? How do they already own 70% of this market? Simply put, the founders of Birchal made it their mission to comply with a complex web of regulations and develop a solution that would make the investment process simple and fast for new brands and investors. The companies that showcase their service or product on Birchal have the opportunity to engage with potential new stakeholders and raise money for their business. On the flip side, Birchal makes it easy for investors to support the brands they love. [IMAGE: Birchal’s user-friendly interface.] Without the easy-to-use platform that handles the financial transactions for them, Birchal’s customers may have never had the opportunity to raise substantial funds to get their business off the ground. Before building Australia’s leading crowdfunding platform, the team at Birchal had ambitious goals in mind with several challenges to overcome. Ultimately, they built a scalable and resilient platform on CockroachDB. Brief backstory on crowdfunding Birchal’s co-founders Matt Vitale and Alan Crabbe worked in vastly different areas prior to creating Birchal. Matt was a securities lawyer, with expertise in Australia’s financial services regulatory environment. Alan co-created Pozible.com , a platform that provides the tools and services for creators to plan, draft, and execute crowdfunding campaigns. From this experience, they knew that they would have to combat a lot of legal and regulatory challenges before they could start to build Birchal. Fortunately, in 2017 the Australian Government introduced crowd-sourced funding (CSF) as a new way for start-ups and small-to-medium-sized companies to raise money from the public to finance their business. Now it was time for Birchal to take off. The team at Birchal knew that they had to build a unique user interface and seamless experience that would set them apart from other crowdfunding platforms. They saw that a majority of their competition took a really dated approach to investing and charged companies a lot to participate. Instead they wanted to treat each individual company as its own project and showcase the opportunity to the right audience. They knew they could take advantage of predictive analytics and digital advertising to make their platform unique. They had big plans for Birchal and knew that they would need to build on an infrastructure that could support them in the future. [IMAGE: Sample company profiles on Birchal.] A unique platform requires a unique database Birchal CTO, Josh Stewart, has a lot of experience with Postgres and wanted to build Birchal on a reliable, SQL database. However, he also knows that Postgres (and traditional legacy databases in general) is difficult to scale horizontally and he wanted a solution that was architected to be cloud-native from the ground up. Josh and his team briefly evaluated Google Spanner since they are a Google Cloud Provider (GCP) customer. However, Spanner is expensive when it comes to managing large workloads and Birchal was a startup at the time and wanted  to keep overhead low. Since Birchal’s business model is centered around financial transactions, they wanted a transaction-oriented system that was built to scale for these types of workloads. They thought about pursuing NoSQL options to achieve the scale they needed, but they really wanted to stick with SQL to lower the learning curve for their developers and get the product off the ground quickly. Additionally, guaranteeing consistent transactions was extremely important to them and a lot of NoSQL solutions can only promise eventual consistency. “Not only did we want a database that could work for us when we launched, but we needed a solution that could work for us in the future. We wanted a database that was resilient to failures. A modern cloud database that could scale.” - Josh Stewart, CTO Ultimately, they needed to create resilient infrastructure so that they could promise their customers that their data is always available to them even during peak investment periods. As they moved to new jurisdictions in the future (in Australia and beyond) they wanted the ability to tie data to a location to achieve low latency access and to meet compliance requirements. Summary of requirements: Affordable solution for a startup Resilient to failures & high availability Cloud-native ACID guarantees Transaction-oriented system Ability to tie data to location Consistency at scale After some searching, Josh came across CockroachDB and saw that on the surface, it met all of his requirements. Plus, it was Postgres-wire compatible. Now it was time to test it for himself. Meeting & exceeding ambitious plans Back in 2017, Josh and his team got CockroachDB up and running in a day. They were still in the early stages of building their platform so initially they were using CockroachDB Core as a general purpose datastore. When Birchal became one of the first Australian Securities and Investments Commission (ASIC) approved licensed platforms in 2018, they moved into production and started doing more with transactional data storage. This included collecting payment information, campaign data, company info, investor interest, investment data, and more. “Coming from Postgres, I’m pleasantly surprised by CockroachDB’s performance. With Postgres, you spend so much time and resources into optimizing production workloads. With CockroachDB, the work is already done.” - Josh Stewart, CTO The platform started to gain momentum reaching up to ~200 queries a second or ~300 transactions a minute during the peak period of large campaigns. After 3 years of running a successful platform on CockroachDB Core, Birchal wanted to ensure that their business could infinitely scale in the future. An easy way for them to future-proof their business was to migrate to CockroachCloud. The sky’s the limit with CockroachCloud Not only did Birchal want to take advantage of CockroachCloud’s scaling capabilities, but they wanted a managed service that would take care of day-to-day operations for them. Since the CockroachCloud handles the management for them, they can focus on building their platform instead of investing time and money into maintaining their database. The team was pleasantly surprised with the availability of extra support and enterprise features. As mentioned before, Birchal uses GCP as their cloud provider. They are set up with 4vCPUs and 3 nodes with the ability to increase up to 5 nodes during periods of increased traffic from popular brand launches. A majority of their backend services are event driven and built on microservices. They have a bunch of clients that are connected to GraphQL and communicate through that layer. They also have Rest APIs behind GraphQL that communicate directly with CockroachCloud. [IMAGE: Overview of Birchal’s architecture.] Ultimately the team at Birchal envisions a future where their technology is serverless. They are working hard to create an infrastructure that can manage itself so the team can remain lean and efficient. They trust that CockroachCloud will be able to work well with a serverless architecture in the future because of its automated scaling features and its ability to function as a SQL API in the cloud. (CockroachDB is already in use at other companies as a scalable, serverless backend for applications). Putting the fun back in funding Using CockroachDB, Birchal is able to make informed decisions on future investing behaviors which gives them a competitive edge. For example, they monitor spikes in traffic to predict when a company may raise a large sum of funds. They also analyze past campaign performance to predict how much a certain company may raise. The team is looking into streaming data out of CockroachDB into other data sources so they can run more advanced analytics. Currently, Birchal owns 70% of the Australian crowdfunding market share. In the future, they will look to expand further into APAC given the key financial services industries located in areas such as Singapore and Southeast Asia. Birchal’s fresh approach to the world of equity investment is changing the lives of Australian business owners. Built on the foundation of CockroachDB, Birchal demonstrates how finserv companies can leverage best-in-class tech to build one-of-a-kind products. Interested in building a next-generation platform like Birchal? You should get in touch with our team. Or try out CockroachCloud yourself for free. If you are interested in learning more, check out what our other customers are building .", "date": "2021-04-15"},
{"website": "CockroachLabs", "title": "Building Better IAM with a Scale-Out, OLTP Database", "author": ["Florian Forster"], "link": "https://www.cockroachlabs.com/blog/scale-out-oltp-database/", "abstract": "The identity and access management platforms that I either used or researched in the past always left much to be desired. Most times their audit trails aren’t strong enough. Pricing per user or per session is too expensive. They don’t deploy easily in multiple regions. And they aren’t exactly easy to operate or scale across clouds. There was clearly a gap in the market for a modern IAM platform. I needed such a platform in order to properly do my previous job as a Head of eGovernment and IAM (in Switzerland). But such a platform didn’t exist! This is when we began thinking through and building the platform that has become ZITADEL today. Jump straight to the architecture. What is ZITADEL? ZITADEL is a modern cloud-native IAM platform used by developers to integrate their authentication and authorization needs without the need for running their own code. There are different use cases for ZITADEL and different consumption models, and we have several differentiators like stronger audit trails, and ease of use, but in this blog I want to focus on how we built a scalable, cloud-native IAM platform that can scale across regions and across clouds. Why do we rely on CockroachDB We started the journey of building ZITADEL back in 2019 and one the biggest questions at that time was: which database could suit all our needs!? As we hypothesized that we want to build an ES system (Event Sourced), mainly to get a great audit trail embedded into the architecture, we came to the conclusion to use CQRS (Command Query Responsibility Segregation) as well. So we needed to evaluate which database could handle those two design patterns, while still being able to fulfill our requirements as listed below. IAM Database Requirements: Strong consistency for the eventstore Great availability guarantees for our query databases Cloud-native design Able to run on Kubernetes out of the box Horizontal scalability was a must Ability to run across multiple datacenters and regions Easy to automate After an initial evaluation effort we settled with CockroachDB because it checked all our boxes. The most important checkbox being the fact that CockroachDB scales horizontally while still maintaining strong consistency. Moreover, storing our query side databases in the same cluster is a great plus - from an operational perspective - it is convenient not to have a service for each job. That would just make operations tricky and it enables our developers to focus on one battle-tested storage without the need to always reconsider where to store the data. How we run ZITADEL and CockroachDB At CAOS, the creator of ZITADEL, we are strongly committed to GitOps . We have written a blog in german and talked about that in the past . For us this means that each ZITADEL deployment gets its own Git Repository where all the required config and secrets (yes, they are encrypted) are stored. To aid with automating our lifecycle we created our project ORBOS . With ORBOS we are able to declare and run hyper-converged ZITADEL clusters from scratch in about 20 minutes. As you can see in the graphic below ORBOS consists of two operators. ORBITER which takes care of lifecycling infrastructure components including Kubernetes and BOOM who manages the tools. To deploy and lifecycle ZITADEL and CockroachDB we created a ZITADEL Operator which contains all the necessary operations logic to automatically operate ZITADEL. The operator carries out things like schema migrations for new ZITADEL versions, or backup and restore CockroachDB, manages the necessary client certificates to connect to CockroachDB, and so on. We are quite happy with this setup as it allows us to manage and run systems everywhere we like. And with GitOps we also have a great audit trail about our system changes. Currently we run ZITADEL in parallel on GCP and Cloudscale, both providers with data centers in Switzerland. Funny sidestory: ORBOS is used on its own as a Kubernetes platform by a lot of companies we work with. It even powers a government provider’s SaaS infrastructure. The Challenge of Connecting Multiple Clouds Still as of today one of the main challenges we face is the connection between multiple data centers. If you solely rely on one cloud provider this is not exactly a problem for you. Yet we decided early on that we wanted to run our cloud offering zitadel.ch across multiple providers to reduce any negative effect that one provider has – be it from an operational perspective like outages or from a risk standpoint. As we deal with sensitive data from our customers we wanted to reduce the risk of being too reliant on a single provider! Today we connect different providers by utilizing good old IPSec VPNs as a means of transport. And for the Kubernetes connectivity we connect each cluster over the VPNs with BGP Peering and IPIP. However this does not scale well and tends to have quite some management overhead. To connect the Cockroach databases located in each cluster we relied on this really useful blog from CockroachDB Gotchas & Solutions Running a Distributed System Across Kubernetes Clusters as we choose to use the DNS Chaining Method. But as we have full routing capabilities thanks to Project Calico we are able to send traffic over the overlay directly to each cluster’s DNS. To solve the problems of the VPN overhead we are currently testing Cloudflare’s Magic WAN offering as a means to connect our data centers to a virtual WAN with a redundant GRE / IPSec connection. This would mean that we don’t need to build and maintain a mesh-style architecture for the east-west connectivity. The combination of CockroachDB and Magic WAN looks like a really powerful architecture. If you don’t want the burden of managing that all on your own we can recommend CockroachCloud: A True Cloud Database ! The Future of Identity & Access Management We think that CockroachDB is a great match to support our plans to continue building the most innovative Identity and Access Management platform. The two main features we think will enable us in the future are changefeeds and geo-partitioning . Changefeeds could speed up the processing of backend processes which generate our query databases and also analytical things. We are currently thinking about running our events through machine learning to learn patterns to better mitigate attacks. With geo-partitioning we aim to give our customers the ability to decide in which region they want to store their data. For example, today our systems only run in Switzerland but as we expand into the EU we would like to give our customers the option to say in which region their data is stored. If you are interested in learning more about ZITADEL, head over to read Introduction to CAOS and ZITADEL Architecture . There’s also a detailed case study about our use of CockroachDB . For those interested in the code visit caos/zitadel: ZITADEL - Cloud Native Identity and Access Management or caos/orbos: ORBOS - GitOps everything", "date": "2021-05-13"}
]