[
{"website": "Affinity", "title": "optimizing joins in redshift", "author": ["Arzav Jain"], "link": "https://build.affinity.co/optimizing-joins-in-redshift-30039c166092", "abstract": "Design Engineering Affinity.co Recently at Affinity , we accelerated our analytics workloads on Redshift by optimizing our use of joins. In this blog post, we begin by sharing the general takeaways that you can apply as you optimize your Redshift queries. We then share our step-by-step journey in which we apply these takeaways to speeding up the many joins involved in flattening our Entity-attribute-value (EAV) data. If there’s a subquery you execute often, consider materializing it into a table. If you’re using dbt, as we do, you can do this by creating a new dbt model for it. Choose the column most joined on as the distkey for that table. If the table is joined on a different column depending on the query, consider partitioning the table into separate tables each with a different distkey. Alternatively, consider keeping a copy of the entire table on each node with the DISTSTYLE ALL . Re-order joins such that all joins on a given column (say company ID) happen first before switching to joins on a different column (say list entry ID) . This limits the data redistribution to only occur once when the join column changes. Note that this is possible only when a query contains multiple joins that aren’t dependent on each other. There are plenty of other performance optimizations for working with Redshift such as column compression encodings , sortkeys , and more. However, these are beyond the scope of this blog post. For further context on these areas, check out the docs. One of our most heavily used features in Affinity today is the ability for users to create Lists of entities that they want to track. A common use case is tracking a pipeline of organizations (also known as companies) that are being considered for investment. The core data model backing these Lists follows the Entity-attribute-value (EAV) model. Each column in the List above is represented as an Entity Attribute and each value in that column is represented as an Entity Value in our database (except for the Name column). Here’s what the relevant tables look like for the List above: The biggest advantage of the EAV data model is flexibility: every time a user wants to add a new custom column to their List, it doesn’t require us to introduce a schema change. We simply insert a new row into the Entity Attribute table and start storing the corresponding cell values in the Entity Value table. This is great when the user actions follow an OLTP model. However, it falls short when trying to support an analytics workload ( OLAP ) in which users might aggregate across a few columns; doing those joins on Redshift would not be performant. Consequently, we used the popular data transformation tool dbt to flatten the EAV data beforehand (instead of at query time) and generate a database table looking quite similar to the List in the product screenshot above: We followed the general approach outlined here and present below one such sample query that has been simplified for demonstration purposes. Here we flatten the list of companies shown in the screenshot. First, we select from lists_entries to get the IDs of companies that exist on the Fortune 500 list. Next, we join with companies to pull the respective company’s name and website. Then, we left join with 3 subqueries each of which represents a different column (aka Entity Attribute): Owners, Source of Introduction and CEO. In this example, the 3 columns all happen to contain persons as values. So in each of the subqueries we join entity_values with persons to denormalize and pull in the name of the person referred to by the entity_value. Next, let’s take a look at how we can speed up these joins. Before we dive deeper into the query plan, the first thing that sticks out as room for improving the query’s performance is avoiding the multiple joins between entity_values and persons that are repeated once for every column. We can instead perform this join once and re-use the results in each of the left join subqueries. With the help of dbt, let’s create an intermediate table called denormalized_entity_values to store the result of the join and reference this table in our subqueries. Our original query now looks like (omitting dbt refs for readability): Next, let’s take a look at the query plan in Redshift: You might notice that the joins have the biggest cost in the query plan and the cost increases significantly for each subsequent join. This makes sense since these are the steps in which lots of data is being transferred over the network between nodes. This can be orders of magnitude slower than operations within a node such as a sequential scan. As pointed out in the AWS documentation : When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. The goal in selecting a table distribution style is to minimize the impact of the redistribution step by locating the data where it needs to be before the query is run. Let’s walk through each of the joins in the query, beginning with the innermost join, to understand how we can minimize data redistribution. Starting with the inner most join, we see that the join is of type DS_DIST_ALL_NONE (see here for the definition of each redistribution style). This means that no data was redistributed between nodes in order to compute the join. This is because an entire copy of the lists_entries table exists on each node (aka DISTSTYLE ALL ) and the companies table is distributed on the id column (aka DISTSTYLE KEY DISTKEY id ). The result of this join is consequently also distributed on the company ID. To examine the distkey (and sortkey) for a given table, run this query (you will need to delete the admin. unless you have an admin schema) to create the v_generate_table_ddl view and query it like so: SELECT * FROM v_generate_table_ddl where tablename = 'companies'; The next join is a DS_DIST_OUTER join with denormalized_entity_values on its company_id column to get the Source of Introduction (as seen by the filter entity_attribute_id = 1 ). Since this table is not distributed on the company_id column, Redshift performs the redistribution on company_id at execution time and sends the rows of the outer table ( denormalized_entity_values ) to the respective node based on company_id . This is an expensive step due to the data redistribution; the result is now still distributed on the company ID. In order to get the Owner, the next join is performed on a different key, namely list_entry_id . Consequently, this is a DS_DIST_BOTH join where both the result from the previous step and the denormalized_entity_values table are redistributed to each node based on the list_entry_id column. This is a much more expensive step than DS_DIST_OUTER since both sides of the join are being redistributed. The result is now distributed on the new join column list_entry_id . Finally, we have to do another DS_DIST_BOTH join with denormalized_entity_values to get the CEO because the join is on the company_id column but the tables on both sides of the join are distributed on list_entry_id . The general takeaway here is that its usually best to distribute your Redshift tables on the join columns. However, you can only have one DISTKEY . Since the denormalized_entity_values table is joined on both the company_id and list_entry_id columns, we partition the table into two separate tables: one distributed on company_id and the other distributed on list_entry_id . This means that there are now two copies of the same table and this comes at a cost of keeping them in sync. However, in our case, rows where the list_entry_id is not null will only ever be joined on list_entry_id while the other rows will only ever be joined on company_id ; that is, the entity_value is either on the list entry or the company but not both. So separating denormalized_entity_values into two separate tables strictly partitions the table. The resulting query and query plan are shown below. We omit the two new dbt transforms used to produce the two denormalized tables for brevity. You will notice now that in the updated query plan above the Source of Introduction join involves no redistribution of data ( DS_DIST_NONE ) since denormalized_company_entity_values is distributed on the company_id column. However, the Owner join requires the inner result set to be redistributed on lists_entries.id before it can be joined with denormalized_list_entry_entity_values because the result from the previous step was distributed on company_id . This newly joined result is then redistributed again, this time on companies.id in order to perform the CEO join. These latter two joins both involve redistributing the inner result set (aka DS_DIST_INNER ). We can avoid redistributing data on both joins by re-ordering the CEO join to come before the Owner join; this way we can complete all the joins on the same distkey, namely company_id , first before we have to redistribute the data on a new join column, namely list_entry_id . This is only possible here because the left joins don’t depend on the left joins that come before or after it. They only left join data with the base result of the join between lists_entries and companies . In the query plan below, you can see that the Source of Introduction join and CEO join both involve no redistribution of data ( DS_DIST_NONE ). We now only have one data redistribution step DS_DIST_INNER for the Owner join on list_entry_id . This single data reshuffle step is unavoidable due to the change in join column partway through the query. In fact, this step will continue to be the only data redistribution step, even as we add more left joins for various other Entity Attributes in the Affinity List, as long as we order the left joins on company_id to happen first before the left joins on list_entry_id . By applying the takeaways at the beginning of this post, we have brought down the overall cost estimate of the query from 5.98M in the v1 query plan to 1.65M in the v3 query plan. This performance improvement is amplified even further as we add more left joins to our query to flatten other columns in the Affinity List (besides Owner, Source of Introduction and CEO). We hope you found these takeaways helpful. If you have more ideas for how to speed up joins in Redshift, please share them in the comments below! A big thank you to Mike Taluc @ Bytecode IO for sharing his Redshift expertise. Stories and lessons from our journey building Affinity 298 Thanks to Rohan Sahai , Adam Perelman , and Hansen Qian . Data Engineering Redshift Sql Data Analytics Performance 298 claps 298 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-01-26"},
{"website": "Affinity", "title": "how we used postgres extended statistics to achieve a 3000x speedup", "author": ["Jared Rulison"], "link": "https://build.affinity.co/how-we-used-postgres-extended-statistics-to-achieve-a-3000x-speedup-ea93d3dcdc61", "abstract": "Design Engineering Affinity.co Much like the DMV, the PostgreSQL query planner is a powerful, mysterious entity to whom we semi-blindly entrust our well-being. It has the crucial responsibility of picking the most efficient execution plan for every query. Here we’ll explore what data Postgres takes into account when creating query plans, and how we used that context to help the query planner come up with more efficient plans for some of our most important query patterns. Here’s an example slow query issued from our web server, along with the inefficient query plan that Postgres chose. Can you spot the key mistake the query planner made? By far the most expensive step is the second Nested Loop Join: Nested Loop Semi Join (cost=1.01..25.07 rows=1 width=4) (actual time=0.079..122074.806 rows=1958 loops=1) . Postgres estimated that this step would return about 1 row, which was a wild underestimate — it actually returned 1958 rows and took about 122 seconds. (See here for more background on how to interpret Postgres query plans.) Through informed use of Postgres statistics, we brought the time for this query down from 2 minutes to 42 milliseconds — almost a 3000x speedup! Before we dive into the stats adjustments that we made, let’s make sure we understand how the Postgres planner works. Statistics are data collected by Postgres used to inform its selection of query plans. Out of the box, Postgres samples the possible values for each column of each table to create histograms and a list of the most common values (among other things). These are used to estimate how many rows will result from applying some set of filters to a table. For larger tables, the planner can’t keep track of every single value a column holds. Instead, it samples the values of each column and uses those to make estimations. We can tweak how much sampling Postgres does for each column on each table with ALTER TABLE table ALTER column SET STATISTICS {-1 ..10000} where -1 sets it to the default value of 100 ( docs ). This number sets how many buckets are used in the histogram and how many of the most common values are stored. The downsides to increasing the statistics for a column are that more data must be stored in pg_statistic and running ANALYZE on the column's table takes longer. More details can be found in the Postgres docs . Extended statistics are user-defined objects that tell Postgres to collect certain kinds of data for sets of columns, rather than individual columns. Without extended statistics, Postgres estimates the impact of filters on a table by considering each filter independently. For example, consider a database containing 10 Artist records, each of which has 10 Album records referencing it, each of which has 10 Songs referencing that. This totals to 10 Artists, 100 Albums, and 1,000 Songs. Now, consider running the following query: SELECT * FROM songs WHERE (artists_id = 1 and album_id = 1); With perfect sampling, the query plan might look like (cost=0.28..6.05 rows=1 width=159) refers to the planner's estimations while (actual time=5.555..5.562 rows=10 loops=1) refers to the actual results of the executing the plan. The planner estimated 1 row would be returned, but there were actually 10. The planner calculated its row estimate by first taking the total number of Songs (1000), then considering the artists_id filter. 10% of Songs have artists_id = 1 so that leaves 100 Songs. Next it considers the album_id filter. 1% of Songs have album_id = 1 , so it's left with 1 Song. The key piece of information Postgres is missing is that artist_id and album_id are strongly correlated. In fact, knowing the album_id uniquely determines the artist_id . Had Postgres known about this, it could have used only the album_id = 1 filter in its estimation and come up with the correct result of 10 Songs. This kind of correlation can be indicated to Postgres using a dependency statistic. This statistic stores the frequency with which each column uniquely determines the other column. A dependency statistic on (artist_id, album_id) might yield the following: The 1 and 5 under stxkeys and stxddependencies refer to the 1st and 5th columns on the songs table, which are artist_id and album_id , respectively. The value for \"1 => 5\" is 0.1 since artist_id determines album_id 10% of the time. The value for \"5 => 1\" is 1.0 since album_id always determines artist_id . When Postgres is filtering by columns with a matching dependency statistic, it’s able to use that to make a more accurate estimation. There are, of course, other kinds of extended statistics but a dependency statistic makes the most sense for this kind of data distribution. One caveat of extended statistics is that Postgres only knows to use them when filtering on exactly the columns referenced in the statistic and when filtering using simple equality conditions, e.g. artist_id = 5 and not artist_id IN (5, 6) or artist_id < 10 . Use of extended statistics can lead to non-intuitive index choices. If a dependency statistic indicates to Postgres that a column filter is redundant, as in the case of artist_id and album_id , it may opt to use an index that only references one of the columns. In the case of songs , it may use an index on only (album_id) instead of an index on (artist_id, album_id) if both are present. There are three options Postgres has for joining tables: Nested Loop Join. Using this join strategy, Postgres loops through each row in the left relation and scans through the right relation for rows that satisfy the join condition, ideally using an index. This is an effective strategy for when there are very few rows in the left relation. Merge Join. From the docs : “each relation is sorted on the join attributes before the join starts. Then the two relations are scanned in parallel, and matching rows are combined to form join rows. This kind of join is more attractive because each relation has to be scanned only once. The required sorting might be achieved either by an explicit sort step, or by scanning the relation in the proper order using an index on the join key.” Hash Join. From the docs : “the right relation is first scanned and loaded into a hash table, using its join attributes as hash keys. Next the left relation is scanned and the appropriate values of every row found are used as hash keys to locate the matching rows in the table.” For our purposes, the main thing to note here is that the advantage of a Nested Loop Join is that there’s very little overhead compared to the other join strategies. However, this join can go wrong if there are many rows in the left relation. For example, suppose there are 1,000 rows in the left relation and Postgres is using an index to access the right relation. If each index access takes 4ms, the entire join will take 4s, which is too slow in the context of responding to a user request. Now that we understand the different type of joins, let’s revisit the Nested Loop Join that struck us as problematic. Without going into too much detail about our data model at Affinity, all you need to know is that on our tables entity_values and lists_entries , the column org_id is uniquely determined by list_id or entity_attribute_id , meaning that in order to estimate the selectivity of a set of filters on these columns, the filters should not be considered individually. Our slow queries were the result of Postgres underestimating the number of rows that would result from applying a filter condition and opting to use a nested loop join because of that underestimation. Let’s look back at our original problem query. By far, the most costly step was looping over the index access to entity_values_org_id_entity_attribute_id_company_id_index a whopping 13,769 times. To encourage the planner to use a different join strategy, we needed to improve its estimates for filters on lists_entries and entity_values . Based on the filters applied, we maxed out the per-column statistics for: among other tables and columns for different query patterns. We also added dependency statistics on: among other dependency statistics for other tables and columns, since both list_id and entity_attribute_id uniquely determine the org_id . After we made these adjustments, Postgres chose the following query plan for our original query: Here, the estimates are much more accurate and the planner opted for a hash join for the inner join — and the query took 42 milliseconds instead of the original 2 minutes. Increasing the per-column statistics and adding dependency statistics have helped tremendously, but there is still progress to be made. As you may have noticed in the improved query plan, the planner underestimates the number of rows resulting from the inner join. While the outer nested loop join didn’t take long this time, it’s not hard to imagine a query where the inner join results in many rows and the outer join becomes a bottleneck. We hope this post has given you some ideas about how to improve your query plans, or at the very least taught you something about the magic of Postgres! Stories and lessons from our journey building Affinity 508 Thanks to Adam Perelman and Hansen Qian . Postgres Sql Engineering Performance Database 508 claps 508 Written by Oh that’s. Yep. Yeah. We’re lost. Stories and lessons from our journey building Affinity Written by Oh that’s. Yep. Yeah. We’re lost. Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-12"},
{"website": "Affinity", "title": "affinity named one of fortunes best workplaces for 2020", "author": ["Shubham Goel"], "link": "https://build.affinity.co/affinity-named-one-of-fortunes-best-workplaces-for-2020-e4ee68689231", "abstract": "Design Engineering Affinity.co At Affinity , we know we’re nothing without our employees. That’s why cultivating a workplace environment that supports and strengthens professional relationships is as important to us as any feature of our platform. Even during a tumultuous 2020, as the remote office became the norm, we wanted to foster a culture where employees feel empowered and inspired to do their best work. And we’re excited to see these efforts are gaining public acknowledgment. We’re excited to announce that Affinity has been named one of Fortune’s Best Small & Medium Workplaces in 2020. These rankings are determined using Great Place to Work’s Trust Index , a rigorous, research-backed employee assessment conducted over a two-week period. We are honored to receive this recognition from the community and our colleagues, the people who truly make Affinity a great place to work. At Affinity, we believe that by cultivating each other’s strengths while celebrating our achievements, we position ourselves for success. Mutual support, appreciation, and empowerment are foundational to our culture. To that end, we host team building events and proactively address issues of diversity and inclusion. We give back to our community and prioritize environmental sustainability. And when it comes to our people, we support collaboration and learning while rewarding the hard work put in by every single team member. Affinity also ranked in at #3 for Fortune and Great Place to Work’s 100 Best Small and Medium Workplaces in Technology and #24 in Fortune’s Best Workplaces in Technology , thanks to the overwhelmingly positive employee feedback. While this year has been challenging, it’s gratifying to hear that our work culture makes a positive impact. We are proud to be acknowledged by Fortune and Great Places to work alongside technology companies like Asana and NerdWallet. This recognition validates the foundational work we’ve invested in our company culture. Over the past year, we have continued to build on our existing programs, including our leadership development and community engagement initiatives. Our employees see the impact, and together, our team is ready to take on new and exciting challenges. We want to thank Fortune and Great Places to Work for highlighting our work, and our employees for making our efforts worthwhile. We hope you’re as thrilled to work for us as we are to work for you. PS: If you’re looking to join a fast-growing startup that is reinventing how professionals leverage their networks, we are hiring across 15+ roles at Affinity. Check out the open roles on our careers page or reach out to me at shubham (at) affinity (dot) co. Stories and lessons from our journey building Affinity 51 Workplace Culture Startup Awards Innovation 51 claps 51 Written by Co-Founder @ Affinity (https://affinity.co/) Stories and lessons from our journey building Affinity Written by Co-Founder @ Affinity (https://affinity.co/) Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-27"},
{"website": "Affinity", "title": "affinity design library", "author": ["Filip Skrzesinski"], "link": "https://build.affinity.co/affinity-design-library-d321969716fa", "abstract": "Design Engineering Affinity.co Design as a craft is largely a product of labor — design-thinking, hands-on pixel pushing, creating documentation — all in the process of transforming an idea into reality. But what we read can say a lot about how we think, where those ideas come from, and ultimately, how we design. When “____-from-home” is attached to more things than we’d like these days, one thing that’s certain is that we have more time… at home, and reading remains a good way to fill that time. We asked ourselves to take a look at our own bookshelves and chronicle not only design reference books, but also the ones that influenced our craft and creative endeavors in less direct ways: from cooking, to history, to poetry. Part reading list, part functioning library, part portrait of our team’s collective knowledge, the design library aims to serve as both a practical resource and source of inspiration in our own corners of the world. Check out some of our team’s book reviews below or visit the Airtable link to browse the entire library (or use it as a template for your own bookshelf!). airtable.com Note: In an effort to make it easy to find out more about each book & purchase, we’ve included links to do so. While many link to Amazon as it’s often the most affordable & convenient method, we encourage people to purchase from local, independent book sellers, especially during these times of hardship for small businesses. Stories and lessons from our journey building Affinity 110 Design Book Recommendations Art 110 claps 110 Written by fffffff.co Stories and lessons from our journey building Affinity Written by fffffff.co Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-15"},
{"website": "Affinity", "title": "sharing code between web react native why how to configure metro for code sharing", "author": ["Shrey Gupta"], "link": "https://build.affinity.co/sharing-code-between-web-react-native-why-how-to-configure-metro-for-code-sharing-d6ec73427e08", "abstract": "Design Engineering Affinity.co Over the past year, we’ve added an abundance of new features to our mobile app at Affinity . Over time, we realized that this resulted in a lot of copied front-end logic as well as TypeScript type files between our web and mobile codebases. As we worked to re-organize our codebases to share files between the them, we ran into a number of issues configuring Metro , which is the JavaScript bundler for React Native. If you’re looking for tips on how to configure Metro to allow seamless code sharing, we hope this post helps! As with many modern companies, at Affinity we support multiple platforms. We have a web app, a mobile app, and a Chrome extension. Since all of our products are built in JavaScript (and more recently in TypeScript ), and primarily in React and React Native, a common theme has been: how do we share code effectively among these different codebases? For our Chrome extension, the answer was pretty straightforward because both the web app and Chrome extension use the same bundler: webpack . Also, since Chrome extensions are written to operate on the standard HTML DOM, we were able to re-use a lot of our React components from our web app. We made sure to configure code sharing from the very beginning of our work on the Chrome extension. On mobile, on the other hand, we use React Native, which uses native UI components rather than HTML elements for rendering and only allows styling to be in-line. Because of these fundamental differences from web, we didn’t bother to setup seamless codesharing for our mobile repo. As our product evolved and we started using newer technologies like TypeScript , we started feeling the pain of not being able to share code across web and mobile. We ended up copying over type files (for our backend API responses) and complex frontend logic code into mobile. Over time, these copied files became stale. For example, if an engineer made a change to our API, they’d sometimes update the corresponding type declarations for web but forget to update them in our mobile repo, leading to bugs in our mobile app. For our Chrome extension, we use webpack and npm to configure our web repo as a dependency so we can reference its files. We added web as a node module and then configured webpack to allow imports from that node module. React Native uses a different bundler called Metro, but there’s a webpack-based alternative called Haul that’s built for React Native projects. Although this solution might have helped us to solve our problems around code duplication and bugs, it risked introducing its own problems. For one, Haul is not part of the out-of-the-box React Native tooling, so when updates to React Native are released, there’s a risk of incompatibility. In addition, even if we used this approach to reference our web repo as a node module, importing it at a specific commit hash, this approach could result in stale code if we didn’t update the commit hash often enough. Whenever we did get around to updating the commit hash, especially after a breaking change in our web repo, we might still run into bugs. There’s lots of online guides to walk you through how to share code between React Native and React by setting up your project as a monorepo , which is simply a repository that contains more than one logical project (e.g. web and mobile). These logical projects are most commonly nested under one common directory (often called packages ) because it makes dependency management easier. One drawback of this structure is that because all the codebases are essentially combined, things like integration tests and builds can be a bit more challenging to setup. Other workflows, like sharing code, are easier, because every package belongs to the same repository and follows the same structure. Coordinating a large-scale refactor can also become easier. An API change that affects multiple parts of the codebase can be done in a single pull request. There’s great tools available to help manage a monorepo setup, like Lerna , git submodules , yarn workspaces , and Bit . Here’s a great article to help you setup your project structure using one of these tools. For our own purposes, we wanted to avoid the large time investment that would have been required to implement a large structural change to our main web repo, so we decided to divert slightly from the classic monorepo structure where all the projects are nested under a common directory. Instead, we decided to have a parent-child project relationship, with our web app as the parent and our other projects (mobile and our Chrome extension) as children. If you’re starting fresh, you might want to go with a more classic monorepo structure — we recommend referencing the article above to decide which tools you should use for managing it! Now, we had to figure out a way to make imports work with our parent-child directory structure. This forced us to do a deep dive into Metro , which is the JavaScript bundler that React Native uses by default. Here’s the overall strategy we used: 1. Move our mobile repo to be a sub-directory in the main web repo using steps outlined here . We made sure that both projects continued to build and deploy successfully, and removed all the obsolete git files ( .gitattribues , .gitignore , etc). 2. Reference a file from web in the mobile project. This required making configuration changes to rn-cli.config.js in the mobile folder — see the next section for details. 3. In our mobile codebase, start referencing web files for shared code (e.g. type files) and deleting the corresponding, duplicate files from our mobile repo. The first step from our game plan was very straightforward and took only a day or two to implement and test. Then, we came to the hard part: referencing a file from web in our mobile repo . Looking at the Metro config , we found that there were relevant parameters listed under Resolver Options . In order to import files from outside the React Native project, we would have to enable the resolver in Metro to look at and resolve the files in the web directory containing JavaScript files. Here is a list of some of those options and what they do: projectRoot : The root folder of your project. watchFolders : Specify any additional (to projectRoot) watch folders, this is used to know which files to watch. extraNodeModules : Which other node_modules to include besides the ones relative to the project directory. This is keyed by dependency name. Our mobile javascript assets were contained in mobile/src/ and our web javascript assets were contained in assets/javascripts . Our goal was to reference something from assets/javascripts/util or assets/javascripts/types in mobile/src . Initially, we didn’t set projectRoot , which meant that the root was mobile/ . We tried multiple configurations where we added the parent folders in watchFolders , but we still weren’t able to reference a file. After multiple different configs, we realized that in order for Metro to resolve files outside of where the config file is placed, the projectRoot has to be set to the folder where we want to reference the assets from. In this case, that meant setting it to assets/javascripts . Because we changed projectRoot , we also had to move our index.ios.js and index.android.js files to this directory in order for the app to build and run. It looked like Metro was now able to resolve these files but there were issues when we tried to import a file that had a dependency. We knew that this probably stemmed from the fact that web and mobile used different package.json . After some digging, we realized that we had to modify the watchFolders config to reference both project node_module . After these changes, we were able to start referencing web files successfully. Now we could reference web files, but we had broken our mobile imports along the way. Because we changed the projectRoot , we broke the way our imports worked in our mobile repo. We tried to make multiple changes to the config, but none panned out. As a last resort, we converted all of our mobile files to use relative imports. Since our mobile codebase wasn’t that big, this was pretty straightforward to do using some regexes to find and replace. Of course, we made sure to build the app and test thoroughly after these sweeping changes! After all these changes, we ended up with a rn-cli.config.js that looked like this: Of course, you might learn that you need to make a few small additional changes of your own based on your particular repository setup, TypeScript configuration, and so on. Since our app is quite complex and we reference these type and util files from hundreds of files, we decided to start gradually migrating these files to reference the versions from our web repository. We’re excited to be reducing our tech debt on this front, and we hope this post helps you do the same! Stories and lessons from our journey building Affinity 244 1 Thanks to Adam Perelman , Hansen Qian , and Arlan Jaska . React Native Mobile Metro Typescript Engineering 244 claps 244 1 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-22"},
{"website": "Affinity", "title": "an interview with joshua goldenberg head of design at slack", "author": ["Kelly Akashi"], "link": "https://build.affinity.co/an-interview-with-joshua-goldenberg-head-of-design-at-slack-fa958624d061", "abstract": "Design Engineering Affinity.co Recently, we had the opportunity to interview Joshua Goldenberg — Head of Design at Slack and design advisor to Affinity. Prior to Slack, Joshua served as Head of Design at Palantir from 2011 through 2016, where he built the design arm of the company. Before Palantir, he designed Oscar-nominated software at Lucasfilm. Here’s what he had to share about his inspiring journey across all corners of the design world, from print to product. My interest in visual things started at a very young age. Even as young as a fifth or sixth grader, I was taking as many art classes as I could, and paying more attention in those classes. By the time I was a high school senior, I had managed to engineer my schedule as a senior to be almost all art classes. In retrospect, I don’t know how I graduated, but I got exactly the right amount of credits. I was doing a metal sculpting class, a ceramics class, AP Art, Photography, and Commercial Art. It was like, Gym and five or six art classes. I deeply cared about artmaking — they weren’t blow-off classes for me, and I was super engaged. After high school, I decided to go to San Francisco Art Institute as a painting major — which, at that time, was what I thought I was going to do with my life! I got there and I loved it, but it didn’t take me very long to realize that painting wasn’t a great opening career move. It just so happened that I had a friend who had started a record label in Los Angeles, and that I was living with someone who had a really killer Mac with a lot of software on it. So I started using Photoshop to make digital art and started to experiment with type. I had no understanding of what being a designer was. My mindset was that I was making art with a computer. This guy who started the label in Los Angeles came over one day and saw what I had created. He told me that he wanted to use it for his label. He paid me for it and I was like, “Wait a second… I could do this for a living.” That was how it started. Shortly after that happened, I dropped out of art school after nine months as an 18-year-old and tried to “make it” as a one-man design studio for the next five years, which was pretty rough. Not knowing what I was doing at all and without any formal training, I had to figure out that you’re not supposed to send a printer Photoshop files with the text embedded as pixels. I had to learn about basic things like vector-based art, typography and how to do pre-press. I wound up working with a weird mix of random local businesses, and then a bunch of music industry stuff that was spread out all over the country. This was all around 1994 to 1998. During that time, the web (or at least the commercial web) was coming into being, and I was learning HTML and CSS. At some point, I was sitting exactly between print and web design. I realized that I was getting bored and I wanted to work on more complex things. Working on more complex things meant that I should be working with teams, instead of being by myself all the time. So I went to work for my first design agency in 1999, which was VSA Partners in Chicago (a great agency). I thought I would be there for a long time (like many of the other people who worked there) because it was such a great place. But it happened that after 7 months, I had befriended a brilliant engineer (the technical director) and he was leaving with a few other people to start another agency that would be a little more technology-focused, and potentially more product-focused. I didn’t fully understand what that meant back then — it was really a time before you could build products on the web. But I had learned what it was like to work with a great engineer, and the idea lit me up. I was like, “I’m going with this guy because I actually want to be close to a great engineer.” I was more interested in being a designer paired with an amazing engineer. So the five of us started an agency — right as the dot-com crash happened. That was hard. There was a sudden, massive drop in people spending money on agency work, and a lot of the potential web stuff we were going to work on dried up. It was really hard, but the experience of working with that amazing engineer was great and taught me what it was like to have awesome partners in engineering. It also gave me a taste of working on apps, even though it was still too early for modern JavaScript and web applications, and was way before smartphones and app stores and modern software distribution. In late 2002, I came back to California and became a freelancer again. I contracted with two agencies and became both a designer and a creative director — working for agencies in Chicago and in California. I did that for a few more years until I began to realize that I didn’t want to work on marketing-focused work anymore. That was my exit from the agency world. Around 2006 or 2007, product design as a role in software had become more of an idea in the design zeitgeist, and I landed some work with a product studio called 80/20. I worked on a redesign of Second Life with the Linden Labs team, a team of engineers and product folks, and the problems were complex and interesting. That was what made me fall in love with working on tools and software. When that ended, I knew I wanted to move fully into the space as a product designer, and focus on challenging interaction problems, with deep engineering teams. And that realization led me to understand that the only way to really engage with problems like that was to join an in-house team and be able to iterate on a problem from the inside Around that time I had a friend who was working at Industrial Light & Magic (ILM), and introduced me to the folks in Research and Development there. I wound up working on a stealth team inside ILM that was ultimately directed by George Lucas. He had carved out a budget to work on a pre-visualization system — he basically wanted to see if we could get rid of storyboarding by hand, and do it entirely digitally. We used a relatively low-fidelity real-time game-like engine, and built a bunch of tools to support editors and directors and previs artists to be able to quickly sketch out shots and sequences in filmmaking. Working on this was a lot of fun, and there were a lot of really interesting design challenges. I worked on that tool at ILM for a year and a half — and once it became stable, there wasn’t much design to do for it, so I started to work on the broader visual effects pipeline of ILM. After doing that for another year and a half, I realized that I wasn’t a great fit for visual effects: I wanted to work on a team that was focused on making the tool the absolute best it could be, as opposed to making the production the absolute best it could. The software and the design weren’t secondary to me. So I started looking. I had an old friend who was an early hire at Palantir, and he asked me to come talk to them. Right around the time that I was thinking about leaving ILM, I spent a day with folks at Palantir and totally fell in love with the problems they were working on. I was one of three designers in a company of almost 400 people, which was a wild period of time. They were growing so fast. I was doing design work, but also figuring out how to build a design team, how to recruit, how to onboard, how to design an intern program and college hiring program, how to allocate and rebalance teams, develop design process, and a ton of other things focused on design leadership. I ended up building a team that was (at its peak) 40 people over the course of four and a half years. There was a lot of change and transition, but the team was a great team full of wonderful people, many of whom I still talk to. After those four and a half years, I felt ready to do something else. So I started putting feelers out. I probably met between 18 and 20 different companies and founders. I really wanted to get a sense of how current startup founders were thinking about design in their organizations, how they were structuring design teams, who those teams reported to, and what their hiring plans were. I went through everything from really small startups to 400-person companies. I wasn’t really looking at anything bigger than that. And randomly, a couple of friends in enterprise venture capital introduced me to Stewart Butterfield and Brandon Velestuk at Slack. I had set two principles for myself when I left Palantir. One was that I wanted to work on something that I had more of a first-person relationship with: meaning something that I would use every day. The other was that I wanted to work at a place, where design already had a seat at the table. After spending time meeting the team at Slack, and realizing it was a product and a company that I’d love to work with, I joined in April 2016. They were really supportive of everything I wanted to do creatively. On the visual side, it turns out that both of my grandmothers were artists. There are still pieces of theirs hanging at my parents’ houses. One of my grandmothers did many different kinds of painting, and my other grandmother did wonderful charcoal drawings. There was a time when (during my last two years of high school, I think) I took over the garage as an art studio and worked on some big fun pieces that, in retrospect, I would never hang up. But they were totally supportive. They were initially weirded out by the idea of me going to art school because they were thinking, “How is this going to apply career-wise? But whatever, it’ll probably work out.” I had a lot of encouragement from them. There is a part of me that wishes that I had known what design was, instead of just what art was. Design was harder to access when I was much younger. The awareness and design literacy wasn’t as prevalent as it is today. It’s very different now. You can connect with design as a high school student much more easily than one could when I was a kid. I wish that I had that — because I wonder how much more I could have been shaped by design if I had encountered it as something completely distinct from art at a much younger age. I was a print designer first and, for a long time, there was a group called The Designers Republic . They had a huge influence on my work. They did so much of the foundational design and shaping of what electronic music looks like today. Anselm Kiefer probably had the biggest influence on me as a painter. Also in design, I grew up in the age of Raygun and David Carson, and I was attracted to more organic and dirty things, as opposed to the Swiss work I would probably gravitate towards more now. I was attracted to design that felt like the art I liked, partly because when I was younger I really only understood design through the lens of painting and visual art. One great manager I had was a woman named Terri Tomcisin at VSA. She was a really empathetic, kind manager who gave great feedback, and also gave me my first real job — at that point, I had worked by myself for a number of years as a freelancer. Michael Lopp — the VP of Engineering at Slack — has been writing books and blog posts on management for years. We worked together at Palantir and I had read a bunch of his work before I met him. His writing is more engineering-focused, but everything applies to design — in terms of working with people, interpersonal situations, giving guidance, and being a good listener. That had a pretty big influence on me. He’s a good friend who I work with every day, and his work had an influence on me before I even met him in person. And, I’m really fortunate to get to work with a ton of great leaders at Slack, like Stewart and April Underwood. Definitely. The points that stand out: when I realized I wanted something bigger than design for marketing. At that time, there wasn’t quite a name for product design yet — before apps and app stores. Software was still distributed on CDs at Best Buy. The space just wasn’t nearly as big. But I had the intuition that software was going to explode, at least on the web. It was still really early, so figuring out how to work on software in a non-marketing sense — and instead, in a purely tools sense — was hard. That felt extremely risky. I had no qualifications to be really doing product design at that time, but I thought it was something I could be good at. I had a sense for interaction design that had come from thinking about user experiences throughout my marketing projects. I took a risk by switching to something I had to build knowledge and credibility in, and also found people willing to take a chance on me, only having worked in adjacent design areas. I’ve made it a point since to try to enable the same chances for other designers. Yeah, and I’m not sure that it’s necessarily good either. Let me separate those things out because there’s a subtle distinction. Having a graphic design background is, for sure, a very good input for being a product designer. I would encourage anyone to do that. To anyone in school who can dive into a mix of computer science and product design while getting a formal, foundational education in typography and graphic design layout (those are the things that inevitably carry any design discipline forward really well) — I would totally encourage it. But coming from a fine arts background is a little bit different because it actually teaches you anti-patterns for design which you have to undo. Understanding composition and color and spatial relationships and negative space are all valuable. You develop your eye. But while art and design look like they’re really close together, they’re actually really far apart when you zoom all the way in. You have a need as an artist to put some part of yourself into everything. You have to learn how to undo that as a designer because that’s just straight up not what design is. When you become a designer, you figure out that you’re actually here to solve someone else’s problems. Often, the best thing you can do is to take yourself out of it. The thing you’re making has to be used by other people. It’s so different, at that point, from being an artist. You have to undo imposing your own will, aesthetics, and creativity into everything. There’s a bridge. The bridge between these two sides is developing a sense of taste. And obviously, that’s important and very hard to teach. That’s hard to say. I regret working alone for so long. My resume is kind of weird-looking. I was mostly an individual contributor for like 15 years. Even when I was doing creative-director-type of work, I wasn’t a manager in the sense that people didn’t report to me, and I wasn’t helping them with career development. I worked as an individual contributor for a really long time, and that ultimately had value because I stayed so close to the work for so long. But only after long periods of time as a freelancer did I learn I wanted to be on a team, focusing deeply on solving problems. The actual worst decision I ever made — and I couldn’t have possibly known this — was probably starting an agency six months before the dot-com crash. That was horrible timing. The best decision I ever made was almost certainly switching to product. I think it really saved my interest in and passion for design. I was so burned out on marketing — and I don’t have anything against marketing in general — but doing random agency work, I couldn’t get the thought out of my head that I was just helping people sell things that I didn’t care about. Even then, I think I would have felt differently if I had moved to an in-house team sooner, but, I had the strong sense that there was more I could be doing with design — and I found it. Making that switch was the best career move I’ve ever made. That’s a good question. I’m pretty patient. I’ve been able to think about things on a slightly longer timeframe. Developing that sense of patience has helped. I’m ruthless about taking breaks. If I’m frustrated with a problem, I’ll go for a walk. If I’m frustrated with something longer-term, I’ll take a vacation. I’ve identified the things that help me unplug and reset. I’m good at knowing when I need to do that, and going out and doing it. First, be a graphic designer too. I have definitely said this before, but foundations in information architecture, information design, layout, typography, prioritization of how things read… these are supremely important to being a great product designer. Oliver Reichenstein said that 99% of web design is typography. That is largely true of software design too, whether it’s deployed on a web browser or not. The second piece of advice is much harder for someone to just go out and practice, but you have to remember that design exists at the very top of the stack. That means it’s the thing that every discipline in a company can see and touch — and therefore have opinions on. No one is ever going to walk up to an infrastructure engineer look over their shoulder and say, “Hey, you should probably change that line of code.” If that happened, it’s going to be an engineering manager who is intimately familiar with the particular problem on the screen. But designers can experience that 8 hours a day, because our work exists at the top of the stack and it’s there for everyone to see and comment on, regardless of their design literacy. And to be clear, that’s ok. But, that means we have to be way more resilient than a lot of other disciplines to feedback, and to be adaptive in a way that is extremely demanding. It can crush people who aren’t used to it, and it can burn people out. You have to figure out how to manage it and consistently be really open to feedback all the time. You also have to synthesize it in a way that not just includes people, but also gives yourself a chance to put all the pieces together in the way that you think is best — because you are the actual professional who does this for a living. You then have to reflect that back out in a way that’s persuasive. It’s a whole set of really hard things that are somewhat unique to product designers, on product teams. That’s a lot to become good at. Great designers take feedback from everywhere and pull it all in and wind up with something that’s better for the sake of the feedback that it received — whether that’s from users, research, peers, or stakeholders. But it is not design-by-committee. This is a hard distinction for many people. When I think about that, I think about my kids. I hope that my kids are wonderful people who are satisfied in their lives and contribute good to the world. As a professional, I hope I’ve left behind things that people love to use. As an organizational leader, I hope I’ve helped advance and develop other leaders and managers who’ll go on to lead organizations that feel well cared for, are healthy, and produce great work. If you’re interested in learning more about what we’re up to at Affinity, come check our website out at affinity.co . Stories and lessons from our journey building Affinity 239 Design Design Advice Design Process Design Career 239 claps 239 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-26"},
{"website": "Affinity", "title": "engineering onboarding at affinity", "author": ["Hansen Qian"], "link": "https://build.affinity.co/engineering-onboarding-at-affinity-67d7741f25b6", "abstract": "Design Engineering Affinity.co This summer, our team added four new engineers, growing our existing engineering team by 33% to a total of 16! As their start dates approached this summer, we knew we needed to clarify our onboarding process to ensure each engineer had an effective start to their career at Affinity. There are a million blog posts on the internet describing how to onboard engineers effectively (and now with this blog post, a million and one). After reading through a few, we realized that different companies took quite different approaches to onboarding (some companies had a 6-week bootcamp, some had very little onboarding structure), especially startups at different stages of growth. We needed to clarify our own priorities and approach, especially given our size and stage. We started out by defining our goals: to ensure (1) that new hires immediately feel like members of our team and (2) that they learn our codebase and become effective engineers at Affinity as quickly as possible. Our revamped onboarding process consists of three key components: a venue for any and all questions, a welcoming existing team, and a quick productivity ramp as soon as new engineers join — below, I’ll expand on each of these components. If you’re looking to design a similar onboarding program at your startup, we hope this is helpful in conceptualizing and operationalizing the first weeks of a new engineer’s experience. Two years ago, Affinity’s onboarding had only one formal component: upon joining, another engineer would be assigned as the official “buddy”. It was the buddy’s job to onboard the new engineer, help set up their dev environment, and answer questions as they came up . With fewer than ten engineers at the time, we couldn’t devote too many resources towards standardizing the buddy’s responsibilities. This iteration was fairly unstructured as a result: each buddy developed their own approach towards how to onboard someone effectively. However, we still enforced one key condition: each buddy had the responsibility of being available to answer questions. Since then, feedback from new engineers consistently emphasized that the most valuable part of this informal onboarding process was the freedom to ask questions to an experienced engineer. As a result, our formalized buddy system now requires being available to answer questions — no matter how busy you are, how frequent the asks, or how simple the question — as an explicit core responsibility. New hires should expect that any time they have a question, there will be someone available to answer them (if the buddy is OOO, it’s their job to find a substitute). Even though the buddy now has a checklist of additional buddy responsibilities (choosing onboarding tasks, conducting initial 1:1s, help set up dev environment, etc), being available for questions still remains their top priority. This demonstrates that we’re invested in the new hire’s success, minimizes time wasted on understanding complex abstractions, and accelerates the onboarding process. We believe that the most effective engineering teams work as a cohesive unit, not as individuals. As such, we continually aim to reinforce collaboration and communication across our team. Just as important as having a buddy who will guide them through onboarding is having a welcoming integration with the rest of the team . The buddy is responsible for scheduling lunches throughout the first few weeks between the new hire and other engineers, designers, product managers, etc: not only to get to know their domains of expertise, but also to get to know them as people. Different engineers have different areas of responsibility, and onboarding tasks are chosen by the buddy to expose the new hire to as many parts of the codebase as possible. This implicitly encourages collaboration across the team and promotes interactions with as many engineers as possible. Even though a buddy is officially assigned as the main go-to for questions, all other engineers are expected to play a similar role! Unblocking others, being helpful, and explaining concepts are core expectations of an Affinity engineer — even when doing so might interrupt their own work. There should be no need to feel bad asking any other engineer a question, as we’re all invested in the new hire’s success, regardless of who the official buddy is. Many other onboarding frameworks (for example, Facebook’s Bootcamp ) spend the first few days or weeks introducing the new hire to the codebase, describing core abstractions, and teaching classes on engineering concepts to help the new hire get up to speed. Feedback from engineers who’ve joined in recent months, however, taught us that having a fixed onboarding was actually unhelpful, as introducing concepts without product-facing tasks to reinforce them didn’t lead to much retention of new concepts. As a result, our onboarding is designed to introduce the new hire to our codebase as fast as possible . On day one, they’re assigned a set of tasks with immediate product impact, with the goal of completing a few of them within the first week. They’ll learn how to write Ruby and TypeScript, modify API endpoints, and update React components by completing actual tasks, rather than sitting through a tutorial. Onboarding meetings are evenly spread out over the first few weeks instead of the first few days, giving new engineers more time to absorb our culture and spend more of their first week jumping into real code. Of course, the buddy will also be available to pair program, help explore the codebase, and give tips if they get stuck. That doesn’t mean that all of our technical onboarding is dependent on jumping into code and asking questions. For certain more complex abstractions, having documentation to follow is of course helpful. We took inspiration from Quora , and wrote our own set of Codelabs (for example, on git )! The goal, as always, is to help new hires become effective engineers at Affinity as quickly as possible, and all of our onboarding strategies and tools are in service of that goal. To operationalize and formalize these three key components of our onboarding program, we’ve created two Asana templates: one for the new hire and one for the buddy (screenshotted above). We’ve noticed that new hires tend to speed through checklists to complete every single task as soon as they can, so we’ve also sectioned the checklists into explicit “Day One”, “Week One”, and “Week Two and Beyond” sections. These two templates help us ensure that all onboarding steps are addressed and given proper consideration. Since joining, all four of our new hires from this summer have since graduated from our onboarding program and are now full members of our engineering team! One of them even mentioned that Affinity had “the best engineering onboarding experience I’ve ever been through.” That’s great to hear, but we know that our onboarding, like anything else, will require continual improvement to become even better and and keep pace with our team’s growth. We’ve already taken our new summer hires’ feedback and re-incorporated it into our onboarding process for the next new hire. Whether you’re an engineering candidate interviewing with us, someone designing the onboarding process at your own startup, or just a curious reader, we hope this post has been useful! Hit us up at eng@affinity.co with any questions, comments, or suggestions on how to improve the onboarding process. And if you’re looking for new opportunities, we’re hiring ! Stories and lessons from our journey building Affinity 312 Thanks to Tom Li and Gabriel Fan . Startup Engineering Engineering Mangement Employee Onboarding Engineering Culture 312 claps 312 Written by untitled Stories and lessons from our journey building Affinity Written by untitled Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-11"},
{"website": "Affinity", "title": "joe lonsdale on investing your time at your first job", "author": ["Hansen Qian"], "link": "https://build.affinity.co/joe-lonsdale-on-investing-your-time-at-your-first-job-9f41e143472e", "abstract": "Design Engineering Affinity.co Everyone knows that time is arguably their most valuable resource, but very few people take a structured approach towards using it as effectively as possible. Nowhere is this more important than in a new grad’s first job out of college. Joe Lonsdale, co-founder of 8VC, Palantir, and Addepar, as well as founding chairman of Affinity , recommends a diligent approach: “If you’re in a position where you’re trying to figure out what to do, you have to think about your time like an investor .” Recently, Joe spoke at Affinity’s first-ever engineering open house, and shared a few tips with the audience on how college grads should think about making the most of their first job. These tips are especially targeted towards those hungry to learn and grow as much, and as quickly, as possible. When Joe was looking to break into tech, he made sure to work with “other really smart people who inspired me and who I was going to learn from.” Expertise and intelligence diffuse across an entire team as teammates motivate and learn from each other. According to Joe, “The best way to invest your time is in something with great people and high growth, where you’re going to benefit from that strong community.” In a virtuous cycle, a strong starting team is also in a better position to continue attracting great people in the future. As Joe recounted, a lot of the early Palantir employees “could have started their own company, but decided that being a part of a superstar team meant that they could learn and grow even faster.” This positive feedback loop accelerates learning and growth for a strong team as well as for its individual members. For Joe, one of the most effective ways to open up new opportunities is to be a part of wins. Find a team that’s making progress and finding early success in their goals. “If you’re a part of wins, there tend to be a lot more wins that come along with them,” he shared. “By working for a great company, you perform, you impress people,” and you gain more exposure. Wins have the effect of amplifying accomplishments, elevating the profile of everybody involved in them and opening up new opportunities. Back when Joe was interning at Paypal, he noticed something in common with all of the founders: they were unconventional thinkers. The early culture was a reflection of the founders and the early team, whose effects on Silicon Valley are widely acknowledged as monumental. According to Joe, “you had a culture that was very open to new ideas. It’s true that you’ll get some bad ideas, but you can also get some really awesome ideas about how things should work.” Having an openness to new ideas is extremely important for startups and individuals because by their very nature, startups seek to redefine how something should be done. Joe recounted a story from the early days of Palantir. Some people treated Palantir as “the biggest joke of them all”, asking questions like, “Why are you letting Joe fly to DC [for a Palantir project]? It’s wasting our budget and not worth our time.” It just goes to show, according to Joe, that “new things are new and ridiculous until they’re not.” It’s worth investing time into new ideas to see how they pan out, before dismissing them completely. How do you convince others to open up to new and unconventional ideas? Joe thinks that the key is to turn the idea into a cause — once there is a cause, there is something people can rally around. “The hardest thing about starting a company is making it a cause that people believe in. A cause doesn’t take on a life of its own until you get people to start believing in it.” Spend your time investing in, developing, and turning new ideas into causes. As Joe transitioned away from operating roles at Palantir and Addepar towards building a fund, he sought out those who complemented his skills. Hiring experts to fill out his gaps was a strategic move, in order to form a strong investing team with wide expertise. Says Joe, “ I hired friends who knew things I didn’t know. I built teams with skill sets that I didn’t have to help my founders.” This way, Joe’s fund was able to help their portfolio companies across all aspects, rather than solely relying on Joe’s expertise and domain knowledge. Joe knew that his operating experience and thought process only represented a thin slice of the wide range of possible challenges faced by portfolio companies. By hiring for a diverse range of skillsets, he was able to flesh out the rest of the slices and offer a larger range of expertise. “Every time you build a successful company, you have to figure out how to do 15–20 things with people who know how to do these things. The next time this happens, you can rely on them to help out. That’s why Silicon Valley works, right? Diversity of skills is what’s so important here.” We hope Joe’s ideas are helpful for those building a framework to choose their first job after graduation. Look for great people who complement your skills, seek out early-stage wins, and keep an eye out for fresh ideas. “Don’t forget that your time is an investment. It’s the same as me investing in my fund, and the best way is in an early high-growth company… the risk/reward is awesome for something early-stage that’s working.” If you’re interested in joining a fast-growing startup, check out Affinity ! We’re hiring for new-grad engineering positions, and we’d love to have you on our team. Quotes have been edited for clarity and length. Stories and lessons from our journey building Affinity 372 Thanks to Adam Perelman . Startup Engineering Entrepreneurship Careers Technology 372 claps 372 Written by untitled Stories and lessons from our journey building Affinity Written by untitled Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-24"},
{"website": "Affinity", "title": "building a highly responsive sheet view with react native", "author": ["Shrey Gupta"], "link": "https://build.affinity.co/building-a-highly-responsive-sheet-view-with-react-native-51129ec34c63", "abstract": "Design Engineering Affinity.co A few months ago at Affinity , we decided to take the most-used feature in our web app, sheets (see the image below), and add it to our mobile app. This blog post explains the React Native limitations that we ran into and how we overcame them. If you’re having difficulties building a responsive table supporting frozen columns and rows with React Native, then please read ahead! We needed to build a table view of our data that supported a frozen header row, a frozen column, and potentially hundreds of rows. This table view also needed to support a variety of interactions including editing cells, filtering, and sorting. FlatList : This component renders data in a scrolling container that provides many features out of the box such as lazy loading and frozen row/column support. It’s important to note that FlatList is a PureComponent , which means that it will not re-render if props remain shallow-equal. Although this is a very powerful component, it has its limitations. It makes performance optimizations only when used in a vertical orientation and provides support for a frozen row/column only in one direction based on the orientation (i.e. frozen row support if vertical orientation and frozen column support for horizontal orientation). ScrollView : This is a simple React Native component that wraps elements in a scrollable view and renders all its elements at once. It provides support for frozen rows/columns like the FlatList, but does not offer any significant performance optimizations like lazy loading. Animated : This is a React Native library that can be used to make animations more fluid and powerful. It’s very flexible — using a function called createAnimatedComponent , you can make any React Native component animatable and bind an animated value to a property on the component. The only downside to this library is that it’s not very intuitive to use. We went to Google for answers… A lot of people had solutions using some combination of FlatList, ScrollView, and Animated to create a good experience for either a frozen column or frozen row, but there was no solution that supported both. We then decided to look at existing apps that supported table views and potentially used React Native. We looked at both Airtable and Quip but realized that neither must use React Native as their solutions were different for iOS and Android. Given what React Native provides and the solutions we saw online, we knew that we could leverage the frozen row/column support on ScrollView or FlatList for one scroll direction and implement the other direction ourselves. This initially seemed like the best approach because we could use a vertical FlatList and leverage all its performance improvements along with scroll loading. In order to support a frozen column, we created a ScrollView that contained all the information for the first column and bound its vertical scroll position to the vertical scroll position of the FlatList. FlatList has a onScroll prop that can be leveraged to get the current scroll position. This seemed to work great if there were only 15–20 rows loaded on screen but as soon as more than 30 rows were loaded, scrolling performance took a huge hit. We realized this might not be the best approach, especially because it is very important for our users to be able to see many rows (potentially hundreds). In order to leverage the frozen column support from FlatList, we had to set the orientation to horizontal which meant that we wouldn’t be getting the performance improvements or the scroll loading. Similar to above, we created a ScrollView that contained all the information for the first row and bound its horizontal scroll position to the horizontal scroll position of the FlatList. This seemed to work no matter how many rows we loaded, but the scroll performance took a hit if there were more than 30 columns shown. Our users rarely need to see more than a couple dozen columns at a time, so we decided that this was a much better solution for our use case. This approach seemed promising but wasn’t perfect so we tried to make it better. Although we had a rough solution, the horizontal scrolling position of the frozen row was slow to sync at times, especially if the user scrolled the table horizontally very fast. We tried a few tactics, like changing the scrollEventThrottle which controls how often the scroll event fires while scrolling. Eventually, after doing some research online, we realized we might be able to make this scroll transition smoother using React Native’s Animated library. By using an animated value for scroll position of the frozen row, the horizontal scrolling of the frozen row was much smoother and felt exactly in sync with the FlatList’s scroll position. We still needed to implement scroll loading, so we decided to wrap both the header row and the body (a horizontal FlatList) inside a vertical FlatList so we could leverage scroll loading. Of course, this still means that we aren’t leveraging FlatList’s performance improvements, but we were happy with our results. You can test and use our solution by checking out this Expo snack ! Overall, this solution works well for us, but it has its drawbacks. Here are the main strengths and limitations as you look to use or build off of our solution: Strengths: smooth scrolling frozen row and column implementation Limitations: no performance optimizations for vertical scrolling (all cells are rendered at all times) table starts to lag once many rows are loaded limitations on number of columns rendered (scrolling starts to degrade with over 20 columns after adding styling and interaction logic for each cell) I’m sure there are a few ideas we didn’t consider and this solution could be better so please feel free to leave comments/ideas! I’d love to chat with anyone about our implementation. Stories and lessons from our journey building Affinity 454 10 Thanks to Adam Perelman . React Native Mobile Mobile App Development Engineering 454 claps 454 10 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-13"},
{"website": "Affinity", "title": "scaling incident response at affinity", "author": ["Rohan Sahai"], "link": "https://build.affinity.co/scaling-incident-response-at-affinity-f49c16cdf836", "abstract": "Design Engineering Affinity.co It’s 3pm. You’re surveying all your snack options because it’s that time of day. You feel your phone vibrate, take a peek — oh shit, “@channel the site is down right now”. You start sprinting back to your desk. This is the moment you’ve been training your whole life for. You put on your firefighter hat, check the logs, and start furiously searching stack overflow to find out what the hell an “nginx 457 your server has escaped our time space consortium” means. You have an idea of what’s going on and start a thread with the sales rep on your team who reported the issue. Meanwhile you overhear the head of support talking to your manager about the issue. Your manager is giving the head of support info that is totally different from what you found. You ask her where she heard that and she points to another engineer who knows their shit when it comes to nginx and the 4th dimension. Oh no, you told the sales rep the wrong thing, now there is misinformation spreading internally and maybe externally. It’s 3:25 pm and the issue is at least fixed. You talk to the other engineer about a long term fix, and it sounds like they are taking care of it, so that’s good.You both hop into an important 3:30 meeting regarding another important project with a potential customer. At 3:45 the issue pops up again, the customer support reps try to find other engineers with context on the issue but have no luck. Finally, after 15 minutes of other engineers with less context fumbling around, you both are out of the meeting and patch it up again. Ok this time it’s fixed for real — turns out we didn’t exit vim correctly. One week later, 3pm: “@channel the site is down right now”. You find the other engineer who worked on this issue with you. They thought you were handling the long term, you thought they were. The rest of the company is second guessing whether the engineering team knows their head from their toes. Communication breakdown!! Even though the root cause of the above issue is presumably from some technical bug, most of the disaster actually had to do with coordination and communication. At Affinity, as we scaled from a team that could all huddle around one engineer’s laptop to grill him or her about what had happened and why, to a team large enough that {insert food delivery service here} couldn’t possibly get all the lunch orders correct for, we quickly learned that the on the fly technical solutions are usually the easy part of incident response, and everything else is hard — and gets harder with scale. Luckily over time at Affinity we’ve made sure to be just as retrospective about our fire fighting process as the actual technical issues causing the fires under the hood. As a result, we’ve come up with a playbook, or list of suggested actions, that has worked well for us in critical times. Most of the line items are extremely simple, like determining a response lead on engineering and support, or creating a custom slack channel for the issue, but little things like that can go a long way in preventing wires crossing, misinformation spreading, and a general confusion from those following the issue peripherally. Note — we’re currently just under 100 people at Affinity. I’m sure some of our items here would have been overkill when we were 10 people, and will probably need to be refined once we are 200, but hopefully this simple list is helpful for other growing startups out there! A clear point person on relevant teams avoids scenarios where internal stakeholders are reaching out to different people for information, which can then cause differing accounts of the incident spreading throughout the company. Setting a clear engineering response lead early on also gives the engineers doing the more technical fire fighting time to just focus on fixing the issue at hand. The person we determine to be the engineering response lead also tackles the rest of the items in this playbook. Similar to above — a centralized place for discussion avoids creating more avenues for repeat explanations and different accounts of the issue at hand. We really like creating a custom slack channel for the incident at Affinity so: Anyone can optionally follow along. We don’t end up clogging up other channels that are dedicated to other topics. It’s easy to look back and create a timeline of events when we retrospect the incident. We have a dedicated urgent channel that the whole company is required to be in. This is a good way to notify everyone that an issue exists, we are aware of it, and working on a solution. We generally post something like “There is currently a partial site outage, follow along in #partial-outage-jan-22–2020 for more granular updates”. This lifts a weight off the response teams back. No one likes putting out a fire while also thinking about the fact they have somewhere to be soon. It also can lead to situations where the response team changes members halfway through the incident, and the new members don’t have enough context to effectively take over. A quick sync with stakeholders is important so everyone is aligned on priorities. An engineer may assume they should patch the issue and redeploy because reverting the changes may cause some data loss, while someone else may bring up the fact that we can write a quick script to save relevant data and revert. There are often many different directions, and tradeoffs to weigh when trying to put out a fire. This sync can save people spending too much time on the wrong solution. Not only is a status page useful for customers to understand why your product may not be working as expected, but it’s great for an internal support team to understand the time at which issues were occurring so they know which bug tickets are likely related to the incident and which are not. There are usually two major things the customer facing teams care about: When can we expect a resolution? How many people are affected? Communicating this as you understand more about the issue is extremely important in enabling them to do their jobs to the best of their ability. Retrospective meetings probably deserve a blog post on their own, but here is a quick rundown on how our retrospective meetings happen at Affinity: The meeting should happen within a week or so of the incident. The meeting should include major stakeholders. The response lead should create a document with an overview of what happened, the scope of impact, and a timeline of all the events from when the incident was reported to when the issue was resolved before the meeting. In the meeting itself, attendees should take a few minutes to read the timeline to refresh themselves on the incident, and then brainstorm: What went well? What didn’t go well? And what should we do next time? It never hurts to emphasize blamelessness in these meetings, especially if your meeting will include people who are new to the process Transparency around major incidents goes a long way in building trust across departments. One department may start resenting another if they don’t have any insight into what happened and what will be done differently in the future. Sharing the retro notes with the whole company is a great way to address this. We keep this playbook in the form of an Asana project template, so when an issue comes up we can easily copy it and complete/assign tasks as needed. Anything you would add? Disagree with? Share your thoughts in the comments section! Stories and lessons from our journey building Affinity 147 Thanks to Adam Perelman . Engineering Incident Response Site Reliability Company Culture Software Development 147 claps 147 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-29"},
{"website": "Affinity", "title": "the path to design leadership a recap from sf design week 2018", "author": ["Bobby Zhang"], "link": "https://build.affinity.co/the-path-to-design-leadership-a-recap-from-sf-design-week-2018-e4c8efc41f06", "abstract": "Design Engineering Affinity.co This year at San Francisco Design Week, we had the opportunity to chat with Andy Montgomery , Head of Design at Square Capital; Deny Khoung , Director of Design at 8VC; Faith Bolliger , VP of Design at SoFi; and Wesley Yun , Senior Manager of Product Design at Uber Freight on design leadership and how they got to where they are today. Here are some lessons we learned from them! Design management consists of coaching, partnerships, advocacy, recruiting, and connecting the dots for design teams. A design manager is essentially responsible for building and maintaining his or her company’s design machine. From Uber Freight, Wesley described his role as making sure his team has constant clarity on projects and deadlines, recruiting, and ensuring everyone is producing at a high level of quality. Andy from Square Capital spoke to collaborating closely with fellow executives to define product strategy and a long term roadmap. Design leadership, on the other hand, can emerge at any level in a company’s organizational hierarchy. A designer doesn’t need a big title to voice key design opinions, influence culture, or educate peers on what makes great design. Faith, from SoFi; and Deny, who collaborated with many startup design leaders in his directing role at 8VC; both described their approaches to design leadership as education around design and its value at all levels of a startup — from executives and managers in all departments to even the CEO. In their view, great design leadership comes from a combination of soft and hard skills to rally stakeholders around supporting good design as a fundamental value. Designers are driven by different goals and interests that, in turn, influence how they bring out their best work. Wesley shared his framework for addressing this: by recognizing different archetypes of designers, such as “mavericks” and “architects.” Mavericks are designers who are constantly hungry to build and move quickly. They thrive in environments that give them massive freedom and little convention or process to start. They’re excited by the idea of building from a blank slate and thrive at startups, especially at the onsets of their journeys. Architects, on the other hand, thrive designing scalable, reusable components by building on existing infrastructure. They’re more likely to do their best work at companies focused on crafting design systems that will truly succeed at scale. Wesley cautioned that design teams can fall into the trap of hiring only designers of one archetype over others. Great teams are led by designers who are hyperconscious of their companies’ needs, and how many different archetypes fit into their projects and priorities. Andy emphasized the importance of investing in scalable design systems and processes earlier than later. While they can initially feel like a burden for a design team itching to move quickly, this builds a foundation for everyone to do their best creative work in sync. At Square Capital, it created consistency and longevity for each designer’s work and magnified design’s overall impact at the organizational level. Designers tend to naturally gravitate towards other designers. But in thinking about creating great products, Andy and Wesley both emphasized that a designer’s primary partners should actually be his/her counterparts in product management and engineering. Andy spoke to the importance of close cross-department collaboration on all fronts, from shipping design to recruiting. He encourages his designers to interact directly with their engineers and product managers to defend and debate their ideas. Andy also highlighted the importance of involving design in engineering and product recruiting: he personally plays a role in recruiting Square Capital’s engineers and product managers to vet for experience collaborating with and overall respect for good design. Wesley warned that inconsistency and a lack of clarity between design and engineering/product leaders can lead to distrust within a company. If design leadership does a poor job of communicating constantly with its peers in engineering and product, it can breed a culture in which design regularly disregards engineering constraints, or misunderstands how the end user will truly think. Faith emphasized the value of identifying individual strengths and weaknesses in building design partnerships. At a great team, individual contributors and managers are highly aware of and constantly building upon each other’s strengths. At the same time, they constantly are identifying their shared weaknesses and hiring people to fill in those gaps. These differences between designers can lead to tension, but Faith encourages her designers to welcome that tension and be comfortable navigating around it. Thank you to all of the San Francisco Design Week organizers, our attendees, and our amazing panelists for making Affinity’s first SFDW event a huge success! If you’re interested in learning more about what we’re up to at Affinity, come check our website out at affinity.co . Stories and lessons from our journey building Affinity 317 Design Product Design Leadership Design Thinking 317 claps 317 Written by Product Designer | Affinity Stories and lessons from our journey building Affinity Written by Product Designer | Affinity Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-06"},
{"website": "Affinity", "title": "affinity and beyond thoughts from my internship", "author": ["Greg Umali"], "link": "https://build.affinity.co/affinity-and-beyond-thoughts-from-my-internship-31468b520430", "abstract": "Design Engineering Affinity.co Hey, everyone, thanks for clicking on my blog post! A bit about myself — I’m a CS student currently taking a gap year before finishing out my senior year of college. After switching from pre-medicine to CS my junior fall and completing an internship with a big bank that summer, I wanted some time to explore and learn as much as possible in the tech field (and pursue my other passion in dance)! In this post, I just wanted to take a minute to talk about my spring internship at Affinity in San Francisco! Of course, I want to preface this assuring you that everything here is my own opinion, no matter how glowing the endorsements are — I really did just like it that much here! Read on to find out what Affinity does, what I worked on, what a day at work was like, what makes the company different, and what I learned from my time here. Affinity is a platform that uses data from your communication streams to help you understand and manage your connections. Started in 2015, founders Ray Zhou and Shubham Goel envisioned a world where anyone could achieve their goals — land a dream job, close big deals, or start a company —by using their networks to have the right resources at the right time. It’s this vision that guides Affinity to this day: using technology to harness your most valuable resource — your network. Affinity as a company is still pretty young and lean, but scaling quite rapidly. In just 10 weeks, I saw it grow in almost every aspect — new hires, evolving features, and uncharted markets. To me, a smaller startup with this kind of upward trajectory was the perfect place to learn as much as possible and really understand a growing company — exactly the kind of opportunity I was looking for during my gap year. It honestly baffles me to look back at how much I grew as an engineer in just 10 weeks at Affinity. I learned a whole new programming language (Ruby), grew more proficient in frontend development using Typescript, React, and Redux, and even learned a bit of SQL and system design as I learned how each component of Affinity’s architecture fits together. I worked on a variety of projects as well: mobile development, implementing new features on their core web product, and developing their relatively new Alliances initiative! With so many resources out there, you really don’t need an internship to learn and practice a new programming language. However, there are some things that you can only learn by working on a living codebase — one that grows, changes, and needs to be maintained. Because Affinity is both relatively young and scaling relatively quickly, I could observe their distinct focus on good programming practices: establishing standards of clean code, codebase design, and naming conventions in addition to the effective enforcement of these standards. At the end of the day, these development frameworks are definitely the most valuable technical lessons I took from my internship at Affinity. So what does a day in the life of an Affinity software engineer look like? It definitely varies by the day, and this variety is what made the experience so interesting — so maybe think of this as a bit more of a sample platter of a day at Affinity! 10:20AM — The engineering team warms up with a quick group game before standup, where we make announcements, update each other on projects, and make sure we’re all at work at a reasonable time. 👀 10:30AM — Once or twice a week, we’ll have opportunities for some learning outside our day-to-day work in the form of technical deep dives, or engineering book club discussions (while I was here, we read Brotopia —which I highly recommend). 11:00AM — Work until lunch! This was usually a good time to pair with other engineers on work, which was super helpful to me as an intern trying to learn the ropes. 12:00PM — Affinity sponsors our lunch, so grabbing a meal around the area with a small group was a great opportunity to get to know other coworkers and try out some of SF’s many great restaurants. With Affinity’s office situated in SF’s financial district, there was never a shortage of great food to discover! 1:00PM — Focus time! Whether through policies of not bothering people with headphones on to having no-meeting afternoons every Wednesday and Thursday, Affinity is really good about making sure we have stretches of time to really focus on our work. That said, there’s always something to look forward to in the afternoon — açaí bowls on Mondays, boba on Wednesdays, or even just a 1:1 with someone I haven’t talked to in a while. 6:00PM — Every two weeks, Affinity has a company dinner, usually followed by board games or some very competitive games of Uno. Just another opportunity to unwind and get to know other people on the team! Maybe you could get a sense from the previous section, but Affinity does things a bit differently. My previous software experience included internships at both a major bank and a 10-person crypto startup, and Affinity is certainly nothing like any other company I’ve worked for. In just my first weeks, what really stood out to me was how well they take care of new hires. Their onboarding process and tasks to help familiarize me with the codebase was incredibly streamlined and helpful (shoutout to Hansen on this one — check out his post to learn more)! But at the same time, it was also very holistic — part of my onboarding included shadowing sales pitches and customer success calls, which helped me really understand the company as a whole, not just the engineering team. Throughout the internship, it was clear that Affinity really cared about you as an individual. Rather than just making sure I pushed code, my manager Shrey would meet with me to give me feedback on my work, have conversations to get to know my background and what motivates me, and to even give me career advice and make an action plan for my next few years. Since I was just an intern only there for 10 weeks, Shrey really didn’t have to do any of that — but it was something I really appreciated. Truly, what really made this internship special was the people. The team is relatively young, but consists of some of the most welcoming, intelligent, and passionate people I’ve ever met. These are people who could not only help me with a nasty Redux bug or diagnose some erratic Sequel queries, but also DJ a trap set at a club, write their own alternative and jazz music, perform magic, go to a dance class with you, or teach you how to cross-stitch at art club! Not to mention the variety of team bonding events I was a part of in my short time here — a company 10K, a week-long engineering team hackathon, a San Francisco scavenger hunt, and even a trip to Daly City for some axe throwing! What underlies all of this is Affinity’s focus on its five core values — from interviews to public shoutouts on Slack, they were apparent throughout my experience. Personally, just working with this engineering every day proved that Affinity not only looks for people who are always learning and take pride in their work, but will also care about you beyond work, doing whatever they can to help you and the company succeed. When finally making the decision to take a gap year, I had one broad goal in mind: to not waste it. So I flew back home to Orlando in November, and (between numerous games of Catan with my family) promptly spent all my time recruiting for internships for the fall, spring, and summer. After spending the winter at a crypto startup in NYC, I was fortunate enough that Affinity allowed me to join as an intern for the spring. So before I go, I just want to offer a bit of advice to anyone who may also be in the internship hunt! Don’t be afraid to take “risks .” Some new grads are averse to smaller companies because of the inherent “risk” of less brand name, or the chance that the company might suddenly fail. But the way I see it, being a young new grad in tech starting their first job comes with a pretty high ceiling, but also a pretty high floor — even if you take those chances early in your career and they don’t work out, your experience is still inherently valuable, and enough demand exists that you will be able to find more opportunities if you try. So perhaps the real risk is in not taking these chances at all! Optimize for learning . When looking for internships, try to look for new experiences that will help you explore a new realm of tech, work on a cutting-edge technology, or will generally just allow you to continuously learn something new. Between marketmaking for a stablecoin cryptocurrency in the fall, working on relationship management software with Affinity in the spring, and building out architecture for high-frequency trading this upcoming summer, my gap year internships couldn’t be farther apart — and that was by design. Even if you end up not loving one specific company, the experience and perspective you gain on your own goals and passions is inherently valuable. Culture and values are everything. These two things will underlie your entire internship experience, and finding that right fit will allow you to make friends, find mentors, and create memories that will remain long after the internship ends. So how do you find the right culture? Ask questions! Your interviewers are resources for you to get a glimpse into the company — definitely learn as much as you can from them! Take notes. I started using Evernote to take down everything I learned and was working on throughout the internship. Not only can it be a helpful reference, but it also helps trace the story of your whole experience at the company as well. Don’t be afraid to ask questions. At Affinity, they made it very clear that helping and unblocking newer hires is one of the most high-leverage activities anyone on the team can spend time on — to the company, it provides the most benefit for the least effort. So don’t be afraid to reach out; everybody on your team wants you to succeed! Stay hungry and excited, or work on something else. When optimizing for learning, I find that working on something I’m personally excited about provides that drive that makes me want to learn as much as possible. Make sure you are continuing to learn, and tell your manager if you feel like you’re running in place. Get to know your co-workers! As much as I came into my experiences wanting to grow as a software engineer, I instead found that sometimes the most valuable takeaways from an internship are the people you meet along the way — it sure was for me! Thanks for reading, and huge props to Affinity for making my time in SF special! Stories and lessons from our journey building Affinity 412 Thanks to Shrey Gupta , Sean Zhu , Hansen Qian , and Adam Perelman . Internships Software Engineering Gap Year Affinity Company Culture 412 claps 412 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-10"},
{"website": "Affinity", "title": "migrating to typescript five practical tips", "author": ["Arlan Jaska"], "link": "https://build.affinity.co/migrating-to-typescript-five-practical-tips-7a57149034d8", "abstract": "Design Engineering Affinity.co At Affinity , we recently finished converting over 100k lines of JavaScript to TypeScript. The conversion took around a year with a relatively low-overhead process, and was totally worth it. Our strategy was fairly straightforward: first we added types to our core data models, and then we began writing new code in TypeScript. We converted existing code from JavaScript to TypeScript anytime we had to make major changes to it. The remaining files were tackled by volunteers, who migrated one file every two-week sprint. We learned a lot of small tips on our adventure that would’ve been nice to know before starting, so here’s our top advice for anyone else who’s just getting started. When updating code that’s years old, you’re guaranteed to find code that doesn’t do things the “right” way. Stop! Don’t fix it! Instead, prefer documenting the existing behavior with types (use any if need be), and make a note to fix it in a separate diff. A lot of “wrong”, “weird”, or “useless” code often does fix at least one hard-to-find edge case and “fixing it” leaves a good chance you’ll introduce some new bug. Forcing yourself not to rewrite things, of course, isn’t a new concept , but I guarantee you’ll get the urge at some point. Fight it. Having this discipline also has the nice side effect of a cleaner commit history. If, further down the line, some other engineer is debugging code introduced by a fix of yours, it’ll be much easier to see which code actually changed when it’s in a separate commit from the type-adding commit. When we analyzed a list of user-facing bugs, we found a significant number that resulted from “fixing” things while converting JavaScript to TypeScript. No bugs were introduced from commits that solely added types. This one is short but important. TypeScript will make your lines longer and your code harder to format. Don’t insert 30 new rules into your style guide. Even the members of our team with the most reservations about using Prettier quickly came to love it after seeing it in action. Use Prettier! When TypeScripting, sometimes you’ll find a type that you’ll need to express as any . This can happen for a variety of reasons: something that’s too complex to figure out in the moment, something that just can’t be expressed in TypeScript, etc. It can be helpful to declare aliases for any so that you can differentiate the any s that you want to fix eventually from the ones you don’t. You can start with just a type FIXME_any = any , and as you encounter scenarios that you want to handle differently, introduce other any aliases. One example of where we used this was our largest, most ancient behemoth of a JavaScript file. By converting it with FIXME_any , we got many benefits of TypeScript immediately, while still being able to slowly go through and incrementally convert it. Converting a file to TypeScript often involves a lot of changes as well as a rename. That’s unfortunate, because git tools often have trouble following renames when this happens in the same commit. We found a solution that works well, involving three separate commits. The first commit just renames the file; the second commit runs prettier (to distinguish formatting vs functionality changes); and the third commit adds the actual types. A reviewer will then be able to look at each individual commit and it’ll be much easier to verify each set of changes. Don’t be afraid to contribute your type declarations or fixes for external libraries to the public type repository, DefinitelyTyped . You’ll get the benefit of the TypeScript community helping to improve your types, and you’ll also help out a ton of other people. Here are some of the contributions we’ve made. Contributing to DefinitelyTyped is also a great, low-stress launching point into contributing to open source, if that’s something you’re new to and want to get into. For a high-quality guide to contributing your first types, check out the instructions provided in the DefinitelyTyped README file. Stories and lessons from our journey building Affinity 510 1 Thanks to Adam Perelman , Hansen Qian , and Rohan Sahai . JavaScript Typescript Refactoring Front End Development Engineering 510 claps 510 1 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-06"},
{"website": "Affinity", "title": "design at affinity", "author": "Unknown", "link": "https://build.affinity.co/design-at-affinity-f5902502db26", "abstract": "Design Engineering Affinity.co When thinking of SaaS software companies, you’d be forgiven for assuming that they aren’t necessarily known for being design-driven, but here at Affinity, we believe human-centered design has a leading role in the success of our customers and the success of our business. Over the last year and a quarter we have significantly grown the design team and our design muscle. We’re excited about this new chapter and thought this would be a great time to introduce you to the new faces on the team. Product Designer Designer based in SF, excited about building products that will have a positive, lasting impact on the world. Born and raised in Southern California, but fell in love with the Bay after moving here for college. Outside of work, you can find me hiking, doing a DIY project, or catching up on reading. What are some design books you recommend? Sprint by Jake Knapp The Design of Everyday Things by Donald A. Norman Whose work do you look to for inspiration? Graffiti artist: Hueman Digital art: teamLab Design team: Airbnb Watercolor artist: Natalia Kadantseva What is your daily ritual? Wake up and stretch (tech neck problems 😓) Make some tea and drink it while watering my plants Eat breakfast while reading the news Start work with some schedule planning for the day, and head into standup/meetings/focus time End the day with a workout, dinner and some DIY projects or TV What are your favorite hex codes? #000000 & #D5FAE7 Photo notes: Officially obsessed with plants. My indoor garden is steadily getting bigger as I propagate my plants and add new ones to the family. Product Designer David is a designer who grew up on the East Coast of Canada and is currently living and working in Toronto. As a designer, David follows Dieter Rams’ “Less, but better” philosophy and believes good design goes beyond aesthetics and should create better experiences. What are some design books you recommend? Thinking in Systems by Donella Meadows Whose work do you look to for inspiration? Dieter Rams Wes Anderson What is your daily ritual? Wake up later than I planned Daily grocery run Make an effort to be active by going to the gym, running or taking a long walk Cook dinner Go to bed later than I planned What are your favorite hex codes? #E5E5E5 & #F5F5F5 Photo notes: Nike Free Run 5.0: Favorite pair of running shoes Zerg Plush: From my favorite video game StarCraft 2 Terrarium Product Designer Filip is a designer crafting thoughtful & intentional experiences. He was born in Poland and grew up outside of Chicago before moving to California to forget what winters feel like. He likes to take things apart and (sometimes) put them back together. With a background in computer science, studio art, and early stage startups, you’ll often find him wearing many hats (though not in real life — they make his head look small). What are some design books you recommend? A New Program for Graphic Design by David Reinfurt Interaction of Color by Josef Albers Slow Reader: A Resource for Design Thinking and Practice by Ana Paula Pais & Carolyn F. Strauss Ruined by Design by Mike Monteiro Whose work do you look to for inspiration? Sam Youkilis Mark Allen & Machine Project Work & Co. The talented folks behind makespace.fun IDEO What is your daily ritual? Making hand-ground pour over coffee for my partner Emily and me the same way every morning for the last 4 years (ever since I did this ). What’s your favorite hex code? #587E6D (color) #FAFAFA (hex) Photo notes: The Vitra Tip Ton chair, designed by Edward Barber & Jay Osgerby. Colorful, indestructible, and with a rocking motion perfect for the little boy in me who can’t keep still. Brand Designer Kaitlin is a mission-driven brand designer, outdoor enthusiast, and native New Yorker. You can usually find her with a camera in her hand when she’s not at her desk. What are some design books you recommend? Flow: The Psychology of Optimal Experience by Mihaly Csikszentmihalyi Designing Brand Identity: An Essential Guide for the Whole Branding Team by Alina Wheeler Whose work do you look to for inspiration? Photographers: Henri Cartier-Bresson, Annie Leibovitz, Ansel Adams Illustrators: Early Disney films, Ernst Haeckel, editorial pieces on the New York Times Designers: Erik Nitsche, Tina Roth Eisenberg Other talented beings: Mother Nature, Marc Maron, Hannah Gadsby, James Turrell, my partner Matt McKee What is your daily ritual? Morning tea, sunshine, and a stroll around Berkeley. What’s your favorite hex code? #E1E100 & #A3007A Photo notes: My shelter-in-place set-up with abundant sunshine, a Gerald Scarfe print from Pink Floyd’s The Wall , and my Canon AE1. Currently grooving on Kodak Tri-X 100 film stock. Brand and Marketing Designer Born in Montreal, raised in Tampa Bay, and now residing in Toronto, Sandy is a brand designer who is passionate about growing brands through good design. In her free time, Sandy loves watching films and annoying her neighbors with her tap dancing. What are some design books you recommend? Interaction of Color by Josef Albers How Brands Grow by Byron Sharp The Brand Gap by Marty Neumeie Whose work do you look to for inspiration? Illustrators: Aubrey Beardsley, Elliot Snowman, Kensuke Koike Directors: Terrence Malick, David Lean, David Cronenberg Design Agencies: Red Antler, Concrete, Koto Tattoo Artists: Jessie Preston, Hoode, Nacho Eterno What is your daily ritual? I’m not a morning person so I need a little extra time to get my day started.The first thing I do when I wake up is make myself a cup of coffee. After my coffee, I’ll take Morty on his morning walk. When I get to my desk, I always like to read Brand New so I can get some good inspiration flowing before I jump into my work. What are your favorite hex codes? #286755 & #F1E6E4 Photo notes: A page from my sketchbook, aka my workspace. All of my creative work, ideas and musings live here. It’s messy, and half the time I can’t read my handwriting, but nothing beats putting a pen to paper. VP of Design Travis is a San Francisco based design leader and has been a designer in tech since the early days of e-commerce on the web. He joined Affinity in March 2019 and is our head of design. Previously, Travis led design efforts for Prosper Marketplace and Snapfish. Outside of work you can find Travis DJing , writing , publishing , and hiking. What are some design books you recommend? Lean UX by Jef Gothelf How to Be a Graphic Designer Without Losing Your Soul by Adrian Shaughnessy A-Z of The Designers Republic by Ian Anderson Whose work do you look to for inspiration? Hiroshi Sugimoto (photographer) Robert Frank (photographer) Diane Arbus (photographer) Annie Leibovitz (photographer) Bruce Conner (mixed media) Agnes Martin (painter) Jeremy Fish Shepard Fairey Margaret Kilgallen Barry McGee What is your daily ritual? First thing in the morning I do yoga, then make a cappuccino and cook breakfast. Then I walk the dog on Bernal Hill for an hour. What’s your favorite hex code? #FF0073 (I've been digging this bright color as a bold contrast to dark colors) Product Designer I was born in central valley California, and my grandma got me into competitive sailing at a young age which allowed me to travel the globe before I was 21. Living 6 years in Europe taught me more about myself than any life experiences before this period. I love the ocean, and like the desert once in a while. I throw clay pottery as it makes me feel very grounded. I am a connector of people and enjoy my alone-time when I can grab it. What are some design books you recommend? Don’t Make Me Think by Steve Krug Don’t Push the River: It Flows by Itself by Barry Stevens Sprint by Jake Knapp Whose work do you look to for inspiration? Tadao Ando Modern art museums Airbnb design Yves Behar What is your daily ritual? Piano and cello to wake, meditate, write down what’s on my mind… then I begin my day for whats to come! What’s your favorite hex code? #CD00EE Photo notes: Antique study lamp originally with oil and wick. 1850 made in London near where I lived 170 years later. I wonder who studied near the lamp, and wonder why and how it made the trip to California. The shelter-in-place photo is simple desk with fresh air, views of the street, and always waving to neighbors. Product Designer Mika is a product design leader, business strategist, and entrepreneur based in SF. He works at the intersection of design, strategy, and data science. His primary focus is on technology and human-centric design. He joined Affinity in June 2020 and is a Product Designer on Mobile Team. Before Affinity, he co-founded Roadster and worked as Principal Designer at Fandom. Outside of work you can find him playing with his family on a beach, or see him stare at art installations at modern art museums. What are some design books you recommend? The Startup Playbook by David S. Kidder Ten Principles for Good Design by Dieter Rams The Laws of Simplicity by John Maeda Hooked: How to Build Habit-Forming Products by Nir Eyal Whose work do you look to for inspiration? Ai Weiwei Banksy NDT — Netherland Dance Theater Bauhaus I find regular people doing regular things the most inspiring. What is your daily ritual? Two cups of coffee Morning play time with kids Dog walk 3 minutes meditation What’s your favorite hex code? #010101 & #F7F7F7 To learn more about Affinity, visit affinity.co . Stories and lessons from our journey building Affinity 111 Design Team SaaS San Francisco 111 claps 111 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-04"},
{"website": "Affinity", "title": "scaling postgres at affinity with citus", "author": ["Arzav Jain"], "link": "https://build.affinity.co/scaling-postgres-at-affinity-with-citus-bc93bb7f4441", "abstract": "Design Engineering Affinity.co In a previous blog post , we discussed how we at Affinity migrated a large column in our biggest Postgres table to DynamoDB. Below we outline why we took the next step to scale to a rapidly growing user base: migrating the entire table to a sharded Postgres cluster managed by Citus . Each user that joins Affinity brings along a plethora of emails that are synced into our emails table. This email data is the backbone of how Affinity powers relationship intelligence for our customers. There are two main clients that read and write to the emails table: syncers and consumers . In our original single-node Postgres architecture, the emails table resided together with all the other tables in the same database. The syncers constantly fetched emails from Gmail and Exchange web servers and inserted them into the primary Postgres database. These writes were streamed to the Read Replica, from which the emails were read by our consumers. The consumers modified these emails to pull out the relevant information that makes most of Affinity’s features possible. These modifications were written back to the primary database. Given the large volume of emails ingested by Affinity, this meant that our reads and writes were dominated by those to the emails table, and growing fast: To give some perspective, in the architecture described above, the emails table comprised approximately 75% of our entire database size on disk, while the next biggest table comprised a mere 8%. Some consequences of such a read-write pattern were: The cache hit rate for the emails table was 79% on the read replica and slowly decreasing over time. As more users joined the platform, the size of the table and indexes grew over time and less and less of the frequently accessed data was able to fit in memory. Not surprisingly, the decreasing hit rate was accompanied by increasing database swap usage. Both of the above led to degraded performance for queries not just to the emails table but also to other tables, since the other tables and their indexes were fighting for space in the same cache as the emails table. For a while, we combatted these problems by vertically scaling with bigger and better Postgres RDS instances. But we were soon approaching the upper limit on RDS Postgres memory size, not to mention the increasingly prohibitive costs of continuing to vertically scale. Hence, we considered scaling horizontally instead. We began by sharding only our emails table and leaving the other tables in the Postgres RDS setup (shown above). The emails table was the best first candidate because: As described above, it dominated the read-write pattern and suffered the lowest cache hit rate of all our tables. It was involved in few joins with other tables and so moving it to a different datastore meant rewriting only a few queries. By migrating only our biggest table instead of the all the tables, we get most of the wins we are looking for while still having a short feedback loop to encounter pitfalls and get familiar with the new datastore. Equipped with more experience, we can in the future migrate the rest of our database over to the new datastore. Our choice for the new datastore was Citus. Reasons being: Citus uses Postgres under the hood and is simply an extension to plan and distribute queries. Consequently, our developers can continue to work with the same database engine and query language, allowing us to make use of and continue to build on our in-house Postgres expertise. Citus has great documentation and tooling to help us migrate our data from Postgres with no downtime (namely, a feature called Warp , which takes advantage of the logical decoding feature in Postgres 9.4 and up). Citus abstracts out the sharding so that application developers writing queries to the Citus database (mostly) don’t have to think about it. The coordinator node in the Citus cluster rewrites the queries and fetches data from the appropriate shards before aggregating and returning the results back to the client. After migrating the emails table to Citus, our new read-write pattern looks like this: We were pleased to see our cache hit rate for the emails table go from 79% to 99.9%. This helped reduce query latencies across the board; as an example we show the 99 percentile latencies below for a frequently hit application server endpoint that reads from both our Postgres RDS setup as well as Citus. As you may notice, there’s a sharp drop in latencies on May 12th, which is when we made the final switch over to Citus. Huge thanks to Rohan Sahai and Adam Perelman who worked together to implement this migration and read drafts of this post. We’d also like to give a shoutout to the Citus team for being available and answering all our questions promptly. Stories and lessons from our journey building Affinity 343 Database Engineering Postgres Distributed Systems Scaling 343 claps 343 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-20"},
{"website": "Affinity", "title": "build a staging site in 15 minutes", "author": ["Arlan Jaska"], "link": "https://build.affinity.co/build-a-staging-site-in-15-minutes-cb56b54688da", "abstract": "Design Engineering Affinity.co This is how we added a staging site to Affinity without touching a line of application code. At Affinity, we do a big deploy of our main branch once per day, and smaller deploys of specific features or bug fixes throughout the day. We’ve found that this provides a good balance of preventing bugs from popping up throughout the day while also keeping the code in production fresh. One developer pain point we had was that after merging in code, it might not go live for hours (or days, if it was merged in on the weekend). We wanted a way to be able to test our changes in a production environment, before our changes reached our users. After considering a few different approaches for building the staging environment, we decided that the fewer lines of code we touched, the more realistic our staging environment would be. Our key insight was that we could implement staging without touching the application code at all — by changing configuration at the nginx level. Normally, we use nginx to pass through requests to our application servers which run our deploy branch. These servers respond to requests for *.affinity.co , where the subdomain identifies the customer. Each of our customers gets their own subdomain. For example, affinity.affinity.co serves Affinity’s own instance of Affinity. Our goal was to create a new subdomain, staging.affinity.co , to serve the same data as affinity.affinity.co , but running a more recent version of our application code. That way, we’d be able to try things on our staging server with real data. We started by spinning up an additional instance of our application server to serve our staging site. The only change we had to make here was to specify our main branch instead of our deploy branch. We added a cronjob to check for new builds every 5 minutes and then re-deploy the staging server. Then, we added special nginx configuration listening for the domain staging.affinity.co . For requests directed to this staging domain, we rewrite the headers as if the request were going to the main site before passing it off to the staging server instead of the regular application server. That means that the staging server has no idea it isn’t affinity.affinity.co ! To finish things up, we also have to rewrite some headers coming back from the staging server to fix redirects. This required just a few lines of nginx configuration. Depending on your production setup, this configuration could look a little different; but as long as you apply the right headers you’ll be fine. If you’re using a Kubernetes nginx ingress controller, you can make a separate ingress controller configuration file and use the following annotations: This worked really well for testing out new code in a realistic production environment! After trying it out for a few weeks, we decided to always serve from staging for the instance of Affinity that we at Affinity use ourselves. Because we’re heavy users of our own platform, this has helped to catch bugs and regressions before they reach our customers. Stories and lessons from our journey building Affinity 415 Thanks to Adam Perelman and Rohan Sahai . Web Development Nginx DevOps Continuous Delivery Engineering 415 claps 415 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-08"},
{"website": "Affinity", "title": "persisting sessions with react native", "author": ["Shrey Gupta"], "link": "https://build.affinity.co/persisting-sessions-with-react-native-4c46af3bfd83", "abstract": "Design Engineering Affinity.co A few weeks ago, we ran into a particularly tricky bug: some users told us that our mobile app was logging them out every time they closed the app. This blog post explains the underlying issue as well as how we resolved it. If you’re running into issues managing sessions reliably with React Native, we hope this helps! After extensive debugging, we found that the mobile app was sometimes sending an invalid cookie to the server when it was re-opened, thus causing the logout issue. We couldn’t figure out why the invalid cookies were getting sent until we stumbled across the core issue: the cookie storage provided to React Native by the underlying native frameworks isn’t 100% reliable. (If you’d rather just read about our fix, skip to the next section.) fetch on React Native is implemented on top of the native networking stacks (Objective-C on iOS, and Java on Android). Cookies are automatically managed by these native stacks rather than by React Native itself [ GitHub ][ StackOverflow ]. We found two possible causes of our bug: Possible cause #1: Native APIs storing the incorrect cookie We took a look at Apple and Google’s cookie acceptance documentation for their native networking stacks: iOS ( NSURLConnection ): The URL loading system automatically sends any stored cookies appropriate for an NSURLRequest object unless the request specifies not to send cookies. Likewise, cookies returned in an NSURLResponse object are accepted in accordance with the current cookie acceptance policy. Android ( CookieManager ): Cookies are manipulated according to RFC2109. Since React Native is implemented on top of these APIs, it’s possible that even a cookie manually specified on the application layer can be modified by the native API and replaced with the cookie stored in the native cookie storage ( cookies on iOS, CookieStore on Android). Because cookies are handled transparently, debugging this hypothesis further would have required writing native code, which we decided to avoid for now. Possible cause #2: Cookies aren’t persisted properly By default, for performance reasons, cookies are first stored in RAM and then eventually synced to persistent storage (by CookieSyncManager on Android and NSHTTPCookieStorage on iOS). It’s possible that the cookie never ends up making it to persistent storage (e.g. if the app is closed soon after being opened) — on both iOS [ StackOverflow ] and Android [ StackOverflow ]. The StackOverflow recommendation for Android involves forcing a cookie re-sync; for iOS, it involves extending the native API to manage cookies manually. We still weren’t certain about the particular conditions that might cause cookie persistence to fail, but at this point it looked like building a workaround might be faster than working on fixes at the native layer. Since we couldn’t rely on default functionality to implement reliable cookie storage, we decided to take matters into our own hands and manage session cookies ourselves. We chose to use AsyncStorage to store our cookie and set that cookie in the http header in each request. This way, we can ensure that the correct cookie is persisted even if the app is closed or updated. We also use a third party library called react-native-cookies to flush the cookies that are stored by the native cookie managers before each request so that the correct cookie (the one we retrieve ourselves from AsyncStorage ) is sent with the request. Here’s a snippet of our code to demonstrate how this process works: With this change, we’ve resolved the issue, and our users can now stay happily logged in to our mobile app (on iOS and Android ). If you’re running into similar issues, we hope you find this useful! Stories and lessons from our journey building Affinity 735 9 Thanks to Hansen Qian and Rohan Sahai . Engineering React Native iOS Android 735 claps 735 9 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-26"},
{"website": "Affinity", "title": "learn fast and read things why and how we started a technical reading group", "author": ["Adam Perelman"], "link": "https://build.affinity.co/learn-fast-and-read-things-why-and-how-we-started-a-technical-reading-group-56a61db515bd", "abstract": "Design Engineering Affinity.co At Affinity , we strive to make time not only for those things that are urgent — fixing bugs, shipping features, supporting our customers — but also for those things that are important but never urgent. Learning is one of those key priorities: there’s rarely an urgent, short-term reason to spend time learning, but in the long run, it’s the most important thing we can do to increase our effectiveness. As software engineers, we’re lucky to have tons of great and widely accessible ways to learn: we can attend meet-ups and conferences, allocate time to experiment with new tools and techniques, take online classes, and more. On the Affinity engineering team, one of our favorite tools to accelerate our learning is our technical reading group, which we kicked off a few months ago. We hope this blog post convinces you to do the same and gives you some ideas for how to start. Here’s how our technical reading group works: We meet once a week for 45 minutes. We alternate between reading a book for a few weeks, and then reading shorter articles for a few weeks. We rotate which engineer is responsible for facilitating the discussion. Facilitators aim to create discussions where everyone participates. Often, this means doing smaller, focused brainstorms or conversations in pairs or small groups before we regroup for a larger discussion. For any given week, doing the reading and attending the group is completely optional, so engineers choose which sessions to join based on interest. We’ve chosen a few different kinds of topics. Sometimes we read about specific, technical topics that are particularly relevant or timely given our tech stack and roadmap. For instance, we spent a week reading about Kubernetes (which we use in production — you can read more about our early experiences with Kubernetes here ). We spent another week reading about Citus when we were evaluating whether to use it to scale out our data storage (we’ve since decided it’s worth trying out, and are currently in the middle of that migration project). And we spent a week learning about GraphQL (we decided to avoid migrating to GraphQL for now, though we might revisit that decision down the road). Other weeks, we read about general engineering principles and practices that we can apply at Affinity. For instance, we spent four weeks reading The Effective Engineer . Our group discussions of that book led us to several concrete action items: among other changes we’ve made since reading it, we’ve been more careful to avoid one-person projects when possible and to improve our project estimation by tackling the riskiest, highest-variance tasks first. And sometimes, we choose topics that are purely to satisfy our engineering interest, even if there’s not much short-term relevance to our day-to-day work. Early this year, we spent a week digging deep into the mechanics of the Meltdown and Spectre attacks (we thought this blog post on Meltdown was fascinating and very readable — check it out if you haven’t already!). Another of our upcoming readings involves a seminal paper on avoiding network congestion . Of course, this all takes time: an hour or so of reading and 45 minutes of discussion each week. But the long-term payoff is worth it. Say, for instance, that reading The Effective Engineer increased our effectiveness by a conservative 1%. Over the course of a 2000-hour year of work, that means each of us on the team will save about 20 hours. That’s a great return for a book that took less than 10 hours to both read and discuss — and those returns keep compounding over time, paying for our initial time investment many times over. And beyond increasing our direct impact at Affinity, this practice of continual learning increases our fulfillment and enjoyment of our work. Facebook famously used to promote a culture where engineers could “Move Fast and Break Things” (though they’ve since changed their stance on breaking things ). There’s a lot to be said for focusing on speed and execution, but we hope that wherever you work, you can also make time for another priority: Learn Fast and Read Things. What strategies does your team use to accelerate learning? We’d love to hear about them! Stories and lessons from our journey building Affinity 388 Thanks to Rohan Sahai and Hansen Qian . Startup Engineering Learning Reading Software Development 388 claps 388 Written by CTO @ Affinity Stories and lessons from our journey building Affinity Written by CTO @ Affinity Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-03"},
{"website": "Affinity", "title": "introducing design at affinity", "author": ["Bobby Zhang"], "link": "https://build.affinity.co/introducing-design-at-affinity-fa7850c403b0", "abstract": "Design Engineering Affinity.co Hey-o! Welcome to Affinity Design’s learnings and stories from our journey to redesign how businesses manage and leverage their networks. We’ll be posting along with our talented engineering team to talk about how we build Affinity together. Affinity is a relationship intelligence platform to expand and evolve the traditional CRM. To our nimble design team, that means focusing on building an intuitive experience around people’s communication data and providing a clear and accurate picture of their network. To ensure that our team is driven by a high standard of craftsmanship, we constantly ask ourselves how we can be better and learn from our shortcomings. We look forward to sharing all of those learnings with you folks here as we build this blog. We’ve come a long way since Affinity’s earliest days, when we iterated fast to discover the right synergy between design and engineering. Since then, we’ve learned a lot about building a startup’s design team from the ground up. We created a culture of design-reviewing where engineers and designers have deliberate and intentional conversations around what problem we’re solving for our customers and what the best experience we can implement is with every feature. We crafted procedures and practices to ensure every feature we ship is at a level of polish that our whole team can be confident in when releasing to our customers. Design was a part of Affinity’s earliest DNA, when we were just 4 people with an idea. Now that we’re grown exponentially and have seen first-hand the value our work has created for our customers, we hope to share our success, failures, and learnings with you. Our journey has only just begun, but we’re stoked to be able to have you along for the ride. Please feel free to leave us your feedback and tell us your own stories so we can be better designers together. ✌️ If you’re interested in learning more about what we’re up to at Affinity, check out our new marketing website at affinity.co ! Also, we’re growing our design team. Come join us ! Stories and lessons from our journey building Affinity 348 Design Design Process Product Design Designer Design Thinking 348 claps 348 Written by Product Designer | Affinity Stories and lessons from our journey building Affinity Written by Product Designer | Affinity Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-17"},
{"website": "Affinity", "title": "running parallel jobs on circleci", "author": ["Rohan Sahai"], "link": "https://build.affinity.co/running-parallel-jobs-on-circleci-9fd7c6cb76f5", "abstract": "Design Engineering Affinity.co tl;dr — If you use CircleCI and want to speed up your builds through parallelism, you have a couple of options: (1) you can parallelize tests running within a particular CircleCI job, or (2) you can run multiple jobs in parallel (or you can do both). There’s a lot of documentation online on how to parallelize tests, but not as much about parallelizing jobs (or at least it’s not very intuitive to find this documentation), so this blog post explains how we use CircleCI to run multiple jobs in parallel. We use CircleCI at Affinity to automate testing and building our applications. It runs our test suite on every commit, builds static assets, and pushes our docker image to a registry we can pull from in order to deploy. It’s a great product, and has been a big asset to our development workflow. However, we recently started running into an issue where our main application builds were taking far too long. Our CircleCI setup was pretty simple: 1- Checkout code 2- Build assets if on production 3- Build application docker image 4- Run backend tests 5- Run frontend tests 6- Push docker image to our docker repository Naturally, one of the first things we looked into for speeding this process up was some sort of job parallelization. Frontend tests don’t need to wait until backend tests are done running, so why don’t we run them at the same time? Thankfully, CircleCI does support this. Workflows is the feature we were looking for. Long story short, if you are using CircleCI 2.0 (this is a requirement), running jobs in parallel is as simple as defining jobs like you may have done previously and adding the ‘workflows’ blurb at the bottom: Boom, so simple! This was a bit trickier to figure out than I expected. There's lots of articles online on test suite splitting and other parallelization options that have nothing to do with running jobs in parallel. Part of the confusion also comes from the fact that CircleCI implements parallel jobs as part of their Workflows feature, which includes lots of functionality beyond just running jobs in parallel. There are a couple of caveats of using workflows. 1- Each job uses its own container, so you might be occupying more containers per build. Of course, you may save enough time from running jobs in parallel that it doesn’t really matter. 2- As of this post it doesn’t seem like CircleCI has fixed their notification hooks to actually work with workflows. Previously, if your job failed you got a ‘build has failed’ email… and then when your job ran successfully you got a ‘build is fixed’ email. Great. Now with multiple jobs in a workflow, if one fails you get the ‘build has failed email’ , but if another unrelated job in the workflow passes you get the ‘build is fixed’ email. The notifications don’t seem to realize that there are multiple jobs in a workflow. I imagine CircleCI will fix this sooner rather than later. 3- Weak API support for workflows. We use the CircleCI API for some internal tools (so that we can show a list of successful builds on a given branch). Unfortunately, the API still seems optimized for the pre-workflows setup. You can’t simply fetch successful workflows, you can only fetch builds from within a workflow…so this forces us to do some manual data cleaning to populate a list of successfully built workflows for a given branch. CircleCI Workflows are great, but new so still missing some stuff, but still great because they save us time and time is money. Stories and lessons from our journey building Affinity 166 Thanks to Adam Perelman . Docker Continuous Integration Deployment Automation Engineering 166 claps 166 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-22"},
{"website": "Affinity", "title": "diving into mobile with react native", "author": ["Shrey Gupta"], "link": "https://build.affinity.co/diving-into-mobile-with-react-native-7ea0eb89313", "abstract": "Design Engineering Affinity.co A few months ago, Affinity ventured into new territory for front end engineering: mobile. Since many of our users are busy executives who spend much of their time on the go, we knew a mobile app had to be a core part of our product lineup. This blog post outlines how two of us on the engineering team took a chance on React Native to build out the mobile counterpart for our web app. The meat of this post outlines how we solved a few particular problems with React Native (navigation, data consistency, and animations). Feel free to jump straight to the Learnings section below to read about these challenges. Before starting anything at all, we had a few questions to ask ourselves… Do we need to build an app for both iOS and Android? Yes — as with most companies, our customers used a mix of iPhone and Android devices, so we needed to support both. Do any of our engineers know Swift or Java? Yes and no — most of our engineers had used Java but had never used it to program an Android application. None of us had used Swift before. Do any of our engineers have extensive prior mobile experience? No — some of us had worked on simple mobile apps, but no one had built a sophisticated enterprise product on mobile before. Should we invest the time to learn Swift and Java? Ideally not — we wanted to hit the ground running and make it easy for future engineers to apply their web skillset to mobile as directly as possible. Are there frameworks that allow building a single app for both platforms? Yes — there were (and are) many such frameworks out there, like Xamarin , Ionic , jQuery Mobile , Ratchet , React Native , and many more. Our gut told us that we should build this app with a JavaScript framework so that 1) it worked on both platforms and 2) wouldn’t require a huge learning curve for our engineers, who already knew JavaScript very well. Our web app is build on React, so it made sense to check out React Native . We’d really enjoyed programming with the React paradigm on the web, so we were excited to use it on mobile and transfer all our React skills and best practices to this new platform. We knew that as a relatively new technology, React Native might have its flaws, but we were confident that with Facebook behind it, we were moving into a technology ecosystem that would strengthen and grow over time. Given intense demand from customers, we wanted to build this app fast. Another engineer and I allocated 7 weeks to build all of it out. Luckily, we knew that we could reuse most of our existing API endpoints. A few endpoints would have to be different, of course, because mobile required a few data representations that were different from the website. None of our React components from our website would be usable on mobile; HTML tags such as div mean nothing in React Native, which uses native UI components rather than HTML elements. We had to familiarize ourselves with using View and Text instead and write all components from scratch. With those constraints in mind, we broke down the application into the different screens that the user could access, as mocked out by the design team. We divided the project based on how much work we thought each screen would take, created a 6-week plan with weekly milestones, and gave ourselves 1 additional week as a buffer to account for unexpected obstacles. We’re very happy that we went with React Native. It’s a powerful framework and surprisingly easy to learn. On our journey towards a successful and on-time launch, we faced a few tough engineering problems. I’d like to highlight a few of those and share the tips we came up with. Navigation is a fundamental necessity for any application with more than one screen. There are 3 basic types of navigation: tab, stack, and drawer. Tab navigation is generally seen on the bottom of the screen, where each tab represents a different entry point into the app. Along with the navigation on the bottom of the app, there has to be some sort of navigation within each tab so that clicking on a certain element can take you to a new screen. This is known as stack navigation . ( Drawer navigation is a third navigation pattern that we didn’t end up using.) To maintain all this navigation state, we decided to use a library called React Navigation which included the tab navigator UI container and the state management for both tab and stack navigation. One of the advantages of this library is that it’s very customizable. For example, it allows you to manually manage navigation state. We took advantage of this because we wanted our push notifications to navigate the user to certain pages in the app (where some screens may be nested in the stack); React Navigation has a detailed guide for this. With any app, screen tracking is very important and goes hand-in-hand with navigation. It’s important for things like controlling the color of the status bar, where the color might change depending on which screen is displayed. In order to do this manually, this guide from React Navigation advised creating a screen recording Redux mix-in. One problem we had to solve for was resetting the stack navigation for a particular tab. Stack navigation may contain 10 screens, so if you want to go back to the first screen, you have to hit the back button 10 times. A common pattern to solve for this issue is to simply double tap that tab to reset the stack. React Navigation didn’t have this functionality out of the box, but luckily someone had made a PR for this feature and it had already been approved by the React Native team (though not yet merged in). We forked the repo with the proposed change and used it to build this feature out. This feature has now been added to the standard React Navigation distribution (details can be found here ). Overall, we found React Navigation to be a solid and useful library. We did face a few issues and had to write some custom code to perform screen tracking and manage navigation state, but we’re really happy with the results. Data consistency is a common problem on web apps. On mobile apps, it’s even more pronounced. When you’re using a browser, opening a new tab or window fetches up-to-date data from a server automatically. On mobile, in contrast, when you move to a new screen, update data, and go back, the data on the previous screen doesn’t update — the screens are stacked, and popping a screen from the top of the stack doesn’t re-render the screen beneath it. To avoid showing users inconsistent state, we needed data to be updated instantly, so we decided to use Redux to manage our global state. In retrospect, we wish we had integrated a Redux store into our code earlier to avoid the monotonous task of converting existing code to Redux. React Native uses JavaScript stylesheets and inline styles for styling its components and everything is display: flex . If you know flexbox, then you’re probably thinking: that’s awesome! When we first started, we had never used flexbox and didn’t really know what to think. A lot of our styles for web relied on things like display: table and float: left. These styles work, but they often became overly nested and complex. Flexbox made our lives much easier — it just makes sense when you read and write it. We shared our learnings with the rest of the engineering team, and now we style elements using flexbox on web as well as mobile. Animations bring life to mobile content and can make a user’s experience much more enjoyable. React Native provides a great animation library out of the box. If you plan on using animations, take some time to read and try out a few of the different examples. In our app, we wanted to give users the ability to take action on an item by swiping left/right on it (similar to taking an action on an email in many mail apps). We found many libraries online, but they didn’t cater to our specific needs. For example, a lot of them had great animations but didn’t let us customize the styling of the action buttons. We also wanted to add a small jiggle animation if the user clicked on a row to teach them that the row could be swiped. We ended up writing a component ourselves that mimicked the core behavior of other open source components but with these added features. As mentioned earlier, React Native is still a fairly young library. Although a ton of great companies are using it, there’s still relatively few mature open source components available. Quite a few times, we found ourselves evaluating open source components that looked nice but didn’t quite cover all of our use cases. We had to write our own custom components, which took time but also provided us with invaluable knowledge for when we work on the next version of our mobile app. We’ve also realized that some of the components we’ve built out might be really useful for others, so we’re considering making them open-source them in the future — if there are particular components mentioned here that would be useful to you, please let us know! Stories and lessons from our journey building Affinity 311 Thanks to Hansen Qian , Rohan Sahai , Adam Perelman , and Gabriel Fan . React Native Mobile App Development Flexbox Engineering 311 claps 311 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-22"},
{"website": "Affinity", "title": "beyond culture fit non technical hiring criteria for engineers", "author": ["Adam Perelman"], "link": "https://build.affinity.co/beyond-culture-fit-non-technical-hiring-criteria-for-engineers-c89c0fc82668", "abstract": "Design Engineering Affinity.co At Affinity , our top priority has always been to build the strongest possible team. In the overused but still powerful words of Jack Welch: “This whole game of business revolves around one thing. You build the best team, you win.” We’ve taken this principle to heart and are careful never to let the pressing needs of a fast-growing company override our high bar for hiring candidates. In order to keep a high bar, though, you need to be clear about what the bar measures — in other words, about your hiring criteria. If you’re hiring software engineers, there’s plenty of tough questions to answer about what technical criteria to use and about how to measure them. Do you want generalists or specialists? Do you care about algorithmic skill or about pragmatic programming ability? Should you use whiteboards or allow candidates to use their own IDEs? These technical questions matter. But they’re not all that matters. In this post, I’ll focus on explaining Affinity’s non-technical hiring criteria for engineers. Back when Affinity was starting out and we were just a few people, we lumped our non-technical hiring criteria into a broad category we called “culture fit.” This seemed fine at first, and put us in good company. You can find articles in the Harvard Business Review arguing that “culture fit is the glue that holds an organization together” and any number of articles explaining why fit is important. When it came to actually making tough hiring decisions, though, culture fit wasn’t a particularly clear or useful concept. Did it mean that we should want to be friends with a candidate? That we should have similar senses of humor? That we should have interests in common? Or just that we shouldn’t hire jerks? Culture fit was too vague to help us make decisions. As we did a bit more research, we found an additional reason to question culture fit as a criterion: its tendency to push us towards bias. As articulated by the Medium engineering team , “Culture fit is a polite way of saying ‘be like us.’” Unsurprisingly, looking for people who look, sound, and act just like us makes it harder to build a diverse team. As we worked to refine our interview process and hiring criteria, we learned quite a bit about unconscious bias and the role that factors like race, gender, and age can play in triggering evaluations of a candidate based on stereotypes rather than real, job-relevant skills. One of our main takeaways from our research into unconscious bias was that it’s critical to have clear, well-defined evaluation criteria . Culture fit certainly wasn’t helping us in that regard. On a similar note, Stripe’s engineering team is well-known for its Sunday Test , which used to ask interviewers to decide: “If this person were alone in the office on a Sunday, would that make you more likely to come in and want to work with them?” Stripe has since abandoned the Sunday Test, though, explaining, “We want people who would make excellent coworkers rather than be our personal friends.” We take diversity seriously at Affinity — both because it’s right thing to do and because we believe that diverse perspectives make our team stronger. Culture fit was poorly defined, pushed us to favor candidates who looked and sounded like the current team, and left us vulnerable to unconscious bias, so we worked to define clearer, fairer, and more job-relevant hiring criteria. We started out by taking a page from the Good Eggs engineering team . All of us gathered, took a few minutes to individually write down our top criteria on sticky notes, and took turns putting our criteria up on the wall and explaining them. We then clustered them into themes and used dot voting to identify the criteria we cared most about as a team. This gave us a great start. Over time, we’ve refined and clarified our non-technical criteria into the following four categories: Initiative. We hire people who are eager to take the lead on doing what’s important. Empathy. We hire people who care about others, including their teammates and users. Communication. We hire people who explain their ideas clearly and effectively. Flexibility. We hire people who are willing to adapt to the changing needs of their company and their teammates, are willing to change their opinion based on data, and are open to feedback on how to improve. We know these criteria aren’t perfect, and we’ll continue to iterate on them, as we do with everything else. But they have helped us quite a bit. Whenever we make a hiring decision, we ask ourselves how we’d rate a candidate in each of these areas. This process helps us get on the same page about a candidate’s strengths and weaknesses, and reminds us to focus on the key factors that define what makes a great engineer at Affinity. It also reminds us of the things that we shouldn’t focus on: whether a candidate is similar to us, whether we’d want to hang out with them on a weekend, or any of other the questions related to “culture fit” that easily cross the line into bias. We’ve found that topgrading interviews give us lots of insight into a candidate’s initiative, flexibility, empathy, and communication. But almost every interview, no matter how technical, reveals some data about these categories. Even in a coding interview, for example, we often notice flexibility, or lack thereof: does the candidate take feedback well? If they get stuck trying one approach, and we suggest another, how do they react? Communication is important in coding interviews too: can candidates explain the design behind their implementation? And so on. We incorporate data from all of our interviews when we evaluate candidates on these criteria. We are as committed to our non-technical criteria as we are to our technical criteria. We’ve rejected brilliant engineers because of gaps like a stark lack of empathy, and we’ll continue to do so. If you’re building a team, and you want to hire outstanding people, make sure you know what “outstanding” means for you. Stories and lessons from our journey building Affinity 276 1 Thanks to Rohan Sahai and Hansen Qian . Hiring Software Engineering Recruiting Diversity In Tech Engineering 276 claps 276 1 Written by CTO @ Affinity Stories and lessons from our journey building Affinity Written by CTO @ Affinity Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-23"},
{"website": "Affinity", "title": "migrating large postgresql columns to dynamodb", "author": ["Rohan Sahai"], "link": "https://build.affinity.co/migrating-large-postgresql-columns-to-dynamodb-4af413cf5f5d", "abstract": "Design Engineering Affinity.co UPDATE (over a ~year later): Looking back at this project it was worthwhile for us to move email bodies out of our main RDS database, and it was worthwhile to test out DynamoDB in a real world setting. That said, if I were to do it again, I would probably move email bodies to a smaller separate RDS instance to reduce the complexity of our infrastructure. Also, the cost savings did make sense at the time, but as we scaled up, DynamoDB became pricier which offset the savings from delaying an RDS upgrade. Hopefully this article is still useful/an interesting read! There are definitely still valid use cases for Dynamo. Before the data migration described in this post, more than half of our Postgres storage space was eaten up by ONE column from a single table. This blog post outlines how we moved all this large data into what we think is a more suitable datastore given our query patterns. Scroll to the bottom if you aren’t interested in the background or implementation and just want to see some tricks/tips/headaches related to DynamoDB. At Affinity we sync with our clients’ email inboxes, which means we have an emails table in Postgres. This table has a body column which is responsible for storing the actual content of the various email records. Email bodies can be fairly large (not including attachments). A rough average we calculated at one point was ~8kb per email body (after encryption). That might not sound like a lot, but if you have 60 million + email records that ends up being ~500 GB of storage. So what’s the issue with these really large bodies in Postgres? Postgres is designed to handle large attributes fairly well. There is a technique known as TOAST (The Oversized-Attribute Storage Technique) in which tuples that are over 8kb, which is commonly the fixed page size in Postgres, are broken up into multiple physical rows - so Postgres natively avoids bloated pages. But we were still compelled to test out storing these columns in NoSQL for the following reasons: We want to keep these potentially giant entries out of Postgres memory so other queries can remain as fast as possible. Similar to the above point, but we’d had issues in the past from our DB working set size being larger than our DB memory capacity— hopefully this would alleviate that, or postpone the need for another memory upgrade to our master database instance. We’d like to by default not include email bodies in queries hitting our emails table so we’re not also clogging our application memory. We have extremely high write volume on our emails table — and NoSQL is designed to horizontally scale more easily than most relational databases. Even though we are just moving one column for now, this can be a POC for potentially moving the entire table to NoSQL. Storage is more expensive on Amazon RDS (where our Postgres instance is hosted) than most hosted NoSQL solutions. The ‘body’ field would be fairly easy to port to another data store since we never filter queries based on the this field, thus our query patterns would be mostly unaffected. As a small startup without a dedicated devOps team, a fully managed solution is crucial for us. This was the real separating factor between Dynamo and other popular NoSQL solutions such as Cassandra. We also already use AWS for quite a bit of our architecture and we’ve had mostly positive experiences, so it made sense to first explore the Amazon option before looking into other fully managed solutions. Getting our Dynamo table up and running was incredibly simple. Since we were only moving a single column from our Postgres emails table, we didn’t have to worry about implementing a new PK strategy… we just needed to store a reference to the existing email record and the body. Creating the table was as simple as: The trickier parts of the problem were: 1- Figuring out how to alter as little of our application code as possible so developers could still interact with our ORM’s Email model the same way they did previously. 2- Coming up with a smooth migration strategy so we could drop our ‘body’ column in Postgres safely. This involved writing to both Dynamo and Postgres, and then gradually converting reads to Dynamo. Most of our backend at Affinity is in Ruby and we use an ORM called Sequel to interact with our database records. Our email records are referenced in lots of places throughout the codebase, and updating each of those places individually to account for the new Dynamo column would be a hassle. What we did to mitigate this was essentially write a wrapper around our ORM, so NoSQL columns were updated/fetched automatically. Well… we only fetch the body from Dynamo if the body method of our email record is specifically called, not just when the email record is fetched, otherwise we’d be loading up the bodies into application memory unnecessarily. It wasn’t quite this simple, but the wrapper looked something like this: Tapping into Sequel to do this properly with the various different update types could occupy its own small blog post so I won’t dive into more detail here. Migration Strategy Email syncing is at the core of our product at Affinity, so a smooth rollout of our Dynamo implementation was crucial. Instead of just switching over to reading and writing from Dynamo in one go, we broke up the feature into several steps that were rolled out over a span of roughly two weeks. Step 1 — Update our worker responsible for writing emails to Postgres, to ALSO write to Dynamo. Step 2 — Backfill all Postgres bodies from before step 1 into Dynamo. Fun fact, at about 5000 WCU migrating ~40 million emails took around 40 hours. Step 3 — Update codebase to always read email bodies from Dynamo and never from Postgres. Step 4 — Stop writing email bodies to Postgres. Step 5 — Drop the body column like it’s hot. This is essentially a collection of things I would tell someone if they were about to dive into Dynamo — that aren’t immediately clear when you start using Dynamo. Dynamo’s pricing model has nothing to do with how many machines you have or how powerful they are — it’s based on usage. More specifically Write Capacity Units (WCU) and Read Capacity Units (RCU). The WCU/RCU which you have to set in advance for your tables (unless you use auto scaling… more on that below) are based on not just number of read/writes, but also the size of the items you are reading or writing. One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for items up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of read capacity units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. One write capacity unit represents one write per second for items up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB will need to consume additional write capacity units. The total number of write capacity units required depends on the item size. One pretty important detail that I breezed over when initially diving into the docs was that Dynamo by default does not have strongly consistent reads, and instead eventually consistent reads are the default. What does that mean? When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data. And the time factor which was surprisingly hard to hunt down in their docs Consistency across all copies of data is usually reached within a second. Fortunately for us, eventually consistent reads are fine since we’re not reading that data within a second of writing it. Also keep in mind strongly consistent reads require more RCU and so are more expensive — 2x the price as of this date. Relevant Docs . It would be awesome if Amazon didn’t force us to pay for unused RCU’s and WCU’s — for instance our application is less active at night so we can probably lower the capacity units significantly. You could do this programmatically which is a bit annoying, or you can use the new (as of this blog post) Auto Scaling Feature ! A couple of gotchas with autoscaling: 1- The automatic updates to RCUs and WCUs don’t happen immediately. When there is a burst in traffic you should still expect throttling errors and handle them appropriately. This may be a deal breaker on the auto scaling feature for many applications, since it might not be worth the cost savings if some users have to deal with throttling. Luckily for us most of our Dynamo writing/reading actually comes from background jobs, where a bit of throttling is fine and doesn’t affect our users. The following charts outline the issue more clearly. You can see around 21:00 we have a burst in writes, and about 20 minutes later the autoscaling kicks in and provisions us more write capacity Even though autoscaling does kick in eventually and keeps the throttling from spiking further, we still had ~15 minutes of throttling against the lower the write capacity. 2- There is no good documentation on how to programmatically configure autoscaling. The client libraries essentially have nothing auto scaling related in their documentation. If this was possible the throttling issues brought up in point 1 could be slightly alleviated. We could set the minimum capacities higher during the day to prevent throttling for typical morning spikes. Maybe the auto scaling will get smarter as it collects more data and this won’t be an issue! 3- If you use the Dynamo to S3 AWS Data pipeline template for backups, autoscaling can give you inconsistent backup speeds. More on that in the section below. As of this writing there isn’t great documentation on Dynamo backups. There seems to be one semi-standard solution which involves using AWS Data Pipeline to export Dynamo DB Tables to AWS S3. Unfortunately it’s a sort of fragile system without great documentation or error handling. I’ve had some backups fail without much information, and backups often get stuck in the preparation phase for hours before actually running. Also the default template leaves you with data pipeline warnings that are confusing to fix. One nice thing about the template is that you can specify what percentage of the table’s RCU capacity you would like to use for the backup. For example if your table is provisioned with 1000 RCU’s, and your backup is set to use 1% of the capacity, the backups will consume 10 RCU’s per second. However… if you use autoscaling, you’ll have no idea what your provisioned RCU’s are when the backup runs. In our case, we want really high read capacity for the backup so we can get it done in less than a day over the weekend. We hacked together a workaround in which we have a cron job update the RCU before the backup runs on AWS. But this isn’t foolproof because as I said before sometimes the backups don’t run immediately, and the autoscaling function may scale our RCU down before the backup runs. Dynamo supports batched reads and writes, but with limits. You can at most request 100 items to read at a time, and write (PUT/DELETE) up to 25 items at a time. One important thing to note that wasn’t immediately obvious to me at first, is that some items in the batched request can get throttled while others don’t. So it’s important to properly handle this in application code. The most obvious solution is to recursively call your batch request function with the throttled IDs until all the requests are complete. DynamoDB has been fairly easy to use/learn, and we haven’t hit any major road blockers or deal breakers with the technology. We haven’t even come close to any sort of upper bound on WCU/RCU so it seems that any issues that arise with Dynamo in the future won’t be related to load. It hasn’t been too long since we started using Dynamo, but so far it’s been a great solution to our bloated column problem. Stories and lessons from our journey building Affinity 403 3 Thanks to Adam Perelman and Hansen Qian . AWS Dynamodb Postgres Data Migration Engineering 403 claps 403 3 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-06"},
{"website": "Affinity", "title": "understanding tricky joins and multi table updates in postgresql using boolean algebra", "author": ["Paul Martinez"], "link": "https://build.affinity.co/understanding-tricky-joins-and-multi-table-updates-in-postgresql-using-boolean-algebra-7b329606ca45", "abstract": "Design Engineering Affinity.co Writing a proper SQL UPDATE query involving multiple tables in Postgres can be tricky and counterintuitive. This stems from the fact that when performing an UPDATE , other tables are made available using a FROM clause, instead of the JOIN clause that’s normally used when fetching data from multiple tables in a SELECT statement. These have slightly different semantics, which can lead to confusion when debugging an incorrect UPDATE query. (Note that MySQL does have support for proper joins in UPDATE queries, unlike Postgres.) A couple of weeks ago we were writing a data migration and we ran into some difficulties that directly resulted from our lack of understanding of the differences between the two types of queries. Before diving into the specific issue we ran into, let’s review how joins work in SQL: We’ll first create two tables with some sample data and use them to give a quick rundown of the different types of joins. The most common syntax for performing a join is T1 <JOIN TYPE> T2 ON <expression> , where T1 and T2 are tables, and expression is the join condition which determines if a row in T1 and a row T2 “match.” JOIN TYPE can be one of the following (words in square brackets are optional), each generating a different result set: [INNER] JOIN : For each row R1 in T1 , the joined table has a row for every row R2 in T2 that satisfies the join condition: LEFT [OUTER] JOIN : First an inner join is performed, then a row is added with NULL values for the columns of T2 for each row R1 in T1 that was not matched with a row in T2 . This ensures that every row of T1 will be present in the outputted table: RIGHT [OUTER] JOIN : The same as a left outer join, but with the roles of T1 and T2 swapped, ensuring that each row of T2 will be present in the outputted table. FULL [OUTER] JOIN : The combination of a left outer join and a right outer join. An inner join is performed, then a new row is added for each unmatched row from T1 and T2 : CROSS JOIN : Every row in T1 with every row in T2 , forming the Cartesian product. When using this join type there is no ON <expression> clause specifying a join condition. There is another, less commonly used way to select data from multiple tables. Tables can simply be listed one after another in a comma separated list in the FROM clause, and this is identical to performing a cross join. (At least when only two tables are listed and no filtering happens in a WHERE clause.) Notice that this is also the result one would get performing a regular join with TRUE as a condition. And going the other way, notice that one can perform an inner join on a condition C by performing a cross join and then using a WHERE clause with condition C . That is, the statements SELECT * FROM T1 INNER JOIN T2 ON C and SELECT * FROM T1 CROSS JOIN T2 WHERE C are equivalent. Because outer joins are defined in terms of inner joins, generating the result set for a query can involve alternating periods where rows are added and where rows are removed. This can lead to complex behavior when combining an outer join with a WHERE clause. Note that the following two queries are not equivalent: See the results applied to our example tables: The query of the form ON A WHERE B returns a subset of the results returned by the query of the form ON A AND B . (Is it always a subset? Yes! Proving so is left as an exercise for the reader.) If we walk through the steps to generate the result sets, we can see how the two queries differ. The first query does two separate filtering passes, separated by the addition of rows due to the outer join, while the second query just does a single filtering pass by both conditions. The filtering pass for condition B occurs at a different time in each query, and operates over different intermediate results, so it’s easy to see how the two queries are not equivalent. In general, the vast majority of queries only use inner joins and left joins, but it’s crucial to understand their relationship with cross joins as well to understand how UPDATE queries work. Let’s take a closer look at the syntax for UPDATE queries. Here is a stripped down grammar for the most common cases (starting from the Postgres grammar ): Updating a row based on a row in another table (“performing an update with a join,” in other words), is trickier because you can only specify additional tables in the FROM clause. The Postgres documentation has this to say on the subject: When a FROM clause is present, what essentially happens is that the target table is joined to the tables mentioned in the from_list , and each output row of the join represents an update operation for the target table. When using FROM you should ensure that the join produces at most one output row for each row to be modified. In other words, a target row shouldn’t join to more than one row from the other table(s). If it does, then only one of the join rows will be used to update the target row, but which one will be used is not readily predictable. The following query shows how UPDATE queries can have unpredictable results. (The contents of the two tables is included for reference.) What should the missing value of value1 be? Should it be W or X ? It could be either depending on the order of the joined rows constructed by Postgres. While this is certainly something to watch out for, more commonly we might just want to join in a table to decide which rows we want to update. Suppose we only want to update rows in table_2 that correspond to a table_1 row with value1 = c , or rows in table_2 that don’t correspond to any value in table_1 . We can express which rows we want to update easily with a SELECT query: But if we naively try to convert this to an UPDATE query, we get: Because the FROM clause performs a cross join, there won’t be any rows in the intermediate table with a null table_1.id . This makes this query impossible to represent without resorting to nested queries! More evidence that UPDATE queries involving multiple tables can be quite difficult to write correctly. A recent product update required a data migration that left us scratching our heads trying to figure out why our queries weren’t working. We’ll provide some context about the schema of the table we were modifying, explain the changes we needed to make, then explain how we used boolean algebra to help debug the issue. In the Affinity web app, one of our main interfaces is a structured spreadsheet. You create new lists of persons or companies through this interface, but you can also use it to view all the people or organizations your company has ever communicated with. In this spreadsheet rows are people or organizations and columns are either custom columns that the the user manually edits, such the Status of a Deal, or data that Affinity automatically calculates and displays, such as the date of the last email correspondence with an entity. Users can rearrange the order of columns, and choose to show or hide certain columns. These actions are local to the user, so if they hide a column it isn’t hidden for their coworkers, and persistent, so if you log out and log back in you’ll see the columns in the same order as they were originally. Initially we stored this data in the user’s browser, using LocalStorage, but this turned into an unmaintainable nightmare, so we decided to store the column state in our database. We used the following schema: The user_id column indicates which user’s state we’re storing. list_id and all_list_type together indicate which page’s appearance we’re saving. If this is data for a list view, then list_id is set and all_list_type is NULL . If this is state for viewing the page of all persons or organizations, then all_list_type is set to 0 or 1 respectively and list_id is NULL . (The lists table has a model_type column that’s also set to 0 or 1 to indicate if it’s a list of persons or companies.) field_id and key together indicate the data shown in this column. For user populated columns, like Status, field_id points to the record that represents the Status column. For automatically populated data, key will be set to a string constant, like last_email or next_event . And finally index and hidden are used to store the order of columns and which ones have been hidden by the user. Therefore my saved view of all the organizations we’ve talked to at Affinity might be represented by the following records: and my view of our engineering recruiting list might look like this: Previously we had displayed Source of Introduction as a value that we had computed ourselves, but we got feedback that this data wasn’t always correct and that users wanted to be able to change this value. This column was originally indicated by having key set to source and field_id set to NULL , but in order to enable manually editing, we had to create two new Fields, one for persons and one for organizations, and update all the appropriate rows in our user_sheet_columns table. We could perform this migration with two UPDATE queries, one to update the columns on the All Persons page and lists of people, and one to update the columns on the All Organizations page and lists of organizations. Now, selecting which rows we wanted to update in each case could easily be expressed using a left outer join: But with UPDATE queries, you can’t use a JOIN expression and instead have to bring in other tables in a FROM clause: UPDATE T1 SET ... FROM T2... . We tried to use the same approach here: But this query didn’t work. For some reason all the rows where list_id was NULL were getting updated, even those with all_list_type set to 1. But it was right there in our query! We joined in the list table appropriately (first condition), then if list_id and lists.model_type is NULL , we only update the row if all_list_type is 0 . Clearly something was wrong… Desperate and frustrated, we used the time honored tradition of throwing out our work and just tried rewriting the query a different way: Sure enough, this version worked. Clearly this version looks a little simpler, but why did this one work while the other one didn’t? Recall that the tables are cross joined together when using a FROM clause in an UPDATE query. In the context of our user_sheet_columns query, this means that every user_sheet_columns row was paired with every row in lists , and every combined row that matches the WHERE condition gets updated. But we wrote the condition for the first query as if we were doing a join, relying on certain rows to get filtered out, then others added before doing another filtering pass. If we don’t want to update a row, then we need to make sure it doesn’t end up in the result set. Let’s use boolean algebra to deliberately break these conditions down and see if we can track down the issue. Both queries have the key = 'source' condition, which we can be pretty sure is correct, so let’s ignore that. Other than that we have four other equality checks present between the two queries: Let’s assign these the letters A , B , C , and D so they’re easier to manipulate. And substitute these in the actual conditions: We observed that the rows that were incorrectly getting update were rows with all_list_type = 1 (and so also list_id was set to NULL ). This implied that for these rows A was false, B was true, and C was false. This meant the condition for these rows simplified to simply D . Since each row in user_sheet_columns was paired with every column in lists , then as long as there was a row in lists that satisfied D , then the rows with all_list_type = 1 would be included in the result set. And of course there were lists rows that satisfied D , because a list satisfied D if it was a list of persons, and we sure had plenty of those! So we were updating these rows for the all organizations pages because they had been joined with completely unrelated lists of people. So how did the correct query avoid this issue? Well, the rows for the all organizations pages had all_list_type = 1 , so C was false. So then to be included in the result set A ∧ D had to hold, translated back to the original as (list_id = lists.id) AND (lists.model_type = 0) . But since all_list_type was set for these rows, that meant that list_id was NULL , so condition A was false, guaranteeing their exclusion from the result set. Thanks, George Boole. Fun fact: While looking up how perform updates in various RDBMS dialects, I tried to find the actual SQL standard. It can be found here , for a cool 178 Swiss Francs. Stories and lessons from our journey building Affinity 714 1 Thanks to Rohan Sahai and Adam Perelman . Postgres Sql Rdbms Engineering 714 claps 714 1 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-22"},
{"website": "Affinity", "title": "early experiences with kubernetes debugging unresponsive nodes", "author": ["Paul Martinez"], "link": "https://build.affinity.co/early-experiences-with-kubernetes-debugging-unresponsive-nodes-ac68bf1734dc", "abstract": "Design Engineering Affinity.co Here at Affinity we’ve chosen to use Kubernetes to manage our production cluster. Billed as a system for “automating deployment, scaling, and management of containerized applications,” it has been invaluable in helping us scale past just a few machines. It has eliminated an enormous amount of devops work and allowed us to largely focus on building new features and developing our core product into something that provides real value to our users. Kubernetes hides a lot of the complexity associated with running dozens of different services in production, but it’s a mistake to assume that it’s all you need to run a fully-fledged production service. In our initial naïveté, we expected Kubernetes to somehow expertly manage our cluster straight out of the box, maximally optimizing our resource usage, and automatically detecting failures within the cluster and repairing itself with no human intervention required. But we soon discovered that Kubernetes is a complex system unto itself. It requires configuration, fine-tuning, and programmer guidance and input to operate safely and efficiently, just like any other large system. Kubernetes won’t know how to efficiently use memory and CPU unless you tell it how much memory and CPU a job needs . Kubernetes can recover from certain classes of failures, but you still need a robust monitoring system, beyond some basic options that Kubernetes provides, to ensure the health of your system and minimize downtime. This is the tale of one of our first infrastructure incidents at Affinity. In it Kubernetes stars as both the villain and hero, causing mischief and obstructing our debugging efforts before supplying a solution to our problem. In the end we realize that it is just one character of a bigger story, a powerful ally in our engineering adventure. Our production jobs at Affinity can be clustered into four main groups: Basic web server, serving all requests to affinity.vc Syncers, which fetch emails and events for processing Consumers, which process each email and event to detect things like introductions, and emails that expect a response cron jobs, to handle things like sending daily round-up emails each morning, or refreshing some interaction data that is too expensive to compute in real-time As advertised, Kubernetes makes it super easy to deploy all of these jobs to our cluster. (You can skip ahead to Outage! if you’re already familiar with Kubernetes.) Some of its explicit features we take advantage of include: Automatic load balancing between machines ( nodes in Kubernetes terminology): Given a set of jobs to run Kubernetes will evenly distribute them amongst the nodes in your cluster. Each instance of job running on a node is called a pod . Easily adding new nodes to a cluster: We’re not yet at the scale where we need our cluster to dynamically scale according to load, but if we notice things are starting to get a bit cramped on our nodes we can bump up a number in a config file and Kubernetes takes care of the rest. Automatic restarting of jobs: Kubernetes detects when a node goes down and will create new pods on other nodes to replace the ones on the failed node. If a pod randomly fails then Kubernetes will also restart it. Rolling deployments: Kubernetes has built-in support for rolling out an update, bringing up new instances one a time while decommissioning the old ones. This allows us to deploy new changes in the middle of the day with no downtime. Kubernetes doesn’t do everything for us. There are some features we’d like it to support and other things that are out of scope that then need to be fit into the Kubernetes system. These include: Intelligently distributing jobs based on CPU or memory usage: By default Kubernetes will evenly distribute pods amongst your available nodes. If you have some jobs that eat up a lot of memory ideally each would live on a different node, but you might get unlucky and have them all assigned to the same node. You can tell Kubernetes how much memory or CPU you expect a job to need, and it will make sure that pods are scheduled to make good use of resources. When you’re just starting, however, it’s difficult to know what are reasonable values to set, so you might just be flying blind for a while. Redistribute jobs when new resources are added: When a new node is added to the cluster Kubernetes doesn’t move any existing pods to the new node until the next deploy. In larger clusters this isn’t a big problem, but in a small cluster one node failure puts substantially more stress on the other nodes and can be an inefficient usage of resources. Monitoring: This is a bit disingenuous. Once a cluster is setup, there are couple options that can be set up with literally a single command, but they’re pretty bare-bones: The Kubernetes Dashboard provides a nice looking UI, but all the information here can be fetched from the CLI, and Heapster provides a better look into the internals of your cluster, surfacing information about CPU, memory and network usage of individual pods. Heapster only collects quantitative information from your cluster, and it has no concept of the higher level Kubernetes concepts. Alerting: Kubernetes won’t alert you if one of your jobs keeps failing because you mistyped a command in a config file. Because alerting is highly dependent on the monitoring infrastructure, it lies pretty far outside of the scope of Kubernetes. Fix the bugs in your code: Bummer. On February 13th our site went down, and we had no idea what was going wrong. At this point we hadn’t set up any monitoring, so our only way to assess the state of the cluster was through the Kubernetes command line interface, kubectl . We checked to make sure that all of our jobs were running and found that we had a lot of pods in an Unknown state, and multiple nodes were reporting as NotReady . We use ReplicationControllers to make sure that each of our jobs has at least one instance running. When a node went down, Kubernetes would put all the pods running on that node into the Unknown state. To satisfy the contract of the ReplicationControllers , Kubernetes would then reschedule those jobs on other nodes. On this particular day multiple nodes went down and Kubernetes was unable to schedule all of our jobs. Notably, our web servers weren’t running, hence the chaos. What was happening to our nodes? We had no idea, and Kubernetes wasn’t telling us anything useful. We looked at our EC2 dashboard to see if AWS could tell us anything more about the machines, but curiously AWS said the machines were totally fine. Without any leads concerning what was wrong, we opted for the nuclear option and totally redeployed a new cluster. While this got our website back up and running, we hadn’t resolved anything and later than day another node in our new cluster went down. Luckily we didn’t have multiple nodes go down again, so we didn’t experience the cascading failures that seemed to cause the outage earlier in the morning. Unable to find a way to fix the NotReady nodes, we realized that we could simply terminate the EC2 instances through the AWS console. The AWS Auto Scaling Group backing our cluster would quickly bring up a new instance and Kubernetes would discover it and incorporate it into the cluster. Since Kubernetes wouldn’t redistribute jobs, we’d have to make sure to re-deploy after doing this to avoid ending up with two or three idling machines. We had found a temporary solution for making sure our cluster didn’t fall over again, but it was obviously not ideal. Over the next few days, these are some of steps we took to figure out what was going wrong. Some of them were dead ends we followed that didn’t get us closer to a solution, but did teach us more about the Kubernetes ecosystem. We asked the kubernetes-users Slack channel for advice on how best to debug the issues we were facing. Someone suggested getting access to the Kubelet logs on the nodes that were in the NotReady state. Kubelet is a process that runs on every node in a cluster and is in charge of managing the containers in the pods assigned to node. Getting access to those could possibly give us more detailed information about what was going wrong, but this turned out to be difficult. We hadn’t set up any sort of centralized logging, so in order to access the logs we needed to ssh into a node and run journalctl -u kubelet . However, we had previously moved our cluster into a private subnet inside our AWS VPC so that all traffic from our website could appear to come to a single IP address. This meant that ssh traffic couldn’t reach the machines. We reverted this change so that our nodes were temporarily publicly accessible, but then we discovered that our deploy machine didn’t have the SSH private key needed to access the cluster. We had originally set up our cluster with kops locally from one of our developer’s laptops, and by default it saves the public SSH key at ~/.ssh/id_rsa.pub to copy to new instances. Eventually we were able to navigate through our network and get access the Kubelet logs, but they were unhelpful. What was helpful was what we learned about kops , the preferred way of setting up a new Kubernetes cluster on AWS. All of these issues we ran into are solved by kops . When creating a cluster with kops you can provide a public key via the --ssh-public-key option. kops can also set up your cluster in a private AWS subnet via the --topology private option, which ALSO requires you to specify a networking option. (We went with --networking weave when we recreated our cluster again a few weeks later.) And to solve the issue of not being able to access the nodes in private subnet, you can also use the --bastion option to create a bastion host that has access via AWS’s security groups. Wanting to avoid another disaster where our whole site went down, we set up a simple cron job to check for unhealthy nodes in our cluster. Every 5 minutes it processes the output of kubectl get nodes,pods -o json and sends a message to Slack telling us which node is down and which pods were running on that node. Each time a node went down over the next few days we got a list of pods that were running on that node and this eventually led us to a single suspect: The job that powers our Unanswered Emails uses an NLP library that runs on the JVM. To get an idea of how much memory it was using we ssh’d into a Kubernetes node and asked the Docker daemon: Getting this sort of peek at what Kubernetes was actually doing was pretty cool. Kubelet just starts running the Docker images and we’re able to use Docker to tell us what we want to know. docker ps shows all the current containers running, and we’re able to use that to see the container ID of the unanswered emails job. Then running docker stats gives us an updating dashboard of memory usage by container. And sure enough, there was the unanswered emails container, using up well more than its fair share, and constantly increasing! We ran the job locally and determined that it indeed constantly used more and more memory. Efforts to find the source of a memory leak were unsuccessful though… Digging around in the EC2 dashboard gave us another piece of information that allowed us to come up with a more complete story. In the Description section of the unhealthy node we clicked the link next to “Root Device” to take us to the page describing the EBS volume backing the instance. Clicking on the Monitoring tab for the EBS volume showed that when K8s-health-check reported the node going down all the monitoring graphs spiked. All of sudden idle time dropped to zero and read throughput sky-rocketed. The evidence seemed to suggest that the unanswered emails job would use up all the memory and cause our instance to start thrashing memory . To view this data in a little more manageable way, we deployed Heapster into our cluster (which took all of a single command: Kubernetes can be pretty impressive), which collects quantitative data from our cluster and provides Grafana as a graphing front-end with some very simple pre-built dashboards. One of these lets you view memory usage of a single pod over time, and there was the unanswered emails job, steadily climbing. And, as we only discovered at the time of writing this post, Heapster also collects the rate of major page faults, which spikes at the exact time our K8s-health-check reported an issue! We had found the root cause of our problem. Great! How could we fix it though? It turns out there was a pretty easy solution. Docker allows you set to set memory limits on a container, and it will kill the container if it tries to use more than that. Kubernetes exposes this through memory limits . This provided a way to prevent the unanswered emails job from going too crazy, but not from getting assigned to a node with another memory hungry process. To handle this case you can also give Kubernetes memory (and CPU) requests , which it uses for scheduling decisions. With our shiny new Heapster monitoring we looked at some recent history of all of our jobs and noted about how much memory they needed to run, and how much they would spike to, and set these as memory requests and limits on all of our specifications. This ended up actually being a pretty tedious and inexact process. Some of our jobs had very spiky memory usage, so it felt wasteful assigning them so much memory. Still, adding an extra machine (with just a few commands) is worth the peace of mind gained by knowing we won’t suffer another outage like we did. Our experience with Kubernetes has for the most part been fantastic, but debugging this issue over the course of weeks was fairly painful. A lot of it was caused by our own inexperience, so here are our last bits of advice and wisdom that we gained from this harrowing adventure. Set memory request and limits on all of your controllers! If you don’t Kubernetes will blindly schedule things, possibly leaving you with multiple memory gobbling jobs on the same node. Be especially careful with any third party services you add from a single YML file. They might not have set them, and they might be placed in a different namespace and you might forget about them. Deploy Heapster! But watch out for filesystem usage. Heapster can provide a lot of useful data for individual jobs in your cluster, and it’s definitely better than no monitoring at all. But by default it just writes its data to the temporary EBS volume attached to the EC2 instance, so if it ever gets evicted you’ll lose all of the collected data. Thanks to Adam Perelman for reading through early drafts of this. Stories and lessons from our journey building Affinity 337 3 Docker Kubernetes DevOps Engineering 337 claps 337 3 Written by Stories and lessons from our journey building Affinity Written by Stories and lessons from our journey building Affinity Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-22"}
]