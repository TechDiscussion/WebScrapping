[
{"website": "BigCommerce", "title": "Joining an engineering team", "author": ["Joe LeBlanc"], "link": "https://www.bigeng.io/joining-an-engineering-team-2/", "abstract": "It’s 8:47pm and you still haven’t been outside today. Your last meal, which you ate at your desk, was toast… with an egg. You haven’t had a chance to go grocery shopping in a couple of days. You’ll have that time once you finally get this piece of code to connect to the “API” (which strangely enough, involves FTPing an SVG file) consistently. At this precise moment though, you receive an email: If this sounds like your current job, you’re probably a freelancer and it’s definitely time to join an engineering team. Joining an engineering team frees you from constantly hustling for projects. You stop playing bill collector and worrying about where your next paycheck is coming from. Best of all: you’re finally surrounded by people who care about writing good code. Despite the relief of finally getting away from freelance work, joining a team can be a bit of a culture shock. You’re adjusting from being the sole expert to being surrounded by peers. Which brings us to the first of several challenges… Your days of bower install latest-hotness-you-saw-on-twitter-dot-js --save && git commit -am yolo && git push origin master are over. You may be used to having clients defer to your judgment: after all, you’re a professional. Now that you’re on a team, both your code and your third-party library choices will be reviewed. Be prepared to defend what you commit. Your peers have a vested interest in understanding the code making it into the repository. You will not always necessarily be around to help out when your code needs to be maintained (e.g., when you go on vacation). And of course, the libraries you choose have to solve the problem, scale, and not break the rest of the system. On the other hand, your team is constantly adding new features. Plenty of opportunities arise for using new libraries and tools, many of which wouldn’t make sense to use in a freelance environment. You’ll still have the opportunity to incorporate new things; you’ll just have to justify adding them. One of my freelance clients needed a multi-step, conditionally advancing form with validations on every input. I took the opportunity to learn and apply some advanced JavaScript techniques. When I presented the finished product, they were thrilled! As a bonus, my design handled inevitable last-minute changes with ease. (+1 to learning from prior experience!) Now, if my team were given a similar project and I submitted the same solution, I imagine I’d get feedback like this: And then to top it all off: Ouch. One-off approaches that would delight freelance clients will not always be sufficient in the context of an ongoing software project. Adjust your expectations of how quickly your code will get merged and deployed. Your teammates are seeing the code from different perspectives and applying lessons they’ve learned from when things went wrong. Be patient, address the issues, and submit your code for review again. Once you go through the process regularly for a few months, you’ll gain a better understanding of the needs of the system you’re working on. When you do a freelance project, you typically pull up your framework of choice, write the code, deliver it to the client, and you’re done: you’ve authored all of the code specific to that project and you know it it all. If something breaks or your client wants to do a follow up project, you can dive right back in. It’s just not possible to know it all on a project with an engineering team. Even if you do master one part of the system, it’s likely you’ll come back later only to find someone else heavily refactored it and you have to learn the new way it works. So don’t. Instead of treating the project like a classic novel set in time, treat it more like Wikipedia where anyone can (and will) edit and reorganize. Don’t approach the project as something you will read from top to bottom, understand completely, and then adjust as necessary. Instead, strive to understand the problem the project solves. Understand the major components of the system and how they interact with other systems. Then you can dive more deeply into specific portions as you work on various features. When you complete a freelance project for a client (and it went well), you are right there at the moment when they say “Fantastic, looks great! Thank you so much!” This moment is unique: you and your client have successfully bridged the gap between their needs and your talents. Ideally, you’ve learned a bit about a cause, a product, or a place and have created a piece of software meeting that world on its own terms. Your client is now confident with the solution you’ve provided and now your software is in the wild where people can use it! If you’re used to getting direct praise for your work, receiving indirect recognition can take some getting used to. Your efforts are now a much smaller portion of what’s being delivered. Aside from all of the other engineers making their contributions to the project, there are entire departments dedicated to tasks like management, sales, and support. Take a moment to step back and realize the product you’re helping create is much larger than any freelance project you could possibly take on by yourself. And more often than not, you’re delivering reusable software clients wouldn’t otherwise be able to afford as a one-off system. Despite the challenges of joining an engineering team, there’s one talent you may not already realize you have: context switching. When you’re freelancing, you’re not only an engineer: you’re project management, sales, IT support, quality assurance, and many other roles all at the same time. Being a freelancer exercises those muscles, making it easier for you to see the bigger picture. It’s also easier for you to pick up new tools: you’re often called on to write “glue code” holding several systems together. You also have a good idea for what can go wrong and how you can go about fixing it. Once you leave the freelance world, you still have those skills, but you can focus more of your energy on discussing problems with your peers rather than chasing down clients with invoices. It’s now 2:07pm. Lunch with the engineering team was delicious and my grocery shopping is already done. (Thanks, Instacart !) This morning, I merged in a pull request, fixed up another one, and reviewed two more. Just before lunch, I had time to do a deep dive on a bug report to make sure we’re not overlooking some edge cases. While it is quick, the development process isn’t rushed and we’re not making careless mistakes. When I get stuck, I know my teammates are only a Slack message away. I do miss freelancing, but only nostalgically. While freelancing gave me a great start, being a part of an engineering team is making me into a better programmer. I’m no longer distracted by the duties of running my own shop and I now have peers who hold me accountable to the technology choices I make. Better still: I’m able to contribute to something I wouldn’t have been able to have done on my own. Are you currently freelancing? Are you wanting to make a “context switch” in your career? Join us! We have engineering openings in San Francisco, Austin, and Sydney.", "date": "2015-06-11"},
{"website": "BigCommerce", "title": "Max Levchin on Entrepreneurship", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/max-levchin-on-entrepreneurship/", "abstract": "One week after Bigcommerce and PayPal announced a strategic partnership, we were fortunate to host Max Levchin in our SF office. Max was one of the original cofounders of PayPal where he served as the CTO until its acquisition by Ebay in 2002. In 2004, he helped start Yelp (he is currently Chairman of the Board) and founded Slide (acquired by Google in 2010). Max also serves on multiple company boards including: Yahoo!, Yelp, Evernote, Emerald Therapeutics and Quid. Max is currently CEO of Affirm (which he founded) and chairman of Glow. He is a thought-leader and tech luminary from his experiences as a serial entrepreneur, and investor/advisor to startups. He also hails from my alma mater, the University of Illinois at Urbana-Champaign . We were thrilled that Max accepted our invitation to speak at Bigcommerce. Tim Schulz (our Chief Product Officer) and I facilitated the chat with our team, starting with a tasting of artisanal coffees — a common obsession for those in the tech industry. The conversation was casual and spirited. Discussion topics spanned the gamut: the mission of Affirm, observations on commerce, ideation and investment for startups, and a glimpse into Max’s morning routine. We asked Max about how he comes up with ideas for new companies — he apparently has a list of ideas that is constantly growing. Max’s says he gets ideas all the time but perhaps “99.9% of them are terrible”. He first gets excited about a segment or theme. He then digs into the segment and searches rigorously for a “thread” that is ignored by conventional thinking. Max tries to determine if there is a novel (and tenable) idea that hasn’t yet been exploited by others. If these criteria are met, there is the possibility of a viable startup idea. We also discussed Max’s approach to evaluating startups for investment. He focuses 95% of his attention on the team, and asks himself “ are these people awesome ?” Max wants to invest in people that are deeply, intellectually curious — because those types are tenacious and won’t be easily discouraged by startup challenges. Max also made an interesting, nuanced distinction between “founders” and “entrepreneurs” . When asked about leadership lessons, Max pointed us to this Quora answer as his advice to budding entrepreneurs. It’s filled with great and sometimes controversial insights, like this one: Max says that he is still learning, and that he has matured as a leader from observing others. For example, he trusts team members more now than he would have earlier in his career. Max mentioned one childhood nickname and the lesson he learned from it. He also jokingly said that most of his childhood nicknames are…unprintable. Max also offered to give us espresso-making lessons, and we’ll be sure to take him up on it! (Watch the full video of our chat here )", "date": "2015-05-29"},
{"website": "BigCommerce", "title": "Plotting with Dots", "author": ["Patrick Killelea"], "link": "https://www.bigeng.io/plotting-with-dots/", "abstract": "Most system monitoring tools and load test tools output line graphs, where data values are averaged per second or per minute, and the resulting set of averages is then plotted joined with lines. In doing this averaging and connecting with lines, important information is lost , along with lots of interesting drama and eye candy. For example, here is a line graph of one-minute average ping times from the Bigcommerce office in San Francisco to www.google.com: It’s moderately informative, showing latency rising during business hours, but if we plot each individual ping time separately as a dot , we see additional range and distribution information. Note that the highest average ping time in the graph above is about 160ms, but the highest absolute time in the dot graph is close to 300ms. As you can see, plotting with dots gives you a higher density of information. Let’s try plotting a much larger data set. Here is a plot of 10 million web server response times from nginx logs, averaged per second with the averages connected by lines: Plotting the same 10 million points individually paints a more nuanced picture. We can now see that the traffic in the first half of the time range was significantly lighter, and that this is probably why there is greater variation in average response time. The horizontal streaks at the bottom are due to the limit of our web server’s response timing resolution of 1ms. Connecting the dots also obscures gaps in your data. Here is the average latency of responses during a load test: And here is the same data with every result latency plotted individually: It turns out that responses came back in clumps with gaps of silence between the clumps, but we did not see that attribute in the top graph. The curves in the bottom graph seem odd at first, but can be understood as an artifact of plotting time against time. The x-axis is the time the request started, and the y-axis is the amount of time the request took to complete. Since we are plotting on a log scale, the curves in the bottom graph are the result you get when a set of requests which started at different times all return at the same time. Here’s the result of a load test which was making requests to three different servers: From the usual line graph, you would not know that one of the three servers was far slower than the others, but it really stands out when you plot the response time points individually: Here’s a load test result of smoothly ramping up the load on a web page backed by 3 servers: The latency suddenly “breaks” at about second 160 when one of the servers spins out of control, raising overall latency. This is interesting, but not as interesting as actually seeing how the response times bifurcated at that point, which is what a dot graph of the same data shows. The one bad server tied up a lot of the load generation clients for 10 seconds at a time, resulting in lower load on the remaining two servers and good response times for them. And when load test results start to get really weird, we can use dot graphs to distinguish a trifurcation in the response times and to see long stalls before responses resume and then crash again: Network packet timeouts and retransmits are a classic performance problem made visible by plotting with points. Here is a graph of a load test against a web server with a moderately bad network connection: From the upper graph it is not at all obvious that any packets were lost and retransmitted, but as soon as we plot all the points in the same raw data, we see the characteristic horizontal bands caused by exponentially increasing TCP timeouts. The timeout bands are a fixed width apart on this log scale graph. Below is another load test shows a bump in response time during an experiment. What the line graph fails to show is the greatly restricted range of latencies during that experiment: We also don’t get a good feel for the pulsating nature of the responses during this load test until we plot with dots: Similarly, this line graph doesn’t really illustrate the smoothing of the waves in response times as the test goes on: But the dot graph of the same data does: Finally, we plot results of an extreme load test with 200 OK responses in green and 502 Bad Gateway responses in red: Informative, but the line graph hides the fact that the Bad Gateway latencies are actually continuous with OK latencies at many points: In summary, line graphs are** simple to create and understand, but they hide lots of useful information** like data range, volume, distribution, outliers, striping, gaps, etc. There are some downsides of creating dot graphs though. They require much more data storage (every data point, of course), and the standard tools such as graphite generally don’t support them. These graphs were created with G nuplot . In case you want to get started with Gnuplot on your own, here is what I used to create the second graph in this post. The data file used to generate the graph: The GnuPlot configuration file: With these two files, all you need to do is run the Gnuplot command to generate the graph:", "date": "2015-06-04"},
{"website": "BigCommerce", "title": "Down Under with Chris Fry", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/down-under-with-chris-fry/", "abstract": "I’ve worked with Chris Fry in various capacities over the past 10 years on the Engineering teams of two great companies: Salesforce.com and Twitter. I’m excited that our professional relationship continues at Bigcommerce with Chris in his advisory role. Chris is an authentic leader who understands technology, but fully embraces the human aspect of managing people. I learned a lot in the past decade working with Chris. So I was thrilled to have him visit the Bigcommerce Sydney office this week to spend time with the team. The Sydney team gathered around Chris for an informal discussion, and the San Francisco contingent joined via videoconference. The topics discussed were far-reaching: practicing agile software development, building high-performance teams, decomposing an app into services, and coordinating a geographically distributed team. I speak with Chris on a regular basis for his counsel; our conversations often gravitate towards these themes. We recorded the entire 45-minute session and posted to our YouTube channel. You can view the recording in its entirety here: Q&A with Chris Fry in Sydney . Chris has achieved more over the past 10 years than most people do in their entire careers. Chris led the Agile transformation at Salesforce.com, removed the “Fail Whale” from Twitter, and is considered an expert at leading organizations at scale. One of the many things I’ve learned from Chris is the importance of nurturing and developing teams. During our tenure together at Twitter, we established Twitter University to bring continuous education to Twitter engineers and the broader community. I plan to extend this effort at Bigcommerce; we’ll regularly host meetups and talks to foster knowledge sharing among local technologists. My affiliation with Chris is the longest and most rewarding work relationship of my career; I’m happy that it will continue at Bigcommerce. While in Sydney, Chris and I reminisced about events of the past — we accomplished a lot together over the past decade. But I’m far more excited about what the future will bring.", "date": "2015-03-02"},
{"website": "BigCommerce", "title": "Creativity in the Workplace", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/creativity-in-the-workplace/", "abstract": "We’re putting the finishing touches on our new office in San Francisco. We outgrew our old space , so we needed a larger office in which to operate. Beyond that, we wanted a space that would inspire our team to create and deliver software that changes people’s lives. Building software products is a creative process. Getting computers to do something useful is complicated. Before an engineer writes a line of code, she has many things to consider including: choice of language, libraries to reuse, interfaces to define, interactions with existing subsystems, and overall system performance. She should also consider refactoring existing code if it results in a more elegant solution. Developing software is more than just cranking out code. Thoughtfulness in technical design pays dividends for the lifespan of the software. Defining an extensible architecture provides a blueprint for future iterations of the code and how it will perform at scale. But there’s a hazard in overthinking design to the point of inaction; the best engineers need to be flexible and creative to avoid “analysis-paralysis” . Software creation is also multidisciplinary . It involves the work of Product Managers, UI/UX Designers, Software Developers, Quality Engineers, and Project Managers. In this regard, software development is similar to a construction project — think of bridges or buildings — but in the digital realm. Everyone involved needs a shared vision on the final outcome and how to achieve it. Team members should propose different ideas (and investigate multiple approaches) before deciding on a final construction plan. During the software design process, an engineer might code a proof-of-concept to determine the feasibility of an approach. This is similar to an architect’s drafts prior to a final blueprint, or an artist’s sketches before laying paint to canvas. Software done properly is akin to art. So it’s befitting that we’ve placed artwork throughout our new office in SF. We’ve adorned our walls and hallways with murals by Bay Area artists. Our design team created custom graphics for each of our conference rooms. The floor plan has multiple open spaces for team collaboration. With the new Bigcommerce office, we’ve created an environment that is colorful, professional, fun and engaging. Our new office will inspire our team in the creative endeavor of software development. Here’s a glimpse of the amazing murals in our office, as photographed by the artists: “I’ve been working all week on a fun little project for @bigcommerce ’s new office in San Francisco. Here’s a glimpse of the progress.“ - Ricky Watts ”Let’s get weird, 2015. Commission for @bigcommerce new office in #sanfrancisco“ - Ricky Watts - Cannon Dill \"Done #aarondelacruz” - Aaron De La Cruz", "date": "2015-02-16"},
{"website": "BigCommerce", "title": "Rewriting our Ruby API Client", "author": ["Patrick Edelman"], "link": "https://www.bigeng.io/rewriting-our-ruby-api-client/", "abstract": "Having recently joined Bigcommerce in San Francisco, I wanted to get up to speed quickly on the API and development process. When I was given then opportunity to work on rewriting our Ruby API Client, I jumped at the chance, knowing it would serve me well to learn the ins and outs of the API, but to also have the opportunity to publish a open source ruby gem. For me, that’s just exciting! With our API client, we try our very best to maintain Semantic Versioning. When my boss let me know that I could bump the major version, I had to pounce at the opportunity. From SemVer Given a version number MAJOR.MINOR.PATCH, increment the: The key is incompatible changes. I had the flexibility to completely change the developer facing API. With that in mind, I set off defining what I wanted to see in a rewrite. Just like how front-end developers start with designing wire frames and mockups, I started by defining how I wanted developers to use the API client. I wanted a simple to use interface that closely resembled the simplicity of ActiveRecord. I wrote a few variants of the interface, with different benefits in each, and then passed this document off internally to some of our senior developers. I found that to be a really useful exercise, they were able to give some extremely valuable advice and before we hit the code, we knew what our interface would look like. The previous version of the Gem had been unmaintained, we had been adding new features and expanding the API, and our API client did not reflect those changes. I knew when working on the rewrite, an important goal was to develop abstractions, to reduce duplication, and speed up development. Something which I really wanted to create for the new API client was robust documentation and examples. I think its important for people evaluating a library to not have doubt whether it can satisfy their needs. With that, I wrote a comprehensive test suite, full example coverage, and an extensive set of docs on using the library, and the process to maintain it. One thing which is always frustrating when opening PR’s or doing code reviews, is giving style feedback. Its important when developing a codebase to have a consistent set of patterns and style to ensure clean interfaces and consistency. For this we used rubocop. Rubocop will quickly analyze code and enforce ruby style patterns. We had to make some small tweaks, but now that it is run along with our RSpec test suite, when people open a PR, it will automatically check for style, taking that burden away from our code reviewers and allowing for more streamlined code review. One cool pattern I used was the module factory. In this pattern, you can dynamically construct a module with methods you define based on a set of conditions you can set or control externally. The problem was that some resources have limited actions that we expose on the API. We properly handle the case when a developer accidentally sends a request to one of these endpoints, but as a client library developer, I have the opportunity to help devs find this behavior out in a development environment, before dealing with production or integration settings. Lets take a look at how we can solve this with a little ruby magic. At this phase, we define a new class Actions which will inherit from Module . What this does is allow us to call Actions.new() with some params.In our case, we want to keep the API flexible so we allow a hash of arguments. We are defining the way in which our new module will be included, this is a great point to manage what methods we will dynamically include in this module. When we call base.extend(ClassMethods) , we are pulling all of the ClassMethods into the new module, then look at the options hash we used in the initialization and call remove_method if a method is defined inside the :disable_methods array. The top level API looks like the following to configure a new resource and its actions. One thing I really enjoy about using other API clients is when you are returned a nice ruby object with clean interfaces to the data. For this, we felt Hashie was a good choice. This allows us to define properties on an object, that once called .new() on, can be automatically set. This trick turned out to be quite handy when I wrote the response parser and object factory. After I complete the response status, I am able to simpley create a new object, based on which class made the request. In the API client, we support two different authentication schemes. This present a little bit of a challenge when developing a generalized authentication middleware. For the private app authentication scheme, we use a http basic authentication. We provide our developers with the ability to create usernames, and we associate an API key which is used as the password. Luckily this is build directly into Faraday, so we just had to write a simple flag to handle this case. The more complex case is using a custom middleware built to handle our OAuth flow. We expect very specific headers to get sent with the payload, something which should be very transparent to our developers. For this we can define a simple middleware which sets the headers we care about.", "date": "2015-05-28"},
{"website": "BigCommerce", "title": "OAuth Authentication and Session Management in Hapi.js 8.0 using bell and hapi-auth-cookie", "author": ["Cody Lundquist"], "link": "https://www.bigeng.io/oauth-authentication-and-session-management-in/", "abstract": "We recently started on a new project and we’ve decided to use Node.js.  After a week or so of researching which framework to use, we came to the conclusion that Hapi.js far exceeded our requirements.  In only a week we had a working prototype in place and the quality and structure of the code is something we could be proud of.  However, there were some things that didn’t have definitive documentation and we had to figure out a best practice ourselves.  One of those things was connecting bell with hapi-auth-cookie .  As it turns out, it was super simple, but not completely apparent. Hapi.js makes it extremely easy to hook up to many of the major OAuth providers (Google, Facebook, Twitter, Github, etc.) using one of the many plugins in the Hapi.js family called bell .  This plugin only does one thing and that is authenticate with the provider. It doesn’t create a long lived session for the users of your application, which is where the hapi-auth-cookie plugin comes into play.  Once bell is done authenticating with the provider, you simply pass the profile to hapi-auth-cookie and it will store that information in a HttpOnly encrypted cookie for the life of the users session. Note: This article assumes that you already have an application and are wanting to add in OAuth functionality. The first step you need to do is install the plugins: Next you need register both bell and hapi-auth-cookie as authentication strategies . Note: It is best practice and highly recommended that you store your secrets in a server environment variable rather than checking them into source control. Now all we need to do is create some routes.  One to login, another to logout, and any others that need a user logged in to use. And that is all the code you need! Bell uses the handler for the /login route as a callback which you can use to pull out the users profile and assign it to the cookie using request.auth.session.set() .  Just by doing that, it will create the users session and they will stay logged in until it’s cleared. To learn more about how to use different authentication strategies and schemes, you can checkout Hapi’s own tutorial on Authentication .", "date": "2015-02-25"},
{"website": "BigCommerce", "title": "PHP Memory Optimization", "author": ["Qifeng Zhao"], "link": "https://www.bigeng.io/php-memory-optimization/", "abstract": "Lately, I’ve been working on optimizing the memory of some of our backend PHP applications and wanted to share some of the tricks that I have come across, especially dealing with large set of data using PHP. Always cap your internal in-memory caching In day to day PHP coding, you probably have come across some simple functions that simply retrieve data from a data source and apply some business logic to transform the data into a certain structure and return that for a caller to consume. At some later point, you might find that the consumer code calls it repeatedly within a single request. In the example below, product and category have an M-1 relationship, so exporting different products with the same category results in the same category data being retrieved and processed multiple times. Obviously having repeated computation for the same data degrades performance, so in order to avoid that, we can simply cache the data in the memory, like this For most of the normal web requests, this might work quite well, given the size of data to be cached is small. However, as soon as you start using it for long running background job that handles a large amounts of data, you will likely hit your PHP memory limit quickly. To avoid such a potential issue, you can create a new cache class that cap your in-memory cache to a fixed size. In PHP, it can be something like This is a very simple and generic FIFO implementation. There are various other more efficient algorithms (LRU, MRU, LFU just to name a few), but in order to pick the best caching algorithm, you need to know locality/pattern of the data you are caching, which is not always obvious depending on the type of data you are dealing with. No matter what caching algorithm you choose, the simple fact is that you cannot hold arbitrary size data entirely in PHP memory. Use layered caching With the cache being capped to a certain size, it is unavoidable that some data are to be discarded from the cache to make room for new data, and when the discarded data is required again, you have to do the expensive query again. Using layers of caches can mitigate this issue. By inserting another external caching layer (eg. with larger space and ability to set ttl on the data, such as redis), data is written through all the layers, and the fastest cache be refilled by pulling data from the next faster layer. This setup can potentially help you avoid expensive repeat queries altogether. DB Resources Calling mysqli_free_result() once you are done with the result data, especially when dealing with long running functions when the resource is not freed automatically upon exiting function scope. In the above example, the mysqli_query() call runs the query in MySQL and transfers the entire data set from MySQL to PHP’s internal buffer, ready to be consumed by PHP. Although we have already used the DB result in formatData() , the memory holding the buffered data is not freed until the function returns, and therefore is completely wasted and not available to be used by the heavy_lifting() function. Calling mysqli_free_result() explicitly here notifies the PHP that the buffered DB result can be freed now and that memory can be reclaimed to be used for heavy_lifting() . Similar to freeing the db resource as early as possible, it can be a good idea to free variables holding large chunks of data as well (especially in a long-running loop). Now when you run the above script you get something similar to Notice the peak memory doubles for the second iteration and onward? That is because after the first iteration, $data variable always holds a large chunk of memory containing the data computed in the previous iteration while at the same time the openssl_random_pseudo_bytes() needs to allocate another large chunk of memory for its result. If you limit your PHP memory resource to 2M, you’ll get out of memory error: The unset() call tells PHP to free up the memory we know we do not need anymore, to reduce the peak memory usage. Now uncomment the unset($data) at line 7 and re-run the script, it should succeed this time, with constant memory usage. This is not a case of “memory leaking”, it simply requires too much memory to carry out the work. If the data is not upper bounded, you will definitely still hit the memory limit, and you need to find a better way to carry out the work without generating such a huge data set first. This trick is meant to be used as a quick fix in situations where it might be too complex or impossible to rewrite the data processing code.", "date": "2015-05-21"},
{"website": "BigCommerce", "title": "Counting Things at Low & High Concurrency With Redis and MySQL", "author": ["Gwilym Evans"], "link": "https://www.bigeng.io/counting-things-at-low-high-concurrency-with/", "abstract": "In the lead-up to the busy holiday season, we were actively reducing the amount of write operations that go to our MySQL databases, particularly writes which occur frequently where alternate solutions are readily available. Ultimately this would improve the reliability of our databases under the heavy loads we were expecting over Black Friday, Cyber Monday and in the lead up to Christmas itself. One particular case is that of store-level counters. Things like store visitors in a day, views of a product, et. al. Since forever, we’ve been writing things like daily visitor counts and per-product views directly and immediately to the database. This works OK for light loads, but a busy site can do a lot of needless writing as visitors and crawlers navigate the storefront. This causes unwanted disk IO and replica sync traffic on the database servers, which under extreme circumstances can cause replicas to become uncomfortably behind on their syncs. The most obvious replacement for this is Redis; an in-memory, optionally disk-backed, single-threaded, advanced key-value store. To the uninitiated, the single-threaded nature seems like a turn off, but it greatly simplifies Redis’ architecture and makes available many fast, atomic operations which can be used to safely and accurately represent the counter data we need. Redis is essentially a data structure service. There are many solutions for this dotted around the internet. Ranging from simply counting in Redis and reading that back, to cron-based syncs back into the database. None of the solutions we found fit our specific case. As usual, our case is a little more complex than most: While we don’t want to actively write the frequent hits we get to MySQL, we do eventually want to read that data back to show to the Merchant in their control panel. For unique visitors, they’re stored on a daily basis so that specific date ranges can be queried by the Merchant. Continuing this write to the database also reduced the amount of change needed in the rest of the app code since the Merchant data source is unchanged. We wanted to maintain some semblance of “real-time” updates that writing directly to the database gives. That is, for Merchants which are just starting out, they should be able to visit their storefront once and see their visitor counter go up by one, rather than waiting a day. For well-established Merchants who receive a lot of traffic, they should be able to see the visitor count grow during the day. Both of these give the Merchant a feeling that their Store is “alive” somewhat. We didn’t want to continuously fill Redis with historical data, because Redis keeps all of its data in memory at all times, meaning capacity on a single Redis node is always limited to how much memory you can give it. Keeping visitor counts from weeks, months or years ago in Redis is a waste of relatively expensive and limited RAM resources. Our current MySQL and Redis clustering setups have an inherent level of isolation between stores—for a mix of historical and security reasons—making it impractical to write all this to one central location for batch processing later (rather, in our case, the batch process would be awkward to implement and difficult to maintain). There’s no guarantee that a daily sync job would execute at the right time and within the expected time frame to accurately capture exactly 24 hours of metrics. Maybe the job fails, or starts late, or takes longer than expected, so the ‘counter’ data is skewed for a given time frame because counts kept coming in without getting synced back. All this means we needed some sort of solution which supports precise daily buckets, doesn’t suffer from concurrency issues (like two concurrent requests resulting in a double-sync of the same counts), can reflect updates more frequently than every 24 hours, doesn’t leave data behind in Redis over time, works within store-level isolation and is able to recover from faults or delays. The general solution we found using Redis was: Write counters to a hash for each store, using the date as the hash key (e.g. 20140920) and the count as the value. Specifically, using the HINCRBY command to atomically increment by 1. Use the return value from HINCRBY (since it returns the new value) to trigger a “sync every x hits” behaviour. Set a second, expiring lock key ( SETEX ) whenever a sync happens. Use that expiring lock key to power a “sync every y seconds” behaviour. Specifically, use SETNX , which will only work if the key did not exist, and we use that fact to trigger a sync, which will SETEX the key to make it expire since SETNX does not set an expiration. Sync by renaming the hash to a random name before processing it to isolate it from new data. Use HSCAN to sync all fields of the hash, which will pick up all data regardless of the date of entry. The rename can fail if another process caused a sync concurrently, in which case we do nothing because there’s no data. The rename can also fail in the unlikely event that we collide with another random rename, in which case we try the rename again until it works. For each date in the renamed hash, merge the numbers into the database (e.g., using INSERT ON DUPLICATE UPDATE in MySQL) The lock key, the hash and the rename are the real tricks to this, in tandem they give us correctness and consistency in both low and high frequency situations. At Bigcommerce, we have two separate clusters of Redis servers; one completely volatile for cache usage, one disk-backed “persistent” redis for data we care about or can’t rebuild. Both are still limited by Redis core limitations such as always-in-memory datasets, but they are managed and accessed differently because of their different uses. In this case, we are using the disk-backed Redis. In terms of Redis commands, it looks something like the following, where our (adjustable) hit threshold is 50 hits and our time threshold is 60 seconds ( for example’s sake). Note: An “X” on the left indicates a “visit” to the public-facing app, where multiple commands may be sent to redis during the handling of this visit. Note: The SETNX / SETEX could be done via a single EVAL , but is shown in expanded form. First visitor ever (or first visitor of the day, of the hour, of the minute, etc.) A relatively busy store Hits across day boundaries get synced eventually A store with no hits on a given day still works P.S. Since first writing this, we found the SET command supports flags that could simplify this, but I’m leaving these examples as-is.\nSo this worked for our case of unique, daily visitors. What about total per-product views? The same solution works , we just replace the day stamp with a product ID and it all fits in! All the renaming and syncing works just the same.", "date": "2015-01-19"},
{"website": "BigCommerce", "title": "How we use Elasticsearch", "author": ["Pav Zwierzynski"], "link": "https://www.bigeng.io/how-we-use-elasticsearch/", "abstract": "At Bigcommerce, we’ve recently migrated our search infrastructure from MySQL full-text indices to Elasticsearch . Running searches using MySQL full-text indices would take a long time on large stores and occasionally cause web requests to time out. Because full-text indices aren’t scalable, we saw instances where searches on stores with 1M products could take 20+ seconds. The queries also put enormous load on our database cluster. In addition, we found that the accuracy of our MySQL full-text searches were not consistent and users would often see irrelevant search results. We chose not to use Elasticsearch as the primary source of truth for our data for several reasons— it is possible for Elasticsearch to lose writes, it doesn’t support transactions, and updates to indexed documents are not always available immediately. To keep MySQL and Elasticsearch in sync we implemented an observer pattern. The indexing code subscribes to create, update and delete events for all entities that we want to push to Elasticsearch (products, variants, categories, brands) and triggers an async jobs to index the data. This pattern allows us to completely decouple the indexing code from our database models. The async job is simply a Resque job running the code responsible for indexing. When a customer searches a store’s catalog, our Elasticsearch cluster is queried for matching product IDs. These IDs are then used to retrieve full records from MySQL, our primary source of truth. MySQL may not be the best for full-text searching but it works very well when it comes to retrieving records by their primary keys. Combining Elasticsearch with MySQL for search is a good compromise for getting relevant, fast and accurate results. Since we were not using Elasticsearch as the primary source of truth, we decided only to store properties that would be useful for searches. In Elasticsearch every shard is refreshed automatically once every second which means that it supports near real-time accuracy. Document changes are not visible to search immediately, but will become visible within one second. Making schema changes in Elasticsearch can be problematic— data is not reindexed or analyzed when the when the schema is changed. You have to first change the mapping (Elasticsearch DDL) and then reindex the documents belonging to that mapping. In some cases, it’s not even possible to change the mapping of a field (e.g., when changing a flat object into a nested object). We needed the ability to change schema and reindex data without any downtime in case we wanted to make modifications to the index. When making incompatible schema changes, you have two options: delete the current index and reindex it or create a new index and reindex into it. This process is relatively fast when you only have a few hundred records, but if you’re dealing with hundred of thousands of records it can take hours, during which search would be missing data. To avoid this, we decided to reindex the data into a secondary index with the new schema while reading the data from the primary index with the old schema. We also have to deal with any modifications that happen during the reindexing process. Any changes made to records after they have been already reindexed would not be reflected in the new index since we’re still using the old index for all CRUD operations. To avoid that, we decided to dual write the data into two indices simultaneously during reindexing to ensure that both indices have the correct data while still reading from the primary one. Once the reindexing process has been completed we need to point the codebase to the secondary index. Since there is no command to rename an index in the Elasticsearch API, the recommended way of renaming an index is to create an alias. An alias sits on top of an index and you point your requests to the alias instead of to the index directly. This gives you an extra layer of abstraction with the flexibility of quickly renaming your index on the fly. After the index schema has been changed and the reindexing process has has been completed, we simply change the alias to point to the new index by making an HTTP request to delete the old alias and create the new alias. This allows us to change which index the codebase is using without suffering any downtime. Cluster: 480 shards, 4 masters, 6 data nodes across two DCs Size of our cluster: 500GB 50/50 split MySQL vs Elasticsearch, MySQL was having big trouble when there was a spike whereas Elasticsearch was able to handle it without any problems. Historical data of MySQL performance, very volatile, whereas Elasticsearch is stable and nearly constant.", "date": "2015-04-30"},
{"website": "BigCommerce", "title": "TechWomen Tunisia Delegation Trip", "author": ["Meg Desko"], "link": "https://www.bigeng.io/techwomen-tunisia-delegation-trip/", "abstract": "Our group visiting the awesome [in the truest sense of the word] Bardo National Museum. [Photo credit: Erin Wilkinson] In mid-March, I was a member of the TechWomen delegation* to Tunisia. Tunisia is a beautiful country on the Mediterranean nestled between Algeria and Libya; a small piece of the Sahara Desert is in the South and the Northern part is greener than I expected with a temperate climate and lovely beaches. Nearly everyone I met speaks at least 2 languages (Arabic and French) and the Tunisian dialect of Arabic, with many of them fluent or conversant in English. More than 60% of Tunisians have college degrees, and gender equity in STEM fields is much greater in Tunisia than in the U.S. Why yes, I am standing in the Roman Baths in Carthage. [Photo credit: Katy Dickinson] We dipped our toes into the history of Tunisia on the first day of our trip. The ancient city of Carthage (or, rather, the ruins thereof) is located a short drive from the capital city, Tunis. [Side note: when taking photos at Carthage, one must be careful not to point their camera toward the nearby presidential palace.] Our group visited parts of Carthage, spending a good bit of time wandering through the public baths and taking photos. We walked on and were awed by 3000+ year old mosaics and learned about the Punic, Phoenician, and Roman people who had called Tunisia home at the Bardo National Museum. There was much more to see, but we had to move on to walk through the Medina (old city), which is full of beautiful, colorful doors. Our main purpose was meeting Tunisians and having exchanges with them, scientific, technical, and cultural, and keeping ties with other program members strong. On our first day, our group visited a training session for the Women’s Enterprise for Sustainability program. We learned about the centers they have in locations throughout Tunisia and the ways they work with female entrepreneurs in their locales. Many of the women who use the centers are artisans, so members of our group hosted discussions about eCommerce , social media marketing, selling on Facebook, mentoring programs, and how to encourage innovation. It was fun to hear the insightful questions asked by the participants and to listen to the rapid interpretation between English, French, Tunisian, and Arabic. We also had an opportunity to purchase some crafts made by women entrepreneurs through the centers. The second day of our program was science day! We visited the Institut Pasteur de Tunis where one of our TechWomen Fellows is a bioinformatician. The director gave us a high level overview of the research programs at the Pasteur. We had three “keynotes” from members of our group. Katy talked about the impact of women on computer science. Larissa talked about Open Science , specifically what members of Mozilla are doing in that area. My keynote was titled “Experimenting with Social Media for Scientists,” and touched on different ways scientific researchers can leverage social media (Twitter! Facebook! ResearchGate !) to further their careers, network, and gain access to resources. During our coffee break [cultural note: Tunisian events often feature coffee breaks that feature coffee, incredible pastries, sweetened mint tea, and strawberry juice. Highly recommended.], we networked with scientists from throughout Africa who were at the Pasteur for a bioinformatics course run by Amel, a Tunisian TechWomen Fellow. We then had several bioinformatics talks from scientists at the Institute discussing the genetic makeup of Tunisian people, diagnosing genetic diseases in resource-poor areas, and vaccine development in Tunisia. Me presenting “Experimenting with Social Media for Scientists” at the Institut Pasteur de Tunis[Photo credit: Jill Finlayson] After delicious lunch (tagine! harissa!) at Hotel Africa, we headed to a Green Tech event hosted by Olfa, another TechWomen EL, and her Maya Organization . We learned about TEARN , a resource for teachers and heard business plan pitches for green tech companies from young Tunisians. The the event wrapped up with a panel of TechWomen mentors answering questions about green tech, renewable energy, and how to pursue your career dreams in the face of family and societal expectations. On the third day of the program, our group visited the Esprit Innovation Hub in Tunis to spend the day with high school girls interested in technology who were interested enough to join us during their school vacation week. Among the participants were two TechGirls participants and two soon-to-be TechGirls. Ines, one of last year’s Tech Girls, planned to demo her award-winning Android app. She was having difficulty with her development environment [Eclipse], was not working correctly [software engineers will not be surprised about this], so I was asked to help out. Ines, “my” TechWomen Fellow from 2014, Racha, and I spent a good bit of the next hour or so debugging Java to get her app working, succeeding right before lunch. During that time, Sabine, a TechWomen Fellow from Lebanon ran workshops where the participants built cars, turned wood, and built circuits using kits she’d developed for her company Kids Genius to get young people excited about science and engineering. Eclipse, please behave! Ines, Racha, and I scheme to get the app working. [Photo credit: Katy Dickinson] While we were enjoying a traditional Tunisian lunch with the girls and their teachers, our program leaders learned about the tragedy at the Bardo Museum and we returned to our hotel. Many of us spent time in the lobby between meetings to discuss the program, the remainder of which was cancelled. We also got to see Ines’s app, and took the “quiz” as a group, which was a bright spot in a confusing afternoon. Me, demoing Ines’s very cool Android app. [Photo credit: Katy Dickinson] Several members of our delegation were scheduled to speak at Tunisia Digital Day on Thursday, a program put together by Ines Nasri, a Tunisian TechWomen Fellow from 2014. While our group did not attend in person, I recorded a video message for participants and am incredibly happy to hear of the event’s success. Members of our group still in Tunisia met on Thursday to play with motherboards [a workshop that was scheduled for Wednesday afternoon] and build cars using Sabine’s Kid Genius materials. We heard from the TechWomen fellows how the TechWomen program affected their lives and careers. Some of us rode camels on the beach outside the hotel :-) Riding a camel by the Mediterranean. Photo credit: Jill Finlayson Even though our delegation program was cancelled partway through our trip, I feel our trip was successful. Our group discussed science and technology with Tunisian women and girls. We experienced Tunisian hospitality through the amazing Tunisian Emerging leaders [Thank you, Merci, Shukran], our wonderful tour guides , and other people we interacted with. We visited an incredible museum and saw the city filled with flags for Independence day. Yes, something terrible happened while we were in Tunisia, but the people who committed those crimes are not Tunisia. Tunisia is app developers and green entrepreneurs. It is scientists who are looking for the genetic basis of unusual diseases found in the South of the country. It is people who want to share their cultural heritage with visitors and girls who spend their break from school learning. Tunisia is small business owners learning how to market their goods and people running centers to help them. This small taste of Tunisia left me wanting more; to see more, to meet more lovely people, to eat more :-). It also reminded me why TechWomen and exchange programs like it are so incredibly important. These taxpayer funded programs bring citizens from different countries and cultures together to learn about and from one another, to come together as human beings. They let us see how other people live, work, and play and to get a new perspective on life and the world we share. They let us understand Tunisia in a way that news reports about the Jasmine revolution and the Bardo Museum attack never could: through the eyes of people who live there. Tunisia, we’ll be back. [Photo credit: Erin Wilkinson] * TechWomen is a U.S. State Department exchange program that brings women from the Middle East, North Africa, Sub-Saharan Africa, and Central Asia to the San Francisco Bay Area for an internship at a science or technology company and immersion in the culture of Silicon Valley. During the program, the Emerging Leaders have two mentors: a professional mentor at their host company and a cultural mentor to do fun things with. The TechWomen program takes place in the fall; the following spring women from the bay area visit two of the countries participating in the program as a delegation. Originally posted on my blog, megdesko.com , home of an infrequently updated blog.", "date": "2015-04-03"},
{"website": "BigCommerce", "title": "Elasticsearch meetup at BC Sydney office", "author": ["Pav Zwierzynski"], "link": "https://www.bigeng.io/elasticsearch-meetup-at-bc-sydney-office/", "abstract": "Last week we had the pleasure of hosting an Elasticsearch meetup at our Sydney office. Elasticsearch is a powerful open source search engine built on Lucene that provides scalable and high performance full text searching and analytics. We use Elasticsearch to power the search of all of our stores so when we were offered to host the event we jumped on the opportunity. The meetup attracted interesting speakers and over 50 guests. The first speaker was Leslie Hawthorn , Director of Developer Relations for Elasticsearch Inc. Leslie leads Elasticsearch’s community outreach efforts from their Amsterdam EU HQ. Her talk consisted of all matters related to the ELK stack (Elasticsearch, Logstash and Kibana) and its community. The second speaker was the CMO and community manager at Found , Cecilie Myhre . Cecile talked about Elasticsearch-as-a-Service and the hurdles of running Elasticsearch clusters. The third and fourth speakers were BC engineers: myself ( Pav Zwierzynski ) and Ray Ward . My talk was about why Bigcommerce switched from MySQL to Elasticsearch and the path that led us to a successful migration. Ray’s talk covered managing relational data and aggregations and how they can be leveraged to create faceted search . Keep an eye on more meetups hosted by Bigcommerce and drop us a line if you want to find out more about engineering at BC . This week, we’re hosting Douglas Crockford , the creator of JSON, at our San Francisco Office. PS. Ray and I will be writing blog posts about our talks so make sure to check back in a little while!", "date": "2015-02-03"},
{"website": "BigCommerce", "title": "What keeps us Go'ing", "author": ["Subramanyam Chitti"], "link": "https://www.bigeng.io/what-keeps-us-going/", "abstract": "A while ago I had to opportunity to give a lightning talk at the Sydney Go meetup. Go is increasingly being used at Bigcommerce, especially within the Data team, and I'd like to outline with some examples why we like and use Go. First, check out the slides , and then I will expand on some of the more interesting points. We've been using Go at Bigcommerce for over an year now. We have built an event collector, that acts as an API endpoint to which anyone can send event data to. This system accepts, validates and stores events, and currently processes around 8 billion events a month, with close to 100% uptime. Go is the first language we consider for all new data projects. Here's why. Go has goroutines and channels that make  building concurrent programs simple. Goroutines are like lightweight threads, and channels are typed queues that allow allow different parts of the program to communicate. We create a (fixed size) channel by: and the producers can enqueue messages on the channel using: To complete the picture, multiple goroutines can read from the same channel by: And so, we have just broken down our data processing pipeline into multiple stages, each of which can be parallelised independently, all without using error prone concurrency primitives like locking, mutexes or having to declare atomic/volatile types. JSON is the lingua franca of the modern web, and Go has very good support for it. Consider: where we define a type, and how it should be unmarshalled from a JSON string. When you want to express the idea that the userPayload is an expected part of various request types, Struct Embedding allows us to easily do so: and we don't need to repeat any of the marshalling/unmarshalling code. We also note that creating web servers in Go is similarly simple and there exist thousands of examples on the internet already. Go's testing support makes it very clear that the language designers didn't just add support for testing as an afterthought, but designed for it from the beginning Prepare to be blown away by the CPU and memory utilization of your typical go program compared to say, an equivalent python or ruby program. A typical go project compilation takes 2-5 seconds. We're talking thousands of LoC. And even though Go is garbage collected, it doesn't feel like that coming from Java - a typical Go program matches Java in performance while taking way less memory and way fewer GC pauses. Since Go programs compile down to a static binary, deployment is usually some version of \"scp, symlink; run\". Painless deploys, especially compared to Python, Ruby or JS, will make your ops team love you.  In most startups, the ops team is basically just you, so this is personal at least some of the time. The tooling thats included with Go makes it easier to write clear, readable, and correct code - For backend code, Go will allow you to put working solutions in front of your customers faster than any other language I've used. Its also an extremely simple language to pick up - any developer should be able to become productive in go within a month.", "date": "2015-07-29"},
{"website": "BigCommerce", "title": "Recovering Redis Data with GDB", "author": ["David Basden"], "link": "https://www.bigeng.io/recovering-redis-data-with-gdb/", "abstract": "We recently had an incident where we had a Redis instance blocked on writing to disk, hung inside the kernel. Through a variety of other circumstances this meant the only up to date copy of business critical data was in memory on a single machine, multiple independent replicas and backups were unavailable, and that machine could crash at any moment. The problem was eventually solved without data loss of any kind due to our Techops team being utterly brilliant, but during the “incident” a number of ideas that would otherwise be dismissed as absolutely crazy were discussed. One of the ideas was based on the simple idea that redis was an in-memory database. We had access to the machine. And the repeated question came up of just copying all the memory off of the machine, or at least out of the process, and wrest our data from it’s grasp. Although mostly not serious, at least to start with, as options lessened the idea of taking a core-dump and picking it apart looked more and more like a straw worth clutching at. It ended up not being needed, which is a good thing, but I figured out that it was at least possible, if you are really really really desperate, and have some time on your hands. On the off-chance anyone is really that desperate in the future, I thought I’d at least list out what direction you would need to take, and wish you the best of luck. (Past this point I’m going to assume an exceptional knowledge of C, a good knowledge of GDB, ELF loading, symbol lookup, the amd64 ABI used by Linux, memory layout and generally how the dynamic linker works. If interested check out the osdev.org wiki, especially on linking, ELF loaders etc and http://www.x86-64.org/documentation/abi.pdf – Or just fake it and read on) Given you’re at point of desperation, you’re probably going to be limited in either time or what you can do.  At the very least you are going to want to get a core dump of the redis process, taking into account that it’s going to have to write to somewhere and if you can’t just do a redis SAVE, then there is a chance that you can’t write to disk. If you have to write to /tmp using tmpfs, realise that you might run out of memory at some point when writing it, and although the OOM killer probably won’t kick in, be very very sure first.  You might be able to use the remote gdb stub/server to dump out the core over the network rather than locally in a pinch. You can get gdb to attach to a process, dump out a core file using generate-core-file and then detach from the process and continue executing: Other things you want to get at this point if you can.  Some of these might not be possible, or might not be possible to get off: Get this off of the server ASAP and somewhere you can work on it. At the time of writing, gdb had a couple of really big frustrating limitations and/or bugs when writing corefiles out. The biggest is that although at the time of dumping it was quite happy loading in relative symbols from the binaries and libraries, and then translating them to absolute addresses from where the ELF loader had put the individual sections at runtime, the corefile had neither the symbol information, or the section load information (or at least not in a way that gdb could read it itself afterwards) If we load the corefile we just dumped: gdb no longer has any idea about what is going on. If the core was dumped with a recent version of gdb, you might get some success with running gdb like this: gdb /usr/bin/redis -c <corefile> But, this wasn’t working on the version shipping with Debian wheezy. The reason is that gdb just wasn’t saving the section mappings, or annotating them in full in the coredump. Here is the difference between a recent gdb coredump, and the coredump I got from production in the same method: If you can, use a very recent version of gdb. If you can’t, you’re going to need to: If you don’t have the /proc/($pid)/maps file, you might have to use readelf/objdump or something to disassemble the original binary, disassemble part of your coredump’s text section, and find some code blocks that match. This isn’t quite as bad as it looks, but almost.  If you have a chance to use gdb more on the original machine running redis, dump out the symbol location of a well-known function inside redis-server, and use that to figure out the .text section base. e.g.: Then, you should be able to look up the same symbol in the redis-server binary: The difference of which will give you the .text section start. You still have to rewrite symbol table, or otherwise tell gdb where the .text section for the binary is (although there is an option for gdb to do that when loading symbols, it didn’t actually work for me as documented. gdb is a twisty maze of mostly working code) Another option is to use the maint print symbol <filename> and other maint print *symbol commands to dump out gdb’s internal state while attached to the original process. It’s in a human, not machine readable format, but will give you the information. This is probably an hour or two’s work, just to start being able to read the corefile working around gdb’s eccentricities. This is going to be the case with almost any version of gdb, made more fun by the fact that the amd64 arch code isn’t quite as mature as the x86 code. Now we just need to get the source code for that exact version of redis. If we’re using debian, this is pretty easy if you have the package and version name; Just use apt-get source . You will probably find it much easier past this point to be running the same development environment that the package was built with, but you will absolutely need the same version of GCC; Older versions of GCC will pack memory differently and optimise differently. You will almost certainly need the same versions of libc, libjemalloc, libpthread etc as were running on the original server. If you’re really, really lucky, you should at this point be able to build redis-server and produce close to an identical binary as was running on the server (There is actually a reproducable-build patch in debian’s redis package which I suspect helps with this as well). Moreover, you should be able to have access to both the coredump, and the underlying source code and datastructures. So at this point, hopefully we have a gdb at a point that it not only has read the corefile, but also knows where the symbols are in memory. This is absolutely brilliant, because redis.c has the most awesome global variable ever: Because it’s declared as a global variable, it means that the linker has put it in the .bss segment so the ELF loader will zero out the memory for it at load time. It also means that it has it’s own symbol table entry, so even from the core dump we know where all our redis server state is. At this point though, gdb doesn’t know the structure of the data there: What we need to do is rebuild redis-server from the same version, and then use the binary without the debugging data  stripped out to let gdb figure out where the source code is, and from there gdb will be able to go through the redis source and figure out how to introspect the datastructures. I just used dpkg-buildpackage in the unpacked debian source for the same version as the redis-server I took a coredump from, and then went into the ‘src/’ directory where the built binaries and source code were sitting.  Then we need to get gdb to load in the debugging data from the unstripped redis-server binary without disregarding the existing symbol information etc from the coredump. This is possible, but gdb can’t figure out on it’s own where the .text section was loaded to in the process we coredumped.  We can introspect gdb’s state again and get out the .text base address, and then just tell gdb when loading in the new redis-server binary to offset the .text section by that amount. “info files” will show where gdb has loaded in each file so far, and where it has mapped each file section to. In this case, the .text section of /usr/bin/redis-server was loaded in the original process at a base of 0x00007f3aa2ae5f50 We can pass this to add-symbol-file, load the recently build version of redis-server, and if we’re in the same directory then gdb will also load in all the source code. At this point, we can actually start looking at what was going on inside redis at the time of the core dump: struct redisServer is defined in redis.h, but what we’re looking at is the redis data itself. database 0 is located at server.db[0] as a struct redisDb . The main hashtable for the db is in serverdb[0].ht, with db.h and db.c defining the hash table structure, operations, and a light API around it. This is about as far as I got, but it was enough to start introspecting the data in a structured way. Getting past here you have wonderful options such as: < p>The second is probably the best option. The third would be pretty cool, but the details would start being really annoying (like having to rewrite all the pointers in the entire db if you can’t map in the data into exactly the same addresses)", "date": "2015-05-14"},
{"website": "BigCommerce", "title": "Big Growth in Austin", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/big-growth-in-austin/", "abstract": "Four months after our first acquisition , I’m thrilled to announce that Bigcommerce is expanding to Downtown Austin! We were looking for an office befitting a startup , with the charm and character that would foster creative work . Our desired location would be nearby other tech startups and in the heart of downtown Austin. The ideal space would have an open seating plan, and have an array of windows for ample natural lighting. And if we could wish for one more thing, it would be a location of significance in the Austin technology landscape. Our search began not far from Capital Factory, the startup incubator. A friend and former colleague of mine works at Mass Relevance (which merged with Spredfast in 2014). She tipped me off to a suite that the combined company no longer needed; it was the office where Mass Relevance began its ascent. In April 2015, a small group of us took a tour of the space at 8th and Brazos. I fell in love with the space and recognized its potential. Four months later our lease was signed, and I finally posted a video of our office tour : We’re so excited about our downtown expansion, and we couldn’t have wished for a better office! We took occupancy this week and opened the office in celebratory fashion — with cocktails, of course! Our downtown office can be considered a satellite to our larger HQ near Lake Travis. But it’s also a testament of how we’re building a unified team across all of our offices in Austin, San Francisco, and Sydney. Just as we’ve done in our other Engineering offices, we plan to give back to the community by hosting meetups on a regular basis. We want to engage and inspire technologists in the Austin area. In the Information Age, nothing is more important than the cultivation of ideas. Our intent is to be a thought leader and a positive influence for the Austin tech community. To all our engineers working in Austin: dust off your boots and hang up your hat. We’ve got work to do! There are a few competitors in our space , but I know we’ll rise to the occasion. And remember to: Think Big , every day (this is Texas, after all).", "date": "2015-09-09"},
{"website": "BigCommerce", "title": "Advanced Apache Spark Meetup @ Bigcommerce", "author": ["Deepak Srinivasan"], "link": "https://www.bigeng.io/advanced-apache-spark-meetup-bigcommerce/", "abstract": "Past Thursday, we at Bigcommerce hosted our first Advanced Apache Spark meetup from our SF office on the topic \"How Spark Beat Hadoop @ 100 TB Sort\" . Thanks to Chris for coming up with such great topics with emphasis on deep dives into Spark internals. We had a great turnout and looking forward to host many such great meetups. The premise of the topic \"Sorting in Spark\" is explained in the paper by Reynold and team - http://sortbenchmark.org/ApacheSpark2014.pdf Theme of the talk: Excerpts from the meetup: Key metric measured: Throughput of sorting 100TB of 100-byte data, 10-byte key Total time includes launching app and writing output file. Resources: Generated 500TB of disk I/O, 200TB network I/O on a commercially available hardware. Winning Results: Key takeaway: Spark sorted the same data 3X faster using 10X fewer machines . All the sorting took place on disk (HDFS), without using Spark’s in-memory cache. PB time beats previously reported results based on Hadoop MapReduce (16 hours on 3800 machines) Slides 23 through 46: Shuffle overview, configuration attributes, operation pitfalls, {key, pointer} shuffle-sort tip, and Spark CPU & Memory optimization principles are worth noting. By winning Daytona GraySort contest, Spark further substantiated the reason for its widespread adoption as the successor to Hadoop MapReduce. References: Slides - http://www.slideshare.net/cfregly/advanced-apache-spark-meetup-how-spark-beat-hadoop-100-tb-daytona-graysort-challenge Stay tuned for more meetups hosted by Bigcommerce. If you are interested in joining Data Engineering team at Bigcommerce then please review Lead Data Engineer profile or our careers page for all other open positions.", "date": "2015-09-08"},
{"website": "BigCommerce", "title": "Web Accessibility", "author": ["Oliver Ng"], "link": "https://www.bigeng.io/web-accessibility/", "abstract": "What comes to mind when you think of a disability?  For many of us, the image of a disabled person parking placard with a wheelchair comes to mind.  In reality, disabilities cover more than a person with an externally obvious physical impairment.  Hearing loss, disorders of fine motor skills and colorblindness are all examples of conditions that are not obvious when you meet someone on the street.  These affect the way people interact with the world, and many everyday objects are made so that everyone can use them normally.  This is also true of the web. Accessibility is the design of products, services or environments for people with disabilities.  As of the last United States census in 2010, it is estimated that 56.6 million people in the country have some form of disability.  This means that accessible products and services are critical to nearly 1 in 5 people in this country.  Wheelchair ramps, closed captioning, glasses and even the positioning of traffic lights are examples of objects and services that provide an accessible environment. As everyday life becomes increasingly intertwined with the web, it is also increasingly important that accessibility is extended to services provided online.  Everyone uses the internet, and not just via the traditional keyboard/mouse/monitor setup either.  Our phones, televisions and even refrigerators feature internet connectivity. We, the developers of services on the internet, cannot ignore any segment of the population.  Case in point: I had a colleague who had adopted the iPhone years before I did, and he just happens to be blind.  Not your traditional target audience, but one that exists, and one that we must acknowledge.  Developing for the greatest audience should be done altruistically in the name of equal access, but many of us work for corporations, so company revenue is also a consideration.  Therefore, we must recognize the potential spending power of a population that is easily in 9 figures or more across the globe. Developing for web accessibility has been facilitated by the standards organizations that have developed HTML5 and the browser vendors that have implemented those standards.  Modern semantic markup have accessibility features built into them, and writing code with accessibility in mind takes little or no extra effort if the code is structured well and your content placed correctly. In addition to impacting your company’s bottom line, a good faith effort to be accessible also ensures that you are in line with the law.  A well known accessibility lawsuit was brought against Target Corporation several years ago, resulting in a multi-million dollar settlement on behalf of the plaintiffs.  Money lost to legal fees and damages would have been much better spent on development expenses to add accessibility, which would have resulted in additional sales. At Bigcommerce, we recognize the impact that an accessible site has on our customer, the merchants, as well as their customers.  We are taking concrete steps towards building a more accessible site at multiple steps in the development cycle.  Design proposals are reviewed so that visuals meet at least WCAG AA requirements.  During development, engineers are encouraged to use alternate methods of navigation (keyboard only, screen readers) during testing, and code is reviewed so that markup is semantic and code provides enough data for these alternate experiences.  Finally, our quality engineers will test for accessibility concerns, and ensure that the final product can be used by an expanded audience. As these software improvements are made, we can partner with merchants who have accessibility needs, and they in turn can sell to customers who do not access the internet in a traditional way.  The audience is expanded, and everyone wins.", "date": "2015-04-20"},
{"website": "BigCommerce", "title": "SFPHP Meetup at Bigcommerce", "author": ["Brian Fenton"], "link": "https://www.bigeng.io/sfphp-meetup-at-bigcommerce/", "abstract": "Last week Bigcommerce hosted the SF PHP User Group in our San Francisco office, where around 40 hungry developers gathered to hear me talk about Unit Testing Done Right . This talk covered some of the historical context around unit testing, and how we got from the historical roots of unit testing to where we are today. I content that what Kent Beck was talking about in his Test-Driven Development By Example book was really more akin to SpecBDD, but that term didn’t exist at the time. We also covered some of the main causes of flaky tests, and strategies to deal with each: Furthermore, we talked about test coverage metrics, and how line coverage (the most common kind) is not as thorough or useful as we had been led to believe. Finally, we covered some characteristics of good unit tests, along with some testing smells and what they mean. We love hosting meetup groups at the Bigcommerce offices— if you know of any awesome software or technology meetups in SF, Syndey, or Austin, please get in touch with us!", "date": "2015-08-31"},
{"website": "BigCommerce", "title": "GoSF Meetup at Bigcommerce", "author": ["Steve Corona"], "link": "https://www.bigeng.io/gosf-meetup-at-bigcommerce/", "abstract": "Last night, we had the pleasure of hosting the GoSF meetup in our San Francisco office. The attendance was great (over 100 people) and we were able to listen to three awesome speakers. By the way, we love hosting tech meetups in our offices. If you know of any meetup that’s looking for a host (with good beer) in San Francisco, Sydney, or Austin, reach out to us on Twitter ! The first speaker was Jonathan Boulle , a technical product lead at the excellent CoreOS (probably one of the coolest and game changing pieces of OSS software). Jon showcased the multitude of different RPC options available to use with Go, covering everything from REST to Protocol Buffers to automatic client generation. I’m really excited about the the RPC space heating up, especially the automatic client generation— no one wants to write a million boilerplate clients. Jon also talked about a new, next-generation, RPC system from Google, called gRPC . gRPC builds on Protocol Buffers 3.0, by adding an HTTP/2 RPC framework with auto-generation of clients in Golang, C++, Ruby, PHP, Javascript, Java, and more. I’m really looking forward to seeing gRPC pick up momentum. Even cooler, there’s a new OSS project called grpc-gateway that acts as a reverse proxy for your public JSON API to translate calls to your gRPC services— I can’t wait to try it out. The next speaker was Alex Toombs , a software engineer from Apcera. Alex talked about various different ways to handle identity management, without having to become an identity provider. For instance, Apcera has no \"users\" table in their application— authentication is handled 100% by external providers. The other interesting bit that Alex talked about was how they used cgo to integrate Golang with Kerberos — pretty neat stuff, and I highly recommend watching the video for this part. I haven’t had a chance to play with cgo yet, but being able to work with C libraries seems incredibly useful, especially when extending existing libraries. The last speaker of the night was Sarah Adams — her talk was about experimenting with automatic API doc generation in Go. I really enjoyed Sarah’s talk because it was about a problem that all developers face at one-point or another… keeping outdated documentation up to date. Sarah showcased different attempts that ultimately didn’t allow her to accomplish her goal of completely hands-off API documentation— off the shelf tools ended up consuming hours of her time per week. That is, until she invented her own documentation tool called test2doc . Test2doc is a really clever piece of code that injects itself as the HTTPServer for your tests, captures the request/response, and uses that information (along with comments in the tests) to build self-generating API documentation. Amazing! Watch the full video of the meetup below:", "date": "2015-06-18"},
{"website": "BigCommerce", "title": "On the Usefulness (Or Lack Thereof) of Foreign Keys", "author": ["Tim Ellis"], "link": "https://www.bigeng.io/on-the-usefulness-or-lack-thereof-of-foreign-keys/", "abstract": "I recently spoke with Sebastian Machuca (a Sydney-based engineer) about the concept of having FK constraints in Dev and Staging environments, but disabled in Production. Here is the ensuing dialogue, edited for brevity and for making-me-look-like-a-genius. Our dialogue was good, but we were unable to come up with a concrete list of problems that the database will incur when FK constraints are removed from production databases. I found some strongly-worded opinions on the interwebs about FKs, but nothing that had precise technical backing. For example, there is a Stack Overflow Q&A that both Sebastian and I found that contains this assertion: Everything I know about statistics-based optimizers tells me this statement is false. Does anyone have a reference that talks about this in more (technical) detail? S: Why do you say that is it wise to drop foreign keys in a production environment? Do not most advise to have them? I would personally consider an anti-pattern to not have enabled the foreign keys. T: Well, this is a subtle point. So let's say you create some FK relationships. And you write some code that violates them. What happens? S: Breaks all the things. 500 error. T: So then you fix some bugs... less 500 errors... you fix more... And finally it doesn't break. At that point, the code is correct, yes? S: Yes. T: And so if I leave the FK constraints in place, they really do nothing [ed: at this point, we've not recalled the cascading delete feature of FKs] But FK constraints use a lot of DB resource, for enforcing business rules that have already been enforced by the (now-fixed) code! S: But concurrency is a key factor in here. When you are in development or staging, you have very little concurrency. Also, Referential Integrity should be handled at the lowest possible level, which would be the underlying database. Relational Database Management Systems are optimized to handle this. T: Well. Referential integrity can be relegated to the database, and it has the tools (Foreign Keys) to do it, but it's not necessary. Recall that as your app gets errors that you fix, you are encoding the RI into the application. So once your application doesn't have errors anymore (assuming you got all possible errors) then your RI is perfectly encoded in the applicaiton. So the THEORY is, once the app is debugged, then the RI is fully in the app, and FK is completely redundant. S: But even if your code is “perfect”, that doesn’t mean it's perfect for high concurrency. T: I think you're saying that if the app tries to keep two tables in sync without using a transaction, that a FK would save the application from error? But let's look at the example. We have a parent table with people==(id, name, address_id) and child table addresses==(id, address) . We want to delete this person. FK will cascade the delete and ensure the addresses table row also gets deleted. So the app must, absent FK, do delete from people... delete from addresses... Maybe this is the case we're concerned about? That the app forgets to do the cascading delete? S: Correct. That is just one example. T: When I search Google trying to find why people think FK are necessary, this seems to be the main case they're worried about. \"Orphaned Rows.\" [ed: we can encode deletes of the child table rows into the DB access layer] T: Okay, so then what's the other case? When it comes time to insert... the application will insert into addresses... insert into people right? And whatever ID it gets when inserting to addresses it'll use as the ID for people.address_id ? But how would a high-concurrency situation change this? No matter what, when you insert into addresses , you'll have an ID assigned... No other thread can get it, whether an FK is there or not? S: Updates as well can be complicated. T: When you UPDATE, you will never need to change the ID. Maybe let's imagine for a second that you do, though. So you update the addresses ID from 1 to 5... And another thread at the same time thinks it can change 2 to 5... The primary key (or unique index) saves from two threads changing id=5. S: Okay. I give up... FOR NOW. Tell me why FKs exist at all, then. T: So relational databases were made in... the 1960's? 1970's? Something around then. At the time, everything... and I do mean everything would happen on one computer. Codd and all those brilliant guys thought of how to have the database make sure shit was unbroken at the closest layer to the data as possible, in the database. This made sense at the time. It even a little bit makes sense today, but here's what's changed: we started building systems that run on many computers. When we did that, we found out some components scale horizontally really well, like stateless application code. And some components are really hard to scale horizontally, like stateful database code. So typically the former runs on a whole lot of cheap machines. The latter runs on fewer very expensive machines. So a bunch of smart people sometime in the past 10 years or so figured this out: When you can move the work that used to go into the database into the application layer... The whole system becomes more efficient/cheap. We used to have these \"stored procedures\" and people would write entire applications worth of code in them. No-one does that anymore, because in so doing, you're pushing all your compute to expensive hardware that doesn't scale well. Oh, and stored procedure languages are ugly... but I digress. S: Nice.  I get what you are saying. Just, relying on the app to handle the edge cases makes me nervous. I think if our development environment would have a comparable stress test as production then I wouldn’t worry so much. T: I feel like there is potentially a legitimate concern in here, but we can't seem to put a precise condition on it (outside of the cascading DELETEs). So yeah, a very real problem is if no-one ever tests in dev/staging. Then no FK constraints are ever violated. Then we go to production where I've removed the FK constraints, and data gets all wonky. But really, the edge cases around FK violations are pretty boring/trivial. It's all about \"I deleted the row from the parent table and left the row in the child table, oops!\" and... that's about it, outside of UPDATEs to the child table without subsequent UPDATE of the parent table (and I'm still struggling to think why you'd do that anyway). There can be \"I created a parent table entry and left the ID for child NULL and never created a child row\"... But we can define the parent table ID column as NOT NULL to solve that problem. There are a lot of DB professionals (esp in the MySQL community) who strongly claim FK constraints are NEVER necessary. In any environment, anywhere. They say code FK constraint logic into DB layer in your app. I'm not quite that extreme. It's an interesting discussion. I haven't put a LOT of thought into it... just enough that I feel about 95% confident we can drop FK constraints in production, and getting that performance win would be great. S: But Foreign Keys enforce the lowest level business rules, for example, \"if an order  exists, it must be related to a customer that exists.\" If you remove the  constraint, you no longer have enforcement of that rule, which would allow any client of the database to (for example) remove a customer who  owns orders, while leaving the orders in place. You would have to write more (error-prone) code in your application (and test that code) to  prevent this. In some places in the code, we don't use an ORM, and have just raw SQL in the code base. I think a good experiment would be to run some queries in search of orphans in our DBs. I see as a benefit the use of constraints, because they are an additional layer of correctness/sanity checking on your application. They will expose bugs that would otherwise stay hidden, in development and in production. And I prefer to detect a bug than to \"have an order with no customer\". T: I guess what it comes down to is the following: In my experience, at most orgs (and my gut instinct is that this holds at Bigcommerce), #1 is little, #2 is insignificant, and #3 is significant. As to your particular example, I do find it hard to imagine the situation where an order might get created for a customer that doesn't exist. Transactions can solve that problem. Also, \"broken\" RI can be a benefit. For example, if we truly DELETE a customer from the database, we might still want a record of the fact that orders were placed by (a now-unknown) customer. If we enforce cascading DELETEs to save us from orphaned rows, we cannot have this arguably quite useful state in the database. If the answer is to just flag the customer as deleted without actually DELETEing the row, then we've left the realm of where FKs help again.", "date": "2015-09-28"},
{"website": "BigCommerce", "title": "Design+Performance @ Bigcommerce San Francisco", "author": ["Eric Joe"], "link": "https://www.bigeng.io/design-performance-bigcommerce-san-francisco/", "abstract": "This week we had the honor of hosting the creator of YSlow and HTTP Archive, former Head Performance Engineer at Google, and overall performance Boss - Steve Souders to speak on Design + Performance . He opened by talking about the gap that commonly exists today in how web apps are built, between designers and developers with regard to performance. The relationship can be adversarial and contentious, but it doesn’t have to be and it can’t be that way to build engaging successful web applications. It can and should be a complementary relationship, like the Yin and the Yang. The challenge is finding the intersection of design and performance. There’s a lack of awareness of how a design can impact performance that would benefit by bringing groups together early in the product development process. A summary of where are we now Where do we start? At the beginning of a project, create a small interdisciplinary team (comprised of all the parts of the development process - Development, Design, Product, Marketing, Sales, Ops, etc ) that will stay together for the life of the project. Establish communication channels and if possible, co-locate them. Bringing people in halfway thru is a much harder hill to climb. As a team, define guiding principles for the project. Ask yourselves “What are the most important things?” Prototype early with designers and developers and have everyone on the team engage. Start measuring performance from the beginning. You don’t have to optimize from the beginning, but you’ll get an idea of the challenges you have ahead. Establish a performance budget - limits on load time, render time, css and js payload sizes. Measuring performance We need to move away from `window.onload()` as it doesn’t reflect the user experience for today’s websites. As an example with Gmail, onload fires at 3.9s, but there’s no content for the user to see or use. With Amazon, onload fires at 9.7s, but above the fold (ATF) rendering happens at 2s. There are two metrics that reflect what the user is experiencing on your website: WebPageTest also displays filmstrips of the timeline of what’s painted in a browser for a given page Custom metrics The standard metrics aren’t the only way to judge performance. You can create custom metrics based on what you deem to be the most important things on your page/site: Tying it all together This brings us full circle, back to the beginning of the project. Sit down early with the entire team - designers, developers, and other decision makers, and decide on what it means for your website to produce an enjoyable, engaging experience. Thank you to Steve for taking his time to talk to us! You can find Steve’s presentation on his site and can watch his talk below.", "date": "2015-04-23"},
{"website": "BigCommerce", "title": "The Better Parts of Javascript @ Bigcommerce San Francisco", "author": ["Mick Ryan"], "link": "https://www.bigeng.io/the-better-parts-of-javascript-bigcommerce-san/", "abstract": "Bigcommerce SF had the pleasure to host Douglas Crockford for our first tech talk in our new SF office on 2/3. Douglas is a guiding voice in the Javascript world with his no-nonsense approach to development by utilizing “The Good Parts” of Javascript, made clear by his best selling book Javascript: The Good Parts . The event was open to the public and we had 100 seats available for guests with pizza and a nice Scrimshaw brew on tap. The stage was set (we actually have a stage) and the talk started with the Crockford humor that is notable in his other works. Douglas explained the groundwork that led to him writing the Good parts and the mistakes that he made that lead him to develop JSLint. After the release of The Good Parts, he compiled a list the arguments he received that went against the good parts. Danger Driven Development (DDD) We then heard about Danger Driven Development using the dangerous parts of JS such as ASI (Automatic Semicolon Insertion), Global Scope, == instead of ===, among others. You may find certain functionality of your code will not always behave in the way which you believe it should. That is what the good parts and the better parts are all about. When you can identify practices and patterns that cause you to practice DDD, it will make it easy to remove them from your workflow increasing your chances that you will not be affected by that pattern again. Better Parts and ES6 Douglas spent a good amount of time laying the groundwork for talking about the new Better Parts of JS and ES6. It was great to see what he identifies as his list of “Better Parts” and gave a short description on why he identified it as a better part. He also discussed a few of the new Bad parts that are coming in ES6 as well. The Next Language This part of the talk is where Douglas really focused his attention in what he wants to see in the next language after Javascript. He believes in a two tier approach to languages, having a systems language and an application level language. The application language having much more abstraction away from the bare metal of the machine. The wrap up Douglas also spoke extensively about the technical details of some of the new features, expounded on the bad parts of integers, and spoke about features that he would like to see in the next language such as a single number system, dec64 . We ended the talk with learning about JSON, the recent popularity boom and overtaking XML. He also gave advice to data format standard designers. Douglas finished with the most insightful advice to all of us as well, Don’t Make Bugs and he also made time to autograph and chat with his fans! Overall, it was a fantastic night at Bigcommerce SF! We made new friends and everyone greatly appreciated Douglas taking the time to come share his advice, stories, and insight with us for an evening. Douglas Crockford w/ Bigcommerce SF Front-End Engineering Team", "date": "2015-02-03"},
{"website": "BigCommerce", "title": "AdvancedAWS meetup at BC San Francisco", "author": ["Steve Corona"], "link": "https://www.bigeng.io/advancedaws-meetup-at-bc-san-francisco/", "abstract": "This week, we hosted the San Francisco AdvancedAWS Meetup at our San Francisco office. We had two awesome speakers talk about advanced AWS tools that I think many people running on Amazon will find useful. The first was a tool called CloudCoreo , created by the presenter Paul Allen , that aims to be a replacement for AWS CloudFormation. CloudCoreo gives you the ability to share and extend server configuration templates. I personally like that it solves many of the headaches with CloudFormation that I’ve had to deal with in the past— templates getting out of date or servers drifting from the original template. The second talk by Jon Skelton gave us a deep dive into Suricata , a tool for doing IDS on AWS. Jon walked the audience through some advanced use-cases for Suricata. We love hosting meetups at the Bigcommerce offices— if you know of any awesome software or technology meetups in SF, Syndey, or Austin, please get in touch with us! We will be hosting the GoSF meetup in June, so make sure to RSVP! You can watch the full talk from the AdvancedAWS Meetup below:", "date": "2015-03-28"},
{"website": "BigCommerce", "title": "Aurora + Mesos in Practice at Twitter", "author": ["Steve Corona"], "link": "https://www.bigeng.io/aurora-mesos-in-practice-at-twitter/", "abstract": "This week we hosted Bill Farner and Maxim Khutornenko from Twitter in our San Francisco office to speak to our engineering team about Aurora and Mesos. Bill and Maxim both work at Twitter and lead the Apache Aurora project. Aurora is the platform that almost all of the production critical services and caches run at Twitter, as well as internal services and cron jobs. The powerful combination of Aurora and Mesos have made it easy and cheap to deploy services at Twitter (it’s essentially their own internal PaaS). It took them about 4 years to go from “Hello World” on Mesos to fully migate all of their production services to Aurora. The big takeaway, for me, was that Aurora has unlocked the engineering team at Twitter to have self-service access to computing resources, while still providing stability and uptime. We also learned about a few of the features coming soon in Aurora— specifically: You can watch the entire talk below—", "date": "2015-03-07"},
{"website": "BigCommerce", "title": "Dave Lester: Production-Ready Containers (Using Aurora and Mesos)", "author": ["Steve Corona"], "link": "https://www.bigeng.io/dave-lester-production-ready-containers-using-aurora-and-mesos/", "abstract": "This week, we had the pleasure of hosting Dave Lester in our San Francisco office to give his talk \" Production-Ready Containers: Using Aurora and Mesos \" . Dave is an Open Source Developer Advocate at Twitter and gave us the rundown of how Twitter is currently using Mesos+Aurora in production. We're particularly interested in these technologies at Bigcommerce because we're currently in the early-phases of moving pieces of our infrastructure into containers, which leads us to many questions: Needless to say, the industry is still in the early days of picking a clear winner or centralizing on a popular standard for productionizing containers, which is why it's nice to hear from Dave, who's able to share first-hand how Twitter is doing it. Twitter is running Apache Aurora on their Mesos cluster (they are also the original developers of the Aurora project and contribute heavily to both). Dave says that Mesos gives them four value propositions that make it worth using— Reliability, Utilization, Manageability, and Approachability. In particular, I personally find approachability to be the most important feature of any system that you want your entire engineering team to use. If you centralize on a tool that's difficult to use, odds are that it won't be well-adopted within the company. Although Mesos allows you to run many different scheduler frameworks simultaneously, Twitter is only running Aurora in production. The reason for \"running one scheduler to rule them all\" is that it gives them a single interface to manage long-running services and cron-jobs, as well as giving them a centralized priority and quota management system to kill of non-crucial tasks during high-traffic events. Overall, I think that Dave really made a good case for Aurora, especially because it's been battle-tested in production at Twitter. I encourage you to watch the full talk, included below. Thanks again, Dave, for spending some time with the Bigcommerce team.", "date": "2015-10-05"},
{"website": "BigCommerce", "title": "Team on a Mission", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/team-on-a-mission/", "abstract": "Reflecting back on the past year, I’m proud of the team that we’ve built at Bigcommerce and what we’ve achieved together. No matter how talented any one individual is, there’s a limit to what can be accomplished in isolation. It takes the collaboration of a team , with a variety of skills and expertise, to achieve something truly remarkable. This is our mission. Our goal is to provide the world’s best commerce platform for fast-growing merchants. We’ve just passed the peak holiday shopping season dubbed Cyber Five — the five days from Thanksgiving thru Cyber Monday. During this critical period, the Bigcommerce Engineering team made the conscious decision to implement a code freeze to minimize changes to our platform. It’s a strategy that we used to provide 100% uptime during Cyber Five for two years in a row . Our merchants benefited with impressive 50%  year-over-year gains in gross merchandise value (GMV) and order volume. We couldn’t be more thrilled for our customers! Our Engineering team has made many improvements in features, uptime, security, and performance of our platform. Because we provide Software-as-a-Service , our customers reap the benefits of our code releases immediately — a significant advantage over traditional software. We’re proud of how we’ve evolved the platform over the past 12 months, but we have even more to accomplish. In 2016, we’ll release a series of features that will help our merchants Sell More, Sell Efficiently, and Sell Everywhere. We’ve put a lot of work into our platform this year, and our reward is seeing our merchants succeed every day. With the end of the year upon us, I’ve encouraged our team to take a much deserved rest from the breakneck pace in 2015. We’ll be back in the new year to continue our pursuit of building software that changes people’s lives. Bigcommerce is more than just one team; like many organizations it is actually a “ Team of Teams ”. We have functional departments that you would find at most other companies, and each of these teams specializes in what they do best. Although we’re globally distributed across Austin, San Francisco, and Sydney, we all trust in each other and our  shared commitment to our mission. We’ve learned to adapt as we’ve grown the company, reflected in a new structure we’re embracing in the engineering team. At the start of 2015, engineering work was primarily orchestrated in a command-and-control structure. That will change in 2016, as we subdivide into self-directed Durable Teams; each team will own specific areas of the platform, coordinating with the broader organization on our core mission. I’ve been talking to team members about this “Team of Teams” approach for a few months, and everyone is eager to adopt this nimble approach to enhancing our platform. Staying true to our mission means keeping sight of the beneficiaries of our work: our customers. We’ve built a platform that powers commerce for our merchants, and we’re motivated by their success. We enjoy talking directly to our customers to learn about their businesses and how we can improve our service. Over the past 12 months we’ve been visited by several customers including StyleRocks , Travertine Spa , and Cigar Hut . If you’re a Bigcommerce merchant and you’d like to speak with our team, please contact me — we want to connect with our customers! Over the past month, I’ve also been connecting internally with members of the engineering team. My goal was to speak with every individual on the team before the end of the year (approximately 120 separate conversations). The discussions have been illuminating, giving me insight into what our team does well, and what things we can improve in 2016. It’s also been humbling to meet with everyone, realizing the level of talent and commitment we have across our global engineering team. Ultimately, it’s the collective effort of the team that will determine whether we achieve our shared mission. I want to acknowledge the amazing Bigcommerce Engineering team and all of the work they’ve done in 2015. I’m looking forward to shaping commerce in 2016 together. I’m honored to be a part of this team. Stay on target.", "date": "2015-12-22"},
{"website": "BigCommerce", "title": "Fun with git - breaking apart a large PR into more manageable chunks", "author": ["Meg Desko"], "link": "https://www.bigeng.io/git-magic/", "abstract": "Sometimes a team moves so quickly on a project that they end up with a rather large feature branch in a short amount of time. The sheer size of the change set makes it difficult to merge the feature branch code into the broader code base. On a recent project, our team faced just this situation.  We were cranking along, building a new anti-fraud integration for Bigcommerce Enterprise , when we (seemingly) suddenly neared the end of the project.  We put in a pull request, which showed us how much code we had added and changed- a lot.  The PR added or changed around 7500 lines of code.  Needless to say, it would be challenging for one of our colleagues to review all of this code at once, so we decided to submit these changes in a few change sets to make code review more manageable. Starting Point: +7500 LOC in feature branch At Bigcommerce, we make schema changes with some releases and not others, so the first code we split off from our anti-fraud-feature branch was a schema change we needed to make. To do this, we checked out a new branch off of our main branch. Then we checked out the schema change file from our anti-fraud-feature branch. We pushed the code to our remote anti-fraud-schema-change branch, creating a new branch, and put in a pull request from this branch to master. 100 LOC removed, +7400 in feature branch We also manually removed the schema change files from our anti-fraud-feature branch and pushed the resulting code to our remote anti-fraud-feature branch. This schema change introduced a UI change that we wanted to prevent merchants from seeing during rollout, so we also reverted the UI change until we had better controls around it. We pushed this branch up to a remote anti-fraud-ui-update branch, and then reverted the commit in our feature branch. 5 LOC removed, +7395 in feature branch Next, we decided to separate a library that moves around data for our integration into a new branch called anti-fraud-data-sender . We chose to remove this library because it is not integrated with the existing Bigcommerce codebase. We also pulled the unit tests for this library into the branch to validate our code during the process of separating it from our feature branch. To do this, we checked out the library and test directories from our anti-fraud-feature branch onto a new branch created from master . Failing unit tests showed us where we did not have appropriate mocks for classes not present in this branch.  We updated our unit test mocks to remove these dependencies and amended our previous commit to include our minor changes.  We put in a pull request to master for this branch so we could get feedback from our colleagues. 2300 LOC copied from feature branch While waiting for feedback on that pull request, we pulled a helper library with some tie-ins to the main Bigcommerce codebase into a new branch.  Each of these data gathering classes has unit tests, so we pulled the library and its tests into an anti-fraud-data-gather branch. Naturally, the unit tests failed because we had made some small changes in other parts of the codebase to support our data collection.  We cherry-pick ed those commits over to the current branch, pushed it to our remote anti-fraud-data-gather branch, and put in a pull request for review. 3495 LOC copied from feature branch After both the anti-fraud-data-sender and anti-fraud-data-gather branches were merged into master, we looked at several paths forward for the remaining code in the feature branch.  It was important to get this code in relatively quickly, as it is the code that ties our anti-fraud libraries in with the rest of the codebase and makes our feature viable and many of these files change often. The first option, which is how we typically move forward with smaller change sets, was to git pull --rebase origin master into our anti-fraud-feature branch.  This would pull the current master and then try to add our changes on top; we would resolve major conflicts manually.  This turned out to be a terrible idea for many reasons, including the 140 commits and the many updates we had made to the libraries we merged into master during the project— git rebase --abort . Next we looked at taking master, git merge ing our anti-fraud-feature branch into it, and merging that into master.  That has the unfortunate effect of changing the git history relative to master and doesn't follow our typical development flow, so that path is non-ideal as well. We asked ourselves, \"Wouldn't it be great if we could scrub our data-sending and data-gathering directories from our git history like they were never there so we could merge our remaining code?\" That's when we discovered git filter-branch . git filter-branch allows you to remove files or even entire directories from your git history (if you use the tree-filter option).  It's like those files never existed . First, we found the last common commit SHA with our master branch by looking at our git history in github.  This was important because tree-filter ing is no small feat and takes a good while even for 140 commits.  Then we backed up our feature branch to 3 locations, just in case. Then we ran the filter-branch git command with the tree-filter option.  We force removed the directory that housed our data gathering code, from the last common commit SHA to the current commit/ HEAD . And waited.  And waited.  And waited.  (It probably took about 3 minutes, which seems like a long time when waiting to see if you broke a feature your team worked on for several months.)  When it was done, our path/to/data/gather/ directory was gone.  We repeated this process with the other directories we wanted git to forget in our anti-fraud-feature branch. When our filter-branch finished, we confirmed that the directories that were supposed to be gone were removed, and then updated our anti-fraud-feature branch with changes made to master, using our standard git pull --rebase origin master workflow.  As expected, there were no conflicts pulling in our libraries and unit tests. We proceeded to resolve other conflicts manually; changes had been made to several of the files we modified for our integration and these needed resolution. 5795 LOC removed, +1600 in feature branch The most difficult conflict to resolve was a mass whitespace change affecting 2 files we modified for our integration.  There was no good way to resolve this change, so we opted to git checkout --ours /path/to/modified/file to ensure our team's changes stayed around.  We pushed the rebased anti-fraud-feature branch to our remote and staged a pull request into master to compare those files.  Then we manually updated a few lines that were changed by someone outside our project team, and committed those changes. In the meantime, some changes had been made to configuration files, so we needed to rebase again.  We did git pull --rebase origin master as usual and pushed the result up to our remote.  When we went to stage the PR again, we noticed that the last changes we'd made were not applied at the end of the rebase, and we'd just removed that commit from our history- local and remote.  Not a good moment. Since we'd recently had a version of our anti-fraud-feature branch checked out that contained our change, we used git reflog to show the recent history: and used git reset --hard HEAD@{3} to move back in time to before the rebase.  We tried it again.  This time, the rebase worked as expected, and applied our changes on top of those from master.  We pushed our anti-fraud-feature branch for the last time, pre-review.  The whitespace changes to those files added nearly 3000 changed lines of code, so we asked people to view the Github PR with ?w=1 at the end of the url to hide the whitespace changes. Over the course of a week or so, we were able to whittle down our feature branch pull request so that our final anti-fraud-feature PR contained 1600 LOC (4500 including whitespace changes).  This change set, while not small, is considerably smaller than the 7500 lines we started with.  Once that pull request was reviewed (and fixed), we were able to merge our last group of changes into our master branch.", "date": "2015-12-14"},
{"website": "BigCommerce", "title": "How we \"CSS\" at BigCommerce", "author": ["Simon Taggart"], "link": "https://www.bigeng.io/how-we-css-at-bigcommerce/", "abstract": "TL;DR Our SASS Style Guide is available on GitHub CSS is hard. Writing good CSS is harder. Writing good CSS with a big team on a large codebase… wow, just wow. We're not exactly unique as a software company; 120 engineers, 4 offices, 3 countries, 3 time zones and 7 years present an environment for a codebase we all know well. Everyone has had a go, there are 30 different button styles, 4 variations of your \"Brand color\" and a package.json / bower.json file listing every possible JavaScript package on the Internet. CSS just seems to be, in relative comparison to other languages, the most poorly neglected child, where the least amount of care is particularly given. There's no set rules, no conventions and no built in tools to prevent you from yourself. It's a minefield. We've all been there and a lot of people and teams will still be neck deep in it. At BC we decided that we could at least tackle some of the common problems in writing a lot of CSS, just by setting some ground rules and making everyone who contributes CSS follow them. Our SASS Style guide is nothing new or groundbreaking, and the concept is very similar to AirBnB's excellent JavaScript Style guide . I'm not going to copy and paste verbatim in this blog post either, instead you can find it on GitHub and have a read in full for yourself. I did think, however, that it would be much more useful to call out some specific rules and explain them in greater detail, list the stack we build on and the tools we use to help everyone stay on the same page. First and foremost, what we wanted to achieve wasn't trying to be clever, cutting edge or highly optimised. We were after an open policy; sensible over optimised, clear over clever. It was aimed at making the codebase easy to on-board and share across a large team. You'll notice language like \"readable and understandable\", \"simple\", \"short as possible but as long as necessary\", and \"Just because you can, doesn't mean you should\" throughout the document to give us a common sense approach to writing CSS. Our CSS contribution is based on some guiding principles to how we think about CSS and components. I'll call out a couple of super important ones to us as they're either key or not particularly obvious: Don't try to prematurely optimise your code; keep it readable and understandable Our CSS code base is SASS, specifically SCSS syntax. SASS is wonderful, powerful and terrible all at the same time. With any powerful tool, comes the risk of software engineers doing the very thing they're really good at: Over Engineering things. The phrase \"Just because you can, doesn't mean you should\" applies a lot to SASS. I've seen some really crazy complex SASS functions generating a bunch of crazy clever CSS and the danger is, not many people really pay much attention to the output. Output is pretty important, especially with weight and specificity. Also clever syntax or selector nesting like the Parent Selector Suffix are neat, but are hell-a-difficult to search for in a codebase. Don't be clever, be a good citizen, play nice with others. It makes it really hard for me to just pick up your code and contribute to it. Make it simpler and let post-processing do some of the clever stuff, I'll thank you for it later. Break down complex components until they are made up of simple components Undeniably the most important thing when composing components in your HTML and CSS patterns. Naming conventions like BEM or SUITCSS or SMACSS are really handy tools for keeping your modules modularised, but following the \"convention\" too strictly can lead to complex and long class names when dealing with deeply nested child elements. Start abstracting out some common child patterns as early as you can to prevent these kind of dreaded dreaded selectors: Build your component as a mixin which outputs optional css This is an interesting one. We as a team build patterns, common markup and CSS rules for displaying a certain type of data in the UI, in a particular way. Our framework doesn't output CSS by default, you have to opt in to the components you want. Our framework also serves multiple, varied domains or properties, where the data might be the same, the pattern might be similar but for whatever reason the name we've chosen for our very generic pattern doesn't suit. Maybe our \"card\" component is better suited to be your \"product\" component in the context of that domains codebase. So every component we build is always a mixin, wrapped in a generic class name. Because you can opt out of the CSS that's generated, you are free to rename the component to your choosing, include the mixin, and still get the agreed upon design pattern. I'll highlight some key rules we think are important to a happy codebase, used on a large-ish product. Aim for selectors that are as low in specificity as you can humanly make them. It'll help abstract components into smaller chunks, allow for greater re-use and re-mix of patterns, and it'll stop you having a lot of specificity clashes in the future. When building a large codebase of patterns, try to only style the property you are explicitly concerned with to avoid overzealously resetting something you might want to inherit. Declaring a shorthand property of background for example, resets background-position , background-image , background-size etc which you may not want to do. Play nice with others. @extend first, then @include , then set your properties. Ideally the extend and include don't have to override or clash with your properties. Followed by my personal favourite rule, alphabetical order , always. There's been a lot of think pieces by lots of different people about all the magical and logical ways people like to group their CSS properties together inside a rule. Don't force people to learn your opinion or \"logic\" each time a new starter comes onboard. The order literally doesn't matter. Aim for common sense, predictability and wide adoption; a lot of people know the alphabet and it'll let you spot repeat declarations easily. Don't. Or at least try your damned hardest not to. The output of your compiled CSS is extremely easy to lose track of. You can easily break Specificity and Performance guidelines when creating your selectors when you start nesting with SASS. Just because you can, doesn't mean you should. We aim for a maximum of 1 level deep of nesting, with the use of common sense when that's not achievable. Abstract the name of your variables. Don't name your variables, for example, the name of the colour you are setting. This is no longer a variable, and is no different to finding and replacing a hex colour code in your codebase, if you decide to change the value of $background-color-blue , to be red. As described by the excellent Erskine Design Article, Friendlier colour names with SASS maps , we use SASS maps for a lot of global style properties, not just colours, that our developers are going to need frequent access to. It allows a simple, predictable API for them and a set scale for things like z-index, font-weight and line-height. We'll cover this in much more detail is a coming blog post. We took pretty heavy influence from SuitCSS and slightly modified it to our tastes and needs. For example we opted for camel case instead of pascal case. As I mentioned earlier, correctly naming your descendant children is pretty important and we take a fairly pragmatic approach. Just because an element is a descendant of a descendant to the root of your component, doesn't mean it has to live at that level in the DOM. It could easily function the same way and be adjacent to the first descendant. When dealing with plurals of something, perhaps the descendant name is better suited to be the singular version, and not appended to the parent name. It's much better to avoid verbose descendant class names, by keeping class names as short as possible and as long as necessary. As I've mentioned our new CSS code base is in SASS and of course like all the other cool kids, we use libSass to compile our stylesheets. There are a couple of projects that use Ruby Sass, and the performance slow down is extremely noticeable. I also mentioned about doing clever things with your code post compilation. An example of this is vendor prefixes for CSS features that may not be fully adopted by certain browsers. Instead of littering our code with these vendor prefixes,  proprietary implementations, or making Sass do a bunch of extra grunt work, we use Autoprefixer to do it for us after Sass has done it's job. In terms of output optimisation, we use CSSO to optimise our code when we perform a deploy of our core CSS libraries. CSSO does the usual things you'd expect from minification like stripping out all the whitespace, but it also does some structural optimisations on the code for us. Grouping like selectors together from different components, shortening syntax where it can, shaving off small bites that we may introduce in our more \"common sense\", \"clear over clever\" approach to writing our code. Sounds risky, I know, but so far we haven't noticed anything breaking and it works really well. I'm sure some of you will have read along and through the guide and thrown your arms up in dismay at \"the repetition of code\" we'd introduce with some of our rules. Well CSSO helps us deal with that after the fact, and we can rely heavily on Gzip to remove some of the other repetitive code snippets that might remain. This leaves our code base readable, clear and obvious. Let tools do the work for you. Lastly, how do you check your fellow team members are adhering to the rules? A good Pull Request policy will help most of the time, but on large teams that's not exactly scalable from a small CSS team. We make use of scss-lint to analyse our code as we write it, and upon creating a pull request to the core libraries (just in case you thought you could just sneak that CSS in without spinning any of it up in a browser). If it fails to adhere to the styleguide, your code doesn't build on your machine, travis fails and your PR is marked as so. Helpfully we include the YAML file for our rule set which seems to get us really close to the style guide, so anyone can follow it. This configuration is also stored in our common grunt tasks that every new Front End project starts with, so you get CSS code linting out of the box. Despite our best efforts, it's still really difficult to enforce these ideas over a wide team. The tools only get you so far and you can still contribute CSS that is functional but doesn't make the grade. We found education and coaching worked best, coupled with the tools and guidelines as reference. I particularly found that in many cases you really have to learn from your own mistakes with CSS before it really \"clicks\". Writing functional CSS that \"just does the job\" is extremely easy to do. Learning to spot how that will play in a wider eco-system and predict what side effects it might cause in the future, takes some time. On the plus side, distributing our linting rules as part of our grunt plugin package was extremely handy to gain adoption and people generally found it extremely useful. The conventions we put in place for our map based properties like fonts, sizes, spacing, line heights and z-indexes were certainly a highlight for our JavaScript Engineers, as it was completely predictable and easy to remember. CSS in large teams on a large codebase is hard but you can make it suck less by implementing a few guidelines, tools and training sessions to help your teammates stay on the same page. Overall I think we've done a pretty good job so far. Now, to preempt another one of those \"But what about X which solves that better\" moments I know you're having right now, I'd like to quickly acknowledge some of the great work people have been doing around this exact topic with regards to \" CSS in JavaScript \" \" Inline CSS \" or the particularly awesome \" CSS Modules \". These deal with legit problems, I'm not going to rubbish them whilst protecting the \"old guard\" way of doing CSS, though there are a few reasons why we haven't gone down this path. Some things we can't deal with. Some things we actually really like about CSS like media queries. Most of these ideas come from the React eco-system which we don't use. Most come from the fortunate place where the majority of your front-end is already in JavaScript, and ours certainly isn't. The chances are your codebase is newer than ours, significantly smaller or you've got more money and developers than sense. We envy you. It doesn't mean we or you are wrong. So that is our approach. Aimed for our environment, our eco-system and a place where (I imagine) a lot of other teams who aren't Facebook or live in a super ideal world, would find themselves in. I hope it'll help you, because with the combination of a well reasoned, pragmatic code style guide that's fairly easy for people to understand, coupled with post processing tools and code linting, we are able to find a relatively happy place in terms of a large CSS codebase. It's obviously not bullet proof, especially on it's own, and we'll be following up on this post with a few articles around \"How we CSS\" and how we make things \"less terrible\". We'll be tackling: We're currently hiring for amazing Front-End Engineers that understand these principles to join our UI Platform team. If you're experienced in creating and shaping user interface guidelines and you like what you read, get in touch .", "date": "2016-02-01"},
{"website": "BigCommerce", "title": "Girl Geek Dinner @ Bigcommerce", "author": ["Sherry Li"], "link": "https://www.bigeng.io/girl-geek-dinner-at-bigcommerce/", "abstract": "Bigcommerce's San Francisco office had the pleasure to host the Bay Area Girl Geek Dinner event earlier this month. The topic was on \"Pathways to Tech\", how Bigcommerce girl geeks with diverse backgrounds are now in tech space. We had MJ Chun (Senior Product Manager), leading the panel discussion with Cynthia Chu (Senior Director of Front-End Engineering), Meg Desko (Software Engineer), and Andrea Wagner (Head of Design) as our speakers. MJ asked our speakers how they got into tech, stayed in tech, keep up with the industry trends, and more. Here are some key take aways: Cynthia started off as an accountant intern at a web development firm. Whenever she finished her work early, she would help the firm develop websites and ultimately, developed a passion for coding. She stays in the tech industry because it is constantly evolving that she never gets bored. There are always new challenges on responsive design for devices in different dimensions, as well as new software framework and new programming language to learn. While it is hard to stay on top of the trends in tech, she learns the most from her team and her friends in the same industry. After being through several big corporate firms prior to Bigcommerce, Cynthia advices college grads to join a startup, because one can learn a lot more from startups. Meg got a Ph.D in Chemistry and became a professor, but then she decided to look for another science and tech related job. While she was working as an office manager in a tech company, she learned how to code with a mentor, who always said, \"four eyes always better than two\". (Pair programming at its finest!) She enjoys working in the tech industry because there is always another project to start on and another bug to fix. Moreover, in Bigcommerce, we acknowledge the individuals and the teams whenever a milestone is hit, a big project is finished, and a critical bug is fixed. Meg's advice to younger devs - Do not spend 3 days on a javascript bug, timebox it, and if you are still struggling with it then talk to someone else who knows it better. Andrea has always been interested in tech, in fact, the main reason she attended CMU was that she saw robots playing soccer on campus. Other than tech, she also likes to help others, and Bigcommerce allows her to take part in empowering people to build a successful business. Not only so, she likes how every individual is solving problems here unlike the top down driving directions in traditional firms. Andrea has a quick technique for staying on top of her game - she spends some time on learning new trends, but if she doesn't feel like it is beneficial, then she moves on, and if she hears about it again, then she will revisit it, however it is also okay not to try everything. Thanks again for everyone that joined us!", "date": "2016-01-21"},
{"website": "BigCommerce", "title": "Mind Maps for Rapid Testing", "author": ["Venus Nautiyal"], "link": "https://www.bigeng.io/mind-maps-for-rapid-testing/", "abstract": "As a Quality Engineer at Bigcommerce, I like to participate in my project right from the initial phases, including going to brainstorming sessions and planning phases. Of course during this, I like to jot down all the areas that need to be tested, especially the high priority testing areas of the project. Normally, a project planning or brainstorm meeting would end with a few notes in my computer explaining what areas I need to focus on testing, but lately, I’ve been trying a new process. Instead of just taking notes, I’ve replaced the notepad with a mind-map sheet before going into the meeting. When I started doing this, I noticed how the creative side of my brain kept joining the dots of the project and by the time I stepped out of the discussion, I had a rough mind-map of the entire test plan for the project! I call this mind-map the “Rapid Test Document”. The best part is, because it’s so easy to read, the team can review it right away. If the team feels the need to modify it, that can be done right away, too. Having a mind map makes writing the detailed test case document way easier as well, and that’s why I call it a Rapid Test Document. We prefer the Rapid Testing technique because of the ease of its on-the-go creation, instant review & the agility when creating a detailed test document. For medium to large sized projects, the Rapid Test Document is considered an Intermediate step for the eventual full-fledged test case document. However, for a mini project, a time-consuming detailed test case document can be skipped altogether and the Rapid Test Document can serve as the final test document. Projects like this involve testing a minor production issue, a small feature being introduced in the application, or a minor enhancement.", "date": "2015-03-31"},
{"website": "BigCommerce", "title": "When do I stop testing?", "author": ["Venus Nautiyal"], "link": "https://www.bigeng.io/when-do-i-stop-testing/", "abstract": "Bigcommerce is a highly customer facing product. As many merchants as we support and create on a daily basis, it can be a little nerve-racking to release particularly sensitive functionality such as payments, shipping, tax.. the list is endless. As a Quality Engineer, I try to think of all the things that could go wrong and pay a lot of attention to keeping the quality of the product. At the same time, it is of utmost importance to keep the velocity of the team and release these amazing functionalities well in time. One of the key balances of my job is to be able to answer this simple question - “When do I stop testing?”, or in other words - “Have I tested enough?” In order to answer this make-or-break question, I generally follow a bunch of steps for every project- Create & Share Test Plan- It is a common practice to create Test Plan documents, right? I suggest it is a good idea to share it as well. In BC, QEs share test plans for a particular project at 2 levels - one with their fellow QEs and two, with their immediate project team. Sharing it with the fellow QEs opens up doors for more areas for testing that might have been missed the first time. And, sharing it with the project team (which includes the Product Owner, Project Manager and developers) really helps to filter down areas that might not even be touched at all, or scope out the testing areas better. Test Plan is also shared on BC intranet for future references. Review PRs- Once the development has begun, it’s always good to not only write automation code but also review developers’ PRs which gives an insight to precise code changes, making it easier to create test scenarios. Automate important scenarios first- When I get around to automating scenarios, I generally write the ones that are the most critical first and share with the project team. Then as I keep building on more automation scenarios, I also make sure the tests are run for every PR that’s raised - thus reducing redundant testing efforts. Bug Prioritisation- Many-a-times the test scenarios or bugs raised are not prioritised keeping in mind the realtime number of users of that feature. At BC, the data team is super helpful in determining real data for the feature in use - thus raising or lowering the priority of a test scenario or issue. Example- say a new modal window is not loading it’s CSS properly only on IE. It’s always good to fetch some data around how many BC merchants have used IE browser in the last quarter or so. If the numbers are really low, the issue automatically becomes lower priority and vice versa. Blitz Test- Once the entire testing is complete from QE’s end, arranging a “Blitz session” is always a good idea where folks from various teams are invited- QE, Dev, Sales, PM, PMO, etc. They all have easy access to the new feature on testing environment and everyone tests the feature or in other words tries to “break the feature”. Each new person brings a different testing perspective and rewarding the most critical bug-finder can be a good addon too! Post-blitz, all issues are curated by the team and fixed accordingly. Automation Test Suite- Another great idea to test smartly is to filter down the automation tests in accordance to the feature being tested which must be automated during the sprint. Say the new feature is related to payments. If I run all automation tests related to payments post code complete, I’ll immediately bring down a lot manual effort. Although, post test complete, it’s a good idea to run the entire test suite for a good sanity check. As I follow along this list of ideas, by the end of the project, the entire team gains confidence in our functionality and gets perked up for the release rather than being anxious. Following all of these steps has not only helped me test better but also helped me test smarter by involving fresh pair of eyes, automating the redundant tests and most importantly, knowing when to stop testing.", "date": "2016-02-09"},
{"website": "BigCommerce", "title": "Bringing Scala on Board", "author": ["Jon Wiese"], "link": "https://www.bigeng.io/bringing-scala-onboard/", "abstract": "Bringing a new language into an organization can present some pretty interesting challenges. These problems are multiplied by Scala which is not only unfamiliar, but encompasses the entirely different programming model in the form of functional programming. There are many different ways to approach bringing people up to speed on Scala, but at Bigcommerce we were looking for something a bit different. We wanted people to not only learn Scala, but learn it in a way that would allow them to quickly turn around and start using their Scala skills to provide value to the company. This means that instead of focusing on a lot of theoretical functional principles we take a more practical and incremental approach to learning and doing as a team. This method allowed us to ramp up multiple people who were able to begin quickly pushing consistent, working code. Recently, I gave a talk about this exact topic at the Austin Scala Enthusiasts Meetup . Learn exactly how we do it by watching the recording below: If you're interested in seeing some of these methods first hand, we're currently hiring Scala developers for our Downtown Austin office! Apply online at https://www.bigcommerce.com/careers/", "date": "2016-02-22"},
{"website": "BigCommerce", "title": "Transaction Isolation Level (TXIL) Subtleties in MySQL", "author": ["Tim Ellis"], "link": "https://www.bigeng.io/transaction-isolation-level-txil-subtleties-in-mysql/", "abstract": "Continuing the discussion around transactions, consistency, and MySQL, we consider transaction isolation levels. Remember in MySQL we have auto-commit=1 . When you set the transaction isolation level with a statement, that only lasts one transaction if you don't specify the session keyword, so the TXIL will revert to the global default immediately, before you can do a \"start transaction.\" See how the addition of the session keyword in the following makes things behave as we'd expect: When you are running transactions in Repeatable Read TXIL (as we do at Bigcommerce), when do we need to append the FOR UPDATE modifier to SELECT statements? Remember MySQL works very hard to make sure no reader ever needs to block. This is the snapshot mentality around avoiding concurrency problems. The 1970's thought process used exclusive locks for the same thing. Although less performant, the subtleties around exclusive locks are easier to understand. Adding the FOR UPDATE modifier to a SELECT will explicitly put an exclusive lock on the rows affected. This changes the effective isolation level SERVER-WIDE for the duration of your transaction. All other transactions, when attempting to read your rows, will block until you COMMIT . See the following: You can get most of the same benefit by running in SERIALIZABLE isolation level, except that T2 won't block: Note what happens is that T2 is allowed to read the values SELECT ed or UPDATE d by T1. But as soon as it tries to UPDATE a value that's currently being modified by T1, it gets a deadlock and dies. Note that the winning transaction is the one that started first. So if T2 had done a start transaction before T1 did, then T1 would have received the deadlock error, not T2. It does not matter who UPDATE d first. Only who START TRANSACTION ed first. Yes. Depending on your definition of \"outdated.\" In MySQL that no matter what the TXIL, you can read a value that another transaction has updated (but not yet COMMIT ted), and when you do, you'll see the value as it was before the other transaction started. Because each transaction is in its own snapshot version of reality, and its reality will not leak outside its context until it specifies a FOR UPDATE modifier to the rows it's SELECT ing. So if you are expecting UPDATE d values from one transaction to be reflected immediately (before transaction end) to other transactions, you will be surprised. All UPDATE d values will be readable by other transactions in their original un- UPDATE d form until your COMMIT . If it's important other transactions don't see this outdated value during your transaction, be sure to add the FOR UPDATE modifier.", "date": "2016-05-25"},
{"website": "BigCommerce", "title": "AngularJS SF Meetup", "author": ["Daniel Almaguer"], "link": "https://www.bigeng.io/unidirectional-angularjs/", "abstract": "Bigcommerce's hosted the February Angular SF Meetup and our Front-End Engineer, Alex Taylor gave the talk: \"Unidirectional data flow in Angular 1.x apps\" (9.5/10 for that name) Alex mentions two pain points: Data flow and Code Organization MVC architecture in large applications can become difficult to reason about. Tracking back problems or adding new features to the existing flow is often complicated. Alex suggests using Flux. Flux has been popularized by React, but that doesn't mean we can't use it in Angular. It gives us an application architecture based around unidirectional data flow. He also mentioned that what he really enjoys about writing React apps is the architecture, not just the templating. So why not bring the same architecture to our Angular apps. Angular services need supporting architecture or guidelines around their use. Maybe we shouldn't throw everything into the services folder. This is how the folder structure would look like in normal Angular (left) vs Flux architecture (right). Instead of just having one /services folder for our common Angular services, we have multiple folders with clearly defined meaning. If you want to add some new functionality to your application you know exactly where to put it. Example: Keep track of new state? => That's definitely going to be under stores/ Need a helper function? => Can you guess? Yes, is that obvious utils/ \"Flux is simply a pattern that you can incorporate on your own.\" - Author of ng-flux. tl'dr: You don't need them. Alex continued his talk with some demos showcasing two chat application written in Angular, one using Redux and one using Alt. You should definitely check them out in the video.", "date": "2016-02-24"},
{"website": "BigCommerce", "title": "How Much Does Consistency Matter?", "author": ["Tim Ellis"], "link": "https://www.bigeng.io/how-much-does-consistency-matter/", "abstract": "Is consistency even important? We know it's contentious. Some people find it extremely important, and we all at least pay lip service to it. But really... how much effort should we put toward making sure we've got it? As you might have noticed, recently we saw Aphyr doing a Jepsen run against Galera , shortly followed by Percona giving a lackluster response (after recent edits to the Percona response, it could be summed up as \"Aphyr is right, but there is one word we don't like him using\"), the Galera engineers writing a blog post that's internally-inconsistent (SI is provided but applications cannot depend on SI semantics), and myself doing a few internal writeups on this for Bigcommerce (BC). We use Galera for a small cluster currently, and there is some interest in using Galera more extensively within our database infrastructure. Recently several BC engineers and I sat down together and spent some time trying to understand what transaction isolation levels are, what they mean, and practical ways of understanding them. Jeff Hubbard got Jepsen, built a cluster, and began playing around with it. So what have we learned? The results are somewhat surprising. First, let's talk about what Consistency and Transaction Isolation Levels even are. The four defined ANSI Transaction Isolation Levels enumerate a series of possible data inconsistencies and how they're prevented by each level. Essentially, these four levels were defined based on actual implementations of consistent databases using locking mechanisms in the mid 1900's. To quote Aphyr, who did a relatively good job of summing it up: If you’re having trouble figuring out what these isolation levels actually allow [as opposed to prevent], you’re not alone. The anomalies prevented (and allowed!) by Read Uncommitted, Read Committed, etc are derived from specific implementation strategies. ... The standard doesn’t guarantee understandable behavior – it just codifies the behavior given by existing, lock-oriented databases. Locks aren’t the only ways we can protect transactional integrity. Some MVCC databases, like Oracle and Postgres [and InnoDB-backed MySQL], offer a different kind of model: Snapshot Isolation. This hints at where the problem began for our industry. There was a bit of a shift in the industry back in the 1990's where we decided it was better to implement consistency using MVCC instead of locking, which changed the consistency model to Snapshot Isolation (which isn't any of the four ANSI transaction isolation levels), and then try to emulate the behaviours of locking databases using Snapshot Isolation minus (or plus) some extra semantics. But attempts at formalizing this shift didn't make it to the mainstream. The peculiarities of lock-based ANSI transaction isolation level implementations are different than for MVCC-based ANSI transaction isolation level implementations, and it seems none of our industry luminaries know (or have bothered to share) how. To complicate matters a bit more, there's no standard about which peculiarities are okay (only which ones aren't). The precise-sounding statement \"My transaction runs within a Repeatable Read Transaction Isolation Level,\" is actually not precise at all. There's a large class of problems you know won't occur, but there's a significant class of problems that aren't even addressed. The theory around consistency and what implementations should allow/prevent is murky, at best. The academic literature is somewhat precise (though only a few seem to have a handle on it), but our industry standards are most certainly imprecise. But that's not the worst of it. We also generally assume our favorite DBMS properly implements the transaction isolation levels claimed. But if you've been following Aphyr's posts over the past few years, you know that, in fact, that's an uncommon occurrence. Most of the time the guarantees claimed by the DBMS aren't delivered. It would appear that no-one implementing DBMSs in the past several decades has any precise technical documentation to define what they should be implementing. So they implement whatever they want, label it however they want, and (we) the consumers don't really have anything to say about it. All this is currently changing with Aphyr spearheading the effort to make the reality match the academic literature, but the process is slow-going. InnoDB implements MVCC, so that a given transaction sees the universe as a static image from the moment the START TRANSACTION statement is issued. Any other transaction that makes an attempt to read data modified by this transaction will succeed in seeing a version of the data from before the transaction began. Until the COMMIT is sent, other transactions can read the data changed-but-not-committed by the transaction in question. Depending on how poorly the DBMS implements consistency, this can (and does) lead to inconsistent data in a highly-concurrent environment. What I find interesting is that Percona suggest adding a FOR UPDATE modifier to the SELECT statements issued as part of its transaction. What does that mean? A FOR UPDATE modifier will lock the rows that are touched by the SELECT . That is to say, even though InnoDB is an MVCC-based transaction system, exclusive locks are placed on rows. This makes the implementation of the consistency model a hybrid approach, including both MVCC and locks. And if you lock rows in your MVCC-based database, you are modifying the transaction isolation level! Not just for your transaction, but globally for every other transaction that might touch the rows you've locked. I find it extremely difficult to reason about the meaning of the FOR UPDATE modifier. It's clear that placing locks on those rows will make inconsistencies much harder to work their way into the system, but it is unclear what the new (elevated/more strict) transaction isolation level is, nor how this might affect performance on a system with many concurrent transactions. If we were okay with a locking mechanism to provide consistency, why did we bother with MVCC to start with? One of the early practical operational pioneers in the Distributed Database Realm (Cassandra in particular), Rob Coli, posited the following: If you are using a distributed database, you probably don't care about consistency, even if you think you do. -- Robert Coli If it turns out consistency really isn't as important as performance, maybe that's why we implemented MVCC rather than locking. The Coli Conjecture is based on the observation that distributed databases have gained huge inroads in places where consistency is claimed to be important, yet these databases rarely (if ever) deliver the consistency guarantees they promise. Even today, Cassandra presents new bugs that show the (eventual) consistency guarantees fail in several common administrative situations, like when compactions are run at the wrong intervals, or when nodes fail during critical moments, or when nodes fail and recover unexpectedly. The Coli Conjecture is, in fact, probably an obvious conjecture in the face of historical evidence. Actual complete consistency in any human endeavour appears to be a vanishingly-rare occurance, even where you'd expect it to be the norm. In banking, accounts payable/receivable, and inventory, with nearly every company you'll ever deal with, there are errors in these systems that are corrected with a resource-intensive out-of-band process. Computers reduced the degree of the errors, but they certainly still occur regularly. And as we're starting to realise within our industry, computers, with their databases that should be predictable, in fact have a significant class of data inconsistency possibilities that aren't even defined in standards, and aren't taken care of by our databases during failure (and recall: failure is the norm). Frankly, it's surprising that our bank account balances and automated bill systems don't have more problems than they do. It seems that, when a business does the value analysis of total consistency vs performance and cost, total consistency never wins. Maybe it's wise to ask the question: is guaranteed total consistency really that important? Is it worth the cost to us? If 99.9% of the time the data is consistent and right, is that good enough? Perhaps the other 0.1% can be dealt with by human processes, like phone calls, letters, meetings, hand-shakes. There's a point of diminishing returns on consistency. Baron Schwartz pointed out that Galera cluster is actually running in production in a surprisingly large number of locations. There doesn't seem to be a huge uproar in the revelation that Galera doesn't support Snapshot Isolation (despite its claims to the contrary). Why? The Coli Conjecture is based on observations of real-world companies like Netflix, Apple, and Google being totally fine with usually-consistent databases. Maybe it just isn't that important. If you want to work on these sorts of problems, we're hiring engineers to build microservices in Go, Ruby, Node.js, and Scala on AWS.", "date": "2016-04-04"},
{"website": "BigCommerce", "title": "Advanced MySQL Basics", "author": ["Tim Ellis"], "link": "https://www.bigeng.io/advanced-mysql-basics/", "abstract": "About once a year, there's a blog post from one of the current Tech Unicorns about how they had a big data problem and solved it with a slightly non-standard usage of MySQL. This is a really good thing, because MySQL is an extremely versatile tool. Assembled in a modular fashion and with open source, it's perfect as a robust, well-understood component in a larger architecture. That said, I am often surprised at the basic mistakes and misunderstandings about MySQL of very smart and advanced tech professionals. It's become fashionable not only to discard the relational aspect of the database, but to be completely ignorant of it. It's a trend I cannot get behind. It's better to reject something based on an educated opinion than simply because you don't understand it. Therefore, I've put together this basic high-level overview of MySQL for the software professional of 2016, geared toward those that want to use it as more or less than a relational database, and potentially as a non-relational datastore. In it, I present several rules-of-thumb for how to use MySQL in the ways I've seen these Tech Unicorns want to use it. In each section, rules of thumb are quoted sections of text, like this. Rules of thumb are not laws of nature. They have exceptions. A good rule of thumb will, however, lead down the path of least resistance most of the time. Let us begin! This is something that crops up a lot. It's actually a great idea. MySQL is an extremely fast and stable storage engine and, if you use it properly, will beat out a lot of the current best-of-breed NoSQL K/V options. Here is an example of an organization using MySQL as a K/V store: Wix Engineering Posting . The crew seems intelligent and well-spoken. I'm glad they wrote up their thoughts. Although this post will read a bit like a take-down of their efforts, please resist the urge to think of it that way. Theirs is just a fairly recent and extensive posting that will serve to illustrate some of our points. The Wix team must have read some of the early distributed database literature, since they formed a rule about never using a JOIN. However, they are so afraid of joins that they build bad queries. One of their queries looks like this: There is no possible way this could be more efficient than just joining the two tables together (though it is possible that one day MySQL will figure out how to optimize this by automatically turning it into a JOIN, due to the = before the sub-query). They understood JOINs in your application code prevent putting the two tables on different servers. But due to blindness of their hard rule \"NEVER JOIN,\" they failed to understand that if the two tables are on the same server, and you're going to optimize the query, doing so with a JOIN is better than with a sub-select. It's a good rule, too-strictly followed, and improperly broken. Here's the proper way to do it, with a JOIN: When you want to use MySQL as a distributed K/V store, and you want to be able to split tables across servers without touching application code , that's when the \"no JOINs\" rule becomes unbreakable, but by the same token, so then would their WHERE id = (subselect) formulation be illegal. Have one process SELECT the IDs out of the first table into your application memory, then use the ID list to generate a SELECT ... WHERE id IN (id1,id2,...,idN) for a series of records from the second table. This makes the tables independent, and they can live on physically separate machines. If the two tables live physically on the same server, and you want to optimize for that fact, use a JOIN. You will need to modify application code when you split the tables onto separate servers. You probably also want to visit the section below about making MySQL into a distributed data store, and the second section below about using MySQL as a K/V store. But first, let's address another issue the Wix team grappled with. The Wix team is trying to make a system with no single points of failure. This is a laudable goal. However, they choose an Active/Active(/Active) setup to achieve it. This is a mistake (not to pick on the Wix team - I've seen many make this same mistake). One of the objections they have about auto_increment columns (they call them \"serial keys\") is that they interfere with active/active replication. This isn't the only problem, nor even the worst , with auto_increment columns. The auto_increment_offset and related parameters trivially solve the problem they're probably alluding to. Their obsession misses the point in two ways. The first and minor way is that an integer primary key is a MySQL-specific optimization that comes from the fact that index leaf nodes store the entire primary key for each row pointed to, meaning a large primary key uses more memory and I/O. Also, using an incrementing integer keeps rows inserted around the same time clustered together on the I/O medium. In MySQL, it is most efficient to assign a simple incrementing integer as the primary key for the table. To indicate row uniqueness, use an additional unique key. The second, more egregious way the point is missed is that Active/Active replication with MySQL creates inconsistencies in your data. Why isn't this more-often noted by MySQL vendors? I will note it now. MySQL (default asynchronous) Active/Active replication almost guarantees data inconsistency. How? MySQL is asynchronous replication, and creating an Active/Active setup creates a split-brain problem... on purpose. Even if the network is 100% available, what happens when two clients, each talking to a different master database, are trying to achieve the same thing? For example, both are trying to add the same user, or change the same data about the same user in a different way. Data inconsistency is nearly guaranteed and, to make matters worse, your application probably won't get errors, so you won't even notice your inconsistencies for a long time. This is a well-studied problem, and the upshot is you, your app, and your data aren't going to be happy. The proper way to avoid single points of failure while keeping the performance of asynchronous replication is Master/Slave/Slave using GTIDs or a tool like MHA that will allow you to automatically fail over to a Candidate Slave when the Master fails. Why Master/Slave/Slave? When you fail over from the (failed) Master to one Slave, you'll want the other Slave to build a new Master without downtime. For a fault-tolerant asynchronous replication cluster, choose Master/Slave/Slave (active/passive/passive). When the Master fails, fail over to one of the Slaves, then immediately begin building a second slave from the remaining good one. If you automatically fail over to a Slave (itself a dangerous action), you should still alert a human to wake up and come build another slave. If you lose a second machine in this cluster, the rebuild requires some down-time or degraded performance. If you have LVM, XtraBackup, or other instant-snapshot tools available, your performance will be degraded during the rebuild. If you don't have those options, you'll need downtime! If a Master fails in your three-node replication cluster, immediately rebuild another node, for another loss will result in degraded performance or down-time for the cluster. Also, it can be tempting to automate the whole setup. If the Master fails, automatically fail-over to a candidate Slave. If the Slave then fails or performs badly, and the Master seems okay, automatically fail-back to the Master. Resist this temptation. You will build a system that spends hours happily failing over and back again while the service that depends on the database cluster is 0% available the entire time. There is a rich history of cases where bright engineers built such a system and regretted it. Don't join their ranks. Never automate the fail back. Fail-overs from Master to Candidate Slave: automatic is hard but doable. Fail-backs after things have settled down (or the Candidate Slave has also failed): always with human intervention! Also, always alert a human when a fail-over happens so they can fix the cluster state. Also, it is often bad to leave the old failed Master in a usable state. Ensure it cannot take writes however you must. The safest method is to have the new Master shut off its power from the PDU, though admittedly, I have only rarely seen this level of thoroughness implemented, as it is also disconcerting. When the old Master fails, be sure it cannot take writes while the new Master is acting Master of the cluster. In general, implementing an automatic Master failover system is complex, error-prone, and time-consuming. Unless your org has defined a very strict SLA and given you the resources to follow through with a multi-month plan to implement a failover, you probably are better off just making it 100% manual. Automatic Failover is hard. Weight each of the following for your org: How often an unplanned failover might happen (in my experience, once every couple years in a reasonably high-volume shop). The impact of the minutes of downtime while waiting on the human to fail it over (in my experience, 3-5 minutes). The risk involved with attempting an automatic failover (in my experience: great, potentially creating a multi-hour outage). Alright, enough about failover, and back to the point of this section. What if your org simply must have Active/Active? There is a way, though you will be losing asynchronous replication, with the added latencies and potential locking issues that come with it. If you absolutely need Active/Active replication, choose a Galera Cluster solution. Every master is active with synchronous replication. Continuing from the above section about using MySQL as a K/V store, let's take it a bit further. You're already abandoning the relational and transactional portions of MySQL. Why not abandon the SQL parsing overhead as well? There's a low-level interface into MySQL that allows you to write and read rows without any SQL at all called Handler Socket . Using it you can achieve some amazing read/write throughputs . The benchmarks from that 2012 project have it performing about 1.5x faster than memcached. If raw handler socket coding is too confusing for you, but memcached is something you understand, you can get almost exactly the same phenomenal performance using the MySQL memcached extension , which makes MySQL look and act exactly like memcached, but with persistence. If you're using MySQL as a K/V store, strongly consider abandoning the SQL parsing portion and directly storing/retrieving values. The memcached protocol support can dramatically simplify this. One of the easiest ways to make a distributed datastore backed by MySQL is to combine the MySQL memcached extension, mentioned earlier, with a memcached routing mechanism like Facebook's McRouter . With just these two tools, you can build some seriously powerful distributed NoSQL clusters. You also are given some tools for creating redundant data storage, and so may do away with MySQL's replication entirely. Now you have a highly-available highly-performant distributed K/V store. But if you want to go with something a little more traditional, there is the YouTube Vitess project that will make a big fault-tolerant sharded distributed database backed by MySQL for you. And it's written in Go, so you have your excuse to play with golang! If you need an architecture that's a little more traditional, you can always use the Tumblr Jetpants tools to build a large fault-tolerant sharded MySQL cluster. I've worked with it before, and it was a pleasure. Keep in mind these projects were not easy to build. Not even the apparently-simple ones. One group of engineers I worked with mistook this for an easy problem and tasked a couple junior engineers to building a distributed MySQL-backed database in 6 months. The upshot? 1 year of wasted engineering resources, several months of wasted operational resources, an unstable, unworkable cluster, and eventually the entire codebase completely scrapped. This rule-of-thumb shall be a warning... If you want a MySQL distributed datastore, use an established technology. Do not write your own. Down that road lies disaster and untold pain. If you must travel the road, go prepared. Several of your most talented engineers will need a couple of years to travel it. Recently MySQL added the capability of storing JSON-based documents . This means you can get the stability, speed, and scalability of MySQL without the common data loss experienced when using MangoDB which, though arguably the fastest database available as of this writing in 2016, leaves a bit to be desired in its persistence layer. The past 8 years have shown that document stores tend to be long on PR talent and short on tech talent. This leads us to our rule-of-thumb: When choosing a document store, ignore hype. Look for stability, uptime, and scalability. Look for proof of these elements. Unfortunately, that means in this particular case, MySQL isn't the obvious win. The JSON capability is pretty new. I'll be paying attention over the next year for evidence that MySQL's JSON document store feature is fast and stable. If history predicts the future, we have great things to look forward to. In the meantime, trust, but verify. For complex technical reasons, there are multiple ways to try to replicate only part of a database to a slave. You want to ignore most of them and stick to the wild methods. For replicating only some of a database, stick to e.g. the replicate-wild-ignore-table option. SQL is a set-based language and there are two natural ways to solve each of the two types of problems \"get all the rows from one table where there is (or is not) a matching set of rows in another table.\" You want all the rows from one table where the IDs are in the second? Here are the two ways to do that: Similarly, the two ways to get all the rows from one table where the IDs are not in the second: And here is where the problems begin. The first syntax (using IN or NOT IN clauses) is more intuitive to every single person I know. It's more intuitive to me, and I see it used by Engineers everywhere all the time. I suspect it's universally more clear to humans. But MySQL traditionally (and to some extent today) is more efficient at planning and executing the second query . That is to say, when you have a choice to do an IN or NOT IN clause, but could possibly reformulate it as a JOIN (INNER JOIN for IN clauses, OUTER JOIN for NOT IN clauses), you should. This is the go-to advice of MySQL experts currently. It looks like MySQL 5.7 improves the situation , but you should not count on this until you've seen benchmarks. I hope one day we can finally retire this go-to piece of advice, because IN and NOT IN are so much more intuitive to humans. I expect this to take several years. I'd be surprised if it's fixed before 2020. Replace IN and NOT IN clauses with JOINs when possible. Here's the quick history of MySQL. First, there was MySQL AB, founded by Monty Widenius and David Axmark. They made MySQL good. Sun Microsystems bought MySQL AB, and a few of the core engineers split off to form their own company, Percona. Oracle bought Sun, and Monty split off to form his own company, MariaDB. An ASCII diagram is required at this point: Percona are several of the earliest and best MySQL engineers, and as far as an alternative MySQL goes, it's been the #1 choice for 10 years. MariaDB , although founded by the first MySQL fellow, is actually a more recent addition to the game. Percona patches Oracle's MySQL to get better performance. Maria tends to add entirely new features to Oracle's MySQL. If you really want to live on the bleeding edge, and forego support entirely, there are other projects, like Webscale MySQL headed up by some of the biggest names on the internet. They provide patches of code that help if your throughput is measured in petabytes instead of terabytes. \"Never got fired for choosing ______\" safety comes from Oracle. Better be lunching with your CFO every day. Some of the best MySQL engineers that ever existed with reasonable support terms come from Percona. The special sauce MariaDB brings to the table (mainly, the Aria engine ), along with a name that will never give up on MySQL might be your thing. If you want to dedicate engineers to supporting your own MySQL variant, Webscale MySQL is a great choice, and there are many others. Special thanks to my friends of the digg diaspora, at Krux and elsewhere, for their input to topics covered, for proof-reading, and generally tolerating my obsessions over technical correctness. Without them I don't think this would ever have been written. And all the other names that are left out due to my short-term memory.", "date": "2016-06-13"},
{"website": "BigCommerce", "title": "Building a Global Engineering Team", "author": ["Ron Pragides"], "link": "https://www.bigeng.io/building-a-global-engineering-team/", "abstract": "dawn of a new day At the beginning of 2014, I was looking for a new challenge. My two most recent roles were at fast-growing technology companies which became household names. I enjoyed their successes, but felt the urge to join a much smaller team. I wanted to build . I was looking for a pre-IPO technology startup that had the potential to be the Next Big Thing. I was already in final rounds with other startups when an e-commerce SaaS company contacted me. Bigcommerce already had a great product/market fit, an established (paying) customer base of nearly 50,000 merchants, and impressive YoY revenue growth. The more research I did, the more excited I became about the opportunity. Bigcommerce was cited by Inc. Magazine on their list of fastest growing private companies in 2010, 2011, 2012, and 2013 (spoiler: it happened again in 2014). The company also had an aspirational goal that resonated with me: to democratize e-commerce for merchants around the globe. I was thoroughly impressed with the intelligence and passion of co-founders Eddie Machaalani and Mitch Harper . They established their company in Sydney, expanded to a 2nd location in Austin, and were planning a brand new office in the SoMa district of SF. I visited the SF office shortly after the lease was signed, and had one thought: potential . In the span of a few days, I went through a flurry of video interviews with the leadership team in Sydney and Austin. This team moved fast! Everyone I spoke with was ambitious and intelligent, but also understated and down-to-earth. Bigcommerce has a word to describe this rare combination of humility and ambition: humbitious . I soon found myself in Sydney to meet the team in-person. All of the conversations I had with the leaders and the engineering team reinforced the feeling that I should join this company . I decided to seize the opportunity and see where it would lead. think big I arrived back from my visit to Sydney in a bit of a haze. I’d committed to leading the engineering team and to staffing a new SF office from scratch. That would mean hiring engineers in one of the most competitive markets on the planet. It was a big challenge. Think Big is a motto at Bigcommerce, and we had it painted on the wall of our fledgling SF office. It seemed appropriate. Our executive leadership team was aligned: hiring for the new office was a company priority. Bigcommerce started to get press coverage about our ambitions to grow the team, but that wouldn’t be enough to get noticed in the SF technology market. We launched a recruiting campaign that was sure to make waves in SF: marketing our jobs at the shuttle stops of established tech companies. We handed out breakfast along with cards that declared: Bigcommerce is hiring engineers in San Francisco. Get off the bus and onto a rocketship! Our campaign was creative and fun. It got the attention of the press, career sites , and (most importantly) candidates across the Bay Area. To help with recruiting and interviewing we invited our engineers from Sydney for extended stays in SF. A beneficial side effect was improving the time overlap between corporate HQ in Austin and the engineering team. Staffing a new team from scratch was exhausting. Every day was a constant stream of sourcing, phone screening, interviewing, networking, and reviewing resumes. I was selling the mission and the future of the company to every single candidate. Although tiring, it actually wasn’t hard._ I was so convinced of the opportunity at Bigcommerce that I made the leap of faith myself._ With some members of the team now in SF, we started a rhythm of inter-office standup meetings (caveat: we are still trying to improve A/V quality). These meetings started as a daily routine, but we’ve since scaled back the frequency to a few times per week. This strikes a good balance between transparency and having enough content to make the meetings worthwhile. The earliest days in the SF office were daunting. Would we be able to hire great engineers? How quickly can we staff up the team? Hiring the first traunch of engineers was the toughest hurdle. After that point, we were able to get a snowball effect and draw others into the company. I’ve always found employee referrals to be the best source of qualified applicants. Of course we offered a referral bonus, but once again decided to do it in a creative and fun way . As we grew our engineering team, I shifted my focus to staffing great leaders. One member of our Sydney team was promoted to Head of Engineering for the Sydney office. For the rest of my team, I looked for accomplished technology leaders who were specialists in their area. But that wasn’t enough. I also wanted a leadership team that would work well together . I was hiring not just for competency but also for culture fit. At Bigcommerce, we call this hiring for both bucket 1 (competency) and_ bucket 2_ (culture). After passing over many candidates who were qualified but not the right fit , we eventually assembled an amazing leadership team . It’s the best group of leaders that I’ve had the pleasure of working with. change people’s lives We’ve grown our SF presence significantly in the past 9 months, with recent great additions to Engineering, Product and Program Management. There are now 75 people in our SF office! In fact, we’re moving to a bigger office in the next two weeks to accommodate our future growth plans. The broader team at Bigcommerce also continues to impress me. Our executive leadership team is top-notch, humbitious , and incredibly aligned. Every week I’m so thankful for the support I receive from each member of the group. We’ve accomplished so much together in 2014 in our product and our business. Our phenomenal team also includes our investors and advisors. This year we closed our $50M Series D which expanded our investors to SoftBank, Telstra, and American Express in addition to General Catalyst and Revolution. We also expanded our advisory network to include Chris Fry and Patty McCord . In November, we had our first simultaneous Hackathon across all three of our offices. The Hackathon had participation among 85% of our engineering team and was a roaring success. Our CEO made it to the SF office for the presentation of the Hackathon awards (and requisite photobomb). I’m especially proud of the performance of our team and platform this Holiday season. During the critical period from Black Friday thru Cyber Monday, our platform had 100% uptime for our merchants. In addition, we observed 300% more traffic for merchants on our platform. By providing a solid platform during the holiday season, the engineering team delivered on the Bigcommerce mission statement: Be the #1 e-commerce provider in the world by creating and delivering software that changes people’s lives. our destination in 2015 In December, we had our first R&D Leadership Offsite in SF. We discussed where we want to be as a team at the end of 2015. The conversation was fruitful, and led to specific goals across: Process , Product , Platform , and People & Culture . Here’s our list for People & Culture: It’s a short list, and will be something we strive to achieve every day. We will continue to grow and refine how we work together as one global unified team. I’m looking forward to 2015. If it’s anything like this past year, it’s going to be a blast! Bigcommerce R&D in SF Bigcommerce R&D in Sydney (and a few visitors from SF) (this post originally published on Medium )", "date": "2015-01-09"},
{"website": "BigCommerce", "title": "Iteratively Decoupling Legacy Architecture", "author": ["Adam Benson"], "link": "https://www.bigeng.io/iteratively-decoupling-legacy-architecture/", "abstract": "TL;DR: Move Service Locator calls up the object graph iteratively. ALSO: Poor Man's Dependency Injection breaks testing. Avoid it. More and more, application architecture is migrating towards Dependency Injection to decouple components and ease testing. One of the challenges of applying this to existing applications is how to get the dependencies that an object requires to that object during construction. By design, constructor dependency injection means any object that instantiates another object also depends on the dependencies of the objects they create. To illustrate this, let’s say I have created a ReportService , which creates an InvoiceQuery , which depends on DatabaseService to load query data, and then return a new Report object. As the ReportService is directly instantiating the InvoiceQuery object, it needs a reference to DatabaseService to pass to its constructor, and thus itself depends on DatabaseService. This is one of the reasons factories and DI Containers are so useful - the ReportService shouldn’t need to depend on the InvoiceQuery’s own dependencies, so instead we can move this dependency management into the container for objects that are shared through the application or into factories for objects like our Query which are dynamically instantiated AFTER bootstrap Refactoring an object to use dependency injection that is already used ubiquitously throughout the application and currently fetches its dependencies through service location becomes particularly challenging. One of the patterns often employed to try to solve this divide between dependency injection and service location is allowing objects to optionally receive their dependencies or obtain them via Service Location if they haven't been provided. We can think of this as Poor Man's Dependency Injection, as illustrated below: Specifically, the constructor of the PoorMansQuery above includes the following line: $this->db = $db ?: Services::getDbMysqlClient(); Now when we write our Unit tests for PoorMansQuery we can optionally create a mock \\Db_Mysql object and pass it in - successfully decoupling our code and using Dependency Injection, right? Unfortunately, whilst this does allow the PoorMansQuery object itself to be tested in isolation, by allowing the dependency to be optional any class which doesn't explicitly provide this dependency is now implicitly coupled to the service fetched from the container. Instantiating the object is a tight coupling (we can't mock PoorMansQuery as it’s directly instantiated by the PoorManService), and as PoorManService doesn't provide the dependency to the Query's constructor, the service locator serves this dependency from the container - thus making it difficult to unit test PoorManService in isolation from the container. Not only this, the ternary operator within PoorMansQuery represents two behavioural pathways that our object supports. Unless we exercise the \"missing dependency fetches from container\" behaviour, we're not testing all the behaviours our object provides and thus haven't completely tested PoorMansQuery. The fallback can be to bootstrap/setUp a dummy container containing mock dependencies and getting the Service Locator to instead receive this mock. Whilst this does provide a workaround, it’s cumbersome and fragile: we’re fiddling with global state, which risks breaking other tests if you forget to undo replacing the container, and you're still effectively coupled to a container. The recommended way to avoid being coupled to a container is to construct all your dependencies during application dispatch via the DI Container. The challenge when refactoring existing legacy application code is this requires all objects that depend on the object you're creating/modifying to also be constructed via the DI Container, and so on and so forth all the way up your object graph, which can be a massive undertaking. However, whilst you may not have time to refactor the entire object graph, consider moving the Service Locator call up a level into where the objects are being constructed. As per the following refactored example: This still doesn't decouple PoorManService from the container/Service Locator - and testing PoorManService will be equally challenging. Upon the next iteration, a developer working with the PoorManService can then \"bubble\" these Service Locator calls up to the next level and so on - thus decoupling PoorManService and allowing it to be unit tested in isolation. This also means we don't have to test the \"missing dependency\" behaviour provided by the PoorMansQuery as it no longer exists within the Query itself. Iterative bubbling of Service Locator calls eventually brings service construction to the dispatch boundary, allowing us to move those dependencies into the container, greatly simplifying dependency management throughout the application. Eliminating Poor Man's DI is an excellent first step towards this goal.", "date": "2017-01-05"},
{"website": "BigCommerce", "title": "How we use Sass Maps for Design Tokens and Developer Happiness", "author": ["Simon Taggart"], "link": "https://www.bigeng.io/how-we-use-sass-maps-for-design-tokens-and-developer-happiness/", "abstract": "When building a design system there is always a set of global, shared properties that become the basis of everything that gets built. This is part of the atom level of the \"Atomic Design\" principle – the font sizes, weights, line heights, colours, borders, background, spacing, sizing and z-indexes that are the very core of every single piece of Interface in your UI library. If you are particularly intent on building a system that promotes visual consistency across the entire application, and you really should be, then spending time defining this set of properties is invaluable. The Salesforce Lightning Design System call these \" Design Tokens \", and are a perfect candidate to become shared in your codebase for re-use; as they have done in defining Sass variables for them. Here at BC I've already mentioned our affection for Sass maps and map functions and that is especially relevant when we start considering sharing design tokens across our apps. I don't know about you, but no matter how good or clever your variable naming conventions are, naming stuff is hard and I find them invariably impossible to remember. You might use a full blown IDE that supports autocompletion, but I don't and we're really fond of building tools that help people without forcing too many opinions onto peoples workflows. Use whatever tools make you productive and we'll try and solve problems with some common sense and predictability. That's why we think following the advice Erskine Design had for Friendlier Colour name with Sass maps can also be applied to things outside of just colours. It just makes sense. To quickly cover the approach again, we can create a function is Sass that takes an argument, which matches the key in a specific Map, and return the value of the key: fontSize(\"large\") for example. To create something like this you firstly define the map: Then you would create a function which takes a single argument of \"key\", that is matched to the corresponding map: Now fontSize(\"large\") returns 20px and fontSize(\"small\") returns 14px . This can be extended really easily to take a second argument, which is super useful if your design token, like a colour, is split further down into tones. Again start by creating a map: With this added depth in the map, we can make a minor adjustment to the example function above, so it can take a second argument to return the tone of the color we specify: Now color(\"primary\") would return #00abc9 due to the default value of $tone being set to base , so no need to pass it in. color(\"primary\", \"dark\") would return #009cb7 . Super useful! This is starting to feel a bit more manageable, and if we couple this with a simple convention our developer experience should become a lot better for those who really don't get along with CSS very well. We first started by defining what types of design tokens we needed and settled on a camelCase naming convention, which essentially maps to the name of the CSS property it relates to (where ever possible). So we ended up with a list of design tokens that looks like: We then set out how we would define the variation in each property, based on a predictable scale of comparative adjectives that best relate to the type of property. Large or Small, Dark or Light, High or Low. Some examples might be: lineHeight(\"large\") , zIndex(\"lowest\") , color(\"grey\", \"darker\") . As you can imagine people find this incredibly easy to deal with and remember, it's one of the favourite features our JavaScript engineers like about our CSS framework. Not only is it easy to remember but it helps build consistency and predictability into our code. Basing values on a set scale makes dealing with something like z-index on a large application, so much easier. No more fighting magic random numbers! Obviously from the list of design tokens, not all of them fit the scale so we have a couple of special cases that align more to their specific concerns or values. Font weight makes more sense to follow the real weight values you can set in CSS, for example. To make things easier to create sizes and spacing in our UI, we use a fraction based scale: single , half , quarter etc The Spacing map is pretty special and we'll be covering that in a lot more detail in another post, but in brief; any declaration of padding, margin and positioning should only ever be one, or a combination of, any of the units listed above: padding: spacing(\"half\"); . Our final map is \"container\". As with a lot of websites and applications, the practice of \"containing\" content or information is pretty standard practice. We use panels, wells, modals, tables, forms; all kinds of concepts in any one application to contains parts of our user interface and consistency in how they look is key. So to make sure all our \"containers\" use the same background fills, border styles and drop shadows, we use a Sass map to help us. A typical implementation of a contained component might look something like: Obviously if every contained component were always that consistent, we could just abstract that into it's own re-useable container style, but in reality a lot of our containers might share similarities but have slightly different combinations and variations of those properties. Our panels for example only have an internal border, and the bottom drop shadow, but those styles are shared with our tables which also have borders all the way round (the same borders) and they both share a background colour. If we wanted to change how we visually treat containment, the chances are we'd change both panels and tables equally. They do a similar job, they're just treated slightly differently based on their use case or data type. We've found this is a really great way to deal with these kind of design tokens without engineers re-inventing the wheel in every project, and potentially having interface components becoming visually out of sync across the application. In summary, the way we've decided to use Sass maps and map functions to handle our design tokens has been incredibly successful. Our engineers love the simplicity it brings to their work flow, reducing the amount of decisions they need to make, and find them ridiculously easy to remember. The consistency it brings to our code is extremely useful, especially when you have had to struggle with magic numbers an engineer may choose when solving a particular, but common problem. The predictability in knowing the range of pre-defined values a property could only ever be, is really handy in debugging z-index issues for example. Often the constraint it places on the design of our components and patterns also really helps us think a lot more about the quality and shared visual consistency across the entire application. It reduces decision fatigue and creates a really solid feel to our component library. Give it a go, maybe you'll find the same kind of successes we did in implementing a similar system in your codebase. We're currently hiring for amazing Front-End Engineers that understand these principles to join our UI Platform team. If you're experienced in creating and shaping user interface guidelines and you like what you read, get in touch .", "date": "2016-02-20"},
{"website": "BigCommerce", "title": "The “Living” Style Guide - Pattern-Lab", "author": ["Simon Taggart"], "link": "https://www.bigeng.io/the-living-style-guide-pattern-lab/", "abstract": "Front End style guides have been well publicised recently, with many examples being made available for all to see . They're a popular and effective way of creating usage guidelines for Interfaces that adhere to an agreed upon design ethic. They help you speed up, maintain, and keep UI consistency. At BigCommerce, we've started to think about the best way we could implement something similar that would help us achieve those goals. Style guides in our experience fail most of the time as they become stale reference sites that people copy and paste from but never update. They can be too loose,  and allow too much freedom; effectively you skin Bootstrap and everyone thinks they're a designer. There are some situations of course, where you're lucky enough to be able to dedicate resources and a team to solely working on your style guide and shaping its direction. Many of us aren't that lucky, but if by some miracle you manage it, the next most likely thing that will ruin all your dreams is mutations . A lack of an effective integration guide will cause your style guide to become obsolete, as teams will deviate from your agreed approach, making any upgrade path impossible. These mutations more often than not appear because it's really easy to copy, paste, add an additional CSS class to a component and create some variation of it from the core library. Often the excuse is \"it didn't quite feel right\", or \"just because\" but this has a serious detrimental effect on your ability to maintain a consistent look and feel, and familiar interaction patterns in your application or website; especially if you are dealing with hundreds of pages. Those small mutations mean any significant change to a component won't roll out. The changes you've made will mean new upstream CSS changes will not take effect due to the cascade and you'll quickly create fragmentation and a frankenstein of an interface, introducing technical and design debt to your system. This essentially leads to the origin of the naming – a Laboratory, not a Library. This takes a lot of inspiration from the Pattern Lab project and the Atomic Design system. Essentially, this thing should live . It shouldn't just be a library piece; something that sits on the design departments shelf to look at every now and then. It should be constantly worked on and iterated over. It should enable us to push new ideas on our design patterns to our app quickly and easily, which means it's necessary that we deal in and produce production code. That production code is not only used to build the pattern-lab itself, but also the application it describes. A single source of truth, what you see is what's in production and what the Engineers on your team are using. 100% confidence. A Pattern-Lab is a set of very well defined, strict guidelines and patterns designed solely for a very specific domain. It's not a one-size fits all framework to build something BigCommerce-esque, but a razor-sharp, narrow focused toolkit to build a particular part of the BigCommerce eco-system; in particular, for our Control Panel. We specifically wanted to set out that if an internal team was building something that's not the control panel, and they found they were missing significant chunks of UI, rather than doing a hack job they may benefit from building their own Pattern-Lab for that specific domain. A Pattern-Lab is designed to only have what we want the delivery teams to have to build that domain. Nothing more, and hopefully nothing less. We plan to build a Pattern-Lab to serve each domain separately, as they will all have different design aesthetics based on their purpose, but most likely a very similar set of core components and patterns. That's where Citadel, our CSS framework, comes into play. More on that in another post. A Pattern-Lab probably get's you 90% of the way for each domain, but that's not to say a Pattern-Lab is inflexible. Product Managers and Designers love a \"Snowflake\", so we expose all the global and component maps, variables and mixins, with their corresponding Pattern-Lab \"theme\", to enable a consuming app the ability to build custom Components, which look right at home and on brand with the wider app domain. A Pattern-Lab is a joint effort between the design team and engineering, but it is certainly not an engineering tool. It should be lead and owned by the design team. The Pattern-Lab shouldn't just be a documentation site where engineers pick out code snippets, although code snippets are included. It should include the design thinking and language behind the \"why\" and the \"when\" a particular pattern should be used. Why you should use a colour for a certain situation, when you should use a certain level of alert, the situations it's appropriate to use a modal etc. Other areas worth considering are defined animations and rules around how and when to use them in the application or interaction. Again, the Pattern-Lab should describe how to build the best possible experience for our product and for that to be a success, it requires rules around how to use it. Think of it as an appropriate use guideline, or an interaction playbook. We're not quite there yet, but we're well on our way to defining a solid design language and guidelines on how to implement it in our app. The work by Lonely Planet on their \" living style guide \" Rizzo , seems like the natural progression in the style guide world, so we set about defining a way that we can do the same within the context of how we currently build our Control Panel. The beauty of modern JavaScript frameworks, React, Ember and what we use at BC, Angular, is that they're all aligning pretty strongly to the concept of Web Components. Directives, in Angular speak. This allows us to define small chunks of our UI into standalone, dynamic pieces, that we can then stitch together. It offers a number of advantages, in that each piece is testable separately from other components. We get to abstract complex HTML patterns, semantics and accessibility attributes away from JavaScript engineers who just need implement custom tags or elements into their apps. With that, we get full control of that HTML as it's compiled into JavaScript objects, meaning we can update a component and rollout it out without the dreaded \"find and replace\" across thousands of lines of slightly differently formatted HTML. The JavaScript engineers also get to only concentrate on the things they really care about – application logic, architecture, and performance. The UI and design teams get to own a significant chunk of user interaction and accessibility best practices, which is exactly what they care about the most. We're now getting closer to solving the problem of our style guide going out of date, by building components for our guide that get used in production. Solving for mutations is a little more tricky, but from my observations, engineers  are less inclined to add a CSS class to a custom element when they could just paste in \"normal HTML\". Which is somewhat of a win. Because it's JavaScript code, Sass and compiled CSS we can start to think of our Pattern-Lab as a small software project. With that comes the ability to create releases and version tags that we can use to help distribute our code and provide a controlled method of upgrade to our apps by treating the Pattern Lab as a dependency. The neat thing about a versioned design language is that it gives the teams some advantages in control, predictability and workflow. By adopting Semantic Versioning (or SemVer), we give our teams control over how they work with our design language by deciding what version they are using for their particular product area. They get predictability as they know we're not going to push a change to them that will break all of their things without them knowing, which would make a very unhappy and stressed engineering team. Predictability and control leads to confidence in your design language and more stable estimations for the project teams, which is super important in enabling them to ship things on time. With confidence comes adoption. Teams are willing to include the design language in their product area, and because change is predictable, are actively engaged and encourage the evolution of those patterns to make their product areas better. WIN! Another strong feature of versioning and distribution is testing. With good design comes iteration as you learn and refine your patterns to create to best possible product experiences. Componentisation not only allows you create quick, high fidelity prototypes for your usability and design research, but it can also potentially allow you to split test versions of certain components as you release beta or pre release versions of the Pattern-Lab. We're somewhat lucky in that we're aiming for a more service orientated architecture in our main app, which also includes Front End code. We build single page apps for each section of the control panel as we systematically upgrade the platform, using common core libraries to build each \"micro-app\", and then stitch them all together to make one Control Panel. This means we can potentially run minor updates to our Pattern-Lab in certain parts of the app and not others. What that allows us to do is feed in improvements to a particular component in the Lab, based on the research and testing we've done. We can split test that update in a small part of the application with real users and track any improvements to task completion that we identify as important metrics. This could be huge when thinking about a long term design language where we slowly iterate over design rather than attempting a \"big bang\" re-design every 3-4 years. No users like big changes and giving us real data in a low risk manor on our design thinking is a massive win towards a happy product experience. Have we made much progress towards our utopian future? Not exactly, no. We've definitely made progress, just nowhere near as much as we would have hoped for, for a number of reasons. We have a fairly broad set of building blocks; elements and slightly more complex components which we've used to build a number of new sections of the app with great success. But what they're really missing is the glue that's used to stick them all together into more rich patterns and layouts. The rules, the guidelines, the do's and don't. In \"atomic design\" speak, we've built a bunch of atoms and molecules and not a lot of anything else, and what we're finding is that that is the really important stuff to making our teams successful. Over the coming months we'll be addressing just that, to help our project teams build more high fidelity features, more quickly and with less guessing. In terms of the technology choice itself, we might start to rethink the whole approach. We're still convinced that components are the best way forward, but we're struggling with adoption, since only about 5% of the app is converted over to Angular. As a UI platform team we're pondering if Angular is the best choice for us to be providing our components in. Mainly because Angular is so \"all in\" on the \"Angular way\" of doing things, it really leaves no flexibility in implementation. Even if your project is super simple, it has to be a full blown single page Angular app, with all the abstractions and conventions that come with it. Some of our teams are resource and time lucky, meaning a full re-write in Angular is completely feasible. Other teams are not so lucky; they may lack time or Angular expertise making a full re-write seem overkill or unnecessary. I think we should definitely still provide JavaScript components but perhaps the \"framework\" buy-in is holding us back. I'm personally fairly confident that a more lightweight utility; a view or component library like React , Riot or Skate , would be better suited to our needs. Simpler abstractions, boiled down to encapsulated custom elements which consist of plain DOM elements, coupled with event and callback hooks that respond to users interacting with them. Whatever JavaScript framework you use (or don't use) to control the data layer, can then hook into these and starting piecing them together to build apps. Essentially just a collection of serialisable DOM elements, with a nice developer API and no framework specific conventions to opt into. Pattern-Lab itself then becomes a much more self service tool and really wouldn't  care about your technology stack. That's obviously a pretty big stretch goal and the project as a whole is super ambitious, but that's not going to stop us from trying. For a sizeable app like BigCommerce we think it's super important in enabling our product and engineering teams to deliver new and improved features quickly with less re-inventing, and keeping our UI manageable. We're currently hiring for amazing Front-End Engineers that understand these principles to join our UI Platform team. If you're experienced in creating and shaping user interface guidelines and you like what you read, get in touch .", "date": "2016-03-04"},
{"website": "BigCommerce", "title": "Why the way we look at technical debt is wrong", "author": ["Shaun McCormick"], "link": "https://www.bigeng.io/why-the-way-we-look-at-technical-debt-is-wrong/", "abstract": "I’ll be straightforward at the start of this post: I firmly believe that the most important issue a company deals with is how it reconciles itself with technical debt. Period. Now, here’s the real point of this post: Technical Debt is a Positive and Necessary Step in software engineering. First off, let’s define technical debt for the purposes of this post, as the words can mean many different things to different people. I view technical debt as any code that decreases agility as the project matures . Note how I didn’t say bad code (as that is often subjective) or broken code. A good example is this: you have, say, a shipping application that doesn’t have 100% test coverage, has many classes violating SRP , and has poor domain modeling. That’s not a bad thing. Why? The Most Important Thing is Getting the Thing to the User Startups fail, often. Actually, scratch that. Startups fail, nearly always. The reasons for this are legion, but more often than not in my experience the reason is speed . Systems take too long to get to market, which causes marketing efforts to stall or become irrelevant, and competitors beat out the startup to an offering. Often I meet engineers who want to build their own company where they can “do software right”. They’d build their own application idea from scratch, doing TDD and 100% test coverage; it’d be widely scalable; it’d have flawless class modeling; it’d use the latest technologies and have robust APIs; and it’d just generally be a system that any engineer who laid eyes on it would suddenly fall down on their knees and claim “we’re not worthy!” This is a flawed worldview. Tech businesses aren’t successful because they had perfect code and systems. They’re successful because they got a product to market fast and at the right time. They got out their protoype fast (hacks and all), tested and tweaked it, and then scaled. It is wholly arrogant to believe you will have a product on first launch that will be widely acclaimed and quickly adopted. Often it will take fast, iterative changes and tweaks with a small userbase and constant feedback until the right mix of features and pricing are met, along with aggressive marketing — and then growth happens. So why do we feel we need to build software in an entirely different way? Think of it like a city. The most successful cities in the world, such as New York, London, Vancouver, Sydney, etc — were most definitely not built with populations of millions of people in mind. They were iteratively improved as time went on and demand grew. Roads were improved. Highways built. Subways and mass transit added. I could go on, but the idea is the same; as an engineer there is no realistic data point proving that you’ll need to scale to 1,000,000 users on your first day. So don’t waste time building for that at start. Agility, Agility, Agility Often when discussing tech debt as a strategy, questions come up like, “How are we going to be able to tweak with a crappy system?” “Don’t iterative improvements need scalable architecture?” These are valid concerns that deserve thoughtful answers. And, like most difficult questions in engineering, the answers are mixed. I’m going to posit that there are areas of your system — and this may vary based on your use case — that shouldn’t need to care about tech debt at all. On the other hand, there are areas you should guard furiously against accruing large amounts of debt. The focus on technical debt should be centered around agility. On your first launch of a product, or feature, or new system, you need to ensure specific parts of that system can change rapidly. UIs, for example, are often the areas tweaked the most — copy needs changing, move this field this way, or this form could be streamlined. Integrations with other services via APIs, however, likely stay pretty constant. For example, in your shipping app above, you’re likely to not change your API integration with USPS or FedEx too much after the initial connection is built. What you do with the data and pieces may change, but the service bridge itself will likely stay the same. So — I’d argue it’s actually OK to accrue technical debt in areas of your application that are not going to change often. The reason is this: Technical Debt is Not Real Debt Technical debt is only debt in that it hampers agility. Codebases that are poor take more time to develop new features against. Testing is slower. Switching domain modeling is more painful. Financial debt, however, accrues interest and hurts more regardless of what it is incurred for. You can have areas of your application that have significant portions of technical debt for years and still not actually negatively harm the business or product. This is simply because those areas don’t change often, and therefore don’t actually harm development against them. This is why in large companies you often see services with older language code that has been untouched for years. A reminder, here, though — technical debt is not buggy code or systems that are broken. That isn’t technical debt, and that will harm your product. Focusing on Agility in Development So what areas of the application need to keep tech debt balances low to maintain overall business agility? In my experience, two of the most important ones are: User Interfaces Often these require extensive change and need to be agile enough to embrace change. This is why single-page applications have grown in such favor over the past few years; they allow quick agility at the UI layer without the need for API changes. Data Structures As Linus Torvalds once said: This is absolutely true. At the beginning of every project, the most important thing you can do is get your data structures correct. For instance, if you’re using a relational database, and your models are improperly structured, this will cause you significant pain later on. If you build a system using NoSQL that later needs to become highly relational, you will incur massive frustration in the future. Getting your data structures correct is paramount. Ensure you’ve got that right at first. Application Segmentation and Debt So this is great and all, but how does one actually do this in practice? One easy way is through segmentation . Build your application into siloed areas of concern. One way to do this is through microservices; another is through just separating your persistence models from your business logic (using service layer or interactor patterns is a great way to do this). This keeps areas of your system clean and concerned only with the actions they perform. It also segments technical debt. That USPS integration? Well, if you have that siloed in a microservice, it doesn’t really matter to the rest of the larger product if the tech debt is high in it, as long as the interfaces stay consistent. The rest of the product will perform merrily with rapid changes, and not incur any slowdown because the systems are decoupled. Furthermore, with regard to testing, heavily test areas of the application you anticipate getting lots of load and traffic. Test highly probable use cases and unhappy paths. Don’t waste a bunch of time bulletproofing the system on a use case that’s likely to happen very rarely and that would cause little harm. Always remember the impact and larger perspective on what you work on. Time is infinitely valuable, so why waste it? In Summary Technical debt is ok, and often a solid product strategy. The importance is getting to market. When launching a new system or feature, focus your effort and time on areas of the product that need to be agile, and move quickly through areas that don’t. Later, if the product proves that it can drive revenue, you can revisit those areas and improve if they need to scale. Software evolves, and the best software iteratively improves itself to meet demand. It doesn’t start out massively scalable; it gets there through time and user adoption. It’s always important to understand the context: there’s no reason to build an eighth wonder of the world if no one’s going to be there to look at it.", "date": "2017-02-27"},
{"website": "BigCommerce", "title": "Kate Walters: From Law to Tech", "author": ["Kate Walters"], "link": "https://www.bigeng.io/kate-walters-from-law-to-tech/", "abstract": "BigCommerce is made of diverse employees who together build an online e-commerce platform helping anyone anywhere in the world realize their dreams. Our people come from all walks of life, none a straight line, no path the same. Our strengths are in the stories that stretch out behind us, and how those lessons of yesterday make us better today and tomorrow. These are our origin stories. When I was a kid, I had a lot of ideas of what I wanted to be when I grew up: a veterinarian, a novelist, and Britney Spears were at the top of the list. While I enjoyed playing around on the computer and even taking apart old keyboards to see how they worked (with parental permission, of course), I wasn't introduced to the big, wide world of software development until a few years after I graduated college. I started my career believing I would be a lawyer, as finding loopholes and paying attention to detail are some of my strong suits. Fast forward one Bachelor's degree in Communication, several visits to law schools, and two weeks working as a paralegal at a law office before I left. As an interim while I figured out what I wanted to do with my life, I found myself working in customer service answering phone calls at a tech company in Austin. Part of my job was making small changes in HTML and CSS to customer websites and creating tickets for Ruby errors in our app. This turned into my favorite part of the job, and I would work on coding tutorials in between taking phone calls and jumping on the tickets that required code updates. I made friends with a lot of the engineering team and learned about coding bootcamp from a few of the junior developers there. After a couple of weeks of debating myself and a gentle push from my partner, I submitted an application. It took twelve weeks of nonstop hard work and three presentations of an app I built in two weeks, and I had the building blocks to be a software developer. I was soon to discover that my newfound knowledge was nowhere near scratching the surface of all the things I could do, but I landed my first job a few months later. There was a steep learning curve, but with the help of my manager and the contractors I worked closely with in Costa Rica, I was able to keep my head above water as the pieces started falling into place. I worked on refactoring our main api and was part of the effort to speed up the load time of a major landing page from 12 seconds to 4 seconds (on average). I won a hackathon writing a Google script that sent out budget reminders via a Slack bot, and they even let me make changes in the production database. (yikes!) At BigCommerce , I've been able to further the skills I started to hone at my first job. Since joining the engineering team back in March, I've played a part in delivering two highly anticipated payment gateways and have pivoted to a new team that is building out a new service that will make it easy for partners to offer BigCommerce stores to their merchants. Not bad for a failed lawyer-to-be. My most notable moment where my non-traditional background came into play was onboarding a new team member. We went through the API docs as external developers ( https://developer.bigcommerce.com/ ), and built an app that used our APIs. We presented our findings to people from a few different teams, which included a lot of critiquing. Thanks to my experience working in customer service, it was easy to step back and put myself in a developer's shoes who wouldn't have any background knowledge of our system. It helped me pick apart things that might be confusing or provide a less than stellar experience. My learnings while studying communication in college helped to frame pain points in a productive way that facilitated lots of nodding and consensus around the room, rather than finger pointing. Empathy and communication were key in not only doing the project justice and presenting the findings in a helpful way, but it also got the ball rolling on improvements and even generated further ideas. My introduction into development was not a straight path nor a traditional one, but I believe there is value in working with people of all sorts of backgrounds. I've used my experience working in customer service to cultivate empathy with customers and my teammates, and the tactics I picked up working toward my college degree to more effectively communicate with my team in technical and nontechnical ways. I learn something new every day from people who have gotten CS degrees, people who taught themselves to code in elementary school, people who learned on accident and people who had entirely different careers before stumbling into tech (like me). With our combined abilities, we can solve all sorts of issues in innovative ways thanks to our collective experience. There is power in diversity. Origin Stories is a special series about the people of BigCommerce Product and Engineering. Twice a month we ask a BC-er, “How did you get here, from where? What’s your origin story?”", "date": "2018-12-03"},
{"website": "BigCommerce", "title": "BigCommerce, gRPC, and gruf - a Ruby gRPC Framework", "author": ["Shaun McCormick"], "link": "https://www.bigeng.io/introducing-gruf-ruby-grpc-framework/", "abstract": "Today, we'd like to announce the availability of our latest open source project: gruf , a Ruby framework for building gRPC services. Over the past year at BigCommerce, we've begun using gRPC for our internal services. We believe gRPC provides the standardization and performance benefits that will greatly benefit our organization, and want to share some news regarding our adoption of gRPC at BigCommerce, as well as how we're releasing some of that back to the larger open source community. For those new to gRPC, it's a RPC-based protocol that uses protocol buffers for serialization. Protocol buffers are a binary format developed by Google for serializing data over the wire. They are 3-10 times smaller than a normal XML or JSON payload, decode much faster than JSON, are less ambiguous than traditional REST declaratives, and use code generation to generate clients automatically in all the languages we support at BigCommerce; this means far less time writing and maintaining client and server code. gRPC also uses HTTP/2 by default, which allows using the same TCP connection for multiple requests simultaneously, which vastly speeds up requests per minute and efficiency for a server. You also get free bi-directional streaming, flow control, binary framing, and header compression out-of-the-box with gRPC. Around January this year, BigCommerce Engineering started adopting gRPC for our internal services, as a way of improving performance, streamlining development, and increasing standardization. As we were developing Ruby gRPC services, we noticed we were adding a lot of boilerplate functionality to gRPC's core Ruby libraries. For one, the Ruby library as-is did not offer any kind of interceptors; this meant that if we, say, wanted to authorize gRPC requests with anything but TLS, we had to call a custom authorize! method at the start of every call. This got cumbersome, fast. We wanted a way to automatically intercept every incoming server method and execute logic against it, both before, after, and around those calls. While gRPC's request/response message format via protobuf is wonderful for defining APIs, we also wanted to be able to seamlessly handle error messages that would occur, outside of just returning status codes. For example, field validation errors for situations like \"Please provide a valid zip code\" became an issue - other than including a custom error message in every response message, which was cumbersome and unnecessary, could we instead push it into the metadata? We adopted this: by serializing an Error proto message in the trailing metadata of the response, we could then have our clients – in any language – deserialize that and properly handle errors, field-level validation, and debug logging – automatically and implicitly, in any response message. Instrumentation was also an issue: getting insight into gRPC requests was fairly opaque out of the box. We use statsd and Zipkin at BigCommerce, and wanted greater insight into our services during their request call flow. The core Ruby libraries didn't have this interoperability. Furthermore, gRPC requires a heavy amount of boilerplate for handling the initialization of a server. We wanted a more Rails-like setup for configuring and running a server on a Ruby service, and to make the interfaces as easy and standardized as possible. We won't hide it: gRPC is fantastic, and has worked very well for us at BigCommerce. But at its core it is only client and server libraries; to make it usable and repeatable at scale in a service-oriented architecture, we needed something more robust that offered more of a framework in which to build gRPC-backed APIs for our Ruby services. Amongst all of this, we quickly found ourselves building a gem that wrapped the gRPC libraries. This gem eventually came to be named \" gruf \", for g RPC Ru by F ramework. Gruf provides an abstracted server and client for gRPC services, along with other tools to help get gRPC services in Ruby up fast and efficiently at scale: Gruf supports Ruby 2.2-2.5, and works with any Ruby framework: Rails, Grape, etc. Setting up gruf to have gRPC endpoints in a Rails application, for example, is extremely easy. We built gruf to be framework-agnostic - but still easy to integrate - so that someone could just drop it in and run with it. Running a server is simple. Let's say we have a proto file like so: We'll generate the Ruby code for it using gRPC's protoc tool. Once that's done, we'll create a new directory under app/rpc/ , where all of our gRPC services will live. Let's add a server like so to app/rpc/demo/job_controller.rb : Gruf will automatically mount this server to its registry. Next, we'll want to setup some initialization, in Rails' standard config/initializers/grpc.rb : This binds our server to the 50051 port. From there, it's as simple as starting up the grpc server: And we're good to go! Gruf automatically setup the server, loaded the appropriate service, and initialized everything. Also, you can see there's some syntactical sugar in there with the fail! method, which sends back an appropriate GRPC::BadStatus code and serialized error payload for you. You can customize those serializers - it defaults to JSON, but at BigCommerce we actually use a custom protobuf message that's tailored for our services. Furthermore, gruf automatically detects if you're running a Rails app, and autoloads the environment for you - so all of your classes will be available for use in your gruf services. It's important to note that gRPC servers run as a separate process from your normal HTTP/1 frameworks such as Rails; they can easily share code, but you'll need to manage their process separately (Docker with kubernetes or nomad - or foreman for more traditional deployments - can make this easy for you). A simple Rails foreman Procfile, for instance, might look like this: Gruf has a fairly extensive middleware and hook system. These hooks allow you to inject various functionality into a gRPC server without having to modify the underlying gRPC stubs or framework code. The pluggable interface allows for modularity in what functionality you need per-service, and you can customize servers to your systems' needs. First off, it provides a interceptor-based approach to authentication, allowing you to write simple classes that can provide whatever authentication mechanism you want for authenticating your gruf-backed services. This is separate from the TLS-backed auth provided by the core gRPC libraries. It comes packaged with basic authentication support, but one could easily write a LDAP or Hawk-based interceptor for it. For example, utilizing basic auth is as simple as adding these lines in an initializer: This will require all gruf servers to provide basic authentication with the specified credentials in the metadata headers of the gRPC request. You can also specify a list of accepted credentials (for example, to enable zero-downtime credential rotation). Instrumentation is done similarly; gruf provides StatsD support out-of-the-box, but you can use the gruf-zipkin gem to integrate with Zipkin for distributed tracing of your requests as well. The instrumentation system uses the same interceptor system as auth, so classes can easily be written to support other systems such as FluentD or DataDog . For example, let's install the gruf-zipkin gem in our app, and then setup its configuration in an initializer: And then in our config.ru file: And then we restart our server, and there we are, distributed tracing automatically supported, giving way to traces like these: What's neat about this, is that this will carry across all of your gruf services. So if you're making delegated requests out to other services as your transaction completes, you'll get to see a fully distributed trace (including to infrastructure) with as much detail as you choose to measure. The most powerful setup for gruf lies in its interceptor system - it provides the following interceptors that anyone can write a simple class for. Adding an interceptor is easy: And you're done! You can imagine quite a few things you can do here with interceptor - for example, parameter validation, entity marshalling, and delegated permission authorization become quite easy with access to the request object and metadata headers. Because of the separation of channel, method, request, and response, utilizing a gRPC client can be kind of verbose. It also does most of its error handling through exceptions passed as GRPC::BadStatus codes, which leaves a bit to be desired when dealing with specificity, debugging, or field-level validation responses in your API. We attempted in gruf to clean that up by wrapping responses in a Response object, and providing extra functionality on top of the gRPC core. For example, a typical gRPC client request: In gruf, it's more straightforward: Gruf automatically looks up the RPC descriptor and translates the parameters for you in the call method. It can also automatically deserialize any error messages, should you have passed them in in the server. For example, if an error returns: Viola - fine-grained error messaging! We've open sourced gruf under the MIT license to help adoption in the Ruby community of gRPC, which we see as transformational for inter-service communication. We've built a few plugins for it already, such as the Zipkin support in gruf-zipkin for distributed tracing mentioned above, a circuit breaker plugin , and a request profiler . We hope that these libraries will be as much of use to others as it has been to us at BigCommerce, and welcome contributions and collaboration! You can find more information about gruf at the GitHub repository and its README . Thanks, and enjoy!", "date": "2017-07-25"},
{"website": "BigCommerce", "title": "Diversity opens doors in product and engineering", "author": ["Dan Murrell"], "link": "https://www.bigeng.io/diversity-opens-doors-in-product-and-engineering/", "abstract": "BigCommerce is full of brilliant, dedicated people focused on revolutionizing the world of e-commerce, with offices in Sydney, San Francisco, and two offices in Austin – corporate headquarters and a downtown product and engineering office. It goes without saying that employee diversity is crucial to a global company with customers from around the world and all walks of life. Recently a local chapter of Tribe, BC’s employee resource group for women, formed in Austin’s downtown office. Seeing that the new chapter’s leadership consisted of three different perspectives on product and engineering, it seemed like the perfect opportunity to talk to them about diversity in a P&E organization. To examine the role of diversity in any organization often means looking at its individuals’ backgrounds. A common thread running through their origin stories is that each woman had a non-traditional path to their present role. One might argue this is a major source of their strengths. “I started in the second grade,” said Theresa Garritano, Product Designer at BC. “I would stay after school everyday in art class – my teacher would let me.  I was obsessed with art. In high school, I took a branding design class, and someone said I should take graphic design in college. “I took a graphical user interface class and fell in love with the concept of interviewing people and understanding what makes them tick. At the time I didn’t have a smartphone, so I was so beyond what I knew, but everything was fascinating to me. That’s how I got interested in product development, working with engineers on projects in college and internships,” Theresa said. “I always was a math numbers-based person, but I actually was a finance major in college,” added Kelsey Isaacson, Senior Software Engineer on the Powered By team at BC. “I was working as a trading analyst on the floor [in Chicago] for a couple of years after school. The head of our analytics group had built a lot of our reports in a language called R. After being there for two years, I realized [working on those reports] was my favorite part of the job. So I looked into how I could do more of that full-time.” Kelsey moved to Austin to attend a coding boot camp. Next she worked at a startup writing Ruby and Javascript, and creating React front ends. After a couple of years, she moved on to BigCommerce where she’s been for a year and a half. Stevie Huval, Product Manager for the Apps and Themes Marketplace team, was a Biology major, but ultimately decided her interests lay elsewhere. “I basically ended up in financial services sales,” Huval said. “I worked for Wells Fargo for years, and had a stint as an entrepreneur in my early 20’s. “I’ve always been this kind of person who keeps a notebook of business ideas and inventions, since I was a little kid,” she said. “I was working for Wells, and had this software I was noodling on I wanted to bring to market. I didn’t know what product management was – I had never heard those words. [But] I thought if I was interested in one day bringing a technology to market, I should go work for a technology company and see what it takes. “I started at BigCommerce in sales, then moved to our sales engineering team and did that for a few years. That’s when I really came to understand what product management was,” said Stevie. “As a sales engineer you interface with product management a lot. That was the first time I heard of the concept and thought, that’s what I wanted to be doing . “Those were the people who do the research, prioritize the order we attack feature requests or opportunities, and help shape the product roadmap,” Huval said. “I realized that’s the thing I came here to learn, and just didn’t know to call it that.” “It worked out because I enjoy the problem-solving aspect of it, and the numbers aspect of it,” Kelsey said of her career. “I like that there’s a lot more creativity to it than I found in a financial analyst job. It’s not the same kind of creativity that Theresa’s using as a designer, but it’s my own creativity how I structure things.” “We’re all creative problem-solvers,” said Theresa. “People conflate creativity with artistic output, but creativity is something that exists independent of that,” Stevie added. “Good problem-solvers are highly creative people, and sometimes art is your medium and sometimes other things are your medium.” So many different influences translate into an abundance of strengths for product and engineering professionals. “Every product person is inherently an optimist,” explained Stevie. “You have to be able to find problems and actually see a path to a solution. And then just be really objective – is it more or less important than any of these other things we want to do. “Having gone through majoring in biology, understanding how research works, forming a hypothesis, and not having a bias towards proving your own hypothesis… I was lucky to have this foundation, and apply it often,” she said. “I’m an overthinker, an overanalyzer,” Isaacson said. “With code that can be a really good thing. Product people appreciate when engineers build things that makes them easy to iterate on, easy to change, and makes them more flexible.” Garritano, who also minored in psychology, said the empathy she developed in her early studies is critical to product development. “Being empathetic is important because you’re thinking like the user,” said Theresa. “My role is to make it desirable, and to do that, you have to know what the user’s next move is before they’re ever going to make it. That takes a lot of intuition and practice, to understand what people are going to do.” With experience, also comes advice. Learning from those who came before you and sharing with those who come after are qualities most product people share. “Whether it’s what subjects you take in school, what to major in, or what job to choose, my recommendation is always do the most technical thing you’re interested in,” said Kelsey, suggesting that it’s much harder to acquire that experience later. She said a lot of people did not want to give her a chance. “Even though I now have several years experience, I still don’t have a CS degree and there are still some people who will hold that against me – rightly or wrongly. “You can always go toward another direction. But having that technical background, that technical experience is invaluable,” Kelsey said. Garritano took a turn being a web engineer for a while before concentrating on design, which has paid off in product engineering. “Working with engineers is so much easier, because I actually know what they’re building,” she said. In addition to diversity of experience contributing to success in product and engineering, it cannot be overlooked that the role of diversity in general, such as gender, is equally important. “There are skills we think of as highly feminine that actually work really well in product and engineering,” Huval said, echoing Theresa and Kelsey on problem-solving, analytical thinking, and empathy. “There are strengths society built up in females that just apply here, and you can show up as you are and be very valuable,” Stevie said. “I hope in the future we do a better job of making little girls understand that.” Kelsey Isaacson is a Senior Software Engineer at BigCommerce, a die-hard Chicago Cubs fan, and a travel junkie. (Up next, Peru!) Stevie Huval is a Product Manager at BigCommerce, a passionate DNI advocate, and a dog person. Theresa Garritano is a Product Designer currently building rad experiences in the e-commerce software space, while also attempting to eat all of the tacos in Austin. Content edited for space and clarity.", "date": "2018-10-30"},
{"website": "BigCommerce", "title": "International Women in Tech Visit BigCommerce", "author": ["Meg Desko"], "link": "https://www.bigeng.io/international-women-in-tech-visit-bigcommerce/", "abstract": "For the third time in four years, BigCommerce participated in TechWomen , an exchange program run by the United States Department of State (Foreign Office) Bureau of Educational and Cultural Affairs .  TechWomen brings professional women working in Science, Technology, Engineering, and Mathematics (STEM) fields to the San Francisco Bay Area for a \"crash course in Silicon Valley.\" The program started 8 years ago with 40 women from the Middle East and North Africa and 80 mentors.  In 2018, the program had 100 Emerging Leaders from 20 countries across the Middle East, Africa, and Central Asia and over 300 San Francisco Bay Area based mentors. The program consists of: The BigCommerce San Francisco office hosted two TechWomen Emerging Leaders, each working with Professional Mentors on a project in their areas of interest. Mahendra Kumar, Director of Data Engineering, Susan Phillips, Information Security Compliance Senior Analyst, and Santhosh Saminathan, Lead Data Engineer, mentored software engineer Yulduz Khudaykolova from Uzbekistan .  Yulduz learned about data engineering, data science, and data mining at BigCommerce. Vinil Bhandari, Director of Engineering, and Meg Desko, Senior Software Engineer, mentored TravelHub Managing Director Elena Chigibaeva from Kyrgyzstan .  Elena focused on project management, partnerships and integrations with other companies, company management and culture, and making work and life fit together.  Being new to BigCommerce’s product, she was also interested in user testing some new features for our Catalog team.  Elena provided excellent feedback on the design, and the team was happy to have a new user on site to test with. As part of their mentorship, Yulduz and Elena met BigCommerce employees from across the company. They learned how BigCommerce helps merchants succeed with members of our client success team. They met with CEO Brent Bellm and CFO Robert Alvarez, who offered interesting perspectives on how to grow a company. Prior to BigCommerce, Alvarez had led four companies to acquisition or IPO, while Bellm led HomeAway through an IPO and period of high growth. The partnerships team shared insights into how working with other companies helps our merchants, partner companies, and BigCommerce itself. Chief Development Office Russell Klein built on this, discussing how venture capital can help fuel a company’s growth and change its trajectory. \"All [of] the team was so supportive and willing to share their knowledge and tips, which I really appreciate,\" Elena said. \"I appreciate and value everything I have learned from all of you,\" Yulduz wrote, following her experience at BigCommerce.  \"It was great and [an] unbelievable experience in my career. Thanks all who shared with me [their] own knowledge, and gave tips. \"During this internship I understand the meaning of success, how to build the team, [and how to] support them.  From you I have learned the value of tolerance, patience, and trust in business, you inspired me to new success,\" Yulduz said. Elena Chigibaeva said, \"back home I am managing a travel startup www.travelhub.tours - the first online platform for tours and activities booking in Central Asia, Caucasus, Russia, and Mongolia.  I was lucky to be hosted at BigCommerce as a major part of my TechWomen Program journey.\" She expressed gratitude for her professional mentors, saying \"they are amazing people with big hearts and high professionalism.\" Elena continued, \"I have learnt a lot with them and the entire Team at BigCommerce’s help about B2B, ecommerce and APIs, scrum project management, Team management and collaboration, tools and techniques that can be used to scale and improve the business.\"  BigCommerce \"inspired me to continuously grow and be a leader of excellence,\" she added. Elena said the once in a lifetime opportunity of being a TechWomen fellow allowed her to practice pitching and negotiation skills, and to explore San Francisco and Silicon Valley.  She said she was inspired by visiting the giants of the tech world: LinkedIn, SalesForce, Twitter, Google, Mozilla, BigCommerce, AirBnB, NASA, Singularity University and more. \"I will be missing the open-minded and supportive atmosphere at BigCommerce and in San Francisco. I will be missing the Waffle Wednesdays. I will be missing a beautiful view of the Bay Area from my window on 16th floor and I will remember this experience and learnings forever.\" she said. Elena’s parting wishes were for BigCommerce and its employees to achieve our dreams and goals... and for us to come visit her in Kyrgyzstan! The TechWomen program is impactful not only for the Emerging Leaders who participate each year, but also for the host companies. Santhosh Saminathan, a first year Professional Mentor for TechWomen at BigCommerce said \"I personally liked the enthusiasm shown by both of them and sharing the knowledge related to Data Engineering and Data Science. They both were interested in learning the cutting edge technologies in the data science domain.\" Another first year mentor, Susan Phillips said, \"While working with our two mentees was interesting, challenging and enlightening, it wasn’t until I saw the entire group of mentees that I became really impressed. Seeing all those women, from so many corners of the globe, and hearing their stories was truly awe inspiring. \"Listening to their stories, I had to hold back tears while I envisioned the hardships the people who live in the areas they represent have to overcome,\" she said. \"Despite those hardships, they remain positive, passionate and driven to try to make a difference. The work TechWomen is doing is fabulous, and the exposure to it this year has expanded my world view.\" Mahendra Kumar, also a first year mentor, said  \"I really enjoyed the experience of mentoring them and learnt a lot from them. It provided me an opportunity to understand diverse work styles and approaches. I was encouraged to see their energy and passion to learn and contribute to our projects.\"  Mahendra was happy that the TechWomen would take their learnings back and share their experiences with a broader audience back home. It is an empowering experience for the Emerging Leaders and is a great platform for women entrepreneurs to see and work with tech companies in Silicon Valley,\" he said. For me (Meg Desko) personally, TechWomen has been one of the highlights of my career in the tech industry and is something I look forward to each year .  2018 was my seventh year participating in TechWomen, and my fourth as a Professional Mentor.  Each year has been different and rewarding in its own way; as a cultural mentor I’ve been able to interact with a greater number of Emerging Leaders and get to know them, while as a Professional Mentor, I work directly with a single TechWomen fellow to help her meet her goals for the program.  I love the cultural exchange that occurs during the program and representing the U.S. as a citizen diplomat.  Participating in the TechWomen program as a mentor allows me to pay some of the opportunities I’ve had working in the epicenter of the tech industry forward to other women around the globe. As Jimmy Duvall, Chief Product Officer said, “spending time with the TechWomen over the last two years has greatly expanded our global view and reach of technology and commerce.  We are proud to be able to participate in such an outstanding program and look forward to working with next years participants.”", "date": "2018-12-11"},
{"website": "BigCommerce", "title": "Native & React Native @ BigCommerce", "author": ["Matt Weiss"], "link": "https://www.bigeng.io/native-react-native-bigcommerce/", "abstract": "In 2018 BigCommerce decided to explore a variety of native mobile apps to expand our platform and better serve our merchants. We carefully evaluated the latest technology trends, the skills of our existing engineers, and the potential job market for hiring new engineers. Our research revealed there were four unique paths we could take to explore mobile development: 1. Native 2. Bridge 3. Hybrid 4. Progressive Coding native iOS apps in either Objective-C/Swift or Android apps in Kotlin/Java. •\tUnrivaled speed/experience on mobile devices •\tFull support for threading and performance optimization •\tSolid development ecosystem: Xcode & Android Studio •\tEasy dependency management •\tNative look/feel comes right out of the box •\tHaving to create and manage two dedicated codebases •\tFinding engineers that know both Android & iOS is difficult so teams are generally siloed •\tLonger development lifecycle to launch features on both apps simultaneously •\tSteep learning curve There have been a number of tools over the past few years that have allowed you to create mobile apps by either leveraging existing codebases or writing languages other than native code. Examples here are: • Ionic Framework • Xamarin •\tFaster development lifecycle •\tSingle codebase powers iOS & Android •\tYou can probably port your existing system to mobile with minimum fuss •\tSuboptimal user experience •\tLack of access to the full suite of native functionality •\tNo support for true threading and optimization •\tDebugging can be difficult Bridge platforms fall somewhere between Hybrid and Native. They give you the ability to code in a non-native language which directly wraps native elements. Additionally when you want to step into pure native code you have the ability to do so and export that functionality via the bridge layer. Example here are: • React Native • Flutter •\tFast development lifecycle •\tParity with native speed/experience on mobile devices •\tFull support for threading and optimization •\tSingle codebase powers iOS & Android •\tMost code changes can be deployed without submitting a new app store build and purely by updating the JS/Dart packages remotely •\tNewer technology so debugging can be complex and bugs can occur which will often require 3rd party library fixes to resolve •\tMedium learning curve. You can get very far without touching pure native code but to truly drive this technology you need to work in the native and bridge layers of code Progressive web apps are applications where the assets (HTML, CSS, Javascript, Images, etc) are kept in the browser, using web workers. •\tMobile users do not need to download an app and basically get a web experience on steroids •\tSome native Android and iOS functionality is exposed via things like Web APK •\tYour website is the exact same codeset as your mobile experience so there is a single codeset for all 3 domains: Web, iOS, and Android •\tStyling and functionality are universal since CSS/JS experiences are relatively seamless across all modern browsers these days •\tNo need to submit builds to the app store, deploys are instantaneous •\tIf we need an app store presence PWA basically makes that a non-starter •\tNot all native functionality is readily exposed •\tBackground processes, when the browser is closed, are not fully supported yet \t•\tPerformance is still browser based so you will not get a full native feel BigCommerce had previously attempted mobile apps in 2013 using the hybrid methodology and the results were mixed. While we were able to mostly use our existing codebase to power our application the look, feel, and functionality felt foreign on mobile devices and the user experience was viewed poorly by our end-users. Lack of a mobile app was one of the top complaints our merchants voiced and we were eager to deliver a product that would meet their growing needs. Having learned our lessons we set out to try and kick the wheels on this venture again. During late 2017/early 2018 BigCommerce had started to bet heavily on React and TypeScript to drive new areas of our platform. BigCommerce had a wide range of talented JavaScript engineers on our staff, but did not have engineers with experience in native mobile development. Furthermore most of these engineers were excited to get a chance to bridge their skills to React Native and contribute to our mobile projects. These internal trends combined with the number of premier tech companies that (at the time) had advertised their usage of React Native (e.g., Facebook, Airbnb, Walmart, Microsoft, Uber Eats, etc…) led us to give React Native serious consideration for our mobile pursuits. Prior to making a final decision we read a number of tech blogs and the trend seemed to be that companies that had started with pure native and transitioned to a React Native hybrid regretted the decision and backed out of it (e.g., Airbnb). Companies that started in pure React Native tended to enjoy the experience much more. React Native is still young and there are a few big names pushing the development space. Companies like Microsoft, Wix and Facebook are driving most of the open source documentation and libraries that can get you from 0-60 quickly. If you decide to pursue React Native we cannot stress enough using the list of libraries & tools below. They were stable, performant, well documented, and greatly improved our experience: • Fastlane & Match for automating your builds, code signing & releases • React Native Navigation for managing your application flows • Detox for greybox automation testing of your applications • Firebase for quick prototyping of push notifications, cloud functions, and some quick storage • Microsoft TypeScript + React Native Starter Kit to help you get up and running • Ignite CLI for exploring how to structure apps and code them using React Native Like most React Native users we started by using expo-cli / create-react-native-app to speed things along. However we quickly realized that we had a high likelihood of having to blend native libraries and code to meet our business requirements. This led to us ejecting early on in the process so that we could write occasional swift/java code when needed. This ended up being the case for push notifications and interactive UI charts. Manually adding 3rd party libraries to an ejected react native project sucks. Nobody has time to drag and drop Xcode projects and manually link libraries. If you are new to Xcode as a whole the entire configuration can feel foreign. It’s so much easier to use react-native link + Cocoa Pods to drive your dependency management for native libraries. Getting from 0-60 in pure JavaScript and React Native is a breeze. For anyone familiar with React the transition was near seamless. Over the course of several hackathons multiple groups of engineers had been able to build impressive prototypes. Furthermore when we started this endeavor our team of engineers would meet after work for 1 night per week to hack on it and 90% of that time was spent breezing through React Native request wiring, business logic, and screens. One of the main selling points is that React Native will get you to about ~90% proximity match across platform without much effort. React Native has a Platform module and even allows Platform specific files (e.g., .ios.js vs. .android.js) that helps you target custom code, designs, functionality per platform where needed. BigCommerce loves TypeScript. When we started this project most of the tutorials, sample codebases, & books were only Javascript + React Native based. Microsoft had started to publish their TypeScript + React Native guidelines but we were learning to adapt on the fly. Over time we noticed a lack of @types/* libraries for many open source dependencies we had planned on using. This meant that our engineers either needed to take the time to analyze the dependency libraries and write out the type definitions or embrace the variability of the Any type. When you are committed to typing, a dynamic language using Any just feels wrong, but when it comes to fast-moving prototyping, corners occasionally need to be cut to prove out ideas. As the popularities of TypeScript & React Native increase this issue should self correct, but given that our package.json file was quickly bloating with dependencies this issue started to spiral out of control. Perhaps the most painful negative we encountered was the fact that we were dealing with 2 tiers of dependency management. On top of the basic NPM packages managed via package.json we also worked with Native packages via Cocoapods. Where we ran into issues were when Native packages relied on specific Swift versions or configurations that clashed with other modules. The end result would be adding the JS libs, Native Libs and react-native linking would work but trying to debug the flurry of iOS or Android errors thrown when attempting to build the app would be endless. A particularly painful example we ran into was trying to get the React Native modules for Segment Analytics and React-Native-Charts-Wrapper to play nice together. Our team lost a few days just troubleshooting iOS build issues. We never even had a chance to see if there were Android issues since we ended up throwing the towel on several 3rd party libraries. The prevailing trends in React and JavaScript are pointing towards observables being the control flow for most SPA’s. You can look at leading talks by companies like Netflix that describe this pattern. Crafting observables usually requires you to create actions, epics, reducers and services with the ultimate goal of dynamic pub/sub for your SPA. For our React Native app we followed this model and adopted redux-observables . As our app grew in complexity we noticed that the simple act of adding new API requests or flows into our app and storing the state so that it could be passed to any number of flows or screens became increasingly complex. It wouldn’t be uncommon to see 500-700 line pull requests for such a task. Additionally as our app moved from single flow to multiple flows the act of maintaining state between them became unmanageable. This complexity is part of the reason why when we rebuilt our mobile app using native tech, we made a precise decision to pursue PromiseKit . Most engineers are familiar with the simple paradigms it follows: request().then().catch() etc.... You just can’t go wrong with it and we have yet to encounter a feature it cannot easily support. Naturally you might ask “why didn’t you just swap out observables for promises with React Native?”. The answer to that is that it felt counter productive. Almost like we were fighting with the tool we were trying to use. Android and iOS have many platform-specific quirks. An obvious example could be iOS alerts vs. Android’s toast implementation for notifications. There are React Native libraries that can bridge this functionality cross-platform but then you are essentially disrupting the natural behaviors to force a new standard. Given how closely users are tied to their phone behaviors this is a high risk. We are now in the merchant beta for our new iOS mobile app. In three weeks we were able to fully port and improve our React Native app to Swift and are in the process of beginning our cloning of Swift to Kotlin for our Android app. TL;DR - The overarching question here is “do we regret choosing react-native?” My answer to that is absolutely not. React Native allowed a team of motivated engineers who had, mostly, never coded mobile apps to create an impressive full-functioning prototype that turned this exploratory project into a product driven venture for our team.  We will most likely use React Native again for other app prototypes we want to explore. For any developer or team looking to explore mobile or quickly build prototypes, that has NOT worked on native mobile apps before, I would immediately suggest using React Native. However, when it comes to a fully productized mobile app ready for showtime I would encourage the pursuit of pure native code and not chasing unicorns. The “unicorn” of software development is one tool that covers all bases. Usually when trying to achieve this lofty goal corners are cut. For the sake of developer efficiency you lose some of the icing that makes the cake so appealing to end users. As a result there’s a laundry list of failed unicorns strewn throughout the annals of engineering. Had BigCommerce not hired a strong lead engineer with experience in building native apps in Swift & Objective-C this decision to pivot from react-native to native would have been harder. Ultimately we still feel we made the right decision even if that means we need to hire specialized engineers to support both iOS and Android apps independently. If you are interested in helping us drive mobile development at BigCommerce we are actively hiring. Please explore our careers page and drop us a line.", "date": "2019-02-28"},
{"website": "BigCommerce", "title": "README.md to Manage Humans", "author": ["Vandana Premkumar"], "link": "https://www.bigeng.io/readme-md-to-manage-humans/", "abstract": "If you stumbled on this blog post you may either be an individual contributor (IC) aspiring to be a manager, or a good manager looking for ways to promote an IC to be a manager. This is a step-by-step guide to become a successful people manager. I'm presenting the steps in the form of a README.md doc, which is typically used for a step-by-step guide to install a project. The journey of an IC to an Engineering Manager is just like a project. Hopefully this document will help you in your journey or project from IC to people manager. Most software engineers at one point of their career hit the crossroad of which path to pick — technical ladder or management ladder. Software engineers may face this crossroad more than once in their lives. This README.md is to help those engineers who are looking to grow their management skills. The book “Managing Humans” by Michael Lopp explains various important aspects to managing your team. Lopp talks about how as a manager you become a connector between your team and the rest of the organization. He explains many different aspects of manager jobs like offsites, 1:1 meetings, meetings and so on. Another author, Rob Wormley , articulates a 12 step process for leadership : Listening and taking feedback from your team members is a good, on-the-job test. Increase your “test coverage” by seeking feedback from peer managers, from other teams and companies. Having 90% test coverage and a good supporting manager is key to deploying an IC to manager. “Managing Humans” | Third Edition | Michael Lopp What Makes a Good Manager? https://www.wrike.com/blog/what-makes-a-good-manager/ | Brianna Hansen The 12-Step Process For Improving Your People Management Skills | Rob Wormley https://www.radicalcandor.com/ | Kim Scott Originally published on Medium .", "date": "2018-12-18"},
{"website": "BigCommerce", "title": "Re-Platforming Data @BigCommerce: five second latency on Petabytes of data", "author": ["Mahendra Kumar"], "link": "https://www.bigeng.io/re-platforming-data-at-bigcommerce-5-second-latency-on-petabytes-of-data/", "abstract": "Many analysts are hailing Data as the new oil. However, contrary to what they're saying, oil is a depleting resource. Data on the other hand, is an expanding resource. At best, Data can be compared to crude oil, in that both are buried way beneath and their enormous value can only be realized by extracting and processing them. The ability to derive actionable insights in real time on massive volumes of ever increasing data is a key differentiator as businesses compete in this era of digital transformation. BigCommerce offers a comprehensive set of Analytics Reports for Merchants to analyze their business across all key metrics. The legacy Data platform at BigCommerce consisted of some 40-odd Hadoop Mapreduce jobs that were triggered using cron jobs. It took about 6 hours before merchants could see their analytics data. Any job failure often required manual restart of the jobs resulting in an increased lag. Adding new functionality was extremely difficult. Data at BigCommerce powers merchant analytics, as well as BigCommerce’s own marketing and accounting needs. Data was being served through two independent systems, resulting in redundant and siloed data. It required a separate set of ETLs and Data pipelines, adding cost and complexity. The actionable value of a business event goes down drastically with time after an event is generated. The sooner a business event becomes actionable through data, the higher its value. At BigCommerce, we process over 800 million events every day. Tens of millions of shoppers shop every day on our platform, generating hundreds of millions of clicks, visits, views, orders, carts, checkout and other events per day. We set out to build a new cloud Data platform which is scalable, multi-tenant, realtime, secure and built on an open source stack. For merchant-side analytics, we set three simple business goals : On the technical architecture side, we embraced the following distributed data design principles: To accomplish the above goals, we incorporated the following modern data approaches : For stream processing , we used Kafka streams. Kafka streams is a lightweight library that processes events one at a time with very low latency. There is no need for a stream processing framework and cluster manager. For Datastore, we chose HBase, as it provides very high write throughput because of LSM (Log Structured Merge) based write mechanism where files are written sequentially in memory and copied on disk. A separate process (compaction) takes care of merging them in background. A typical RDBMS cannot consume a massive amount of events because of the requirement to write directly into scattered data files on the disk. To achieve sub-second response for analytics reports, we pre-aggregate data. In the eCommerce domain, aggregates are not simple immutable time series rollups. For example, an order that was placed a week ago can be cancelled or edited. This impacts hourly, daily, weekly, monthly and year-to-date aggregates across all metrics. Our aggregation process determines the impact of changed data, and aggregates affected metrics in a scalable and efficient way. This greatly reduced our compute resources as well as compute time. Data is made available through RESTful API for front end and other systems of our platform. We came up with a json schema to describe events. Events are generated in store front and sent to Kafka using Filebeat. Our tech stack consists of : Data platform is fully managed through terraform and puppet. Terraform is an infrastructure automation tool used for managing resources in cloud. Desired state of resources are expressed using Terraform configuration. We use Puppet to install and configure software inside cloud VM instances. Any time we deal with real time data processing, monitoring and alerting is critical. We use a host of monitoring and alerting tools such as Kibana, Grafana, Sentry, CloudWatch, etc. We log various event metrics on a scheduled frequency and alert if those metrics fall outside our upper and lower bound thresholds. Here are some sample monitoring charts: We wanted to transition from legacy data platform to new data platform without introducing any down time, since our merchants heavily use analytics throughout the day. We came up with a cutoff time for copying over large sets of historical data and for the new data, we rewound kafka offsets to re-process few days of data preceding the cutoff time. Along the way, we also corrected historical data inaccuracies. We rolled out Blaze to all merchants in November 2018. The new pipeline has been performing extremely well without any hiccups including the period of two major events for ecommerce domain: Black Friday and Cyber Monday. The new data infrastructure has been very stable and reliable. Here is how we measured against our set business goals : As we retire redundant infrastructure, we will see cost savings to the tune of 30% to 40%. Large visit counts due to bots: We saw 2x to 3x visit counts in Blaze compared to legacy data pipeline. This massively impacted conversion ratios. It turns out that Bots that do not enable java scripts were firing bot visits. We then switched to a Javascript mechanism to fire visit events. This brought the visit count down to normal range. Order time zone mismatch: BigCommerce serves merchants in 120+ countries. When a merchant changes their timezone between order creation and order update, the historical aggregates would get bucketed incorrectly. We ended up updating the event schema to include order creation time zone offset, cleared previous aggregates and recomputed all aggregates. We now have a foundational Data platform and we are just getting started! We are building deeper insights to empower merchants to discover hidden sales opportunities and grow their business. We are also building machine learning models for prediction and recommendations to enhance merchant and shopper experiences.", "date": "2019-03-05"},
{"website": "BigCommerce", "title": "VP of Engineering: The first 100 days", "author": ["David Hauser"], "link": "https://www.bigeng.io/vp-of-engineering-the-first-100-days/", "abstract": "I started the next stage of my career journey as VP of Software Engineering here at BigCommerce this past August. My first hundred days have been quite the wild ride – crazier and more fun than I could have imagined. I recently took a moment to reflect on how I got here and what the experience has been like. Here is my story: It was important for the people here to know who I am, where I came from both an overview of my work background as well as some fun personal facts, likes, and hobbies that are part of who I am. For me that is among other things: family – raising my 3 young kids through the trials and tribulations of elementary school with my amazing wife of 13 years, tennis – I played in college back in my glory days and now keep my juices flowing by playing competitive league tennis, Bruce Springsteen – been to every tour in the past 20 years, and the Oakland A’s – for those outside of the U.S. that is the local baseball team. First the good stuff. Engineers here are PASSIONATE . Engineers here have STRONG opinions on what needs to change to unlock the next level of performance and execution. Engineers here are NOT afraid to ask the new guy the tough questions. I love it! Partnering across three engineering hubs is hard but we are getting better at it. Product and Engineering executive leaders are both very well aligned and open to new ideas and ways of doing things, which is not always the case. And finally at the top, I have never met a CEO more direct and candid than Brent, which I learned very quickly in week two. This was very refreshing. Next were some things that were either question marks or things that I clearly wanted to help impact. We have a decentralized technical decision making process that is very different than what I am used to. This seems to be working at times but at other times causing quite a bit of pain. Finally, all of our managers, directors, and execs in engineering are men. The biggest mistake I made the last time I sat in this seat was that I didn’t want to make major changes in the early going because I felt that the team was working well together. I didn’t want to risk upsetting the dynamic before I established myself and gained their trust. Bad idea. You are put in a leadership role because someone trusts your judgement. If you see something that you feel is a priority to change, work with the right people to come up with a plan, roll the plan out, and then gather feedback and iterate along the way.  And if it doesn’t work, kill it and own the end result. Better to err on the side of action and iterate until you reach a successful place than hesitation and extended pontification. Lesson learned and I made a promise to myself I would not make that mistake again. My role is very interesting in the sense that while there are some things that have to be done, most of what I do on the day to day is self-directed. This means that while there are 100 things on my list to get done at any given time, it is on me to figure out which to go after in which order and which will make the biggest impact. I have always been good at prioritizing. Earning a CS degree while playing a varsity sport in college while also trying to have a reasonable social life is a bit of a forcing function for that. Then I got really good when I had kids. My wife works full time so we pretty much share the parenting duties. No longer can I put in regular large chunks of extra hours to get ahead. There aren’t enough hours in the day and there is a negative ROI for me after more than one bad night of sleep in a row. So, how to figure out what to do? A few early wins: for couple of managers who had too much on their plate, we did an exercise to delegate to their teams to help them scale. Both are in a very good place right now. For the organization, it was to take all of the organizational initiatives we are driving, tracking it publicly, and running them as sprints which contain user stories. Tracking the work publicly creates org-wide visibility, accountability, and ownership along with some healthy peer pressure for those who currently may be too busy to take on an initiative. So far we have had over 20 people sign up to contribute to things such as rolling out a new on-call process, creating a mentorship program, and resurrecting our dormant BigEng blog which you are reading right now, and where we will share all of the incredible engineering things we are doing here. Diversity and inclusion is an important issue and one that I take very seriously. On a professional level, I know that diverse teams outperform homogeneous ones and ultimately build better products, not just because the data shows it but because I have lived it. In my last job we had a really special management team which at one point was made up of 5 out of 8 women. On a personal level, I care about this issue as I have seen the challenges impact my wife in her career. My commitment to the business is to seek ways to recruit more diverse candidates into the team. While working on that, in parallel, I am attempting to create as diverse of an environment as I can with the team we have and make the female voices currently on the team heard. I added the two women who are in Team Lead roles to my weekly staff meeting with my direct reports for two reasons. One, creating more diverse viewpoints adds value to the group, and two, to show the rest of the organization that I am committed to leading an organization that supports all groups. I am ok with doing something that may be seen as controversial by some if I believe it will help the greater good. I fully understand that this may not sit well with male leaders in similar roles in that they might feel excluded. However to compensate I am committed to ensure that they have the access and support they need to make their mark and be wildly successful in their careers here and beyond. This is not a zero sum game. We can make room for more voices without compromising anything. Finally, we are starting to see some good results. After hitting a bit of a lull in hiring, four out of our last six engineering hires across our Austin, SF, and Sydney teams are women along with a few others in other parts of the greater Product & Engineering group. SF, Austin, SF, Sydney, SF, Kiev, SF, Austin, SF, SF, Austin.  These were my first 11 weeks. I have 3 kids at home and my wife Sarah started a new job where she was doing quite a bit of traveling right around the same time. We share the parenting duty and are fortunate enough to have an amazing nanny to make it all work, but the first couple of months were a bit challenging on the home front. We barely saw each other and on weekends we both scrambled to take our kids here and there to their activities. Growing up, my dad used to travel for his job and then come directly from the airport to watch me play in my tennis tournaments. I have a newfound appreciation for what he did and can also fully understand why it was important to him. Watching my 10 year old Eli go through the ups and downs of a weekend of baseball, learning how to be a good teammate, handle winning and losing, and regardless of the outcome eating at the nearest Chili’s and goofing off with all of his buddies is incredibly satisfying as a parent (except for the eating at Chili’s). And when I am tired and jet lagged, I get very grumpy at home and become a lot less pleasant to be around. But this is getting better. I am learning how to sleep a bit better on long haul flights as well as to plan for some downtime afterwards on the weekends to sneak a power nap in. It was right around my 60 day mark where two key managers resigned in a span of two days. This was one of those moments where you don’t quite know where you are going to land. The short of it was after a brief period of everyone involved processing the changes, followed by some heart to heart discussions, several individuals stepped up to take the opportunity to fill the leadership gaps. The teams involved are now running as strong as ever. The first lesson learned was a bit of an eye opener for me in that our decentralized way of doing architecture actually does work very well in practice. Empowering teams (as opposed to a centralized architecture committee) to own their technology decisions along with the understanding that they will iterate on them is a beautiful thing.  It just needed a little fine tuning on the margins, mainly related to written communication. The second is that communication across a large distributed group is hard and there is always room for improvement. The third one is that after 100 days I still have a ton to learn around product and technical knowledge to be more effective. I intentionally weighted myself towards the people and processes. That is where I felt my contributions could make the most immediate impact and also happens to be where my personal strengths are. And also probably why I was brought in here in the first place. But I still have plenty of work to do on the product and tech side of things. The first 100 days were an incredible, wild ride. There are a ton of things to accomplish but I will share some of the highlights which I am most excited about. Over the next 100 days and beyond we will be rolling out a new operational framework that will allow us to scale product and engineering through our growth during 2019 and beyond. We have some major technology initiatives that will enable us to become the best open ecommerce platform in the world, hands down. And I want to lead a more structured career development program, where every person in my organization has a career growth plan and knows where they stand and what they need to do to get to the next stage of their journey. And we are going to continue with regular content on our BigEng blog , to give everyone out there a bit of a taste of what we are all about.", "date": "2019-01-25"},
{"website": "BigCommerce", "title": "A Simple Architecture for Mobile Apps", "author": ["Dan Murrell"], "link": "https://www.bigeng.io/a-simple-architecture-for-mobile-apps/", "abstract": "Before joining BigCommerce as a Senior Mobile Software Engineer, I was fortunate to have worked at Mutual Mobile for over five years holding positions from iOS engineer to architect. For those unfamiliar with MM, they are a global digital innovation and emerging technology agency, headquartered in Austin, Texas (same as BC). One of the great benefits of being a mobile developer at an agency is the diverse range of projects. One day you could be working on a banking app. Six months later it might be a social app built around a sports community, a fitness app, a mesh networking system, a smart home manager, or a dozen more. As diverse as the clients were, they always had things in common, and at Mutual Mobile we developed and open sourced tools and architectures that many other developers use to this day. One such architecture called VIPER , developed and championed at Mutual Mobile by Jeff Gilbert and Conrad Stoll , gained a lot of traction in the iOS community and beyond. When starting a new mobile project recently at BigCommerce, I considered returning to that familiar architecture. VIPER is an application of Clean Architecture for mobile apps, with a focus on testable code that separates logic from view controllers. In the mobile world most apps use MVC architecture, which often means view controllers that contain application logic, aka Massive View Controllers. By separating views from business logic, VIPER makes classes that are easier to understand, test, and maintain. But ultimately I decided that VIPER was more than I needed. While it is a great, testable, modern architecture, it tends to require a lot of files to do it right. Several open source projects are available to generate the files needed to do VIPER (although if you do it TDD-style like Gilbert, you only create those files as you need them). I decided to give my own personal architecture idea a shot instead. It is very minimal, not boilerplate-y, and highly portable. I’ve written apps in Objective-C, Swift and JavaScript using it, and there’s no reason why it couldn’t work in a dozen others. It is definitely influenced by VIPER and the thinking behind it. Its central thesis is this: a mobile app is a construction of screens managed by data-passing flows and sub-flows, that start, end, move forward and back, and sometimes execute actions. Because of this, I call it the Flow Architecture. Whether on-boarding a user (which was the genesis for this architecture), signing in to a service, or simply using a tab-based app with navigation stacks of different screens, almost any app process can be described as a flow of screens and information. It all begins with a start. And of course any flow that can be started can also be ended. Some flows, like on-boarding or logins, may need to step forward and back. Occasionally a flow may need to execute some action that does not necessarily cause a step forward or back. An action needs an id to identify it, and may provide some data to give it context. I also may log an action instead, such as for analytics. No actual change to the flow takes place. Because any flow could both be a sub-flow or parent to a sub-flow, it needs to signal to the parent. These delegate-like functions are automatically invoked on a parent when a flow starts or ends (remember to call your flow subclasses' super functions). You may also want to check the current state of the flow. In practice, I like to define this flow capability as a private protocol and then provide a concrete Flow base class for the implementation. I then subclass that to create any flow objects I need to construct the app, as well as create structs that implement the FlowAction protocol for any custom actions I want to handle. I also add a bit of logging to each function so I can follow the flow in the debug console. A custom logging function or framework makes it easy to disable this in production builds, to have useful flow logging without leaking implementation details in public. One of the great benefits of thinking in flows is that there are opportunities to make decisions while information is being collected. The genesis of this architecture was on-boarding a user. Instead of a static, linear progression of screens, we needed to customize steps based on initial responses. VIPER is great at handling this scenario, and is highly tailored to views and interactions. As the presenter collects inputs from the user, it uses the wireframe to navigate to the next screen, whatever that may be. It could be the next in a static set of screens, or it could change based on a decision tree. But what if there is no user input, or even a screen? Another benefit of Flow Architecture is it has very few requirements, implying only a start, end, and some number of steps or actions between. Like VIPER, flows are easy to test. However, unlike VIPER where you always know the data types being passed around, there is some uncertainty with the Any? nature of the data. That tradeoff is made for simplicity and flexibility. This particular iteration is written in Swift, so it comes with the benefits of that strongly-typed language. When a Swift flow receives data it can check its type, whether a primitive or a struct of other objects. In Javascript, the data could be any object. In my apps, I use flows to handle all the decision-making, sub-flow creation, screen creation, network requests, OS API calls, validations, and so on. They're initialized with only whatever they need to do their work. My view controllers do little more than setup and update their views. The flows do all the work, and when they start to look like they’re doing too much, that probably means you can extract a sub-flow instead. In part 2, we'll look at a typical scenario in modern mobile apps: on-boarding a new user.", "date": "2019-02-26"},
{"website": "BigCommerce", "title": "Opening the Door to Future Women Leaders", "author": ["Kate Walters"], "link": "https://www.bigeng.io/opening-the-door-for-future-women-leaders/", "abstract": "On a Friday this February, four juniors from the Ann Richards School for Young Women Leaders stood in front of the entire BigCommerce downtown Austin office. Behind them a projector screen displayed the fruits of their labor for that week: a presentation of what they accomplished each day, what they learned, roadblocks they ran into, and finally a demo of a live store they conceived and built themselves. The mood of the room fluctuated between excitement and immense pride as Nayel Bautista, Karen Mondragon, Marti Cuevas, and Andrea Majalca each answered a plethora of questions from BigCommerce employees. The most telling moment came when they were asked if they would consider pursuing a career in tech. Marti smiled, “Before this internship I’ve never considered myself a techie person. I’m the kind of person that breaks a computer.” But after four days being immersed in the world of tech, they learned it’s not about already knowing everything. It’s about the effort to learn and understand the problem so you can get to the solution. All they needed was the opportunity to crack open the door. Each of the young women expressed similar sentiments to Marti, and one by one they answered yes. The Ann Richards School (ARS) is an all-girls STEM school that “dedicates itself to prepare young women to attend and graduate from college, commit to a healthy and well-balanced lifestyle, lead with courage and compassion, and solve problems creatively and ethically in support of our global community.” Data shows 56% of the school’s student body are economically-disadvantaged, and 65% of graduates will be the first in their family to graduate from college. One of our senior engineers, Dan Murrell , introduced BigCommerce to the Ann Richards School. His interest in Diversity and Inclusion stems from his own college experience. As a CS major in 1989, he recalled his classes being full of both men and women, staff included. When he returned years later to complete his degree, his classes were much less diverse – almost all the women were absent. “I don't think you can have a healthy tech industry in that reality,\" he said. \"Until I found the Ann Richards School and came to understand their mission – creating young women leaders – I didn't know how you could fix it.” In its annual report, Silicon Valley Bank states out of tech and healthcare companies surveyed in the United States, China, Canada, and the UK, “56 percent of startups have at least one woman in an executive position, and only 40 percent have at least one woman on the board of directors.” ( Silicon Valley Bank Women in Technology Leadership report, 2019 ) That leaves half of all startups without a single woman on their leadership team. Companies that are willing to host interns, take them under their wing, and teach them the ropes may be the key to solving this problem. BigCommerce recognized how detrimental this lack of diversity is for both the success of a rapidly growing company and its culture. In response to the mountain of evidence proving diversity drives innovation, they backed a global employee resource group for women, non-binary people, and allies, aptly named Tribe. The downtown Tribe chapter is led by Kelsey Isaacson , Senior Software Engineer, Stevie Huval , Product Manager, and Theresa Garritano , Product Designer. When these women leaders learned about ARS, how their values lined up with Tribe, and the internship opportunity, their wheels started turning. So they pitched the internship to Chief Product Officer Jimmy Duvall for executive sponsorship. Though BigCommerce had never done something like this before, he didn’t need convincing. “As the father of two teen girls, the program really struck a chord with me on a personal level,” Duvall said. “Whether or not my daughters pursue a career in tech is immaterial, but that needs to be their own choice, not because they didn't have access or exposure to the opportunities. I love that the Ann Richards School is taking a highly practical, hands-on approach to solving this issue.” Once the Tribe leaders had executive buy-in, they agreed to host the internship. They intentionally chose a project that was relatable to high school juniors, would offer them takeaways they could apply across multiple areas of their lives (even if they didn’t end up in ecommerce), and could be completed in a week. After going through multiple ideas, they settled on the perfect project: walking the young women through a complete, yet condensed, product lifecycle. The interns would build a live, online store and be given all the tools they needed to be successful, mimicking the type of work they could expect from a job in tech. They would walk away with a working store to iterate on and use after the internship week was over. By going through the planning process, they would be able to understand how to come up with, organize, and execute on ideas. The week started with a brainstorming activity to decide the theme of the store. The interns cycled through ideas like a makeup store, but after grouping together their ideas, they noticed a common theme: school swag. They liked the idea of taking a real world problem, sales and distribution of ARS shirts, stickers, calendars and more, and making that process better. Stevie noted they naturally gravitated towards who and what they had the most empathy for – they focused their target audience on students rather than parents or faculty, and had a keen eye for organizing the products in a way that would make sense. After agreeing on what to build, they had to figure out how. The team prioritized what they wanted to do, picked up user stories, independently figured out how to use BigCommerce’s tooling, and were increasingly enthusiastic about getting into the code and making front-end changes. Though none of the young women had previous experience working in code, they dove in head first. Stevie noticed they were exhibiting a hallmark trait of developers - they would try and fail and try again, fully embodying the attitude of not giving up. Theresa said when Andrea saw her CSS code working, “her eyes lit up.” Seasoned developers have to admit, it’s easy to lose the magic of daily tasks. The rate at which the students absorbed information and were able to apply it to their project surprised everyone who worked with the young women. While the Tribe leaders set the objective and many BC employees volunteered to help answer questions, the interns made their own decisions. Tech companies aren’t used to high schoolers wandering through their offices, but their accomplishments serve to further demonstrate the value of diversity in the tech community. And judging by the high number of volunteers who contributed to making the week a success, the employees of BigCommerce would agree. Kelsey noted that she was surprised how many people were eager to be involved - nearly twenty BCers volunteered for anything from being available to answer questions to leading learning sessions. An unintended side effect of the internship was what BC calls a mojo boost – an increase in morale, excitement, and engagement with other team members. Aside from generating excitement throughout the office, the young leaders were also able to assist BC by giving us a fresh perspective on our product. While there was no business agenda behind the decision to take on the internship, there was a blindspot that the interns surfaced – since they didn’t have a lot of technical experience themselves, they were able to honestly point out how BC could better communicate with their non-technical users. It’s deceptively easy to assume everyone using the tools we build will understand the value and background that went into certain decisions. One thing that stuck out was how difficult the experience of launching a store could be for non-technical users. Merchants are somewhat forced into the developer experience, though that’s not the first prerequisite that comes to mind when a user wants to launch a store. When doing things like choosing the color scheme and layout and deciding on the look and feel of their store, Stevie realized “not everyone in our audience is a seasoned developer like we expect them to be.” Thanks to these young women’s honesty and the rapport BC was able to build with them, we have new data for our product planning. Tech employees usually work in a bubble. We talk to the same people about similar issues, and try to infer what people with different experiences want and need. But by spending just one week with the ARS interns, Stevie felt like she’d had a revelation. She noted at BigCommerce we make huge investments in the platform, some taking months to complete from start to finish. Long timelines can start to wear on morale. It’s easy to forget that there are things that can be completed in the context of a week or two, and that quick turnaround is invigorating. “You’re starting with nothing and end up with something,” she said. “You’re better off than you were before, and [I realized] I can bring that enthusiasm back to the team. It’s fun to end the week with something shipped and live in production.” Further, the young women served as a refresher on how to work with a team. When a decision needed to be made, they consulted among themselves and only asked for help or guidance when it was truly needed. By having everyone in one room and available, blockers were all but eliminated. The interns iterated quickly in their short time with BC, and Stevie said that begs the question, “Am I asking for permission when I don’t need to?” She joked that although the interns hadn’t been exposed to the Agile method of iterating, they were sometimes more Agile than BigCommerce. There were so many obvious gains at the product level, but at the company level, the opportunities are even more substantial. Kelsey pointed out this is a great way to show BigCommerce is a company that supports and actively seeks out opportunities to prove our dedication to diversity and inclusion. “It’s a way to get our name out there and become an involved member in the community,” which Kelsey noted should be highly valued everywhere, but is especially so in Austin. “It was cool to see how many people got involved, and there were tears in people’s eyes after the presentation at the All Hands [meeting].” Internships are not a new concept, and the benefits of BigCommerce’s participation are abundantly clear: the mojo boost, the relationship building, seeing what we work on every day through new eyes so we can build a better product. Jimmy said he is “acutely aware of the under-representation of women in tech and that much of this is attributed to not having a healthy pipeline of girls interested in tech at a young age.” One solution is to hold an open door for young women who want to try on a tech role to see how they like it. Silicon Valley Bank’s report details that 59 percent of startups have some type of program in place designed to increase the number of women in leadership positions (2019). Kelsey says participating in the internship reminded her that “what we do is exciting, what we do does make a difference.” If all it takes for four young women to get excited about a future in tech is four and a half days in a room and a handful of volunteers to assist them, why shouldn’t we be fostering their interest? It took a small leap of faith, but based on the success of the internship, BigCommerce will continue to champion initiatives to inspire future techies.", "date": "2019-03-08"},
{"website": "BigCommerce", "title": "Implementing iOS 13 Dark Mode in the new BigCommerce mobile app", "author": ["Dan Murrell"], "link": "https://www.bigeng.io/implementing-ios-13-dark-mode-in-the-new-bigcommerce-mobile-app/", "abstract": "When Apple announced iOS 13 in early summer 2019, among the many new changes to the UI was the long-awaited Dark Mode feature, which allows the device owner to change the overall theme between light and dark colors. With the BC app scheduled to ship 1.0 before the release and because its default appearance is extremely bright, the team took steps to minimize the amount of work required to support this feature. (We all wake up at 4am, checking our stores while still in bed, right?) One helpful trick when developing around a new version of iOS is as long as you compile with a prior build system, you’re protected from side-effects of a new operating system. Apps that are compiled with an iOS 12 build system will act like an iOS 12 app, even on an iOS 13 device. Normally this would go unnoticed, but in iOS 13, a lot of system native user interface, even within your app, automatically adapt to dark mode. Compiling with the iOS 12 SDK prevents that. The BC app needed to ship shortly before iOS 13, so publishing a build compiled with the iOS 12 SDK protected it from iOS 13 side-effects. However, once Apple released iOS 13, a developer is confronted with a decision to upgrade to the latest version of Xcode or battle Software Update to protect the now outdated version from being automatically updated. Who wants that hassle (more than they want the latest Xcode)? Fortunately we were ahead of the game. If you’ve worked on long-lived mobile applications for a while, you know that designs and tastes change over time, especially when new versions of system software arrive. The transition from iOS 6 to iOS 7, for example, represented a huge shift from skeuomorphic to flat design. That felt-textured background in your app went from “cool” to “eww” overnight. It was as glaring and jarring as a stark white background on a dark mode device today. A developer tasked with a complete overhaul of the design of an app can either hunt through potentially hundreds of files and make as many changes, or they can update one or two. We use five. :) By centralizing all the important design decisions, such as colors, fonts, icons, view margins, and spacing, a developer can make a sweeping change in as little as one line. All our colors are contained in a single struct, named using the language of our Design team (with a few exceptions). If Design tweaks the values of blue40 (our main action color), the entire app would change its action color. This is a start, but it’s not perfect for making an interface with a changeable theme. For that you need to use semantic colors (which we hint at at the end). Semantic colors attach a meaning to a color, regardless of the values of the color. Background, overlay, primaryText, secondaryText, actionText, errorText, and so on work as semantic colors. Fortunately, because we centralize our styles, this change was easy to do. But first, we need to add a small bit of code to our styles to not just know when we’re in Dark Mode, but also when we can use it. Because Apple laid the foundation for Dark Mode in iOS 12, there is already a trait available to check the user interface style. It just doesn’t hold a useful value until iOS 13. Swift also provides an easy way to check the capabilities of the device running your app, which is useful because we also allow the app to run on iOS 11. We combine these helpful Swift features to add a calculated property to our Style struct: Now, any time we need to check the Dark Mode setting, we can check Style.darkMode to find out if it’s turned on. The if #available(iOS 12.0) makes sure we don’t attempt to check the userInterfaceStyle before it’s available on the device, and before iOS 13, this value will always be .unspecified. Now we’re ready to convert the app to dark mode-capable. With a little help from the Design team, we create semantic colors to apply to UI differently, based on the darkMode setting. By using calculated properties, we can now evaluate the Dark Mode setting in real time each time we use a color. When the Dark Mode trait on a view is toggled, redrawing the view is all it takes to get a dark or light interface. Whenever the trait changes, UIKit calls this function on every view and view controller. To make this nearly automatic, we added a protocol that uses traitCollectionDidChange() so that any view or viewController conforming to the DarkModeAdapting protocol could override updateColorsAfterModeChange() to adapt to the new mode. Now what's left is to change every subclass of view, view controller, tableView controller, tableView cell in use to a DarkModeAdapting counterpart, and override updateColorsAfterModeChange() with your redraw logic. For example, Here’s a fun trick most developers use in an IDE that uses a type-safe language like Swift. Break your code and see what errors are generated. Then go fix them. Even though we refer to our colors as generally their appearance, in reality we tend to use these colors semantically. Essentials.blue40 is actually our actionText color. Essentials.grey70 is usually our primaryText color. But just to be sure, we can apply the fileprivate keyword to each individual color, causing the compiler to generate errors everywhere they are used in the source. fileprivate makes the property basically invisible everywhere in the source outside of this one file. Then we can manually inspect the 20 or so errors to make sure that yep, that’s our primaryText . Then it becomes a simple Find..Replace to change from grey70 to primaryText , blue40 to actionText , red40 to errorText , and so on. (Background was already done for us. :D ) It took only a few hours to make a drastic, sweeping change to the app to support a popular new feature in the new version of iOS, along with a combination of Swift and Xcode features, a few good habits formed over years of experience working on a variety of mobile apps, and an organized Design team. What could have been a frustrating, hunt and seek time sink, was actually little more than simple, methodical process with an anticlimactic “that was it?” at the end.", "date": "2019-10-25"},
{"website": "BigCommerce", "title": "GCP Instances, Go To Sleep!", "author": ["Gonzalo Bañuelos"], "link": "https://www.bigeng.io/gcp-instances-go-the-to-sleep/", "abstract": "How do you keep your developers happily developing, specifically within the context of a meshed, micro-service solution, requiring dozens of services and underlying executables, while preventing them from losing their ever-loving mind when they can’t reach reddit for a quick read because their workstation is petered out of memory and cpu? You give them a clustered development environment on which to work. Instead of running everything on a container orchestrator on the workstation, you spread the load out onto the cloud, on a few virtual machine instances (which we’ll call a cluster ) as well as onto the local workstation. You then use a bit of dnsmasq magic and a service registry (Consul) to make all references to running services trivial. It wouldn’t be complete if you weren’t able to code on any one component, so you add a way to bypass any running service with a local, in-development copy on the local workstation. Using any of their favorite IDEs, developers can write and test code running in the mesh without overloading the local workstation with everything at once. The solution itself was a work of art as much as it was a herculean effort to implement. Each developer now has her workstation as well as her cluster to work with. But think of that for a second, instead of just having one workstation, paid for and depreciating in value, a developer now has essentially 1 + n , where n is the number of vms you’ve allocated for them. These additional n machines are not depreciating in value and come with a sometimes steep monthly cost, whether you are using them or not. This additional monthly expense adds up quickly. So do you continue paying for the n instances that are still up and running on the cloud doing nothing if the developer is done for the day? Or, do you rely on the engineer to stop their instances and restart them when they next need them? How about neither. Google Cloud Platform (GCP) is more than happy to keep taking your money while you are asleep while the instances you’ve provisioned are not. There is a way to resolve this dilemma and capture some of that excess spend. Your average developer works around 8 hours a day. That’s eight hours of coding, testing, deploying, rinsing and repeating, sprinkled with trips to get coffee. GCP instances are charged on 1 hour increments. So do you leave the instances up and running when the day’s over so as to avoid any loss of productivity when next you pick up where you left off? No, you don’t. You try to leverage GCP’s apis to minimize your costs, as any responsible business should. As a pleasant side-effect, and as my experience has shown me, when you strive to save some money, you inevitably make your infrastructure leaner and more efficient. You find ways to use the least amount of resources while balancing the need to maintain a responsive, reproducible system. With the intention to save a few infrastructure dollars, we set out to implement a solution: enter Suspend and Resume. Until recently, GCP only offered you the ability to stop and start your instances. Just like any machine, stopping an instance will clear its memory, and shut down gracefully, where possible. Starting an instance works similarly, though you are responsible for loading back into memory any process you need to have running. This may entail using the init.d directory to run a script or any number of other monitoring tools. It’s up to you to make sure a vm start puts everything back where it needs to be. GCP has no clue what process you want running. You could say that perhaps the right approach is to stop and start instances not in use. That is a solution. But it’s a solution that will stop any running process an individual developer may have launched off-script. That is, if your mesh requires services a through n to run and is tooled for restart of such, if a developer wants to launch service n+1 , they’ll also have to make sure it can be restarted. Perhaps they aren’t that far along in their implementation yet. Perhaps they are running something completely unrelated to the running services, like a monitoring tool, or some ETL process. Stopping their instance will stop them in their tracks and possibly force them to recreate a “snowflake” they’d been working on when their own time and energy ran out. Let’s not do that to them. As much as we want to create an atmosphere of homogeneity among our developer class, they are as individual in what they are working on, and the way they work, as they are in their own person. So where do we turn? As luck would have it, GCP released — in an Alpha update to its compute API — the ability to suspend and resume an instance. This single enhancement changes the game. Instead of relying on developers to self-regulate and turn their instances off during non-working hours, we could do this for them. Importantly, a suspend will suspend the running processes and snapshot the memory, so that a subsequent resume continues exactly at the same place it left off. You don’t have to gracefully quit running processes and you don’t have to tool to get services back to where they were before a suspend . Also, and more importantly, we reap the same reward as stopping an instance altogether. That is, we incur no cost while a machine is suspended except storage of the snapshot, which is chump change compared to keeping the instances running. If your developers only actively use their clusters of instances 8 hours a day, we can save up to 16 hours of cost by suspending the underlying instances. Win, win! So let’s expound on the part about putting things to sleep for our developers while they are off the clock. The interval between automatic suspending and resuming an instance is what we term “Automatic Sleep”. This is a period of time bracketed with a suspend, followed by a resume that — hopefully — goes unnoticed by the corresponding owning developer and harmlessly saves us a wad of cash. In an organization with hundreds of developers and contractors, spread across multiple time zones, keeping track of who owns what instances and what state those instances are in is a job for a piece of software we wrote that we affectionately call Maestro. Like a well-coordinated orchestra comprised of wind, percussion and string instruments, a Maestro coordinates who plays and at what time. Similarly, our Maestro keeps track of our developer clusters. It knows when to automatically put them to sleep and when to wake them, always with the owner’s corresponding time zone in mind. We typically automatically sleep the clusters during the early hours of the morning, when we presume most people will be asleep themselves. Additionally, if developers are on extended leaves of absence, they can put their clusters to sleep indefinitely, waking them upon their return. Using a cli tool we built, developers can ask for their clusters to be awoken, suspended, rebuilt, etc. The tool also integrates with Slack to give some useful notifications. Suspend and resume can be done asynchronously if you wish to fire and forget. However, because we like to report back to our developers when their cluster is available if they manually resume them — since they really can’t continue working until their cluster is up and awake — we use GCP’s operations api with a predetermined timeout period to poll for when the operation of suspending/resuming has completed. Polling for completion also allows us to update our object model with the current state of the underlying instances. What we found is that suspends take about 4x more than resumes . Resuming a single instance is still under 4 minutes. Your mileage may vary. Our instances are typically 2cpu x 24GB-RAM x 200GB-Storage in size. If you plan to suspend hundreds of instances at once, it’s best to use multiple threads or work queues to perform them. It’s not uncommon for some developers to be night-owls. I’m one of them. I work best from 8pm to 1am. So having a pre-determined automatic sleep cycle that turns off my instances when I’m most productive is not the best use of my patience. So going forward, we intend to give users the ability to set their own optimal work hours. This would free the instances to be put to sleep during the complement of that period of time; any time not during the optimal work hours. As well, when there’s that off-chance a fire drill keeps us up late, even the day-walkers, having the ability to anti-snooze would be good. It would work like a snooze button on your alarm. But instead of delaying a period of time before it tries to wake you, our anti-snooze button would delay when automatic sleep kicks in, giving you a day or so before your instances are next put to sleep. Originally published 4/2/2019 on Medium .", "date": "2019-04-03"},
{"website": "BigCommerce", "title": "More than a POC - Remembering Matt Weiss", "author": ["J. Colby Fisher"], "link": "https://www.bigeng.io/more-than-a-poc-remembering-matt-weiss/", "abstract": "There are some people you cannot help but notice. You know, the kind who stand out even when they are sitting down. The kind you always want by your side. That's Matt. Calling him impactful is an understatement. He had this (un)natural ability to bring out the best in people. You can probably imagine how this played out within BigCommerce (BC). Matt was someone you wanted to be around, and he was the one who would always help you accomplish your goals. Matt's 2020 endeavor was to expand the billing domain at BC. He, and others, saw a huge opportunity to grow BC by redefining what a traditionally \"behind the scenes\" team could do. He spent the first quarter (Q1) crafting a new strategy that would solve the complexities of the business problems while creating a vibrant environment that could naturally motivate the team tasked with the work. By the end of Q2, his small task force produced a rough proof of concept (POC) for a brand-new direct-to-paid (DtP) onboarding experience for new BC merchants. This DtP flow was to be the first of many new opportunities enabled by the newly expanded billing domain. Having proven the concept, Matt handed off the \"Billing POC\" to the Billing Platform team. While the overall strategy was grand, our first deliverable was incredibly tangible. His POC provided solutions to many technical hurdles, and gave us clearance to knock down any walls that might get in our way. First, we had to figure out how to provide a paid store to a brand new merchant. BC only supported trial stores for new merchants, and our systems did not account for adding payment methods from our new service. To solve this, we created a new API that syncs our system with our billing provider synchronously. Running this between adding the merchant's payment method and provisioning their paid store solved the problem! Next, we had to figure out how to correctly authorize requests from this unique service. Thankfully, we were able to resolve this problem without adding additional complexity. Finally, we had to gracefully handle failures. For any failure that happens prior to creating a BC account, we present the error to the user on the page where the error occurred. These failures generally fall under the \"invalid input\" category, so users are able to correct these errors themselves. If something fails after BC account creation, we fall back to the tried and true trial store flow. In the unlikely event the trial store fallback fails too, we present our user with every Support contact method we have. Our Ninjas will take care of them from there. Our team's primary goal for this proof of concept was to prove we had the ability to redefine what a store checkout experience is for our merchants. We cut some scope, compromised effectively, and did not let perfect become the enemy of good. The Billing Platform team reached a code-complete state in November. The launch went pretty smoothly: all we needed to do was replace a single set of stale credentials. Celebratory gifs, encouragement and congratulatory remarks were shared by all. It was a great couple of days. Matt was especially proud of our success. We created a solid foundation of knowledge and experience that will be critical for expanding the billing domain at BC. We happened to ship a fully functional product along the way. The team accomplished the first goal (of many!) set by Matt. We did it. Matt passed away one week later. From the get-go, we knew the code for our direct-to-paid application would eventually be deprecated. This was just the first of many stepping stones on our journey to expand the billing platform for BC. One day, we will sunset this service. Goodbyes are hard, but we took solace knowing it would probably be a long time before that day came. We did not want to believe we would say goodbye to Matt before our work was finished. And yet, life forced us to. Honestly, it has not been easy for everyone to come to terms with his passing. We remember our last meetings with him. Our last conversation on Slack. It's hard. As the Austin office shared their favorite and last memories of Matt, there was always a common theme: Matt lifting up his teams and making sure each person was recognized for their efforts. This blog post is the result of my last conversation with Matt: I know what I've written isn't what he had envisioned for this post. Matt would never dream of spending any time celebrating himself. Matt sought to celebrate and edify the people around him without bringing any attention to himself. It was not about him; it was about the team. \"How can I empower my team? What can I do to encourage and promote them?\" Matt thought about and acted in our best interest until the end. The impact he had on each one of us - even the ones he never \"met\" in person - was unreal. BC has a Matt-shaped hole that will never be filled. We mourn his loss, yet we also give thanks for the time he spent devoted to the people he worked with. While he may no longer be with us in the day-to-day, his impact will never be forgotten. Now, it is our responsibility to continue his legacy. I'd like to invite you to join BC in that effort too. How can we edify one another? What can we do to make a difference in someone's life? Who is that someone you can start investing in? How can you make a difference every day?", "date": "2021-05-04"},
{"website": "BigCommerce", "title": "MFW: How I took back control of my work / life balance", "author": ["Jason Schmitt"], "link": "https://www.bigeng.io/mfw-how-i-took-back-control-work-life-balance/", "abstract": "I know many of us struggle with having multiple streams of content coming at us from all directions in all phases of our lives. Last year, I reached a critical point for myself and realized that it was time for me to regain control. In the end, I feel ALL phases of my life have improved. I would like to share some of the ideas and changes I implemented to get me there. This is my story and your mileage may vary, but hopefully you can find some inspiration if you are feeling some extra weight on your shoulders. As a quick introduction, I am a Director of Software Engineering at BigCommerce in our office in downtown Austin, TX. I’ve been with BigCommerce for 3 years. I have been married to my wife, Sheila for 15 years this May and have 2 children, Ellie (6) and Cooper (3). I also have a dog, Scout, who is 14. We adopted her at 8 weeks old from a local shelter. What does “work / life balance” really mean? We are all so connected to everything via our various mobile devices, how do you really balance? For me, this required an understanding of what “things” actually needed balancing. It felt like there was more than just “work” and “life”. “Life” encompasses so much more. This is where the acronym MFW comes in. There are three things in my life which I must balance, not just two...Me → Family → Work. The acronym represents a simple prioritization which can help guide me through my daily decision making. I need to give credit for this acronym to Sheila. This is how she defined her New Year’s resolution for 2019 and when she described it to me, I realized that is what I had started doing in early 2018. MFW: There are three things in my life which I must balance, not just two...Me → Family → Work. By normal definition, you would think about balancing work and non-work but I have realized that I am no good to my family and no good to my job if I am not taking care of myself. This seems selfish on the surface but it is actually the opposite. My ultimate goal is to improve the other 2 aspects of my life. You have to figure out what it means to “take care of yourself,” but in my case this fell into a few categories: I’m not alone here, but trying to remain healthy requires a lot of mental and physical strength. But most importantly it requires time. With 2 young kids at home and a full-time job, finding that time can be hard. If this is important to you, you have to be firm and carve out time that will allow you to focus on this. Depending on your normal workday, this may mean some hard decisions. More on that later. Our office had a timely introduction of weekly yoga classes in the office just as I was starting to look into yoga as a means of keeping myself limber as I grow older...and stiffer. I can't always make it to classes but have been able to carve enough time to feel the benefits. Additionally, I have found other exercise opportunities which work into my schedule a little better than what I had previously been doing and I make sure to carve time out for that. Something fun to look forward to outside of work and family. For me, this has been playing around with smart home automation and, in the fall, college football. Hook ‘em, Horns! As with health / wellness, if this is important to you, you must hold firm and make time for these. Luckily, for me, Sheila is also a huge fan so this is a fun thing we get to do together and have been for 20 years. Sometimes what hurts you most when so many things are bearing down on you is not truly accomplishing anything – because you are trying to impact everything. A favorite phrase of mine is “if everyone owns something, no one owns it,” and that applies here. Finding ways to get “wins”, or truly accomplishing something, can go a long way. This may have to start with the low-hanging fruit. I started by always making sure the dishes were taken care of and that clean laundry (it never stops with 2 kids!) is put away. This turned into keeping the kitchen island clean and then all the common areas tidy. These were tasks that Sheila had no interest in taking on, while I was not thrilled with giving the kids baths. Previously, we were trying to share all tasks which was terribly inefficient and many times meant she was doing most of them. By actively splitting these duties, I was feeling great about my contributions at home...getting some wins where it matters most to me. If I’m not taking care of myself then I am not going to be as happy as I can be, if I am not my happiest, then I am less likely to give my all for my family and my work. Most everything I do is for my family. They are the most important thing to me. It is very important that I am able to spend time and be involved in the things happening with my kids. There is still a balance, however. For example, I can’t go to every family lunch offered at my son’s day care but I can block off time to be a guest reader for my daughter’s elementary school class. I can say no to later meetings if they conflict with allowing my daughter to participate in after-school gymnastics. I can find ways to make up for the content from those meetings. If you can miss meetings for PTO, you can miss them for family events as well. Sheila also works and is building a career of her own. I strive to make sure that we are truly splitting the parental duties right down the middle and take on extra when her job requires me to and she does the same. A big thing I focus on is controlling how often my face is buried in my phone when I’m at home. From 6pm until ~8pm is dinner, homework, bath, and bedtime. During dinner, the phone goes dark and the focus is on how the day went for my kids and Sheila. During bath, I’m doing dishes and cleaning the mess from dinner. Because family is so important to me, if I wasn’t pulling what I felt to be my proper weight, that would cause this aspect of my life to be in a suffering state which will, in turn, carry into my work. Gotta get family right. Do you live to work or do you work to live? There is not one right answer to that question. Many people are able to devote their lives to their work. As long as that is their choice, more power to them! For me, I work to live. This means that I do need to work for the life I want to provide for myself and my family, but the end goal is the quality of my life outside of work. In the sections above, I call out being firm about taking the time for the things that will get me and my family taken care of but, as with everything, there is a balance. Your time is precious and valuable and no one is going to protect it for you. If your daily work routine consists mostly of meetings, as mine does, these may feel like immovable barriers! But ask yourself this: “will the team / company fail to meet its goals if I don’t attend the X meeting?” In most cases, the answer is probably “no”. Another question, depending on your role: “Can I rely on someone else to attend that meeting for me?” In some cases this may be “yes”. Can a meeting's agenda/goals be addressed asynchronously? Maybe using a Google Doc to collect comments on an idea or plan. Your time is precious and valuable and no one is going to protect it for you. Additionally, be open with your manager about your stress levels and what you feel may relieve those stresses. If you have a supportive manager, they will work with you to find the right balance. While you do have obligations that come with your paycheck, there is usually some flexibility in how you meet those obligations. Once I went through the exercise of eliminating (read: declining) or handing off some of my “obligations” where it made sense, I started seeing more open blocks on my calendar. I started filling these open blocks of time with work that has resulted in newly defined Engineering Roles and an Engineering Mentorship program among a few other things. It’s freed me up to spend some time learning and researching other topics which I now feel I will be able to tackle. It has also freed my mind up so that when I am home, I am not stressing about all the things I wanted to make progress on that day but just couldn’t find the time to. It’s made me more present at home so that when I’m back at work, I’m not distracted by all the things I’m not doing at home. Every time I start feeling anxiety or stress or other negative feelings creep in, I revisit my priority list -- Me → Family → Work -- and ensure I’m adhering to that and it always gets me back right. How are you taking control of your work / life balance? You are the only one who can make sure it’s got the right balance to suit you.", "date": "2019-04-10"}
]