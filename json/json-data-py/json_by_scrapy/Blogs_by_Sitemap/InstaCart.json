[
{"website": "InstaCart", "title": "know your carrots 5 alex", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-5-alex-b31dcdb57d1f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher , Google Play or Tunein I was on vacation last week so this week we not only present a new episode but make up for the gap with something extra — a screencast with Alex implementing a small feature for the psycopg library. Here is a video of Alex working on the PR: drive.google.com And here is the short interview: soundcloud.com When joining Instacart, Alex moved from Canada, he is a big lisper (have you read a book yet?), plays electric piano for fun, and keeps pushing the frontier of new technologies used by Instacart’s Catalog team (we actually call it CATMAN). As always, big thanks to our amazing Forge team for all the help with the podcast. See you next time for the interview with Dominic, who will share his advice for newcomers in software engineering. Instacart Engineering 2 1 Podcast Culture Software Engineering Technology Instacart 2 claps 2 1 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-16"},
{"website": "InstaCart", "title": "building a data science product in 10 days", "author": ["Houtao Deng"], "link": "https://tech.instacart.com/building-a-data-science-product-in-10-days-d2f4688567b0", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At startups, we often have the chance to create products from scratch. In this article, I’ll share how to quickly build valuable data science products, using my first project at Instacart as an example. Here is the problem. After adding items to the shopping cart on Instacart, a customer can select a delivery window during checkout (illustrated in Figure 1). Then, an Instacart shopper would try to deliver the groceries to the customer within the window. During peak times, our system often accepted more orders than our shoppers could handle, and some orders would be delivered late. We decided to leverage data science to address the lateness issue. The idea was to use data science models to estimate the delivery capacity for each window, and a window would be closed when the number of orders placed reaches its capacity. Here is how we built a v1 product in 10 days. We started with planning so that we could work on the right things and develop a solution fast. First, we defined the metrics to measure the project progress. Second, we identified an area that was achievable with a high impact (low-hanging fruit). Third, we came up with a simple solution that could be implemented quickly. Metrics . The percentage of late deliveries per day was used to measure lateness. We didn’t want to close delivery windows too early and fail to capture the orders that could be delivered on time. So, the number of deliveries per day was used as a counter metric. (We now use shopper utilization as a counter metric.) Low-hanging fruit . Data showed that for days with a lot of late deliveries, the majority of the orders were placed one day before. Therefore, we decided to focus on the next-day delivery windows ( Tomorrow ’s windows in Figure 1). Solution . To deliver an order within time window T (between T and T+1) , a shopper may start working on the order before T . Figure 2 illustrates that a shopper begins to work on an order in window T-2 , and delivers the order in window T . As most orders took less than two hours, the capacity of a delivery window T mostly depends on the number of shoppers at time window T, T-1, and T-2. Assuming a linear relationship, the capacity of a delivery window T can be written as capacity(T) = a+b0*#shoppers(T)+b1*#shoppers(T-1)+b2*#shoppers(T-2) There can be other factors (e.g., weather) also affecting capacity, but we decided to start simple. We followed a typical modeling process: feature engineering, creating training and testing data, and comparing different models. However, once we felt the models were reasonably accurate, we did not invest more time in models. Firstly, models were only one part of the system. Secondly, the improvement in model accuracy did not necessarily translate to the same degree of improvement in metrics. Features and Data . For each delivery window in the past, we had the following data: the orders delivered within the window, and shoppers’ time spent on the orders. Figure 3 illustrates for three orders delivered in time window T (between T and T+1 ), there were 2 shoppers worked in window T-2, and 3 shoppers worked in both window T-1 and T . Figure 4 shows one row of data created from this example. Linear model . Using #orders( T ) as the response variable, other variables as the predictors, we built a linear model on a training data set and tested it on a validation data set. The form of the model is as follows #orders(T) = a+b0*#shoppers(T)+b1*#shoppers(T-1)+b2*#shoppers(T-2) The predicted values vs. actual values for the validation data are plotted in Figure 5 (left). The mean of the actual values at each predicted value and the 45-degree line are also plotted. Non-linear model . We built a random forest model for comparison. The predicted values vs. actual values are plotted in Figure 5 (right). The random forest model was not substantially better than the linear model, and so we felt comfortable to proceed with the linear model that is easier to interpret and implement. Prediction . With the linear model, we can estimate the capacity for a future delivery window T with the following formula capacity(T) = a+b0*#shoppers(T)+b1*#shoppers(T-1)+b2*#shoppers(T-2) Note that in this formula, #shoppers(t) represents the number of shoppers scheduled at a future time window t (t = T, T-1 or T-2) . We used databases as the interfaces between data science and engineering components. In this way, the dependency between data science and engineering can be reduced (vs. embedding a data science model in the engineering code), and the ownership of different components can be clearly defined. Figure 6 illustrates how the system works. Data science components . There were two data science jobs, model training job and prediction job, both triggered by cron (a time-based scheduler) at pre-defined frequencies. The model training job ran every week, fetched the most recent order_shoppers data (orders and shoppers’ time spent on the orders), fitted the models and saved them into a database table (models). The prediction job ran every night, fetched the models and scheduled_hours (future scheduled shopper hours) data, and estimated the capacity for future delivery windows. The estimates were then saved to the capacity_estimates table. Engineering components . The capacity counting job was created to consume capacity estimates and provide the delivery availability of each window for the customer app. It was scheduled to run every minute, got the capacity estimates and existing orders, calculated if a delivery window was available, and saved the availability information to the delivery_availabilities table. Also, when a customer placed an order, the order information would be saved to the orders table, and the capacity counting job would be triggered. We did a sanity check on capacity estimates, and two modeling issues were found and fixed. Sanity check . We ran the prediction job that generated the capacity estimates for future delivery windows. Then, after a window became obsolete, we compared the estimated capacity of the window to the orders actually accepted by the existing system. We found that in some cases the existing system took fewer orders than the estimated capacity but with substantial lateness (illustrated in Figure 7). This indicated the capacity was over-estimated in those cases. Based on this insight, we found two issues. Issue 1: mean prediction . The models we built predicted the mean. It can be seen from Figure 5 (left) that there are data points below the mean line. The mean predictions would over-estimate the capacity for those data points. To solve this, a prediction interval was constructed, and a lower percentile level was used. Figure 8 shows the 25th percentile and 75th percentile levels. Issue 2: data inconsistency . #shoppers used in prediction is the number of scheduled shoppers at a future window , and shoppers can cancel their scheduled hours before the window. However, #shoppers used in model training did not include canceled hours. So, the data used for prediction and training were not consistent. To fix it, cancelation rates were estimated and included in the formula capacity(T) = a+b0*#shoppers(T)* {1-cancelation_rate(T)} + … Before launching the new system to customers, we started internal tests (without impacting customers) and made adjustments accordingly. Percentile level . We adjusted the percentile level to pass the sanity check mentioned in the previous section. Caching . Jobs were made faster by caching (storing frequently used data on the servers to avoid redundant calls to the databases). Figure 9 shows the percentage of late deliveries by day around the product launching time. The new system achieved our goal of substantially reducing late deliveries (without reducing the number of deliveries). It was a quick success. Since the initial launch, we’ve continued to iterate, including estimating the capacity for same-day delivery windows. As a data scientist who joined a startup for the first time, I learned the following lessons in quickly building valuable data science products identifying impactful and achievable work reducing the dependency between engineering and data science components focusing on improving the metrics, not necessarily the model accuracy starting simple and iterating fast The learnings are still applicable to the projects we do at Instacart today. Note : Andrew Kane contributed to the first version of the engineering components, and Tahir Mobashir and Sherin Kurian contributed to the later iterations. 🥕 Interested in joining us? Apply here . Instacart Engineering 791 5 Thanks to Ji Chen , Sharath Rao , Kaushik Gopal , Jagannath Putrevu , and Ganesh Krishnan . Data Science Capacity System Logistics Machine Learning 791 claps 791 5 Written by Machine learning @Instacart softwaredeng@gmail.com Instacart Engineering Written by Machine learning @Instacart softwaredeng@gmail.com Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-27"},
{"website": "InstaCart", "title": "know your carrots episode 3 starring udi", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-episode-3-starring-udi-809ca57db0bd", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher or Tunein Last time I promised interview with Gordon, but we are keeping Gordon for next week. This week I present Udi Nir. Udi helped build our culture for more than three years and watched our team grow from fewer than 50 to over 150 engineers, so it is no surprise that he mostly dealt with hiring, high level technical decisions, and coaching the team, even though he is also an accomplished engineer. A few quotes from our guest: What do you do right now at Instacart? The people aspect is trying to understand what is going on, which has a lot of listening involved, trying to talk to as many people as possible on theengineering side, trying to understand what is working, what is not working and how or where we can make the change… … another things is process — trying to get our team to be more efficient and more effective... how we set goals, do roadmap reviews and performance management, compensation, leveling and other things… …the technological aspect … big decisions that impact the entire org. I like to be involved and also get other people who are even more relevant [to be involved too] … and I want us to make those decisions jointly … I generally definitely believe in decentralized systems and team ownership, but we need to make some decisions cross-team … if we decide to add new languages to our toolkit we need to discuss it as engineering, because otherwise it is very hard to share and move quickly … we have to discuss it and do it consciously, instead of just let it happen… Is it fair to say that your focus moved from technology to people over time? …it really depends on the size of the company … if the company is large I am probably not the best coder compared to anybody else, on the other hand, I will have much larger impact while dealing with people and processes, and higher-level technical decisions … in a smaller company I would be hands-on coding, as it will add more impact … it is understanding where the leverage is. What are some technologies that you are excited about? I know that many people at Instacart are really excited about Snowflake. I think Snowflake is great, it brings the simplicity and ubiquity of SQL with the scale of distributed systems, it is definitely something that I like, I like the concept, we don’t yet have a lot of “mileage” on it, so we can’t say it is as great as advertised, but we are getting there. I have very strong preference to being pragmatic, that is why I fully support the SQL path, if SQL works, why do some crazy *** as Hadoop. And Snowflake still brings benefits that other distributed systems have. For the same reason we are still on Ruby On Rails, it allows developer productivity, which is the most crucial and scarce resource that we have , and yes it is not fully optimized, we could run on way fewer servers if we change everything to Go, but it does not matter to our business models. What we some other pragmatic decisions? … Adopting React was one more [pragmatic decision] … a lot of engineers like it … we had similar discussions when we introduced Cassandra and then Druid… it is not that we don’t want new technologies, its just that it must make sense for us on a broad set of criteria, not just “the most efficient” or “the most advanced in terms of technology” … Any example of not so great decisions we’ve made? might be debatable — but we have two message buses, Kinesis and Hub (RabbitMQ) not sure that’s wise… I am not sure I would make the same decision, but it was not a huge mistake, it is working for us pretty nicely… … the way we split the monolith probably could have been done better… we started really quickly with Catalog DB, and …. now that’s what matters most for development productivity… we did not do lots of technical decisions, that Ican think I want to reverse, we’ve been pretty pragmatic and evaluation was right for that time. Big thanks to Udi, Jon, Muffy, Dominic, and Bill for their help with this project. If you have any feedback about the format or the content of the podcast, please send it to Viktor at instacart dot com. Also, stay tuned for our next episode starring Gordon, which was promised last time. We are going to discuss infrastructure, catalog, tooling, distributed systems and Gordon’s personal projects. Til next time! Instacart Engineering 11 Instacart Culture Software Development Interview Podcast 11 claps 11 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-26"},
{"website": "InstaCart", "title": "how i survived carrot university", "author": ["Jeremy Flanagan"], "link": "https://tech.instacart.com/how-i-survived-carrot-university-4067d16291c0", "abstract": "Eng Open Source Data Science Machine Learning Android iOS My journey from Classical Musician to Instacart Shopper to Software Engineer Five years ago, if you had told me I would be a writing code for a successful startup, I probably would have laughed and gone back to practicing my clarinet. But the universe sometimes has a way of cajoling you into an unforeseen path. The tale that follows is one of intrigue, struggle, laughter, and occasionally pandemonium. Here’s WHAT happened. I came to Instacart in November 2013 as a full service shopper. As a classically trained musician with sufficient student loan debt, I was blessed with the typical rock/hard place conundrum of pursuing my artistic passion, but also paying my bills while resisting moving away from the Bay Area just to procure work. My truck was too old for Lyft and I needed the flexibility of contractor work to continue teaching and gigging. And so I made the choice to sign up with Instacart as a personal shopper. Within only a few weeks I learned how to use my parking brake on the crazier hills in San Francisco and that the giant bags of trail mix from Whole Foods are not meant to be a single serving. Fast forward to September 2014. Instacart was growing and needed to provide more robust support for its shoppers. Six other veteran shoppers and myself embarked on a 6-month journey to form the first official Shopper Happiness Team. It was an exciting time. We were policy-less and fancy free, developing procedure on the fly, putting out fires, wrangling rogue orders, and generally saving the Instacart world from pandemonium one call at a time. But as with all things, we started to grow up. Structure and policy started to seep in. In order to scale, we joined forces with the then separate Customer Support Team. I was promoted to Project Lead of Tools, charged with maintaining and optimizing our phone and ticketing systems. Up until this point, I had taken a SQL course in house (V useful) and few Ruby workshops/online courses. However, without daily repetition and practical application the skills didn’t stick. I mean, I could execute simple iteration over an array of fruits, which may seem relevant when working for an on demand grocery delivery service, but it turns out that wasn’t enough. Luckily, in order to serve the growing needs of the business, I needed to learn to manipulate the tools better and that required learning some actual programming. It was REALLY HARD… at first, but I only broke stuff like six ( read: a thousand ) times before I realized that writing directly to a production script is not a best practice. My confidence grew and I started dipping my toe into Zendesk’s API console. Despite the calls of my manager(s) to focus more long term tool strategy, agent training, product request negotiations, I spent most of my time beefing up the tech skills, and then yet again, fate stepped in. In March 2017, Instacart announced that it would be hosting an in house “bootcamp” of sorts for employees in a non-technical role who were interested in expanding their technical skills and potentially transitioning to a role as an engineer. SQUEEE! Application Proposal => Interview => Acceptance => Cancel all my social calendar events for the foreseeable future. Cue Rocky style montage with a with a delightful SQL instructor , having no clue how to create the Game of Life without dead up copying someone else’s code, and finally screaming “ current_user, current_user !!!!!” when we figured out how to manage user authentication on a Rails app with a React client (spoiler alert: That is not beginner struggz. I’m talkin’ DAYS ). Here’s WHY it happened. Zendesk, in a nut shell, is a ticketing system with users and organizations and custom fields and action based triggers and APIs — a veritable amusement park for the unbeknownst dormant programmer within. I had been given the keys to a very messy kingdom akin to Robespierre’s France and I spent the next two years turning that House of Cards into something resembling a stable, predictable, and, most importantly, alarm blaring system. And let me tell you, I was ready to King Arthur that sword right out of the stone. The first few months were rough as I needed to do my job and make necessary changes, but also take time to explore the current configuration, which turns out is a V useful skill when starting as a engineering intern at an established company. Over time, I was able to not only manage the changes needed, but also improve our setup to track potential errors, remove “dead code” as it were, and implement what I would consider a scalable configuration. Eventually there was a need to write ruby scripts for use with the Zendesk API, mostly for expediting user management and ticket export, which cemented my comprehension of collections and nested resources, words that only 6 months earlier had very different meanings. Our telephony setup was even more conducive to the forthcoming metamorphosis. Instead of a conventional keypress or voice activated IVR ( Interactive Voice Response ) — listen to 49 menu options and press 0 repeatedly until you start screaming “REPRESENTATIVE” into the receiver — we decided to route calls automatically based on the caller’s state. One of our engineering teams worked with RingCentral, our telephony provider, to create an API where RingCentral sent Instacart the user’s phone number and Instacart would return a JSON object with the user type and a number of other attributes allowing us to sort the calls accordingly. The biggest UX win was that we could easily prioritize calls that were most likely of an urgent nature. For example, customers with an order in progress would skip to the top of the queue, as would shoppers who had recently experienced a card decline at checkout. Aside from the real time benefits, we would also have an immense data source free from contamination by end-user or agent error. Disclaimer: I didn’t build the aforementioned “APIVR” (patent pending), but I did have to manage it, troubleshoot, and eventually overhaul it, at least from the RingCentral side. Unlike many phone providers that give you a standard looking Admin GUI, I worked in a Windows VM (yes FREALZ) program that sort of looks like Lucid Chart and JTree got smashed together Frankenstein style. The actual functionality was much more robust than a lot of other systems, and allowed to realize our dream of a user-input-free IVR. The superiority complex of being a decade long Apple user being forced to suffer an outdated program on Windows eventually softened into a “I’m a badass rocking it ole’skool” thing, but I digress. Having very little in my programmer toolbox combined with being ignorant of best practices and basic troubleshooting, I sort of just dived in and broke a lot of things. Hell, I didn’t even know what language it was in — turns out the code was in C#, a serendipitous allusion to my former musical life, the curly brace taking on a new and initially more error-prone role. The main logic was a messy block of nested if else statements designed to sort incoming contacts based one of several boolean attributes through elimination. Amusingly enough, the same script employed a switch statement later. I imagine there’s a low hanging refactor in the future. So even before I started an internship I had honed some street cred: Developing a plan of attack when production breaks Working with the labyrinths and quirks of an existing code base Learning a new language and reinforcing common programming concepts Making defensive decisions to track and prevent production errors Examining and optimizing existing code as relevant But alas, it wasn’t all Windows VM shade and ego boosts. In hindsight, among my shortcomings were not actively documenting changes and processes. In the thick of it, my decisions seemed cogent, efficient, and sometimes even brazenly ideal. In fact they were — to me that is. I quickly learned when working with my successor that my design and choices weren’t always the most obvious. It required a great deal of domain knowledge and context. Spending the extra time to not only make my code more clear, but also create reasonable documentation would have saved hours in questions and unnecessary experimentation. The silver lining is I see that from the other end now, and not only do I try to make my own code readable and hopefully reasonably documented, but I also approach existing code with more caution and reverence. So, to come full circle, how did I survive Carrot U ? I didn’t see it as a 4 -month Tough Mudder race followed by a victory party where I eat every donut in sight, but as an extension of the road I was already on. I didn’t come out of the program as a software engineer. I learned how to learn programming, which happens literally every day now. It’s challenging and frustrating and rewarding and exciting and stupid and mind-blowing and wanting to throw your computer out the window because those damned fickle specs are sentiently ruining your life and everything you need to keep your career from getting stale. Overall, a big win. Interested in having your own Instacart adventure and solving for our customers and shoppers? Come work for us! Instacart Engineering 109 Programming Professional Development Engineering Learning To Code Startup 109 claps 109 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-02"},
{"website": "InstaCart", "title": "refactoring for change", "author": ["Peter Lin"], "link": "https://tech.instacart.com/refactoring-for-change-ac4c5e2f4a6", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Working on a large codebase with multiple engineers, you often notice parts of the code remain unchanged for long periods of time, and other parts that seem to change all the time. By identifying and refactoring code parts that are unstable, one can introduce patterns that are open to change. This allows engineers to make updates easily and safely, while keeping the codebase simple and readable. Initially, code is almost always written as if it will be completely stable, so it is usually not designed to be easily changed. This makes sense. When creating a new class or service, it’s difficult to predict how it will evolve in the future. As the product evolves, changes tend to concentrate over particular areas of the code. The frequent updates take a toll, and these code parts often gradually become the messiest in the codebase. They are easy to spot; if you see long if/else statements or a whole chain of method calls, then it’s likely to be an unstable area in the codebase. Once identified, you can make the code more manageable by refactoring with patterns that are open to change . In this blog post, I’ll highlight one area in our codebase that has been particularly unstable for us, and the pattern we implemented to manage ongoing changes. Instacart is a grocery delivery marketplace, so naturally, we have a lot of items available to our customers. These items are stored in ElasticSearch, and after retrieving the items out of ES, we often run various transformations on the item itself, including personalization, data fixes, and pricing updates. Over the span of a few years, we added many item transformation methods, resulting in code that was messy and error-prone. The code gist below shows a subset of transformers, many with their own conditions for transformation. With every transformation added, the code becomes harder to follow, and increasingly at risk of breaking, since a single transformation error can take down items retrieve. When we had the opportunity to rewrite the items retrieve service, we wanted a clean way to handle all of these transformations. We came up with the Transformer pattern, which encapsulates item transformations into classes. Transformers are designed to contain all the logic for a particular transformation and have two methods: applies? , which determines if a particular transformation applies, and a transform! method, which does the transformation on the model. Item transformers are then used in the items retrieve service, like so: In the code above, the transformers array is open to change, in the sense that it is designed to be changed. Adding or removing item transformations can be done with a single line change, without having to make any other changes to the items retrieve service itself. Which transformers apply, and the order in which they are applied, is readable and can be easily understood. In our experience, when you open up areas to allow for change, engineers will happily adopt the pattern if it is straightforward. Engineers will almost always do the right thing if the right thing is easy to do. The transformer pattern also makes testing easier. Instead of creating an item retrieve service instance to test a particular transformation, engineers can test transformers independently, without needing to test the retrieve service itself. Over time, transformers also allow us to build more advanced functionality at the platform level, such as: benchmarks for each transformer to know precisely how much time each transformation is taking assign “owners” to each transformer so we can route errors correctly and efficiently “ safely ” apply transformations, so if a transformer fails, it simply gets skipped, instead of taking down the whole retrieve service each transformer can declare their required attributes, so we only fetch required attributes from the data store, improving performance Changes are inevitable over time as the product evolves. By recognizing areas of the code that are frequently updated, then refactoring with design patterns that are open to change, you can help ensure future updates can be made easily and safely, while keeping the codebase simple and readable. I hope you have found this post interesting. For more posts like this, check out Instacart’s tech blog . We’re also building our engineering team: if you are interested in building great APIs, creating magical consumer app experiences, scaling infra, or anything engineering, we’re hiring :) My thanks to Viktor Evdokimov, Dom Cocchiarella, and Jason Kozemczak for reading a draft of this post. Instacart Engineering 13 1 Refactoring Patterns 13 claps 13 1 Written by Engineering @ Instacart Instacart Engineering Written by Engineering @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-12"},
{"website": "InstaCart", "title": "know your carrots with jonathan phillips", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-with-jonathan-phillips-f7d13922b4bd", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher or Tunein Hi, this your host Viktor and I am presenting the second episode of Know Your Carrots starring Jon Phillips. Jon is Canadian but spent his early years in Bangladesh and Thailand. In this interview, we speak about Jon’s startup experience, how he joined Instacart, and his roles and responsibilities on the Search Infrastructure Team. Jon makes sure the “lights” stay on and customers can find what they’re looking for on the store front. The nature of his role at Instacart requires him to think about improved tooling to aid in stability as Instacart grows. Jon is also a serial hobbyist so stay tuned for his thoughts on how to hobby! A few quotes from our guest: What are you doing at Instacart? Most of our roles and responsibilities on the team are directly related with scaling of our infrastructure, keeping the lights on, making sure we are not on fire, and data ingestion. We handle data ingestion from our partners … and search powers all of it. … on stability of the search We have multiple different search clusters, which are isolated by business use … we have a cluster for background/reporting use, and we have two frontend clusters that are divided by fast and slow queries. You want to keep slow queries separated from the fast queries that are critical for checkout. … on visibility of what is going on in the cluster We built tools to help us know where the issues are. In the past we had issues with visibility of what queries are actually executing and what is actually causing the issues, and now we have tooling around it … on example of problems we have with Elastic Search Elastic Search has a finite capacity … its resources directly relate to how many queries can be running at the same time… so if you have slow running queries, you can get request queuing in ES and as a result, other queries that are usually pretty fast, like fetch by ID all of a sudden start timing out and you start getting 500s and no one knows what is going on. So without knowing what the queries are that are holding up threads, it is very difficult to find the real slow queries because on aggregate they are not executed that often. on tooling for ES …most engineers don’t know ES JSON formatting for queries, and it is hard to go into Kibana and create a dashboard when you don’t know the underlying technology. How many know about Timelion? The search team knows, but probably no one outside the team. So having SQL as the underlying language for tooling and understanding what is going on in a cluster was a requirement. Also we needed 100 percent coverage of what is executed on a cluster and accountability. … with Eventer, Kinesis and Druid, we also have a Blazer SQL interface, we track every single query with the code owner and execution location. Every time we have a slow query we know where they are [in code]… … on how tooling helped with our search infrastructure Two months ago we were at about 3000 searches a second per cluster, which means about 5000–6000 queries per second across both the slow and fast clusters. Now we are down to 250 queries a second. We dramatically decreased the number of queries just by having visibility into what is being executed on the cluster. So thetakeaway here is to never have a data storage technology that does not provide visibility [into core metrics]. … on snowflake One of the things that is super nice is writes and reads are completely separated. And they also have a concept of data warehouses, which has read quotas that we can provision by team. And that’s what didn’t work with redshift with our multi-tenant design. You just provision warehouses, and you can provision a DWH that has a subset of your data. If you start request queueing you can automatically provision read nodes, and queries scale linearly and node provisioning helps a lot. … one of the examples is that some queries that data scientists were doing used to take 4 hours on Redshift and right now are executed in under 4 minutes on Snowflake … on tools Jon uses every day: I am a zsh guy … I am a firm believer that if you did something 3 times you have to automate it … I have a little bash script that detects local branch and repo, it creates a pull request for you and opens it up in the browser … I use autojump to jump between directories … I use vim with a nerdtree in some workflows … I didn’t like Atom … new vscode is an excellent IDE if it’s your thing Links for the episode: kibana timelion zsh autojump vim / nerdtree sublime text vscode swipe keyboard for iOS snowflake redshift asana Big thanks to Jon Phillips, Jon Hsieh, Muffy, Dominic and Bill for helping to make this happen. If you have any feedback about the format or the content of the podcast, please send it to Viktor at instacart dot com. Also stay tuned for our next episode starring Gordon, where we share excitement about infrastructure, catalog, tooling, distributed systems and Gordon’s personal projects. Til next time! Instacart Engineering 76 Podcast Instacart Interview Culture Software Development 76 claps 76 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-25"},
{"website": "InstaCart", "title": "know your carrots with gordon", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-with-gordon-e4698547cb5a", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher or Tunein Hi, this week I am still on vacation, so it is going to be pretty short. Gordon is a Bay Area native who has had quite a journey on his way to Instacart. He is huge fan of distributed systems and Snowflake in particular. As a member of the catalog team, he has to solve the problem of processing an insane amount of data quickly, with very few or no errors, while at the same time keeping our other systems healthy. Gordon is the author of Cubist, a framework that allows you to do feature development. He has promised (after the podcasts) to open source this soon. Cubist includes a test runner to run specs for particular feature and allows you to discover aspects or angles around this feature using version control. As always, big thanks to our amazing Forge team for all the help with the podcast. Some links from this episode Distributed Systems: Principles and Paradigms Designing Data-Intensive Applications Snowflake Airflow Vim , Nerdtree and fzf Clean Architecture by Uncle Bob Martin Till next time and our screencast with Alex! Instacart Engineering 24 Instacart Culture Software Development Podcast Technology 24 claps 24 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-03"},
{"website": "InstaCart", "title": "instacart coding bootcamp experience accountant to engineer", "author": ["Logan Murdock"], "link": "https://tech.instacart.com/instacart-coding-bootcamp-experience-accountant-to-engineer-f7938b640114", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Thanks to Instacart’s first ever coding bootcamp, I was able to learn everything from SQL to React, which enabled me to make the jump from Revenue Accounting Manager to Full-Stack Engineer. I joined Instacart in November 2015 as a Senior Revenue Accountant. This role involved billing, technical accounting, and other accounting duties, but no software engineering. I was however exposed to the innovative, data driven culture of Instacart (by the way, we’re hiring !). This environment allowed my interest in programing, which I have had for a number of years, to flourish to new levels. Because of this interest, I started learning how to build websites on my own and even took advantage of a mentorship program Instacart offered, which paired me with an engineer and resulted in my first Rails app. I found coding to be exciting and fulfilling, but I didn’t know how I could possibly move into the field I enjoyed so much. Quitting my job and paying thousands to attend a coding bootcamp wasn’t a viable option for me. Luckily, Instacart came up with an brilliant idea: create your own candidate funnel by providing an internal bootcamp for non-engineers ( Carrot-U ). I jumped at the opportunity and have zero regrets. From the perspective of a non-engineer, an app like Instacart is pretty much magic. Sure, I had a idea of how a website is built and even some working knowledge of Ruby on Rails, but I was far from being able to understand how single-page app works, much less have a grasp of the infrastructure. Fortunately the Carrot-U mentors assumed we knew nothing, so they taught us everything, starting from the beginning :D The first week or so was an install-fest and overview of the tools needed to get going. This is when I learned the value of a great editor with the right plugins and shortcuts. By watching my mentors, I found that a programmer that has mastered his/her text editor can be twice as effective as a beginner. I started using Sublime Text and actively traded tips with other students and mentors. I found syntax highlighting and linting especially helpful when starting out. Even with the knowledge gained many months, I still learn new tricks constantly. Once we had tools in place, the mentors started teaching us about data types, SQL and database design. The goal was to provide students with knowledge of what makes up a database, how it is designed, created, updated, viewed, etc… before teaching students to write code. We briefly covered theory before getting into application. Each topic was supplemented with hours of content from Treehouse that filled in details at our own pace. A hybrid of Computer Science theory and bootcamp style practicality kept the content interesting and relevant. It wasn’t long before we were working on our first small Ruby assignments. I found the beginning of the program to be helpful practice, but not overly challenging. I think this was due to my background and having some prior experience coding; however, it wasn’t long before I became fully challenged. The mentors provided office hours for questions twice a day, 5 days a week, which I definitely found necessary with topics like React state and Redux. Even with the available help it was still extremely challenging to learn so much in a short period of time. The program required a lot of self motivation and dedication from each student as concepts naturally build upon each other. We couldn’t slow down or take a break without falling behind. This was especially important when we got to the point of putting it all together by connecting the backend knowledge (rails, databases, etc…) with the frontend (css, react, etc…) to build our final project. One of the requirements of the program was that the code school was outside of normal work responsibilities. All students needed to still keep up with their normal job responsibilities while learning to code in their spare time. This of course was super challenging to keep up for four months but certainly not impossible for a motivated student with a clear vision of what they hoped to gain from the program. My normal day went something like this: 5:00am: Get up, eat breakfast, go to the gym 7:45am–12pm: Get to work, start meetings, emails, etc… 12pm-1pm: Programming during lunch 1pm-5pm: Regular work (lots of meetings) 5pm-5:30pm: Attend office hours and get questions answered from the prior day 5:30pm-6:30pm: Catch up on emails 6:30pm: Go home and have dinner 7:30pm-9:30pm: Programming and assigned Treehouse content Rinse and repeat. I put myself on lockdown during the program, so I was also able to pack in another 10–15 hours on the weekends. A typical week was about 20–30 hours of learning / programming. Getting through the week was very difficult, but Mondays were a surprising breath of fresh air, as this is when we had our weekly lecture. Each week one of the mentors would cover a technical topic that ranged from Big-O notation to a React crash course. This provided a casual setting to learn about computer science topics from someone experienced in the industry. The open question format provided a greater insight than a typical lecture. I couldn’t hope for more than this access to knowledgable, experienced professionals…even from the most reputable paid bootcamps. For me, the biggest value of the Carrot-U program came from the access to the best engineers in the industry. Its amazing how much we covered and learned in such a short period of time. The program culminated in a final class project that allowed us to make use of everything we learned in a very pragmatic setting. The project involved making a React single-page application with a Rails backend. We designed the app to have a separation between the front and backend in order to practice building and calling an API. One benefit of this design choice is that each student could take on tasks that were separate but relied on someone else’s work. This way we gained practice in working as a team and communicating needs to get unblocked. We created PRs and requested reviews similar to working in a live codebase. One of the most challenging parts, and best experience gained, was working with Git in a project setting. Getting our project to the finish line to be presented ended up being an exercise in scoping and recognizing which tasks should be prioritized. This was a very valuable experience as it simulated what often occurs in real world development. We presented the result to about half the engineering staff, including the VP of engineering, during a Hacker’s lunch. I would have liked to have done more, but I was proud of what we accomplished and thankfully it was well received 😅 Everything moved quickly after the final project and completion of bootcamp. Those of us that completed the program were given an opportunity to apply for an internship. This required going through the interview process and completing the coding challenge. Carrot-U prepared us well for taking on the interview, but it was still extremely challenging. I was both thrilled and relieved to receive an internship offer. I plan to write more about my internship experience in the future (spoiler alert: it was great!). Two years after starting in Finance, I packed up my succulent and moved into Engineering. I have all the wonderful mentors and program sponsors to thank for making it possible. I think its truly unique and wonderful that a start-up is willing to invest so much in its employees and give them opportunities to change and grow. I hope that there will be many more Carrot-U classes and I plan to help and participate so that others can have the opportunity I did. Instacart Engineering 116 Thanks to Muffy Barkocy and Dave Schwantes . Coding Bootcamp Learning To Code 116 claps 116 Written by Former Revenue Accountant and CPA turned Full-Stack Engineer Instacart. Also a huge Japanophile. Instacart Engineering Written by Former Revenue Accountant and CPA turned Full-Stack Engineer Instacart. Also a huge Japanophile. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-01"},
{"website": "InstaCart", "title": "know your carrots 1 muffy", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-1-muffy-d755d64cde9f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher or Tunein H ello, this is Viktor at Instacart, and today I am talking to Muffy in our pilot episode of Know Your Carrots. Our talk is about how it all started for her, outlooks on job challenges, what it’s like to work with extremely smart people, how to mentor and be mentored, how being lazy helps with her career, and her thoughts on women in tech. Also stay tuned for the best advice she’s got. Let’s get started. A few quotes from our guest: What technologies right now are you most excited about? …I am not afraid of our robot overlords, I am looking forward to it because I think there’s so much random crap that people do, and we’re not good at it … let people do the things that people are good at … I’d really like to see, and this is not a technology change, that is a society change, when things that human beings are good at are more valued than they are right now … some obvious things, like teaching … looking after people What are you working on now and what did you work on before? I was unwilling to … step back and make other people take responsibility. That’s one of the things I’ve always been really bad at … I’m a very helpful person, but you can always take that too far … and then other people don’t learn … You’re not always doing people a favor by doing things for them. And that’s something I need to be reminded of all the time, even 30 years into it. What are some ways you constantly work on yourself and improve? What I really try to do is to find a job where … stuff just happens and I need to deal with it. Can you think of anyone who would be the person who really influenced you and your point of view on software development? There’s no one person. I’ve met a bunch of really really smart people over time. … I love working with someone who is smarter than me. … To be working with someone who is so intimidatingly smart … this is wonderful! Other things Muffy mentioned in the interview emacs Gödel, Escher, Bach LoTR The Diamond Age , by Neal Stephenson IntelliJ (IDE) reminders in slack stack overflow terraform AWS / aws cli heroku bash tmux Perforce (source code control) Automate the boring stuff with Python mystery series 1 + 2 Big thanks to Jonathan Hsieh and Muffy Barkocy herself for the help with getting this started! If you have any feedback about the format or content of the podcast, please send it to viktor at instacart dot com. Please stay tuned for the next installment of Know Your Carrots with Jonathan Phillips and a conversation about search at scale. Instacart Engineering 42 Thanks to Jon Hsieh . Podcast Instacart Interview Software Development Culture 42 claps 42 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-25"},
{"website": "InstaCart", "title": "implementing hyperloglog in redshift and tableau", "author": ["Oliver Gothe"], "link": "https://tech.instacart.com/implementing-hyperloglog-in-redshift-and-tableau-a62081c98e1c", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we use Tableau for key performance indicator (KPI) reporting, KPI trending and determining drops or spikes in our core metrics across key dimensions. In an effort to help the team be more efficient and data-driven in their day-to-day responsibilities [tasks], we also use Tableau to democratize our data by providing curated extracts for advanced data exploration. In the trade-off between flexibility and performance, we have chosen to create data extracts that are very granular instead of building aggregated data sources. Aggregated data sources are far more performant but limit the diversity of possible analyses. Because our data volume is scaling much faster than we anticipated, this trade-off has recently started to degrade the load times of many of our dashboards (from a few seconds to around 20–30 seconds every time a filter is applied). We learned one of the root causes for this slow-down was metrics that included distinct counts. Since these metrics are required for unaggregated data sources, we started exploring alternatives to improve our Tableau performance. Instacart’s license of Tableau does not currently provide any such functionality. We implemented HyperLogLog, a probabilistic counting algorithm, to create data extracts in Tableau. This post describes how to create a data source from a Redshift cluster (other data warehouses will work as well) and the associated measure definitions to implement HyperLogLog in Tableau with a quick toy problem that compares its performance to a measure including a distinct count. HyperLogLog is an algorithm that estimates the number of distinct elements in a data set with an accuracy of approximately 1.6% (using 4096 registers). The algorithm has been described in detail in many places and is fairly straightforward, so I will only discuss it at the very highest level (a more detailed explanation of the algorithm can be found here and a corresponding simulation here ). In HyperLogLog, elements of the the set are first hashed to ensure randomness, then converted into binary and added into a register. A register stores the most unlikely event observed in the set by observing the longest consecutive count of leading zeros. Based on the register’s maximum value, an approximate count of distinct elements can be calculated. The table below demonstrates the likelihood of observing a given number of leading zeros in a register, given the binary representation (limited to 8 bits for simplicity) of an element in the total set: For example, if the longest run of zeros was 4, we would estimate 32 elements. Since this estimation can be heavily influenced by just randomly getting a long run of zeros, the total set of elements is split into many registers and averaged in order to estimate the total number of distinct elements. If our data were split into two registers, and the first had a maximum of 4 leading zeros while the second had only 3 leading zeros, we would estimate a cardinality of (32+16)/2 = 24. A final adjustment to this estimate is necessary to account for the possibility of hash collisions. This adjustment factor is defined by Flajolet et al. in their original HyperLogLog papers [ 10.1.1.76.4286 ] as: where m is the total number of registers. Additionally, in order to minimize the impact of outliers, HyperLogLog uses the harmonic mean instead of a regular average, which gives us the following formula: Where M[j] is the position of the first non-zero value in the register m. The challenge of preparing a data extract for Tableau now lies in recreating some of these non-standard operations in Tableau, and then storing the resultant registers in Tableau where they can be combined to determine distinct counts across many dimensions. The code in its entirety can be found here , but I will build it up step-by-step, referencing the previous step with the shorthand [RESULTS from PREVIOUS QUERY] to minimize repeated text. First, we select the element of which we want to get a distinct count and its associated dimensions. For example, to calculate the distinct number of products sold by Instacart in a given time period: Next, we have to hash the product_id column. Redshift provides several hash functions. In this example, I will use the SHA-1 implementation defined by the FUNC_SHA1 function. The resulting hexadecimal number provides significantly more precision than we need (and are able to store in numeric Redshift data types), so I will use only the 14 rightmost characters as the hash and the 3 leftmost characters as the register number: Since the elements are hashed, they are pseudo-randomly assigned to the register represented by the first three characters. The same value will always be added to the same register. Both the register and the hash are converted from hexadecimal to base 10 using the STRTOL function: Next, the hash has to be converted into binary, but Redshift does not have a standard function to accomplish this. In the absence of writing a UDF, this operation can be accomplished by bit-shifting (a UDF would be much preferable for readability and efficiency but requires appropriate permissions): To determine the first non-zero bit of the element, a regex substring expression can be used: Finally, we have to group by the dimension and register to find the longest run for a given grouping: The final query produces our necessary registers, which can now be combined in Tableau in order to calculate distinct counts. To build our visualizations, we take the query we wrote above and create a data source from it in Tableau. In Tableau, we will have to perform two calculations in order to calculate a distinct count of elements. First, we have to find the maximum number of non_zero elements per register in a given grouping. This can be accomplished with the level of detail calculation shown below: Given this calculated measure, we can now apply the formula to determine the cardinality estimate: Let’s see how the HyperLogLog estimate compares to the equivalent distinct count operation: While there are some minor differences, we can see that the true count is reflected extremely well and the HyperLogLog version performs significantly better. So just how much did HyperLogLog help in this example? Newer versions of Tableau allow you to measure exactly how much time is needed for each operation in rendering a dashboard. Using this feature, we can see in this particular example that the HyperLogLog algorithm performed ~50 times faster in determining a distinct count. More importantly, the computation cost of the HyperLogLog algorithm now scales with the number of dimensions of the dataset instead of scaling linearly with data volume. Consequently, it will not break down as we continue to surface more and more data on our Tableau server. In many cases, Tableau is used as a quick analysis and trending tool. In these cases, the 1.6% error from implementing large data sets as aggregates is perfectly acceptable, and utilizing the HyperLogLog algorithms can significantly improve performance compared to distinct counts on unaggregated data sets. Ultimately, this technique provides the team another tool to quickly access our metrics, determine important trends and become even more data-driven. Instacart Engineering 218 1 Thanks to Jon Hsieh and Muffy Barkocy . Tableau Algorithms Data Science Visualization Analytics 218 claps 218 1 Written by Data Science at Instacart Instacart Engineering Written by Data Science at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-08"},
{"website": "InstaCart", "title": "scalability tip move business logic out of db", "author": ["Peter Lin"], "link": "https://tech.instacart.com/scalability-tip-move-business-logic-out-of-db-c7740661b0ef", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instacart has been growing tremendously. Growth means more traffic to our apps, putting a considerable load on our API, and of course, our database. In our effort to scale, we have found that moving business logic out of the database and, instead, running the logic through code worked well. Here’s a simple example demonstrating this tip: Instacart offers an annual delivery subscription called Express . Customers can save money by buying an Express subscription, offered in multiple pricing tiers, including regular, business, and trial. Naturally, we have a Subscription model, which looks something like this: Since there are so many subscription tiers, over time we ended up with lots of helper methods defined on the User class, like so: As it turns out, it is not uncommon for us to call many of these methods in one request, and unfortunately, it meant that we were doing multiple DB queries on the subscriptions table per request. Since the query itself is different for each of these methods, query caching isn’t able to help us out. Doing multiple queries slows down the request and increases load on the DB, so this is bad for us and bad for our customers. Luckily, one thing we do know is that customers are very unlikely to have multiple active subscriptions at a time. We can take advantage of this fact by loading all current subscriptions and filter run the filtering logic in Ruby, which looks something like this: With this quick change, we have reduced n queries down to one. Even as we add more tiers in the future, we won’t have to do more than one query. As a bonus, all current subscriptions are loaded should you need to access them. I hope you have found this post interesting. For more posts like this, check out Instacart’s tech blog . We are also scaling our engineering team: if you are interested in building great APIs, creating magical consumer app experiences, scale infra, or anything engineering, we’re hiring :) My thanks to Kaushik Gopal, Michael Scheibe, Dan Loman, and Zain Ali for reading a draft of this post. Instacart Engineering 36 Thanks to Dan Loman and Instacart . Ruby on Rails Scaling 36 claps 36 Written by Engineering @ Instacart Instacart Engineering Written by Engineering @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-08"},
{"website": "InstaCart", "title": "rxjava state the basics", "author": ["Laimonas Turauskas"], "link": "https://tech.instacart.com/rxjava-state-the-basics-f842eaee7ee1", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we have increasingly been using a reactive functional approach to manage state. The goal is to have a deterministic Android application that is easy to understand, test and maintain. Let’s say we have a form to add a new comment. Basic requirements are: 1. There is a single text field that the user can modify. 2. There is a submit button that the user can press to add a comment. 3. We only enable the submit button when comment entered is longer than 5 characters. 4. Disable the submit button while we are uploading the comment How would we approach this problem in a reactive functional way? First, let’s define a data class that defines our view state: Now, let’s define how our view will look: We laid out the basic contract for the view. In this case, we made the view as stupid as possible. It doesn’t contain any logic besides taking an immutable state object and updating itself. When user types a new letter, we trigger onTextEntered callback. We then create a new CommentFormViewState object and pass it back to setViewState for the view to re-render. CommentFormViewState is made up of data and callbacks. It is the final product that the view sees. The question is how we update the state after the callback is triggered? To do this, let’s first create a new class that represents only the data and a couple of classes to represent the user events. There are two actions that the user can take: Now, that we have defined data and user actions, we can model the relationship between them. Here is a basic signature of the function that takes user event streams and returns user data stream. Before we implement this function, we need to introduce a concept, common in functional state management, called reducer. A reducer is a function that takes an event, the current state and returns a new state object. Here is a signature of a reducer function: Reducers are supposed to be pure in a functional sense, which means that two identical inputs will provide identical output. Here is our reducer for text changed event: Now, let’s plug this into RxJava We still need to incorporate submit comment actions. When user clicks submit, we want to trigger a network request and keep the data updated. To be able to do that, let’s define an interface for the api. Now that we have our service, let’s setup comment submission. As we are working in a statically typed system, to use RxJava scan operator, all events must have a matching type or a super type. As you can see, we have an issue where the events stream data type becomes Any. To be able to reduce, we would need check if the object is of a particular type. In simple cases, it’s a bit ugly. But when using with types that have generic type parameters, it is hard to maintain type safety. Due to type erasure, you cannot safely check that the generic parameter is of a specific type. Is there another way? Of course! Let’s take a step back and look at our reduce function What if we apply function currying to this? So, what happened here? We split the function! Previously, we had a function that took two parameters and returned a state object. Now, it takes only the event and returns a function that takes the state and returns the state. Functional magic! So, how can we use this? We created a common type (CommentFormData) -> CommentFormData that we can now use. To make this even better, let’s declare a kotlin typealias. And now, let’s replace the code We have managed to define the logic behind comment form data! Now that this part is done, let’s take the data and create a view state using it. To finish this reactive component, let’s delegate user actions to the CommentFormModel and use the data provided to create the view state. We will use RxRelay’s to achieve this: The final step is connecting the view model to the view. You might ask, why would we model our application this way? If you are not familiar with functional patterns and RxJava, this can be intimidating. However, once you learn these concepts and start applying them, the benefits are clear and it’s hard to go back. Testability Each class is responsible only for one thing, code is mostly side-effect free and most of the logic lives outside of the Android framework. This enables us to easily test majority of our code using simple jUnit tests. Predictability The contract between classes is clearly defined. Each class has only one public method that defines it’s API. For the inputs provided, this method will give you the same deterministic output. Maintainability When we combine predictability and testability, we get code that is highly maintainable. Shameless Plug If you are interesting in building functional android applications, Instacart is hiring. Email me at laimonas.turauskas@instacart.com For help with the article, special thanks to: Maksim Golivkin Colin White Instacart Engineering 710 5 Thanks to Maksim Golivkin , Kaushik Gopal , and Colin White . Android Rxjava Reactive Programming Functional Programming Redux 710 claps 710 5 Written by Android, techno, politics in no particular order Instacart Engineering Written by Android, techno, politics in no particular order Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-13"},
{"website": "InstaCart", "title": "carrot u building an in house coding bootcamp", "author": ["Dave Schwantes"], "link": "https://tech.instacart.com/carrot-u-building-an-in-house-coding-bootcamp-d2c0a746dd73", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instsacart built its own bootcamp to help current non-technical employees become engineers. Upon completion of the program two graduates went through internships and are now full-time engineers on our team. Like many fast-growing tech companies, Instacart is always looking for talented engineers. One way to bring on new talent is by recruiting and hiring existing engineers (by the way, we’re hiring !), but we thought we might be able to grow some of our own. After seeing strong interest from around the company when Instacart engineers ran internal sessions about SQL and basic web development, a few of us started to talk about using our love of teaching and mentoring to offer something like a full-fledged coding bootcamp. This program would take someone with little coding background and prepare them for an entry level software engineering role at a company like Instacart. We started designing a program motivated by the following principles: Supporting professional development for all employees is important. If we have great employees at Instacart who are strongly motivated to move from a non-technical role into an engineering position, they’re going to do it with or without our help. Attending a traditional coding bootcamp would likely involve leaving their job (meaning we lose a great employee) and paying A LOT of money (difficult when they just left their job). Offering a program like this is a win for the company and the students. Participants gain new skills while keeping their existing jobs, and Instacart can retain ambitious and valued employees. A junior engineer who has familiarity with and passion for the company and product can have impact quickly Being a new engineer can be overwhelming at times, particularly in a fast moving company. Hiring junior engineers who are not familiar with the company means they have two learning curves. We believe that tapping into a market of potential engineers who already know a lot about Instacart makes it possible for those people to have an easier transition and to be able to make meaningful contributions as quickly as more experienced engineers. Many of our engineers find teaching and mentoring to be very rewarding Instacart is full of talented engineers, all of whom enjoy sharing their knowledge and skills with others. Staffing our bootcamp with volunteer engineers with a passion for teaching allows them to exercise and grow in this area. We knew we couldn’t build an entire series of courses with all the necessary training materials using only a few part-time volunteers. So, we investigated using existing online learning resources combined with customized content produced in-house, to align our lessons with the needs and challenges of Instacart. Engineers with both CS degrees and bootcamp backgrounds contributed to the curriculum plan to give us a broad perspective on what should be the essential skill set for a strong engineer. The topics we ended up with were: Setting Up Using an IDE Using Github Installing packages Our Stack SQL and database design Fundamental coding concepts, taught in Ruby Ruby on Rails HTML/CSS/(vanilla) Javascript React Computer Science Topics Imperative, functional, and object oriented languages Data structures and algorithms Design patterns Regular expressions Getting to Production Logging and debugging Scripting Deploying code on AWS Teamwork Class project Code reviews Google Stack Overflow For many of the topics, particularly the basics of various languages, we made use of video lectures and exercises from Treehouse . We supplemented this with our own lectures and homework, as well as having a Slack channel and regular mentor office hours to make sure no one got stuck. With a lesson plan in place it was time to get some students. We invited any non-engineer in the company (including remote employees) to apply. The application focused more on the potential student’s motivations than their current technical prowess. The only technical question on the application was: Write a method in Ruby that determines if a number is even. This was primarily to make sure the applicant was able to do some basic Googling if presented with something they didn’t initially understand. We didn’t expect our applicants to have previous knowledge of Ruby or even programming, we just wanted a question that was specific enough to have a clear path to a solution for anyone willing to spend a bit of time on it. After reviewing the applications we then conducted in-person interviews with each applicant and narrowed the group down to a class of 10. The class size was kept small so that students could get a lot of personalized help when they needed it. So what did day to day life at Carrot U actually look like? The program ran for 4 months from kick off to graduation, with the expectation that students would commit about 20 hours a week to the work, above their existing responsibilities at the company. Every week students would work through Treehouse videos and exercises, in addition to building custom projects specific to the Carrot U curriculum. Each Monday there would be a live lecture by one of the engineer mentors or a guest speaker from another part of the company such as design or product. This was also a way to get many of the students and mentors together in one place at least once a week. Every day we had two office hour sessions staffed by one of the 5 mentors. These were, of course, open to both our remote and local students. Near the end of the program, much of the work revolved around building a capstone project. This was a single project worked on by all the students in the program. The capstone project allowed students to get experience working on a shared code base and doing project planning with other engineers. We’ll have future blog posts from students and mentors that go into more details and personal experiences from the program. In the end, three students completed the program. Given that we were asking busy employees at a fast moving startup to give an additional 20 hours of their week we were very happy with these numbers. Of those three, two students were actually interested in becoming software engineers and chose to interview for engineering internship positions with Instacart. This interview process was very similar to what we do with any engineering interviewee, including a coding challenge, a pair programming challenge, and a meeting with our VP of Engineering. Both students performed well in their interviews and after working out the appropriate transition logistics with their old teams, started 12-week internships on our Shopper Management and Customer Core Experience engineering teams. Last month those internships wrapped up and both engineers transitioned into full-time roles on their teams. These engineers went from having an interest in software engineering to becoming contributing members of a world class engineering team in under a year, all without having to quit their job or pay for additional education. Our engineering team gained two new engineers who were able to hit the ground running and bring new perspectives to their work. Everybody involved with the Carrot U program worked incredibly hard. It took commitment from the students, mentors, engineering leadership, and HR to achieve these successes. After seeing great results like this, we’re excited to take what we’ve learned here and start planning the next Carrot U session. Instacart Engineering 352 4 Programming Education Bootcamp Internships Hiring 352 claps 352 4 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-27"},
{"website": "InstaCart", "title": "smarter todos with kotlin", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/smarter-todos-with-kotlin-beb522fe9a01", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Kotlin already has TODO s . That’s awesome! but it’s a tad bit aggressive. Take this piece of code for instance: If your app happens to call awesome feature, Kotlin will blow it up! I wanted something similar, but a tad bit gentler: A reminder “in code” that I needed to get something done — the code is my documentation, to-do list etc. I don’t need to sign in or go to another tool to check my code. I’m always in it. Never EVER blow up for my user, or in production — I can’t/won’t allow my users to suffer for my tardiness. Blow up aggressively for me and my fellow developers — they’re tough and can take it (rather they’ll make sure to be tough on me). Provide a “due date” for said blow up — Todo’s have due dates, I don’t always have to do it right now*. I mean the analytics can wait**. Kotlin — the nifty devil that it is — allowed me to make some minor tweaks and get all of the above: There you have it! won’t blow up in production will blow up for developers but only if due date has passed To make it even more nifty, there’s a handy date builder method that allows us to specify dates like a human would: Thoughts? Love it, hate it, am I crazy? The super smart Jesse Wilson convinced me that time-bombs (even in debug variants of the app) can be a tad bit too aggressive. You’re hosing/stopping the work of other developers with this technique – for potentially no fault of theirs. Ralf Wondratschek also points out in the comments that if you happen to ever checkout an older revision of the project — which has this ToDo checked in — you’ve basically halted execution of the app. Jesse suggested that if I really wanted something to this effect, an approach he had seen in the past was to increasingly “sleep” the execution path. This way — if you don’t keep your promises — the app degrades, but doesn’t necessarily halt or stop others. It should successfully make your colleagues very mad at you, without necessarily stopping them from going forward. I love this technique just for how cheekily we could go about implementing it : sleep for 10ms, for each day since the code path was deprecated sleep to max of maybe 5s, so we don’t get hit by an ANR 😎 This should address both concerns. * if you can’t already tell, I’m a wonderful procrastinator ** Just kidding David, we always get the analytics done right away. Instacart Engineering 605 5 Kotlin AndroidDev Todo Android Date 605 claps 605 5 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-14"},
{"website": "InstaCart", "title": "how to build a deep learning model in 15 minutes", "author": ["Montana Low"], "link": "https://tech.instacart.com/how-to-build-a-deep-learning-model-in-15-minutes-a3684c6f71e", "abstract": "Eng Open Source Data Science Machine Learning Android iOS An open source framework for configuring, building, deploying and maintaining deep learning models in Python. As Instacart has grown, we’ve learned a few things the hard way. We’re open sourcing Lore , a framework to make machine learning approachable for Engineers and maintainable for Machine Learning Researchers. A common feeling in Machine Learning: Performance bottlenecks are easy to hit when you’re writing bespoke code at high levels like Python or SQL. Code Complexity grows because valuable models are the result of many iterative changes, making individual insights harder to maintain and communicate as the code evolves in an unstructured way. Repeatability suffers as data and library dependencies are constantly in flux. Information overload makes it easy to miss newly available low hanging fruit when trying to keep up with the latest papers, packages, features, bugs… it’s much worse for people just entering the field. To address these issues we’re standardizing our machine learning in Lore . At Instacart, three of our teams are using Lore for all new machine learning development, and we are currently running a dozen Lore models in production. If you want a super quick demo that serves predictions with no context, you can clone my_app from github. Skip to the Outline if you want the full tour. The best way to understand the advantages is to launch your own deep learning project into production in 15 minutes. If you like to see feature specs before you alt-tab to your terminal and start writing code , here’s a brief overview: Models support hyper parameter search over estimators with a data pipeline. They will efficiently utilize multiple GPUs (if available) with a couple different strategies, and can be saved and distributed for horizontal scalability. Estimators from multiple packages are supported: Keras, XGBoost and SciKit Learn. They can all be subclassed with build , fit or predict overridden to completely customize your algorithm and architecture, while still benefiting from everything else. Pipelines avoid information leaks between train and test sets, and one pipeline allows experimentation with many different estimators. A disk based pipeline is available if you exceed your machines available RAM. Transformers standardize advanced feature engineering. For example, convert an American first name to its statistical age or gender using US Census data. Extract the geographic area code from a free form phone number string. Common date, time and string operations are supported efficiently through pandas. Encoders offer robust input to your estimators, and avoid common problems with missing and long tail values. They are well tested to save you from garbage in/garbage out. IO connections are configured and pooled in a standard way across the app for popular (no)sql databases, with transaction management and read write optimizations for bulk data, rather than typical ORM single row operations. Connections share a configurable query cache, in addition to encrypted S3 buckets for distributing models and datasets. Dependency Management for each individual app in development, that can be 100% replicated to production. No manual activation, or magic env vars, or hidden files that break python for everything else. No knowledge required of venv, pyenv, pyvenv, virtualenv, virtualenvwrapper, pipenv, conda. Ain’t nobody got time for that. Tests for your models can be run in your Continuous Integration environment, allowing Continuous Deployment for code and training updates, without increased work for your infrastructure team. Workflow Support whether you prefer the command line, a python console, jupyter notebook, or IDE. Every environment gets readable logging and timing statements configured for both production and development. Basic python knowledge is all that is required to get started. You can spend the rest of the year exploring the intricacies of machine learning, if your machine refuses to learn. Create a new app (3 min) Design a model (1 min) Generate a scaffold (2 min) Implement a pipeline (5 min) Test the code (1 min) Train the model (1 min) Deploy to production (2 min) * These times are for promotional blogcast purposes only. No sane machine learning researcher spends 1 minute designing a model… none the less, once you grok it, and everything is cached, you too can wow your friends and colleagues by effortlessly building a custom AI from scratch in well under 15 minutes. Lore manages each project’s dependencies independently, to avoid conflicts with your system python or other projects. Install Lore as a standard pip package: It’s difficult to repeat someone else’s work when you can’t reproduce their environment. Lore preserves your system python the way your OS likes it to prevent cryptic dependency errors and project conflicts. Each Lore app gets its own directory, with its own python installation and only the dependencies it needs locked to specified versions in runtime.txt and requirements.txt . This makes sharing Lore apps efficient, and brings us one step closer to trivial repeatability for machine learning projects. With Lore installed, you can create a new app for deep learning projects while you read on. Lore is modular and slim by default, so we’ll need to specify --keras to install deep learning dependencies for this project. For the demo we’re going to build a model to predict how popular a product will be on Instacart’s website based solely on its name and the department we put it in. Manufacturers around the world test product names with various focus groups while retailers optimize their placement in stores to maximize appeal. Our simple AI will provide the same service so retailers and manufacturers can better understand merchandising in our new marketplace. “What’s in a name? That which we call a banana. By any other name would smell as sweet.” One of the hardest parts of machine learning is acquiring good data. Fortunately, Instacart has published 3 million anonymized grocery orders , which we’ll repurpose for this task. We can then formulate our question into a supervised learning regression model that predicts annual sales based on 2 features: product name and department. Note that the model we will build is just for illustration purposes — in fact, it kind of sucks. We leave building a good model as an exercise to the curious reader. Every lore Model consists of a Pipeline to load and encode the data, and an Estimator that implements a particular machine learning algorithm. The interesting part of a model is in the implementation details of the generated classes. Pipelines start with raw data on the left side, and encode it into the desired form on the right. The estimator is then trained with the encoded data, early stopping on the validation set, and evaluated on the test set. Everything can be serialized to the model store, and loaded again for deployment with a one liner. It’s rare to be handed raw data that is well suited for a machine learning algorithm. Usually we load it from a database or download a CSV, encode it suitably for the algorithm, and split it into training and test sets. The base classes in lore.pipelines encapsulate this logic in a standard workflow. lore.pipelines.holdout.Base will split our data into training, validation and test sets, and encode those for our machine learning algorithm. Our subclass will be responsible for defining 3 methods: get_data , get_encoders , and get_output_encoder . The published data from Instacart is spread across multiple csv files, like database tables. Our pipeline’s get_data will download the raw Instacart data, and use pandas to join it into a DataFrame with the features (product_name, department) and response (sales) in total units. Like this: Here is the implementation of get_data : Next, we need to specify an Encoder for each column. A computer scientist might think of encoders as a form of type annotation for effective machine learning. Some products have ridiculously long names, so we’ll truncate those to the first 15 words. That’s it for the pipeline. Our beginning estimator will be a simple subclass of lore.estimators.keras.Regression which implements a classic deep learning architecture, with reasonable defaults. Finally, our model specifies the high level properties of our deep learning architecture, by delegating them back to the estimator, and pulls it’s data from the pipeline we built. A smoke test was created automatically for this model when you generated the scaffolding. The first run will take some time to download the 200MB data set for testing. A good practice would be to trim down the files cached in ./tests/data and check them into your repo to remove a network dependency and speed up test runs. Training a model will cache data in ./data and save artifacts in ./models Follow the logs in a second terminal to see how Lore is spending its time. Try adding more hidden layers to see if that helps with your model’s score . You can edit the model file, or pass any property directly via the command line call to fit, e.g. --hidden_layers=5 . It should take about 30 seconds with a cached data set. You can run jupyter notebooks in your lore env. Lore will install a custom jupyter kernel that will reference your app’s virtual env for both lore notebook and lore console . Browse to notebooks/product_popularity/features.ipynb and “run all” to see some visualizations for the last fitting of your model. You can see how well the model’s predictions (blue) track the test set (gold) when aggregated for a particular feature. In this case there are 21 departments with fairly good overlap, except for “produce” where the model does not fully account for how much of an outlier it is. You can get also see the deep learning architecture that was generated by running the notebook in notebooks/product_popularity/architecture.ipynb 15 tokenized parts of the name run through an LSTM on the left side, and the department name is fed into an embedding on the right side, then both go through the hidden layers. Lore apps can be run locally as an HTTP API to models. By default, models will expose their “predict” method via an HTTP GET endpoint. My results indicate adding “Organic” to “Banana” will sell more than twice as much fruit in our “produce” department. “Green Banana” is predicted to sell worse than “Brown Banana”. If you really want to juice sales — wait for it — “Organic Yellow Banana”. Who knew? Lore apps are deployable via any infrastructure that supports Heroku buildpacks. Buildpacks install the specifications in runtime.txt and requirements.txt in a container for deploys. If you want horizontal scalability in the cloud, you can follow the getting started guide on heroku. You can see the results of each time you issued the lore fit command in ./models/my_app.models.product_popularity/Keras/ . This directory and ./data/ are in .gitignore by default because your code can always recreate them. A simple strategy for deployment, is to check in the model version you want to publish. Heroku makes it easy to publish an app. Checkout their getting started guide. devcenter.heroku.com Here’s the TLDR: Now you can replace http://localhost:5000/ with your heroku app name and access your predictions from anywhere! * Or, you can interact with my heroku app here . We consider the 0.5 release a strong foundation to build up with the community to 1.0. Patch releases will avoid breaking changes, but minor versions may change functionality depending on community needs. We’ll deprecate and issue warnings to maintain a clear upgrade path for existing apps. Here are some features we want to add before 1.0: Web UI with visualizations for model/estimator/feature analysis Integration for distributed computing support during model training and data processing, aka job queuing Tests for bad data or architecture, not just broken code More documentation, estimators, encoders and transformers Full Windows support What would make Lore more useful for your machine learning work? Thank you to Jeremy Stanley , Emmanuel Turlay and Shrikar Archak for contributing to the code. Instacart Engineering 2.2K 6 Thanks to Jeremy Stanley , Viktor Evdokimov , Maksim Golivkin , Emmanuel Turlay , and Trey Stout . Machine Learning Deep Learning Python TensorFlow Data Science 2.2K claps 2.2K 6 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-26"},
{"website": "InstaCart", "title": "how instacart delivers on time using quantile regression", "author": ["Mathieu Ripert"], "link": "https://tech.instacart.com/how-instacart-delivers-on-time-using-quantile-regression-2383e2e03edb", "abstract": "Eng Open Source Data Science Machine Learning Android iOS How quantile regression is used to manage variance and ensure on-time deliveries. At Instacart, we deliver a lot of groceries . Very soon, 80% of American households will be able to use Instacart. In a previous blog post Space, Time and Groceries by Jeremy Stanley , we talked about how complex our logistics problem can be. In particular, we need to solve a Dynamic Capacitated Vehicle Routing Problem with Time Windows (DCVRPTW) . The goal is to ensure orders are delivered on time and in the most efficient way. Our fulfillment algorithm decides in real time how to route our shoppers to grocery store locations to pick groceries and deliver them to customers’ door-steps in as little as one hour. In order to optimize shopper routes, we need to know how much time a shopper will take to deliver a given order. This is why we built a model to predict delivery times . The exact features that go into this predictive model and the way the model is built is not the purpose of this blog post. Rather we will deep dive into how this model is used by our fulfillment engine. Let’s suppose a customer placed an order for 1pm–2pm . It means we need to plan a trip such that the estimated time arrival (ETA) is between 1pm and 2pm . Before 1pm the order is early , after 2pm the order is late . But to make sure the order is delivered on time, it is not enough to have the ETA before the due time ( 2pm ). We want the shopper to actually deliver the order before the due time. In other words, to make sure the order is actually delivered on time, we need: Actual Arrival Time = ETA + Prediction Error < Due Time Obviously, we do not know the prediction error until the order is delivered, so we need a way to account for how big the predicted error can be. Even if our predictive model is unbiased, we are only correct on average, which means the distribution of errors can be wide. Below is the distribution of the prediction errors in different cities: So to account for this, let’s introduce the notion of a Buffer which will be used to cover the model prediction error. Therefore, when planning a trip to a shopper, we need to use a Buffer such that: ETA + Buffer < Due Time Now the question is: What value do we choose for this buffer? The buffer needs to be high enough to cover the risk of lateness in most of the cases. But if the buffer is too high, we might lose efficiency by reducing the size of the feasible space of the optimization problem, as fewer shoppers might be considered for a given order. As a first and simple solution, we looked at the percentage of late deliveries we had depending on how close to the due time we planned the delivery to be delivered. For example, in San Francisco, if we plan all deliveries to be delivered 10 minutes before the end of the window, we find that about 18% of deliveries are late. We chose a fixed buffer based on the maximum percentage of lateness we were willing to have. We looked the graph above and chose a buffer such that we get 10% of late deliveries in the worst case. Using this approach, we chose 13 minutes for Chicago North, 15 minutes for Manhattan, and 18 minutes for San Francisco and Miami. However, this approach was obviously suboptimal. There are some cases where the risk is higher and some cases where the risk is lower. Hence, a fixed buffer may be sometimes too conservative (higher risk of lateness), and sometimes too aggressive (loss of efficiency). A better approach would be to compute a prediction interval for the delivery time and use the upper bound of the interval. This is where quantile regression comes into play. First of all, let’s take a moment to explain what a quantile regression is. A typical regression aims to fit the mean of the distribution. We try to approximate the conditional mean of the response variable y given certain values of the predictor variables X . In this context the objective is to minimize is the sum of the squared errors. In quantile regression, we want to make a set of predictions Q so that, for a given quantile q , q % of the true values are less than Q . In this case, we try to minimize the following loss function: Now let’s suppose that we want to build a simple linear model to predict the delivery time, as a function of the distance. With quantile regression, we can obtain a prediction interval for the delivery time. Let’s choose q=0.1 for the lower bound and q=0.9 for the upper bound. We immediately see that the quantile regressions give us a prediction interval for the delivery time. We notice that the prediction interval increases as the delivery distance increases, which makes sense as it becomes harder to predict accurately for long distances (more variance, less data). Therefore, we see that this prediction interval is much better than adding a global constant to the mean predicted value (c.f. fixed buffer of the simple approach). In our case, we built a second model, a q=0.9 quantile regression for the delivery time. This model will give us an upper bound of the delivery time, that will be used to make sure the delivery will not be late 90% of the time. When dispatching a delivery to a shopper we need to have: Prediction from 90th quantile < Due time In reality, our fulfillment engine tries to generate trips containing up to 5 deliveries, in order to save shopper minutes and increase system efficiency. When planning this kind of trip, we need to make sure that all orders will be delivered on time and we need to manage the risk of lateness. This risk is cumulative. For example, if the shopper takes more time than expected for a given order, this will impact the remaining deliveries in the trip. To account for this cumulative risk, the buffer that we need to use for a given delivery needs to be a function of the buffers of the previous deliveries in the trip. Let’s first take the example of a double trip, i.e, a trip containing 2 deliveries D1 and D2 . Now, we need to compute the buffer B0->2 we need in order to make sure D2 will be delivered before its due time. Dt0->1 + Dt1->2 + B0->2 < Due Time of D2 To approximate this buffer, let’s assume that delivery times follow a normal distribution, and that they are independent. The sum of two independent normal random variables still follows a normal distribution: We know that for a normal distribution, prediction intervals can be easily computed as [mu — z * sigma, mu + z * sigma] ( z=1.67 for 90% prediction interval). So it means: And finally, the following formula can be used to generalize to a N deliveries trip: We performed an A/B test to measure the impact on the efficiency/lateness tradeoff when using quantile regression vs a fixed buffer. As seen from the graph above, we found out that with quantile regression we were able to plan deliveries closer to their due time without increasing late percentage. This effect allowed us to explore more trip combinations in our fulfillment engine and therefore increase efficiency (one of our most important metric) by 4%. In many prediction problems, you might need to think beyond the mean. Quantile regression is a very powerful tool for that, as it allows you to approximate any percentile of the distribution, and therefore provides you a more comprehensive analysis of the relationship between variables. At Instacart, quantile regression has been used to better understand and manage the risk of late deliveries. 🥕 Interested in joining us? Apply here . Want to learn more about how Instacart works? Read here . Quantile regression implementation in python for linear models Quantile regression implementation in python for random forests Quantile regression on Wikipedia Instacart Engineering 721 3 Thanks to Jeremy Stanley , Sherin Kurian , Houtao Deng , Ji Chen , Emmanuel Turlay , Jingjie Xiao , and Max Mullen . Machine Learning On Demand Logistics Data Science Statistics 721 claps 721 3 Written by Data Science at Instacart Instacart Engineering Written by Data Science at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-06"},
{"website": "InstaCart", "title": "dont let the crow guide your routes", "author": ["Jagannath Putrevu"], "link": "https://tech.instacart.com/dont-let-the-crow-guide-your-routes-f24c96daedba", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Authors: Reza Faturechi & Jagannath Putrevu At the core of Instacart’s grocery delivery service is our Logistics engine that matches our customers’ orders with our shoppers. In any given metro area, we can fulfill from hundreds of store locations, delivering within as fast as two hours for a majority of our orders. To do this effectively, our Logistics engine processes billions of computations every minute to find the most optimal routes to assign to our shoppers. More specifically, we solve a Vehicle Routing Problem , which we introduced before in our post Space, Time, and Groceries . Very often our algorithm attempts to group together orders of multiple customers who live close to each other to suggest efficient routes to shoppers and reduce the number of miles traveled on a trip. If our algorithm makes poor decisions with the grouping of orders, it could delay the timely delivery of our customers’ groceries. For the algorithm to make the best decisions, we need to pass inputs that are as accurate as possible. One of the most important inputs required is the distance between any two origin and destination pairs. Computing distance is not as trivial as we think. Obviously, it’s hard to predict how exactly someone would drive from point A to point B on a map but with enough data, we can get to a reasonable approximation of the distance traveled and time taken. But when we started out, we didn’t have the luxury of large amounts of data to infer the right path between two points. So we had to rely on other methods to compute the distance. The most commonly used formula for distance computation is called Haversine Distance , or more idiomatically, as the crow flies . This can be computed easily using a mathematical formula: We started off using Haversine distance as the key input formula for all our distance computations and designed most of our machine learning models and algorithms using it. But this way of computing distance can be pretty inaccurate. For example, when we compared Haversine distances to the actual distances our shoppers traversed in a region like Orange County, we observed a 33% mean absolute percentage error (MAPE). The other way of computing distance is through a third-party API like Mapbox , whom we partner with. We found the Mapbox estimated distances to be much more accurate than Haversine with a mean absolute percentage error (also in Orange County) of only 5%. While this can be pretty accurate, it can also be pretty expensive. Since we go through billions of computations before selecting the best set of routes, we would have to query these third-party services billions of times every day, which is not viable. But we can still rely on this data to a certain extent through caching our calls to these services. Every time we offer a trip to our shoppers, we call Mapbox to show the route in our Shopper App . When we make this call, we also cache the distances returned from Mapbox for each of the origin and destination pairs in the route. So the next time we come across the same pair of locations, we don’t have to re-query the service and just fetch the distance from our cache. However, this still doesn’t cover all the potential combinations we have to consider before choosing the best set. Every day, new customers sign up on Instacart for the first time, we keep adding new retailers to our platform, and our shoppers can be located anywhere in a city we deliver from, which makes this a very dynamic problem. So we decided to leverage our ever-increasing historical data of cached distances and actual distances traversed by our shoppers to build a Machine Learning model to predict distance. To build a model, we fetched all the historical shopper trips data and extracted the origin & destination location coordinates, the actual distance traveled by our shoppers, and the cached Mapbox distance if available. We also observed that the actual distance data can be noisy since it relies on anonymized GPS data from mobile phones. So we also had to use some heuristics to prune out some outliers from the actual distance data. For the bad outliers, we replaced that data with the cached Mapbox data wherever available. We used this dataset to build an XGBoost model which performed best, both in terms of error and robustness, compared to other models such as Regularized Linear Regression and Random Forest. We built the model with five features: latitude and longitude information of origin and destination points and their corresponding Haversine distance. The model was trained to predict the corrected actual distance as the target variable. Given the semi-static nature of distance, we set up an automatic job to retrain the model on a weekly basis. The average MAPE of predicted distance we observed in a region like Orange County was 11% which is much smaller than that of Haversine distance (33%). In the chart below, you can see the comparative distribution of predicted and haversine distance errors: We can also see that the Mapbox distance is still more accurate than the model predicted distance. So wherever we have cached Mapbox distances available, we continue to use those and supplement missing distances from cache with the ML model predicted distances. As we mentioned earlier, accurate distance computation is very important for us to plan good routes and keep our customers and shoppers happy. With a simple model described above, we were able to improve the accuracy of our distance computation and were able to avoid planning some bad routes we would’ve created otherwise using only Haversine distance. Using this model, we were able to reduce the distance traveled by our shoppers for multi-order trips by 9% across all the areas Instacart operates in, leading to millions of miles saved per year. Want to work on the next generation of Instacart Logistics? Our Marketplace team is hiring! Go to instacart.com/careers to see our current openings. Instacart Engineering 52 1 Machine Learning Logistics Marketplaces On Demand Data Science 52 claps 52 1 Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-07"},
{"website": "InstaCart", "title": "nailing the handoff", "author": ["Instacart"], "link": "https://tech.instacart.com/nailing-the-handoff-b4052003085c", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Exploring Certified Delivery’s checkout and delivery flows While Instacart’s bread and butter has always been and will continue to be grocery, many grocers and specialty retailers have a wide variety of items in their catalogs that extend beyond the pantry and produce aisles. Every day, customers can order same-day delivery of alcohol, prescriptions, home goods, makeup, and even electronics on Instacart. In fact, we’ve seen increased interest in electronics on the platform when compared to last year as people across North America continued to settle into remote work and distance learning routines. Today, we welcomed Best Buy to the Instacart marketplace, and as part of that launch, we’ve introduced Certified Delivery to our Customer and Shopper apps. The new feature makes it easier for customers to place same-day orders for high-value items like small electronics, earbuds, and smart home devices. With Certified Delivery, customers confirm that they will be home to accept and sign for high-value items right from their phone. In developing Certified Delivery, we had to be thoughtful about the user journey for both our customers and our shoppers. It’s a balancing act — how do you add complexity during ordering and checkout while reducing friction down the line? Let’s take a look at the flows. If a customer checks out with a set of noise-canceling headphones in their order that exceeds a pricing threshold, they’ll see a set of new prompts. We’ll alert them that the headphones require Certified Delivery, and ask them to pre-confirm that they will be home to sign for the order in person. They’ll also get signature instructions as they check out. Upon delivery, the customer will receive a prompt asking them to sign for their item. Similarly, in the shopper app, we’ve added a set of prompts to alert the shopper about the Certified Delivery requirement and offer up step-by-step instructions. The shopper will be alerted that the order requires certified delivery. They’ll see a prompt to ask the customer to sign for the delivery. An update will appear when the customer has completed the signature. If a customer is not able to sign for the item on their phone, the shopper will be prompted to scan the customer’s ID in lieu of a signature. By building these flows into the product we aimed to reduce delays and confusion at the moment of handoff. In pilot testing, we saw a notable reduction in contacts to our Care team. With this new set of prompts, the customer (armed with their signature link or an ID) gets the prep they need to accept the order. With the customer prepared, shoppers don’t need to wait as long at the doorstep for delivery confirmation, and they receive step-by-step instructions in their own app to confirm delivery and complete the order. Want to build features like these? Our Product, Engineering, and Design teams are hiring! Check out our current openings. Instacart Engineering 60 Fulfillment Product Management Logistics Technology News Mobile Development 60 claps 60 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-17"},
{"website": "InstaCart", "title": "announcing coil 1 0", "author": ["Colin White"], "link": "https://tech.instacart.com/announcing-coil-1-0-5d57b608dc93", "abstract": "Eng Open Source Data Science Machine Learning Android iOS I’m very excited to announce the release of Coil 1.0 . Coil is a Kotlin-first image loading library for Android built on top of Kotlin Coroutines. It simplifies loading images from the Internet (or any other data source) by automatically handling memory and disk caching, image downsampling, request cancellation, memory management, and more. Coil’s image pipeline is also fully extensible and supports decoding GIFs, SVGs, and video frames. We’ve been using Coil at Instacart in both of our Android apps for over a year with great success. Over the past year we’ve refined its API, fixed plenty of bugs, improved its performance, and added support for new features including direct memory cache access , interceptors , custom transitions , and event listeners . Coil is fast ( slightly faster than Glide ), lightweight (~2000 methods for apps that already use Coroutines and OkHttp), easy to use , and its adoption is growing: Coil is also designed to integrate closely with Jetpack Compose - Android’s next generation UI toolkit. Both Coil and Compose build on top of Kotlin Coroutines and, unlike other image loading libraries, Coil is largely decoupled from Fragments and Views. The Android team even highlighted Coil in the videos for the Jetpack Compose alpha release. Coil currently doesn’t have first party support for Compose, however, we’re excited to add it once Jetpack Compose becomes API-stable. In the meantime check out Chris Banes’ Accompanist library , which adds a CoilImage composable. The community’s response to Coil has been very encouraging and I’m excited to see where Coil goes from here. If you’re interested in helping improve Coil, there are a number of open issues to tackle. Also big thanks to John Carlson, Kaushik Gopal, Chris Banes, and everyone else who has supported the library and offered feedback. Also Instacart is hiring - feel free to message me on Twitter or check out Instacart’s engineering openings if you’re interested joining the team. Instacart Engineering 874 Android Android App Development Kotlin Image AndroidDev 874 claps 874 Written by Android @ Instacart Instacart Engineering Written by Android @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-22"},
{"website": "InstaCart", "title": "7 steps to get started with large scale labeling", "author": ["Instacart"], "link": "https://tech.instacart.com/7-steps-to-get-started-with-large-scale-labeling-1a1eb2bf8141", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Neel Ajjarapu and Omar Alonso Organizations that develop technologies rooted in information retrieval, machine learning, recommender systems, and natural language processing depend on labels for modeling and experimentation. Humans provide these labels in the context of a specific task, and the data collected is used to construct training sets and evaluate the performance of different algorithms. How do we collect human labels? Crowdsourcing has emerged as one of the possible ways to collect labels at scale. Popular services like Amazon Mechanical Turk or FigureEight are examples of platforms where one can create tasks, upload data sets, and pay for work. However, homework needs to be done before a data set is ready to be labeled. This is even more important for new domains where there are no existing training sets or other benchmarks… domains like grocery! At Instacart, we are revolutionizing how people search, discover, and purchase groceries at scale. Every day, our users conduct millions of searches on our platform, and we return hundreds of millions of products for them to choose from. In such a unique domain, collecting human labels at scale has allowed us to augment Instacart search and generate best practices that we hope to share. Introducing our “Pre-flight Checklist” of tasks for implementing large-scale crowdsourcing tasks. This list is independent of a specific crowdsourcing platform and can be adapted to any domain. Assess the lay of the land Identify your use cases Understand your product’s data Design your Human Intelligent Task (HIT) Determine your guidelines Communicate your task Maintain high quality Before we jump in, a note on terminology: we use the terms rater, evaluator, or worker interchangeably to mention a human who is processing a task. In a task, humans are asked to provide answers to one or more questions. This process is usually called labeling, evaluation, or annotation, depending on the domain. The first step to approaching human evaluation is to understand what your organization has already done. Make sure to ask the following questions: Have we done any similar human evaluation tasks before? Do we have any human-labeled data? If your organization has already collected human evaluated data, make sure to understand existing processes. Do you have vendors with whom you already work? Is there an established way to store human-labeled data? Existing approaches can influence how you design your crowdsourcing task, so it’s important to take stock. Understand what went well in previous projects and what lessons were learned. If you’re starting from scratch, focus on an area that the organization would like to know more about. For example, you may not know how good your top-k organic results are and want to quantify that metric. At Instacart, we had previously completed a few ad-hoc projects, but now that we are beginning to run large-scale projects, we are revising the methodology. Creating human evaluated data is often a costly and time-consuming process. Make sure to ask yourself: What do we want the human evaluated data to accomplish? Is there a metric in mind? Why is human evaluated data necessary here? Is this a critical project or nice-to-have? Is this a one-off attempt or part of a larger continuous project? Your data could be used as general training and evaluation data, as a way to quality test the output of your model, or as a reference collection to benchmark current and future models. Each of these use cases may require different approaches, which you should keep in mind. Moreover, make sure that your use cases will genuinely benefit from human labeling. Crowdsourced tasks require proper setup and a budget and should only be reserved for tasks requiring human input. At Instacart, we wanted to measure the relevance of our search results. Labeled data helps us understand how relevant the products we show to users are when they enter a query into their search bar. This data can be used for training and evaluating models and measuring the quality of our search results Familiarity with your product and the data generated is crucial. As you spend time looking at the data, you will begin to understand if you have all the data a rater needs to complete a task, how complex a task it will be, and potential gray areas. Ask yourself: What data is presented to the user? What data is sent by the user? What logging do we capture? Do we have all the information we need for a human to make an informed judgment? This understanding is imperative, as it sets the groundwork for your task design. Without spending the appropriate amount of time, you’ll find many surprises and labels that don’t meet your expectations. At Instacart, our goal was to measure how relevant our search results were to our queries — thoroughly considering all the associated data helped us avoid pitfalls later on. For example, we initially assumed that displaying product names and images would suffice in describing our products. However, as we internally tried to evaluate some data, we ran into trouble evaluating queries that specified product size, such as “six pack beer” or “bulk candy.” By revisiting Instacart search, we recalled that Instacart “Item Cards” display the product’s size and quantity under the product name. We made sure to present the same information in our human evaluation task. Had we not performed the internal exercise and found this discrepancy, raters definitely would have been confused on measurement-specific queries, and we would have been in for a surprise with our labels! Ultimately, our search results return groceries — this is entirely different from airline flight times or restaurant reviews — and has its own set of complexities. You will want to do the same legwork to understand the complexities of your product and data. After defining your use cases and data, you will want to design and implement your Human Intelligent Task (HIT) — the actual task you want your rater to complete. We’ll be using “task” and “HIT” interchangeably from now on. In designing your task, make sure to ask: What exactly do you want to measure using human evaluation? What is the most straightforward way for a human to evaluate this data? Your task should try to answer a single or a small set of questions. Avoid conditional or layered tasks, where the rater needs to answer multiple questions, as this adds additional cognitive overhead. Often raters may have language barriers or optimize their work around the volume of tasks they complete, and so complex multi-layered tasks may put you at risk for low-quality results. If you plan on multilingual tasks, such as evaluating both English and French-language products, make sure you design for the product’s native language version first (in Instacart’s case, that’s English), and then expand to other languages. At Instacart, there are many ways to try to measure the relevance of our search results. With the “query-slate” model, a rater could be presented with the query and an array of products, which he/she rates as a whole for relevance. Alternatively, we could try to capture the relationship between the query and product, for example, whether the product is an ingredient of the query or complementary to the query — which we could then map to a relevance score. Ultimately, we decided that the most straightforward approach would be to ask: “How relevant is this product to this query?” — in which a rater evaluates a single query and product pair. This was the most straightforward task we could present to a rater while addressing the most critical question we wanted to be answered. A simple task was especially crucial for us, especially since search relevance for food is already such a complex area. Grocery searches need to consider brands, dietary restrictions, ingredients, compliments, and more — all of which we needed to capture in our guidelines! Creating labeling guidelines is a bit more art than science. Your guidelines need to walk a fine line between being so broad that your labels are imprecise and so prescriptive that a rater is forced to abandon the intuitive judgment that makes human labeling valuable. Developing your guidelines will be an iterative process. You will need to collect information from your team and users to understand and codify how raters evaluate your data. The following are methods and resources that you can use to develop your guidelines: Internal Labeling Exercises: Create a sample of tasks and have your team evaluate them internally, with limited guidance. Tasks with high disagreement may mean that the task is not intuitive and that more clearly defined guidelines will help guide your raters. User Research: If you plan to evaluate data shown to a user, make sure to leverage teams that interact with users often, such as your User Research team or your Customer Experience team (if you have them!). Ask your User Research team about how users think about certain cases that your team disagreed on. Product Information: Think about how you organize and classify your data right now. Are there certain classes of tasks that you are trying to combine into this HIT? You may need to modify your criteria to handle the complexity of those different classes. At Instacart, we went through all of the above. Beginning with a barebones set of criteria, our team evaluated hundreds of query-product pairs. Our ratings had disagreements, and we had to ask ourselves interesting questions such as: when users search for “gluten-free pasta,” how relevant is wheat pasta? Or for searches like “hot dog buns,” is it okay for us to show complementary results like hot dog wieners? For a brand search like “Coke,” how relevant is a competitor’s product, like “Pepsi”? Once we had identified these types of cases, we incorporated input from our User Research team on how existing users think about these types of search results. We went through this process iteratively until we had a set of guidelines that gave us the precision we needed without being over-prescriptive. It doesn’t matter how straightforward your task is if you can’t clearly convey how you want raters to label that data. As the designer of the task, you likely have an amorphous set of rules laid out, which aren’t easily codified. Packaging that information into a digestible set of instructions is a challenge. Make sure to ask yourself: How can we clearly and concisely convey our guidelines to a rater? What examples will be most effective in teaching our task to a rater? How can we present the information consistently with how the information is presented in our product? Creating instructions for raters will require creating a document that encapsulates the criteria you want them to understand and implement. These instructions can be in the form of a booklet, a slide deck, or any other medium. In these instructions, make sure to communicate the criteria step-by-step and present plenty of examples along the way. As you create these instructions, show them to people who aren’t working on your crowdsourcing project. At this point, you are likely intimately familiar with the task and guidelines and will benefit from the feedback of people who’ve never seen the project before. When presenting the information to a rater, think about how a user would interact with that same information in your product. Your rater’s UI should resemble your product’s UI — including text size, fonts, image quality — as closely as possible. At Instacart, we created a set of slides that walked the rater through our criteria, including quizzes that confirm their understanding of the key concepts we presented. We made sure to display the information as close as possible to a typical Instacart Item Card UI with all the associated product information. By creating a simple HIT, a clear and understandable set of guidelines, and a well-communicated set of instructions — you’ve built the foundation for high-quality results. These additional methods will help you measure and maintain rater quality during the evaluation period. Ask yourself: How do we select the best possible judges for our task? How do we avoid incorrect ratings and measure quality? How do we handle bad input data or extremely tricky tasks? In selecting your raters, it is important to know who your raters are. You may want control and visibility into your raters’ specific demographic information, including spoken language, nationality, gender, and age. This can help set up tasks where you want your rater pool to match your users’ demographic makeup. It can also help improve quality — for example, if you are rating English-only results, you likely want only English speakers evaluating that data. Gating by demographics can potentially increase the cost of your ratings, but it can be well worth it for the quality improvement. You also need to make sure that your raters understand the task. After communicating your task guidelines with raters, test them on a series of hand-chosen HITs that reflect the guidelines’ complexity. This will confirm that they understood the task and its intricacies before they can rate actual data. Only allow raters to rate your data if they score on your test above the threshold you’ve chosen. You will want to trust the labels that you get back from your platform. That is, the data needs to be reliable. Data is reliable if workers agree on the answers. Different workers produce similar results if they understand the instructions that we have provided to them. If two or more workers agree on the same answer, there is a high probability that the final label is correct. At Instacart, we had five raters evaluate each task and took the consensus rating (3 or more raters in agreement) as the final score. Inter-rater agreement reliability measures the extent to which independent raters assess a task and produce the same answer. One of the most widely used statistics to compute agreement is Cohen’s kappa (k), a chance-adjust measure of agreement between two raters. A generalization for n raters is Fleiss’ kappa . Both statistics are available in standard libraries and packages like R or scikit-learn. We strongly recommend using inter-rater statistics to measure reliability on every data set. Another common strategy to ensure high-quality work is to include predefined gold standard data in the data set at random, so we can test how workers perform. This technique is known as “honey pots”, “gold data”, or “verifiable answers”. If you know the correct labels for a set of HITs, you can use that precomputed information to test workers. By interleaving honey pots in the data set, it is possible to identify workers who might be performing poorly. If all workers are performing poorly on any particular honeypots, this may also indicate that there is a mismatch between your intended label and how workers are interpreting your guidelines. How do we build a set of honey pots? As part of your internal labeling exercise, identify cases where you and your team have reached consensus. Those cases can serve as your precomputed honey pots. You can then randomly add the gold data into the data set that needs to be labeled, so raters evaluate the honey pot tasks the same as your unevaluated data set. In some cases, the HIT may have poor data, such as an incorrect product image or severely misspelled term. For cases like these, it can help offer the rater an “I Don’t Know” option instead of having them guess. Going one step further, you may want to ask raters who select the option to explain why they cannot evaluate the task. You can provide a list of reasons from which they select or add a text field. These options can help you diagnose your information quality and have the added benefit of deterring excessive use of the option. Additional safeguards, such as rate-limiting the usage of the “I Don’t Know” option, can also be used to ensure that raters don’t abuse the option. Now that you’ve completed the pre-flight checklist, you’re almost ready to label large data sets! With your task and process clearly defined, it shouldn’t be too difficult to find a crowdsourcing platform that will satisfy your needs. The next step is to create processes for the data you want to evaluate, by sampling, partitioning, and preparing the data for continuous evaluation. Crowdsourcing-based labeling is a good alternative for collecting data for evaluation and for constructing training data sets. That said, there is little information on how to set up this type of project and the amount of time and preparation needed. Many projects tend to underestimate the preparation steps and focused only on the specific crowdsourcing platform. Shortcuts like these can, unfortunately, lead to subpar results, as crowdsourcing is more than just the choice of platform. We believe that the checklist is useful for making sure that the project is successful and that the collected labels are of good quality. In our next post for this series, we will focus on the details of running crowdsourcing tasks continuously and at scale. We thank our team members Jonathan Bender, Nicholas Cooley, Jeremy Diaz, Valery Karpei, Aurora Lin, Jeff Moulton, Angadh Singh, Tyler Tate, Tejaswi Tenneti, Aditya Subramanian, and Rachel Zhang. Thanks to Haixun Wang for providing additional feedback. If you are interested in learning more, there is a dedicated conference, HCOMP (Human Computation) , that groups many disciplines such as artificial intelligence, human-computer interaction, economics, social computing, policy, and ethics. These bonus reads offer a good introduction to these topics: O. Alonso. “The Practice of Crowdsourcing”, Morgan & Claypool, 2019. A. Doan, R. Ramakrishnan, A. Halevy. “Crowdsourcing systems on the World-Wide Web”, Commun. ACM 54(4): 86–96, 2011. A. Marcus and A. Parameswaran. “Crowdsourced Data Management: Industry and Academic Perspectives”, Found. Trends Databases 6(1–2), 2015. J. Wortman Vaughan. “Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research”, J. Mach. Learn. Res. 18, 2017. Want to design large-scale data projects like these? Our Algorithms team is hiring! Go to instacart.com/careers to see our current opening Instacart Engineering 32 1 Machine Learning Knowledge Graph Search Crowdsourcing Labelling 32 claps 32 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-27"},
{"website": "InstaCart", "title": "avacado or avocado", "author": ["Jagannath Putrevu"], "link": "https://tech.instacart.com/avacado-or-avocado-4b4b78dc0698", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, our search engine is one of the most important tools customers rely on to quickly find their favorite grocery items in the digital aisles. But, oftentimes, some of the most common grocery goods are the trickiest to spell. We’re not all spellers or digital natives — if you mistype or forget how to spell 🥑 (it’s Avocado by the way) you’re not alone. But, that typo shouldn’t automatically mean you have a poor search experience. Some of our most popular misspelled queries are: Siracha ♨️ Zuchinni 🥒 Jalepeno 🌶 Cantelope 🍈 Parmesean 🧀 During the early days of Instacart our team was small and scrappy, and we used to manually add correction terms for commonly misspelled queries. But, this approach doesn’t scale. My first project as the first Machine Learning Engineer on Instacart’s Search & Discovery team 5 years ago, was to fix this problem. In technical terms, we wanted to improve the Recall of our search engine for misspelled queries. In general, we encounter two main query correction problems: non-word query correction (spelling errors that result in non-words, for example, “avacado” for “avocado”) real-word query correction (spelling errors that accidentally result in an actual word, for example “line” for “lime”) Non-word query corrections can be solved by using a dictionary and some form of distance function to map the incorrect word with a correct word from the dictionary. Real-word query corrections are much harder problems and require more state-of-the-art techniques like building a language model. Peter Norvig wrote a great post on “ How to Write a Spelling Corrector ,” which walks through some of these concepts. Given the short one-word queries we typically see with grocery searches, query correction is not an easy problem. There are thousands of papers on this topic and this is still a very active area of research. For us, the non-word query corrections were the most prevalent and accounted for most of the spelling errors that resulted in zero-result queries in the early days of Instacart. For any given problem, there may be standard “state-of-the-art” solutions that are not always trivial to implement. In the early days, we had to solve a number of technical problems when building the Instacart marketplace, and it was impossible to prioritize each of these problems. So, we had to get scrappy vs. state-of-the-art when tackling these issues. Our commonly used approach to problem-solving in those days was — “can we use a simple approach that solves 80%-90% of the problem in a short period of time?” We often went for a quick, yet very effective, heuristic. It not only solved most of the incorrect spelling problems but also gave us the added benefit of helping with the query reformulation problem , which we’ll explain below. After entering a search query, a customer will take one of the following actions: Add an item to their cart if the query yields precise results with good recall (this event is called a conversion ) Search for a variation of the same query if they do not get any results or are not satisfied with the results Search for a different query altogether if they decide to not convert on the given query Go to the checkout page to place the order Bounce off Instacart.com if they are not satisfied with the experience For every query, each of these can be considered a different state the customer can move to in a Markov Chain . We are interested in helping the customer add an item to their basket (conversion state) as efficiently as possible. If we take the misspelled query “avacado” for example, the transition probabilities look something like this: If the most probable state the user transitions to next is a conversion, then we don’t have to correct the query. If it is any other state, then it most likely indicates a spelling mistake and we want to help the customer reach a converting state in a frictionless manner. By looking at historical search query logs, we can build this Markov model and correct bad queries and lead the user to a successful query. We came up with the following heuristic to build this Markov Chain and easily generate query correction pairs: Identify all consecutive search query pairs from historical search query data (the more data you have, the better) Keep only those query pairs where the second query leads to a conversion Identify the frequency of occurrence of these query pairs in history Use a threshold for minimum frequency (say 10 or 100) and discard other query pairs For each query, compute the probability of going to each subsequent query from the query pair data Compute the Levenshtein Distance between the queries in each pair (it is basically the number of letters you need to change to get to the new query) If the Levenshtein distance is less than a small threshold (we chose 2), then the pairs can be classified as spell correction pairs Using this heuristic, we were able to correct all of our commonly misspelled queries: We also realized that by looking at pairs whose Levenshtein Distance is greater than the chosen threshold (say 2), you could infer potential query reformulations as well. For example: In the above cases, customers didn’t necessarily misspell their queries, but we did not have relevant items for what they searched for. They themselves tried a different alternative for which we did carry relevant items and converted. We used that historical data to automatically rewrite their query to a more suitable one which would yield the most relevant results. Using the heuristic above, we auto-generated thousands of query correction and reformulation pairs and used them to automatically redirect customers to the query with the highest conversion probability when the original query did not yield any results. The resulting experience for customers looks like this: We were able to bootstrap our query correction and reformulation heuristic using our own data. In other words, we crowd-sourced our first search query correction heuristic :) It’s not always necessary to start with the most sophisticated approach to solve a problem, especially when you’re just getting started. Often simple heuristics give you a lot of mileage on most of the problems. Over time, we deployed more advanced Machine Learning techniques to improve on this heuristic. Stay tuned for future blog posts on this topic! Want to work on the next generations of Instacart Search? Our Algorithms team is hiring! Go to instacart.com/careers to see our current openings. Instacart Engineering 156 Machine Learning Search Information Retrieval Startup Query Understanding 156 claps 156 Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-14"},
{"website": "InstaCart", "title": "building an ad engine for grocery", "author": ["Instacart"], "link": "https://tech.instacart.com/building-an-ad-engine-for-grocery-8d6bb1753831", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We’re excited to announce that we just welcomed Vik Gupta to Instacart as our first-ever Vice President Engineering, Advertising. Vik comes to us from Google where he was a VP of Engineering and spent nearly 15 years building and scaling a ton of technologies that many of us interact with every day. Early on, Vik was one of the first engineers to work on GSuite. He then led the development of Google’s agency-focused advertising product, Search Ads 360; and built best-in-class Ads trust and safety systems. He also oversaw the company’s effort to stop fraudulent ads traffic, where he led the team through a pivot to large scale machine learning. More recently Vik led efforts on the Local Search engineering team, including making COVID-19 testing centers available on Google Search and Google Maps to users around the world. At Instacart, Vik will lead the team that’s streamlining and scaling the technologies behind our self-service advertising platform and managed ad products. We sat down with Vik to talk about transitioning from an individual contributor into an Engineering Manager, radical product rewrites, and (of course) what’s in his cart. You’ve got a long career that spans some of the biggest names in technology — what are some of the key learnings you’ve gained from each? I’ve been fortunate to work at some of the most enduring companies in Silicon Valley, including Oracle, Intuit, and Google. I’ve also had the chance to try my hand at a couple of startups. Every environment is different, and I’ve learned so much at each stop. At Intuit, I was part of the team that built an ahead-of-its-time web-based personal finance product. There, I first got to really understand usability studies: bringing in real-world users so we could study them and really get to know how they used our products. Of course, this is common practice now. When I joined Google and started working on GSuite, I came to appreciate the awesome power of scale. We had a big ambition to revolutionize how productivity applications worked for business and education users. Even as a new member of the team, I was working on systems that were expected to scale to millions of users. That expectation of scale completely changed how I approached building systems. In my decade on the Google Ads team, I learned that in order to run a long term profitable ads business, there needs to be a commitment to not just making the systems a little better year after year, but radically better. In Google’s case, this meant that at any given time, 10 or more significant parts of the system were under a full rewrite. I led three of these rewrites, and contributed to countless others. Reimagining systems can be rewarding and super fun. What made you want to move from an IC role into an engineering management path? I worked as an individual contributor and tech lead of smaller teams for the first decade or so of my career. In 2007, I transitioned into engineering management, and I quickly realized that I had found my calling. I love working on hard problems — building great systems to tackle the business need — but I enjoy the process of building teams just as much. I really believe that it’s a privilege to work on problems so large that you need a team to get after them, and I find it incredibly rewarding to bring those teams together. I also really love helping folks figure out what they want to do in their careers; guiding folks with a little of my own experience is particularly fulfilling. Why did you make the jump to Instacart? After interviewing with the team, did anything surprise you? I’m so excited to be joining Instacart! There were many aspects that led me to take the leap, but just to name a few: First, the core product is useful and helpful. The product “aha” moment for me came during the early days of the COVID-19 shelter-in-place orders. My parents live in the Twin Cities. When things closed down there, my brother set them up with weekly deliveries from Instacart. It was an absolute lifeline. My wife and I set up Instacart for our weekly groceries, and it’s been a gamechanger for us too — the convenience is just amazing. And our kids love not having to go to the supermarket! Second, I was incredibly inspired by the broader advertising opportunity and ability to connect leading and emerging brands directly with customers in the digital aisles of the retailers they love. Finally, and most importantly, the people: everyone I met was motivated, passionate, and really smart! What qualities do you look for in an engineer? I believe that you hire people, not positions. Put another way, I see my role as matchmaker — bringing the intelligence and passion of an engineer together with a hard problem that needs to be solved. So, I look for hunger first and foremost. I also look for folks who are curious and passionate about technology. And since most engineering work happens in teams, I look for folks that are excited to collaborate with others and create things that are bigger than what any one of us could do on our own. It’s day one — what big project are you excited to tackle? On day one, I’m just focused on learning the core consumer Instacart business, as well as our ads business. I’m personally excited to figure out how we can create ads that make the customer’s experience even better. I really like that Instacart has leaned into building a search ads business. Thinking longer term, I am dreaming about how to bring more inspirational content into the ads. Lastly, what’s always in your cart? With two boys at home, it’s a combination of what my wife and I call “grow food” — chicken, milk, yogurt, spinach and more … along with some fun treats like dark chocolate! Want to work with Vik? We’re hiring! Check out our careers page to see our current Engineering openings. Instacart Engineering 142 142 claps 142 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-29"},
{"website": "InstaCart", "title": "understanding software quality at scale", "author": ["Dragan Rakas"], "link": "https://tech.instacart.com/understanding-software-quality-at-scale-79ebb25cb3ac", "abstract": "Eng Open Source Data Science Machine Learning Android iOS A common challenge when building good software is: how do you measure your product quality at scale? What feedback would you expect to get when you’re building good software? With hundreds of Instacart Enterprise deployments per day impacting dozens of retailers, it is impossible to verify everything by hand. To measure and monitor Enterprise software quality, the Enterprise Test Automation team has built Mango , which is an API test automation solution embedded across all of our development, UAT, and production environments. In this post, we’ll dive into the metrics collection and reporting workflows we built. An important part of measuring product quality is to embed running the automation suite throughout the entire Software Development Life Cycle (SDLC). Test runs are triggered each time engineers commit product code, but they are also triggered by every deployment including the final deployment to production. At Instacart, we also run tests on an hourly basis since a lot of test scenarios are driven by the product catalog which changes over time. These frequent test runs help us capture, collect, and report on software quality metrics. By incorporating test runs into all of these workflows, we can also capture the frequency of test failures based on the environment. This allows us to set goals around test failures and bugs: we expect the number of bugs and test failures to decrease from the highest count, to a very low number (or none!) by the time our code reaches production canary services. If for some reason this metric is reversed then it’s likely that not enough bugs are being caught early during Pull Requests, which tells us we should be focusing on improving the local development workflow. We aim to instrument every part of the test process so that we can publish these metrics and build team and organization goals around them. Below is a list of a few key metrics that we capture Test Status : How many test cases passed, failed, xfailed, xpassed, or were skipped. This is our most fundamental metric — in particular how many tests have failed or xfailed. When tests are xfailed, there is either a bug causing the failure or the test code itself is outdated. This is a great way to measure tech debt. Test Duration : We aggregate by environment, team, and feature. The goal is to continually reduce the average duration per test case to maintain a highly efficient test suite. Using this metric, we are also able to identify which tests are the slowest, which gives us ideas on what code we can focus on to speed up our systems. Bugs in JIRA : How many bugs are reported in JIRA. When we investigate test failures, we create bugs in JIRA and attach them to the automation test failure. This is also a good indicator of technical debt and overall quality, and we want to keep this metric low. Test Case Counts : We aggregate this number by team, feature, and contributors. As mentioned earlier, this allows us to give recognition to top teams and people that contribute tests, while also allowing us to identify and encourage change on teams with no tests. Code Coverage : Lines of code coverage, aggregated by platform (API, UI, etc.). By deploying an instrumented version of the product codebase, we can collect and track code coverage metrics. This allows us to see which product areas have no tests at all, and also which code is completely unreachable! It’s really common across many companies that code changes get merged which are never actually executed in the deployed application. Static Analysis Reports : Automatically generated metrics, which include cyclomatic complexity, average lines per class, average lines per function, etc. We do not strictly enforce limits here — we use this to provide visibility to our engineering teams, which helps to proactively reinforce good development habits. All these metrics serve one primary purpose: dashboards! Dashboards allow us to add alerts, notify the right stakeholders, and, most importantly, create measurable goals and hold ourselves accountable to maintaining a high-quality product. Collecting and publishing test automation metrics allows our Enterprise Engineering teams to build comprehensive dashboards that give us a good snapshot of our product quality. Since metrics are published to Datadog at test case granularity, we can drill down from the high-level overview shown above into test results by feature and even by test case. These metrics are great because in situations where a test case fails in just one environment the root cause is typically a specific integration issue, while a test case failure across the board is a good indicator that bad code made it through to a production deployment. Instacart’s Mango framework leverages pytest’s test case states and attaches a meaning for each state. The diagram below illustrates each of these states, and also the actions required to move between states: We find that these states reflect a very typical process in our SDLC. Capturing these states also provides us a way to measure how many tests need to be worked on (those in FAIL status), and also a way to measure technical debt, (a count of tests in XFAIL status). When building our quality dashboards, the most important thing we consider is “ how is the information being displayed actionable? ” If a dashboard exists but no actions can be taken to investigate failures and improve the metric, then it’s not really helpful to investigate the issue. With Instacart Enterprise, Mango attaches a request ID header for every API request sent, which is then published to Datadog for any test case that failed: To further improve the feedback cycle, test owners are notified via Slack each time a test run fails. The appropriate on-call team member is pinged based on the test failure, and we strive to provide enough reporting information so that a failure can be troubleshoot-ed without the need to re-run the tests again locally: We quickly realized that black box testing our API was an unmanageable approach. For example, what if we need to automate a test case that verifies if a $2 for 1 coupon offer is correctly applied to your cart? In an extreme example, a retailer may have 10 stores and only 1 product discount, so how could we quickly find such a product? A common solution is to run a query against a Product Catalog DB or ElasticSearch instance, but this doesn’t scale well. The biggest drawback to this approach is that test code is now tied to your infrastructure and schemas. The more systems you have tests for, the more integrations you need to manage. Maintaining sensitive credentials, database connections, etc. are dependencies too big to introduce to our test framework. We came up with a system which allows our tests to interact directly with our data sources using an endpoint like GET /api/fixtures/products . The test setup methods call the endpoint with a few parameters based on a particular scenario, receive products for their scenario, and carry on with the test. This is convenient because the product code already has all dependencies in place to serve such a request, and the endpoint is also reusable for UI automation tests. Driving adoption among product teams is a critical part of any test automation framework. It is very important to us to embed team and product ownership into automation tests and also to reward our top contributors. A little positive reinforcement goes a long way, and we want to make our engineers feel good about contributing to automation tests by rewarding them, rather than making it feeling like a chore with no positives. Before each merge into master, we capture the github usernames of engineers who added tests and their corresponding product teams. This allows us to give “kudos” to teams that are staying on top of automation efforts, while focusing on framework support and driving adoption for lower-contributing teams. A great way to create an environment of ongoing recognition is to show a monthly snapshot of the dashboard at an all hands meeting as well on a mounted monitor at the office. As Mango matures, driving adoption across the organization is critical. Everyone is a contributor to quality and that’s the attitude and culture we have been encouraging at Instacart. As a new Software Engineer joining the company, you can expect an onboarding workshop scheduled with a Test Automation team member in which you will cover our best quality practices and how to contribute to our automation efforts. The Test Automation team has made it easy for engineers to see how they contribute to quality, and we are now driving Mango’s usage across multiple product areas within Instacart! As Instacart continues to improve our automated test coverage, we need to remind ourselves that it’s not just about the number of test cases — we need to measure our progress and set goals against our observed metrics. A medium to large unorganized test suite adds only short-term value to the organization; it eventually gets scrapped or rewritten because no one knows how much is really being tested and the test code base (as well as the code base being tested!) is too large to easily comprehend. Automation becomes a powerful tool to drive good quality and delivers long term value only after we start capturing meaningful metrics and delivering actionable reporting from our test suites. Want to build tools and processes like these? Our Enterprise engineering team is hiring! Visit our careers page to explore our current openings. Instacart Engineering 118 4 Testing Automation Quality Assurance Software Development Enterprise Technology 118 claps 118 4 Written by Software Engineer at Instacart with a passion for building test automation frameworks from the ground up. Instacart Engineering Written by Software Engineer at Instacart with a passion for building test automation frameworks from the ground up. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-02"},
{"website": "InstaCart", "title": "a small engineering team with a big impact", "author": ["Instacart"], "link": "https://tech.instacart.com/a-small-engineering-team-with-a-big-impact-40184c40d7e", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We just welcomed Haixun Wang to Instacart as our first-ever Vice President of Algorithms and Distinguished Scientist. In his new role, Haixun will lead the team that fine-tunes the artificial intelligence technology and machine learning models that power our four-sided marketplace. These models help us understand what’s on the shelves in stores, recommend item replacements, normalize catalog data, make personalized cart recommendations, serve ads and coupons, and get groceries to your door within those tight 2-hour windows. Haixun spent his career researching artificial intelligence and implementing cutting edge machine learning technologies at IBM, Microsoft, Google, Facebook, Amazon, and WeWork. We sat down with Haixun to talk about AI’s practical applications, building teams, and (of course) what’s always in his cart! You’ve got a long career in research and technology. How did you transition from research to engineering management? In retrospect, I have been a little restless in my more than 20-year career as a computer scientist and engineer. For a large part of it, I worked in industrial research labs. I was with IBM Research for nine years, Microsoft Research for four years, Google Research for two years, and my passion and focus in research was knowledge bases and natural language understanding. I was yearning for some real-life impact, so I moved to Facebook and then Amazon to lead engineering teams in Search. I always wanted to join a startup because I believed it would enable me to see the big picture with regard to priority and strategy. So, after being with many of the biggest tech companies in the world, I joined WeWork to lead its Applied Science organization. It gave me the opportunity to explore new curiosities such as computer vision, IoT, and robotics. The last 2.5 years at WeWork did broaden my scope as I worked on many data-driven projects such as pricing, revenue optimization, and marketplace intelligence, all of which are important to their core business. How did you get to know our product? Unlike a lot of my friends and relatives, I actually knew about Instacart well before the pandemic. After seeing a shopper make a delivery to a coworker, I talked with the shopper and inquired about things like how much capacity his car had, how many orders he typically fulfilled in a day, and how he would like to do things differently. Never had I imagined one day I would be seriously looking into these same questions from a pure algorithmic perspective. I’ve been a loyal customer of Instacart and, since the shelter-in-place orders took effect in San Francisco, I’ve found myself in a “debugging” mode.” As I fill up my cart, I often think: “Hmm, the result of this search needs to be more relevant.” Now, I have an opportunity to actually drive the positive change I’ve wanted to see as a customer. After interviewing with the team, were you surprised by anything you found out about the company? No really big surprises — e-commerce technologies have thrived and matured over the last 20 years as industry and academia continue making big investments in this area. Still, I was both surprised and impressed by how small the technology team is (especially its machine learning and data science component) given what Instacart has achieved as a company. It was amazing see how Instacart has scaled over the last year, and how the company has stepped up to safely serve customers, shoppers and retailers over the last few months in the face of this sudden, unexpected surge in demand spurred by COVID-19. I consider this the best moment to join a company like Instacart. It is in a stage where teams are small, problem spaces are broad and deep, and opportunities are boundless. But, it’s also beyond the stage characterized by ad-hoc solutions and short-term thinking. From an engineer’s point of view, this is the moment when things become more exciting and rewarding. What hard challenges do you foresee the brand new Algorithms team tackling? The Algorithms team consists of machine learning engineers, economists, operations research scientists, as well as system and infrastructure engineers. In the short-term, the biggest challenge is to maximize the business impact that can be made by a team with limited capacity. This requires us to ruthlessly prioritize, to decide where we have to dive deep, and where a minimal viable product (MVP) will suffice for now. In the long term, the biggest challenge in my eyes is to revolutionize the online shopping experience for our customers. We need to go beyond the current “search, click, and ship” paradigm. Today, customers want to be informed, inspired, enlightened, and entertained. How do we do that? Well, certain technologies, such as conversational AI, augmented reality (AR), or combining e-commerce with social media will help, but more importantly, we need to rethink e-commerce with regard to its goal and scope. What qualities do you look for in a Machine Learning Engineer? A machine learning engineer is not just someone who can train a deep learning model in an existing deep learning framework. We are eager to work with scientists and engineers who have a solid background in math and statistics, who understand not just predictions but also actions and payoffs, who possess the intuition to separate possibles from impossibles, and who have the imagination to turn impossibles to possibles. It’s month one. Now that you’re settling in, what project are you going to tackle first? I’ve spent my first week getting to know the team and working hard to understand the status of existing projects. We’re focused on things such as supply and demand prediction and search relevance changes that improve customer experience and satisfaction. There are so many more projects the Algorithms team works on — projects that touch all four sides of our marketplace — but again, it will take time to decide how to prioritize them all. Last but not least— what’s always in your cart? I am nuts about nuts. Almonds, pistachios, walnuts, cashews are my absolute favorites. I also love granola and Greek yogurt. If you catch me turning off video during Zoom meetings, I am probably snacking! Want to explore new practical uses for AI with Haixun? We’re hiring! Check out our careers page to see our current Engineering openings. Instacart Engineering 57 Machine Learning Artificial Intelligence Deep Learning Engineering Ecommerce 57 claps 57 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-15"},
{"website": "InstaCart", "title": "introducing arn a library for working with aws arns", "author": ["Francois Campbell"], "link": "https://tech.instacart.com/introducing-arn-a-library-for-working-with-aws-arns-1c1ee17b43e2", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we run our infrastructure on AWS, so our systems often deal with AWS ARNs . We often run into cluttered code, and needed to develop a solution for simpler, safer code. That’s why today, we’re releasing arn , a Python library that simplifies parsing, validating, and working with AWS ARNs in a type-safe way. Here’s an example of what arn can do, in this case parsing a Target Group ARN: arn also checks that its input is indeed a valid ARN: If you’re using type annotations, arn can help you enforce that function parameters are valid ARNs: If you have multiple resources in your AWS infrastructure that have some attributes in common, arn can also be used to generate an ARN from another: arn is still quite new, so it only supports the AWS resource types that we use here at Instacart, plus a few more popular ones: ECS Capacity provider Container Instance Cluster Service Task Task definition TaskSet ELBv2 Load Balancers (Application and Network) ALB/NLB Listeners ALB/NLB Listener Rules Target Group IAM Role STS Assumed role S3 Access point Bucket Job Object arn supports Python 3.6 and up and has no runtime dependencies (except for a dataclasses backport if you’re on Python 3.6). To install it, simply run: pip install arn or add arn to your setup.py or requirements.txt. The docs are available at https://arn.readthedocs.io/en/latest/ If you’re interested in contributing, or just want to take a look at the source, come visit us at https://github.com/instacart/arn . Instacart Engineering 254 AWS Open Source Python 254 claps 254 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-20"},
{"website": "InstaCart", "title": "building an essential service during a pandemic", "author": ["Instacart"], "link": "https://tech.instacart.com/building-an-essential-service-during-a-pandemic-3e2e51616a45", "abstract": "Eng Open Source Data Science Machine Learning Android iOS It’s been a busy few months. We’re seeing the highest customer demand in Instacart history and have more active shoppers on our platform today than ever before, picking and delivering groceries for consumers across North America. During this crisis, our Engineering, Product, Design, and Data Science teams went into overdrive to strengthen our infrastructure and add new features into the app to better reflect everyone’s new normal — bulk buying and low item availability. COVID-19 has changed the way everyone gets their food, and it’s required us to redesign our products and systems to meet the 500% increase in year-over-year order volume we’re seeing. We’ve sat down with our CTO, Mark Schaaf, to take a look at some of the moves we’ve made behind the scenes to keep our site stable and maintain service during this time. Let’s start at the foundation — how has this influx of demand affected Instacart’s Infrastructure? Rapid scale has a way of revealing bottlenecks. Instacart has experienced three years of projected growth in 30 days, which can put a huge strain on your systems. We’ve essentially had to break and reset our technical infrastructure to rectify our expected growth timeline with the order growth that was happening on the platform as shelter in place orders rolled out across North America. Our number one mission throughout March was to scale out our infrastructure ahead of our trajectory of 20% day-over-day growth. This infrastructure is the foundation for our four-sided marketplace including our customer-facing app, shopper app, enterprise software, and advertising engines. As traffic and orders increased, we had dramatically different read and write usage patterns and had to change the configuration datastores, often making upgrades multiple times a week. I’m proud of how the team has been sprinting to implement three years of technical scale immediately. How has the product changed? Over the last two months, we’ve had to completely reset our product roadmap to focus on features that prioritize shopper and customer safety. Instacart is a data-driven organization — we build and test often, and use as much data as we can gather to inform new features for our customers and shoppers. In March, the team had to walk a fine line between maintaining our data-driven decisionmaking culture and launching features we know in our gut are the right move. One of the most noticeable new features we launched is “Leave at My Door Delivery.” We began developing this feature in late 2019, and at the onset of the pandemic, we observed more and more customers in test groups opting into Leave at My Door Delivery. Knowing how critical this could be to customer and shopper safety, we fast-tracked the experiment window and pushed the feature nationwide in mid-March to meet demand. We made a similar decision for a critical feature we were testing in the Shopper App. Early in 2020, we piloted a Mobile Checkout feature, which allows shoppers to pay at the register using Apple Pay or Google pay. We were piloting this in a few select metro areas, gathering data to inform a larger rollout. In March, we recognized the immediate need for Mobile Checkout and shipped it nationwide, opening up the floodgates. We also greenlit a more streamlined alcohol delivery feature that allows shoppers to scan a customer’s ID upon delivery, eliminating the need for customers to sign a shopper’s phone to help adhere to social distancing parameters. We rolled out Fast and Flexible ordering, which lets customers forego normal pre-scheduled 1-2 hour delivery windows and opt for the first delivery window to open up in their area. To put it simply, it allowed customers to add their orders to a “first available” delivery queue. This went from idea to test to national roll out in about two weeks. What about item availability? Where to start? At the beginning of March, we saw nearly a 30% drop in ordered items being found. The average customer basket size has also grown by more than 35% month-over-month. At the onset of the shelter in place orders, surges in demand, bulk buying, and quantity restrictions on certain items (like bathroom tissue and flour), created a gap between what’s in-store and what was reflected in-app. We needed to make a lot of changes to get a better understanding of what items will be in stock on the day of delivery and set appropriate expectations with customers as they fill their carts. On the back end, the team quickly built out a tool allowing retailers to send us maximum item quantities. This allowed us to roll out retailer-specific item maximums in our customer and shopper apps that match in-store policies. We can now track the different maximums across more than 350 retail partners and over 25,000 stores every day. Our Machine Learning team mobilized quickly to make substantial changes to our Item Availability Model in short order to reflect the reality on grocery store shelves. We doubled the rate at which we were running our item availability model, running it every 60 minutes to better understand availability fluctuations throughout the day. Previously, our availability model looked back at a 30-day period to make solid availability predictions — now, we’ve narrowed that range to one week, and even three-day windows to better understand what products are flying off the shelves. One week it’s hand sanitizer, the next it’s flour. We more than doubled the number of products scored by the model and narrowed the window of historical shops that we look at to reduce noise. At the Product level, this fine-tuned understanding of what’s in stock informed new features to help set expectations for customers and shoppers. We now can automatically filter out of stock items out of search results. If you were looking for baking yeast in early April, for example, and couldn’t find it, this is why it didn’t pop up in your search results 🍞. We also added visible “out of stock badges” to item listings that were likely to be out of stock. How has last-mile delivery changed? Algorithmically, our last mile looks very different now than it did eight weeks ago. We’ve spent years building a carefully choreographed string of Machine Learning models to predict demand, understand capacity, and make order dispatching decisions just in time . Those have all been thrown out of whack due to long social distancing lines outside of stores, and frequently changing store hours as our retail partners respond to increased demand. We’ve rapidly adjusted our fulfillment capacity model that calculates just how much order capacity we have at any given moment throughout the day as shoppers log on and off and orders are completed. This model relies on a host of data like the number of shoppers on the platform in a given area, the shopping speeds at any given retail location, and the number of orders in our queue at different locations to understand our true order capacity on the ground. Since the onset of this crisis, we’ve doubled the rate at which we run this model, taking into account new store hours and restocking times. We re-compute the model every two minutes to get a near real-time understanding of our true fulfillment capacity on the ground. That’s why sometimes customers can check their app throughout the day and see new fulfillment times open up. Want to solve hard technical problems like these? Join our Hacker Org! Visit our careers site to see our current openings . Instacart Engineering 143 1 Engineering Infrastructure Technology News Machine Learning Product Management 143 claps 143 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-26"},
{"website": "InstaCart", "title": "terraforming rds bonus anecdote", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/terraforming-rds-bonus-anecdote-da1437b0403b", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Thanks for making it all the way to the end of this series. See Part 1 for an overview of RDS and Terraform, Part 2 to get the basics of using Terraform with RDS and modules , and Part 3 to understand parameter groups. This last post is just a quick illustration of how the layers of abstraction hid a very important detail from us. We were upgrading one of our databases from Postgres 9.6 to Postgres 10. We had a very large parameter group with dozens of parameters set. When we created the new parameter group for Postgres 10, we copied the 9.6 parameter group and made a few “improvements”. After the upgrade the CPU of the database started spiking on a regular basis, leading to significant problems in our application. It was all hands on deck, including AWS support, trying to diagnose the problem. Obviously, the changes we’d made to the parameter group were suspect, so we reverted it, using the Terraform for the 9.6 parameter group, back to exactly what it had been before. The problems persisted. After days of investigation, updates and reverts of parameters, and ongoing CPU spikes, someone said: IMO we need a way to diff postgres configs directly, not just RDS parameter group settings It occurred to me that we could compare the actual settings by looking directly in the database, so I poked around quickly and found this query: select name, setting, unit from pg_settings; This allowed me to pull all the parameter values from both an old copy of the original database and the new database. A quick sort and diff and I saw that one of the parameters was max_wal_size , which was set to 128 in both databases, just as we specified in the parameter group resource: The units column, however, was different. In the PostgreSQL 9.6 instance: In the PostgreSQL 10 instance: This is very different. With the “same” setting, but a unit of 1 megabyte, the entire max_wal_size is 128 megabytes, 1/16th of what we expected it to be . We went back and looked at the AWS documentation. It did not say anything about a change to the units being used for this parameter. We put in a ticket for them to fix the docs. The response was that the docs would be fixed within a month 😬. This explained the CPU spikes and all the other issues we were having — instead of checkpointing every 5–10 minutes, we were checkpointing more than once a minute! The combination of RDS and Terraform hid this change from us — we relied on the Terraform and believed that our settings were identical when they were not. Want to work on challenges like these? Surprise, Instacart is hiring! Check out our current openings . Instacart Engineering 40 1 40 claps 40 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-04"},
{"website": "InstaCart", "title": "terraforming rds part 3", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/terraforming-rds-part-3-9d81a7e2047f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS See Part 1 for an overview of RDS and Terraform, and Part 2 to get the basics of using Terraform with RDS and modules . This post covers parameter groups. A parameter group is just a list of parameters and values, which you can see in the AWS console: If you were administering your own PostgreSQL instance, you would set these values in various ways: In the server configuration file, postgresql.conf On the command line when starting the server In the database directly, using SQL to set values Using AWS RDS, you don’t have access to the configuration file or the server startup command so AWS provides the “parameter group” resource to configure your RDS instance on startup. Notice the “Apply type” column in the screenshot above. If the value in this column is “dynamic” then the value can be set or updated while the server is running. If it is “static” then the server must be restarted for the parameter to take effect. Since a parameter group is a separate resource from the RDS instance, you can update a static parameter value in the parameter group without restarting the server; AWS will store the change to be applied later. Dynamic and static parameters are handled in Terraform using the apply_method when defining the parameters. For static use apply_method = \"pending-reboot\" For dynamic use apply_method = \"immediate\" Given that each parameter is either static or dynamic and will be applied according to its type, why do we need to specify these? The AWS provider leaves contextual validation to the AWS API; it will only warn you about syntax errors. The API call to add parameters, modify-db-parameter-group requires that the ApplyMethod value be provided. If you did not pass this to the aws_db_parameter_group resource then the AWS provider would have to maintain a list of all possible parameters and their types, which would become a big maintenance problem. If you change an immediate value in the parameter group, it will be applied to the database as soon as you apply the change to the parameter group: Notice that the plan diffs for even this simple change can be a little hard to read, because Terraform removes the old parameter and adds a new parameter rather than simply updating the value. The changes are not grouped in any particular way, so with even a medium-sized parameter group a deletion may not be adjacent to the addition with the new value. It turns out there is also a bug in how parameters are updated which gave us a few sleepless nights. It’s described at length in the GitHub issue , but tl;dr: parameters to be added are are added, then parameters to be removed are removed, meaning that you can end up nulling out a parameter you were trying to update. Okay, back to our example. Apply the change and check the database configuration in the AWS console. AWS will be automatically applying the change, and soon your DB will have the new value. Now let’s change a static parameter. Plan and apply, then check the configuration in the AWS console. You will see that the change has not been applied, and the parameter group is marked as “ pending-reboot ”. You will have to reboot the database for the changes to take effect. After the reboot the parameter group will be “ in-sync ” again. It’s easy enough to understand how the apply_method value works in these cases, but if you specify the wrong apply_method for a parameter, you get some unexpected results. Let’s start by specifying immediate for a static parameter. There’s no indication in the plan that this is not the right apply_method . You don’t find out anything is wrong until you try to apply. AWS warns you and won’t let you change the value. How about the other way around? Plan and apply the changes: There is no objection from AWS. If you look in the AWS console, you will see that the parameter value is being applied right away, even though that is not what you specified in the config. Unfortunately, since the apply_method attribute is part of the parameter block, you will now see a diff every time you plan, because Terraform will note that what you have specified in your HCL does not match what is in AWS. So, if apply_method is set in a way that does not match the parameter type the AWS provider will not do what you expect. Our Terraform team got a Slack message recently, with a section of a plan that looked odd: What’s going on here? Is it really trying to add a parameter that is already there? Taking a look at the HCL for the parameter group, sure enough the parameter had been added to the HCL twice, and the AWS provider happily compared the values and decided we must know what we were doing, so it left the existing value alone, since it had not changed, and tried to add the new one even though it was clearly intended as an update. Once again it is left to AWS to decide what to do with contradictory input, the provider makes as few judgements as possible about the content of your config. Since parameter groups are separate resources in AWS they are defined separately in your Terraform as well, but parameter group changes are tied very closely to db changes in AWS. You show this dependency in your HCL by using the output of the aws_db_parameter_group resource as the input to the aws_db_instance resource. It is also valid HCL to specify the parameter group by name, but in this case Terraform would not be able to deduce that there is a dependency between these resources: This dependency can cause a problem when you are making a major change to the parameter group such as changing the version of Postgres. In this case, Terraform will want to replace the parameter group. Terraform does this by deleting and then creating a new version of the resource. The parameter group resource is separate from the RDS instance, but it is attached to the instance so AWS considers it to be in use and it will not allow you to delete a resource that is in use. The Terraform AWS provider doesn’t check this, so you don’t find out until Terraform tries to apply the changes. If you want to make a change like this, you need to create a new parameter group and attach it to the database instance. Then you can remove the old parameter group. Modules are a great feature of Terraform, but they are a difficult fit with parameter groups. Parameters in the HCL for parameter groups are blocks rather than attributes: We could have exhaustively enumerated every possible parameter in the module inputs, but we don’t want to set most of those values, so we added only inputs for values we changed commonly. However, we then needed to allow for other values that someone might want to change. Blocks can’t be passed as variable values, but it turns out that a group of blocks turns into a list of maps, so we were able to handle this by creating a list out of all the parameters created with variables and using concat to merge it with the other parameters: This works, but it is very confusing to the user. We ended up with situations like this: Which value for autovacuum_naptime is the intended value? It is certainly possible to make a useful parameter group module, but in the end we decided to forgo using a module because it provided relatively little value while making the interface much more confusing for the user. If we revisit creating a parameter group module, I will recommend enumerating all the parameters we would ever allow to be set in the variables. We would do this if we determine that the majority of parameters can be computed from a small number of inputs and we want to standardize these computations. However, this would mean we would need one module per major version of PostgreSQL, as the available parameters can change significantly across major versions. Come back again and I’ll tell you about that time an abstraction bit us really hard! Want to work on challenges like these? Surprise, Instacart is hiring! Check out our current openings . Instacart Engineering 108 108 claps 108 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-27"},
{"website": "InstaCart", "title": "terraforming rds part 2", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/terraforming-rds-part-2-849cedfafa67", "abstract": "Eng Open Source Data Science Machine Learning Android iOS See Part 1 for the basics of RDS and Terraform. This post covers using Terraform with RDS and modules . Keep in mind the abstraction layer stack from the previous post, modules can have a significant impact on how painful this is: According to the Terraform documentation: “A module is a container for multiple resources that are used together.” Coming from a software background, I think of a module as a function with inputs (variables, in Terraform), outputs, and a lot of side effects. In software development, you try to minimize the side effects of a function, as they can surprise the caller, but in Terraform, the side effects are the whole point of a module. Modules can be the most opaque layer of abstraction, behind which almost any magic can occur without the user being aware of it. For this reason, there are several best practices when using modules: Document the API and side effects . Make sure that it’s clear to the user of the module what values they can (or must) put in, what values they will get out, and what additional resources might be created or changed when they use the module. Use a separate GitHub repo . It is possible to create your module in the same repo as your other configuration, but then anyone can change the API or side effects of the module at any time and this will affect every use of the module, not just the one someone is updating the module for. HashiCorp also provides a module registry for useful community modules. Create release versions . Even in a separate repo, someone can change the module, so don’t just reference the master branch everywhere. By using release versions, you can update the module when you need to without affecting other usages until you are ready. Keep releases up-to-date. When someone makes a new release, make sure to update existing usages of the module as soon as is practical so that you get any benefits of the changes. Otherwise, instead of helping you keep things consistent and up to your standards, you’ll end up with a confusing mess of versions that do slightly different things. Note that modules can call other modules, so the layers of abstraction can get as deep as you are willing to go. Generally this is not a good idea, as it makes it extremely difficult to understand what is happening when you use the module, and even more difficult to modify a module which is depended on by other modules. While I’m a big proponent of DRY in code, some amount of duplication is better than too many layers of abstraction in the case of Terraform. We have an RDS instance module which has accumulated a lot functionality over time: It configures our primary and replica databases, with configuration values computed based on the role of the instance It adds DataDog monitors for several different metrics on each database instance It creates a security group for each instance It creates an encryption key for each instance It also used to create the parameter group, but this became extremely problematic to maintain, particularly as we changed versions of PostgreSQL and available parameters were added and removed, so we now manage parameter groups separately. With so much functionality, the inner workings of the module have become increasingly difficult to understand. As we created more databases, we started having errors every time we applied our Terraform: The engineers were understandably frustrated, particularly as it can take around an hour to create a database and this error does not show up until after the database is created. AWS, RDS, and Terraform are all heavily used across the industry, so it was reasonable to conclude that the thing which was unique to us was at fault. We started trying to determine what was wrong with our module. The plan created when using the module looked fine. The error message says that both allocated_storage and iops must be specified in the API call, and both values were clearly specified in the plan: We googled the error message and found that there WAS problem with the AWS provider not sending both values— but it was fixed in 2016 . It’s 2020 and we’re still seeing the same error. What is going on here? After a lot of investigation, here’s what we found. The API call to create a new primary database is create-db-instance. The API call to create a replica is create-replica-db-instance. The create replica call does not allow the --allocated-storage flag; it will always create the replica with exactly the same storage as the database being replicated. The AWS provider understands all of this and makes the correct calls, but our module sends a value for allocated_storage which the provider therefore has to set aside while creating the replica. After creating the instance, the AWS provider has this unapplied value for allocated_storage , so it tries to do precisely what we asked and specify the storage. To do this, it uses the AWS API call modify-db-instance with the argument --allocated-storage and the value we specified, even though this is the same value it already has from the primary. This call can be found in CloudTrail: Examining this call, we can see that this is why we are getting an error message — it contains a value for allocatedStorage but not for iops. Notice that the modify call is also specifying the parameter group name. This is because the parameter group can only be specified for replica creation for Oracle DBs and we’re making an PostgreSQL instance. This means that the parameter group has to be added in a modification, which therefore means that the replica database will almost certainly need to be rebooted after creation in order to apply the parameters. Read the next installment of this series for more detail on how this works. Our module manages both primary and replica DBs, so it requires allocated_storage as an input. Even if it were managing only replicas, it would still have to allow allocated_storage since is possible to modify the storage of a replica to a different value than the primary. We could change the module so that it doesn’t include allocated_storage in the attributes for a replica, but this would mean that when we did want to increase the size of a database we would not be able to use the module to size up the replicas, so we ruled that out. We could instruct our engineers to only set allocated_storage on a replica when they wanted to alter it (either through the module or directly using the aws_db_instance resource), but that is something that is easy to forget and we would certainly end up both: triggering the error again by setting the storage during creation having under-provisioned replicas by forgetting to add the allocated_storage parameter to one of the replicas when increasing the storage on the primary We first considered fixing the call to modify-db-instance to just include the IOPS, but this would not work because you cannot modify the allocated storage at all if: The instance is in the storage-optimization state. The storage has been modified within six hours. When a DB is created, the first thing that happens is that it goes into storage-optimization , so the modify call would fail on that count. If the storage optimization was instantaneous, the call would fail because the storage had just been modified (created). We concluded that it was best to submit a fix to the provider which does not try to modify the allocated storage after database creation, since this call simply should not be made. We’re still waiting for the fix to be approved. Managing dozens of PostgreSQL instances using several different versions of the engine and with many different workloads, it’s difficult to make a module that is everything every server needs. We’re on the fourth major revision of our RDS instance module now. We use it in about 50 stacks in our Terraform repository, covering over 100 database instances. When the version needs to be updated, it is a major undertaking to review all the plan changes, particularly if those changes involve adding new resources such as our DataDog monitors. Large plan diffs increase the risk of making unexpected changes. EEven moree so when changes are being made in the console and not backported to the Terraform while at the same time changes are being made to the module. This means that some plan diffs are expected but some would be reversions, and it’s up to us fallible human beings to figure out which is which. Module version updates are even more cumbersome when the inputs to the module change. With minor changes we can just do a find-and-replace on the module version. With API changes we have to go into each stack and update the attributes. In an ideal world, the module would be designed before your Terraform code base is large. The module would include all the necessary features on creation and would change very little. However the world is rarely ideal and sometimes it is important to encapsulate your best practices in a module even when those best practices are still evolving. As a developer who is a fan of Agile development, I prefer to create modules earlier in the process and iterate on them, even at the cost of having to do many updates across all the stacks. Others prefer to push that work off until the code base is fairly static, but this leads to a lot of copy-and-paste which in turn leads to small inconsistencies accumulating with each copy. In either case, you eventually have to make a number of passes over your repo to clean up these undesired deltas, so it’s a judgement call as to which method is best for your situation. I personally would recommend using modules early, even with all their drawbacks. Come back soon to read about: Our struggles with parameter groups That time an abstraction bit us really hard Want to work on challenges like these? Surprise, Instacart is hiring! Check out our current openings . Instacart Engineering 18 18 claps 18 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-04"},
{"website": "InstaCart", "title": "terraforming rds part 1", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/terraforming-rds-part-1-7cc78f92b24d", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart we are approaching a hundred PostgreSQL instances running in AWS RDS. They are all managed in Terraform. Using Terraform with RDS has caused a lot of pain over the years, but it has saved us even more pain. In this series of posts I’ll cover: What RDS and Terraform are Why it can be painful to manage RDS with Terraform Why managing RDS with Terraform is totally worth it I’ve been working on the Infrastructure team at Instacart for two years; before that, I was a back-end developer for our orders team. The Infra team started using Terraform several months before I joined, and we made quite a few newbie mistakes. I started my work with the team and with Terraform by copying a lot of those mistakes 😅. Two years later, I’ve learned a lot. Along with a few other members of the Infra team and several of our engineers, I’ve led the effort to clean up our Terraform and configure thousands of our AWS resources using Terraform, including dozens of RDS Postgres instances and their replicas. In the last year, we’ve also been working on applying our Terraform configuration to multiple environments. This allows us to create development and staging environments that are as similar as possible to our production environment but also adds significantly to the complexity of our Terraform. RDS is AWS’s Relational Database Service. They manage the hardware, the server installation, setup, and other administrative tasks such as version upgrades and backups. RDS is an abstraction layer — you don’t have direct access to the PostgreSQL server code, the logs, or the operating system. You can read more about this in the AWS documentation . AWS adds another abstraction layer in the form of parameter groups. PostgreSQL has many server configuration parameters. When managing your own server you can set these in the postgresql.conf file, on the command line at server startup, or using SQL. With an AWS-managed server, you set these values using a parameter group . I’ll discuss parameter groups a bit more in another post; for now, it’s sufficient to know that this adds an additional layer of abstraction over your PostgreSQL server. Terraform is a tool for building, changing, and versioning infrastructure. It allows you to treat your infrastructure as code . You define your infrastructure using HashiCorp’s HCL language, check these configurations into source control, and then update your resources by using Terraform to plan and apply the changes. So, Terraform also acts as an abstraction layer over your infrastructure. Terraform is not a very strong abstraction, as it makes use of different providers for different services. This also means that it is not agnostic — if you create a database using the AWS provider, you will not be able to simply change your provider to Google Cloud and then apply the same Terraform. However, once you have all of your resources configured through Terraform, you also have a well-organized description of your infrastructure, so you can “port” your infrastructure code to a different provider just as you might port your software to a different language. As you can imagine, you will be configuring many instances of each type of resource, and so you will end up copying your code over and over. Further, you may want to create certain resources together such as a database instance, parameter group, and PGBouncer service. To support these kinds of reuse, Terraform allows you to create modules , which are reusable blocks of Terraform code with inputs and outputs, much like a function in software. I’ll cover more about modules in another post. Modules add yet another layer of abstraction between you and your resources, and since modules can reference other modules, this layer can be as deep as you allow it to be. Okay, I’ve mentioned abstraction half a dozen times now, so it’s probably important to my point… but isn’t abstraction supposed to be a good thing? It is supposed to help us manage complexity and insulate us from the uncomfortable details of implementation. There’s an old fairy tale about a prince searching for a princess. The real princess is identified when she is given a bed piled with dozens of mattresses on top of a single pea. She tosses and turns all night because she can feel the pea through all the mattresses, which would insulate anyone else from the minor perturbation of the pea. Imperfect abstractions act on your systems like the pea on the princess. They can cover up a detail that nonetheless can be quite painful. When you are working on a system with many layers of abstractions, it’s also hard to dig through all those layers to find the pea. The final post of this series will be a case study of where we were bitten hard by one of these abstractions, but first I’ll go over some of the surprises we encountered as we worked on getting our RDS instances into Terraform and managing them for the last couple of years. Using Terraform to create an RDS instance is very easy. Put the following in an HCL file, such as rds.tf : There are many more attributes you could specify, but this is enough to get started. Now run terraform plan . The plan shows what Terraform will create in AWS. Notice all the values that you did not set in rds.tf that are being set or will be “known after apply”. These are supplied either by the Terraform provider (in italics ) or by AWS RDS (plain text). The values you set in your HCL are bold . Now go ahead and apply the plan with terraform apply . Creating this instance took just a few minutes, but most instances that we create at Instacart take 30–40 minutes. Watching the creation on the command line like this is much easier than frequently refreshing the AWS console in the browser. When it shows you the results of the apply, Terraform doesn’t show you all the values that are actually set on your instance, so you’ll need to take a look at what was created either in the AWS console or by using terraform state show . Look at the output of show below and notice how many values were set for us by either the Terraform provider ( italic ) or RDS itself (plain text). Only the values in bold were explicitly set in our rds.tf file: Magic. Some changes to an RDS instance can be applied while it is still running, while others may require a server restart. The aws_db_instance resource allows you to indicate whether or not to do a restart if needed by using the apply_immediately attribute. However, there is nothing in the documentation of the provider which tells you which changes will cause a restart and which will not. The AWS provider docs do warn you that a server restart might cause “some downtime” — in our experience, a database restart can take several minutes, which doesn’t sound so bad until you think what that means to the overall system. Customers may not be able to place orders, they may not even be able to see our website at all, shoppers may not be able to get information about orders to fill, retailers may not be able to send us new catalog data. So, just setting apply_immediately to true and going for it is not really an option. On the other hand, if you do not apply the changes immediately when DO they get applied? According to the documentation, “during the next maintenance window,” but what does that mean? Probably a time in the middle of the night when no one is watching, so what if something goes wrong with the changes? Applying database changes is something you want to carefully control, and the combination of RDS and Terraform makes that quite a bit harder. You can, of course, dig into the AWS documentation to find out which changes can be applied without a server restart. However, suppose you do not read the docs carefully and you try to change the allocated storage in your rds.tf file, with apply_immediately set to false : Nothing here tells you that the changes have not in fact been applied, but if you look at your database in the AWS console, you will see that it is no larger than it was before. Set apply_immediately to true and plan again. Terraform will show the change to the value of apply_immediately , and it will also show that you have not yet actually updated the allocated storage. Note that this apply took a great deal longer than the one that did not actually change your db. Take a look in the AWS console and you will see that your space has been increased. If you run terraform plan one more time, Terraform shows no differences: Okay, so I said at the beginning that using Terraform with RDS is worthwhile. The biggest reason is that Infrastructure as Code (IaC) is a great idea, and Terraform is one of the best tools around for implementing it. So why IaC? Okay, that’s not actually a good reason, but if all the smart experienced people in your field are doing something, it’s likely a good idea to follow them. Without IaC, how do you describe your environment? Probably you make some pictures which will never be up to date, because things are always changing. IaC gives you the ability to write down what you want in your infrastructure and then create those resources directly from that description, so it always matches what is actually running. Since IaC is implemented with text files, it is easy to add source control and a release cycle to your infrastructure, giving you all the same benefits you get from putting your code in source control. I hope I don’t have to explain why source control is good 😬. We can and do use the same Terraform config to provision all of our production, staging, and development environments. We can also use it to tear down and recreate those environments. Using modules and also cut-and-paste (I know, but we all do it), you can apply your best practices to all resources or groups of resources of the same type much more easily than you can by using the AWS console. Since Terraform is “just code”, you can put it into a GitHub repo and get all the benefits of source control and code reviews. Terraform works with many providers , not just AWS, so you can manage most aspects of your infrastructure with it. For example, we use the DataDog and GitHub providers in addition to the AWS provider, and we have written our own provider to integrate with our service management and CI/CD system. Terraform is quite powerful, but the layers of abstraction mean that you can easily do things that you do not intend to do, especially when one of the layers will“helpfully” do it for you. While Terraform 0.12 has improved support for interpolation, conditional configuration, and mapping over collections, it is still not as full-featured as a programming language, which can lead to some very tortured-looking config that is hard to read. Unless you completely lock down CLI and console changes, you can end up with unexpected differences in your plans due to changes made outside of Terraform. This makes it difficult for both the person updating the HCL and the reviewer, and can lead to unexpected reversions. I barely touched on Terraform state management here, but when you need to import existing resources or you decide to move config around, you have to modify the state that Terraform has stored. Modifying the state is a very manual (and tedious) process, as you have to import, move, or delete resources one by one. Do it. Absolutely. Having our resources configured in a source code repo has saved us an enormous amount of time both spinning up new environments and recovering from errors. It allows us to review infrastructure changes before actually making them, which has let us catch many issues before they go live. It can, of course, introduce other sources of error due to the layers of abstraction, but the net result is hugely positive. I like it. There are other options , but Terraform is supported by a very good company, HashiCorp , and has an excellent community and a wide range of well-maintained providers for all of our infrastructure. I like RDS too. Some of our engineers argue that we should self-host our Postgres instances, but as one of the people who is responsible for system configuration, updates, backups, and other maintenance work on our databases, I’m happy to have that taken off my plate. Subsequent posts will cover: Using parameter groups and Terraform modules That time an abstraction bit us really hard Want to work on challenges like these? Surprise, Instacart is hiring! Check out our current openings . Instacart Engineering 362 3 Infrastructure Terraform AWS Engineering Software Development 362 claps 362 3 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-05"},
{"website": "InstaCart", "title": "building instacart meals", "author": ["Instacart"], "link": "https://tech.instacart.com/building-instacart-meals-f1d9f4750a87", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Neera Chatterjee Today we launched Instacart Meals — a new grocery meals product that powers easy ordering, delivery and pickup of made-to-order food. Right now, Instacart Meals is live as a pilot at select Publix stores in the Orlando area. It will be rolling out to Publix locations across Florida in the coming weeks and to nearly all Publix stores across the Southeast in the months ahead. With Meals, customers can now build their perfect custom sandwich right in the Instacart app. To make this happen, we implemented a number of technical changes to our existing apps and infrastructure. Some of the largest technical lifts involved restructuring our grocery catalog and updating our fulfillment system to accommodate food preparation. For customers to truly customize their sandwich order, we had to build an entirely new functionality into our catalog: the configurable item . Configurable item listings allow customers to append an item (for example, a sandwich) with variable sub-attributes (meat, cheese, mayo, etc.). On the backend, we had to alter our catalog’s structure to enable item listings to be “changeable,” spending months updating our data structures. Now, when customers opt for a “whole” vs. a “half” sandwich, they can see the listing change with updated nutritional information the minute they toggle an attribute on or off. Once the customer finishes designing their perfect sandwich, Instacart Meals automatically offers applicable chip and drink combo options and discounts, enabling customers to capitalize on deals and savings just as they would in-store. While it seems simple, adding changeable listings and meal combos increases the complexity of our catalog by orders of magnitude. We have the largest digital grocery catalog in the world, and when you add configurable items and combo meal options at any retailer, you can quickly get hundreds of millions of combo meal permutations…we did the math! On the fulfillment side, we had to tackle a problem that comes with delivering prepared food — how do we get a cartful of groceries along with a fresh chicken tender sub with mayonnaise to a customer and ensure the bread is still toasty and the lettuce is still crisp? To solve for this, Instacart Meals is designed to integrate directly with existing order management systems (OMS), allowing store employees to receive sandwich orders through a familiar interface. When a customer is ready to check out, we make an initial “handshake” with the retailer’s OMS, fetching a selection of preparation windows from the deli counter. After the order is placed on the customer’s end, we reserve a narrow minute-to-minute preparation window in the OMS, ensuring that it’s as close as possible to the delivery time to ensure freshness. As shoppers are finishing filling their cart, they can simply swing by the counter to pick up the prepared food, driving down wait times in-store and making it easier for customers to get exactly what they want as part of their normal grocery shop. The deli counter is an integral part of every grocery store, and we created this digital deli counter to make sure its just as seamless and accessible online as it is in the store. With this integration, a customer’s perfect meal combo is just a few taps away. Can’t get enough of library science, mobile development, and fulfillment windows? Our Engineering and Product teams are hiring! Check out our current openings . Instacart Engineering 75 Product Management Technology News Mobile App Development Fulfillment Retail Technology 75 claps 75 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-23"},
{"website": "InstaCart", "title": "one grocery catalog to rule them all", "author": ["Instacart"], "link": "https://tech.instacart.com/one-grocery-catalog-to-rule-them-all-8589f5694441", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instacart has the largest grocery catalog in the world , with nearly 5 million unique products, spanning over 25,000 locations across North America. To keep it up to date, we update over 500,000,000 lines of data every day . To set us up for success as we grow, we just welcomed Leho Nigul, an e-commerce veteran, as our first VP of Catalog and Tools. We sat down with Leho to talk about building teams and tech. How’d you get started as an Engineering leader? I started on the technical path at IBM. For a while, I was moving up through technical ranks at the e-commerce division. IBM’s known for middleware, so the e-commerce division was a different animal. Our team was pretty independent and functioned as more a “startup” within a giant company. While at IBM I eventually became a senior technical staff member while also managing some teams. And then my mentor came to me and said “Okay look, you really need to decide — do you want to take the pure technical path towards becoming a Distinguished Engineer? Or do you want to build and manage teams?” I always want to learn and hone new skills, so I went with the management path. After I left IBM, I went to a much smaller Vancouver-based company — Elastic Path — where I built out their product development org from my home base in Toronto. Why do you like building teams in Toronto? I love Toronto — it’s one of the most diverse cities in the world… and it’s a city that’s allowed an immigrant like me to thrive. Then there’s this harder-to-describe quality — there’s not a strong sense of entitlement bubbling up here. Everyone I’ve worked with in Toronto has this humble, collaborative spirit. And I value those qualities so much in the teams I’ve worked on here. Instacart is no exception. What got you excited about building out Instacart’s Catalog and Tools engineering division? Instacart’s catalog is a backbone of an incredibly complex four-sided marketplace. All of the mobile development and infrastructure work we do on the Customer and Fulfillment side would be worthless if the grocery catalog was unreliable. On the Enterprise side, you can onboard incredible retailers to our platform, but if there are hurdles for them to upload their grocery catalog data — if you don’t have a simple integration — it just won’t work, right? So many teams depend on Catalog, so to be successful, you have to really understand not only just how the catalog works, but you have to understand how the catalog works across our tech portfolio. You need to understand what data is most relevant to customers (nutrition info, pricing, imagery, item descriptions, etc.), what data is most relevant for shoppers (availability, in-store location, imagery, etc.) and what data is most relevant for our Care team (pricing, availability, etc). There are so many different dimensions to the catalog, and we need to be able to not only build superb and robust catalog functions but also provide other teams with intuitive and easy tools to use and manage the catalog data. It forces Catalog Engineers to become company experts! What excites you most about the months ahead? Oh and one more thing. I’m a huge cheese person. So on my first day, I was ecstatic to find that all of the conference rooms in our Toronto office were named after cheese. I can’t wait for my next meeting in “Mozzarella”. 🧀🧀 Want to scale our massive catalog with Leho? Our Catalog Engineering and Product teams are hiring! Check out our current openings . Instacart Engineering 83 1 Product Management Technology News Engineering Tooling Management And Leadership 83 claps 83 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-30"},
{"website": "InstaCart", "title": "building instacart pickup", "author": ["Instacart"], "link": "https://tech.instacart.com/building-instacart-pickup-705a406b6350", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Kevin Henrikson I’m driving with my family from San Mateo to Juniper Lake for a camping trip. I need to stock up my cooler on the way, but I want to make sure we get to our campsite before it gets dark — it’s a long drive. I don’t have the time to shop at my regular grocery store, so I open up my Instacart app to see if any store along my route is offering curbside pickup with the new Instacart Pickup product. I pick my store of choice along Interstate 80, fill up my cart, and follow the in-app prompts to my designated pickup parking spot. The experience is powered by a ton of new features we packed into the app including: “Pick Your Pickup” UX that lets customers easily view and select the pickup location most convenient to their route. Smart Storefronts that adapt to location-specific store inventories. Opt-in, location-based alerts that allow customers to let the shopper know when they’re on the way and getting close. Customized navigation and prompts which sends customers to the mapping app of their choice, automatically directing drivers from their current location to the store. As a customer nears a store, we send them prompts in real-time to guide them to the handoff location. Send a Friend or Family Member which lets customers share their order details with friends and family to delegate pickup to another driver. Place the order and send a friend to pick it up! While Pickup and Delivery products share a lot of the same technological DNA, Pickup flips the script of the typical delivery service model, folding the customer into the fulfillment chain’s last mile. When you involve the customer in the fulfillment flow, it presents a bunch of really interesting technical challenges for engineering and product teams. In our Delivery product, we identify a store location to deliver from based on the items a customer puts in their cart. With Pickup, customers choose their exact store — not just the actual retailer but the exact physical location of the store they want to pick up from. This prompted us to build a storefront that changes from location-to-location to reflect store-specific inventories. Our Item Availability model underpins this dynamic storefront. It relies on a combination of historical found/not-found rates, inventory data, and other inputs to predict whether the customer’s preferred items are in stock at any given store location along the customer’s route. In contrast to the passive, one- and two-hour delivery windows available in our Delivery Product, with Pickup, customers participate in a choreographed curbside handoff. Even the smallest hitches and delays are magnified if you’re on the road with a kid in the car seat behind you, trying to get to a campsite before dark. We rely on a host of machine learning models, geolocation technologies, and in-app prompts to nail the second-by-second hand-off. Once a customer selects their store of choice and completes their order, our Drive Time model looks at historical transit times to estimate how long it will take a customer to arrive at the store. We’ve traced rings of geofences around each pickup-eligible store. If a customer with location-based alerts enabled crosses the fence, we’ll adjust the minute-to-minute arrival time. We rely on historical fulfillment data to calculate just how long it can take someone to get in and out of store parking lots — we like to call this the “ Parking Model ”. This is especially useful for pickup customers in crowded urban centers vs. customers in suburban or rural stores with wide-open parking lots. From the data, we can see it takes longer to get through a parking lot on a crowded Sunday in downtown Los Angeles than it does in suburban Orange County, CA. Even though Pickup and Delivery are two different products and experiences, the orders are being fulfilled by the same set of shoppers, and we’ve built an Order Prioritization model to choreograph the hand-off. The model organizes incoming orders in a shopper’s queue based on historical fulfillment data. If a customer crosses a geofence, we will bump the pickup order to the top of a shopper’s queue, alerting the shopper that it’s time to make the handoff. We’ve built the revamped Pickup to scale up in the future — Instacart Pickup is now live in 30 states, and we expect it to be in all 50 states over the next year. We also expect to more than double the number of Pickup stores on the platform this year. Want to help us evolve Instacart Pickup, our fulfillment chain, and last-mile logistics? Our Product and Engineering teams are hiring! Check out our current openings . Instacart Engineering 71 2 On Demand Fulfillment Technology News Machine Learning Mobile App Development 71 claps 71 2 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-14"},
{"website": "InstaCart", "title": "creating a logical replica from a snapshot in rds postgres", "author": ["Christos Christoudias"], "link": "https://tech.instacart.com/creating-a-logical-replica-from-a-snapshot-in-rds-postgres-886d9d2c7343", "abstract": "Eng Open Source Data Science Machine Learning Android iOS In today’s world of data breaches and privacy concerns, every measure you can take to protect your data is another brick in the wall defending you. At Instacart , no matter how confident we feel about our security, we are always evaluating ways to make it even better. A few years ago, Amazon Web Services (AWS) introduced encryption-at-rest for their Relational Database Service (RDS) for Postgres. It is a simple feature to include for new databases, but adding it to existing instances is a challenge. Following AWS’ recommended procedure, we would have to: Take our database offline Take a snapshot Encrypt the snapshot Restore the snapshot For a database with terabytes of storage, following this procedure would mean many hours of downtime. We only have a two hour window between our last deliveries in Hawaii and our first delivery on the East Coast. Absorbing 3 hours of downtime isn’t feasible for a company of Instacart ’s size. A feature like encryption-at-rest is a requirement. Our shoppers and customers deserve it. As we explored our options, we took a long look at logical replication which is available with Postgres 10+. In logical replication, row changes are sent to a replica instead of physical pages. This means that the replica can be a different engine version, on a different filesystem or have a different schema altogether. Creating a logical replica from scratch traditionally means copying all of the data from the primary to the replica. But for an instance that’s a few terabytes large, it would take over a week for the replica to be caught up after being created. Although that’s feasible to work with, that would mean running all schema changes on both instances and adding lots of time to running simple tests and simulations. We wondered if we could use the RDS snapshot/restore process that was already in place and have the restored snapshot resume replication from its restore point. We weren’t able to find any kind of documentation saying it was possible. When we sought feedback from experts at AWS, they told us it was not possible. However one of our core values at Instacart is “Of course, but maybe”, meaning we should never assume anything is impossible or until we prove it is or isn’t ourselves. True to our values, we set out to prove it was possible… or prove it was not. It was. Here is how you do it. On the origin, create a publication and a corresponding replication slot for the logical replica to connect to. Now there is a replication slot with no subscriber. Every change that is happening on your database will queue up in the slot until a replica connects to apply and drain the changes. While the replication slot stores your changes, take a snapshot: 1. Open the AWS console and browse to your instance 2. Select “Take Snapshot” and enter a name for your snapshot 3. Wait a few minutes or hours, depending the size of the changes since the last snapshot In our case, the primary purpose was to create an encrypted instance so we’ll do that here. Do this by simply copying the snapshot and clicking the box that says “Enable Encryption” Once the snapshot is encrypted (if you so desire), restore the snapshot to an instance. Select the basic settings you want like the instance size, IOPS, etc… Once the instance is restored, navigate to it in the AWS console and go to the Logs and Events tab. Scroll down to the logs and page all the way to the end where the most recent log file is. View the most recent log file, and look for a line that includes “ redo done at ”. 019-06-13 03:40:28 UTC::@:[7899]:LOG: redo done at C4A1/7C021F48 The C4A1/7C021F48 represents something called the Log Sequence Number (LSN). This represents the last entry in the Write-Ahead-Log (WAL) that Postgres uses to write transactions. This is the point in time that the Snapshot was restored at. You can confirm that this is the proper current point in time by looking up the current LSN on the restored snapshot with SELECT pg_current_wal_lsn(); You then create a subscription from the restored snapshot to the original database, but don’t enable it yet. So now you have a replication slot that was created at one LSN, and an instance from a snapshot that was taken at a later LSN. You now need to advance the replication slot so its LSN lines up with the restored snapshot. From the destination database, start by getting the “roname”. Use the “roname” and the LSN from the AWS logs to advance the replication slot. You have now advanced the replication slot to line up with the restored snapshot’s current LSN. The final step is to enable the subscription on the destination database. ALTER SUBSCRIPTION logical_replica1 ENABLE; You’ll be able to see the status of the replication slot by querying it from the origin. The confirmed_flush_lsn represents the last LSN that was sent to the replica. The pg_current_wall_lsn represents where the database is now. A lsn_distance of 0 means the replica is caught up. When restoring from a snapshot, the new RDS instance will start all of its reads by hitting S3 so it’ll be slow. This means the LSN distance might actually grow even after you start “draining” the slot. As long as the flushed LSN keeps moving, it’ll eventually catch up. If there are any problems, you’ll see them in the RDS logs of the logical replica. In most cases, you are creating a logical replica to replace your primary database. Promoting it is a matter of a few simple steps: Pause writes on the primary Terminate the subscription from the destination to the origin Reset all sequences in the destination Point all traffic to the new primary The very first time we successfully stood up the logical replica, we felt like it was too good to be true. The first thing we did was make sure there wasn’t any data missing. We double checked it. We triple checked it. We checked it a few other ways. It was all there. We let it run for a few days to see if replication would run into issues. It kept humming along with no issues at all. We repeated the process a 2nd and 3rd time. Each time, it connected flawlessly. Almost a year later, we have successfully stood up over two dozen logical replicas from RDS snapshots. This method is extremely versatile, enabling us to perform multiple upgrades to our databases, including migrating to encryption-at-rest, performing major schema changes, and creating logical shards. Although each promotion of a logical replica feels more and more comfortable, the feeling of elation that it actually worked never gets old🍾. Want to work on challenges like these? Instacart Engineering and Product is hiring! Check out our current openings . Special thanks and credit to Marco Montagna for providing most of the creativity behind this technique. Instacart Engineering 96 4 Thanks to Muffy Barkocy , Instacart , and Muffy Barkocy . Infrastructure AWS Encryption Engineering Postgres 96 claps 96 4 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-10"},
{"website": "InstaCart", "title": "solving the infrastructure equation", "author": ["Instacart"], "link": "https://tech.instacart.com/solving-the-infrastructure-equation-16e6399c421b", "abstract": "Eng Open Source Data Science Machine Learning Android iOS How did you get into Infrastructure Engineering? I started out as a software developer and grew into infrastructure over time. My last few companies have been hyper-growth startups where we’ve needed to introduce SRE engineering and resilience engineering practices quickly. I love that kind of work — you’re building up engineering process, you’re doing recruiting…you’re building up an organization around a great product. Infrastructure teams need to build platforms that make it easier for developers to understand their customer and own the software that they create. You never want to get to a place where ownership becomes so onerous that the whole machine cranks down, and nobody can build anything new because all they’re doing is maintenance and bug fixes. I always work to build a strong DevOps culture into engineering teams, rolling out service ownership and incident response initiatives to help everyone take ownership of what they build. You have to understand your customer and you have to understand what you’re building and wake up when it breaks. That’s just part of the deal. It’s 2020—new decade! How have you seen Infrastructure Engineering change in the last decade? I’ve worked at companies that have grown quickly, but I feel like it’s just the new normal. The next company will be faster. Everyone is moving at the speed of light…and the barriers to entry into the market get lower and lower with more and more services. Now, two people on a laptop can go pretty far, right? This isn’t breaking news, but most people in the infrastructure space now all agree that managed services continue to be the future. A lot of companies have a hard time trusting vendors and so they sometimes create this false security blanket that you get more control if it’s built in-house. Usually, you end up spending a lot more and having a lot more problems. Now that you’ve started what are you going to tackle first? No matter what company I go to, my mission will always be to reduce the cost of service ownership. “Cost” means a few things here — it comes in forms of resource usage in raw costs, but it’s also tied to the cognitive load associated with owning a piece of software or customer experience. Every company can reduce the costs of service ownership so they can own more things with less people and scale responsibly. For small and growing companies this can seem like an unsolvable equation. …but unsolvable problems are the most fun problems for an engineering team to solve, right? Why the jump to Instacart? I’m a builder — I really relish in getting to the bottom of what companies need, building out teams, and honing in on operational excellence. Instacart, in particular, was a really interesting opportunity for me. You have this high-growth company that hits my sweet spot: massive scale, plus a complex product portfolio that spans consumer and enterprise tech. …And then there are the people, right? It doesn’t take you very long to get vibe of the culture at Instacart. Everyone is very collaborative and neighborly. It’s only my first week and I already feel right at home. Want to try and solve this equation with Dusty? Our Engineering team is hiring! Check out our current openings . Instacart Engineering 66 DevOps Infrastructure Engineering Management And Leadership Technology News 66 claps 66 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-29"},
{"website": "InstaCart", "title": "the story behind an instacart order part 3 predicting the shop", "author": ["Instacart"], "link": "https://tech.instacart.com/the-story-behind-an-instacart-order-part-3-predicting-the-shop-d1a3bb9f5d98", "abstract": "Eng Open Source Data Science Machine Learning Android iOS This is the third installment of our “Story Behind an Order” series, where we’re walking you through the technology that surfaces grocery items in our apps, enables storefront browsing, facilitates communication before and during the shop, and powers our last-mile logistics to get dinner to your door in as fast as an hour. If you haven’t already, make sure to check out Part One to see how we gather and normalize our grocery catalog data, and Part Two , which covers the tech behind storefront browsing and buying. In our last installment, we learned about the tech that makes it easy to find and buy the ingredients for your favorite weeknight lemony herbed salmon. In this third installment, we’ll concentrate on the predictive models that help us determine if your Meyer Lemons, Fresh Dill, and Salmon Fillets are in stock. We’re also taking a look at the machine learning model that makes it easier for customers to choose and communicate their preferred item replacements. Our catalog data is the technical foundation of our four-sided marketplace. We have about five million unique products in our catalog and about 950,000,000 product listings in total. Each of these listings has a name, a product ID, and several sub-attributes like departments, aisle numbers, dietary/cuisine tags, and nutritional information. We also rely on historical data, detailing how products behave in the marketplace. This data includes item availability history, the number of times an item has been chosen as a replacement, and more. Together, this data powers many of our machine learning models. Two models in particular work-hand-in-hand to ensure a smooth customer and shopper experience: our Item Availability Model and our Replacement Recommendation Model . We predict the availability of over 500 million listings every 60 minutes. Our Item Availability Model relies on historical retailer availability data, store location, an item’s purchase history, and shopper inputs to predict the likelihood that a particular item in our catalog is or isn’t in stock at any one of nearly 25,000 physical stores. This is really hard to do. We get a sense of an item’s availability about once a day from our retail partners, but as we all know availability can be extremely variable throughout the day. One data drop a day doesn’t give us the hour-by-hour predictions we need to set expectations appropriately…especially for a harder-to-come-by fruit variety like the Meyer Lemon! Some locations may get new shipments from their growers seasonally. And when the fruit is in season, some store locations may only restock lemons in the mornings, while others may be a bit busier and stock produce section multiple times per day. To understand variability throughout the day, we’ve built a model that looks at time-centric data features — notably the time of day and the day of the week that the item has been picked by a shopper in-store — to give each listing an availability score . Here’s a look at how the availability score for Meyer Lemons may change throughout the day: Typically a customer shopping for themselves will replace 7% of the items they’re looking for. Sadly, the Meyer Lemons from our favorite mom and pop retailer received a low availability score. When this happens, our Replacement Recommendation model takes over, firing up a flow in our customer-facing app asking customers to pre-select a replacement as they browse. What makes replacements particularly hard to predict is the vast array of contexts in which our customers purchase items. For example, if we’re hoping to squeeze lemon juice over our salmon to flavor it we can opt for lemon juice as a replacement, but if we’re hoping to garnish the salmon with a lemon slice, we may want an organic lemon. Our current model relies on a combination of categorical data like product names and an item’s replacement history to draw invisible lines between related item listings and stack rank potential replacements. We give historical data like replacement history a lot more weight than categorical data when scoring for a few reasons. Historical data helps us find items that seem like atypical replacements when viewed through the lens of categorical data (aisle, department, name) but are perfectly normal in the human brain. In the case of Meyer Lemons, customers usually pick Meyer Lemon Juice as a replacement which usually doesn’t live in the produce section. By giving more weight in our scoring to historical shopper and customer preferences, we can see that the juice is much more widely preferred, despite products like Ms. Meyer’s Lemon Soap having a strong name match. Should a customer opt not to select a replacement in-app and leave it up to the shopper (which happens approximately half the time) the replacement model will surface a few of the top historical replacements into the app flow for shoppers to pick from. Smoothing out potential roadblocks like out of stock items is critical for the success of our fulfillment chain. These two predictive models have worked hand-in-hand to set availability expectations for one listing’s availability and give us the opportunity to quickly and easily communicate the context of an order to a shopper. Stay tuned for the last installment of this series, where we’ll dig into the tech that guides a shopper through the store and right to your door. Can’t get enough of data science, mobile development, and machine learning? Our Engineering and Product teams are hiring! Check out our current openings . Instacart Engineering 102 Machine Learning Deep Learning Artificial Intelligence Data Science Technology Trends 102 claps 102 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-08"},
{"website": "InstaCart", "title": "exploring images in jetpack compose", "author": ["Colin White"], "link": "https://tech.instacart.com/exploring-images-in-jetpack-compose-c8ba87089c92", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Jetpack Compose was announced at Google IO 2019 and is going to change how we do UI development on Android. I’ve been working a lot with the Android image APIs as I’ve been developing Coil and was curious how it treats images and how the concept of image loaders would fit in. In the current Android UI framework ImageView is the main way to show an image on screen. Images are represented as Drawables , which often (though not always) wrap Bitmaps . Bitmaps are raw, uncompressed pixel data stored in memory. In Jetpack Compose there is no ImageView because there is no View . Instead, views are replaced by Composables which define a composable piece of UI to add to the view hierarchy. Likewise, there are no Drawables . Instead, it’s replaced by Image which is a minimal interface that wraps a NativeImage . At the moment NativeImage is defined as a typealias to Bitmap . Interestingly, NativeImage is prefixed with a commented out expect declaration, which is a Kotlin Multiplatform keyword 🤔. Currently there isn’t any support for animated images, though I’d expect AnimatedImage to be added later. Here are the rough API analogs: View -> Composable ImageView -> DrawImage Drawable -> Image Bitmap -> NativeImage At the moment, there is only one Image creation function, imageResource , which synchronously loads an image from resources. The API is noted as transient and will be replaced with an asynchronous API (likely using Coroutines) in the future. However, if we want to create an Image from a URL, a file, a URI, or another data source we’ll have to write it ourselves for now. Fortunately, we can offload the heavy lifting to an image loading library. The easiest way to accomplish this is to write an effect . Effects are positionally memoized blocks of code that have a return value. They can be called from a Composable and will cause the composition (basically the view hierarchy) to rebuild if its output value is updated. Here’s an image effect implementation backed by Coil: What do these functions do? image(data: Any) is a simple version of image(request: GetRequest) that uses the default options to launch an image request. When image is called as part of a Composable it will emit a null Image and begin asynchronously loading the given data . It will update the image state when it’s successful and Jetpack Compose will re-render the Composable with the updated Image . If the request is in-flight and the Composable is removed from the composition, the request will be automatically cancelled. Cool, now let’s take a look at the JetNews sample app. At the moment it eagerly loads all its resources in MainActivity.onCreate . Using image , we can replace all the eager loading with lazy, non-blocking, asynchronous calls. Additionally, we can replace all the hardcoded resources with URLs! Here’s what PostImage looks like after being converted: Great! We’re done, right? While this will theoretically work (currently Coroutines doesn’t work with the Jetpack Compose compiler), the image function is missing a number of features and can be optimized: Automatic sizing: At the moment, Coil will load the image at its original size (bounded by the size of the display) since it has no way to resolve the size of the parent container. One way to solve this would be to write our own Composable to render the images. However, that’s analogous to writing a custom ImageView which is more restrictive for API consumers. Bitmap pooling : Coil.get prevents recycling the returned drawable’s Bitmap since Coil doesn’t know when it’s safe to return it to the pool. When you load an image into an ImageView Coil knows it’s safe to recycle the Bitmap when either View.onViewDetachedFromWindow occurs, Lifecycle.onDestroy occurs, or another image load request is started on that ImageView . Jetpack Compose provides CommitScope.onDispose as a lifecycle callback to clean up your components and Coil (and other image loaders) will need to treat that as a valid request disposal callback. Most of these issues stem from the clean separation between Compose and the traditional UI framework classes like View and Drawable . That said, separating from those classes is absolutely the right idea since they are tied to the platform, rely on inheritance, and hold a lot of internal state ( View is almost 30k lines long!). Composable and Image aren’t tied to the platform, favour composition over inheritance , and hold minimal to no internal state. Overall I’m extremely excited by the progress on Jetpack Compose and look forward to ensuring Coil works effortlessly with Compose (when it’s ready). Also, if you want to see my fork of the JetNews app with the lazy loading changes, you can find it here . Instacart Engineering 310 Thanks to Kaushik Gopal . Android Kotlin Jetpack Jetpack Compose Kotlin Coroutines 310 claps 310 Written by Android @ Instacart Instacart Engineering Written by Android @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-03"},
{"website": "InstaCart", "title": "managing the unmanageable", "author": ["Instacart"], "link": "https://tech.instacart.com/managing-the-unmanageable-e8bb7c4da5d3", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Orit Shamir Orit is Instacart’s Senior Manager of Technical Program Management. She’s spent her career driving the design, integration, and launch of hardware programs at Apple. Before that, she spent 10+ years at MIT completing as many degrees in EECS as they would let her! Imagine multiple speakers in a circle playing your favorite song, and you standing between them. For the best listening experience, you need to get many details right and to a high degree of accuracy! If you don’t stand exactly at the center of the speakers, ensuring no delay in the song start between them, and with precisely the same volume, your listening experience will be bad; so bad you may not even recognize the song. But if you do get it right, you will hear a beautiful true surround-sound, amplifying and giving new color to the music. Excellence requires deep planning, coordination, and alignment. There is no better way to paint a picture of the value of Technical Program Management, and if you have never worked with a Technical Program Manager, you are missing out. TPMs are a technical swiss army knife. In each TPM’s toolkit, you’ll find a mix of EQ, wisdom, comfort with ambiguity, and (of course) leadership! It’s hard to overemphasize the role EQ and empathy play in leading a program successfully. Challenging an engineer, product manager, designer, or even legal counsel’s outlook when scoping out and managing a major effort is par for the course. To set expectations and align the cross-functional group, you need to challenge positions in a way that proactively offers up solutions, and directly but skillfully addresses the concerns raised. TPMs must master self-awareness, social awareness, self-management, and relationship management to drive their programs, and push team performance forward! Ed Catmull said, “Craft is what we are expected to know; art is the unexpected use of our craft.” For TPMs this is especially true — the strongest program managers are adaptable and know how to apply their skills. They use experiences gained from technical or organizational roles in academia and tech to lead a multitude of complex challenges. At Instacart, we use these skills as we navigate the roll-out plan of an expansion project one day (like Instacart’s Rx delivery pilot!), and guide the development of critical internal platforms on the next. Complex and ambiguous questions are always the most rewarding and engaging to answer! Great TPMs strive to bring order to the chaos by meeting hard challenges head-on, even if the path isn’t obvious. In growth, every stage brings both opportunity and lots of unknowns — how should our infrastructure and systems evolve? What is the best way to future-proof our processes? We manage these open-ended questions and build a narrative and plan around their impact, solution, and implementation. TPM is a highly independent, strategic function. Just as an engineering manager is the stakeholder for their team, and the product manager is the stakeholder for the feature, TPMs are the stakeholder and voice which advocates for the program — whether it be a new product feature, an infra lift-and-shift, or even a compliance initiative. It is up to the TPM to think big and get down into details — to architect the program and define its path from initiation to completion, and to get the stakeholders on board. It’s a tall order and it requires someone who isn’t afraid to blaze their own trail! With my engineering background in EE/Photonics, the move to software has been like drinking from the fire hose in the very best sense. Instacart’s four-sided marketplace — encompassing consumers, shoppers, retailers, and brands — and the products that come with it, present an especially dynamic space for Technical Program Management. There are a ton of opportunities ahead for the business, and I can’t envision a more exciting time to build out and strengthen the TPM function within this engineering organization. If you couldn’t tell, our TPM team is hiring! Check out our current openings . Instacart Engineering 29 Product Development Engineering Technology Product Management Women In Tech 29 claps 29 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-19"},
{"website": "InstaCart", "title": "solving hard problems with soft skills", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/solving-hard-problems-with-soft-skills-19e81a4d07f5", "abstract": "Eng Open Source Data Science Machine Learning Android iOS I’ve been a software engineer since the early 1980s. We called it “computer programmer” then and laughed at people who called themselves “engineer-comma-software” because it sounded pretentious. Through college and all the years since then, the skills which have made me successful have been a combination of the “hard” skills teachable abilities that can be defined and measured and the “soft” skills personal attributes that enable someone to interact effectively and harmoniously with other people Myself, I find “hard” skills easy; I’ve always been a good student and love to read, ask questions, and learn. I find “soft” skills hard; people are scary and don’t always make sense. My first week in college, I walked into the computer lab to do work-study and found my people. I spent the next six years hanging out in computer labs, writing games to play over the network, socializing online, and (eventually) getting a degree in computer science. I fell in love with computers because they allowed me to talk to people both locally and all over the world without being impeded by shyness and social anxiety. My first week at a salaried job, they handed me a big pile of documentation and I came back to tell them it wasn’t very good. I spent the next seven years rewriting the docs, producing training materials, and doing on-site training and consulting, as well as writing a great deal of software. I acted as the interface between the scary-smart founder and the people selling and using the software. I’d walk in one morning and find myself on a plane to the other side of the country to save a deal, because our customers wanted technical people who they could also talk to. When the web came along I started designing web pages. I had basically no design talent, but I loved feeling close to and being able to respond quickly to the needs of the people using my work. After a while, talented designers and UX people took over Web interfaces and I moved to the API layer, but I also became the person all the designers, PMs, and engineers came to to help facilitate communication in the days before TV ads talked about processors and gigabytes and high speed networks. I could talk tech to non-technical people with an understanding of their needs and without talking down to them. I worked at small companies and had days where I couldn’t sit at my desk because everyone was calling me to help with something. I worked at large companies and made trouble 😁 by speaking out about things I saw that I believed were wrong. I did more training, more documentation, more listening to people, more talking. I enjoyed helping bridge the communication gap between tech people and others in my companies as much as I enjoyed developing software. I worked at a company writing support tools for customer support agents who had been conditioned to fear the engineers. I baked cookies and cakes and pies weekly and invited both groups to socialize over food. How can a team of engineers do good work for a customer they can’t talk to and don’t understand? Eventually the two groups grew closer and the teams and tools were better for it. Then I came to work at Instacart as a full-stack engineer. Over time my team and role evolved and I was doing mostly back end work. I also started an internal code school to teach non-engineers how to be full-stack engineers. I’d done a fair amount of teaching before, and I understood my subject, but spending six months creating engineers out of people with little or no previous experience was a major undertaking — and a huge win. Where before I was one engineer helping make other engineers a little better, now I had actually created new engineers with diverse backgrounds who could join our team and bring in new perspectives. It was (and is, we’re on year three now) the most rewarding and the most fun part of my career. Meanwhile our database started growing like crazy and I joined the team working to scale it with no downtime. At the end of the database project, our Infrastructure team was looking for more people. I’d been working with some of them on the database project and infra is an area I hadn’t done a lot in, so I asked to join. I was immediately underwater! Doing a little build and deploy, a little database, a little this and that on AWS as needed for a small company is nothing like working on the infrastructure for a large one. My manager reassured me that he knew I had a lot to learn, but he valued my ability to talk to people, my understanding of the engineering team, and my customer-focused attitude. I used to think that Infra was just about making machines talk to each other, and while that is interesting, it didn’t excite me. Now I find that it is about helping and supporting engineers so they can get their jobs done, which in turn helps all the people who use our site and our apps. It is about solving problems, communicating, and working with our internal customer, the engineers. This makes me so happy — they’re still my people. After two years, I’ve just gotten started learning about Infra, but I have found once again that no matter what part of the system I’m working in, the most important skills to have when working on software are the soft skills . The ability to listen, to learn, to communicate, to care. The desire to solve problems for people. The experience to know that what is right is not always practical, and vice versa, and to make balanced decisions. Sure, I want to sit in a corner and code uninterrupted for hours at a time, but I also want to sit and talk to people and find out how I can make their lives better with my work. I want to answer questions and help people get their work done. I want to keep speaking up and making sure we’re doing the right things for the right reasons. And I want to be on a team who feels the same way — and I am. Infra is one of the nicest, most supportive, and smartest teams I’ve ever been on, and I’ve been on a lot of great teams. In my career I’ve moved all around the system, switching from front end to API to back end to database, and now to systems/Infra. This requires constantly learning new hard skills, which is fun and satisfying, but it’s the soft skills I was raised with and have continued to work on all my life that I use every day and it’s the soft skills that make people I have worked with before eager to work with me again. Want to join me on our Infrastructure team or be one of our wonderful engineering customers? Check out our current openings . Instacart Engineering 260 2 Thanks to Instacart . Software Development Soft Skills Engineering Infrastructure 260 claps 260 2 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-21"},
{"website": "InstaCart", "title": "the story behind an instacart order part 2 browsing and ordering", "author": ["Instacart"], "link": "https://tech.instacart.com/the-story-behind-an-instacart-order-part-2-browsing-and-ordering-6b6a6066a706", "abstract": "Eng Open Source Data Science Machine Learning Android iOS This is the second installment of our “Story Behind an Order” series, where we’re walking you through the technology that surfaces grocery items in our apps, enables storefront browsing, guides the shopper through checkout, and powers our last-mile logistics to get dinner to your door in as fast as an hour. Before you read this post, make sure to check out Part One to see how we gather and normalize our grocery catalog data. In our last installment, we learned about your last-minute dinner plans — weeknight lemony herbed salmon — and dove into how we get the Salmon Fillet, Meyer Lemons, and Dill item listings into our general grocery catalog. In this second installment, we’ll dive into the tech that allows you to find those ingredients, add them to your cart, and check out with the tap of a button. On average a user will make 20 searches per basket before they check out. We built Instacart to give time back to our customers and our search team works to help customers get the most out of those 20 searches. Problem is: making our 500,000,000 item listings across 20,000 locations easily browsable is really hard to do well . If you add “dill” into the search bar, we need to determine whether you want fresh dill, dried dill, or dill pickles, and rank those suggestions in the order we think is most relevant to you. To do this, we rely on Elasticsearch , and a seven-year backlog of catalog and purchase data. Here’s the search framework we can apply to this particular query: 1.) Determining user intent The first thing we need to do is identify intent — based on the search queries, is the searcher using the app to browse options and get inspired or are they here to buy a specific item they have in mind? If a user types “herbs”, for example, we can assume that they are exploring different herbs to top their dish, whereas if they type “organic fresh dill”, the intent is much clearer. 2. Determining product modifiers In every search query we get a core product — in this case, it’s “dill”. In this next layer of search, we look to see if a customer has added product modifiers , which usually correlate with stronger intent. In this example, “organic” and “fresh” are product modifiers. Other strong product modifiers, like specific brands or departments, can help us better organize results. 3. Matching to the catalog When a retailer enters their inventory data, they usually organize it by department. If they don’t, we add it to a pre-set list of standardized departments. Once we’ve used our product modifiers to determine what we think is the right intent, we rely on Elasticsearch as our query-to-products matchmaker, as we sift through the catalog database to find relevant products. 4. Ranking the results Lastly, we need to determine which product(s) customers are most likely to purchase and rank them “above the fold”. When surfacing the results in-app, we look to a combination of keyword relevance, historical purchase data (which item is added to baskets most?) to build out the ideal ranking. As more and more items are added to more and more baskets over time, we get a finer-grained understanding of what products are most relevant to each query. When you make your search for salmon and Meyer lemons, we’ll use this same process of determining intent, querying the catalog, and ranking results to filter out salmon cat food and lemon-scented dish soap! So you’ve landed on the search results that match your intent. Before you put the item into your basket, you start to browse by price. In every basket, we need to evaluate logic provided by the customer profile, retailers, and CPG partners in order to apply coupons and deals to the item and basket totals in real-time. Across our marketplace, we can be running over 10,000,000 deals or discounts that can affect total basket cost. To present listing prices in our app, we take into account various contextual details tied to the customer, the amount of items in the basket, the retailer, and the specific product brands in the basket. A ton of different factors can affect the price of the salmon you see on the storefront and in your larger basket total at check out: Listing-specific deals : CPG and brand partners can offer coupons or sales for specific items on the platform Loyalty club discounts : certain retailers offer loyalty club pricing to customers who enter their club member number into their Instacart user profile. Location-based deals : Retailers often set location- or region-specific sales or coupons. Delivery discounts : Partners can sponsor free delivery for first-time orderers, they can also sponsor the waiving of delivery fees if a customer puts certain products in their basket. Bundled item discounts : Brands can offer a certain dollar amount off of your total basket price if you choose to buy a number of their products at once — ten dollars off your total purchase, should you buy 5 products by O Organics, for example. Another common bundled deal is the classic 10 items for $10 sale you usually see in the yogurt aisle. Instacart Express membership : delivery fees are waived on orders over a certain dollar threshold if the orderer is a member of Instacart Express. Gift cards and credits : dollar amounts added or issued to individual customer profiles. In order to make sure your basket totals take all of these deals and details into account, we use Elasticsearch to fetch our base product attributes from the catalog (i.e.: simple things like item name, image, and the listings set price), and use a set of simple Ruby logic to apply price transformations to the items you’re browsing and adding to your cart. Once the transformations are applied, we get an adjusted item cost that gets factored into your basket totals. As all of this is going on, we’re pulling in other contextual information associated with your account, like loyalty cards, express memberships, first-time ordering, and order credits, to apply to your basket totals in real-time. Once we’re confident that the salmon fillet, fresh dill, and Meyer lemon items in cart have undergone the appropriate transformations, and you click ‘place order’, we persist your order information, which will be used by our Fulfilment team to batch your order to shoppers. As part of this order creation we confirm all of our totals calculations, determine the total charge and create a payment authorization via Stripe ®, our payments processor. Up next in the story of this order? Fulfillment. Stay tuned for the next installment of this series, where we’ll dig into the tech that helps shoppers select your order in-store, and the logistics system that navigates orders to your door. Can’t get enough of Search, Machine Learning, and Dynamic Basket Pricing 🛒? Our Engineering and Product teams are hiring! Check out our current openings . Instacart Engineering 159 1 Ecommerce Payments Engineering Ruby Elasticsearch 159 claps 159 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-21"},
{"website": "InstaCart", "title": "lessons a coding boot camp can and cant teach you", "author": ["Instacart"], "link": "https://tech.instacart.com/lessons-a-coding-boot-camp-can-and-cant-teach-you-494423cf62be", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We created Carrot U , Instacart’s very own coding boot camp, to offer high-performing employees in non-technical roles the chance to learn the engineering basics. At the end of the program, Carrot U graduates get the chance to interview for open engineering roles. In prep for our 2019/2020 Carrot U cycle, we sat down with Sveta and Michael — two of last year’s Carrot U grads who made the jump over to our software engineering team — to talk about the lessons you learn in a coding boot camp, and discuss the lessons that only on-the-job experience can give you. Sveta: When I first joined Instacart, I was on the team that helps onboard new retailers onto our platform. I liaised between our engineering team and our retailers, explaining technical concepts to our retailers and taking their needs back to our engineers. Michael: I’ve had a long journey here at Instacart. I started out as a full-service shopper in 2014. Later, I applied for a role on the customer service team, handling calls and support requests from customers. On the customer support team I found myself doing a bit of QA work, and eventually hopped over to the Logistics Operations team, which handled things like storm management (i.e; if a Hurricane about to hit Florida, what delivery zip codes are going to be impacted? What retailers would be affected?) When I joined Logistics Ops I didn’t even know how to do basic equations in Excel. I had to teach myself Excel and started to learn SQL queries — a team member on set up some basic “learn SQL” lessons for a group of us. Sveta : Yeah, I had dabbled with SQL in my role, too. Looking back, writing simple SQL queries is a really easy way to kickstart technical training. It made becoming an engineer a bit closer in my mind than I initially thought. Sveta : I was always interested in engineering but I felt like I never had the means or the time to go out and do a boot camp. While I learned how to make SQL queries, I never thought I would actually make that jump to engineering. It was actually one of my coworkers here, Muffy , who encouraged me to apply to Carrot U. I met Muffy within my first month here — she’s a big proponent of women in STEM. A few months later, when the applications opened up, I signed up. Michael: I wasn’t even really all that interested in applying at the start — I liked my position! I got interested when a couple of my coworkers pointed out — “Hey, you’re the go-to person on Logistics Ops for SQL. You’ve taught yourself this within the last eight months, you’d be a perfect fit. Are you going to apply for this?”. That’s when I applied. Sveta: To be honest, when I first started it was terrifying. I thought to myself, “I don’t know how I’m going to learn any of this so quickly.” But throughout the program, I learned to rely on mentors for support. A former Carrot U grad, Jeremy, served as my formal mentor through the program. I always went to office hours with other program mentors, Dan and Viktor. Muffy was always there for me, too, serving as an informal mentor on the side. I could always come to any of them with questions. Michael : The final projects were fun. Once we had learned the basic languages and gone over some of the basic comp sci theory, we had to build an application with a team. My group spent a month essentially building a phone book application for Instacart where you can look up employees on different teams and you can find their bio and award virtual badges to them. Sveta: Mentorship was so important for me and other folks throughout the program, so I worked on a group project with two other people where we built an app that matches up mentors and mentees internally. If you were interested in strengthening a specific skill set (like Ruby or Python) you could go on to the app, sign up, and list skills you were interested in learning. Experts at the company in certain languages or skills can sign up on the other side to be a mentor. Sveta : Learning to work with the legacy code was a huge one. Michael: Yeah, anybody can figure out the languages with a little work. Anybody can figure out how to write a complex Ruby method or write some fancy logic (seriously). But being able to actually work with legacy code…that’s where it gets difficult. Working with a shared codebase in Github was definitely challenging at first, but now it’s second nature. Sveta: When you’re working on your final group project, you’re basically building everything from scratch. I found that building from the ground-up to be a little easier — you’re building from what you know and debugging. With legacy code, you really have to take the time to understand what every little change is doing — where it might radiate out and affect other things. I now understand why so many engineers sometimes just need to be “heads down” and why there have these buffer periods during sprints — it’s because if you run into those blockers and it’s going to take time to understand that legacy code and fix any of those issues within it. There’s a lot of ambiguity that you have to account for. Michael : Another big surprise for me was how much of engineering is debugging. The majority of my time is spent debugging and figuring out how to make my local environment behave as it should. You don’t spend as much time as you’d expect actually building something — maybe about 30 to 45 minutes. After that, you spend the rest of your day testing it and debugging the things that aren’t working the way you want them to. Michael: After graduating Carrot U and jumping into my new role, I was really surprised by how ambiguous the problems you solve as an engineer can be. I might have three tasks for the week…and I sit down on Monday morning and I look at those three tasks and say: “You’re kidding me. It should take me five hours to do these three things.” The first task takes me 30 minutes, the second task takes me 15, and then get to the third and I dive into the code base and it may take me three months to complete. Sveta : Yeah I learned really quickly how to work when there isn’t a clear answer or even a clear path to an answer. It was really hard at first — I was a bit terrified, but I got used to it pretty quickly. Getting comfortable with ambiguity helps you think in a different way. As you learn to deal with that ambiguity, you’re just building up your engineering skillset. Want to join Sveta and Michael on our Engineering team? Check out our current openings . Instacart Engineering 125 Programming Software Engineering Bootcamp Frontend Backend 125 claps 125 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-01"},
{"website": "InstaCart", "title": "what the formula managing state on android", "author": ["Laimonas Turauskas"], "link": "https://tech.instacart.com/what-the-formula-managing-state-on-android-f5569ce09274", "abstract": "Eng Open Source Data Science Machine Learning Android iOS State management is usually the most complex part of a UI application. At Instacart, we have been using RxJava to manage state for a few years now. As our Android team grew, we realized that there was a steep learning curve for new developers to get started. We started to experiment with how we could reduce complexity. After a few iterations, we are excited to announce Formula — an open-source state management library built in Kotlin. Formula provides a simple, declarative, and composable API for managing your app complexity. The goal is to express the essence of your program without much ceremony. Probably the easiest way to show you how Formula works is to build a simple stopwatch application. It will show how long the stopwatch has been running and allow the user to start/stop or reset it. When working with Formula, the recommended way to start is by defining what the UI needs to display and what actions the user will be able to take. We model this with Kotlin data classes and call this concept a RenderModel . We model user actions as event listeners on the render model. Render model is an immutable representation of your view. Any time we want to update UI, we will create a new instance and pass it to the RenderView . Render view is responsible for taking a render model and applying it to Android views. With rendering logic out of the way, let’s see how we actually create the render model. Render model creation is the responsibility of the Formula interface. Let’s create a StopwatchFormula class that extends it. Formula interface takes three generic parameters: Input, State and Render Model (we will not use input in this example). In Formula, we keep all our dynamic properties within a single Kotlin data class which we call State . For the time being, we will use an empty State object as a placeholder. The evaluate function takes the current state and is responsible for creating the render model. Our current implementation always returns the same render model. Before we make it dynamic, let’s connect this implementation to our render view. There is an extension function start that creates an RxJava observable from the Formula interface. For simplicity sake, I’ve placed this logic directly in the activity. In a real application, it should be placed within a surface that survives configuration changes such as AndroidX ViewModel or formula-android module. Now that we are observing render model changes, let’s start making the UI dynamic. For example, when the user clicks the “Start” button, we want to reflect this change and update the button to show “Stop”. We need a dynamic property to keep track if the stopwatch was started. Let’s update our State to include isRunning . We initially set isRunning to false . If you have worked with Redux or MVI, this should look familiar to you. Let’s update the start/stop button render model creation. Any time user clicks this button, we want to toggle isRunning and create a new render model. We will use FormulaContext to accomplish this. FormulaContext is a special object passed by the runtime that allows us to create callbacks and define transitions to a new state. Any time onSelected is called we transition to a state where isRunning is inverted (if it was true it becomes false and vice-versa). Instead of mutating the state, we use the data class copy method to create a new instance. A transition will cause evaluate to be called again with the new state, new render model will be created, the Observable will emit the new value and our UI will be updated. Now that we have a functioning button, we actually need to run the stopwatch and update the UI as time passes. Let’s add a new property to our state class. The actual implementation to create a display value from milliseconds is a bit complex, so let’s just append ms to time passed. To update timePassedInMillis we will use the RxJava interval operator. When the stopwatch is running, we want to listen to this observable and update timePassedInMillis on each event. To add this behavior we need to update evaluate function. As part of each state change, Formula can decide what services it wants to run and listen to. We update our Evaluation and use conditional logic within context.updates block to declare when we want our observable to run. Starting the stopwatch now updates the time passed label. The logic here is very simple: when isRunning is true , listen to incrementTimePassedEvents and increment timePassedInMillis . The updates mechanism is agnostic to RxJava so you need to wrap it using RxStream . The event callback is very similar to UI event callbacks except it’s already scoped so you don’t need to use context.callback . We still need to handle the reset button clicks. This is the same as implementing the start/stop button. There are no new concepts to introduce here. Any time reset button is selected we transition to a new state where timePassedInMillis is set to 0 and isRunning is set to false . And we are done. You can find the source code here . There are still a lot of things that we didn’t go over such as composition, testing or side-effects. If you want to learn more, take a look at the documentation , samples or the Github repository . If you have any questions or feedback you can find me on Twitter . Interested in projects like Formula? Check out Instacart’s current engineering openings here . Instacart Engineering 411 4 Thanks to Kaushik Gopal , François Blavoet , and Charles Durham . Android Functional Programming Redux Rxjava Open Source 411 claps 411 4 Written by Android, techno, politics in no particular order Instacart Engineering Written by Android, techno, politics in no particular order Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-08"},
{"website": "InstaCart", "title": "redesigning how we eat", "author": ["Instacart"], "link": "https://tech.instacart.com/redesigning-how-we-eat-ce828fcec9f6", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Ryan Scott Tandy, our new Director of Product Design just joined to lead the team that’s designing the app millions of our customers use to browse and order groceries. Mobile’s nothing new for RST — he’s designed experiences for feature phones, iPhones, cameras and everything in between, with stints at Adobe, Apple, R/GA, and Instagram. We sat down with him to talk about the future of food, the power of simple design, and (of course) what’s in his cart. You’ve had a long career spanning startups, agencies, and big corporations — what are your learnings from each phase of it? I’ve been designing for a long time. I got my start in the dot-com 1.0 era! I took a pause from college when I was 20 to work for a digital sports streaming startup that was pretty ahead of its time, given that the technology and bandwidth just weren’t where they are nowadays. Back in the late 90s, we tracked sailing races around the world and live-streamed a mountaineering ascent ! From early on, I was fascinated by the stories that could be told by design and using technology to create memorable digital experiences. Later, I found myself at Adobe’s Experience Design team, where we thought a lot about reimagining experiences for mobile phones — making things like messaging apps and online banking work on those tiny screens with four-way keypads. Yes, this was in the pre-smartphone era. It was awesome to work on something millions of people use every day — a tool that people rely on. It was the iPhone that later drew me to Apple where I began to focus on designing consumer products, working primarily on mail, as part of the redesign of MobileMe (predecessor to iCloud). It was one of the least flashy things in Apple’s entire lineup, but from a design perspective, it was awesome to work on something millions of people use every day — a tool that people rely on. For the last 3 years I led design on the Sharing team at Instagram, which spanned a range of new product initiatives like Stories, Live, Direct, and Profile that focused on self-expression and communication. I joined Instagram at a time when things were still very simple, there was only one way to share and the entire company fit inside a small room at Facebook. It was awesome to be part of that chapter that really brought the service several jumps forward making Instagram a place for you to share something every day with your closest friends. What project are you most proud of? While I was at R/GA I was lucky enough to lead the design team behind the Nike+ running app. We assembled an interaction design, visual design and copywriting triage team, and devoted 100% of our time to the app. We designed it, built it, ran with it, deployed it, and then iterated on it again. It was a truly hands-on and iterative process, which is so rare in an agency setting. What I’m most proud of is how my team stayed ahead of the technology, not just building web and mobile, but building for newer (at the time) wrist-based wearables, smart-watches, and connected retail. I’m very interested in health and fitness, and it was so exciting to build something that empowers everyday athletes to achieve more and live better. What brought you over to Instacart? A well-designed product or service can be healing — it can give you control over your time and your physical health and I think that Instacart has the power to do both of those things. I’m excited for the team to evolve it from a simple utility to a service that people turn to for inspiration Currently, Instacart’s a tool that has given me and my family a lot of time back into our day. Right now, it plugs easily into that one moment of our day when we realize something’s missing and we need to order groceries. I like the fact that you can shop at Costco, CVS, and Safeway in 15 minutes on the app — something that would take me half a day to complete otherwise. …but there’s a lot more potential locked away in the app. I’m excited for the team to evolve it from a simple utility to a service that people turn to for inspiration around what we eat and how we shop. Aside from ordering groceries, there are so many other food-related moments in our day that Instacart could plug into: discovering new brands and products, exploring new recipes, meal planning for that new keto diet, planning a dinner party, and more. I’m excited to simplify and perfect how people order groceries… and I’m really excited to explore how we can bring those other moments of food inspiration and discovery to life in the product. On top of that, Instacart’s on its way to becoming a household name — we’re in growth mode . In my first month here, I fell in love with being hands-on again, the rapid pace, the cross-functional teams, and the friendly, collaborative people . What’s always in your cart? Doughnuts. Always doughnuts. 🍩🍩🍩🍩🍩🍩 Want to design products with RST ? Instacart Design is hiring! Check out our current openings . Instacart Engineering 132 Design Product Design Mobile Design UX Technology And Design 132 claps 132 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-19"},
{"website": "InstaCart", "title": "life is harder than product", "author": ["Instacart"], "link": "https://tech.instacart.com/life-is-harder-than-product-b7e027ec518f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Shereen Idnani We hosted Women in Product at our San Francisco office to talk about how women in Product can prioritize what matters to them in just the same way as they implement ruthless prioritization for their products. Aye Moah, founder and CEO of Boomerang gave an incredible keynote, and we hosted an impressive line up of product leaders for a roundtable discussion about how they work to prioritize themselves as they balance products, their careers, their families, and their happiness. Here are the top 3 takeaways from the night: Aye Moah kicked off the night with a keynote centered on a simple, but powerful act: raising your hand. Studies show that female applicants are less likely to apply for a job if they don’t fulfill 100% or the qualifications or the requirements. Moah’s advice? Don’t wait for 100% — when you see an opportunity, raise your hand! Earlier in her career, Moah wanted to transfer internally to a UX role, but she didn’t have a portfolio or the design background needed to make the jump. After getting a quick “no”, she started volunteering for projects that involved the UX design team, built out a portfolio in her free time, and eventually secured the role! If you raise your hand, hopefully, someone will be there to see it. Moah closed the keynote with a powerful call to action: be willing to take chances on people. Look for potential and make yourself available for mentorship. Panelist Lesley Kim Grossblatt detailed how, during her transition into Product, there was always something she didn’t have that gave people a reason to tell her “no”. There’s always something — Back in the 90’s everyone had to have an MBA to be a product manager…nowadays the profile has changed. You have to have an engineering background. People wait for others to invite them into professional opportunities. We expect a red carpet to be rolled out for a new role or promotion, but too many times I’ve had to shove the door open for opportunities. Leslie’s advice? Ignore the “buts”, learn what your superpower is, cultivate it, and lean into it. Prioritize for the role you want. You’re not going to be perfect (nobody is!) so you need to create your own opportunities. In the case of Sharmeen Chapp, when she didn’t get the internal transfer/promotion she interviewed for, she was encouraged to get out of her comfort zone and push back and build out a business case arguing for her in the role. Lo and behold, the business case proved to be the right tactic and she got the role. Sharmeen rolled out her own red carpet! A lot of people think they want to follow a certain career path because they’re supposed to. The panel discussed that before you set your career path, you need to understand what your needs are — you’re not going to be happy and thriving if you’re doing what somebody else’s vision for your career. Take some time to prioritize your happiness as you chart your next move. Develop criteria around what makes you happy, and intentionally look for opportunities that meet those criteria. This part of the discussion resonated with me. Earlier in my career. I was offered a director position, but at home, I had ( really young) kids and devoting additional time in the office was not what was going to make me happy. I didn’t accept the role — I liked what I was doing! Now I’m at a point in my life where building out a team and nurturing talent makes me happy… and I’m excited to lead the new Product Operations team here at Instacart. I’m a huge fan of Women in Product. I got my job at Instacart through attending the 2018 Women in Product conference where the theme of the conference was “Breakthrough”. I was so motivated and taken by the conference and knew it was my time for a career breakthrough which is what lead me to pursue a role in Product Leadership. It’s an incredible organization that empowers women across all levels of product. Get involved with the community! Instacart Engineering 6 Product Management Product Development Product Design Women In Tech Women In Product 6 claps 6 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-13"},
{"website": "InstaCart", "title": "introducing coil kotlin first image loading on android", "author": ["Colin White"], "link": "https://tech.instacart.com/introducing-coil-kotlin-first-image-loading-on-android-f0fdc7a2a99e", "abstract": "Eng Open Source Data Science Machine Learning Android iOS I’m excited to announce Coil — an open-source Kotlin-first image loading library for Android. Coil is fast, lightweight, modern, and treats Kotlin Coroutines, OkHttp, Okio, and AndroidX Lifecycles as first-class citizens. Coil is an acronym for Co routine I mage L oader. With hundreds of millions of catalog listings, we load a lot of images at Instacart. On the Android team, our choice of image loading library is one of the most important parts of our app. Here’s what Coil looks like: While there are several image loading library options for Android developers today, we felt there was an opportunity to create a more modern, simpler product. Coil was created with the following goals: Leverage Kotlin language features including extension functions, inlining, lambda params, and sealed classes to create a simple, elegant API. Leverage Kotlin Coroutines as they offer strong support for non-blocking asynchronous computation and work cancellation while maximizing thread reuse. Use modern dependencies . Square’s OkHttp and Okio are standard dependencies for every Android app. They’re efficient by default and allow Coil to avoid reimplementing disk caching and stream buffering. Similarly, AndroidX Lifecycles are now the recommended way to track lifecycle state; Coil is the only image loading library that supports them. Lightweight . Coil has almost 8x fewer lines of code than Glide and slightly less than Picasso. Also, Coil adds ~1500 methods to your APK (for apps that already use OkHttp and Coroutines), which is comparable to Picasso and significantly less than Glide and Fresco. Support extensions . Coil’s image pipeline is composed of three main classes Mappers , Fetchers , and Decoders . These interfaces can be used to augment and/or override the base behavior and add support for new file types in Coil. Improve testing. Coil’s main service class, ImageLoader , is an interface. This allows you to write fake ImageLoader implementations for your tests. Coil also supports dependency injection, as it provides singleton and non-singleton artifacts. Avoid annotation processing , which can often slow down build speeds. Coil relies on Kotlin extension functions instead. We’ve worked hard to make sure Coil covers the existing functionality supported by other image loaders, but we also wanted to throw in an additional unique feature: Suppose you have an image that is 500x500 on disk, but is being loaded into an ImageView that is 100x100. Coil will load the image into memory at 100x100. However, what happens now if you need the image at 500x500? There’s still more “quality” to read from disk, but the image is already loaded into memory at 100x100. Ideally, we would use the 100x100 image as a placeholder while we read the image from disk at 500x500. This is exactly what Coil does and Coil handles this process automatically for all BitmapDrawables (and soon all Drawables ). Paired with a crossfade animation, this can create a pleasant visual effect where the image detail appears to fade in, similar to a progressive JPEG . The placeholder is also set synchronously on the main thread, which prevents white flashes where the ImageView is empty for one frame. We’re extremely excited to be sharing Coil with the community. There’s much more to talk about that we couldn’t fit into this post so check out the documentation and the Github repository . You can also find me on Twitter here . We’re currently using Coil at Instacart and we recommend you give the library a try as well. We’ll also be releasing a benchmarking repository soon, which compares Coil’s runtime performance against other image loading libraries. Interested in projects like Coil? Check out Instacart’s current engineering openings here . Instacart Engineering 3.7K 7 Thanks to Kaushik Gopal . Android Kotlin Kotlin Coroutines Okhttp Open Source 3.7K claps 3.7K 7 Written by Android @ Instacart Instacart Engineering Written by Android @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-12"},
{"website": "InstaCart", "title": "cooking with carrots", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/cooking-with-carrots-bda857649d2", "abstract": "Eng Open Source Data Science Machine Learning Android iOS There’s a class on Udemy called “ If you can cook, you can code ” — absolutely true! What do these two things have in common? Curiosity : being interested in how things are put together and why they work the way they do Building from basics : taking simple building blocks and making complex and (hopefully) delightful things from them Experimentation : trying something crazy just to see if it works, failing, understanding why, and trying again Science : well, obviously Learning : you’ll never know everything about either, so there are endless opportunities to learn and grow Research : you’ll probably spend at least as much time googling as actually creating things So, if all of this sounds like you, then you have all the skills needed to be both a great cook and a great coder. Even better, I can confirm that cooking makes you popular at home and at work, and coding is both fun and profitable, so really, why wouldn’t you do both? I suppose it’s no surprise that people who work at Instacart (they call us “Carrots”, thus my title) are very interested in both food and tech. Me, I’ve been cooking almost all my life. The first thing I remember is learning to make blueberry pancakes from my mom’s New York Times cookbook when I was six. I grew up on the East Coast in the 1970s, so except for summers when we picked fresh blueberries, my blueberries came from a can. I had to be careful making blueberry pancakes and muffins (also from that long-suffering NYT cookbook) because they would turn green if I got too much juice from the can in them. Or so I thought. When I was a teenager we moved to California, land of always-available fresh fruit, so I could make my muffins, pancakes, and blueberry bread pudding using fresh blueberries ( from Instacart now), but sometimes my baked goods still turned green. By googling it, I can now learn in seconds why blueberries turn baked goods green . In college, I took one computer science course and was hooked. I took all the classes I could. I also had no money, so I kept cooking for myself, and my friends. My favorite thing to cook then was my mom’s lasagna and garlic bread, which was a lot of food even for our family of four, so it was a cheap way to have food for me (and friends) for DAYS. I was surprised to find out how many people I was in school with had never learned to cook. This effect was probably magnified by the fact that it was the 1980s and I was studying computer science so the vast majority of my friends were men, who presumably were expected to find some woman to cook for them. 😏👋 What they probably didn’t expect was that I was also able to help them with their computer science homework. After all, a recipe is just a simple program where you take a bunch of inputs, put them through a processor, and you get some sort of output. My decade of practice cooking made coding really obvious. What was even more surprising to me was that the most popular dishes I made were also the easiest to make. I’d go all out making some sort of fancy cake that took HOURS and people would be like “oh, that’s nice” and then clamor for my blueberry (there are those blueberries again!) bread pudding, which takes just minutes to put together. The most crucial thing is just to use good-quality ingredients. This carries over into code as well — use the best ingredients you can find and keep it simple — overly clever code may give you a moment of smug satisfaction, but it is unsustainable and even if other people are impressed with your cleverness, they won’t enjoy it nearly as much as they would some nice clean simple maintainable code. I got my degree in CS and went into software professionally, so I continued to spend my time in majority-male environments and my cooking remained a way to make (and impress) friends. Now that I work on the Infrastructure team at Instacart, though, I’m in a group of people who are not only skilled technically but also really know how to cook! My second-favorite Slack channel (after #instacat) is #cookingwithjon, named after Jonathan Phillips, who sits behind me — he’s a software engineer by day, and apparently a professional chef by night. Jon isn’t the only great cook or thoughtful, smart, and helpful engineer at Instacart. A lot of us post our creations on #cookingwithjon. After almost 50 years cooking and 30 years writing software, I find I still learn something new about both every day from all of my amazing co-workers. Here are some samples from the 4th of July holiday week: No one said, “If you can cook, you can make beautiful food photographs.” I am here to tell you you can’t. Tips from #cookingwithjon: My egg salad photos would look significantly better with better lighting and chives sprinkled on top for the contrast with the dark green. Egg salad (and salads in general) taste even better with the addition of a few drops of soy sauce for the umami flavor. You can save time by using a cheese grater instead of dicing the eggs with a knife (there is debate from my friends on this, they feel an egg slicer is better, but I don’t have an egg slicer and I already have too many kitchen gadgets). My friend brought his portable firepit to grill burgers for us for the 4th. They were AMAZING. We also sat around afterward toasting marshmallows and making ‘smores. Tips from #cookingwithjon: If you don’t have a nice portable firepit, this griddle is great for doing burgers and pizza. The baking steel is totally worth it, it can also be used it for mixing stuff into ice cream because it’s a great cold surface after freezing for a bit. If you’re in a tiny condo, a cast iron pan on the top rack of your oven works really well for pizza. The Bon Appetit trick of cooking the pan for a bit on the stovetop helps a lot with the bottom of the pizza crust. Tips from #cookingwithjon: Be very careful, salt blocks can explode on the grill . Who knew? Now we all do. If you can’t tell, I love working at Instacart. I’m approaching four years and can’t imagine leaving. If you also love food and coding, come work with me and see the daily updates from #cookingwithjon, #eng-til, and of course #instacat (or #instapups if that’s your thing, the company dogs are adorable too). Instacart Engineering is hiring! Check out our current openings . I can’t rhapsodize about them and not share, so here are my three super-simple, super-popular recipes. I learned this one from my mom and have never found a reason to change it. Other garlic bread is over-engineered, though I’ll still eat it, of course. 1 baguette (ideally sourdough) 1 stick butter, at room temperature 1 head garlic Heat oven to 350F. Peel the garlic and smush it through the press. Make sure to use a garlic press with big holes, those little tiny ones are too much work. Mix the garlic into the butter with a fork until it’s totally blended. Slice the bread not quite all the way through, 1/2–3/4 inch slices. Spread the butter in between the slices. There will be a lot. Wrap the whole thing in aluminum foil and bake for half an hour. It’s a good idea to bake on a cookie sheet as some butter inevitably escapes and will burn on the bottom of your oven. I grew up making cheese straws, also learned from my mom, but I’ve simplified the recipe. People‘s faces literally fall if I show up for a party without these. 1 pound sharp cheddar, grated 1 stick butter, at room temperature 2 cups flour 1 teaspoon salt 1–3 teaspoons cayenne pepper 5–6 tablespoons water Heat oven to 425F. Beat the butter with the salt and cayenne. 1 teaspoon cayenne gives it a little life, 3 will make people cry and keep coming back for more (this is the “strangely addictive” part). Mix in the flour. It will be too dry to hang together, so just get it nicely blended. With the mixer running, start adding water, a tablespoon or so at a time. Once it starts to pull together into a ball, stop adding water. Gather the dough into a ball and roll it out on a well-floured board to about 1-inch thickness, or a little less. I learned to make these using a Mirro cookie press , but this is a very stiff dough and rolling it out is much easier. Use a cookie cutter to cut out little biscuits, an inch or a bit more in diameter. I use the smallest of my star-shaped cookie cutters , which is why what are traditionally called “cheese straws” started being called “strangely addictive cheesy poofs” by my friends. Also the South Park episode . Place about an inch apart on a cookie sheet and bake for 10–12 minutes — you’ll know they’re done when they poof up and are lightly golden brown on top. Let cool a few minutes, THEY JUST CAME OUT OF THE OVEN AND ARE HOT, as I keep having to tell people who can’t wait. I had a fancy bread pudding at a restaurant and wanted to have it again without the cost, so I found a recipe on the Web and started tweaking it. It’s so easy you can pretty much NOT mess it up. 1 pound (or thereabouts) of leftover bread, again I like sourdough here 1 quart milk (I always use whole milk, it’s dessert after all!) 1–2 cups sugar (I find the sauce to be enough sugar, but to your taste here, it’s not crucial) 3–5 eggs (you can use anywhere from 3–5 here, also depending on how much bread you actually have, I like it eggier, so I use 5) Splash of vanilla (or not) Couple shakes of cinnamon (or not) 12–16 ounces blueberries (I usually use two 6-ounce containers, but use as many or few as you like or have available) Cut the bread into 1\" cubes and put them in a big bowl. I have four of these . Pour in the milk and let it sit for a while to soak in. It should sit at least half an hour but can sit all day. Stir once to get the top bread cubes under the milk, if you remember. When you’re (almost) ready to cook it, preheat the oven to 350F. Crack in the eggs, add the sugar, and add the vanilla and cinnamon (if using). Stir until the eggs are well-blended in, just a minute or two. This is when it’s very important to have used a very large bowl, as it is splashy. Get a baking dish or two loaf pans and toss in a pat of butter. Stick them in the pre-heating oven to melt the butter. Pull out the pan(s) when the butter is melted (remember, things coming out of the oven are HOT). Swish the butter around to coat the bottom and sides of the pan. Stir in the blueberries (try not to smush them, but, again, no big deal) and pour into the prepared pan(s). Bake 30–40 minutes, until the pudding is set (no more free liquid) and the tops of the bread are lightly browned. Let it cool a bit while you make the sauce. Tempering the egg to bind the sauce is the only really tricky bit, and you’ll get good at it with some practice. 1 stick butter 1 cup sugar 1 egg 1/4 cup whiskey (or really any alcohol you’d like, or just omit this) Melt the butter and sugar together. I do this in a 2-cup pyrex measuring cup in the microwave, but you can do it on the stove too. You want to have most of the sugar dissolved, but it isn’t crucial to get this perfect. Turn off the heat if using the stove. Whisk up the egg in a small bowl. Add a small amount of the hot butter/sugar mix while continuing to whisk to keep from curdling the egg. Keep doing this until you’ve about doubled the amount you are whisking. Now the egg should be tempered. Start whisking the butter/sugar mix and slowly pour in the egg, continuing to whisk as fast as you can. Don’t let it cook and you will get a nice smooth sauce that is bound together by the egg. If using alcohol, whisk it in as well. It should incorporate with a little effort, but if it doesn’t, just stir up the sauce before pouring it. Spoon up some of the bread pudding into a bowl and pour on the sauce. Store extras in the fridge. Reheats very well, but don’t reheat the sauce, just pour the cold sauce onto hot pudding to re-liquefy it. Instacart Engineering 105 2 Engineering Cooking Engineering Team Front End Development Infrastructure 105 claps 105 2 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-16"},
{"website": "InstaCart", "title": "migration from redshift to snowflake the path for success", "author": ["Tamir Rozenberg"], "link": "https://tech.instacart.com/migration-from-redshift-to-snowflake-the-path-for-success-4caaac5e3728", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Picture this: It’s Monday around noon, and our ETL has just completed the previous day’s load after dealing with several issues overnight. We are at 85% disk full and Redshift VACUUM just kicked in to clean up all of our soft deletes from last week — nothing significant can run during this time. We pause all of our processes to let VACUUM complete its job. We’re paying for a cluster that reserves one day a week for data cleanup… Meanwhile, internal and external customers are raising flags about data freshness and overall slowness. The Data Engineering team is heads-down, closing out the backlog and dealing with operational issues instead of developing new functionality. Migrating to a different cluster configuration can take a full 48 hours, but we aren’t sure that it would resolve our speed issues. This used to be a typical day for Instacart’s Data Engineering team. We build and maintain an analytics platform that teams across Instacart (Machine Learning, Catalog, Data Science, Marketing, Finance, and more) depend on to learn more about our operations and build a better product. Redshift was our cloud data storage warehouse for several years, and it served its purpose at the time…however we started running into scaling and stability issues as we grew, so we made the decision to migrate our platform to Snowflake . We pulled together some of our key migration observations and learnings to get you started on your own. We decided to clone the schemas and tables created in Redshift without major changes in data structure. Focusing on migration only (without changing structure) was key for the successful migration of our big, complicated system, filled with dependencies. This approach also allowed us to easily compare the data between the old system and the new system and run them in parallel to gain confidence before the official switch. One of the big advantages Snowflake architecture provides is the separation between storage and compute. We built a dedicated warehouse for our major applications and made sure to name the warehouse so it was easy to recognize who within the organization is using it. Once we provided the permissions for teams to use the warehouse, it was easy for us to identify the cost associated with each application and business unit. This is super helpful and something we could not do in Redshift. We forked the git repository used by Redshift and modified the new branch to work with Snowflake. Every few days we merged the master branch to the new branch so we wouldn’t have a lot of conflicts to resolve during the final migration. As our first step, we took all of the schemas in Redshift and created the same in Snowflake. We used an automated script that scanned Redshift information Schema for a given schema or table. We then extracted DDL (data definition language) from Redshift using vw_admin_generate_table_ddl (Redshift view), enhanced the DDL to Snowflake, unloaded data from Redshift to S3 and loaded it back to Snowflake in a new table. We ran that process several times during migration to make sure Snowflake data aligned with Redshift. Here are some of the code samples we used to pull DDL objects from Redshift: Then, we generated the Redshift DDL: Afterward, we took the DDL and made it compatible with Snowflake. Snowflake’s support team provided us this script to migrate the DDL to Snowflake. Then we unloaded Redshift data to S3 and loaded it from S3 into Snowflake. Redshift and Snowflake use slightly different variants of SQL syntax. Here are the main differences that you might need to consider while migrating the code: DML changes : Redshift has more lenient syntax for deleting tables. Snowflake is stricter with correct syntax, while Redshift can accept just delete table_name without the “from” key word. (Personally, I prefer the Snowflake approach but I was surprised to find how many cases used the Redshift syntax.) Semi-Structured Data : Both Snowflake and Redshift provide parsing capabilities for semi-structured data. You might need to change SQL to the format below in case you are parsing JSON in Redshift as Snowflake syntax is different. Snowflake provides variant datatype which we found valuable for (JSON, Avro, ORC, Parquet, or XML) and performed well as Snowflake stores these types internally in an efficient compressed columnar binary representation of the documents for better performance and efficiency. Snowflake Information Schema is kept as UPPER case. If you are not using their Information Schema (less likely) you will be fine, but if you are referencing Information Schema, you might need to change it to lower or upper case letters in your query. A note: This becomes a major problem if you reference it from Tableau. Tableau metadata is case sensitive meaning same field with upper vs lowercase is treated as two different fields, and as result, the report breaks. We ended up migrating all Tableau data sources manually with the help of an Interworks consulting team. The upside to the manual Tableau migration: we performed some major cleanup. We discovered most of our data sources and dashboards where unused, and ended up migrating just 180 out of 3,000+ workbooks over. We also experienced issues while running SELECT with UPPER function: Snowflake will return the following error in case your Information Schema query is not selective enough. docs.snowflake.net Snowflake’s default account time zone is set to America/Los_Angeles, which means the database clock time will be PST. We ended up changing it to UTC after using it for a long time in PST, but this change was a bit scary as we did not know what might break. (My advice: check it before you start the implementation and set the time based on your needs. I strongly recommend UTC.) docs.snowflake.net We found Redshift is way more forgiving with dates. During our data migration, we spotted many cases where Redshift was able to store a future date in a timestamp column, but Snowflake rejected that date. While I agree with rejecting non-realistic future dates like ‘11457–11–09’, it was hard for us to find the dates and clean them up, as Snowflake’s error messaging wasn’t as detailed as we would have liked. In our case, we decided to use NULL_IF in the copy command, similar to the example below. Snowflake does not provide conditionals like “if date > ‘2030–01–01’ replace with null”. You must specify the exact date in order to mark it null. Our ETL consumes thousands of tables from RDS dbs and produces tens of fact tables. The total storage at the time of migration was 1.2PB compressed. We run hundreds ETLs every day and at peak time we can have 50+ ETL runs simultaneously. We also migrated around 800 views that are used by our reporting tools. It was impossible to manually compare the results between Redshift and Snowflake. We decided to build a comparison tool that provided us the following results: Select count of each table and compare results with Redshift. Select count distinct of each string column and compare with Redshift. Structure comparison of each table. Simple check if table exists. Select sum of each numeric column and compare with Redshift. We ran both systems in parallel for a few weeks to compare data between the two. Then, once we were confident the migration was successful, we slowly switched the applications to use Snowflake as their main data source. We left our Redshift clusters live for one month after the migration and shut them down after confirming zero activity in Redshift stl_query. Starting this massive migration project without support from upper management is a recipe for failure. Before starting the project make sure you get a commitment from management about resource allocation as different groups might be involved in migrating their code to be compatible with Snowflake. While you won’t find all of the major hitches and differences between Snowflake and Redshift in this post, we hope the items above save you lots of time if you’re considering a migration. While the migration is complete, the work is never over. Want to work on projects like this on Instacart’s Data Engineering team and share your findings? Check out our current openings . Instacart Engineering 557 Thanks to Instacart and Muffy Barkocy . Big Data Data Engineering Engineering Data Snowflake 557 claps 557 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-25"},
{"website": "InstaCart", "title": "modeling the unseen", "author": ["Ganesh Krishnan"], "link": "https://tech.instacart.com/modeling-the-unseen-6a51c9a02430", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We give customers a lot of fulfillment options to choose from as they place an order — they can opt for delivery time slots ranging from an hour to a week later. They can also choose to pick their shopped groceries up from the store in certain markets. More options add complexity to a fulfillment chain, and more complexity can lead to efficiency loss… unless you measure where your gaps are. One gap that’s been notoriously hard for us to measure is lost demand . Sometimes outside events like traffic, weather, and shift cancellations can affect shopper availability and the fulfillment options we surface in-app. If we have limited shopper availability relative to customer demand, we may need to remove fulfillment options from the app to avoid overcommitment. For example, two-hour delivery may not be visible in our app in Brookline, MA on the morning of the Boston Marathon, as some shoppers may be sitting in traffic. When we remove options due to lack of shopper availability, a customer may shop for groceries themselves instead. As a result, they did not place an order. This is a classic example of lost demand. A customer may not place an order for many reasons. Perhaps a friend told them about Instacart and they just wanted to test out the product, or maybe the customer wanted a certain grocery item and it wasn’t available on the storefront they were browsing. While a customer may not place an order in these cases, we do not consider this lost demand for fulfillment purposes. For the remainder of this post, Instacart experiences lost demand when a customer does not place an order because their favorite fulfillment option was not available. Understanding and estimating lost demand is one of those classic “we don’t know what we don’t know” conundrums. If we have an undersupply of shopper availability relative to customer demand, we will only observe part of the demand. To observe the full demand, we need either balanced supply or even a little bit of an oversupply. Mathematically, we can then frame the lost demand problem as one that attempts to understand how much additional demand we could have obtained if we had perfect availability across every fulfillment option (balanced supply) for all customers. Here, D_f is the demand that we would have observed if there were 100% fulfillment option availability (counterfactual), while D_a is the demand that we actually observed (factual). The actual demand, D_a , is observed and known. D_f , on the other hand, is unknown and consists of the observed demand ( D_a ) and the lost demand ( L ). This brings us to the fundamental challenge of lost demand estimation — we want to estimate an unobservable: the counterfactual D_f . If we do not estimate this unknown and correct for it, a fulfillment system can suffer from vicious feedback loops. As an example, if there are 100 (unknown) potential orders every day and we underestimate the number of orders, we may only have enough supply for 90 orders. The result of the undersupply is that we will lose ten orders. But it doesn’t stop there. The 90 orders we observed will be fed back into the demand forecast for future orders. As a result, we will again end up in an undersupplied state, causing us to build out an inefficient supply chain, and ultimately lose out on orders. Early on, we relied on a set of heuristics that measured the number of customer visits to our mobile and web apps to predict lost demand. We define the term “visit” as a customer’s journey through the conversion funnel on a single order. One visit could be comprised of several user sessions throughout the day. While incredibly simple, these early heuristics allowed us to get a vague understanding of lost demand at a time when we didn’t have the data in place for more sophisticated modeling. Over the years, the steady accumulation of conversion funnel data, the expansion of fulfillment service options, and (to be honest) the growth of our ML team, has helped us continuously improve our lost demand estimation models. Now, we think the smarter way of estimating lost demand is to build a model that estimates the customer conversion probability as a function of the fulfillment option availability. For this model, we can define “conversion” as completing and placing an order, and we interpret the probability of customer conversion as the average number of orders per visit. To estimate the total demand from conversion data, all we would have to do is to sum the conversion probabilities over all customer visits: As an example, let us say that a customer opened up their app and only saw 3 out of 8 possible fulfillment options available. We can use the model to estimate the expected conversion under this availability state ( p_a ). We can also use the model to estimate what would have happened if all 8 options were available ( p_f ). The difference between p_f and p_a gives us the lost demand for that visit. We can then sum over all visits to get the total lost demand: As a customer journies deeper into the funnel, we can be more certain of the customer intent. In our more recent iterations of the lost demand estimation model, we’ve actually opted to build two different conversion models — one that uses fulfillment option availability data from the pre-checkout stage and another that relies on similar data from the checkout stage. Both these models use additional features like user tenure, past order history, number of item searches etc. Modeling the pre-checkout stage is important. Some customers do not proceed to checkout if their preferred fulfillment option is not available. The pre-checkout stage is key for such customers since it can be a big contributor to lost demand. Data from the checkout stage has a higher signal-to-noise ratio — customer intent is inherently stronger in the checkout stage. If a customer finished filling their cart, and they’ve chosen a fulfillment option, odds are they’re more likely to convert and complete the purchase. Building two models also provides us with one critical advantage: both the checkout and pre-checkout models can be iterated on independently, which allows us to quickly improve the models over time. Good model validation is crucial to building trust with stakeholders. We think about validation in terms of offline and online validation. Setting up good offline metrics is essential for iterating quickly on machine learning models. The offline metric should be chosen so that it correlates well with the online metric of interest. In this case, we chose AUC as the offline metric. We then attempted to maximize the AUC of the model. Since lost demand is unobservable, it was a challenge to validate the estimates produced by the model. An A/B test that offered full availability to some customers would allow us to validate the model. Such an experiment would allow us to estimate the difference in conversion induced by availability alone. One challenge here is that offering all users 100% availability may result in overcommitment and late orders. Hence it is important to restrict the number of customers that see full availability to a small fraction. After running these experiments, we found that the model estimates match very well with the 95% confidence interval from the experiment. In addition, the model improved upon the accuracy of the previous version by almost a 100%! This model is just a step in the right direction. We’re constantly searching for more efficient ways to manage our fulfillment chain. And as our user base increases, our product offerings grow in complexity (and our Data Science Team grows!), we’ll continuously work to improve our estimation models to offer up efficient and convenient fulfillment options for each and every customer on our platform. Special thanks to everyone who contributed to this project and helped review this post. Houtao Deng, Jeremy Stanley, Ji Chen and Abhay Pawar. Want to help improve our lost demand estimation models with me on Instacart’s Machine Learning team? Check out our current openings . Instacart Engineering 461 2 On Demand Forecasting Logistics Machine Learning Technology 461 claps 461 2 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-06"},
{"website": "InstaCart", "title": "the story behind an instacart order part 1 building a digital catalog", "author": ["Instacart"], "link": "https://tech.instacart.com/the-story-behind-an-instacart-order-part-1-building-a-digital-catalog-46df5a8ff705", "abstract": "Eng Open Source Data Science Machine Learning Android iOS By: Gordon McCreight It’s 5:00 pm, you’re heading home from work, and want to make a quick dinner tonight by 7:30 pm. You want to cook up some salmon, but are missing three key ingredients in your fridge: 2.5 lbs of fresh salmon fillets, 1 Meyer lemon, 1 bunch of fresh dill. You pull up Instacart on your iPhone, add these three items to your cart, set your 6:00–7:00 pm delivery window, and complete your order. An hour and a half later, your groceries are delivered to your door right as you’re pre-heating the oven. The moment is simple and satisfying on the surface, but beneath it all lies a complicated web of systems, working together to fulfill your order. In this “Journey of an Order” series, we’ll walk you through the technology that surfaces grocery items in our apps, enables storefront browsing, guides the shopper through checkout, and powers our last mile logistics to get dinner to your door in as fast as an hour. In this first installment, we’ll dive into how we acquire grocery item attributes and inventory data from retail partners, configure locations and inventories in near real-time, and get data into a usable format in our larger grocery “catalog”. Our catalog is the largest online grocery catalog in the world, with over half a billion item listings from over 300 different retailers from nearly 20,000 stores across North America. At a minimum, any single product in our catalog has a subset of attributes : price, name, location, UPC , and SKU . We can accommodate a more detailed list of attributes, including multiple images, nutritional information, seasonality info, dietary labels (gluten-free, kosher, vegan, etc) — the more attributes we collect, the more flexible and fine-grained data taxonomy and search can be for our shoppers and customers down the line. So, how do you acquire all this information? It requires working with many disparate sources of data of varying accuracy. Our catalog is made up of over 30 data sources, including data provided directly by a retailer, third-party content aggregator, manufacturer, and even “homegrown” collected in the field by Instacart shoppers and site managers. We collect two levels of item attributes: Store-specific attributes : availability, specialty items tied to location, location-by-location pricing Product-specific attributes : name, product descriptions, approved photos, nutritional information, size/weight Instacart updates more catalog data points every night than the world tweets in a day. Inventory, prices and other attributes change minute by minute, if we don’t reflect those changes in our catalog, our storefronts don’t reflect accurate listings, resulting in disappointment from customers, shoppers, retailers, and CPG partners. From our early days working with just a handful of retailers, we’ve invested in tooling to enable them to send us regular updates about the products on their shelves. When the price of an item changes at a particular store location, or if the product becomes unavailable, retail partners can alert us in several ways depending on the size of their inventory and specifics of their own technical infrastructure. For manual, one-off edits, we’ve built a user-friendly web portal with a form-fill that allows retailers to add in updated product attributes, at any time they choose. We give retailers the option to drop CSV files on our SFTP server with updated product attributes. They can drop updates onto the SFTP server as often as they like — many choose to push it once daily. Some retailers, often partners who’ve built sophisticated inventory systems in-house, are able to provide us with a real-time stream of “balance-on-hand” data, giving us moment-to-moment updates about items on their shelves. We’ve built out an API that retailers can use to send us changes in availability and product attributes in real-time. That’s the easy stuff. Once you have all of this data, how do you clean it? How do you determine the best, most accurate, and most consistent product attributes for listings in our catalog? Finding the most accurate attributes requires us to pit sources against each other, and prioritize inputs. Product availability is a store-specific attribute that has a massive effect on our customer and shopper experience. If the data we received last night in a CSV file drop tells us that Meyer lemons are in stock at a store in Mobile, Alabama, but a shopper on the ground indicates in their app that the item is out of stock, we have to weigh each input against one another and make a split second decision about what input is correct. We initially built a set of heuristics around this, comparing the reliability of data sources at a macro level to determine the “most accurate” answer to the question of whether the Meyer lemons are actually in the store. We used a bit of good old human intuition to create these heuristics. For example, multiple shoppers reporting an item is out of stock at a location is *usually* more reliable than a 3rd party provider’s data. One-off inputs given in the last 30 minutes through the retailer portal are *usually* more accurate than a nightly update via CSV. It can get more complicated, though. What if a shopper and a retail manager send conflicting inputs at the same time? At our scale, that happens. Human intuition can only take you so far, however. We cooked up these heuristics when we had a lot fewer product listings in our catalog. Currently, we deal with much larger data ingests every day, and we have a lot more inputs coming from shoppers, brands and retailers. Now, we’re looking to machine learning models to find more signals from the noise and build better, more fine-grained prioritization models at scale. Ultimately building out these models gives us pinpoint accuracy, and makes it easier for you to see if your fresh Atlantic Salmon fillets are in-store, or if we’ll need to add 10–15 minutes to our dinner prep schedule to defrost a frozen salmon fillet replacement item. In 2018 alone, we on-boarded more than 100 new retailer partners, each with their own unique store and product-specific attributes Our database infrastructure has traditionally evolved as an “artisanal” system, built over time to address our immediate needs as they came up. In short, it was never architected . Keeping up with the daily mountain of data our partners send and dealing with data quality issues at scale, is one of the primary challenges for our Catalog team. Now with seven years of catalog data, and billions of data points updated every day, we recognized that we needed to build a system to handle “big data problems.” Fun fact: While partners can send us inventory data at any point in the day, we receive most data dumps around 10 pm local time. Certain individual pieces of our system (like Postgres) weren’t configured to handle these 10 pm peak load times efficiently — we didn’t originally build with elastic scalability in mind. To solve this we began a “lift and shift” of our catalog infrastructure from our artisanal system to a SQL-based interface, running on top of a distributed system with inexpensive storage. We’ve decoupled compute from that storage, and in this new system, we rely on Airflow as our unified scheduler to orchestrate that work and Snowflake to store and query the data. Rebuilding our infrastructure now not only helps us deal with load times efficiently, it saves cost in the long run and ensures that we can make more updates every night as our data prioritization models evolve. Once we’re confident that the salmon fillet, fresh dill, and Meyer lemon attributes are the most accurate, up-to-date, and “best” listings we have, we surface that data into the online storefronts that our customers use every day. Stay tuned for the next installment of this series, where we’ll dig into the tech that makes our customer-facing app tick. Can’t get enough of databases and library science 📚 ? Our Catalog team is hiring! Check out our current engineering openings . Instacart Engineering 185 3 Ecommerce Data Science Infrastructure Technology Eng 185 claps 185 3 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-07"},
{"website": "InstaCart", "title": "building an on demand fulfillment engine is hard", "author": ["Instacart"], "link": "https://tech.instacart.com/building-an-on-demand-fulfillment-engine-is-hard-ea7f2d5df7c6", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Kevin Henrikson and JJ Zhuang go together like PB&J — they’re the engineering “odd couple” that leads Instacart’s fulfillment engineering organization. Before they joined us, they cofounded Acompli, the email productivity app that was acquired by Microsoft in 2014. Acompli’s mobile app was relaunched as Outlook Mobile six weeks after the acquisition. Now at Instacart, they’re building another type of productivity product. As VPs of Engineering, they lead the engineering infrastructure, mobile, machine learning, data science, and operations research engineers that build our Shopper app ( iOS and Android ) and our last mile logistics technologies. That’s a lot of bases to cover, so we sat down with Kevin and JJ to chat about their long history working together as technologists, the challenges unique to on-demand fulfillment models, and (of course) what’s in their cart. JJ: We always look for ways to create value in our work, and there’s no other creation more valuable than “creating time”. We built enterprise software because we could make people more productive with their work. But Instacart just brings time-saving to a whole new level, as grocery shopping is a constant in our lives. Kevin: We wanted to make a large impact…and with the growth Instacart’s currently going through, we knew it would provide the size of the opportunity we could really dig into. Millions of people depend on Instacart to bring them fresh groceries in as fast as an hour. Before I interviewed, I signed up to be a shopper, and the delight my customers had when I made my first few deliveries as a shopper let me see firsthand the impact we can have. JJ: We met each other at our first jobs in Silicon Valley almost two decades ago. Kevin: By chance, we shared an office together. Over time we found we had very different working styles and grew up in very different environments (JJ in Shanghai and me on a pig farm in Galt, CA). Both Mechanical Engineers by training, we both stumbled into software a couple of years before we met. JJ: To people who don’t know us, we are the odd couple who are the exact opposite of each other in almost every way and have literally nothing in common; to people who have been working with us, they see our collaboration as the most natural thing. It takes many skill sets to build and lead an effective technical team, and each of us gets to develop our own strengths and play to those strengths. Kevin: I always like to pose it as a souped-up version of the classic traveling salesman problem we all learned in school. How do Instacart shoppers get a hot rotisserie chicken and a cold pint of strawberry ice cream from a store to a customer’s door in a couple of hours? Better yet — How do you get a hot roast chicken and a cold pint of strawberry ice cream from a store to a customer’s door that’s 5 miles away in 103-degree heat in an hour? JJ : Instacart doesn’t maintain our own inventory or warehouses—we act as a layer on top of retailer systems. Our Fulfillment Team develops the iOS and Android shopper apps that shoppers rely on in-store, and maintains the logistical system that batches orders together and routes them to your door. But there are complexities: car problems, bad weather, outside events…all of these complexities add up…so we’re building out a system that predicts demand, re-computes order batches every minute and makes dispatching decisions *just in time* to meet our SLAs. Kevin: We’re also building the tools to recommend better item replacements. We have a sophisticated algorithm and seven years worth of data that helps us understand customers’ food preferences and offer up high-quality replacements based on those preferences. If a store is out of fresh dill, does a customer prefer dried dill? Or maybe fresh fennel stalks? When shoppers mark an item as out-of-stock during their shop, our ML-powered item replacement algorithm kicks in, looking at historical data to suggest an in-stock replacement. These are gnarly problems to solve — I’m never bored. Kevin: Knowing your user is critical. Prior to Instacart, we had spent over ten years building email software. Luckily we were already heavy email users, so we had a decent understanding of the pain points to help get us started. Instacart was different—while I had used the product as a customer (over 700 orders when I joined), I knew I needed to learn more about the tool shoppers rely on every day. Before my first meeting with Instacart, I signed up as a shopper ( iOS or Android ) to learn more about the business and had over 75 screenshots with questions and ideas. I shopped 12 hours a day some days to see what it’s like to work longer shifts. All in all, I logged more than 100 hours as a shopper before I officially joined. And I’m still shopping. Since joining, we’ve been laser-focused on incorporating features that offer more flexibility to our dedicated network of over 70,000 shoppers. JJ: Shopping is hard. Period. Every single engineer who works on the fulfillment team shops as part of their onboarding, and now, the team must log hours every quarter as an Instacart shopper. Understanding the shopper experience in depth was critical for our organization as we kicked off new engineering initiatives to improve the experience — initiatives like instant cashout , integrating in-app support , and offering an on-demand shopping product feature . JJ: T he team culture is just amazing. The founders and early employees built a solid foundation for the culture — the sense of urgency in everything we do, always having each other’s back, and always putting customers and shoppers first. We are pleasantly surprised that with so much team and business growth, the culture endures. It’s a high-growth company, but still scrappy. Kevin: The difficulty, complexity, and number of super interesting problems to solve. JJ: Organic Carrots, Fuji Apples, Mini Watermelons, Kettle Chips, Baguette, Triple Creme Cheese, Pellegrino, Mint Chocolate Chip Ice Cream Kevin: Kirkland Signature Rotisserie Chicken, Bel Mini Babybel Original Cheese, Organic Hass Avocado, LaCroix Sparkling Water, Outshine Mango Fruit Bars Want to build fulfillment chains with Kevin and JJ? Instacart engineering is hiring! Check out our current openings . Instacart Engineering 71 Fulfillment Technology News Logistics Machine Learning Mobile App Development 71 claps 71 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-28"},
{"website": "InstaCart", "title": "writing the recipe for a technical sales team", "author": ["Instacart"], "link": "https://tech.instacart.com/writing-the-recipe-for-a-technical-sales-team-124e7e59d8b4", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instacart’s consumer-facing app and shopper app makes grocery delivery accessible to over 80% of US households and 60% of Canadian households. The e-commerce and fulfillment apps most people see rely on a suite of B2B-focused retailer products — APIs and tools that make it easy for grocers to plug into the Instacart marketplace or build and maintain their own online storefronts. Now, with over 300 retailer partners and over 15,000 physical stores in the Instacart marketplace, we’re excited to welcome Amr Hiram to build out our brand new Technical Delivery team. This new engineering team charts the technical journey of our retail partners, handling everything from technical pre-sales, through implementation, technical account management, product launch, and follow-on support. Amr’s a great addition to the team — he’s got a long history in retail technology. We sat down with Amr to talk about online grocery, engineering with empathy, and snacking! You’ve been working in retail tech for a while — why the jump to Instacart? I love working in retail tech. It has always been a huge part of my career — I started hacking away at Point of Sale software on terminals with 8K of RAM, then I went on to executing multi-vertical retail implementations worldwide at SAP. I led the team that has built multiple digital experiences at Loblaw Digital, laid the foundation at Unata in partnership with Longo’s/Grocery Gateway, and then went on to Tulip Retail where I worked to modernize the tools used by in-store sales associates. Instacart is in a pretty unique position. One on hand, we’re evolving the way people shop for groceries — an experience that has seen relatively little change in decades. On the other hand, we’re helping retailers to join this evolution by transforming the way they service their own customers. I’m excited to be part of the journey to realize the full potential of this evolution. What keeps bringing you back to the online grocery industry? Grocery is fascinating because it’s so fundamental to everyone’s lives…everyone needs to eat, and yet there are so many factors that impact eating habits and purchasing decisions: convenience, budget, shopping experience, and brand loyalty. How do you build solutions that tailor to all permutations of these factors, and how do you do it at scale to support millions of purchases? Working on these challenges keeps me coming back! What do you look for when hiring for a technical delivery team? Empathy and execution. Technical delivery needs to forge relationships with retail partners and internal product/engineering teams alike. Technical Delivery helps us strike a balance between partner requirements and engineering deliverables, and empathy is critical in defining that balance. Deploying improvements in a retail environment requires detailed planning, impact assessment, and risk mitigation — these are vital aspects to flawless execution in a change-sensitive ecosystem. You’re brand new — what’s the first project you’re going to work on once you’re settled? The lifecycle of partnering with a retailer touches on all teams within technical delivery, and my goal is to deep dive into each stage of the cycle, followed by defining the engagement model between our retail partners and each team and stage. It’s important for our retail partners to become intimately familiar with how we operate and this enables us to establish a standard we can continuously improve upon. What item is always in your cart? What do you make with it? Sunflower seeds 🌻! They’re my guilty pleasure. I enjoy snacking on them when I’m winding down, or when relaxing outside during the not-frequent-enough hot days in my hometown just outside of Toronto. Craving a new job? Want to work with Amr on our brand new Technical Delivery Team? Check out our current openings . Instacart Engineering 67 Retail Sales Engineering Toronto Technology News Business Development 67 claps 67 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-03"},
{"website": "InstaCart", "title": "making an accessible web modal", "author": ["Logan Murdock"], "link": "https://tech.instacart.com/making-an-accessible-web-modal-48e4e9d8c284", "abstract": "Eng Open Source Data Science Machine Learning Android iOS While a lot of customers tell us they use Instacart to add valuable time back into their day, a community of users with mobility issues or visual impairments tell us they rely on our app as a dependable way to get the groceries they need. The Customer Engineering Team at Instacart is tasked with building a seamless in-app experience for every customer — so we take in-app accessibility for these customers really seriously. One of the largest accessibility challenges we face is building an elegant (and usable) web modal. For standard situations, we use React-Modal which provides accessibility support right out of the box. Problem solved. Unfortunately, it’s not that simple. Some design and functional situations call for custom modals. This custom work requires that we tackle accessibility issues head-on. Following these best-practices helps us ensure that our modals provide the high-level of accessibility customers need and deserve: Ensure there is a close button available early in the modal DOM Maintain a focus trap and close when ESC is pressed Return focus to original element on close Apply correct roles and labels Include appropriate headings Test using a screen reader Leaving a standard modal can be very difficult and frustrating for keyboard users when there is no obvious exit mechanism. The modal should provide a close button for exiting easily. The button should be early in the content so the user does not need to tab through numerous elements to exit. To optimize the keyboard user experience, it is preferable to focus on the close button when opening the modal. In some cases, adding a close button within the modal or at the beginning of the content conflicts with ideal product design. One useful alternative is to create a hidden button that appears on focus. We build an invisible button by setting the position off-screen and moving it back on-screen when it receives the focus. This provides keyboard users with a simple way to exit when they reach the element. The core design objectives are preserved while we simultaneously provide an excellent accessibility experience! The focus trap is another key accessibility feature for modals. The trap ensures that focus cannot leave the modal until it is explicitly closed. Tabbing with the modal open will cycle through all interactive elements in the modal and finally back to the first element once the end is reached. In the words of Admiral Ackbar “it’s a trap”, which ensures the user stays within the relevant content. To achieve this, focus must be explicitly moved back to the first element in the modal if the user tabs past the last element. Similarly, focus must move to the last element when back-tabbing past the first element. In React this can be achieved by handling the onKeyDown event. This example works by finding the interactive elements through a querySelectorAll and comparing document.activeElement against the first or last interactive element to check if you need to force focus into the trap. While the focus trap optimizes the user experience when the modal is open; it is also critical to maintain a seamless flow when the modal closes. Returning focus to the original element that opened the modal ensures that keyboard and assisted technology users do not lose their location. Without return focus control; users are forced to relocate themselves within the page after close. This makes modals a jarring, unwelcome navigation experience. Not good. We achieve return focus control by storing the active element that opens the modal in state during component mount. When the component unmounts we re-focus on the stored element. This simple logic ensures that modals are a natural flow experience for our users. The preceding best-practices are focused on controlling active navigation of modals. It is also important to handle various declarative attributes of modals deliberately when optimizing apps for accessibility. Modals must have correct roles and labels set so that accessibility software presents them properly to the user. These values need to be placed at the appropriate level and not repeated. Here are several critical attributes: role: dialog — Identifies the container of the modal element for screen readers. aria-modal: true —Lets screen readers know this content is separate from the rest of the page content. tabindex=\"0\" — Allows for programmatically focusing on the dialog Appropriate aria-label or aria-labelledby — Identifies the content of the modal W3C provides detailed explanations of these attributes and how they improve accessibility: https://www.w3.org/TR/wai-aria-practices/examples/dialog-modal/dialog.html In addition to roles and labels; modals need appropriate headings to identify their content since they are intentionally separate from the underlying view. The heading should be at the beginning of the content and included in each subsection as appropriate. Sometimes existing design or technical constraints may discourage new headings for modals. While this might be fine for standard application rendering — it leaves users of accessibility features with a degraded experience. Fortunately, we have a best-practice for this situation: a non-visual heading. Screen reader only headings can be achieved by using the following styles. The magic is clipping the header position and hiding the overflow. This renders the element invisible in standard application views but available to screen-readers. Win-win. To ensure the best experience for assisted technology users; empathy is a must. There are few better ways to understand the struggles of users than stepping into their shoes — whether it’s for a block, a mile, or sometimes a marathon. The final step in accessibility development is thoroughly testing the full experience using a screen reader (VoiceOver, NVDA, JAWS, etc…). You will certainly appreciate the easy close button, the sane modal trap, and clear heading information you built into your app. We did with ours. Maybe you’ll even notice other accessibility challenges beyond these best-practices and add new cutting-edge solutions. Best of all — you will have optimized the experience for a critical segment of your users and hit another milestone in providing a seamless in-app experience for every customer 😄. Want to build an even more accessible app with me and Instacart’s Customer Engineering team? Instacart is hiring! Check out our current engineering openings . Instacart Engineering 471 1 Thanks to Dan Loman . Accessibility Web Development Web Accessibility Modal 471 claps 471 1 Written by Former Revenue Accountant and CPA turned Full-Stack Engineer Instacart. Also a huge Japanophile. Instacart Engineering Written by Former Revenue Accountant and CPA turned Full-Stack Engineer Instacart. Also a huge Japanophile. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-23"},
{"website": "InstaCart", "title": "greetings karney li", "author": ["Instacart"], "link": "https://tech.instacart.com/greetings-karney-li-1efa08d263e8", "abstract": "Eng Open Source Data Science Machine Learning Android iOS A Q&A with our newest VP of Engineering Karney Li just joined Team Instacart to lead our Retail Engineering Team. From our Toronto engineering hub — Instacart North — Karney will build out the team that works on the APIs and products retail companies use to build and maintain their Instacart storefronts. Li’s a fixture in the Toronto tech community. He comes to us from Wealthsimple , where he was CTO, and Amazon before that, where he was a was a Software Engineering Manager for Amazon Fulfillment Technologies. We sat down with him to talk about engineering philosophy, org building, and breakfast 🍳 Every org encounters operational, latency, stability, magnitude, quality, data, and architectural challenges as they grow. Challenges like these are all, in a sense, surmountable with enough time. My goal is always to enable teams to meet these challenges faster . By increasing the rate that we can test hypotheses, we can shorten the feedback time. In turn, this improves our understanding, allowing us to iterate and arrive at a solution faster. That’s one of the most challenging things about moving quickly with a maturing, widely-used product — engineers can’t just “move fast and break things.” It’s important to perform controlled experiments that mimic real conditions as closely as possible. It’s important to engineer safety nets so that regular experimentation can happen with controlled variables. Otherwise, people will become afraid of the consequences of failure. Failure is necessary for innovation…and being able to fail without fear is a great challenge. I think great engineers need to be naturally curious. Examining things closely is the first step in making them better. Attention to detail is incredibly important, too. At Instacart, we’re working with large volumes of data at scale. If you don’t pay attention to the details, small problems can become large problems very quickly. Sometimes people can get overwhelmed by the size of problems — it’s all about the details and understanding their effects. Every great engineer I’ve met has been a collaborative team player . It doesn’t matter how smart someone is—no one can do it all themselves. It’s critical that teammates work to build up those around them. Honestly, it’s hard to say, but I know I’ll be patient . I really want to get to know the people on the team, understand existing tools and software systems, and assess areas needed for growth. I think it’s important to respect and appreciate what’s come before and understand the business objectives before plotting an engineering vision. While there’s always low hanging fruit, I like being thoughtful and strategic and working on things that will deliver long-term value. Tomatoes! I’m pretty easy, I like to scramble them with eggs 🍅 Want to work with Karney? Instacart is hiring! Check out our current openings in the Toronto Office. Instacart Engineering 33 Engineering Management Retail Technology Technology News Full Stack Infrastructure 33 claps 33 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-23"},
{"website": "InstaCart", "title": "view model state preservation using autoconnect", "author": ["Maor Korakin"], "link": "https://tech.instacart.com/view-model-state-preservation-using-autoconnect-d75ee791954b", "abstract": "Eng Open Source Data Science Machine Learning Android iOS To preserve and share the state of a stream (e.g. for rotations, dialogs), we can host the stream in Android’s ViewModel by using .replay(1).autoConnect() to share the stream’s state and a Relay to share the input. For example: Full example here . When disposing a connection to an Observable that is a not a ConnectableObservable , the entire stream is disposed. For example, when subscribing to a network request in an Activity, we lose the state on rotation (or any other configuration change): We may restore the view state in some way (e.g. with savedInstanceState ), but then on rotations we would create another network request (and cancel any network request if active). Hosting the stream in a lifecycle.ViewModel , we can have a stream lifecycle decoupled from the Activity’s. Using .autoConnect() we define the Observable to connect upstream (to the endpoint in this example) when the first Activity subscribes. When the activity disposes the connection, the upstream is kept alive. When the ViewModel itself is disposed (in onCleared) we dispose the upstream: To have user input as the origin of the stream hosted in a ViewModel, we can use a Relay. To extend the example above, on user requests to refresh we create new network requests: Most RxJava operators have some state that we may want to preserve. Some examples: Generally speaking, .autoConnect() (and its fellow operators ) allow us to host a stream and maintain its state in a separate lifecycle from its subscribers. Tying a stream’s lifecycle to a ViewModel’s, we can share its state across rotations. The state and input can also be shared with other views — for example between a dialog and an Activity, or between Fragments in a master-detail design. We can also choose to host streams in other lifecycles, for example for implementing a global in-memory cache, or to have a lifecycle tied to a specific module. Instacart Engineering 22 3 Thanks to Charles Durham . Android App Development Viewmodel Design Patterns Rxjava Lifecycle 22 claps 22 3 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-24"},
{"website": "InstaCart", "title": "predicting real time availability of 200 million grocery items in us canada stores", "author": ["Abhay Pawar"], "link": "https://tech.instacart.com/predicting-real-time-availability-of-200-million-grocery-items-in-us-canada-stores-61f43a16eafe", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Ever wished there was a way to know if your favorite Ben and Jerry’s ice cream flavor is currently available in a grocery store near you? Instacart’s machine learning team has built tools to figure that out! Our marketplace’s scale lets us build sophisticated prediction models. Our community of over 100,000 shoppers scans millions of items per day across 20,000 physical stores and delivers them to the customers. These stores belong to our grocery retail partners like Aldi, Costco, Kroger, Safeway, and Wegmans. Every time a shopper scans an item into their cart or marks an item as “not found”, we get information that helps us make granular predictions of an item’s in-store availability. This helps us set accurate expectations for out-of-stock items and recommend appropriate replacements for items likely to be out-of-stock. As a quick overview of how Instacart works, customers place orders online to be fulfilled from one of our grocery retail partners. A shopper picks items in the store and delivers them in as little as an hour. We have millions of grocery products listed on our website . Each product at a particular store is defined as an “item” and we want to know the availability of each item. If a shopper cannot find an item in the store, we label the item as “not found”. A not-found item is bad for every stakeholder in our marketplace — customers don’t get what they want, retail partners lose out on revenue, shoppers spend more time searching for them, and Instacart fails to deliver the best customer experience. Not-founds occur primarily due to two reasons: Availability : Instacart doesn’t own the logistics supply chain for products listed on its platform (our retail partners do), which makes it difficult for us as a third party to know whether a store has an item at a given time . We get regular updates (typically once a day) from our retail partners on the availability of all items. But items can sell out quickly within a day. We realized that we needed more granular data throughout the day — we needed to know the real-time availability of each item. Find-ability : Sometimes, due to our exhaustive product catalog, shoppers aren’t able to find every item available in the store. It could be because the items are moved to the front of the store for a seasonal promotion or items are paired to drive more sales. For example, chips are placed next to salsa instead of their usual aisle. Recommending easy-to-find items saves shoppers’ time and cuts down on replacements for customers. Hence, to infer real-time availability and capture find-ability, we built an item availability model that constantly predicts the availability of 200 million grocery items every 60 minutes. As we set out to build the model, we formulated it as a classification problem where every ordered item is a training example. In order to capture an item’s availability and find-ability, the model is trained to predict if the item was found by the shopper. Making this model work is a challenging problem, both from the perspective of training a model with good performance as well as the scale at which it needs to perform. Let’s look at the modeling aspects first. For each training sample of found/not-found, we use data from several months prior to that order to create features. All feature engineering is geared towards working well for tree-based models because this model uses XGBoost . We use three broad categories of features to train the model: item level features, time-based features, and categorical features. We build item-level features using an item’s past orders data and associated found/not-founds. Since we’re trying to predict if an item will be found, these should be the most important set of features. An item with a not-found in the last sixty minutes is very likely to be unavailable in store. Or an item with very low historical found rate is difficult to find and hence, is very likely to be a not-found. The most important features from this set are the item’s historical found rate, time since its last not-found, and the expected time to next not-found (based on the historical time between two not-founds of that item). We also use item availability data that retailers send us daily to create more features. We use time-based features like the time of day and the day of the week that the order was picked in store. We typically see better availability of items in mornings. ( Pro tip: Always go grocery shopping in the mornings! The shelves are re-stocked then.) We also have several categorical features (i.e. identifiers for the store, product, retailer, department, aisle, brand, region, etc) which could be used directly in the model. But, some of these have extremely high cardinality (number of unique categories). Cardinality is into millions for product identifier, and into tens of thousands for store identifier. Using these as one-hot-encoded features leads to ineffective learning and they are inefficient from scaling perspective as well (OHE features can blow up data size and model training/scoring time and we need to score 100s of millions of items as frequently as possible!). Also, training embeddings is probably not a good idea given the cardinality of millions. For the model to learn proper embeddings, each category will need to have sufficient samples in training data which will again blow up the training data size. We fixed this problem by doing something simple which also fixed another important issue explained below. While many staples are purchased over and over again, there always are items in the long tail that have been sold maybe once in the past 6 months. Sparse order history leads to weak or nonexistent item-level features. And having a population of such items in training data is definitely concerning as the most important feature set doesn’t work for them. In addition to high cardinality and long-tail item problems, features described above still don’t capture a lot of things like efficiency of supply chains, store-specific restocking patterns, product seasonality, products being discontinued, etc. Getting granular data on these is next to impossible, but we do have explicit data on found-rate of items which is a direct result of these and possibly other factors. We, therefore, use found rates at the granularity of item metadata (such as product, brand, region and their combinations) for this purpose. We do something similar to mean encoding for all categorical features and their combinations. In mean encoding, the categorical value is imputed with the mean value of the dependent variable for that category. The mean of the dependent variable in our case is found-rate and instead of using found rate from within the training data, we use historical found rates. For example, in the store identifier feature, the identifier is replaced with the historical found rate of that store, converting it into a continuous feature. Since these features aren’t dependent on item’s order history, but on its metadata’s order history, these are well-populated for tail items. These features significantly improved the model for tail items and proved to be among the most important features. Specifically, the most important feature for the model is the found-rate of the item’s parent product aggregated over the region in which the item is being sold. This feature largely captures a product’s findability and how good it’s supply chain is in that region. A product with an inefficient supply chain will have low found rates across the region. This feature also captures a product being discontinued or going out of season. It will pick up low found-rate for a product across different stores and propagate this information to all of its items and give them low scores, even if the individual item hasn’t been bought before! Scale of scoring An item’s availability changes in near real-time as they get sold and restocked, and as such we want to predict availability as often as is possible. Sometimes training at scale is a bottleneck, but for this problem scoring at scale is the larger bottleneck. Hence, we spent more effort optimizing the scoring pipeline to allow it to score over 200 million items every 60 minutes. In this pipeline, about 130 features are created for each item and 10s of TB of data is processed every 60 minutes. The new scoring architecture that we built from scratch scores 15x more items, using 1/5 of the resources in 1/4 of the time. Below are a few things that helped us achieve this massive scaling: Performing complicated feature engineering in our snowflake data warehouse instead of python. Identifying and caching features which don’t change frequently — this helped us decrease our feature engineering time. Optimizing data transfers from the data warehouse to an AWS instance. Better parallelization of python scoring code. Quicker and efficient uploading of scores to Postgres table. We currently use item availability predictions in many ways across the product. One such use case is to decide items that customers are able to order. We hide items with very low availability scores and low relevance in search. We also use these predictions to route shoppers to stores with better availability of ordered items. We are just beginning to understand the factors affecting item availability. As we look to the future, we are always identifying better data sources which might drastically improve the model. Currently, we’re playing around with the idea of assigning a findability and availability score for each item for an improved customer and shopper experience. There’s lots more work to be done! Interested in working on such large-scale and high impact projects at Instacart? Check out our careers page at careers.instacart.com . Feel free to reach out with any feedback/questions through comments or mail me at abhay.pawar@instacart.com or message me on Linkedin or twitter . Special thanks to everyone who worked on this over the years to bring it to its current state: Shishir Prasad , Angadh Singh and Sharath Rao . Also, thanks to Jeremey Stanley , Rachel Holm, Tyler Tate and many others whose feedback helped make this post significantly better. Instacart Engineering 924 5 Thanks to Kaushik Gopal . Machine Learning Data Science Technology Artificial Intelligence Predictive Modeling 924 claps 924 5 Written by Data scientist @ StitchFix. Past ML @ Instacart. Columbia, IIT Madras alum. Blogger. Loves data. twitter/linkedin/gmail: @abhayspawar Instacart Engineering Written by Data scientist @ StitchFix. Past ML @ Instacart. Columbia, IIT Madras alum. Blogger. Loves data. twitter/linkedin/gmail: @abhayspawar Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-19"},
{"website": "InstaCart", "title": "varouj chitilian joins instacart as a vp of engineering", "author": ["Instacart"], "link": "https://tech.instacart.com/varouj-chitilian-joins-instacart-as-a-vp-of-engineering-ee4606c8ba3b", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Just this week, we welcomed Varouj Chitilian as our newest Vice President of Engineering. In his new role, Varouj will oversee the consumer and growth teams that build the products Instacart customers interact with every day. Varouj joins us following lengthy tenures at Google and Oracle. He’s managed and scaled engineering teams that built both consumer and advertiser-facing products. He spent years working on AdWords Frontend and Search Ads Quality, before jumping into the digital payments space, where he managed teams behind Android Pay and Google Wallet. We sat down with Varouj to chat frontend engineering, data wrangling, and building out a technical organization. Check out the interview: So you started your career as a front-end engineer. What are some lessons you learned early on that you’ll never forget? Frontend engineering is under appreciated. Both the time it takes to do a great job on UI, and the complexity of building great, performant, and consistent UI is constantly underestimated. On mobile platforms, this is exacerbated — does the animation work on different screen resolutions, different Android OS’, or on low-end devices? What happens when there’s a notch? Making great UI is difficult, doing it at scale in a large application is even harder. When you mess up, the results are glaring. And when things look perfect, it’s taken for granted. At the end of the day, though, it was always gratifying to be able to point to a checkbox and tell your friend — I put that there! Why the jump to Instacart? Instacart has a large, loyal user base and hundreds of retail partners, but the company is at an early enough stage that an individual engineer can make his or her mark. That’s what really excited me about the company. At a larger company, best practices are written in stone, infrastructure has already been scaled, the machine learning infrastructure is in place, and the frameworks are widely used. Basically, all the fun stuff is already done and in the books. At Instacart, I’m excited to help shape these core pieces of process and technology. So far, the brisk pace of development and change has been exhilarating. As an experienced engineer, you’ll get the unique opportunity to scale the company’s technical infrastructure, frameworks, and platforms to accommodate a large growth in customer traffic. You will decide what technologies the company uses and how the technologies are configured to find the optimal architecture for reliability and engineering productivity. You will be integral in designing and upholding processes that promote great coding practices, and ensure the service is running like a finely tuned Ferrari. As a growing engineer, you get the chance to master software engineering. And you can make an impact on day one — no onerous development process and waiting for three weeks to push code to production. Everything you work on will be critical to the company. There are no projects that aren’t absolutely necessary for the growth of our consumer product. Every line of code you write will count! What engineering challenges are unique to Instacart? An Instacart order can be scheduled well in advance, or delivered in as little as an hour. To accomplish this it is optimal to have perfect information — to know with 100% accuracy what products are available to the store, what shoppers are available to deliver the orders, and what the user wants if items in their order are not available. The unique engineering challenge for Instacart is being able to fill these orders in near real-time with imperfect information. Tons of variables are at play in our technology chain — retail inventory, item pricing, shopper availability…even traffic conditions. Getting this right takes a level of reliability in infrastructure, clarity of product, and sophistication of machine learning that is rare…even at the largest tech companies. It is week one — you’re working your way through orientation — what’s the first project you’re going to work on once you’re settled? To get to know the codebase, I’ll probably run some experiments to test some ideas I’ve thought of as an Instacart user! In the long-term, I’m excited about not only building great products but also building out a strong engineering culture. As Instacart grows, one of my personal goals is to build out a world-class engineering team and make sure Instacart becomes a top-tier technical brand. Want to work with Varouj? Instacart is hiring! Check out our current engineering openings . Instacart Engineering 48 1 Front End Development Mobile App Development Technology News Software Development Engineering 48 claps 48 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-14"},
{"website": "InstaCart", "title": "the team behind mightysignal joins our hacker org", "author": ["Instacart"], "link": "https://tech.instacart.com/the-team-behind-mightysignal-joins-our-hacker-org-6f046f0b4da8", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instacart’s Hacker Org just got bigger. We’re pleased to welcome the team behind MightySignal to our engineering family. Over the last three and a half years, this accomplished group of backend and full stack engineers built out a mobile applications index that provides crucial business intelligence signals to B2B sales teams. And now, this team of six will work on something new at Instacart. As the newest members of our Growth team, they’ll build customer engagement-focused product features that delight new and existing Instacart customers across the U.S. and Canada. We’re proud to welcome Shane, Jason, Dawn, Osman, Marco, and Matthew as our newest Carrots! Want to join our growing Hacker Org? Check out all of our current engineering openings on our careers page . Instacart Engineering 126 Software Engineering Mobile App Development Technology News Engineering Technology 126 claps 126 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-01"},
{"website": "InstaCart", "title": "using kotlin extensions to make the android sdk friendlier", "author": ["Colin White"], "link": "https://tech.instacart.com/using-kotlin-extensions-to-make-the-android-sdk-friendlier-c355ec42d3cd", "abstract": "Eng Open Source Data Science Machine Learning Android iOS It’s no secret that the Android SDK has a few rough edges. Imagine that you’re new to Android and trying to display an image. You’ve already added the image to your res folder and now you need to get a Drawable . You write out resources.getDrawable() only to find it’s deprecated and the function it’s replaced by is only available on Lollipop and above. What do you do if you need to support pre-Lollipop? If you’re like me a few years ago, you end up writing your first (of many) if (SDK_INT >= LOLLIPOP) blocks. Ideally , I would know about ResourcesCompat , AppCompatResources , or ContextCompat which act as Utils classes to bridge the gap between platform API changes. However, these classes aren’t nearly as discoverable as having Android Studio autocomplete getDrawable() . Kotlin extension functions allow for better discoverability and more fluent APIs vs. traditional utils classes by “adding” new methods to existing classes. Android Studio will autocomplete these methods and they aren’t tied to a specific version of the Android platform SDK. Additionally, their implementation can be changed as the platform evolves. This is why Google recently introduced Android KTX . The set of libraries seek to make the Android SDK more intuitive by leveraging Kotlin language features (such as extension functions). With that said, I want to share a few of my favourite Android extension functions that are not part of Android KTX: Simplify interoperating between Dp and Px . E.x. 2.pxToDp() 4.dpToPx() Calling back to the example above, these extensions optimize for discoverability. They only act as aliases for the Android X compat functions. E.x. context.getColorCompat(R.color.green) context.getDrawableCompat(R.drawable.vector) tint can be used to tint drawables across all API levels by delegating to Android X. E.x. getDrawableCompat(R.drawable.circle).tint(Color.Black) Simplify the common operation of inflating a child view without adding it to its parent. E.x. val view: CustomView = parent.inflate(R.layout.custom_view) By default, Kotlin’s lazy function uses a thread-safe lock to ensure the initialization code is only run once. For variables that are only accessed from a single thread (like the main thread), we can avoid using a lock to improve performance. unsafeLazy is an alias for this implementation. E.x. val green = unsafeLazy { getColorCompat(R.color.green) } Traverse the Context tree to find the nearest Activity. Open a web page using Chrome Custom tabs or fallback to another browser if Chrome is not available. Note: You’ll need to add androidx.browser:browser to your Gradle build file. E.x. context.openWebPage(\"www.instacart.com\") I’ve compiled all these extensions here , if you’d like to add them to your project. Also let me know your favourite extension functions in the comments! Find this interesting? Come join us; Instacart is hiring! Feel free to reach out to me on Twitter ( @colinwhi ) with any questions/comments/feedback or to just to say hi 👋. Thanks to Jon Hsieh for editing this article. Instacart Engineering 180 2 Thanks to Jon Hsieh . Android Kotlin AndroidDev Software Development 180 claps 180 2 Written by Android @ Instacart Instacart Engineering Written by Android @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-23"},
{"website": "InstaCart", "title": "a q a with mark schaaf instacarts first cto", "author": ["Instacart"], "link": "https://tech.instacart.com/a-q-a-with-mark-schaaf-instacarts-first-cto-3d0e14c9b870", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Big news — we’re excited to welcome Mark Schaaf to the Instacart family as our first-ever CTO . Mark comes to us from Thumbtack, where he was also CTO; and Google before that, where he led a team of engineers within Google’s mobile display advertising division. He’s got a passion for building out powerhouse engineering teams…and that’s exactly what he’s going to do here at Instacart. We’re aiming to double the size of our Hacker Org by the end of 2019. At the end of Mark’s first week, he sat down with Apoorva (our CEO) and the whole team for a lunchtime fireside chat. We talked about a little bit of everything — bagging groceries, WAP phones , and (of course) committing code. Read the full Q&A: Give us a bit of personal history. Where have you been and how did you get here? It all started at my first job working at a grocery store. I grew up in Memphis, Tennessee and started at a regional grocery store — checking, bagging, stocking. So that’s where I started…CTO was definitely my next play. In all seriousness, I came to California, and after working at a few tech companies, I was hired as the second engineer at AdMob. When we got started in mobile ads, the industry revolved around WAP phones in Europe. We had ad liquidity in the marketplace, and once the iPhone came out, we started making ads for smartphones. In 2008 when the app store launched, we were the first SDK in the app store. Three and a half years later, AdMob was acquired by Google. I ran mobile ads at Google, grew the team to a pretty significant size across four sites, and learned a lot about engineering at scale. It was a great 5 years. Startups are my passion, though, and I wanted to go back to scaling up a team, so then I jumped over to Thumbtack. I got in pretty early and saw the team through a rapid growth period…and now I’m here at Instacart, another marketplace startup. I can’t wait to grow this team. So why Instacart? First of all, it’s a product that I use — I’ve been using Instacart since 2014. And when you look under the hood, Instacart’s engineering challenges are really complex. We’ve got a massive, four-sided marketplace; and then we have to run a logistics operation on top of that…that’s a lot of engineering. You’re new — finishing up week one. How do you like to onboard new engineers? So we need to double the size of our engineering team by the end of 2019. When you scale up teams, you need to streamline your onboarding as much as possible. I’m a fan of mentorship programs for new employees—programs wherein they can meet veteran team members and learn from them quickly. I’m also big on lunch-and-learns and forums—they’re a great way for new employees to learn why historical engineering decisions were made. Most importantly, people need to check in code right away on day one. You learn so much about an engineering org when you start coding right away. I can’t emphasize this enough. Describe your ideal software engineer. A lot of great software engineering boils down to a few characteristics: being humble and thoughtful; being a good listener; being a good partner; and importantly, being a good citizen. I value folks who step up to do the things not everyone wants to do. I think we should celebrate refactoring as much as a product or feature launch. It’s important to celebrate the small wins alongside the big ones. Now that week one is over, what’s your top priority moving forward? Well, I’ve already checked in code — so that’s a start. I’m here because of the people, so moving forward, I’m trying to meet everybody over the next month or so. And after that—scaling up the Hacker Org! Want to meet Mark? Instacart is hiring! Check out our current openings . Instacart Engineering 44 Software Development Machine Learning Engineering Logistics Technology News 44 claps 44 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-15"},
{"website": "InstaCart", "title": "free hackathon vote tabulation using google forms kotlin", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/free-hackathon-vote-tabulation-using-google-forms-kotlin-3c7b7080ea", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We recently held our semi-annual hackathon at Instacart — the Carrot Wars 2018! In putting this hackathon together, I noticed a pretty blaring gap — there wasn’t a simple (and free) online service that would quickly tabulate the results for a hackathon event. We looked around and found some nifty options, but most of them were a tad bit too expensive for our liking. They also were not setup for a single event use or required a monthly subscription. There other usage restrictions, too — max vote count, concurrent user count, etc. You’d think there would be at least some option out there, given how popular hackathons are these days. We did some cursory searching but couldn’t find something that would work for us. My co-organizer , admittedly wiser about such things, made it super clear to me, “No KG, we ARE NOT building our own hackathon voting website 2 days before the event! You have more important things to do!”. So I set out to do exactly ( ½ of) just that. If you’re on a time crunch and just want to use this post for a hackathon you’re about to run, jump to the “ How to use this form for your hackathon ” section below. For the juicy details, please continue reading! I wasn’t too thrilled about building an online voting form website (hi 2018!) so instead I chose to just leverage Google Forms . Google Forms is pretty easy to use, can handle a bunch of users slamming your form at the same time, provides a decent enough UI and is ridiculously easy to create and get going. The part I found more interesting about this process was the result tabulation. I could have probably gotten Google Forms to dump the results in a Google Sheet. But my excel/sheets expression fu was not strong enough to conconct the required mathematical expression to get the results I wanted. So instead of a seizure inducing mathematical sheet expression, I did what came more naturally to me and wrote a script in Kotlin to tabulate the votes and println the results nice and cleanly 😎. I chose Kotlin cause I’m an Android developer and we love Kotlin here at Instacart. Interestingly the hackathon also coincided with the release of Kotlin 1.2.50 where the Kotlin scripting support was supposed to have been improved. There’s something very satisfying about running a script in a terminal prompt like you would with bash, but without all the unpleasantness of the bash syntax and all the pleasantness of the Kotlin language. The hackathon had 5 categories (based on our core company values) that folks could participate in On presentation day, we sent out a google form to all hackathon attendees with the list of final projects. They would pick the 1st, 2nd and 3rd places for each category Google Forms allows a pretty convenient way to export these results as a CSV file This Kotlin script ingests the CSV, tabulate the results, does some weighting math and prints the results for each category I can’t recommend enough this Google Forms add-on called formRanger . formRanger allows you to populate your form from a simple google sheet. This was particularly useful for us because we had folks signing up right at the very last moment, wanting to pitch their hackathon project. So up until the very last presentation, the project list was in flux and so the form needed to constantly be modified (we had 5 categories and about 30 pitches. That’s 150 entrees that we would have had to keep consistent — no bueno). With formRanger setup this was a single list in a simple google sheet. You want your questions to be of type multiple choice grid . This way you can link a project (rows) to a position i.e. 1st, 2nd and 3rd place (columns). There’s another important option you want set for each question — “ Limit to one response per column ”. This will make sure you don’t have more than one 1st, 2nd or 3rd place. You can find most of the instructions for setting up the Kotlin script in the github project . tl;dr: In your google form, you should see a tab called “Reponses”. From there you should be able to “download responses (.csv)”. Google Forms has a pretty decent visualization, which might just work for you, but I wanted to add some more sophistication in the tabulation of the results like weighting and maybe filtering based on categories. Basically, moving this to an actual program opens up a whole bunch of possibilities. From a terminal prompt run the command like so: The very first time you run the script, it’ll take sometime as it pulls the necessary dependencies via maven. Give it some time; subsequent runs should be pretty snappy. Here’s what it should look like: Haters hate all you want, but those are my movie choices ! Demo Google voting form (go ahead and vote for your choice!) Sample CSV responses file Google sheet used to populate above form Source repo for Kotlin script that tabulates and prints out the results Want to take part in our next hackathon? Come work here ! Instacart Engineering 85 1 Thanks to Jon Hsieh . Kotlin Kscript Culture Software Development AndroidDev 85 claps 85 1 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-11"},
{"website": "InstaCart", "title": "leveraging elastic demand for forecasting", "author": ["Houtao Deng"], "link": "https://tech.instacart.com/leveraging-elastic-demand-for-forecasting-6278b45f805f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Demand forecasting can help businesses estimate future demand and plan supply. When the actual demand and planned supply mismatch, demand shaping tactics such as pricing can be used to reduce the imbalance. In this article, we propose to consider elastic demand (demand responsive to demand shaping) in the forecasting process. We present a method to reallocate the elastic demand in historical data so that the variance is minimized, leading to more effective forecasts. Take Instacart as an example. We forecast hourly demand using historical data, as illustrated in Figure 1 (left). The following steps can be used to forecast demand for a future hour (e.g., 10–11 am): Extract the historical demand for the hour. Figure 1 (right) shows the 10–11 demand in the past 10 days Use a time series model to forecast demand for the hour Although time series models can capture trend and seasonality in demand, there can still be a large unexplained variance. For the 10–11 demand time series shown in Figure 1 (right), there is no obvious seasonality or trend, and the unexplained variance is high. Figure 2 shows a box plot demonstrating the demand variability at each hour from the data in Figure 1. The impact of variance on forecasting and supply planning is illustrated as follows. Table 1 describes two historical demand patterns (each with a probability of 50%). Pattern 1 has 10 units of demand at 9–10 and 30 at 10–11, and pattern 2 has 20 units of demand at both hours. Consider two forecasts, one using the average of the two patterns at each hour, and another just using pattern 1. Assume one unit of supply can serve one unit of demand, and the cost of losing one unit of demand or holding one unit of excessive supply is 1. The expected costs for the two forecasts are shown in Table 2. Both forecasts cost 10 in total. The variance makes a zero-cost forecast seemingly impossible. In some situations, certain customers are flexible on the product/service options, and so demand shaping strategies such as pricing can influence their choices. The demand that is responsive to demand shaping tactics is referred to as the elastic demand . Note some historical orders were influenced by demand shaping. Based on the availability and pricing information customers saw (illustrated in Figure 3), we can infer the choices customers would have made without demand shaping. For the example shown in Figure 3, the “within 1 hour” and “within 2 hours” options are more expensive than the “1–2pm” option. We also know customers typically prefer faster deliveries. Therefore, for customers placing orders for “1–2pm”, there is a high chance that they could have chosen “within 1 hour” given the same price. In the rest of the article, we assume the amount of elastic demand is known and focus on how to optimize it. For the example in Table 1, assume 10 units of demand at 9–10 in pattern 2 is elastic and can be reallocated to 10–11. As illustrated in Figure 4, the new demand can be any integer value between 10 and 20 at 9–10, and between 20 and 30 at 10–11, respectively. With elastic demand considered, both forecasts have smaller expected costs and forecast 2 has a zero cost, as shown in Table 3. Consider an illustrative example first. Figure 5 (left) shows two time series ( referred to as demand series) each with 3 time slots. We want the difference between the demand series to be small. Assuming each time slot of the demand series has 10 units of elastic demand that can be shifted to its adjacent time slots, we can reallocate the demand as follows so that the two series become identical. First shift 5 units from T1 to T2, resulting in the series shown in Figure 5 (middle). Then, shift 5 units from T2 to T3, resulting in two identical series, as shown in Figure 5 (right). From this example, we can generalize the goal, input, and output as follows Goal: shifting the right amount of elastic demand to minimize the difference between the new demand series Input: historical demand series, and the amount of elastic demand Output: shifted demand series We propose the following formulation with a limitation that elastic demand can only be shifted between adjacent time slots. Part 1 is the demand at t-1 of the kth series. Part 2 is the percentage of demand reallocated from t-1 to t . The multiplication of Part 1 and Part 2 is the demand shifted from t-1 to t for the kth series. Part 3 represents the remaining demand at t after a certain amount is shifted from t to t+1. Therefore, Part 4 is the new demand at t of the kth series after reallocation. Part 5 is the estimate of demand at t for the new demand series, and Part 6 is the sum of squared errors for the estimates for all ts. Minimizing Part 6 can be considered as minimizing the overall variance between the new demand series. Part 7 penalizes the magnitude of shifting percentages, as there can be a cost associated with demand shaping. Part 8 defines the amount of elastic demand at t of the kth series. L (≤0) indicates the amount of elastic demand shiftable to the left time slot, and U (≥0) indicates the amount of elastic demand shiftable to the right time slot. Part 9 forces the estimate of demand at each hour to be non-negative. This is a convex optimization problem with a globally optimal solution. We applied elastic demand optimization to the demand series shown in Figure 1 (left) with the percentage of elastic demand at 10%, 20%, 30%, 40%, and 50%, respectively. Here we use the same percentage of elastic demand for the time slots of all series. The original and new demand series are shown in Figure 6. With elastic demand considered, the variance of the new demand series at each hour becomes smaller, which would make forecasting more effective. Figure 7 shows the average hourly demand variance for each group of demand series in Figure 6. As expected, a larger amount of elastic demand leads to a smaller variance. However, the first 10% elastic demand produces the largest variance reduction. A large demand variance can result in a high cost due to lost sales or excessive supply. We showed historical elastic demand can be reallocated to reduce the variance, hence making demand forecasting and supply planning more effective. As Instacart has hourly delivery windows, we focus on hourly demand with a daily cycle and assume the elastic demand can be reallocated among the adjacent hours from the same day. If there is a day-of-the-week seasonality, the proposed method can be applied to the demand series from different days of the week separately. To extend the method to handle daily demand with a weekly cycle or monthly demand with a yearly cycle, the formulation may need to be changed to allow the elastic demand to shift between the end of a cycle and the start of the next cycle. Ganesh Krishnan, Ji Chen, and Dong Liang also contributed to this project. More details can be found in the technical report . 🥕 Interested in joining us? Apply here . Instacart Engineering 176 2 Thanks to Ji Chen , Ganesh Krishnan , Mathieu Ripert , Sharath Rao , Montana Low , and Jagannath Putrevu . Data Science Supply And Demand Demand Forecasting Elastic Demand 176 claps 176 2 Written by Machine learning @Instacart softwaredeng@gmail.com Instacart Engineering Written by Machine learning @Instacart softwaredeng@gmail.com Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-06"},
{"website": "InstaCart", "title": "know your carrots 7 john meagher", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/know-your-carrots-7-john-meagher-7c4b1819faed", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher , Google Play or Tunein Hi! Today I invited John Meagher, engineer of our Search Infra team who, together with our previous guest Jon, makes sure our search works so you can find those tasty avocados and your favorite ice cream. John will talk about dealing with technical debt, problems working on resources shared across big organization, and our tool ESHero and how it helped the Search team double down on eliminating tons of inefficient queries. In his free time, John enjoys playing golf, so you can meet him at the course on the weekends. Till next time! Instacart Engineering 6 Podcast Culture Instacart Technology Software Engineering 6 claps 6 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-13"},
{"website": "InstaCart", "title": "scaling at instacart distributing data across multiple postgres databases with rails", "author": ["Doug Hyde"], "link": "https://tech.instacart.com/scaling-at-instacart-distributing-data-across-multiple-postgres-databases-with-rails-13b1e4eba202", "abstract": "Eng Open Source Data Science Machine Learning Android iOS I love a good challenge and at Instacart we have lots of exciting challenges. I work on the Shopper Success team, tasked with making shopping and delivering groceries to our customers a good and reliable gig for our shoppers. Part of that responsibility is ensuring the stability of all of our fulfillment systems, so that shoppers can work. We have a lot of data, which is a wonderful problem to have, but also rapidly multiplying as we grow. Our code runs in different apps and services, but we still relied on only two databases, our primary and our item catalog, and were quickly running out of space. The first step of architecting is always to figure out what we’re optimizing for. We’ve enjoyed great success with Postgres and saw no reason to depart. It has given us high performance and has an incredible array of datatypes and query functions that have been extremely useful. Our most urgent priority was finding more space. We were up against the clock and at high risk of hitting the 6TB limit (raised to 12TB before we finished) of Amazon RDS. We had a number of parallel initiatives to potentially address the issue, with the expectation that at least one would resolve the issues for the medium term and a combination would provide longer-term support. One way to improve the robustness of systems is to isolate them, limiting the frequency of incidents and breadth of impact. At Instacart, we serve a four-sided marketplace. We serve the customers and the shoppers who pick and deliver groceries to customers, but we also work closely with retailers and consumer packaged goods companies. We have different SLAs in each area of the business to cater to their needs. With our shoppers, there is a moral imperative to provide a high stability. If our fulfillment (shopper) systems go down, the company can’t deliver orders, but more importantly, we would be letting down those who depend on Instacart for income. Obviously we wanted to maintain high performance, but were not looking to drastically alter our healthy performance. Replicas already handle most of the read load and our write load was manageable. After examining the hundreds of tables we have, we found only a few dozen were needed by multiple domains. These tables often have a number of joins and complex queries that make an API inappropriate. For example, when customers place orders, the order is written to the Customers database in the Customers domain, where it is needed so customers can access and modify their orders. However, some of the data about this order is critical to the Fulfillment domain. In order to provide isolation, we needed to copy the data somewhere. Postgres is tremendously powerful and we saw no reason to move away from it, especially with horizontally-scaling options like Aurora available. We decided to create a dedicated Fulfillment database that would house all of the data needed to complete an order and share the necessary data between databases. This allows complete isolation and greater ownership of relevant data. It’s pretty easy to connect to multiple databases in Rails. The example above shows how we can have models within a single app that have different connections in ActiveRecord. The line include FulfillmentRecordConnection changes the connection of a subclass while inheriting all of the functionality from the parent class or reopened class. When managing data in multiple places, we need to make sure they stay in sync and do not diverge. A change applied to one place needs to be applied elsewhere, eventually — more on this later. We looked at all of the tables needed by multiple domains and found that most had all of their writes originating from a single domain. Having writes from multiple domains turned out to be a good indication for us that the schema was wrong and it was time to split the table into separate entities. This was particularly convenient because master-slave synchronization is far simpler than master-master and the whole host of race conditions that follow. For each shared table, we determined the owning domain where the table and writes would live. Other domains needing this table would have what we call a “copied table.” We were careful not to call them replicas or slaves. Our copied tables are read-only by the production Postgres user to avoid accidental modifications, as well as with a basic application-level write protection. Here is a simplified implementation of our CopiedTable . Simply, it prevents accidentally modifying records and makes it easy to take a generic payload of changes and apply it to a model. To use it, we include CopiedTable on models that are copied. Postgres has a number of database-level mechanisms for sharing and replicating data. Replicas are great for isolating read load, but do nothing to help with space or write isolation. We experimented with foreign data wrappers (FDW) coupled with materialized views to cache foreign data. The experiment wasn’t unsuccessful, but we found materialized view refreshes to be potentially problematic at scale. By replicating at the application-level, it gives us greater logical control. Two different apps might care about different data. Perhaps the Customers domain needs a full record of every order, but the Fulfillment domain only needs information to deliver current orders. The Fulfillment domain can ignore updates to past orders or information about which coupons were used. The performance overhead was a trade-off that made sense for us. To facilitate this, we publish changes over Hub, our pub/sub implementation. We can make a model publish its changes with include PublishChanges . Then we have the CopiedTable module to help receive updates. This snippet of copied table shows the apply_published_changes class method, which allows our publish consumer to trivially call Model.apply_published_changes(params) to create, update or delete from the copied table. The consumer can ignore rows the domain doesn't care about (e.g. past order totals). Eventual consistency is a well-defined concept that basically means a replicated data source will eventually have all of the data that the original data source has, but there will probably be a lag. This is a common trade-off that allows high performance of both reads and writes. If two databases are atomically coupled, then slow performance on either will negatively affect the other. This pattern ensures that any performance issues on the Fulfillment domain don’t prevent customers from placing orders, for example. There’s always a risk data will get out of sync or fall significantly behind. If a bug prevents publishing changes or someone trips over a power cord, we need to be sure that we identify and resolve differences with a sweeper. We have a regularly scheduled job that runs for each copied model and ensures that all updates exist. We can query the first database by updated_at , which we index out of habit, and the second database by id's from the first database. When performed in batches, this approach has worked well. Great, so we have a new architecture in mind, but we can’t simply merge a few PRs and have terabytes of data moved without downtime, errors, or loss of data. Every data migration is different, depending on volume, access patterns and sensitivity. After moving hundreds of tables, we developed an effective process for moving data without negatively affecting live systems. Before moving a table, it’s important to find all joins, triggers, and other related entities that will need to move. I found a few tricks that worked well. First, I simply searched both model and table names in the codebase and moved what I could easily find. Then, on a branch I removed the model and table I wanted to move and let our tests find queries I had missed. Last, I used our profiling tools and logging to examine queries that were run. Our initial state for most tables was multiple domains reading from and writing to a single domain’s database for a particular table. In order to have a clearly defined access pattern with a reliable replication and validation, we wanted a single domain to own the table. Moving this code helped us find lingering or dead code from the old monolith. In every case we saw, when multiple domains were writing to the same table, it was a code smell and we decided to split up or refactor those models. The next step is to create the schema and model in the new database. Copying part of the structure.sql or schema.rb is usually a good starting point. Be sure it indicates in code that the model is not ready for primetime yet or you may have someone querying old/bad data. I found it worked best to create a model with no validation that was used to copy whatever was in the original table. One of our engineers wrote a great tool, called pgsync , for transferring data between postgres databases. We use it most often for pulling obfuscated staging data to local dev environments, but it can also work between production databases. Depending on how much data you’re moving, it could take a while. Perhaps make a note of when you ran so you can sync future updates from that point forward. pgsync the_table --in-batches --from $DATABASE_1_URL --to $DATABASE_2_URL --sleep 0.1 --to-safe As we prepare for “cutover,” it’s important that the tables are in close sync. If we cutover and there’s a one minute lag, we’re at risk of losing that data. Granted, there are ways to prevent, but an easy way to reduce issues stemming from race conditions is the have really short races. This setup will write to the second database in the callback for writing to the first. Note: self.attributes includes the primary key, so the new row will be created without incrementing the auto-increment. This is something you could consider doing manually after each create, perhaps with: Now we have all of our ducks in a row and it’s time to make the jump. Make a single PR that changes the database of the model, creates a new model for the old database, and dual-writes the other direction. You should also disable the sweeper just before the cutover is live and re-enable it the other direction just after. I recommend sticking with synchronous during the cutover to facilitate a cleaner rollback. Query both databases and verify the data looks good. I found a simple query like was remarkably effective in ensuring the source and copied tables contained the same rows. Once we’ve gained confidence the cutover has gone smoothly and we will not need to rollback, it’s time to make our synchronization asynchronous, so we can gain the real benefits of isolation. Start publishing and consuming changes before removing our synchronous dual-write. Once we’re asynchronously writing, then we have decoupled the two databases! Writing to the first database is not dependent on the second database also being healthy and reads are on the local database. In some cases we no longer have a need for the table in the old database. Dropping tables can feel like Russian roulette, but there are some tricks to tame it down. Archiving is basically always a good idea. If you can tolerate failed queries if you’re wrong, revoking permissions is a pragmatic way to verify a table isn’t used. I usually write out the grant statement to restore permissions and have it in a text editor so I can undo as quickly as possible if needed. If your system cannot tolerate a minute of failing queries, a more challenging, but safer approach, is to reset the table’s statistics. Make sure that tracking is enabled, reset that table’s and wait. When you check back, you should see that the access counts are zero. More on this in the Postgres docs . Instacart has a lot of data and has adopted an application-level data pump pattern for replicating data between databases. This approach is generic enough to work for all tables, but customizable enough to allow for denormalization, filtering, and other performance optimizations. We start by moving all of the writes to the eventual owning domain, synchronously dual-writing to both databases, cutting over the source of truth, and then replicating asynchronously. Hopefully my experience re-architecting our production data is helpful to you. Have ideas on how we could do better? Great, we’re hiring ! Come join as we transform the grocery industry! Instacart Engineering 337 Thanks to Montana Low and Kaushik Gopal . Ruby on Rails Scaling Software Development Software Architecture Big Data 337 claps 337 Written by Engineering @ Instacart Instacart Engineering Written by Engineering @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-23"},
{"website": "InstaCart", "title": "episode 6 starring dominic", "author": ["Viktor Evdokimov"], "link": "https://tech.instacart.com/episode-6-starring-dominic-c944a01f9eff", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Subscribe in iTunes , Stitcher , Google Play or Tunein Hi, this is Episode 6 of Know your Carrots. Today I invited Dominic, who is a member of our platform team. Dominic is from Minneapolis, but has also lived on both the East and West Coasts and even Dublin, Ireland! He started his career in game development, and still finds it extremely interesting. He shared several insights on gaming and gender diversity in the industry, the importance of saying “NO”, and how to start and advance your career. Also my favorite quote from interview: Don’t do things that aren’t a good use of you and your time. I think that’s huge, especially as an engineer. Everybody wants your time, everybody wants your skills. So it becomes very important that you choose wisely and that you are in control of you. As always, big thanks to our amazing Forge team for all the help with the podcast. Instacart Engineering 3 Podcast Culture Software Development Technology 3 claps 3 Written by Senior Software Engineer at Instacart Instacart Engineering Written by Senior Software Engineer at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-30"},
{"website": "InstaCart", "title": "building instacarts view model api part 1 why view model", "author": ["Peter Lin"], "link": "https://tech.instacart.com/building-instacarts-view-model-api-part-1-why-view-model-4362f64ffd2a", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Two years ago, the customer engineering team at Instacart began working on a new API for our customer-facing apps. We took a look at our existing API, which was a traditional data API, and found that we were implementing a lot of domain logic on the client-side, resulting in a lot of cross-platform duplication. This meant that features took a longer time to build and to ship, were more prone to bugs, and weren’t consistently released to all client platforms. We wanted to take the opportunity to find an architecture that will give our engineering team the biggest leverage and help us move faster. Ultimately, we settled on a view model architecture , and structured our new API to be mostly view model based. The view model architecture reduces inefficiencies and allows us to take better advantage of our engineering team. Through the migration, we’ve experienced an increase in engineering productivity, along with improved consistency, performance, and stability. We’ve also learned a few lessons along the way. In this blog series, we hope to share these with you. In part one, I will briefly go through the concept of a view model API, how it might be better, and an example of what we built at Instacart. Let’s begin at the start. Instacart’s legacy API was a standard RESTful data API, with the API response largely mirroring the data model. The client apps consumed this API and built out lots of logic on the client side to determine what to render and how it is rendered. This is represented in the graph below. At first glance, this approach seems reasonable. Indeed, this is how Instacart’s API was structured for years. The API response was simply a JSON dump of what was in the database, leaving most domain logic implementation to the client apps. Domain logic, or business logic, is the logic we have implemented to run our business. This includes all kinds of things, such as determining which items are available, calculating item pricing or order totals, and so on. With the data API approach, notice that domain logic is implemented many times, once for each client platform. Implementing business logic on the client means we have to implement the same logic on all three platforms, resulting in lots of duplication, which is inefficient, and prone to bugs and inconsistency. What’s worse, you typically end up implementing business logic on the server side anyway. Consider totals; you’d always want to implement the logic on the server side, as you can’t simply trust amounts provided by the client. So in reality, the same domain logic ends up being implemented four times. Not only does this take up a considerable amount of engineering time, but engineers working on each platform are likely different people on different teams, increasing the likelihood that the domain logic is implemented inconsistently or worse, incorrectly. Duplication of code and effort is not a good thing. However, cross-platform duplication is common, primarily because duplication is less apparent when spread across multiple engineers, teams, platforms, or languages. If a single engineer worked on all platforms, they would almost certainly not implement the same code four times. This hidden inefficiency is a big tax on engineering productivity. We can solve this by doing some cross-platform DRY ing and move all domain logic to the backend, eliminating domain logic on the client side. Since we have moved all the domain logic server side, the API response becomes primarily view based. Clients no longer need data to implement domain logic, and can directly render a view with the view data API response. With the view model approach, the API response models the view , rather than the data . As shown above, domain models are transformed into a View Model , which determines how the data is presented. This view data is then returned to the clients via the API, which clients will use to render the view. This is similar to Martin Fowler’s Two Step View pattern, with Stage 1 being view model transformation, and client rendering as Stage 2. For example, consider a list of items: Here is a screenshot of a list of items rendered on the client, for the Produce department. With a data API, you might return a Department model and a bunch of Item models. Clients would then implement domain logic to figure out how to render the department, which items belong to the Produce department, and how to sort the items appropriately. With a view model, the API response models the view, so the response is modeling what the client apps render and what the customers see . In this case, we define a view model called Items List , which has a title, an array of pre-sorted items, and a label action for the “View more” link. On the server, we first transform a Department and its Items into an Items List , which is then returned to the clients via the API. With the Items List data, clients can render a view for a list of items, without even knowing what a Department is. Since there is nothing that ties an Items List response to a Department , we can easily reuse the Items List for Aisles , Related Products , or any arbitrary list of items, such as the “Your Items” list above, which is a list of items that I’ve previously ordered. The reusability of view models, among other things, provide a significant advantage over the traditional data API. One feature that demonstrates the benefits of the view model is item pricing. Over time, we have built up a variety of pricing variants, such as regular pricing, sale pricing, and loyalty pricing. In addition, we also support various promotions such as coupons, CPG promotions , and complex retailer promotions such as buy n get n free (BOGO), buy n get x% off, or mix & match promotions across a family of brands and products. With a traditional data API, it would end up looking something like this. Each pricing variant with its own API response, and each client has to implement business logic on the client side in order to render correct pricing. This is far from ideal. Every time we add a pricing variant or promotion, it will have to be implemented four times. Then, each updated app has to go through the usual store review and release cycles, taking weeks for a new feature to roll out. There is also significant overhead, since more documentation, sync meetings, and testing will be required to ensure business logic is implemented correctly and consistently on all platforms. Wouldn’t it be great if we could just ship new pricing variants and promotions to all clients with no client-side work? Enter the pricing view model. Looking through how all existing pricing variants are rendered across client apps, we came up with a response that models the view for all pricing variants and promotions. An example pricing response is below: This item is both on sale and is part of a Mix & Match and Buy N get $X off promotion . Consider how complicated it would be to structure an API response to support this, and how much client work would otherwise be required for this to work. The pricing view model eliminates this completely, there is no domain logic required on the client apps for this otherwise complex feature. The logic is already pre-rendered on the server-side, client apps simply present what is returned in the response. Using the pricing view model, we can update the API to return the same pricing response for all pricing variants and promotions. We implemented the “Pricing” view model on the API and client apps after the coupons feature, and as a result, all new promotions below the dotted line were launched with no client-side work. Since the same pricing response is reused for these new promotions, existing client apps that support the pricing view model is able to support these new promotions immediately after the API is deployed, without going through any app updates. In our experience, the view model API approach has improvements in four categories: productivity, consistency, performance, and stability. Productivity The view model approach can significantly improve engineering efficiency, as it reduces duplicate effort across clients, and allows client app engineers to focus on building views rather than implementing domain logic. View models are also often reusable across features and, when reused, allows you to ship features across all clients with little to no client-side work. If you have three client platforms like us, this allows you to ship features with 1/4 the work, effectively making everyone a 4x engineer. Consistency View models improve consistency in three ways: Implementation consistency All of the domain logic can be abstracted away from the clients through the view model response, so this eliminates the possibility of inconsistent domain logic implementation across client apps. Design consistency View model responses encourage consistent design across UI components within an app and across all apps. It is far easier and faster to ship new features using existing view models than it is to create a new one. Feature-set consistency As long as the client app supports a particular view model response, the client can also support all future additional uses of a particular view model. This reduces the number of features that are not supported on particular clients due to lack of engineering resources, improving the number of features consistently available across all clients. Performance A view model response no longer includes the data required for domain logic, which almost always reduces your response payload. This makes your apps more responsive, especially for clients on a less reliable connection, as is often the case with mobile clients. In our experience, we have been able to reduce payload sizes of specific endpoints by more than 10x, which is a significant experience improvement for our customers. Stability A view model response can also greatly improve the stability of the client apps. Moving logic server-side eliminates a lot of client-side code, reducing the probability of bugs on the client apps. The best code is no code at all. Having all of the domain logic server side means you only have one copy of the code to write tests for, increasing the likelihood of having great test coverage and writing bug-free code. Overall, we have experienced significant benefits with a view model API, and it has been a win-win for our customers and us. The app is faster, more stable, more consistent, fully-featured. All of this achieved with higher engineering efficiency and productivity. We can move faster and break fewer things. In our experience, moving to a view model API has improved engineering productivity significantly. Many features that were previously “ web-only ”, are now available across all clients. We’ve also been able to ship many new features much more quickly, with little or no client-side work, and without going through the usual app update cycles. There are certainly many other ways to share code across clients, such as transpiling code or using something like React Native . I’m a big believer that engineering architecture should reflect your organizational structure. With significantly more full-stack and backend engineers than native app (iOS or Android) engineers, the view model approach strikes the right balance for our unique team size and makeup. It allows native app engineers to focus on creating great client apps with full native views, while keeping the API simple and straightforward, and not requiring any new frameworks. Over the last two years, we have taken the view model far beyond the examples used in the post. In part two of this blog series, we will introduce Containers and Modules , a page-based view model response, which almost our entire app is built upon. These new concepts build upon the idea of view models, expanding the benefits and increasing engineer leverage even further. I hope you have found this post interesting. For more posts like this, check out Instacart’s tech blog . We’re also building our engineering team: if you are interested in building great APIs, creating magical consumer app experiences, scaling infra, or anything engineering, we’re hiring :) My thanks to Dominic Cocchiarella, Min Kim, Dan Hsiao, Michael Scheibe, and Kaushik Gopal (with glasses) for reading a draft of this post. Instacart Engineering 283 3 Thanks to Kaushik Gopal , Michael Scheibe , and danhsiao . API Software Development Software Engineering Software Architecture 283 claps 283 3 Written by Engineering @ Instacart Instacart Engineering Written by Engineering @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"},
{"website": "InstaCart", "title": "enabling a seamless payment experience at instacart through marqeta", "author": ["Instacart"], "link": "https://tech.instacart.com/enabling-a-seamless-payment-experience-at-instacart-through-marqeta-9d0e6ef39ad3", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we are continuing to work hard to transform grocery shopping into a seamless and digital endeavor to suit today’s on-demand economy and hectic lifestyle. Since we began in 2012, we have expanded rapidly to be available to ~70% of North American households across 260 markets, partner with over 200 grocery chains (including 7 out of the top 8), and sell billions of dollars of groceries every year through our platform. As we’ve achieved this rapid growth, we have continued to invest in scaling operationally. One critical area we have focused is the ability to seamlessly onboard new Shoppers and enable efficient payments in-store. Since partnering in 2016, Marqeta has been critical to this effort. Through Marqeta’s refined technology we’re able to touchlessly onboard Shoppers, provide payment cards for Shoppers, and manage spending at Instacart-approved-only merchants. Marqeta’s unique technology has enabled us to scale our business through some sophisticated innovations, including: Just-in-Time Funding (JIT) : Marqeta’s card solution empowers Instacart to control where, when, and how much is authorized to spend on each transaction. The card exists with a zero balance until the point of sale, where Marqeta’s JIT funding technology loads the card with the exact transaction amount, according to the aforementioned customized authorization controls configured by Instacart. Consistent Uptime: Since our partnership began, Marqeta has consistently enhanced its uptime capabilities with multiple layers of fail-safe mechanisms and stand-in-processing to ensure continuous uptime performance. Even if one of our platforms go offline, Shoppers can continue transacting with zero interruption. No downtime ensures Shoppers can purchase and deliver to meet our customers’ needs and uphold Instacart’s pledge of same-day delivery. Flexible Shopper Onboarding with JIT Funding: Zero-balance cards funded for the exact transaction amount at the point of sale also allow Instacart’s Shoppers to have flexibility in coming onboard to accept and deliver orders. Paying for customer orders at the point of sale with Just-in-Time funding reduces operational risk, eases execution, and allows tracking and managing our Shopper fleet. Over the course of our partnership, Marqeta has been important in allowing Instacart to grow ambitiously and expand into Canada. We are excited to continue to grow our business together with Marqeta in 2018 & beyond. Instacart Engineering 157 On Demand Grocery Home Delivery Payments Instacart 157 claps 157 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-01"},
{"website": "InstaCart", "title": "freezing pythons dependency hell in 2018", "author": ["Montana Low"], "link": "https://tech.instacart.com/freezing-pythons-dependency-hell-in-2018-f1076d625241", "abstract": "Eng Open Source Data Science Machine Learning Android iOS The challenge of managing dependencies in Python has been described by many different people . It’s a storied past, which has left a history of conflicting posts across the web. Even with state-of-the-art best practices, you can still end up in Dependency Hell when adding new dependencies, because there is an open issue for pip first reported in 2013 to implement dependency resolution. The usual series of workarounds ends up like this relevant XKCD: At Instacart, we’re automating our best practices in Lore , so Data Scientists and Machine Learning Engineers can trivially replicate their work on any computer in any environment, without spending time in Dependency Hell. This helps multiple people collaborate on a single project and switch from one project to another as easily as changing directories. It also eliminates random production issues from unintentional changes to secondary dependencies. When every App maintains its own virtualenv, individual contributors are empowered to manage their dependency updates reliably, without having to update the entire company’s codebase to the latest version. There are still Python 2 vs 3 debates ten years after release, partly because old monolithic code bases hold new projects back. Don’t rely on humans to follow best practices. Write code to do it for them. Use a fresh virtualenv for each project pip freeze > requirements.txt on every change Specify your exact Python version in runtime.txt Project code should be organized in a Python module Lore’s open source command line takes care of everything required to satisfy these steps, without adding any environment variables, updating PATHs, or extra commands. It’s a natural workflow that uses the current working directory, and it’s trivial to install: pip install lore All lore commands will pass extra arguments to their delegate. Using lore means you don’t need some combination of brew, apt-get, anaconda, miniconda, pipenv, pyenv, pyvenv, venv, virtualenv etc. Lore is lightweight and modular by design and will not add any other entries to your App’s requirements.txt . It stands on the shoulders of pip, pyenv and virtualenv behind the scenes to avoid reinventing those wheels. brew, apt-get and other OS package managers don’t allow you to specify your Python minor or patch versions, and will force upgrade you regularly. Docker sort of solves this problem, by freezing your OS image, but this still doesn’t allow specific control of Python or dependency versions. Pyenv gives us fine grained control of multiple Python versions, but doesn’t deal with package dependencies. Pipfile looks promising for managing package dependencies, but is under active development. We may adopt this as an alternative if/when it reaches maturity, but for the time being we use requirements.txt . autoenv, direnv, .venv and others that automagically change your $PATH or other environment variables prevent access to your system Python (and packages) when you’re in those project directories, which will break any shell script that uses #!/usr/bin/env python . Anaconda requires a large installation up front, and while monolithic dependency management that just works is great if you’re the only person working on the code, it makes it harder for other people to replicate your work, unless you also use requirements.txt . That means pip is all that is actually necessary for other contributors to collaborate. (pyenv + virtualenv + pip) or (miniconda + environment.yml) start to look like minimum viable products, but they rely on people to actually know and consistently use their best practices. Many people don’t and won’t, because frankly, we’re concerned with bigger things. These workflows rely on senior team members to catch and corral. In addition, there is nuance around whether or not you should freeze all packages to patch versions. The hope is that if you don’t freeze any versions, you get free upgrades from all those upstream library developers. In reality what you’ll notice are the bugs and breaking changes that randomly get introduced into your continuous integration pipeline, or that the next developer to checkout your project needs to spend 30 minutes figuring out the dependency versions that work, rather than what the most recent versions are. It’s difficult to track down the source of these breakages, because they’re not in your own code and the changes were not tracked or intentional. The same logic applies to patch version changes in Python itself. The nuance is that library maintainers, rather than application developers, should be encouraged to white list ranges of tested dependency versions to reduce the likelihood of causing downstream dependency conflicts with other libraries. If you want to use these best practices for any project, it takes about 2 minutes to complete the one-time setup: If you’re creating a brand new App, lore init my_app will create the directory my_app with a template scaffold from scratch, --bare skips scaffold creation for existing projects. Anyone who checks out a Lore App will instantly be at home. When they change to the directory, and run lore test for the first time, all dependencies will be installed in a brand new virtualenv on their machine. Lore produces reliable builds for CI testing and deployment as well. Python versions and virtualenv packages are Russian Doll cached on the machine for fast and efficient repeatability across many projects. When you import lore , all dependencies will be checked to fail fast if there is a version mismatch or unsatisfied requirement. In development or test environments, new requirements are automatically added to requirements.txt and pushed down the CI pipeline. Lore is rigorous. If you manually launch a python process from outside the virtualenv and try to import an App’s module, it will reboot Python with the correct version in the correct env with the correct dependencies, or die trying (with a helpful error message). Nobody should be wasting time chasing spurious errors caused by subtle dependency bugs. Of course, all of this is configurable via environment variables, configuration directories, hidden .env files, or Python code. We believe strongly in convention over configuration, and also that rules are meant to be broken. Lore dependency management is limited on Windows to the currently installed system Python version, since pyenv is not Windows compatible. We’d love to fix this. Lore adds a few hundred milliseconds to application startup, because it reboots Python into the virtualenv. If that time matters to you, launch lore directly in the correct virtualenv with the appropriate path like ~/.pyenv/versions/3.6.6/envs/my_app/bin/lore . You can find this path and more in lore env . If your system looks like the XKCD at the beginning of the article, you might want to uninstall everything and follow the suggestions in brew doctor . Lore will work around these issues without a cleanup, but it’s nice to have a working system too. If it’s not a project you work on, but a Python script you rely on having installed system wide, you can still shove it in its own virtualenv. If you’re curious how else Lore can make your life easier, read the introductory post: tech.instacart.com Instacart Engineering 2.1K 16 Thanks to Houtao Deng and Sharath Rao . Python Machine Learning Data Science Development Open Source 2.1K claps 2.1K 16 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-26"},
{"website": "InstaCart", "title": "savings as a success metric", "author": ["Liz Barnum"], "link": "https://tech.instacart.com/savings-as-a-success-metric-6cd34ddc2559", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Advertising teams are no strangers to metrics. Impressions, conversions, ROAS, and yield are commonly used to measure success. In truth, there are hundreds of ad metrics teams may choose from. One success metric we track at Instacart is unconventional for an advertising team — customer savings. Why Customer Savings? We have two customers, brands and consumers. Savings is a key metric because it helps both these customers accomplish their goals. Brands - We partner with most of the largest consumer brands in the U.S. food, beverage, & personal care markets. These brands are introducing new products all the time. Savings offers are an effective tactic in encouraging consumers to try these new products. Consumers - Instacart now covers the majority of US households. We serve many consumers who need to stay within a weekly grocery budget. By connecting brand offers with consumers seeking savings, we help consumers stay within budgets while helping brands drive trial. Ideating & Prioritizing for Savings One example of how we’ve executed on increasing customer savings started with a small test that led to a bigger idea. When testing a new UI component in checkout (a progress bar), we saw a sizable click-through rate improvement (2x). We hypothesized the impact could be substantially larger if we expanded this concept further. Our analysis showed that a large portion of Instacart consumers purchased 1 item inside an offer, but not enough to fully qualify for the offer benefit. This suggested to us that consumers were unaware of the potential savings they could realize if they purchased more products. To help these consumers discover and qualify for these savings, we prototyped, user tested, and assessed tradeoffs around ways to communicate these offers. Choosing a Winning Design The winning approach struck a balance of being fast to build and high impact. It leveraged: Building on Intent : We built off of the user’s interest in a brand. Right after consumers add a brand’s product to cart, we communicate a highly relevant offer in connection to their last action (adding to cart). Easy to Act On : We present the offer in a visual and easy to digest way. Consumers are able to quickly understand and act upon the offer with little effort. Building It A naive implementation would’ve involved writing client-specific logic to fit the immediate business need. However, at Instacart, we strive to follow the company value “Every Minute Counts” by building features that scale quickly and endure beyond the immediate need. This empowers developers to accelerate the availability of new features for customers. In this case, we minimized client-specific work and instead implemented our enhanced server-side design. This was a significant undertaking because it required us to redesign how quickly cart changes were sent to the server. We worked through performance obstacles, used smart caching to sync clients and servers, and layered on promotional qualification logic. This approach enabled us to launch rapid iterations without the need to wait on client-side updates. Launching It We rolled this out as an A/B test and monitored our “hold steady” metrics to ensure that this test didn’t cause any unintended consequences. We ultimately launched this to all our customers as a success with a dramatic improvement in overall customer savings. In fact, this is one of our highest converting placements to this day. Looking to join the team? Instacart is hiring ! Many thanks to Aamir Poonawalla, Dan Loman, Peter Lin, Richard King, Serena Wu, and Toni Chau who made this feature possible. Instacart Engineering 162 1 Metrics Product Management User Experience Advertising Women In Tech 162 claps 162 1 Written by Group Product Manager at Instacart Instacart Engineering Written by Group Product Manager at Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-24"},
{"website": "InstaCart", "title": "instacart anytime a data science paradigm", "author": ["Ji Chen"], "link": "https://tech.instacart.com/instacart-anytime-a-data-science-paradigm-33eb25a5c32d", "abstract": "Eng Open Source Data Science Machine Learning Android iOS H ave you ever ordered your groceries online? Partnering with hundreds of retailers, Instacart enables more than 50% of American households to order groceries online from their favorite stores. If you haven’t tried it yet, read The Instacart Experience to learn how Instacart works. When customers shop for groceries on Instacart, they can select a delivery window to receive their groceries. Ideally customers are able to choose any delivery window (shown on the left screenshot above); however, sometimes there is more demand than shoppers can handle. For busy windows, we would have to either close them completely or increase their delivery fees to shift demand to other windows (illustrated on the right screenshot above). This blog post introduces Instacart’s Shopper Staffing team, and our data science approach to achieving a desirable balance between maximizing availability of delivery times to customers and minimizing shopper idleness. Let’s start with our mission statement and system overview , then delve into the subsystems ( marketplace forecasting , supply planning , real-time capacity ), and wrap up with our team snapshot . Back to Top The mission of the shopper staffing team is to match shopper supply to customer demand. We want customers to be able to choose their desired delivery windows without paying additional fees; meanwhile, we don’t want to schedule too many shoppers, which would result in redundant idleness. Essentially, we want to achieve a supply-demand balance that maximizes the aggregate benefits for both customers and shoppers. Our team mission is quantified using these metrics: Availability − % of customer visits with available delivery windows Idleness − % of time when shoppers are not actively working Unmet Demand (i.e., Lost Deliveries from Instacart’s perspective) − potential orders that we may fail to accept due to lack of shoppers during certain time windows Back to Top As depicted above, the staffing system consists of these subsystems: Marketplace Forecasting to predict demand and supply Supply Planning to estimate the total number of shoppers needed to fulfill the predicted long-term demand and to plan shopper hours to fulfill the predicted short-term demand Real-time Capacity to adjust shopper hours in real time, to estimate the capacities of delivery windows, and to optimize delivery prices to maximize demand conversion Now let’s take a deep dive to these subsystems and scope out the data science problems inherent in these components. Back to Top The goal of Marketplace Forecasting is to provide accurate demand and supply predictions for downstream decision-making systems. It includes the following data science problems: Demand Conversion: Every customer who visits Instacart may not place an order. Their order probability is dependent on a number of factors, such as what type of user they are, what delivery options were available to them, which city they are from, etc. The demand conversion model relates the order probability to these features. Unmet Demand (i.e., Lost Deliveries) Estimation: As depicted above, if customers cannot find their favorite delivery windows at checkout, they may abandon their order. Such unmet demands hurt customer experience, and we call those lost deliveries. We want to estimate lost deliveries due to unavailable and busy delivery windows. For this purpose, we use causal inference or Bayesian networks to estimate the impact of specific features on demand conversion. Demand Forecasting: We combine actual and lost deliveries to estimate ideal demand levels, which serve as the input to demand forecasting. We use conventional time-series models to forecast seasonality of grocery sales on Instacart. In addition, we project demand several weeks out at the city level for recommending shopper acquisition, in parallel we also forecast hourly demand to plan supply at the store level. Different spatiotemporal use cases require us to handle different data sparsities when building demand forecasting models. Supply Prediction: Since Instacart shoppers work on their own schedules, we use machine learning models to predict the likelihood of claim and cancelation of any particular hours. Such predictions provide critical information to Supply Planning , so that we can fill shopper hours as accurately as we need in order to fulfill customer demand. Back to Top The goal of Supply Planning is to generate shopper hours as accurately as we need in order to fulfill customer demand. It includes the following data science problems: Staffing Model: This model estimates how many shopper hours we need in order to fulfill the predicted demand. Our estimation considers not only demand levels and shopper efficiency, but also demand origins and destinations and the efficiency of our logistics system. Read No Order Left Behind; No Shopper Left Idle by Jagannath Putrevu to learn more about our Monte Carlo method to derive staffing levels from demand forecasts. Shopper Bench Model: Once we submit long-term demand projections to the staffing model, it predicts the total shopper hours we need for the upcoming weeks. We then use statistical modeling and microeconomic theory to derive the optimal number of shoppers needed for this period of time, which is used to acquire shoppers cost-effectively. Fill Model: As shoppers work on their own schedules, they can sign up or cancel their hours at any time. With supply predictions the fill model computes the right number of open hours at any time before the hours start, in order to maximize the likelihood of filling shopper hours as accurately as we need to fulfill customer demand. Hour Incentive Model: Different hours have different acceptance probabilities. With supply predictions the hour incentive model classifies hours into different categories based on acceptance probabilities and applies incentives accordingly. It is worth noting that hour incentives can in turn significantly change hour acceptance probabilities; therefore, we need to model the feedback loop between incentives and acceptance rates into a dynamic hour incentive system. Back to Top Real-time Capacity serves as the gatekeeper of our system, allowing us to adapt to reality more accurately as we have more data in real time. It includes the following data science problems: Real-time Staffing Model: Supply Planning generates shopper hours based on batch predictions made at least one day before hours start, which can sometimes deviate from reality significantly. Real-time staffing model uses real-time demand forecasts, supply predictions, and any supply-demand mismatch to dynamically adjust supply to match demand. Capacity Model: This model predicts the number of orders we can fulfill for each delivery window for a given store based on the existing supply and demand levels. We use a machine learning model to trade off between capacity and lateness. If the model estimates capacity too conservatively, it will unnecessarily limit the availability of delivery windows and increase shoppers idleness due to lack of orders. On the other hand, overly aggressive capacity estimation will take too many orders and result in late deliveries, hurting the customer experience. Delivery Pricing Model: The capacity model provides insights into how much demand shoppers can serve. Actually, there is more we can do! For example, we price delivery windows differently to shift demand from busy times to idle times. Demand elasticity also depends on other factors such as user types, baskets, etc.; therefore, the demand conversion model is also useful for balancing supply and demand in real time. Back to Top This post was a high-level overview of the data science challenges that our team grapples with every day. Stay tuned for upcoming posts from our team to give you a more detailed view of some of these challenges. Last but not least, our team effort goes beyond data science. We are a multidisciplinary team of data scientists, machine learning engineers, full-stack and mobile engineers, data analysts, designers, and product managers. We learn from each other, collaborate cross-functionally, deliver a powerful impact to Instacart’s business, and strive for Instacart Anytime ! Interested in joining us? Apply here , or contact us ! Back to Top Instacart provides a seamless online grocery shopping experience. As shown above, customers can open the app, select their favorite store, shop for groceries online, choose a delivery window, and receive their groceries within the selected time window, which can be as fast as one hour. After an order is placed, our system would assign it to a particular shopper. As shown above, the shopper would acknowledge the order, go to the store, find and scan the items on the order list, confirm the items with the customer, and check out for delivery. Both customer and shopper experiences on Instacart are straightforward; however, the system behind the scenes is very complex (read Space, Time and Groceries by Jeremy Stanley ). We want customers can place orders with their desirable delivery windows, and shoppers can be well utilized and deliver the orders efficiently and in time. Every minute counts! Want to learn more about how Instacart works? Read here . Instacart Engineering 323 1 Thanks to Houtao Deng , Milton Bose , Ganesh Krishnan , Eric Rynerson , Jagannath Putrevu , Andrew Kane , Emmanuel Turlay , Tahir Mobashir , and Houtao Deng . On Demand Data Science Machine Learning Optimization Logistics 323 claps 323 1 Written by Tech Lead Manager, Shopper Staffing @Instacart, Previously Software Engineer @Uber, Associate Professor @Emory Instacart Engineering Written by Tech Lead Manager, Shopper Staffing @Instacart, Previously Software Engineer @Uber, Associate Professor @Emory Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-08"},
{"website": "InstaCart", "title": "it all depends", "author": ["Jingjie Xiao"], "link": "https://tech.instacart.com/it-all-depends-4bb7b22e854b", "abstract": "Eng Open Source Data Science Machine Learning Android iOS How randomized, controlled experiments and multivariate regression are used to continuously improve the grocery delivery engine @ Instacart. Instacart is growing at an accelerating rate. Last year, we expanded from 30 to 190 markets in North America. Meanwhile, new and expanded partnerships with top grocers have driven up the number of deliveries everywhere. As a customer, you open the app or the website, pick a favorite store, add products to cart (often including 🍌, our most popular item followed by 🥑 and 🍋), select a delivery window (usually 1 hour long), and place the order. In any given city, over the course of a couple of hours, there could be thousands of orders out for delivery. The obvious logistics challenge is to make sure customers receive their groceries on time . Additionally, we care a lot about the efficiency of the dispatching plans, since the more orders a shopper can fulfill in an hour the higher their earnings potential will be. Generally speaking, the logistics problem is to decide which orders each shopper should fulfill. More precisely, it is a Stochastic Capacitated Vehicle Routing Problem with Time Windows for Multiple Trips (😰). Interested in detailed explanation and compelling visualizations? Read Space, Time and Groceries , by Jeremy Stanley . There are two models we use to fulfill orders — full-service and handoff. In the full-service model, a shopper/driver rides to a store, picks groceries from the aisles, checks out the orders, and then drops them off in sequence: Whereas in the handoff model, picking and delivering are done by different persons. An in-store shopper collects all the products, and stages them in advance. When the driver arrives at the store, she directly picks up multiple orders from the staging area for delivering: Solving this dispatching problem efficiently is the Holy Grail of the delivery logistics team. Our dispatching engine is constantly changing — “move fast and get things done” is the motto — and interacting with many other systems. Naturally, one big challenge is how to measure the outcome caused by a particular change — be it rolling out an algorithm improvement or fine-tuning a parameter. Consider one of our recent algorithm modifications. To cluster orders into delivery trips , we take a two-stage approach. First, we find the fastest handoff routes that can be completed on time. Second, we build full-service trips with the remaining orders. Although each of these two sub-problems can be solved to near optimality, it’s easy to see that the overall solution is not optimal. In theory, a better approach would be to optimize for both models simultaneously and find the best split: A/B testing has long been held up as the “gold standard” for cause-and-effect discoveries. In customer application design, A/B tests are used extensively to identify which experience variant (e.g. text size) maximizes the outcome of interest (e.g. purchase rate). There is just one problem. In our logistics system, we cannot split samples either by customer or by shopper since they are all interdependent : For example, the deliveries from both algorithm variants might be dispatched to the same shopper. Also, which orders one shopper fulfills could affect the assignment of other shoppers. What are the alternatives to A/B testing? Let’s examine three of the popular methods one by one to see why they are not good enough to prove a causal relationship. 1. Simulations . With time-based simulations, we “replayed” the history of customer and shopper behaviors with the existing algorithm and the new one to see how the dispatching plan would pan out differently in San Francisco. When the global optimizer was employed, there appeared to be a 2.1% uptick in efficiency: But simulations are imperfect. The result is only as good as the quality of the simulation models. The dispatching engine is just one component of the real-time logistics system. For example, the feedback loop between dispatching (which shopper fulfills what orders) and capacity (how many orders we take) is missing, which undoubtedly affects the overall system and efficiency. Moreover, measuring some important guardrail metrics such as shopper happiness is not a straightforward task using simulations. 2. Before and after analysis. Seeing the positive outcome from the simulation, we decided to launch the new algorithm in San Francisco to see what would actually happen. After a few days, we estimated that efficiency went up by 2.9% (after vs. before): Can we thus claim that the 2.9% increase resulted entirely from the algorithm change? Of course not. Because correlation does not imply causation ! It is not clear how much of the effect was caused by exogenous factors such as weather conditions. 3. Difference in differences . Being resourceful, we checked the metric for Oakland where the experiment never took place. It turned out that it remained stable (the orange line): Oakland is geographically close to San Francisco; therefore, weather conditions are controlled for to some extent. In addition, the metrics in the two zones exhibit a high correlation before and after the change: It is tempting to attribute the increase in the intercept of the regression lines to the experiment. But in fact, the comparison suffers the same correlation-is-causation fallacy. Oakland is far from being a perfect representation of SF. There is an endless list of other factors which might be at play such as traffic conditions, customer promotions, system changes, etc. Is it possible to split samples in a practical fashion? Yes, and here’s how. In our systems, we don’t have to optimize for all orders and shoppers across the country at once. For example, it doesn’t make sense to combine an order in San Francisco and an order in Austin into one trip, or send a shopper in Austin to deliver in SF. To keep the problem more tractable, we segment the markets and operate independently in geographic areas called zones : In addition, our delivery system “clears” overnight as the orders are same-day. Thus any two days of the same zone are also independent. This implies that we can split samples by zone and day , and randomly decide which algorithm variant — A for the existing version or B for the new version — to apply based on a schedule: The sample distribution of efficiency for the new algorithm (variant B ) appears to have a higher mean, but we are not sure if the difference is significant: Using the t.test() function in R, we apply a two-sample t-test to determine if the means of the two groups are the same: The result shows that the efficiency associated with the new algorithm is 2.6% higher with a p-value of 0.079 . Using a significance level of 0.05 (allowing a false positive rate of 5% ), we cannot accept the alternative hypothesis that the change has a significant effect. Note that a t-test is equivalent to a simple linear regression of the response variable efficiency on the group variable variant . Fitting a linear model to compute the regression coefficient produces an identical estimate, and a similar p-value (slightly off due to different degree of freedom assumptions): However, simple regression can be inaccurate if important explanatory variables are left out. In our example, there are indeed many other factors that may affect the outcome: Zone : Geographical dispersion, order volume, store locations, etc. may have a strong influence on efficiency. Day of week (DoW) : Consumer patterns in terms of order timing and volume, how busy a store is, etc. depend on the day of week. Week number due to the growth trend. How can we isolate the particular change we care about? To run a multivariate regression , we include the additional predictive variables (all qualitative) in the linear regression model: Based on the model, the contribution of the algorithm change is 3.0% when zone, day of week, and week number are controlled for. More notably, the standard error is reduced from 0.013 to 0.007 , and the p-value is dropped to 0.0003 (😲): The positive impact that the new algorithm has on efficiency is in fact significant. By controlling for other variables, multivariate regression dramatically reduces the standard error of the estimate and hence the p-value and false negatives . For the very same reason, it shortens the test run time needed. Running controlled experiments can be expensive . A good practice is to run a power analysis to determine the sample size — in our example, the number of weeks and zones — prior to the experiment and estimate the cost. Other things being equal, larger samples are needed to account for higher variability of the population. The sampling distributions of the effect estimate for both multivariate and simple regression for different sample sizes are shown below: Each distribution is generated by running A/A testing 500 times. In each run, A/B is used to test two identical variants — which are randomly assigned by zone and day — to output one estimate. It is no coincidence that all of the distributions are centered around 0. The true difference in effect between two identical variants is indeed 0. With the sample size being equal, multivariate regression yields lower variability. The longer the experiment time, the smaller the variance tends to be. Using the sampling distributions, we can determine the magnitude of the effect detectable for different numbers of weeks at 0.05 significance and 0.8 power (🎯 requiring a correct rejection rate of 80% when the null hypothesis is wrong and there is a real impact): In order to detect a 1% difference in effect, the experiment will take about 4 weeks using multivariate regression. What about simple regression? The same level of statistical significance and power will not be achieved within 8 weeks! This explains why, in our example, the multivariate regression picked up the signal, which the simple regression failed to detect. Without multivariate regression, our pace of innovation would be much slower! In addition to the primary metric (i.e. efficiency), a suite of secondary and guardrail metrics were compared to draw patterns and identify red flags. To prevent high false discovery rates associated with multiple comparisons , Bonferroni correction is used. It’s shown that the global optimizer leads to significant efficiency gains and happier shoppers (😃): In a system that is dynamic, interdependent, and noisy, A/B testing coupled with multivariate regression analysis is just the tool needed to quantify the outcome of a particular change we care about at a minimal cost. It enables our delivery logistics engine to evolve in a rigorous, data-driven, and automated fashion. 🙏 Many thanks to Jeremy Stanley , Max Mullen , Monica Rogati , Hadley Wickham , and the fulfillment team for inspiration and feedback. 🥕 Want to join us? Check out our careers page . 📙 Look for a vivid introduction in statistics and the intuitions behind? Highly recommend Naked Statistics: Stripping the Dread from the Data. 👩 Check out the slides for the lighting talk @ R-Ladies SF meetup . Instacart Engineering 446 7 Thanks to Sherin Kurian , Jagannath Putrevu , and Ji Chen . Data Science A B Testing Logistics Statistics Women In Tech 446 claps 446 7 Written by Machine Learning @ Instacart 👩🏻‍🌾 Instacart Engineering Written by Machine Learning @ Instacart 👩🏻‍🌾 Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-08"},
{"website": "InstaCart", "title": "a month of android accessibility", "author": ["Maksim Golivkin"], "link": "https://tech.instacart.com/a-month-of-android-accessibility-1032e2566b25", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, Solve for the Customer is our first and most important value. Recently we took a month to improve the accessibility of our Android app and serve a wider range of customers. Shopping for groceries involves doing quite a few searches, browsing stores and “virtual aisles” as well as finding the best coupons. To enable blind & visually impaired users to confidently navigate our app, we ended up implementing the following tactic: Keyboard & accessibility navigation (TalkBack) focus should be consistent On a new screen focus should always be on a toolbar or a header Screen title or content description has to be read through TalkBack Having to treat keyboard and accessibility focus separately would require us to design two additional UI interactions on every screen. In order to minimize maintenance costs, we decided to treat keyboard and accessibility focus as one. Hence we started with a simple approach to focusing on a view: Unfortunately, this wasn’t reliable in most cases. The first problem was related to Accessibility Service working “asynchronously” from Activity lifecycle. Calling the code above in Activity or Fragment onResume , wouldn’t always work. Secondly, a child view would sometimes “steal” focus from a parent view. For example, the Toolbar ¹ would lose focus if Toolbar.setNavigationOnClickListener was called. Unfortunately, the most reliable way to set a focus on a toolbar came out not that pretty. Below is a simplified code sample: One interesting exception, when we decided to have accessibility and keyboard focus on separate views, were dialogs with input fields. Having keyboard focus on an input field allowed users to provide input as soon as the dialog was displayed. At the same time accessibility focus ensured that the title is announced when the dialog is launched and that a visually impaired user didn’t miss input instructions located between dialog title and the field. Most of Instacart app screens require a network operation and take time to load. To assist TalkBack users we started announcing network operation progress as “X is loading”, “Y has loaded” and errors via voice messages. Some of the screens make a number of network requests, display partial information and get updated as information comes in. Announcements made too fast, one after another quickly become indiscernible, as the older messages are stopped mid-sentence. To avoid this, we implemented a queue with a debounce of 1 second (if two messaged are enqueued within 1 second, the first message is dropped). Contrast between text and its background is important for users with low vision. WCAG Contrast minimum success criterion requires at least a ratio of 4.5:1 between text and background, which meant that we would need to redesign most of the application, as we were using a wide range of dark grays on a light gray background. To avoid a review & customization of every screen, we implemented a high contrast mode and introduced a prominent opt-in during signup and in user account settings. In high contrast mode, lighter text colors would become pure white and all the darker colors switch to pure black. We already had some backend driven theming mechanism for actionable views, toolbars, and other key navigation elements. In order to apply high contrast colors to text, we also replaced every TextView in the application with a custom text view. The above summarizes a few of the design decisions we applied to make our Android app more accessible. With this blog post we aim to encourage a wider discussion and sharing of knowledge around the topic. Some resources that helped us to quickly ramp up and have proven useful: Developing Accessible Android applications by Renato Iwashima Designing Android Apps For Vision Impaired Users by Ataul Munim Text Input Accessibility by Renato Iwashima Notably, starting the last summer e-commerce industry has seen an increasing number of lawsuits related to website accessibility, so we expect the topic to gain wider prominence. Footnotes ¹ For context, we are using the Toolbar as a regular view — often times it is inside a Fragment or a view group and we don’t set setSupportActionBar on an activity every time a new view with a toolbar comes into the foreground. Instacart Engineering 220 2 Accessibility Contrast Focus Talkback AndroidDev 220 claps 220 2 Written by Engineering Manager @ Instacart, ex- Uber, ex- Kiva Systems Instacart Engineering Written by Engineering Manager @ Instacart, ex- Uber, ex- Kiva Systems Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-04"},
{"website": "InstaCart", "title": "i love interviewing", "author": ["Muffy Barkocy"], "link": "https://tech.instacart.com/i-love-interviewing-96ea9a5de885", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Well, mostly. I’m not always enthusiastic about talking to people, especially people I don’t know. I’m super introverted, and sometimes the time comes for the phone call and I’m thinking “I really really hope they won’t answer so I can avoid talking to someone I don’t know.” However, by the end of the call I’m glad I made it and I’m glad they answered. It wasn’t always that way. I have enjoyed being the interviewee since my very first interviews in college. I get to meet interesting people and talk about interesting problems, and I usually learn something. When the interview is particularly good, it’s like going to a great party full of smart people who all want to talk to me. Sure, there have been the bad interviews, where there was absolutely no fit, where the people talking to me clearly didn’t want to be there, where I was only asked to do whiteboard “coding” (UGH!) and didn’t get to actually talk. However, over the course of a 30 year career and a dozen different jobs, I’ve had a great time being interviewed, both for the jobs I got and the jobs I didn’t. What I liked less for many years was being the interviewer. It’s a lot of responsibility. If the conversation isn’t going well, I’m the one who has to make it work. I have only a short period of time in which to form a pretty consequential opinion. Some companies required me to administer whiteboarding tests, which I think are tedious and useless. Some companies were clearly putting me on the interview panel because I was female, which I found offensive and dishonest. On the positive side, just like being interviewed, when the conversation went well it was great. So, for many years I did lots of interviews with little enthusiasm. Then I got to Instacart. Our interview process has the following features: We ask people to do coding, not algorithms (no whiteboarding!) We trust that smart people can learn skills they don’t currently have We regard a degree in CS as relevant but not required We place people according to what they want to work on and learn We get people through the process without delay After doing quite a few phone screens I suddenly realized that I was saying “yes” to almost every candidate. In over two years now, I’ve only said no to three or four people, and I do several phone calls a week. I felt weird about this. Isn’t it my job to “screen” them, as in “phone screen”? Why was I always saying “yes”? Was our recruiting staff just THAT GOOD? (Well, they’re pretty good.) A few of the people we hired after my “screens” told me how much they enjoyed our conversation, they said it was the best part of the process. One of them even said he decided to take the job entirely based on the conversation with me. I only talk to people for half an hour, and the step after the phone screen is for the candidates to do a 3–4 hour coding exercise, at home, in their own environment, in the language of their choice. This led me to a mental switch. I realized my primary purpose is not to judge whether or not the interviewee is a great engineer. I’m not even there to determine if they are competent engineers, that’s what the coding exercise is for. My primary purpose is to help the interviewee determine if they would enjoy working at my company. One reason I enjoyed being interviewed was the knowledge that I’m judging the people I talk to and the company as much or more than they are judging me. This changes the power balance in the interaction. I’m not a supplicant begging for a job, I’m a competent professional offering my services to the right business. Of course, I’m interviewing because I need a job, but I don’t need a job where I (or the company) will be unhappy. I will be unhappy if the goals and values of the company are too dissimilar from my own. I will be unhappy if my skills are not the right set for the job. I will be unhappy if I can’t talk to the people I’m working with. I will be unhappy if I am not respected and valued for what I personally can contribute. Most companies don’t seem to realize just how much they are being judged, so past training I had received for doing interviews was all about judging and very little about being judged. I assumed my attitude was unusual, but I finally realized that it was the standard interview process that was broken. Many companies, even those competing for scarce skilled professionals, still treat it as a process where they sort through candidates and pick the ones they want. One company I worked for considered it reasonable that it only took them an average of 43 days from first contact to hiring someone. Their interview process was based around making the interviewees prove they had the basic skills of their profession. Why would anyone want to work at a place that started out treating them with so little respect? (Why did I? That’s a different story.) Now in my phone calls I intentionally spend my time making sure that the candidate knows what it’s like to be an engineer at my company. I want to cover all the reasons I enjoy working there and find out what it is that they enjoy in a working environment. I sometimes confuse people by this approach; they expect a technical grilling and instead I ask them to ask ME questions, which I answer as honestly as I can. Usually those questions lead into some good technical discussions, often they lead to philosophical or social discussions as well. It’s a GREAT party. So now I love interviews. I love helping people. Doing these phone calls is a way of helping everyone involved. I’m helping the company find good people and I’m helping the interviewee determine if it’s a good job for them. Even better, I frequently learn something myself. I never learned anything when I was just giving people one of the approved list of algorithm questions (the answer to which invariably involved a binary tree). Is it any wonder I found it unpleasant and boring? I love to talk with people about things they care about. Someone who has spent years developing and using a set of skills cares about those skills and the things they have created with them. They also enjoy the opportunity to talk about applying those skills, but they don’t enjoy having to prove that they know the most basic things about their profession. I love respecting people and their time and effort. Since the next step, the coding exercise, will take several hours, I want to be sure to convince the people I’m interviewing that it is worth their time. If they aren’t going to like our environment, if they aren’t enthusiastic about our values and our processes and the problems they’ll get to solve, then there’s no point in them spending time on it, and the exercise is crucial to our process. I love participating in a process I believe in. We use the exercise as the basis for the rest of the process. One of our engineers reviews it, and we give feedback. When we bring people into the office, we discuss the decisions they made and pair program with them to make modifications to their code. I believe this is the best way to interview because we’re asking people to do things they would do in the job: writing code and talking to people about code you’re going to write or have already written. So yes, I love interviews. I get to talk to several new people every week about things I love and find out if they love them too. Sure, I still need to wrap up with a judgement, but hey, I’m human, I form opinions without even trying. Instacart Engineering 267 Interviewing Culture 267 claps 267 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-27"},
{"website": "InstaCart", "title": "leveraging pay with google", "author": ["Maksim Golivkin"], "link": "https://tech.instacart.com/leveraging-pay-with-google-9b5fd44396dd", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Selecting a good default saves user time and mental effort. In e-commerce, payment information, billing and shipping address are mandatory and filling them up during checkout or sign-up is annoying for the consumer and results in conversion drop-offs. Naturally, whenever there is an opportunity to improve this experience, Instacart is the first to take advantage of it. In 2015, Instacart was a launch partner for Android Pay. It was a better user experience and a bet to increase checkout conversion. Android Pay was definitely a step in the right direction but it didn’t result in a huge win. The API had two problems: it was slow (to a point being mentally taxing for users) and not many customers were using it. When Google presented Pay with Google , it struck a chord with us immediately, because it leverages credit cards stored in Chrome, Play Store, Youtube and other Google e-properties API feels faster as Google reduced the number of API calls (2 to 1), as well as made other latency optimizations new API is so simple that our developer became one of the champions of the project Pay with Google doesn’t fully replace Android Pay. The new API and brand are used in merchant apps, such as Instacart, while Android Pay remains a separate app on the user phone, acting like a wallet with credit cards. Android Pay is used to pay for physical goods in brick & mortar stores. There are minor differences in user experience on the phone, too. For instance, when using a card saved outside of Android Pay, Pay with Google will prompt the user to enter the card’s security code. Additionally, once a transaction with Pay with Google is made, the user receives a notification suggesting to save her credit card in Android Pay (so she could use it in the physical store) if she has the app. The interplay between the two is somewhat confusing, however, introduction of Pay with Google is still exciting, since it comes with a prospect of many more users having their billing and shipping information available for merchant apps. To provide a better experience, we are using Pay with Google as a default for new users. Unfortunately, the new API will only disclose if the user has a credit card saved in Android Pay, but not if she has any cards in other Google properties (Chrome, Youtube, etc). We decided to skip billing information step completely if the user has a card in Android Pay (which is compatible with our former Android Pay flow), but also to provide Pay with Google as a payment option if the API is available for the customer. To summarize, at Instacart, we will use every opportunity to save user’s time and effort. Transition to Pay with Google API only took our developer a few days of work. Right now we are keeping a close eye on metrics and plan to report the improvements as we see them. Pay with Google already feels like a great step forward to simplify and improve user shopping experience on Android and we are excited to take advantage of it. P.S. Pay with Google also allows to obtain user shipping address and phone number. For v1, we didn’t leverage this information, but we are looking forward to try out such an experiment soon and would love to learn from other industry participants about their experience. Instacart Engineering 133 Payments Growth Mobile E-commerce AndroidDev 133 claps 133 Written by Engineering Manager @ Instacart, ex- Uber, ex- Kiva Systems Instacart Engineering Written by Engineering Manager @ Instacart, ex- Uber, ex- Kiva Systems Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-16"},
{"website": "InstaCart", "title": "3 nips papers we loved", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/3-nips-papers-we-loved-befb39a75ec2", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Know your model’s limits, interpret it’s behavior and learn from variable length sets. At NIPS 2017 what surprised me the most was not the size of the crowds (they were huge), the extravagance of the parties (I sleep early) or the controversy of the “ rigor police ” debate (it was entertaining). No, what surprised me the most was the number of papers I saw that (when combined with talks and posters) were both relatively easy to understand and of immediate practical use. In this post, I will briefly explain three of our favorites: Knowing your model’s limits Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Lakshminarayanan et. al 2017, paper & video (1:00:10) Interpreting model behavior A Unified Approach to Interpreting Model Predictions Lundberg et al. 2017, paper , video (17:45) & github Learning from variable length sets Deep Sets Zaheer et al. 2017, paper & video (16:00) I’d like to extend a huge thank-you to Balaji Lakshminarayanan , Scott Lundberg , Manzil Zaheer and their co-authors for doing this work and presenting at NIPS. Their cogent presentations and detailed answers to my many questions at their poster sessions enabled and inspired this post. Deep learning models can be surprisingly brittle. They can fail to generalize on data drawn from slightly different distributions and can give very different predictions given minor changes in the learning algorithm or initialization. This begs the question — can we know when our deep learning models are uncertain about their predictions? If so, this would help in many applications at Instacart, such as: How uncertain are we about an item being in stock at a store location? How much risk is there in a grocery delivery being late? Is there a chance we should explore showing a rare item for a search? What range of delivery demand should we anticipate at a store location? In particular, anytime you make a decision based upon many noisy predictions, you risk favoring observations with large noise values (common in ranking for search or ads, or in optimization for pricing or logistics applications). Ensuring you control for prediction uncertainty to avoid this effect can be important. Other methods can be used to quantify uncertainty, but have drawbacks. For example, bayesian methods require assumptions about priors and are computationally expensive. This paper provides an elegant method to quantify the uncertainty in deep learning models: In practice you: Choose a distribution for your output (gaussian if you are optimizing for MSE, poisson for counts, etc.) Change the final layer in your deep network to output a variance estimate (or other distribution parameters) in addition to an estimate for the mean Minimize the negative log-likelihood for the output distribution (e.g., with a custom loss function in Keras) Train M networks in this way, each with a different random initialization Let your final predicted distribution be the evenly weighted mixture of distributions from the M networks While the paper also adds adversarial training (hard to implement for discrete inputs), some of their experiments showed that this was less important. What is critical is that your network must produce an estimate of mean and variance, and then optimize the negative log likelihood loss function. If you assume your errors are gaussian distributed, then your loss function is: Where 𝜇 is the network’s estimate of the mean (conditioned on weights θ and input 𝒙), and σ² is the networks’ estimate of the variance. If you assume a constant σ, this can be simplified to classical regression with MSE. For an example on implementing a similar loss function in Keras, see the WTTE package , which uses a Weibull distribution instead of a Gaussian. The following toy example from their paper illustrates the impact, where each red point is drawn from y = x³ + ε where ε ∼ N(0, 32), the blue line is y = x³ and the grey range is the method’s variance estimate conditioned on x: The leftmost plot shows the variance of training M=5 simple networks which only output the mean and were optimized for MSE. Each model produces only a point estimate, and there is little variance observed over the ensemble. The second plot shows the results of following the above recipe but with M=1. In this case, the network produces a distribution, but it’s level of uncertainty remains constant even when generalizing outside of it’s domain. The third plot includes adversarial training (note how little difference it makes) with M=1, and the final plot does everything (mean and variance outputs, adversarial training and M=5.) Only the final plot does a reasonable job of estimating uncertainty outside of the range of the training data. The authors then show that an ensemble of networks trained in this way on digit classification with MNIST data do a far better job of estimating their uncertainty than other techniques like monte-carlo dropout: In the above visualization, they vary the number of networks in the ensemble, and compare monte-carlo dropout (green) to a simple ensemble (red) to an ensemble with adversarial training (blue). The grey curves use random data augmentation (rather than adversarial), and show that using the adversarial approach is what adds incremental value to a simple ensemble. Finally, and perhaps most impressive of all, the authors show that their method responds appropriately when presented with data from an entirely different domain (letters rather than numbers): The blue plots show the uncertainty (measured in entropy given this is a classification problem) for digit classification when presented with numbers. The bottom red plots show the uncertainty when presented with letters. When using just 1 network in the ensemble (how most deep learning models are deployed), the model trained only on numbers gives equally confident (but obviously wrong) classification results for letters! But increasing to even just 5 networks produces significantly less confident predictions. Most complex machine learning models are black boxes — we simply cannot fully understand how they work. However, we can gain deeper insight locally into the predictions that they make, and through this insight can better understand our data and models. This understanding can be used to: Build intuition for how our algorithms behave Alter end user experiences to provide more context for predictions Debug model building issues arising from data quality, model fit or generalization ability Measure the value of different features in a model, and inform decisions for future data collection and engineering At Instacart, we often want to deeply understand models we build such as: The expected time until a user places their next order, as a function of their past order, delivery, site and rating behavior What product pairs are good replacements for each-other in case we cannot find what the customer originally requested How our customers react to limited delivery availability options or busy pricing The SHAP (SHapley Additive exPlanations) paper and package provides an elegant way to decompose a model’s predictions into additive effects, which can then be easily visualized. For example, here is a visualization that explains a Light GBM prediction of the chance a household earns $50k or more from a UCI census dataset : In this case, the log-odds likelihood of high income is -1.94, and the largest factor depressing this chance is young age (blue), and the largest factor increasing income is marital status (red). Furthermore, you can visualize the aggregate impact of features on model predictions over an entire dataset with visualizations like these: Here they find that Age is most predictive, but really because there is a group (young) which is separated and low income. Capital Gain is the next most predictive, in part because of both very high and very low contributions. This is a huge improvement over the typical information gain based variable importance visualizations commonly used with packages like XGBoost and LightGBM, which only show the relative importance of each feature: The package can also provide rich partial dependence plots which show the range of impact that a feature has across the training dataset population: Note that the vertical spread of values in the above plot represent interaction effects between Age and other variables (the effect of Age changes with other variables). This is in contrast to traditional partial dependence plots which show only the effect of varying Age in isolation. To understand how the SHAP algorithm works, consider this example for a single observation: Their model is predicting the chance of high income, and on average predicts a base rate of 20% for the entire population, denoted by E [ f ( x )]. For this specific example (named John in the talk), they predict a 55% probability, denoted by f ( x ). The SHAP values answer the question of how they got from 20% to 50% for John. They begin by ordering the features randomly, perhaps starting with Age, and ask how much the average prediction of 20% changes for users whose age is the same as John’s, denoted E [ f (x) | x ₁]. This can be found by integrating f (x) over all other features besides x ₁ in the training dataset (a process that can be done efficiently in trees). Suppose that they find that the prediction goes up to 35%, and so this gives them an estimate for the effect of Age, ϕ ₁=15%. They then iteratively repeat this process through the remaining variables (concluding with marital status), to estimate ϕ ₂, ϕ ₃ and ϕ ₄ for each of the other three features in this example: However, unless a model is purely additive, the estimates for ϕ will vary with the ordering of features chosen. The SHAP algorithm solves this by averaging over all possible 2ᴺ orderings. The computational burden of computing all such orderings is alleviated by sampling M of them and using a regression model to attribute the impact from the samples to each feature. The paper justifies the above approach using game theory, and further shows that this theory unifies other interpretation methodologies such as LIME and DeepLIFT: And finally, because no NIPS paper would be complete without an MNIST example, they show that the SHAP algorithm does a better job at explaining what part of an 8 represents the essence of an 8 (as opposed to a 3): This shows that their approach can work well even for deep learning models. Established deep learning architectures exist for modeling sparse categorical data (embeddings), sequence data (LSTMs) and image data (CNNs). But what do you do if you want your model to depend upon a variable length unordered set of inputs? This was precisely the question we asked ourselves at Instacart a year ago while pondering our work on sorting grocery shopping lists in our Deep Learning with Emojis (Not Math) post. I was overjoyed (and humbled) to see this paper at the NIPS poster session Wednesday night, which generalizes our work, and immediately reminded me of this tweet by Rachel Thomas : In the Deep Sets paper, the authors explain that set based modeling problems fall into two classes: In the permutation invariant case, you want to be able to re-order the inputs into your model without affecting the prediction (which is often into a space of a different dimension from your input). For example, at Instacart we could predict: How much time it will take to pick a basket of groceries at a store location Will a user add to cart any item given a query and a set of product search results How efficient will we be in a city given a set of deliveries and their location and due times, and a set of shoppers and their locations and current status In the permutation equivariant case, you will produce a predicted value for every input in the set, and you want to be able to re-order the inputs and ensure that the ordering of the outputs changes accordingly. For example, at Instacart we could predict: The probability that each item in a set will be picked by an in-store shopper next given the previous item and store (our Deep Learning with Emojis (Not Math) use case) Which of the products a user has purchased in the past they will re-purchase in their next order (our 3 Million Instacart Orders, Open Sourced use case) The paper proves that any such set based architecture must take the following form: For the permutation invariant case, the architecture will look like this: Where ϕ is an arbitrary neural network architecture applied iteratively over every set element 𝒙 (for example, using the Keras TimeDistributed layer wrapper ). The outputs of ϕ must then be summed along the set dimension, and can then be passed into yet another arbitrary neural network ⍴ , which can produce the final output predictions. For the permutation equivariant case, the architecture is the same as above, but instead of using ⍴ you use DeepSets layers: Where you can see that the output is invariant to the ordering of the input given the symmetry in weight sharing. The paper provides an obligatory MNIST example, where they seek to learn an architecture that can sum hand-written digits: In this case you want the architecture to be permutation invariant, so that sum(1, 2) = sum(2, 1), and to handle variable length input such as sum(1, 2, 7). Two simple alternative approaches both fail: On the left hand side, they concatenate the digits and pass them into a hidden layer, but this fails to handle variable sequence length inputs. On the right hand side, they pass them into a recurrent layer, but the results will not be order invariant. How big of a deal is that? In practice, they found that both GRU and LSTM layers failed dramatically to generalize to sequence lengths longer than 10: This paper is particularly rich with application examples, ranging from image tagging, to outlier detection, to point-cloud classification: Beyond all the hype, NIPS 2017 was an amazing event, and these three papers demonstrate how practically useful these conferences are for applied AI and Machine Learning work. In each case, the author’s work provided mathematical rigor, practical advice, and experimental validation for questions we have been pondering at Instacart. I hope that you are now as excited by these ideas as we are! If you are interested in working on one of the many challenging problems we have at Instacart, check out our careers page at careers.instacart.com . Again, I’d like to thank Balaji Lakshminarayanan , Scott Lundberg , Manzil Zaheer and their co-authors for their work, and to everyone involved in organizing NIPS 2017. I’d also like to thank Jeremy Howard for his feedback on this post. Instacart Engineering 350 2 Thanks to Montana Low and Jagannath Putrevu . Machine Learning Artificial Intelligence Deep Learning Visualization Data Science 350 claps 350 2 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-15"},
{"website": "InstaCart", "title": "jardin a dataframe based orm for python", "author": ["Emmanuel Turlay"], "link": "https://tech.instacart.com/jardin-a-dataframe-based-orm-for-python-178e02e1c21", "abstract": "Eng Open Source Data Science Machine Learning Android iOS jardin (noun, french, /ʒaʁ.dɛ̃/ ) — garden, yard, grove. Data Scientists who use Python as their language of choice love to play with Pandas dataframes . This data structure stores collections of rows in memory and allows for fast operations (filtering, joining, querying, aggregations, transformations, …) thanks to its algorithms tweaked in Cython . Pandas can be fed a SQL query as a string to return a dataframe: That is all well and nice but an abstraction layer is missing here. Writing queries as strings can become cumbersome in large code bases. Code is not easily reusable, it is not parametric and the database connection needs to be carried up and down the call stack. jardin aims to solve this. Using jardin, the above snipet would look like this. Once models have been defined centrally, by simply inheriting from the jardin.Model class, they can be used anywhere to easily query the associated table. Here are a bunch of ways to query a table with jardin: There are plenty of other fun ways to build a query, check out the documentation for an exhaustive view. Other types of queries can also be executed, here is a sample. This API makes for a much cleaner and portable code. Many more features are available in jardin: Support for master/replica database-split Support for multiple databases Has-many and belongs-to associations Query watermarking à la ActiveRecord query scopes Custom classes for individual records Connection drops recovery Replica lag measurements Transactions Check out the full documentation for more details and the GitHub repository . Instacart Engineering 171 Python Pandas Data Science Open Source 171 claps 171 Written by Rock’N’Roll nerd: Software engineer, cyclist, runner, guitarist, singer-songwriter, drummer, world contemplator, absurd, French. Instacart Engineering Written by Rock’N’Roll nerd: Software engineer, cyclist, runner, guitarist, singer-songwriter, drummer, world contemplator, absurd, French. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-11"},
{"website": "InstaCart", "title": "no order left behind no shopper left idle", "author": ["Jagannath Putrevu"], "link": "https://tech.instacart.com/no-order-left-behind-no-shopper-left-idle-24ba0600f04f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, our goal is to make grocery delivery accessible to everyone. In the last 9 months, we grew tremendously from a handful of established markets to over 100+ markets in the US. Such rapid expansion brings a lot of growth potential but also adds more unpredictability to our demand. Offering every customer same day delivery, while keeping our shoppers busy becomes a very hard problem. This post is about how we use Monte Carlo simulations to balance supply and demand in a rapidly growing, high-variance marketplace. Balancing supply and demand is extremely important for any company that is in the business of taking orders and fulfilling them. If demand exceeds supply, we have a system that decides to stop taking orders and that hinders us from honoring our promise of same day delivery. We define the percentage of demand that was lost on a given day as Lost Deliveries . If we have more supply than demand, our shoppers, the couriers who pick/deliver groceries, are left idle and that hurts their earning potential because they complete fewer orders per hour. Our shoppers like being busy and making good use of their time on our platform. If we had the same demand every day, we would staff exactly the same number of shoppers every day. But there are several factors contributing to unpredictability in our demand and supply, especially in our new markets, and that makes staffing a very hard problem. How do we staff the right number of shoppers in each market on each day for each hour, to make sure we do not lose too many deliveries and also not keep our shoppers idle? What makes this problem so hard? Where is the variance coming from? We will answer all of these questions, but first let’s define the problem we are trying to solve more formally. Problem Statement For a given market and a given day, how do we staff : number of shoppers at every hour so as to: minimize shopper idleness minimize lost deliveries The final output of this problem looks something like this: We call these numbers Staffing Levels . Variances We have to deal with the standard supply and demand variations, which only intensify during rush hours, hurricanes, snowstorms, Game of Thrones season finales and long weekends. Shopper productivity isn’t uniform either. Some can zip through an order at an item a minute, while some, especially new shoppers, take their time as they get used to the platform and ramp up. Shoppers also have the flexibility to cancel their shifts as needed, which adds even more unpredictability to our supply. Every component below has variance and impacts staffing decisions: Constraints There are several constraints that make the problem even more complicated. Our demand can be fulfilled from different store locations in each city. Stores are open for different hours. Our picking and delivery times can vary based on how busy the stores and streets are. There are constraints on how long a shopper can work with us if they are a part-time employee. We work with 160+ retailers, and often times we have to adhere to retailer specific rules. Unification of objectives We must also quantify the costs associated with idleness and lost deliveries, and unify them into a single function to make better staffing decisions. Idleness leads to increased labor costs while lost deliveries lead to losses in revenue. Lost deliveries can also have a long term cost if we lose a potential repeat customer. Heuristics based methods for staffing In the beginning, we solved this problem using a mixture of forecasting and heuristics based methods. The core idea behind these approaches was based on adaptive course correction: forecast using previous weeks’ staffing numbers correct for next week based on previous week’s idleness and lost deliveries adjust for next week’s demand forecast changes But with these approaches, we would often lose too many deliveries or keep too many of our shoppers idle. The dependencies on prior week’s outcomes can result in a potentially vicious feedback loop by settling at a local optimum. For example, it might be more optimal to put more shoppers in a store location with more order density around it, than at a store that is not as centrally located. Nuances like that are really hard to capture with these approaches. To deal with all this complexity and uncertainty, we decided to try Monte Carlo Simulations . Monte Carlo simulation methods offer several advantages for problems like this: no dependencies on previous week’s outcomes model complex systems as interactions between random variables perform stochastic optimization to deal with variances better The idea is to simulate a lot of different variations of demand and supply, and solve for a set of staffing levels that minimizes idleness and lost deliveries costs across all of the simulation runs. Each simulation needs to represent a future date for the market we want to staff for. This means that we need orders, stores, locations of shoppers, predictive models to figure out how long it takes to fulfill the orders, and an estimate on how many shoppers are likely to cancel. Simulating Orders We first have to start with how many orders we expect on a future date. We have a demand forecasting algorithm that produces a point estimate for demand for each market for each day. But the actual demand doesn’t always match the forecast and is prone to variance. To account for the variability, we build a distribution of our demand forecast based on its error distribution to determine the number of orders we need to fulfill in each simulation. Let’s say we sample a number from this distribution and it is 3100 deliveries. We then sample those many number of orders from all the orders that have been placed historically in that market. If we do this over and over again, we address both the variance in the number of orders, and also the variance in space and time density. Simulating Shoppers Not every shopper has the same style of working. Some are slow, some are super fast. We model their speeds as a gamma distribution and sample from that. They are also not static in space. They begin their shifts somewhere between their home locations and store locations, and can be anywhere between their last delivery location and a nearest store location during their shifts. By using Markov models, we model distributions of their locations. Staffing for each simulation Once we bootstrap a simulation with orders, shoppers, and stores, we figure out the ideal staffing required for that simulation. We do so by solving our fulfillment problem, which is basically a Vehicle Routing Problem (VRP) with time windows. We use the same algorithm that we actually use to fulfill our orders in a real-time basis, so as to be consistent and make planning match reality as closely as possible. Solving the above problem gives us an optimal set of routes, each of which has a start and end time, based on when they are due and how long it takes to fulfill them. From these optimal routes, we work backwards and figure out how many shoppers we need. This approach also allows us to automatically staff more shoppers in stores that are part of more optimal routes. Multiple Simulations To account for all the variability, we repeat the above process hundreds of times by repeatedly sampling and solving for staffing in each simulation. Solving For Final Staffing Levels Ultimately we have to solve for one final set of staffing levels that minimizes all our costs. Let’s say x_h (h: 8 AM to 10PM, x_h are integer variables that range from [0, ∞) ) represents the number of shoppers required at hour h . The objective is to solve for all x_h . In this particular simulation, if x_h take values represented by the red line, in the regions marked with 1️, we overstaff and hence incur an idleness cost. In the regions marked with 2, we understaff and hence incur a lost deliveries cost. By minimizing these costs across all simulation runs, we solve for all x_h : N : number of simulation runs h : hour of day ld_cost : lost deliveries cost x_h : non-negative integer variables l_h,i : shoppers required at hour h in simulation i The red line represents the values of x_h after solving the above optimization problem. Architecture Putting everything we discussed so far together, the architecture of our staffing engine looks like this: We have a core simulation engine that runs several scenarios. We then have an optimizer that takes these simulation runs and constructs a final set of staffing levels. To test the performance of this approach, we used the staffing levels generated through this approach in production and tracked metrics for our two main objectives: Lost Deliveries and Idleness, and compared them with our previous approaches. Comparing density plots of Lost Deliveries and Idleness In our markets that we have historically staffed high to encourage growth, we were able to significantly reduce idleness while maintaining almost the same amount of lost deliveries. This means that we are able to keep growing fast while keeping our shoppers busy and their earning potential high. In markets where we have historically kept staffing tight to maintain a low level of idleness, we were able to lower our lost deliveries while keeping our idleness the same. This makes us even more available for our customers and allows us to grow faster. In a follow up post, we will discuss how we expanded on this approach to staff for hyper-growth and some of the lessons we learnt from doing this project. If you are interested in working on one of the many challenging problems we have at Instacart, check out our careers page . Instacart Engineering 1.3K 16 On Demand Logistics Marketplaces Simulation Machine Learning 1.3K claps 1.3K 16 Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Written by Logistics @Instacart, Previously @WalmartLabs, Operations Research @GeorgiaTech. Tweets at twitter.com/jputrevu. Personal website: https://jagannath.work/ Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-02"},
{"website": "InstaCart", "title": "rxjava 1 rxjava 2 disposing subscriptions", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/rxjava-1-rxjava-2-disposing-subscriptions-48988798eccf", "abstract": "Eng Open Source Data Science Machine Learning Android iOS This is a continuation post in a 3 part series: Understanding the changes Disposing subscriptions Miscellaneous changes This was the part that I initially found most tricky to grasp but also most important to know as an AndroidDev (memory leak and all). Jedi master Karnok explains this best in the wiki: In RxJava 1.x, the interface rx.Subscription was responsible for stream and resource lifecycle management, namely unsubscribing a sequence and releasing general resources such as scheduled tasks. The Reactive-Streams specification took this name for specifying an interaction point between a source and a consumer: org.reactivestreams.Subscription allows requesting a positive amount from the upstream and allows cancelling the sequence. From that definition alone, it would appear like nothing’s changed but that is definitely not the case. In my first post, I pointed out: The use of => vs = was intentional. If you look at the source code for Publisher ’s subscribe method again, you’ll notice a return type of void viz. it doesn’t return a Subscription for you to tack on to a CompositeSubscription (which you can then conveniently dispose of onStop/onDestroy). Karnok again: Because Reactive-Streams base interface, org.reactivestreams.Publisher defines the subscribe() method as void, Flowable.subscribe(Subscriber) no longer returns any Subscription (or Disposable). The other base reactive types also follow this signature with their respective subscriber types. So if you look at the declarations again Notice the return type void in all of them. So you may ask how do I get a hold off that Subscription then (so that you might rightly cancel or dispose it off like a responsible citizen)? Let’s take a look at the the Subscriber ’s onSubscribe method: You are now given the Subscription class as a parameter in your onSubscribe callback. So within the OnSubscribe method, you have a hold of the subscription and can then conveniently dispose of the Subscription inside the onSubscribe callback. This was actually a pretty well thought off change because this really makes the interface for a Subscriber lightweight. In RxJava 1 land, Subscribers were more “heavy” cause they had to deal with a lot of the internal state handling. … but who are we kidding: that is not convenient at all (atleast for those of us who need the subscriber to depend on our lifecycle). I’d rather just shove everything into a CompositeSubscription like before and be on my merry way. But such are the rulings of the Reactive Streams spec. Thankfully the maintainers of RxJava in all their benevolence realized this trade-off and have remedied this with convenient helpers. But first, some more definitions: What we called Subscription in RxJava 1 is now called Disposable . Why couldn’t we just keep the name Subscription ? (per my understanding): You have to remember the Reactive Streams spec already has this name reserved and the maintainers of RxJava 2 are serious about the spec adherence. We don’t want confusion about there being more functionality with an Rx Subscription vs other Reactive Stream spec adhering libraries We still want some of the behaviors and conveniences of RxJava 1 like CompositeSubscriptions. So if Disposables is what we’re using now, by that token we have a CompositeDisposable which is the object you want to be using and tacking all your Disposables onto. It functions pretty similarly to how we used CompositeSubscription before. Ok, back to the original question: how do I get a hold of the Disposable? Now before we go any further, if you’re adding your callbacks directly in the form of lambdas, this is not a problem as most observable sources return a Disposable with their subscribe method call when not provided with a subscriber object: So if you look at some sample code, the below works fine no problem: However if I rewrote that code just a little differently: The above code won’t compile. If you want to pass a Subscriber object (like the above FlowableSubscriber , ObservableSource or an Observer ) then this strategy won’t work. A lot of existing RxJava 1 code uses this strategy a lot, so the RxJava maintainers very kindly added a handy method on most Publishers called subscribeWith . From the wiki: Due to the Reactive-Streams specification, Publisher.subscribe returns void and the pattern by itself no longer works in 2.0. To remedy this, the method E subscribeWith(E subscriber) has been added to each base reactive class which returns its input subscriber/observer as is. If you’re still following closely, you’d ask… wait! that doesn’t return a Disposable! why the hell is this even remotely more convenient? Well… it says that the Subscriber you pass is sent back to you with subscribeWith . But what if your Subscriber itself “implemented” the Disposable interface? If you had a DisposableSubscriber, you could for all practical purposes treat it as a disposable and tack it on to a CompositeDisposable, while still using it as a Subscriber. That’s typically the pattern you want to adopt. Here’s some code that should make these techniques clear: Apart from DisposableSubscriber , there’s also a ResourceSubscriber which implements Disposable. There’s also a DefaultSubscriber which doesn’t implement the Disposable interface, so you can’t use it with subscribeWith (you could use it but you wouldn’t get anything “disposable” out of it). It seems like both DisposableSubscriber and ResourceSubscriber do the same thing. Why do both of these exist you ask? The original 1.x Subscriber had the ability to take Subscriptions which allowed “disposing” the additional resources that particular Subscriber needed when the lifecycle ended or the Subscriber got unsubscribed. Since 2.x Subscriber is an interface declared externally, the old functionality had to be implemented via a separate abstract class: “ResourceSubscriber”. A key difference is you can create and associate Disposable resources with it and dispose them together from within the onError() and onComplete() methods you implement. Have a look at the example from within the docs There’s no longer an unsubscribe call on CompositeDisposable. It’s been renamed to dispose ☝️️ but you don’t want to be using either of those anyway. The clear method remains, and is most likely the method you want to use. unsubscribe/dispose terminates even future subscriptions while clear doesn’t allowing you to reuse the CompositeDisposable. In the next and final part, we’ll look at some of the miscellaneous changes. My thanks to Donn Felker & David Karnok for reviewing this post. Special thanks to David for correcting some of my misconceptions. Originally published at blog.kaush.co on June 21, 2017. Instacart Engineering 77 5 Rxjava2 Rxjava Reactive Streams Reactive Programming AndroidDev 77 claps 77 5 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-04"},
{"website": "InstaCart", "title": "rxjava 1 rxjava 2 understanding the changes", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/rxjava-1-rxjava-2-understanding-the-changes-5370c461bea", "abstract": "Eng Open Source Data Science Machine Learning Android iOS In case you haven’t heard: RxJava 2 was released sometime back. RxJava 2 was a massive rewrite with breaking apis (but for good reasons). Most dependent libraries have upgraded by now though, so you’re safe to pull that migration trigger with your codebases. Folks starting out directly with Rx2 might enjoy this guide but it’s the ones that started with Rx 1 that will probably appreciate it the most . I’ve split this guide into 3 parts: Understanding the changes Disposing subscriptions Miscellaneous changes Let’s get started. In this first part, I want to dive into making sense of the Rx2 changes from the point of view of an Rx1 user. Reactive Streams is a standard for doing “reactive” programming and RxJava now implements the Reactive Streams specs with version 2.x. RxJava was sort of a trailblazer in reactive programming land but it wasn’t the only library around. There were others that also dealt with reactive paradigms. But with all the libraries adhering to the Reactive Streams spec now, interop between the libraries is a tad bit easier. The spec per say is pretty straightforward with just 4 interfaces: Publisher (anything that publishes events, so Observable , Flowable etc. - more on this later) Subscriber (anything that listens to a Publisher) Subscription ( Publisher.subscribe(Subscriber) => Subscription when you join a Publisher and a Subscriber, you are given a connection also called a Subscription ) Processor (a Publisher + a Subscriber, sound familiar? yep Subject s for us RxJava 1 luddites) If you’re slightly more curious about the design goals, I also suggest the following resources: What’s different in 2.0 wiki page — this is really the place I kept coming back to and referencing when I needed to understand the details Fragmented Ep #53 with JakeWharton (forgive the shameless promotion) — ultimate lazy person’s guide to understand why/what things changed with RxJava2, as explained by an actual demigod. This was the one that really made it first click for me. Thought process behind the 2.0 design for the truly loyal Ep 11 of The Context — by my friends Hannes and Artem :) We should probably get one important change out of the way: Search: Replace: A minor change in your gradle dependency pull (“2” suffix). The actual classes though have been moved to a new package internally io.reactivex (vs rx ). So you’ll have to change those import statements. Also you “could” theoretically have both Rx1 and Rx2 running simultaneously but this is a bad idea because certain primary constructs like Observables have a very different notion of handling streams (backpressure). It can be a nightmare during that interim period where you have to remember the behavior differences. Also if you happen to use both Rx1 and Rx2 Observables you have to be careful about qualifying them explicitly with the right package name ( rx.Observable or io.reactivex.Observable ). This is very easy to mix up and get wrong. Bite the bullet and migrate it all in one shot. Another super important change: Flowable is the new Observable. Succinctly — it’s a backpressure-enabled base reactive class. You want to be using Flowable everywhere now, not Observable. Use this as your default. Observables are still available for use, but unless you really understand backpressure, you probably don’t want to be using them anymore. Flowable = Observable + backpressure handling Remember Publisher ? it’s basically the Reactive Streams interface for anything that produces events ( Reread the Reactive Streams spec section above if this is not making sense ). Flowable implements Publisher . This is our new base default reactive class and implements the Reactive Streams spec 1 <-> 1. Think of of Flowable as primero uno “Publisher” (this is also partly the reason I recommend Flowable as the new default, in the previous section). The other base reactive classes that are Publishers include Observable, Single, Completable and Maybe. But they don’t implement the Publisher interface directly. Why you ask? Well the other base classes are now considered “Rx” specific constructs with specialized behavior pertaining to Rx. These are not necessarily notions you would find in the Reactive Streams specs. We can look at the actual interface declarations and it’ll be clear. So if Publisher is the Reactive Streams event producer, Subscriber is the Reactive Streams event “listener” (it’s extremely helpful to keep these terms firmly grounded in our heads, hence the incessant repetition). Looking at the actual interface code declaration should offer more clarity to the above two sections. As noted before, Publisher and Subscriber are part of the Reactive Streams spec. Flowable -which is now numero uno base reactive class of choice- implements Publisher . All good so far. But what about the other base reactive classes like Observable and Single that we’ve come to love and use? On the publishing side, instead of implementing the standard Publisher interface the other event producers implement a “similar” interface. Notice: instead of having the standard Subscriber (Reactive Streams standard) the other base reactive classes ( Observable , Single etc.) now have corresponding “special” Rx specific Subscriber or event listeners called “Observer”s. That’s it for Part 1. In the next part , we’ll look at disposing subscriptions. My thanks to David Karnok & Donn Felker for reviewing this post. Originally published at blog.kaush.co on June 21, 2017. Instacart Engineering 96 1 Rxjava2 Rxjava Reactive Streams Reactive Programming AndroidDev 96 claps 96 1 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-04"},
{"website": "InstaCart", "title": "space time and groceries", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/space-time-and-groceries-a315925acf3a", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Grocery delivery visualized in python with datashader . At Instacart, we deliver a lot of groceries. By the end of next year, 80% of American households will be able to use Instacart. Our challenge: complete every delivery on-time, with the right groceries. Over the course of a week, we traverse cities all over the United States many times over while delivering groceries: How do we bring order to the chaos? In the remainder of this post, we’ll first introduce the logistics problem Instacart is solving, outline the architecture of our systems and describe the GPS data we collect. Then we will conclude by touring a series of datashader visualizations: Visualizations like these help us to build intuition about our system, generate hypotheses for improvements, sanity check our changes, identify best practices and improve our operations. But before we get too caught up in these visualizations, let’s first quickly cover the problem we are solving. When using our app to order groceries, you first choose a retailer, and then shop for groceries to be delivered. Over the course of a few hours, we have thousands of such orders to deliver. Doing this efficiently is the job of our logistics systems. At it’s simplest, our logistics problem can be viewed as solving a TSP ( traveling salesman problem ) where the shopper must go to the store first. For example, the shopper drives to the store, picks your groceries (along with two other orders), and then delivers them in a sequence: There are many algorithms for solving TSPs, and perfect solutions can be found for up to tens of thousands of deliveries. Even with millions of deliveries, heuristics can come within 2–3% of the optimal solution. But in practice we have a fleet of shoppers to fulfill orders. Each will be given a batch of orders to shop for, and will then deliver those orders in sequence: This problem is called a VRP ( Vehicle Routing Problem ), which generalizes the traveling salesman problem. ( Generalizes is a mathy euphemism for ‘even harder to solve optimally’.) But we can’t stop there. Instacart is named Insta cart for a reason — we commit to narrow delivery windows for our customers (usually 1 hour long). So only a subset of assigned routes will be viable, and we must jointly optimize the expected timeliness of our deliveries with the speed of our movement. This is called a VRPTW (Vehicle Routing Problem with Time Windows): If only life were so simple! In reality, not all of our shoppers are equivalent. Some have large vehicles, others have small ones. Some have club-cards for retailers like Costco, while others do not. Some can fulfill alcohol orders, while others may not. This means our problem is capacitated , and so we can add a big C to the front of our acronym. Furthermore, each vehicle can take more than one trip, which lets us append the letters MT (for Multiple Trips). So really, we have a CVRPTWMT: Oh, and everything evolves continuously under many sources of uncertainty. New orders are placed. Shoppers come on and off of shift. Weather, traffic and other events wreak havoc on plans. Such problems are referred to as being stochastic , which gives us one more letter — an S! So in the end, we are left to solve a SCVRPTWMT (😰). They say that the longer the acronym, the harder the problem is to solve. But don’t fret, all is not lost. A simple system can be implemented for routing shoppers that accomplishes some of our goals without a great deal of complexity: Sort orders by when they are due Find the shopper who is free that can do the first order the fastest Search remaining orders for any that can be added without being late Dispatch the orders found to this shopper Repeat This will optimize for fulfilling the most urgent orders in the most timely fashion, and seek efficiencies where possible as a secondary objective. We began with a variation of this kind of simple greedy algorithm, and have since introduced novel approaches that have had a dramatic impact on our speed, without compromising on late deliveries or order quality: Some of the changes we have introduced include: Machine learning to predict the distribution of time expected for any given shopper and assignment Decomposing the CVRPTW into sub-problems (clustering deliveries, shopper assignment) and solving these sub-problems to near optimality Applying heuristics for limiting search spaces, dealing with anomalies, fine-tuning solutions and adapting under uncertainty Re-computing batch plans every minute and making dispatching decisions just in time The application that decides what orders each shopper should fulfill is called our ‘fulfillment engine’, and it is just one component of our overall logistics system, which also forecasts demand and shopper behavior, manages capacity and busy pricing and plans and adapts our staffing: These systems are highly interdependent, and we are increasingly using simulations to optimize them jointly under many sources of uncertainty. In the months to come we will publish more detailed posts about these systems and the fun engineering, machine learning, optimization and operations challenges they present, so stay tuned! In order to optimize the assignment and routing of our shoppers, and to communicate effectively with our consumers, we collect a stream of GPS location data. For example, these are what ten updates might look like for a fictional shopper: Every ~10 seconds, we collect the timestamp, latitude and longitude, speed, direction and accuracy reported by the device. The latitude and longitude are shown here rounded to 4 digits, but are collected to 6 digits in production. The speed is measured in miles per hour (this fictional shopper might be walking to their car). The direction is in degrees, and is -1 when a shopper is at a halt. The accuracy is in meters, and indicates the expected error of the measurement from the real position. Over the course of a single day, we collect 10s of millions of these updates across the country. Datashader provides the ability to quickly and interactively visualize millions, or even billions of points. For more information on datashader, I recommend you start with their plotting pitfalls notebook. Many of the visualizations in this post are modeled after their NYC Taxi and OpenSky notebooks. Note that for these visualizations, we show only data points where shoppers are moving quickly while driving and delivering groceries, or we are zoomed into store locations. This is to protect the privacy of our shoppers and our customers. First, let’s inspect the accuracy of the GPS data we collect: The red points represent inaccurate measurements (more than 10 meters), whereas those in blue are accurate measurements (10 meters or less). We can immediately see that accuracy is poorer in the financial district (upper-right), where tall buildings obstruct the GPS. But even there the data accumulates to clearly show an outline of the streets. The measurements are also less accurate inside of any city block, where presumably the shoppers are indoors and the GPS signal is obstructed. We can zoom into one of our store locations and see the shoppers moving through the parking lot with highly accurate GPS locations, but losing that signal within the stores themselves: Furthermore, there appear to be buildings or other obstructions that ‘shade’ the GPS accuracy over certain parts of this parking lot (see the left hand side). We can filter the the data to just the accurate observations, and then color the observations based on the speed the shopper is moving at: This clearly shows our shoppers moving fastest on the highways in San Francisco, and slowest in the financial district. It also shows that moving quickly from the south to the north side of the city is difficult, as there are no fast routes making that connection. If we instead color each point by the direction the shopper was moving in, we can clearly see the organization of the city streets: One way roads alternate from one block to the next, some roads have traffic moving both ways, and other roads switch directions at certain intersections. The roundabouts and circular exit and entrance ramps make nice color wheels. When shoppers are delivering groceries, we know the store location they originated from, and so can color the map to visualize what stores frequently deliver to each neighborhood: Some stores dominate large areas, especially on the edge of the city. In more congested neighborhoods many streets are frequently traversed by shoppers from multiple stores, and the colors blend together into mixed hues. We also measure where each shopper is in their workflow at any given moment. This lets us see what shoppers are doing inside the store locations (when measurement is accurate enough): The checkout area (brown) is near the shopping area (purple), but the staging area (pink) is on another side of the store. Or, by visualizing paths instead of points, we can clearly see the movement of shoppers through store parking lots: Each lane is (mostly) one way (pink or orange), and shoppers enter the store to pick up groceries on one side of the building (blue). You can even see where shoppers typically park while waiting for their next order (yellow). Visualizations like these help us to: Build intuition for how our logistics system functions at scale Generate hypotheses for ways to improve our algorithms or operations Confirm that changes to production have the expected behavior Make better operational decisions about parking spaces, store locations and our product offering If you are interested in joining the team to help us engineer, optimize or analyze our logistics systems, or to work on any of the other many challenging problems we have at Instacart, check out our careers page at careers.instacart.com . Instacart Engineering 1.2K 10 Thanks to Jagannath Putrevu . Data Visualization On Demand Logistics Maps Machine Learning 1.2K claps 1.2K 10 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-30"},
{"website": "InstaCart", "title": "bootstrapping postgres users", "author": ["Andrew Kane"], "link": "https://tech.instacart.com/bootstrapping-postgres-users-cd594e5f28e9", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Setting up database users for an app can be challenging if you don’t do it often. Good permissions add a layer of security and minimize the chance of developer mistakes. The three types of users we’ll cover are: migrations , for schema changes apps , for reading and writing data analytics , for data analysis and reporting Before we jump into it, there’s something you should know about new databases. After creating a new database, all users can access it and create tables in the public schema. This isn’t what we want. To fix this, run: Be sure to replace mydb with your database name. PostgreSQL uses the concept of roles to manage privileges. Roles can be used to define groups and users. A user is simply a role with a password and permission to log in. The approach we’ll take is to create a group and add users to it. This makes it easy to rotate credentials in the future: just add a second user to the group, set your app’s configuration to the new user, and remove the original one. First, we need a group to manage the schema. We could use a superuser, but this isn’t a great idea, as superusers can access all databases, change permissions, and create new roles. Instead, let’s create a new group. We set a lock timeout so migrations don’t disrupt normal database activity while attempting to acquire a lock. Now, we can create a user who’s a member of the group. The last statement ensures tables created by the user are owned by the group. You can generate a nice password from the command line with: Next, let’s create a group for our app. It’ll need to read and write data but shouldn’t need to modify the schema or truncate tables. We also want to set a statement timeout to prevent long running queries from degrading database performance. Note: The default privileges statements reference the group used for migrations. If you use Amazon RDS, you must run these statements as the migrator user we created above (since you don’t have access to a true superuser). Then, create a user with: Finally, let’s create a group to be used for data analysis, reporting, and business intelligence tools (like Blazer , our open-source one). These users are often referred to as a read-only users . We don’t want them to be able to mistakenly update data. Once again, creating a user is relatively straightforward. You now know how to create different types of Postgres users. Spending a bit of time upfront to configure your users can make them easier to manage in the long run. This should give you a nice foundation. If you’re passionate about building infrastructure, we’re hiring . For Postgres performance alerts, check out PgHero . Instacart Engineering 51 1 Postgres Database Web Development Software Development 51 claps 51 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-23"},
{"website": "InstaCart", "title": "3 million instacart orders open sourced", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Curious about the food Americans eat? Look no further. Instacart is excited to announce our first public dataset release, “The Instacart Online Grocery Shopping Dataset 2017”. This anonymized dataset contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders. In this post, we’ll briefly introduce the dataset and why it is important to Instacart and provide a link to download the data. Then, we’ll finish by highlighting a few interesting patterns we’ve found in the data. Can you guess what product is most likely to be ordered late at night? We hope the machine learning community will use this data to test models for predicting products that a user will buy again, try for the first time or add to cart next during a session. Instacart currently uses XGBoost , word2vec and Annoy in production on similar data to sort items for users to “buy again”: and to recommend items for users while they shop: This data, and the algorithms trained upon it, are enabling Instacart to revolutionize how consumers discover and purchase groceries. The dataset is provided as-is for non-commercial use, and can be downloaded from S3 at: https://www.instacart.com/datasets/grocery-shopping-2017 and is subject to our Terms and Conditions . For information about the contents of the files, see this data dictionary . For example, the first two orders for user_id 1 are: If you make use of this dataset, please use the following citation: “The Instacart Online Grocery Shopping Dataset 2017”, Accessed from https://www.instacart.com/datasets/grocery-shopping-2017 on <date> If you have questions about this dataset, you can reach out to us directly at open.data@instacart.com . We have taken great care to protect the privacy of our users and retail partners and to ensure that the data is entirely anonymous: The only information provided about users is their sequence of orders and the products in those orders All of the IDs in the dataset are entirely randomized, and cannot be linked back to any other ID Only products that are bought by multiple people at multiple retailers are included, and no retailer ID is provided Note that this dataset includes orders from many different retailers and is a heavily biased subset of Instacart’s production data, and so is not a representative sample of our products, users or their purchasing behavior. There are many interesting patterns to be found in this dataset. For example, this plot shows that the most common aisles are more likely to be reordered from. Fruits are reordered more frequently than vegetables — perhaps because vegetables are more intermittently purchased for recipes. Staples like soups and baking ingredients are least likely to be reordered — perhaps because they are less frequently needed. We can also see the time of day that users purchase specific products. Healthier snacks and staples tend to be purchased earlier in the day, whereas ice cream (especially Half Baked and The Tonight Dough) are far more popular when customers are ordering in the evening. In fact, of the top 25 latest ordered products, the first 24 are ice cream! The last one, of course, is a frozen pizza. We are excited to see what research the data inspires and enables. If you are interested in joining us to work with the real 🍨, check out our careers site . Instacart Engineering 2.2K 34 Data Science Machine Learning Open Data Grocery Shopping 2.2K claps 2.2K 34 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-04"},
{"website": "InstaCart", "title": "lce modeling data loading in rxjava", "author": ["Laimonas Turauskas"], "link": "https://tech.instacart.com/lce-modeling-data-loading-in-rxjava-b798ac98d80", "abstract": "Eng Open Source Data Science Machine Learning Android iOS If you are a mobile application developer, one situation you will commonly run into is loading and displaying data to a user. With that reality, there are almost always two caveats: Data might take time to arrive, or It might fail to arrive due to unforeseen circumstances like a bad network connection A common situation when using RxJava to load data As long as data loading conditions are very simple, this is effective. But, what if the conditions defining the data can change and we need to reload the data? What if data is updated and we need to propagate that update? We can try to model it in an idiomatic way There are a couple of issues here: We lose the context of when the data is refreshing/loading Either we are not propagating errors or it will kill our stream To solve this issue, let’s create an immutable value type that provides this context to us. Let’s update our repository signature to Now we can consume the data So far, I’ve only shown the stubbed version of Observable<Lce<Data>>. The reason is that it is an implementation detail. How you construct it will depend on your data requirements. Here is how a simple single request implementation could look like when using a Retrofit api If you want it to be refreshable The view implementation doesn’t have to change much or at all to handle this change. We are hiding implementation details from the view layer while at the same time keeping the logic unit testable to ensure correct behavior. Let’s say that the view requires data combined from two sources. We can easily compose these observables into a single stream If something triggers user or content update, this stream will propagate the correct events down to the view. So if either the user or the content is loading, the view will show that. On the Instacart consumer app, we require user data to be available before we make any subsequent API calls. To display the store chooser, we need the list of stores. Stores depend on the zip code you are in. Every time the zip code changes, we need to refresh the stores. The conditions for when to show the progress bar or error state often become messy and complicated. I’ve seen and wrote screens where showProgress(true/false) was sprinkled across many methods. This made adding and modifying code very complicated where a small change could easily break a loading condition on some edge case. This pattern helps to avoid such situations, while allowing loading / content / error logic to be tested. Update: you can find the library here https://github.com/Laimiux/lce Instacart Engineering 613 7 Rxjava Android Reactive State AndroidDev 613 claps 613 7 Written by Android, techno, politics in no particular order Instacart Engineering Written by Android, techno, politics in no particular order Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-19"},
{"website": "InstaCart", "title": "the garden instacarts physical staging environment", "author": ["Arnaud Ferreri"], "link": "https://tech.instacart.com/the-garden-instacarts-physical-staging-environment-7204fd063616", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We move at an extraordinary pace at Instacart. In order to release new versions of our products multiple times a day, we need a testing infrastructure that can keep up. Testing our product end-to-end is extremely difficult though because our service has to handle pretty complex transactions in the physical world. It became clear to us: in order to roll out features confidently, we need to be testing at real physical grocery stores. However, driving to the grocery store is a pain! Just as Instacart solves this pain for our customers, The Garden solves this pain for our engineering, product & design teams. When a staging server is not enough, engineers use The Garden . The Garden is a replica of a typical grocery store inside of Instacart’s San Francisco office. We use this space to simulate orders and to mimic the conditions of a real grocery store. Spill on aisle 6? Carts in your way? No cell service? We can test any scenario imaginable. At a recent company hackathon, a team formed around the idea of building a grocery store in our office. The hackathon was a mere 48 hours long, making a project of this scale a bold proposition (exactly the kind of challenge we like 😉). On the first day of the hackathon we ordered shelving and had them delivered the next day. We ordered fake food online, and ordered real boxed and canned items for delivery to the office via Instacart. We inventoried all of the items and created a digital storefront available on instacart.com for employees only. We set it up in our Shopper app too so that we could play the role of a Personal Shopper fulfilling an order! In practice, we use The Garden every day to test new releases, new ideas and new processes and procedures before we test them with our retailer partners. The Garden also allows us to rapidly test experimental features, for example: creating 3D GPS-style maps of products in a retail store evaluating the quality of barcode scanning algorithms under difficult lighting conditions (on an assortment of test devices) building new technology, processes and workflows for Shoppers, and then inviting them to try the new tech in a safe space Testing new apps we’re working on for Customers and performing user research studies For us, The Garden has become a physical staging server. We can deploy new ideas both digitally and physically to run tests. We can simulate a number of scenarios which would be impossible to do in production. And most importantly, we can move at Instacart speed! Instacart Engineering 169 3 Staging Testing Hackathon Software Development Culture 169 claps 169 3 Written by pedestrian / musician / photographer / living in SF / listening to spotify / carrot at Instacart. Instacart Engineering Written by pedestrian / musician / photographer / living in SF / listening to spotify / carrot at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-13"},
{"website": "InstaCart", "title": "deep learning with emojis not math", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Sorting shopping lists with deep learning using Keras and Tensorflow . Shopping for groceries is hard. Stores are large and have complex layouts that are confusing to navigate. The hummus you want could be in the dairy section, the deli section, or somewhere else entirely. Efficiently navigating a store can be a daunting task. At Instacart , our customers can order millions of products from hundreds of retail partners. Our fleet of tens of thousands of personal shoppers must find these items at thousands of store locations. We are always looking for opportunities to enable our shoppers to move faster. By observing how our shoppers have picked millions of customer orders through our app, we have built models that predict the sequences our fastest shoppers will follow. Then, when a shopper is given a new order to pick, we use this predicted fastest sequence to sort the items for them. This approach has reduced our shopping times by minute s per trip. At scale, every minute saved will translate into 618 years of shopping time per year. So how do we do it? First, we can’t build warehouses, get accurate store data or map each store location. Also, traditional machine learning approaches (we ❤️ XGBoost ) don’t work either due to the sequential nature of the problem. So instead, we spread some deep learning on it. We ran a test where every batch (a set of items to be picked by a shopper) was randomly assigned to one of four list sorting algorithms: Control (red): departments sorted in a random order; items sorted alphabetically within departments Human (green): aisles sorted by humans using store layouts; items sorted alphabetically within aisles TSP (teal): a traveling salesman solution using average inter-department picking times; items sorted alphabetically within departments Deep (purple) : our final deep learning architecture, which directly sorts items in the batch We then look at how each sort performed in terms of shopper speed (y-axis) as a function of the size of the batch picked (x-axis): The control sort performed the worst (as expected). The TSP and Human sorts performed significantly better, but were statistically no different from each-other. The deep learning model beat them all by a large margin — the increase in picking speed from human to deep learning is 50% higher than from control to human at large batch sizes. In the remainder of this post, we will define the problem (using emojis of course), and then introduce a naive initial architecture. We will inspect that architecture and point out some key flaws, and then conclude with a final architecture that is more efficient and effective. Suppose a customer ordered 10 items and their personal shopper picked those items in this sequence: We can observe this sequence, as our shoppers weigh or scan bar codes for every item picked. In order to learn the sequence, we need to formulate this as a supervised learning problem. Suppose that we are looking back in time at this order, and we pause after the shopper picks the 🍞: We want to predict the next item that the shopper will pick (a 🍪 in this case), given they just picked the 🍞 and can choose from one of the five candidate products remaining (🍪🍫🍕🍖☕). Are you getting hungry yet? In emoji-math (one of the benefits of working at Instacart is doing emoji math), we can re-write this as: Note that this probability is non-trivial to compute. It’s not enough to ask how often cookies are picked after bread. Cookies may be incredibly common (they are in my household), so this naive probability might be biased high. We want to measure how likely cookies are to be chosen given we can only choose from a fixed set of remaining items. Our initial deep learning architecture, implemented in Tensorflow using Keras, was the following: We begin on the left hand side with the product that was last picked (🍞), and end on the right hand side with a prediction for the product that was picked next (🍪). Along the way, we have to account for the candidate set of products that could have been selected. First, let’s cover the embeddings: Using the Embedding layer in Keras, we embed the 🍞 into a 10-dimensional vector space. This uses 10 million parameters since we have 1 million potential products to consider. Similarly, we embed the store locations (1k stores = 10k parameters) and shoppers (10k shoppers = 100k parameters). The store location embedding enables the model to learn store layouts and generalize learnings across retailers and locations. The shopper embedding learns that shoppers may take consistently different routes through stores. We proceed by concatenating all of these embeddings together into a 30-dimensional vector, which is fed into a sequence of fully connected Dense hidden layers (700k parameters) with non-linear activations ( relu ). This produces a final vector of length 10 which captures all of the relevant information about the prior product, store location and shopper needed to decide which future item is most likely to be picked next. To produce a prediction, we have to ‘fan out’ in a final layer, which operates like a reverse embedding. This projects our 10-dimensional vector into the space of potential future products. This was accomplished with a Dense layer using a linear activation function. Then we can compute a masked-soft max, which turns our million length candidate vector into zeros for all non-candidates and positive probability estimates for the 5 candidate products. This was accomplished by combining a Lambda layer to exponentiate, followed by a Merge layer to mask the candidates, followed by another Lambda layer to ensure the probabilities sum to 100%. Suppose that in this case we predict that 🍪 (the right answer) is 32% likely to be chosen next. Then this prediction can be fed into a categorical_crossentropy loss function along with the indicator that 🍪 was, in-fact, chosen next in this case. Tensorflow can then backpropogate the errors to train the final, hidden and embedding layers, and we can learn this model at scale. This architecture can be implemented in Keras using the following code: The final trick to training this model is to use fit_generator to limit computing the dense million-length candidate input to small batch sizes. The most interesting part of this network is the product embedding on the left hand side. We can project this embedding down to a 2-dimensional space using t-SNE for dimensionality reduction: Each circle represents a product, and is sized in proportion to how often it is picked. Clearly the model has learned an interesting structure. We can reveal most of this structure by color-coding every department: Most of these clusters correspond to departments, even though the department data was never used to learn the embeddings. Further, we can zoom into a region, like the blue meat and seafood department in the upper left. There are other products that appear near the meat and seafood, but aren’t meat and seafood. Instead, these are products (like spices, marinades, deli or other items) that are sold at the meat and seafood counter. The model is learning the organization of the store better than the department and aisle encoding data we have. While this architecture works, it suffers from three deficiencies: First, we embed products into 10-dimensions on the left hand side, then later implement what essentially amounts to a reversal of this embedding in the final layer to project into the “next product” space. Thus, the product embedding and final layer are trying to learn similar relationships. Second, we have a very long and dense 1-million length vector being operated on in the middle, but in this example only 5 of those million values are relevant for the final outcome. This represents a lot of inefficient memory use and wasted computations. Finally, there is no way to inject additional metadata about the candidate products into this architecture. If we want to learn from the aisle and department we have associated with products, we can feed them into the left hand side of this network based on the last product (bread). But if cookies are in the same aisle, there is no way to represent this information in this architecture. The candidate items are simply binary masks applied to the million length score vector. Our final architecture addresses these limitations, and produced significant performance and efficiency gains. We will create a ‘scoring generator’ that will be re-used with the TimeDistributed layer in Keras for each candidate. To illustrate this, let’s take the 🍫 and move it to the left hand side of the architecture: Then we can use a Keras shared layer in the functional API to use the product embedding for the 🍞 to also project the 🍫 into the same 10-dimensional space: Keep in mind that the number of parameters do not increase when we do this — they are simply re-used for multiple purposes (and can be optimized jointly). We can then use a Merge layer to concatenate all of these embeddings together into a 40-dimensional vector, and feed that into hidden layers, which then produce a score: In this case, 🍫 is given a score of 0.7. We will design the rest of the architecture so that this score is used as a real-valued indicator of how likely 🍫 is to be picked next given we just picked 🍞. Positive values are more likely to be picked next, and negative values are less likely to be picked next. Let’s hide the complexity of this component to the architecture under a ‘score generator’ module: Which will have in total 10.81 million parameters, and can be simplified as: This score generator depends on the candidate (🍫), the prior product (🍞), the store and shopper. We can replicate this process to score the remaining candidate items: Suppose we find that the 🍖 is less likely to be selected next (-0.2), the ☕ is least likely to be selected next (-0.4), and the 🍪 is most likely to be selected next (1.3), followed closely by the 🍕 (1.1). All of these scores can be fed into a simple soft-max layer, which produces a prediction that the 🍪 has a 36% probability of being selected next: This prediction is then fed into the cross-entropy loss function, along with the indicator that a 🍪 was, in-fact selected next. What is key to this architecture is that we can share the score generator over all of the candidates, using the TimeDistributed layer in Keras. Thus, we still only use 10.81 million parameters, and TensorFlow can compute the gradient updates for the cross entropy loss jointly over all of the score generators for all of the candidates for a single sequence position. For now, we leave the Keras code for producing this final architecture as an exercise for the reader 😉. Using this architecture, we achieved the following improvements over the initial architecture: The model trains 10x faster because (a) we have cut the number of parameters in half and (b) we don’t have the wasteful 1 million length intermediate vectors. We achieved a 10% higher prediction accuracy because we were able to inject aisle and department metadata about both the prior product picked and the candidate product. Finally, our prediction times were slower by a factor of 2x, from ~50ms to ~100ms for generating the full predicted sequence for a 20-item order (requires 20 calls to predict ). This was acceptable given our production requirements, where lists need not be sorted in real-time. We were thrilled with this reduction in training time and accuracy increase despite the reduction in prediction speed! This work has been a very quick and iterative process over the past four months. At times, we went to Whole Foods with a Jupyter notebook running an early iteration of the model sitting in a shopping cart to test the quality of the generated sequences. Nobody even batted an eye — that’s the bay area for you! We are currently testing whether lists that are personalized to the shopper perform better than those based on our fastest shoppers. Next up on our roadmap is to use LSTMs to embed product descriptions and CNNs to embed product images, to increase generalization performance over rarely shopped for products. We are also planning to test using LSTMs to model the full picking sequence. A huge thanks to our core team focused on improving our shopping app list sorting: If you are interested in working on one of the many challenging problems we have at Instacart, check out our careers page at careers.instacart.com . This post was significantly improved because of the feedback of many. A special thanks to Greg Brockman , Ilya Sutskever and Andrej Karpathy from OpenAI , and to Daniel Gross of Y Combinator for their suggestions! Instacart Engineering 1.8K 17 Thanks to Max Mullen . Machine Learning Deep Learning Emoji TensorFlow Grocery Shopping 1.8K claps 1.8K 17 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-29"},
{"website": "InstaCart", "title": "safeguard avoiding silent failures with swift", "author": ["Dan Loman"], "link": "https://tech.instacart.com/safeguard-avoiding-silent-failures-with-swift-7e714444a525", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Instacart ❤️’s Swift. For the past year and a half or so, we have been actively moving our customer-facing* iOS codebase from Objective-C to Swift and have all been thoroughly enjoying the transition. There are a lot of advantages gained with the move to Swift and the Swift-ier our code gets, the happier you’ll tend to see us (well, except for Min, he’s our Prince of Darkness… 😈😉) As with anything, there are a couple of quirks to the language and the evolving conventions that come along with it. Swift, more than almost anything else, is focused on safety . In most cases, as a Swift developer, you want to do whatever it takes to avoid a 💥CRASH💥. The guard statement is a great tool for this. If you want to make sure a condition exists prior to executing some code, consider utilizing a guard statement, and you’ll likely be safe to assume things are as you’d expect them to be. Crash successfully avoided. Yay! Using a guard let even allows you to unwrap Swift’s Optionals and access their unwrapped values below the guard . Awesome. Seriously. We love it. With guard let , however, you find yourself creating a fair amount of these: This else block is a great place to handle the unexpected case, but, when guard is used extensively throughout your app, that’s a lot of special cases when you were probably just assuming that you were unwrapping an Optional that should have some value. It seems likely you don’t expect that else block to run fairly often. So, as is seeming to be the general Swift convention these days, you probably leave it to else { return } , except for a few rare cases. That. That right there. That’s where we see an issue. Unless you are consistently very good about logging or handling the else block appropriately, you basically have no idea how often it’s failing in the wild. 🙀 Ohhh jeez. No one likes being left in the dark like that. To handle this situation, and give ourselves a general idea of how to track down where issues may be occurring more often than we’d think, we wrote a lightweight framework which we have called Safeguard . The meat and potatoes of what Safeguard consists of, is really just a simple extension on Optional with built in logging capabilities that can easily plug into your existing logging system. By implementing the framework's safeguard() function in mission-critical areas, we are able to pass important information to our logger to help us track down what the issue may be. safeguard() passes the #function where it was called, the #file , the #line and the Type . In addition to these clues, with Safeguard ’s extra customLoggingParams , we can pass up relevant session info which may help us reproduce and diagnose the problem later on. Safeguard also provides us with a customizable callback: nilHandler , which makes it easy to manage custom use cases when an Optional has failed to unwrap. This nilHandler also conveniently passes a Bool flag to indicate whether or not the app is running in DEBUG mode. Agh! So many helpful things!!! To implement safeguard(), and give ourselves this nice bit of reassurance, we can add all of the above functionality to any Optional simply by adding: Pretty easy. One word added. World saved. Woot woot! 🎉 In the spirit of saving the world and sharing all the goodness, we have open-sourced Safeguard ( here ), and have made it easy to install through either Carthage or Cocoapods . We’ve also included some basic installation , usage and configuration instructions in the README file. (Of course, if you have any suggestions for improvements or additions, please feel free to add an issue or submit a pull request !) We hope you enjoy turning all those stupid 🙉 silent failures into 📣 loud failures! Till next time, -Dan & the Customer iOS team Instacart Engineering 7 Swift iOS 7 claps 7 Written by iOS Engineer @ Instacart — bringing 🥕 to an iPhone near you Instacart Engineering Written by iOS Engineer @ Instacart — bringing 🥕 to an iPhone near you Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-07"},
{"website": "InstaCart", "title": "productivity tip github integration with your editor", "author": ["Ryan Dick"], "link": "https://tech.instacart.com/productivity-tip-github-integration-with-your-editor-a5287f19a152", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Here at Instacart, we care a lot about developer happiness and productivity. We share tips with each other and will start posting them here. You can follow along with this link . When using a Github feature, it’s often related to something you’re working on locally. For example, creating a pull request for our current branch, or linking to a line of code you’re currently editing. This can mean a significant amount of context switching if it’s done frequently. To smooth this process, it can be helpful to set up integration between Github and your local environment. Below are some suggested tools based on your editor that can help. Check out the Sublime Github package . fugitive.vim is a popular package, and you can use the :Gbrowse command to open a file in Github. Check out the Open on Github package . These IDEs ship with Github integration. It can be helpful to set up shortcut key bindings for frequently used Github features. Happy hacking! Instacart Engineering 2 Github Sublimetext Instacart Productivity Tip Software Development 2 claps 2 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-05"},
{"website": "InstaCart", "title": "productivity tip aliases", "author": ["Andrew Kane"], "link": "https://tech.instacart.com/productivity-tip-aliases-9f675733aaee", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Here at Instacart, we care a lot about developer happiness and productivity. We share tips with each other and will start posting them here. You can follow along with this link . From time to time, it’s nice to check you have aliases set up for commands you run the most. Huffshell makes it easy to do a quick check. Your fingers will thank you. Instacart Engineering 1 Engineering Productivity Productivity Tip Tip Software Development 1 clap 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-26"},
{"website": "InstaCart", "title": "our first engineering intern", "author": ["Dave Schwantes"], "link": "https://tech.instacart.com/our-first-engineering-intern-fbf949f9227f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Today, we’re catching up with Dave Schwantes, senior software engineer, and Andrea Coravos, Instacart’s first software engineering intern, to learn more about what it’s like to work at Instacart. Dave: About a year ago we started designing Instacart’s first engineering internship program but it got shelved in order to focus on other company goals. Then back in April my teammate Erik suggested to our VP that we should have an internship program. He made a pretty convincing case and soon we were working together with other members of the Engineering and Recruiting teams to bring on our first interns. Dave: The goals of this program are threefold. First we want to provide an enriching experience for new engineers, giving them the chance to make a real impact while growing their skill set. Also we want to provide mentorship and leadership opportunities for our existing senior engineers, which helps to grow our engineering culture. Finally we want to bring diverse perspectives into the company. New faces bring new ideas and new ways of looking at problems. Dave: The most important skills for a developer at any level to have are curiosity and tenacity. New developers in particular will be inundated with technologies, methodologies, and problems that they may have never seen before and in order to thrive they need to honestly want to understand. We have a lot to do as a company and we’re looking for interns that can come in and make a real impact quickly. Strong technical skills are obviously important but a true desire to grow and make an impact are what make an ideal candidate. Dave: We’re the Orders/Express engineering team. Our team’s job is to handle everything that relates to the order process: we integrate our payment processing, build the discounts infrastructure, and create an infrastructure to deal with fraud and debt. We also work on Instacart Express , our subscription service that offers perks like unlimited free delivery. Our team members are (left to right) Erik Michaels-Ober, Emmanuel Turlay, Ashu Khaitan, Andrea Coravos, Dave Schwantes (me), Maria Kalalang, and Muffy Barkocy. Andrea: Best team ever. Working on payments is high-stakes, and any bugs or badly-written code hits the bottom-line quickly. I was fortunate to work with some of the best engineers, PMs and designers. Andrea: My first day at Instacart was a spot-on introduction into what my summer would look like. When I got to my desk, Dave showed me around and then said “We’re in this together, and the goal is for you to ship production code before you go home.” I was both nervous and surprised, because most of my friends told me that it’s rare for interns to work on production-level work, and I should expect to cut my teeth on a side-project for a few weeks before working on the main codebase. But not at Instacart. By the end of the day, my dev environment was up and running and I had code merged to master and deployed on Instacart.com. That’s in the blood at Instacart. Every day over the summer I wrote something that shipped on a mostly daily basis. Compared to my friends who interned at other places, I feel like I had more responsibility to work on big, important parts of the business. My days were varied. I worked a third of the time fixing bugs, which was phenomenal way for me to get to learn our codebase — and also to interact with our Happiness support team to better understand how our customers use the product. The rest of the time I worked on my main project, the debt collection infrastructure. That included everything from building out the back-end infrastructure to proposing and implementing the product design and UX. Most of my work was back-end Ruby/Rails/SQL, which was my preference, but I also got to work on our front-end, too. We have a sweet single-page-app that uses React.js. Since Instacart is still relatively small, a little over 300 people total, with 250 of us in San Francisco, many of us eat lunch together in the cafe at noon. There’s a yoga room that I used a lot for workouts after the day ended. We also have fun events like the Instacart offsite and well-known speakers who come to talk, including Jeff Weiner from LinkedIn and Kakul Srivastava from GitHub. Andrea: Overall, the best was that I never felt like an “intern.” From the first day, I felt I welcomed like any other full-time teammate. Instacart is great at making sure everyone sees all parts of the business. I worked as a “shopper”, meaning I worked at Whole Foods filling Instacart customer orders. I shadowed one of our Happiness service agents to see what our customers call in for and how we help them. The training is exceptional. Because I was working on features and bugs that were in production, I had many extra eyes on my work and helpful code reviews. I also had a lot of space during my day to dive into areas that I wasn’t as good at yet. My team is stacked with senior engineers who are some of the best mentors I’ve had to date. I got involved in some of the other initiatives at Instacart, like Open Source Summer, where the engineering team would dedicate time to build out Instacart’s open source offerings . There were also many events in the evenings. Instacart is an Andreessen Horowitz and YC portfolio company, and both of those firms organize events for interns: a16z had multiple events including a BBQ and YC hosted an intern dinner with Qasar Younis (pictured below with the interns at Coinbase). Andrea: I’m proud to work on a product that gives more people access to food. Instacart makes people’s lives better, they can spend more time with their families and friends, and it’s fun to see all the customer love. I’m most proud to have shipped features that affect a huge number of people who use our service every day. Everytime someone buys something on instacart, code that I wrote impacts that experience. Dave: Seeing Andrea grow as a software engineer during her time at Instacart was incredibly rewarding. Knowing that we’ve built a team and company culture where everyone feels supported in a way that allows them to flourish makes me really proud to work here. Andrea: I learned a lot this summer. These were my top three lessons: Being comfortable with being uncomfortable. If you’re not occasionally breaking something, perhaps the scope of the work is too small. Feeling nervous to ship code, but realizing but it’s more dangerous to have a fix on your computer rather than deployed. I wrote a bug fix that impacted a large number of users on the site, and I waited to deploy it for hours because I was nervous. My teammate, Ted, gave me the best reframe: it’s more “dangerous” to leave the PR sitting idly on GitHub rather than in production, likely fixing the problem. The mind, once stretched by a new idea, never goes back to its original dimensions. Deliveries are a complicated business that require tight integration with retailers, customers, and operations — all while making the process as simple as possible to use. Working in software is empowering. My time at Instacart opened my eyes to how even seemingly impossible challenges can have solutions. Dave: I learned that working with newer developers can really help strengthen my own skills. Having to come up with clear explanations for systems or techniques that have become second nature really forced me to build better understandings for myself. There’s nothing that solidifies (and grows!) your knowledge like sharing it with someone who is eager to learn. Dave: Absolutely. After having a fantastic experience with Andrea we’re really excited to expand the program. We currently have another intern, Dan, doing iOS work on our consumer products team. He actually just accepted our offer to join us in a full time role! We’re also working with Path Forward , an organization that helps people who left the workforce to be caregivers get back into their careers. I’ll also be at a few upcoming college recruiting events, talking to people about internship opportunities for next summer. If you are interested in joining us, please check out our careers page . Instacart Engineering 62 Engineering Software Development Culture 62 claps 62 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "offline first introducing truetime for swift and android", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/offline-first-introducing-truetime-for-swift-and-android-15e5d968df96", "abstract": "Eng Open Source Data Science Machine Learning Android iOS TrueTime, an NTP library for Swift and Android . A common complaint we’ve heard from our shoppers is that when they’re deep in the aisles of a grocery store, the app is frequently held hostage to spotty network connectivity. We try to empower our shoppers with the app and give them all the tools they need to fulfill their order quickly. Spotty network connectivity is a big problem. In this series of blog posts, we’d like to share with you some of our ideas and learnings from moving primary parts of our shopper app offline — making the apps for our shoppers first-class partners to our APIs. Here’s what we have in store: Part 1: Introducing TrueTime Part 2: Batch Processing Requests Part 3: Tighter Instrumentation So how does one solve the problem of spotty network connectivity between our client app and the server? Simple! You move the logic to the client — a deceptively straightforward objective. We’d like to share one of our first solutions that helped achieve that goal. The device is now considered the source of truth. This includes the time we send with up with requests — no longer can we trust the time when the server receives an API request, as the actual action could have happened many minutes ago. If you want the current time, you have to ask the device and trust it. Here’s an exercise though: go to your device settings, change your device’s date and time. Now try initializing a new Date() or an NSDate(). It will reflect your date and time changes. It may be surprising that there is no simple way to get a time that is unaffected by user time zone or clock adjustments, but neither iOS nor Android provide this functionality. Users may do this for a variety of reasons — like being in different timezones, or trying to be punctual by setting their clocks 5 minutes early. There is also the fact that, despite many phones using NTP to achieve reasonable precision, phones still drift several minutes. At Instacart, where we allow delivery to our customers in as little as an hour, every minute counts. We have to track with guaranteed, high precision the time a customer places an order, when a shopper starts an order, and when it’s ready for our delivery driver to pick it up. We need a common baseline standard time; one that’s impervious to any device customizations or changes. We need… TrueTime. We’re happy to open source today TrueTime for both Android and iOS . We make a network (SNTP) request to a pool of NTP servers that determine the actual time once, right at the very beginning of a session with the app. We then establish the delta between the device uptime and the response from the network request. Each time now is requested subsequently, we account for that offset and return a corrected Date object. We’ve tried to keep the API nice and simple: Head over to the usage section to get more of the juicy details: Android , iOS . You can include a vanilla version of TrueTime that gives you the basics like so: But we’ve also included a niftier RxJava extension to the base TrueTime library. If you’d like to use that, just use this import instead: Have a look at our instructions page for more details on the precise installation using Jitpack and gradle. Here are some cool things that the Rx extension allows: It can take in multiple SNTP hosts to shoot out the UDP request. Those UDP requests are executed in parallel (with a limit of 3 parallel calls). If one of the SNTP requests fail, we retry the failed request (alone) n number of times. As soon as we hear back from any of the hosts, we immediately take the first response and terminate all other requests. TrueTime can be included via Carthage like so: It is currently compatible with iOS 8 and up, macOS 10.9 and tvOS 9 (Linux support forthcoming). Some notes: Since NSDates are just Unix timestamps, it’s safe to hold onto values returned by ReferenceTime.now() or persist them to disk without having to adjust them later. Reachability events are automatically accounted for to pause/start requests. UDP requests are executed in parallel, with a default limit of 3 parallel calls. If one fails, we’ll retry up to 5 times by default. We here at Instacart ❤️ open source . If you think that you can improve TrueTime, we’d love to field your PRs on GitHub: Android , iOS . Oh! And if working on these kinds of solutions interests you, we should have a chat! We’re looking for mobile engineers who can help us solve problems like these. Shout out to our friends at Lyft who seemed to have had similar challenges and recently came up with a similar cool solution for iOS. Do check their solution out as well! Instacart Engineering 440 7 Mobile Open Source Tech AndroidDev 440 claps 440 7 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-04"},
{"website": "InstaCart", "title": "using stripe connect to directly pay retailers", "author": ["Dave Schwantes"], "link": "https://tech.instacart.com/using-stripe-connect-to-directly-pay-retailers-e331f645868f", "abstract": "Eng Open Source Data Science Machine Learning Android iOS We at Instacart really like it when customers can give us money in exchange for their favorite groceries and because mailing us envelopes of cash would make it hard to support deliveries in as fast as 1 hour we make heavy use of Stripe . One of the coolest Stripe features we utilize is Connect . Stripe Connect allows us to function like a marketplace in certain situations, accepting payments from customers and then directly transferring some or all of that payment to a bank account owned by one of the retailers we partner with. Because our payment model can be complex, particularly for marketplace type transactions, we’ve really pushed the limits of Stripe Connect and learned a lot about what it is capable of. In this post, I’ll walk you through how we make the most of this powerful tool. First, in order to use Stripe Connect you will need to register your platform and then connect the retailer Stripe accounts to that platform. I can’t give a better explanation for that than Stripe does in their own documentation , so just take a look at that. One important aspect of this step is storing the Stripe Connected Account id for the retailer when they connect to your platform. We do this by building a custom endpoint on our own server that takes in a retailer id and the authorization code that Stripe will generate. Then we set the Redirect URI in the Stripe Platform Settings in our Stripe Dashboard to a page so it becomes a part of the flow when retailers connect with us. Here is what such an endpoint might look like, built in your stripe_connect_controller.rb: Instacart Engineering 40 1 Engineering Rails Ruby Payments Software Development 40 claps 40 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "ohmycron locking logging and environment setup for cron", "author": ["Jason Dusek"], "link": "https://tech.instacart.com/ohmycron-locking-logging-and-environment-setup-for-cron-cf5026d360cc", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Crons are easy to write but run in a confusing way. Typically the shell is not your shell, the path is not your path, and the environment is not your environment. Cron runs your tasks and they fail and nothing is logged; instead it tries to send email and fails because you didn’t specify an email; and if you had it would still fail because you didn’t set up a mailer. It seems like a lot to worry about, just to run a command in the background. OhMyCron makes cron convenient again by adding an intuitive and conventional approach to logging and environment loading: STDOUT is logged at user.info in Syslog and STDERR is logged at user.notice. Crons are locked (you can set the lock name) with BSD locks, to prevent them from piling up if they run longer than expected. /usr/local/bin is added to the PATH. ohmycron runs jobs with Bash and loads the user’s profile before running the job. Today, we are delighted to open source OhMyCron . At Instacart, OhMyCron is used to wrap well over 75% of cron jobs, functioning as the cron shell: OhMyCron arose from the needs of our platform, data and asset processing teams. All of these teams needed to use Cron; and all of them needed to use Cron in a way that was compatible with the specifics of their application environment. The locking, in particular, is a reflection of unexpected changes in load. Jobs that previously ran in 30 seconds might take a minute and a half a few months later; these jobs can pile up and block one another. By locking each job, OhMyCron ensures that jobs like the one above, which runs for 70 seconds, don’t try to run in the same minute. OhMyCron automatically locks jobs with BSD locks (flock) which are managed by the kernel so there’s no danger of stale lock files preventing your jobs from running. Because BSD locks are tied to open file descriptors, they do not persist across reboots. OhMyCron tries to arrive at a good name for the lock, based on the command line it’s passed; but sometimes we have several tasks that use the same command: In these cases, we use a shell no-op statement — text between : and ; — to tell OhMyCron what the locking token should be: OhMyCron logs both STDOUT and STDERR to Syslog in their entirety, using the command name as the Syslog “program name” by default. So this example: logs with line like: When you set the lock token, OhMyCron treats that as the “program name”, instead. So for this example: you’d see lines like: Job locking, logging and environment management is a big space and we’d like to see OhMyCron expand into a useful role. Although nominally used with Cron, it doesn’t have to be; it can just as easily wrap commands from Chronos or shell-outs from Clockwork. Instacart Engineering 2 Open Source Tech Software Development 2 claps 2 Written by the lyf so short, the craft so long to lerne Instacart Engineering Written by the lyf so short, the craft so long to lerne Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "doing data science right at instacart", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/doing-data-science-right-at-instacart-90392230f68d", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Two weeks ago, I wrote a piece with Daniel Tunkelang (formerly of LinkedIn) summarizing the advice they give to founders who are interested in building data science teams. Doing Data Science Right — Your Most Common Questions Answered explains why data science is so important for many startups, when companies should begin investing in it, where to put data science in their organization and how to build a culture where data science thrives. If you are curious to learn more about how data science is set up at Instacart, read on! We will walk through each of the big questions in the article, and explain how data science at Instacart has handled the topic. In the article, we caution companies from investing significantly in data science unless they have clear answers to these four questions: 1. Are you committed to using data science to either inform strategic decisions or build data products? Data science plays a critical role in many teams at Instacart — helping customers quickly fill your cart, and optimally routing shoppers. Instacart wouldn’t work without data science, and commitment to data driven products and decisions is foundational here. 2. Will you be able to collect the data you need and act on it? As an entirely digital shopping and mobile-driven last mile delivery service, Instacart instruments data collection throughout the entire product, consumer and shopper experience. 3. Will you have enough signal in your data to derive meaningful insights? Unlike many e-commerce platforms, Instacart quickly builds rich profiles of user behavior. Grocery shopping is at least a weekly chore for most, and so we not only gain density about key outcomes (quality of service, delivery timeliness) in macro, but we also gain meaningful signal quickly about our consumers, shoppers and products. 4. Do you need data science to be a core competency, or can you outsource it? At the highest level, we are tackling challenges that are similar to many other companies. But because of the nature of our service (groceries from stores you love in as little as an hour) and the complex interactions of our marketplace participants (shoppers, customers, partners), most data science challenges we face are unique, and many have a tremendous impact on our performance. As such, we have built almost all of our data science products in-house. “Data science requires data to science, and most companies don’t have much data on day one.”* Our CEO, Apoorva Mehta, was previously a logistics engineer at Amazon, and he deeply understands the power of data and algorithms. Early on, the product was launched with limited data science support — humans would do everything behind the scenes. The first hire, Andrew Kane, was an engineer who built version 1.0 of many important data science algorithms, and still leads our Logistics Engineering team today. The team worked with a contractor to build some initial predictive models, and then hired the first full time data scientist, Deepak Tirumalasetty, about two years after the company was founded. Since then we have grown the team to 9 data scientists and are actively recruiting to double the team over the coming year. We have used data science to optimize decisions for the entire fulfillment process, and are now pushing into new frontiers to make online grocery shopping fundamentally better than shopping in stores. “Good management is a reconciliation of centralization and decentralization — a balancing act to get the best combination of responsiveness and leverage.” In the article, we outline three common approaches to organizing data science teams: Standalone : where data science is a centralized function operating in parallel to the engineering organization Embedded : where data scientists are dedicated to engineering product teams, but still report centrally Integrated : where data scientists are fully integrated into engineering product teams For decision science, we have an embedded organizational model. We’ve found that independence in analytics and the flexibility to move resources as needed merits the complexity of the embedded model. For data products, we have an integrated organization model. We’ve optimized for small mission-driven teams that can ship products quickly, and so have integrated the data scientists directly into product teams along with the engineers they collaborate with. In some cases, our data scientists lead these teams. “Build a company culture early that makes it a great place to practice data science, and you’ll reap dividends when they matter most.” At Instacart, data driven decision making is the norm. Our integrated organizational model ensures we work together, “as a village”, and we have clear values we look for when recruiting candidates. In addition to the critical technical skills which we evaluate with off and on-site challenges, we look for: Customer Focus : thinking about problems from a “first principles” basis for what is best for the end user (shopper, customer or partner) Take Ownership : going above and beyond improving algorithms and analyzing data to ensure we have a measureable impact Highest Standards : able to do amazing work and always seeking better ways — be they new algorithms, new processes or new implementations Humility : conscious of their own limitations and always open to the ideas of others — wherever those ideas may come from For even more behind the scenes, check out our prior post: Data Science at Instacart . Cover image courtesy of First Round Instacart Engineering 83 Data Science Engineering Tech 83 claps 83 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-30"},
{"website": "InstaCart", "title": "animating programmatically with redux and react", "author": ["Dominic Cocchiarella"], "link": "https://tech.instacart.com/animating-programmatically-with-redux-and-react-e304965ec828", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Our recent move to React has been really beneficial, but we still have a massive Backbone app holding the ship together. While Backbone has served us well in the past, its mutable models and reliance on direct DOM manipulation for view rendering does not mesh well with React’s ideals. It’s time for that ship to sail. Redux has gained a lot of attention and praise lately, and for good reason. It’s incredibly simple and fast. It also leads to more understandable and performant web applications. However, shifting a codebase of our size over to a new state/data manager is risky and time consuming. When we started using React, the transition went smoothly because we were able to continue using our older tools alongside React. We needed a way to do the same thing with Redux. We know that customers on Instacart who use the search tool to build up their cart are more likely to find what they’re looking for. And, generally get what they want faster. We also know that our retail partners want more opportunities to express their brand and values to Instacart users. With this in mind, we set out to create an entirely new experience for our header and navigation on the site. [video width=”640\" height=”192\" loop=”on” autoplay=”y” mp4=”https://tech.instacart.com/wp-content/uploads/2016/07/headerAnimation.mp4\"][/video] It’s rare for us to recreate an entire portion of the website, so it seemed like the perfect chance to start using Redux at the ground level. You’ll hopefully notice that the header does some very complex animations as the user scrolls. If these animations weren’t happening in constant reaction to user actions, we’d simply use CSS. If we weren’t using React for rendering, we’d just use jQuery like in the old days. But this is a whole new world, with a whole new set of problems. When you need to control an animation dynamically, like in reaction to a stream of user interactions, programmatic control is essential. This is nothing new. However, up until recently, this was tough to do well on the browser. Client-side rendering with JavaScript and HTML did not become performant enough for very smooth dynamic animations until nice JIT compilers like Google’s V8 and methods like React’s rendering system came into play. Using Redux and React, we’re able to simultaneously render many different style changes as quickly as your computer can render a frame. And because Redux state is immutable, you can rewind and replay your state with the Redux dev tools. [iframe src=”http://jsfiddle.net/dcocchia/xa4vtm3y/embedded/result,js,html,css\" width=”100%” height=”500\"] Above, I’m using the Redux Dev Tools custom monitor Redux Slide Monitor . This lets you slide back and forth through time and watch the effects of your state changes. Great for debugging animations where something weird happens for only a single frame. If you’re interested, check the dev console for a stream of state change logs. If you’re unfamiliar with the basics of Redux, Dan Abramov, the creator of Redux, recorded a fantastic set of tutorials . We’ll build a simpler version of the above animation so that only the background position changes. If you’d like to check out the full code from the more complex animation, it’s available here . (This is all done in ES6) Instacart Engineering 113 1 Engineering Js React Redux Software Development 113 claps 113 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "instacart hosts ucla alumni and the new dean of engineering", "author": ["Richard King"], "link": "https://tech.instacart.com/instacart-hosts-ucla-alumni-and-the-new-dean-of-engineering-b2278f62862", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Our focus at Instacart is to allow anyone to order groceries and have them delivered to their door in as little as an hour. We’re always hard at work on that goal, but sometimes it’s refreshing to step back, help out the tech community in general and meet new people. In that vein, we host a number of groups, from the more technical ones (Railsbridge, Ruby meetups), to diversity focused (Lesbians Who Tech, Girls In Tech, Women Who Code, Oakland Digital), to universities (domestic and international). Recently Instacart held a UCLA alumni networking event at our office. We hosted Dean Jayathi Murthy, who officially became the Dean of the UCLA engineering school at the beginning of this year. She is the 7th person to hold the position, and is the 1st ever female engineering Dean. It was great to build a relationship with her, the engineering school and the UCLA community living in the Bay Area. As a Bruin and engineer on the Instacart Catalog team, I hosted the event with Melissa Chu, a fellow Bruin and Instacart’s Recruiting Programs Manager. We put together a can’t-miss speaker line-up — first the Dean and then our CEO Apoorva. Among the topics that would be covered were UCLA’s vision for engineering, Instacart’s growth and evolution, and the need for a new generation of talented students. People began filing into our cafeteria half an hour before the Dean’s talk. I loved meeting the variety of alumni, from a venture capitalist and his daughter who was in the middle of choosing which college to attend, to a professor at the University of Texas that was currently on leave and living the Bay Area. It was a lot of fun mixing with the different folks, hearing about their backgrounds and reminiscing over our UCLA experiences. Dean Murthy kicked off her talk with some impressive statistics — this year UCLA Engineering is expecting 23,000 applicants for only around 700 freshman spots. An Engineering degree from UCLA is clearly in high demand and UCLA Engineering has distinguished itself as a top destination for talented students all over the world. She then went on to describe her vision of the future for UCLA Engineering. She wants to build out the Engineering school — adding 1000 more students and 50 faculty to the school — and improve the representation of female students. Also, she wants to help students after they graduate, and building relationships with companies like Instacart is a big part of that. In the Dean’s words, entrepreneurial thinking and being a self-starter are critical traits for today’s engineers and these traits reflect the school’s mission of producing creative and critical thinkers who are inspired to have a big impact. This fits well with Instacart’s values too. After the Dean finished, Apoorva and our VP of People Mat Caldwell came on stage to have a fireside chat. Mat began with a couple prepared questions and the audience soon started to chime in with thoughtful questions too. The Q&A session was originally scheduled for just 30 minutes, but both Apoorva and the audience were so engaged that it went on for over an hour! Even I learned something new: when Apoorva was building the first Instacart prototype, he holed himself up in his apartment for 3 months. Once it was ready, he tested it out by using the app to place an order, then drove to the store to fulfill the order and finally delivered the order, all by himself. In doing so, he was the first user, first in-store shopper and the very first customer of Instacart! Connecting with Alumni is something that can be lost by the wayside in all the business of everyday life. However, the school we went to played such a huge part in shaping our lives today that it’s important to both give back and build new connections. We loved hosting this UCLA alumni event, meeting the Dean and look forward to doing more events in the future! Instacart Engineering 1 Engineering UCLA Instacart Culture 1 clap 1 Written by Engineer @ Instacart Instacart Engineering Written by Engineer @ Instacart Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-25"},
{"website": "InstaCart", "title": "data science at instacart", "author": ["Jeremy Stanley"], "link": "https://tech.instacart.com/data-science-at-instacart-dabbd2d3f279", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we are revolutionizing the way people buy groceries. We give busy professionals, parents and elderly back valuable time they don’t have to spend shopping. We also give flexible work opportunities to thousands of personal shoppers, and we extend the reach and sales volume for our hundreds of retail partners. We work incredibly hard to make Instacart easy to use. Our site and app are intuitive — you fill your shopping cart, pick the hour you want delivery to occur in, and then the groceries are handed to you at your doorstep. But achieving this simplicity cost effectively at scale requires an enormous investment in engineering and data science. Fulfillment At its core, Instacart is a real-time logistics platform. We are in the business of moving goods from A (a store) to B (your front door) as efficiently and predictably as we can. At any given time, for every market we operate in, we can have thousands of customers expecting delivery in the coming hours. We also can have thousands of shoppers actively working or waiting to be dispatched through our mobile shopper application. Our fulfillment algorithm decides in real time how to route those shoppers to store locations to pick groceries and deliver them to customers’ door-steps in as little as one hour. We re-compute this optimization every minute, because the world is constantly changing. We have to balance speed (some shoppers shop faster, some stores are less busy) with efficiency (can we deliver multiple orders simultaneously) with quality (does the customer get the exact groceries they want) and timeliness (is the order delivered within the hour it is due — no earlier, no later). Optimizing multiple objectives while routing thousands of shoppers every minute to fulfill millions of orders is a tremendous data science challenge. Instacart operates a very dynamic and complex fulfillment marketplace. Our consumers place orders and our shoppers fulfill those ordersin as little as an hour. If supply exceeds demand in a market, we lose money and reduce shopper happiness due to shoppers sitting idle. If instead order demand exceeds available shoppers in a market, customers due to limited availability. Balancing orders and available shoppers requires sophisticated systems for forecasting behavior down to individual store locations by hour of day, many days into the future. We then create plans that blend multiple different labor role types to optimize our efficiency while ensuring high availability for our customers. This is made even more challenging by the many different queues we must manage across stores and division of labor. Then in real time, we have to estimate our capacity for orders every time a user visits our site or one of our apps, and then dynamically control availability to smooth demand and create the optimal customer experience. These systems operate over multiple time horizons, have to solve for multiple competing objectives, and control for many erratic sources of variation (shopper behavior, weather, special events, etc.). We will always have huge opportunities to make improvements here. Search & Personalization Instacart isn’t just grocery delivery, we’re creating a better grocery shopping experience. A majority of grocery shopping is about finding the food on your list. In a traditional grocery store, the search engine is the customer’s two feet. At Instacart, it’s a massive algorithm that can mine billions of searches to ensure every product a customer wants is at the edge of their fingertips. At a physical grocery store, you have to discover new products on your own. But at Instacart, we can curate the experience for you through personalization. What could be more personal than food? We have an intimate relationship with it every day — we put it in our bodies! As much as movie recommendations were critical to the success of Netflix, so too are product recommendations critical to Instacart. Our consumers order large basket sizes of diverse foods over and over and over again from us. We have more density on our user behavior than any e-commerce company I have ever seen. We are just beginning to use that data to provide incredibly valuable personalized experiences for our users on Instacart in search, in product discovery and in suggestions we make to our users. We A/B test everything, and are thinking really hard about the long term impacts of our changes. Through investments in search and personalization, Instacart has the opportunity to go beyond convenience in shopping online, and into a future where everyone finds more food they love faster. We have made the conscious decision to embed our data scientists into our product teams, side-by-side with their engineers, designers and product managers and reporting into the engineering leader for the team. So to answer this question, you first have to understand how engineering works at Instacart. At Instacart, we place a high value on ownership, speed and ultimately shipping products that have a huge measure-able impact. In engineering, we have organized to optimize for these values. We have many product teams, each of which have full-stack and/or mobile developers, designers, analysts, product managers and engineering leaders dedicated to them. Some teams are only 3 people, others are up to 10. Each team completely owns their ‘product’, and defines their key metrics and sets their roadmap weekly. We align all of these teams to a small (three or fewer) set of company wide goals that are updated whenever they are achieved or exceeded. So every product team answers the question every week “what can we do to have the biggest impact on our company’s goals this week?”. For technology companies, data science can either be an integral component to huge value creation, or an expensive and distracting hobby. Many factors determine the outcome, but how you organize your data scientists is one of the biggest contributing factors. By embedding our data scientists into product teams, we’ve ensured that they are as integral a part of their teams as they can be. As the VP of data science, it’s my job to make sure that the data scientists stay connected, have the mentorship they need, and are having the biggest impact they can within their teams. The data scientists have a tremendous amount of traction in this model. Their ideas can directly shape not only product innovation, but also data collection and infrastructure innovation to fuel future product ideas. They work directly with their team to bring their products the ‘last mile’ to production. This lets data scientists put new ideas into production in days (from inception), and to rapidly iterate on those ideas as they receive feedback from their consumers. This also gives data scientists a holistic view of their product, and helps to ensure they are optimizing for the right objectives as effectively as possible. Our organizational structure works because we have amazing talent. You can’t move as fast as we do, with as much distributed ownership as we have, all while solving challenges like ours without the right people. Our values form the corner-stone of our culture, and these in particular are key for hiring data scientists: Customer Focus “Everything we do is in service to our customers. We will work tirelessly to gain the trust of our customers, and to improve their lives. This is the first priority for everyone at Instacart.” We seek to understand the problems we work on as holistically as we can, and to reason through the physics of the system and how our many constituents (consumers, shoppers, our partners) will experience the changes we drive. We look for candidates that naturally think about problems from a “first principles” basis for what is best for the end user. Take Ownership “We will take full ownership of our projects. We take pride in our work and relentlessly execute to get things completely finished.” In data science, this means improving algorithms and analyzing data are never enough. We own the problem, the solution, the implementation and the measurement — along with everyone else on our team. Simply put, until the desired impact has been measured, our work isn’t done. We look for candidates who crave this opportunity for impact. Sense of Urgency “We work extremely fast to drive our projects to completion and we will not rest until they are done.” Many data science teams think about impact in quarters, months or weeks. Our teams regularly iterate on hard problems in a matter of days — from R&D to implementation and measurement. We look for candidates with a bias towards action, and the fortitude to pursue aggresive goals relentlessly. Highest Standards “We put our heart and soul into the projects to deliver the highest quality work product. We only produce work that we are proud of.” With ownership and a mandate for urgency comes a great responsibility — we must maintain the highest standards possible for the work we produce, as it has the potential to impact millions of consumers, thousands of shoppers and hundreds of retail partners. We look for exceptional candidates who can do amazing work, and are always seeking better ways — be they new algorithms, new processes or new implementations. Humility “We appreciate that great ideas can come from anywhere and we will be humble and open minded in considering the ideas of others.” Many of our best data science ideas have come from Instacart employees in the field — working directly with our shoppers in our stores, or interacting directly with our customers. Ensuring our eyes are wide open to these ideas, and that we collaborate openly within our teams and are always open to questioning our biases and assumptions is critically important. We look for candidates who are conscious of their limitations, and always open to the ideas of others — wherever those ideas may come from. We are looking for data scientists with expertise in forecasting, predictive modeling, ads optimization, search and recommendations. We are also looking for operations research scientists with expertise in planning, logistics and real time control systems. Our team uses Python, R, SQL (Postgres & Redshift) and Spark extensively, so mastery of some of those tools and technologies is also helpful. If you would like to join the team, apply here ! Instacart Engineering 158 1 Data Science Engineering Instacart Online Grocery Shopping 158 claps 158 1 Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Written by Founder and CTO at Anomalo; previously VP Data Science at Instacart. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-30"},
{"website": "InstaCart", "title": "why we moved to react", "author": ["Dominic Cocchiarella"], "link": "https://tech.instacart.com/why-we-moved-to-react-9df4a44a7070", "abstract": "Eng Open Source Data Science Machine Learning Android iOS Back in 2012, when Instacart was just getting started, the site was created as a large, single page app written with Backbone, jQuery, Underscore and Haml. This fit well with the underlying Rails stack and, given that most of our engineers criss-crossed between the backend and frontend lines, it made sense to have our frontend codebase style match our backend’s. Considering our speed of growth and output, this worked exceptionally well. Fast forward a couple years… our needs and outlook had changed dramatically. Our team had grown many times over, our customer base had become much larger and the number of retail partners we work with had gone through the roof. The complexities of the app now had to be managed by a larger team across a larger number of ever-increasing features and special cases. Our Backbone app was starting to get cumbersome, slow and hard to debug. And it was only going to get worse. We needed to make a change… but why React? As mentioned earlier, we had a huge Backbone app already running the majority of the site. To completely rewrite the site in one attempt would be incredibly difficult, bordering on insanity. Luckily, React doesn’t require a total buy-in to get started. In your traditional Model/View/Controller pattern, React can act as just the View layer. This allowed us to maintain our Backbone models while converting the Backbone Views into React Components. However, this did create a few complications. For one, React components using Backbone models for props or state need to know when those models update. Easy enough, just use Backbone’s built in event system to listen for changes. (We write in Coffeescript right now). Make sure your React component stops listening when it unmounts. If you don’t, you’ll run into memory leaks as your React code waits around for model changes to update a no longer mounted component. If you’re calling setState() in response to these triggered events, you’ll get a nice warning from React. The other big issue we ran into was the urge to use Backbone models as props and then call set() on the model in response to other activities. If you’re used to writing Backbone apps, this is a hard pattern to stop. Props in React should be treated as immutable. If you need to change a prop, move it up to the parent component as state, respond to changes there and then pass it down to your component as a prop. Facebook has a great post about determining where your state should live. State management in Backbone is easy enough when your app has only a few interacting views. Usually, you either create a model or just a regular object in memory and go from there. This pattern breaks down quickly without some kind of architecture to manage the state between an increasing number of Views/Controllers. And, when rendering is expensive (thanks DOM), you write more code trying to guess if you should render at all. Unfortunately, Backbone doesn’t have a great solution to this problem and most groups end up creating an in-house state manager that has knowledge of everything. In React, your components should be built only to care about their own state and props. State is moved up as high in the hierarchy as possible, so that fewer parts need to know how or why a state changes. The idea of props, which is mostly immutable data passed down to a component by its parent, really helps keep your components small and self-contained. This style of component removes much of the complexity around how state is managed and passed around your app. You can never escape complexity, but at least now you can control it in a standard and predictable way. Rendering in Backbone is generally done by pushing your data into some kind of template and appending it to the DOM with jQuery. This is fine for smaller apps, but when there are dozens of views interacting with each other and rendering at once, things can get nasty. Manipulating the DOM is expensive, and our Backbone app was doing it frequently. Adding items to cart, chatting, showing notifications, updating your in-route order and so many other things can all be happening at once on the site. A lot of folks first hear about React because of its rendering speed. It keeps an internal model of the DOM in memory, and uses it to compute which parts of the actual DOM need to be changed. This removes the need for checking if a component should render and makes that rendering even faster. React also queues up state changes and takes action on the DOM all at once, to further reduce expensive rendering. When we converted a Backbone view to React we noticed a much faster rendering speed with less code. If even this isn’t fast enough, you can use shouldComponentUpdate to determine if you want to stop React from running the calculation on its internal DOM. More on that from Facebook here. There are a number of small pieces of the site that are almost the same, but different for important reasons. For example, grocery items appear all over the site in all kinds of ways. The data model for these items is always the same, but the views using that data have many kinds of different functionality for favoriting, zooming, adding/removing to cart, quantities, pricing conditions and so much else. It’s hard to make a single ‘item’ view in Backbone without making tons of smaller sub views and huge lengths of state management code. It’s also expensive to have that many views manipulating the DOM. With React, you can create as many components as you need and conditionally include them in your component hierarchy as you need them. This is great for us, because we have a lot of shared functionality across the site, but in very small packages. Because our models are consistent, we can mix and match all these small components together to quickly create complex, but manageable components. Lastly, but possibly most importantly, creating things in React can give you a feeling of javascript zen. Most people recoil at the look of JSX code the first time they see it. Granted, learning to make things in React does require unlearning a great deal of past rules. However, once you’ve created a few complex projects, you’ll never go back. Rendering without worry, logic all in one place and an amazing community of smart developers to lean on really makes the act of writing React components seamless. While most of the Instacart site’s Backbone views are now converted to React, we still have a bit to do. We’ll most likely switch over our Backbone models to something more compatible with reactive style data flows. As an experiment, we tried writing a separate app to try out the popular Flux library. When you rate an order on the site, you get a special single page app that uses React and Flux. No jQuery, no Backbone. We have a ton of work to do on the site, but it’s nice to do that work in a clean and enjoyable way with React. If chatting about this kind of stuff interests you, check this page out and join us :) Instacart Engineering 20 1 React Web Engineering Js Software Development 20 claps 20 1 Written by Instacart Engineering Written by Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-10"},
{"website": "InstaCart", "title": "amountable a gem to manage your money fields", "author": ["Emmanuel Turlay"], "link": "https://tech.instacart.com/amountable-a-gem-to-manage-your-money-fields-a41fcb1f21f7", "abstract": "Eng Open Source Data Science Machine Learning Android iOS At Instacart, we need to keep track of a lot of “money” amounts. How much a specific item costs, how much sales tax has to be paid on an order, determining the correct amount for alcoholic goods in an order… these are just few examples of the myriad of amounts we need to store in our database. Recent versions of Rails have helped by integrating the money gem which provides a ruby object to handle money amounts, and the money-rails gem which makes it easy to save Money objects to a database as part of an ActiveRecord model instance. However, each of these money fields need two columns on your model’s database table. If you need to keep track of a lot of amounts it can quickly become quite cumbersome. It isn’t very flexible either as you need to run a migration each time you need a new amount. In addition to storing individual amounts we also need a way to centrally define which amounts to sum up for various purposes e.g. how much we should charge a customer. Introducing Amountable . Amountable lets you define amounts on any model and declare how they should be grouped and summed without having to migrate your database. Say we have an Order ActiveRecord model: Instacart Engineering 7 Open Source Rails Ruby Amountable Software Development 7 claps 7 Written by Rock’N’Roll nerd: Software engineer, cyclist, runner, guitarist, singer-songwriter, drummer, world contemplator, absurd, French. Instacart Engineering Written by Rock’N’Roll nerd: Software engineer, cyclist, runner, guitarist, singer-songwriter, drummer, world contemplator, absurd, French. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "just getting started", "author": ["Brandon Leonardo"], "link": "https://tech.instacart.com/just-getting-started-5ee3d886efeb", "abstract": "Eng Open Source Data Science Machine Learning Android iOS It’s hard to believe how much has changed at Instacart in 3 short years. We’ve been so busy working, we never took a moment to step back and share our journey with the world. This blog will serve as a place where we share the work we do in Engineering @ Instacart. A lot has changed in the last 3 years: Two dynos on Heroku are now an amazing, robust infrastructure built on AWS One tiny Postgres database migrated to several large, scalable, postgres instances Simple algorithms iterated into advanced, sophisticated data science based algorithms for routing, scheduling and personalization 3 personal shoppers have become over 10,000 working with us throughout the country 3 people in a little conference room have become an Engineering, Product and Design team of about 100 people in San Francisco In all that’s changed, we’re most proud of what’s remained the same: Our engineering values: Customers First, Ownership, Urgency & Quality Solving hard problems to make our customers’ lives better Why do you come to work each morning? Why do you put in the extra effort to do your best work? What drives decisions at Instacart? These are the things we value as a company and as an engineering team. Customers first The customer is our North Star. We believe that we must constantly earn our customer’s trust and improve their lives. Every piece of code, infrastructure, design or process is made in service to our customers. There are many more customers now than there used to be, but our commitment to making them happy hasn’t changed. This also means that we view every desire to play with new technology or build a new feature through this lens: will it make the customer’s life better? If so, it’s important. Ownership Our engineering teams are structured around customers, not functions. It’s not unusual for an engineer to commit code to the Instacart web app, api and mobile apps too. These small teams understand their mission and operate with full autonomy. We do everything in our power to drive decision making down to the individual engineer working on a project. Urgency We work fast. A sense of urgency is critical to making sure we move quickly to launch new features and resolve customer-impacting issues. We move quickly not just by working intensely but by ruthlessly prioritizing — we get more done by only doing projects with the highest leverage. Quality The old maxim goes: fast, cheap, good — pick two. We don’t believe you have to choose. We strive to build the best quality experience that we can for all our customers. We don’t wait until everything is pixel perfect but we constantly iterate until we’ve come up with something that we’re proud to show. How do you keep track of whether or not 10 million grocery items across the country are in stock? More specifically, how likely are ripe avocados to be in stock at Whole Foods at 6:30pm on a Thursday? How do you calculate the most efficient route for a Shopper delivering 4 orders during rush hour? We enjoy solving hard problems. The scope of the problems we’re solving is changing all the time. We’re doing things now that we never imagined being able to do just a year ago. As we continue to grow, our business scale magnifies these problems and the reward for making increases in efficiency are large. Here are 3 examples of our hardest problems: Logistics Customers place orders from different stores, due in the next hour or anytime in the next 7 days, in different parts of the city. We have different kinds of Shoppers: some who shop in-store, some who deliver the groceries directly, some trained to handle alcohol, some who do the fulfillment etc. It’s our job to find the best way to dispatch the deliveries to the right shoppers minimizing lateness and maximizing efficiency. We’re solving the Vehicle Routing Problem (an NP-hard problem) in real-time! There’s lots of uncertainty. How many shoppers will we have (considering hours are flexible)? When and where will orders be placed? How long will shopping take? How long will delivery take? Put it all together and you have a crazy challenging problem. Catalog Ninety percent of what you see on the screen when browsing Instacart is our catalog. It has to be flawless. We are building the most comprehensive catalog filled with accurate, detailed and structured data about every grocery product in the world. We take those products and match them to our partner retailers’ inventory to track the availability and even location in store of millions of items at thousands of retail locations every single day. How do you acquire all this information? It’s surprisingly challenging and requires working with many disparate sources of data of varying accuracy including our own homegrown data, old & new retailer IT systems, third party databases and directly from consumer brands. Much of the data we receive is inaccurate so we are building data science models to predict things like availability: whether an apple is in stock on a Thursday night at a specific store location. We do our best to make sure customers and shoppers find exactly what they’re looking for as quickly as possible. We’ve barely even scratched the surface of what we can do with search, personalization and other features to help customers discover new things to try. Open Source Sometimes we come across problems that we solve so well, the solution is worth open-sourcing. We believe in giving back to the community and we’ve launched several open source projects used by thousands of developers. Learn more at Instacart.com/opensource . We are solving hard problems to improve our customers’ lives. We can’t wait to share more about some of the challenges and solutions we’re finding along the way. Thank you for taking the time to read this. We hope you’ve learned a little about us and continue to learn more as time goes by. If you’re in San Francisco and interested in meeting some of the wizards behind the veil, please drop by for one of our happy hours or events *. Learn more: Our technical stack and how we built it Our amazing data science team Diversity at Instacart Customers first at the Sierra Company Retreat Open Jobs at Instacart — Brandon Leonardo, Max Mullen, Apoorva Mehta & the Instacart Engineering Team We’re hosting the next SFRails meetup on the 17th of November at our new offices. If you’re around we’d love to meet up with you there! Instacart Engineering 22 Tech Instacart Engineering Software Development Online Grocery Shopping 22 claps 22 Written by Co-founder @Instacart. Formerly of @AngelList, @TheStartupBus. Hack of all trades, master of awesome. Instacart Engineering Written by Co-founder @Instacart. Formerly of @AngelList, @TheStartupBus. Hack of all trades, master of awesome. Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-25"},
{"website": "InstaCart", "title": "introducing ahab docker event handling", "author": ["Jason Dusek"], "link": "https://tech.instacart.com/introducing-ahab-docker-event-handling-9ee0d30452df", "abstract": "Eng Open Source Data Science Machine Learning Android iOS The Docker event stream offers a powerful way to customize Docker networking. Today, Instacart is open sourcing Ahab, a Docker event handling library . At Instacart, we use Docker for test and staging environments in a configuration that has been live since late September of 2015. A long-standing challenge with Docker is how to allow developers to SSH into containers. By default, Docker provides an IP address to every container; but that IP is on a network internal to the Docker host. Docker does provide some alternative modes of networking, and while so-called host networking comes close to meeting our needs, it would allow but one container per host to make use of port 22 for SSH (and then what would the host use?). As a general rule we have held to convention over configuration at all layers of our stack. In terms of system deployment, this means we assign meaningful hostnames to all nodes and that we run services on their standard ports. This alleviates two important burdens for us — maintaining a mapping of services to non-standard ports and maintaining a mapping of IPs to names — and it allows us to leverage tools in their stock configurations instead of writing and maintaining wrappers. To deliver on this approach for Docker, we need to assign an IP to each Docker container as it comes up. We can easily attach spare IPs to the Docker host’s NIC; and using NAT, we can send traffic on a spare IP to a Docker container’s Docker-assigned IP. (Here we are sticking with Docker’s conventional mode of IP assignment.) But how are we to run the NAT rule at the right time? And when the Docker container disappears, how do we ensure the rule is removed? Fundamentally, systems of this kind invite two approaches: state synchronization and event handling. Synchronizing state is often the easiest way for complex cases, because it easy to verify that the “result” (in our case, a set of IPTables rules) corresponds to a given state (in our case, a list of Docker containers). However, state synchronization often runs afoul of latency due to polling, unless you are using events to synchronize an internal model of the state which brings us back to event handling. By approaching the problem as one of handling individual events, we can expect low latency — each state change is handled as soon at is registered — and often quite short and intuitive code. The trade-off, however, is that a missed event — for example, when the event handler is restarted — can result in nonsensical behavior. We’d like the event handler to start from a “base state” that makes sense and this can be hard to arrange. In our case, where this can be a problem is if the event handler stops, a container starts and then the event handler starts. The handler never sees the start event; and when it sees a stop even for the container, it will attempt to remove a NAT rule that isn’t there. A similar problem exists for containers that stop while the event handler is down (perhaps to be updated): the NAT rule will never be removed, causing a conflict when we later wish to use that particular spare IP for another container. In practice these problems can be resolved through idempotency; and because the event handler is very stable and is upgraded fairly infrequently, at Instacart we have accepted the possibility that we may have to reboot the Docker hosts from time to time to clear state. (Interestingly, it’s not something we’ve had to do.) The default listener class in Ahab accepts a list of functions as consumers. Here is an example script that, when run on a Docker host with Docker on the default port, prints each Docker event as it arrives: Instacart Engineering 3 Docker Open Source Tech Iptables Software Development 3 claps 3 Written by the lyf so short, the craft so long to lerne Instacart Engineering Written by the lyf so short, the craft so long to lerne Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-24"},
{"website": "InstaCart", "title": "how to think about subjects in rxjava part 1", "author": ["Kaushik Gopal"], "link": "https://tech.instacart.com/how-to-think-about-subjects-in-rxjava-part-1-ca509b981020", "abstract": "Eng Open Source Data Science Machine Learning Android iOS When I first started using RxJava and heard of Subjects I envisioned them as mystical trinkets. When used correctly they seemed to magically do the impossible. When used incorrectly they turned my code to a steaming pile of U+1F4A9. A friend warming up to RxJava echoed a similar sentiment: “Subjects are like a ( colorful adjective ) black box to me. I’m not really sure when I should ever be using them. I run into a corner with RxJava, snoop around the web, copy-paste some code which uses Subjects from StackOverflow, get my code working and hope to never look back at that code again”. We know Observables and Subscribers to be the workhorse constructs. Coupled with an operator ( or two ) you should be on your merry Rx way. But adding Subjects to the mix opens a whole new channel of communication between these constructs. You just have to start thinking about them differently. The goal of this post is to help you warm up to that line of thinking. Let’s start off by looking at the textbook definition : A Subject is a sort of bridge or proxy that acts both as a Subscriber and as an Observable. Because it is a Subscriber, it can subscribe to one or more Observables, and because it is an Observable, it can pass through the items it observes by reemitting them, and it can also emit new items. A Subscriber and an Observable? What demigoddery is this?! You have a producer (Observable) on the one end sending down a stream of events. On the other end, you have a consumer (Subscriber) swallowing up those events. When would you possibly need something that does both? Think pipe connectors Let’s take a specific example. The world of Android development has this common requirement: “Resuming or continuing the work done by a long-running operation like a network call after a screen rotation or configuration change” . I took a stab at the solution using retained fragments from the Android APIs, coupled with some RxJava . You have a UI based fragment (A) which acts as the master. When you want to start your network call you spawn a worker fragment (B) which makes the call. Meanwhile, if (A) is rotated, recreated, destroyed etc. it wouldn’t matter. After (A) finally connects or syncs back with (B), it would only receive subsequent events from (B) without restarting the whole process. Let’s model this example with some code. Say I have a long running Observable: In the real world, this Observable would be your network call or long-running operation. This is executed immediately in the onCreate method of the worker fragment (B) as shown below: Dispose your subscriptions responsibly! Uhh….why not just create the observable in onResume? This works because Subjects are by default “hot”: Which Subject to use? What about operators like .replay, .share or Connected observables ? Cover image courtesy of hisgett on Flickr (used unmodified) John Snow GoT gif courtesy of Giphy Joey gif courtesy of Giphy Instacart Engineering 274 3 Java Tech Mobile Instacart AndroidDev 274 claps 274 3 Written by http://t.co/ZC40a35T Instacart Engineering Written by http://t.co/ZC40a35T Instacart Engineering Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-04"}
]