[
{"website": "Erlang-Solutions", "title": "How to analyse a BEAM core dump?", "author": ["Rafal Studnicki"], "link": "https://www.erlang-solutions.com/blog/how-to-analyse-a-beam-core-dump/", "abstract": "TL;DR It’s relatively easy to debug Erlang system by connecting to the Erlang VM with gdb. To see an example, click here . Introduction Has this ever happened to you? You have your Erlang system running for a long time now and all of a sudden the BEAM process disappears on one of the nodes. In the OS logs you can see that the process was killed because of a segmentation fault, but besides that you have nothing. Obviously, you hadn’t been prepared for such a scenario and a core dump of the crashed process wasn’t generated. This has definitely happened to me (too many times), so now I don’t even start without enabling core dumps in the operating system, by setting ulimit -c unlimited . Now, after waiting (too long) for the segmentation fault to occur again, you ended up with a precious core.xxx file. But what’s next? A story of one core dump In this section I’m going to describe an example analysis of a core dump file, that was generated while load-testing MongooseIM . In fact that was the first core dump analysis I had ever done, and it was suprisingly successful (or it was just beginner’s luck). This is why no prior knowledge of gdb is necessary here. Still, you should understand how calling a function works (it’s using a stack), what pointers are and roughly how Erlang terms are built internally. Starting gdb Let’s assume that the generated core dump file was named core.1474 . gdb is started simply by pointing it at the BEAM executable that generated the core dump file and the core dump file itself. It’s also useful to pass the directory with the source of the VM (the same version), the C level output will then be more comprehensible: gdb erts-6.3/bin/beam.smp -core core.14747 -d /home/azureuser/otp_src_17.4/erts/emulator gdb should tell us what caused the crash and where in the code this has happened: Program terminated with signal 11, Segmentation fault.\n#0  do_minor (nobj=2, objv=0x7f209849e5c0, new_sz=233, p=0x7f20a36010d0) at beam/erl_gc.c:1095\n1095            val = *ptr; As we can see, the segmentation fault happened in a piece of code that belongs to the garbage collector. gdb told us exactly in which function this happened, with what argument it was called and what line is reponsible. We can see clearly that the function tried to dereference an invalid pointer ( ptr ). Where does the pointer point? Considering we used only one command, we already know a lot. Now we just need to answer the question ‘what was the pointer and why was it corrupted’? Let’s start with the most basic gdb command: backtrace -which prints the current stacktrace. A more chatty version backtrace full also exists and prints values of local variables for each function. Here’s the output: (gdb) backtrace\n#0  do_minor (nobj=2, objv=0x7f209849e5c0, new_sz=233, p=0x7f20a36010d0) at beam/erl_gc.c:1095\n#1  minor_collection (recl=<synthetic pointer>, nobj=2, objv=0x7f209849e5c0, need=7, p=0x7f20a36010d0) at beam/erl_gc.c:876\n#2  erts_garbage_collect (p=0x7f20a36010d0, need=7, objv=0x7f209849e5c0, nobj=2) at beam/erl_gc.c:450\n#3  0x00000000005669bb in process_main () at x86_64-unknown-linux-gnu/opt/smp/beam_hot.h:47\n#4  0x000000000049c4b2 in sched_thread_func (vesdp=0x7f209b560a80) at beam/erl_process.c:7743\n#5  0x00000000005d5fc5 in thr_wrapper (vtwd=0x7fffa342fff0) at pthread/ethread.c:106\n#6  0x00007f20a6ee1df3 in start_thread (arg=0x7f20863b0700) at pthread_create.c:308\n#7  0x00007f20a6a071ad in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113 Unfortunately, this is not very helpful. On a positive note, we actually confirmed that the garbage collection has been triggered from inside the Erlang processes and then things went bad. BEAM gdb macros What if we could print a similar thing, but not from the BEAM, C code standpoint, but for the Erlang process that caused the crash? It turns out that it is possible, thanks to the set of macros that are available in the Erlang VM source code. They allow to dump stack, heap, message queue, inspect Erlang terms in memory and many more. Inside the gdb session you load them like this: (gdb) source /home/azureuser/otp_src_17.4/erts/etc/unix/etp-commands.in\n...\n--------------- System Information ---------------\nOTP release: 17\nERTS version: 6.3\nCompile date: Thu Jan 14 11:21:36 2016\nArch: x86_64-unknown-linux-gnu\nEndianess: Little\nWord size: 64-bit\nHalfword: no\nHiPE support: no\nSMP support: yes\nThread support: yes\nKernel poll: Supported but not used\nDebug compiled: no\nLock checking: no\nLock counting: no\nNode name: mongooseim@localhost\nNumber of schedulers: 4\nNumber of async-threads: 10 When gdb prints some basic information about the Erlang VM and the architecture, it is a very good sign and from now on you can use all the macros from the file to analyze the core dump. Let’s first try to find out what address was dereferenced, and by doing that learn another useful feature of gdb – you can access all local and global variables just like that. Remember what variable did the VM try to dereference and failed? Let’s try to print its value: (gdb) print ptr\n$3 = <optimized out> What a pity, it seems that we cannot access this particular variable since it was optimized out by the compiler. But let’s take a look at what’s near by line 1095 here , maybe it is possible to workaround this issue and infer value of the variable manually. In the C file we can see something like this: case TAG_PRIMARY_BOXED: {\n        ptr = boxed_val(gval);\n        val = *ptr; So, how about trying this: (gdb) print gval\n$4 = 139760485281282 Much better. But what’s gval ? Let’s present its value to a hexadecimal number, which is 0x7F1C86142E02 . If you take a look at the boxed_val macro , you will see that it simply subtracts 2 from its argument. In this case the output (and therefore our manually deduced value of the ptr ) is 0x7F1C86142E00 . Here, the two least significant bits of the pointer (value: 2) exist as a tag for a boxed Erlang term, it might have been a tuple, a bignum or something else (at this moment we don’t need to know this), but definitely not a list. Erlang process’ stacktrace Let’s now try to find out more about what this ptr value means. Why don’t we start by fetching some information about the guilty process. ( p is a pointer to the Process struct, and it’s a local variable in the do_minor function, remember we can use them): (gdb) etp-process-info p\n  Pid: <0.2845.0>\n  State: trapping-exit | garbage-collecting | running | active | prq-prio-normal | usr-prio-normal | act-prio-normal\n  CP: #Cp<io_lib_format:collect/2+0x90>\n  I: #Cp<erlang:port_info/1+0x78>\n  Heap size: 233\n  Old-heap size: 376\n  Mbuf size: 0\n  Msgq len: 1 (inner=1, outer=0)\n  Parent: <0.2759.0>\n  Pointer: (Process *) 0x7f20a36010d0 This looks almost like a call to the erlang:process_info/1 BIF. We can also see the currently executed instruction and the continuation pointer. Let’s extend this knowledge by printing the full callstack: (gdb) etp-stacktrace p\n% Stacktrace (27): #Cp<io_lib_format:collect/2+0x90>.\n#Cp<io_lib_format:collect/2+0x150>.\n#Cp<io_lib_format:fwrite/2+0x118>.\n#Cp<io_lib:format/2+0x80>.\n#Cp<wombat_plugin_code_tracer:handle_info/2+0x318>.\n#Cp<wombat_plugin:handle_info/2+0xc78>.\n#Cp<gen_server:try_dispatch/4+0xa0>.\n#Cp<gen_server:handle_msg/5+0x6b8>.\n#Cp<proc_lib:init_p_do_apply/3+0x58>.\n#Cp<terminate process normally>. From this stacktrace we can tell that the guilty process was a gen_server process, that have callbacks implemented in the wombat_plugin module. Since it’s a generic module for all Wombat plugins, this particular module was implemented in the wombat_plugin_code_tracer module. Have I forgotten to mention that the MongooseIM in question was monitored by WombatOAM ? Let’s also print the full stack (whole stackframes, not just functions): (gdb) etp-stackdump p\n% Stackdump (27): #Cp<io_lib_format:collect/2+0x90>.\n#Cp<io_lib_format:collect/2+0x150>.\n[].\n#Cp<io_lib_format:fwrite/2+0x118>.\n32.\n#Cp<io_lib:format/2+0x80>.\n[].\n#Cp<wombat_plugin_code_tracer:handle_info/2+0x318>.\n#1:[[{timeout,115},{port_op,proc_sig}],[Cannot access memory at address 0x7f1c86142e00 Oops, it looks like a corrupted term is present on the stack and moreover it matches the address of the ptr variable! To have a full picture let’s use a macro that doesn’t try to pretty-print the terms to see the full process’ stack: (gdb) etpf-stackdump p\n% Stackdump (27): #Cp<io_lib_format:collect/2+0x90>.\n#Cp<io_lib_format:collect/2+0x150>.\n[].\n#Cp<io_lib_format:fwrite/2+0x118>.\n32.\n#Cp<io_lib:format/2+0x80>.\n[].\n#Cp<wombat_plugin_code_tracer:handle_info/2+0x318>.\n<etpf-cons 0x7cfa8121>. <-- *this is corrupted*\n<etpf-cons 0x6cd4ef89>.\n#Catch<364>.\n#Cp<wombat_plugin:handle_info/2+0xc78>.\n<etpf-cons 0x7cfa8131>.\n[].\n<etpf-boxed 0x7d1770ca>.\n#Cp<gen_server:try_dispatch/4+0xa0>.\n<etpf-boxed 0x7d1770fa>.\n#Catch<1386>.\n#Cp<gen_server:handle_msg/5+0x6b8>.\n#Catch<174>.\n#Cp<proc_lib:init_p_do_apply/3+0x58>.\nwombat_plugin.\n<etpf-boxed 0x7d1770fa>.\n<0.2845.0>.\n<0.2759.0>.\n<etpf-boxed 0x7d17714a>.\n#Cp<terminate process normally>.\n#Catch<126>. Opcodes, registers and stack The Erlang BEAM file (it’s the compiler’s output from the .erl file, more on how to generate a human-readable form of it in a moment) consists of functions (exported, local and anonymous functions defined in the module) and their code in the Erlang assembly. In simple terms, the Erlang assembler code consists of low-level operations (you can see what instructions are understood by the Erlang VM here ). Besides instructions and labels, the assembler interpreter also uses registers for the proper control of an Erlang process flow. They are used to call functions (arguments are stored in registers 0, 1, … and the function call result is always stored in the register 0). Code references to registers as {x,0} to {x,255} . Since some terms need to survive function calls, stack is used to memorize them. Code references to the stack elements as {y,0} , {y,1} and so on, while the {y,0} element is the first element from the top accessible to the code on the stack. The interpreter also needs to keep track of an Instruction Pointer (IP) that is the currently executed instruction in the module and a Continuation Pointer (CP) that points to the code that will be executed after returning from the currently executed function. When an Erlang function realizes (actually, it was the compiler) that it will call another function (in a non tail-recursive way) it allocates some space on the stack (by executing the allocate instruction) in order to migrate terms that need to survive this call. It also has to save the current value of CP on top of the stack (that’s why not the whole stack is accessible by the assembly code), as the CP value will be replaced by an address of the current function (next instruction after an instruction from the call family). Let me explain this on an example execution of a simple module: max(A,B,C) ->\n    max(max(A,B), C).\n\nmax(A, B) when A > B -> A;\nmax(_A, B) -> B. The (simplified) assembly code and the execution of a max(7, 5, 9) call is explained on the following slides. State of all registers and pointers is shown after the particular instruction is executed.https://www.slideshare.net/slideshow/embed_code/key/jqbRvvyI8F8Kb8 Erlang assembly from rstudnicki So the dumped stack (just the relevant part) in a more convenient form (stackframes have been separated) looks like this: +--------------------------------------------------------------------------------------------+\n|Stack element                                       |Ref|Function that the frame belongs to |\n+--------------------------------------------------------------------------------------------+  top\n|#Cp<io_lib_format:collect/2+0x150>.                 |CP |#Cp<io_lib_format:collect/2+0x90>. |   .\n|[].                                                 |y0 |                                   |  / \\\n+--------------------------------------------------------------------------------------------+   |\n|#Cp<io_lib_format:fwrite/2+0x118>.                  |CP |#Cp<io_lib_format:collect/2+0x150>.|   |\n|32.                                                 |y0 |                                   |   |\n+--------------------------------------------------------------------------------------------+   |\n|#Cp<io_lib:format/2+0x80>.                          |CP |#Cp<io_lib_format:fwrite/2+0x118>. |   |\n|[].                                                 |y0 |                                   |   |\n+--------------------------------------------------------------------------------------------+   |\n|#Cp<wombat_plugin_code_tracer:handle_info/2+0x318>. |CP |#Cp<io_lib:format/2+0x80>.         |   |\n|<etpf-cons 0x7cfa8121> <- *this is corrupted*       |y0 |                                   |   |\n|<etpf-cons 0x6cd4ef89>.                             |y1 |                                   |   |\n|#Catch<364>.                                        |y2 |                                   |   |\n+--------------------------------------------------------------------------------------------+ bottom In order to find out what’s the corrupted term we need to dive into the assembly code of the io_lib:format/2 . You can generate a human-friendly Erlang assembly with the following command: erlc -S lib/stdlib/src/io_lib.erl in the OTP source directory. And here’s how the assembly of the module looks like. The snippet ends at the call to the io_lib_format:fwrite function, that we know for sure we didn’t return from. 1 {function, format, 2, 8}.\n 2  {label,7}.\n 3    {line,[{location,\"io_lib.erl\",151}]}.\n 4    {func_info,{atom,io_lib},{atom,format},2}.\n 5  {label,8}.\n 6    {allocate,3,2}.\n 7    {'catch',{y,2},{f,9}}.\n 8    {move,{x,1},{y,0}}.\n 9    {move,{x,0},{y,1}}.\n10    {line,[{location,\"io_lib.erl\",152}]}.\n11    {call_ext,2,{extfunc,io_lib_format,fwrite,2}}. We can tell that the {y,0} stack slot is occupied by what originally was in the {x,1} cell (line 8 of the snippet, the second argument of the function is moved to the top of the stack). Since we are inside the io_lib:format/2 function, this in fact is the data for a format string. The function was called by wombat_plugin_code_tracer:handle_info/2 , like this (this is taken from Wombat’s code): io_lib:format(\" ~p subject info ~p\", [Info, subject_info(Subject)])]; Now we just don’t know yet what message originated the call, since in the Wombat code there are many clauses that match on different messages. But let’s recall what the corrupted term looked like: [[{timeout,115},{port_op,proc_sig}],[Cannot access memory at address 0x7f1c86142e00 It turns out that such term can be generated only by a port long schedule notification (from documentation ), thus the corrupted term is a result of a call to subject_info/1 . In case of a port it’s a result of a call to erlang:port_info/1 , again from the Wombat code: subject_info(Subject) when is_port(Subject) ->\n    erlang:port_info(Subject); Well done, we have found out exactly what call corrupts the memory inside the Erlang VM. Solution If something is wrong with the erlang:port_info/1 function, let’s move to its source code. But first let’s use Google to check if by pure luck, someone else has already noticed any issue with this function. Luckily enough, a race condition in this function has recently been fixed . After migrating to Erlang/OTP 17.5 the segmentation fault didn’t happen again. You have to admit this was a heck of a beginner’s luck. What if…? If you are having trouble with reproducing the segmentation fault but still want to debug your system on the gdb level, you can generate a core dump from a process that is currently running using the gcore command. Alternatively, you can attach gdb to the currently running process and play with it. Keep in mind that this freezes BEAM for the duration of the whole gdb session, things such as timers will be affected after resuming the process. On the other hand, if you want to learn more about the parts of the Erlang VM that are not covered by the built-in macros you can still create your own! You will be required to dig into the BEAM’s C codebase in order to learn what C structure fields you’re going to access and what variables you need to decode. You can see an example of both (attaching to the running BEAM and fetching information using a custom macro) here . In some other project I needed to verify somehow that timers inside the Erlang VM were synchronizing. Because I couldn’t find any existing functions to help me with this I decided to write a gdb macro. Summary gdb and a set of macros that are shipped with Erlang VM can be really useful for both post-mortem analysis and debugging a running system. Although some knowledge of BEAM internals is required for this task, you can learn a lot just from experimenting with the currently existing macros and then implementing your own. Acknowledgments My (many) thanks for help during my first core dump analysis go to Lukas Larsson and Simon Zelazny . Go back to the blog", "date": "2016-01-28"},
{"website": "Erlang-Solutions", "title": "Erlang Solutions partners with Cockroach Labs", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/erlang-solutions-partners-with-cockroach-labs/", "abstract": "Bulletproof systems, scalable distributed SQL databases. The new partnership between Erlang Solutions and Cockroach Labs allows organizations to build fault-tolerant and scalable technology on an open source distributed SQL database. Erlang Solutions is the world’s leading provider of Erlang and Elixir solutions and Cockroach Labs is the creator of CockroachDB, the most highly evolved cloud-native, distributed SQL database on the planet. Working together gives our clients access to end-to-end scalable solutions, built for resilience, that are able to handle extreme spikes in concurrent users, without any weak links in the tech stack. Erlang Solutions is dedicated to making fault-tolerance and scalability commonplace, we work with the world’s most ambitious companies to ensure their systems are elegantly and reliably able to handle the demands of their users.  CockroachDB guarantees built-in data locality for lower latency and faster performance, and a data infrastructure that is always on and always consistent. Using CockroachDB in our systems will allow us to easily ensure that databasing components meets the demands and standards of the solutions we build. “ The Cockroach Labs team is excited to formally announce its partnership with Erlang Solutions. Both organizations align strongly, not only within the verticals we service, but in our commitment to excellence and delivering world class solutions to our customers. We look forward to growing a strong global customer base with Erlang in 2021 and beyond.” – Jen Murphy, Global Head of Channels & Alliances CockroachDB is the latest technology partner to join forces with Erlang Solutions enabling our expert consultants to support, offer and work with the widest range of up-to-date solutions and ensure we are able to deliver our partners the best tool for the job every time. “We are delighted to be working with CockroachDB to provide industry-leading distributed SQL databases to our customers. Our customers come to us because of our deep expertise in building highly scalable, distributed systems and the addition of CockroachDB to our portfolio further enhances that capability.” – Francesco Cesarini, CEO of Erlang Solutions Webinar – Scalable, bulletproof distributed SQL databases made easy with CockroachDB About Cockroach Labs Cockroach Labs is the company behind CockroachDB, the cloud-native, distributed SQL database that provides next-level consistency, ultra-resilience, data locality, and massive scale to modern cloud applications. Companies like Comcast, Lush, and Bose are building their cloud data architectures on CockroachDB. Cockroach Labs was founded by a team of engineers dedicated to building cutting edge systems infrastructure, and has investments from Benchmark, G/V, Index Ventures, and Redpoint. For more information, visit https://www.cockroachlabs.com .", "date": "2021-04-15"},
{"website": "Erlang-Solutions", "title": "FinTech Highlights – Best Of The BEAM", "author": ["Michael Jaiyeola"], "link": "https://www.erlang-solutions.com/blog/fintech-highlights-2019-best-of-the-beam/", "abstract": "We’ve been delighted to begin relationships with several global financial services leaders on some mission-critical projects that are powered by Erlang, Elixir and BEAM VM technology. Although the specifics of the work are under strict NDAs, we have outlined some of the projects in this article along with those ubiquitous end-of year musings on the broader FinTech ecosystem. You’re also invited to visit our industry hub page for more info on our experience in building scalable, reliable systems for the FinTech industry. Recent FinTech projects Universal multichannel bank-insurer This firm focuses on private clients and SMEs in Northern Europe and South-East Asia. We supported the building of a mobile app which uses the highly scalable message server powered by XMPP, MongooseIM , as one of its core components. Major African based financial services group The Erlang-based message broker software using AMQP and MQTT, RabbitMQ , is used here for various business applications to implement critical banking operations around offers of personal and business banking, credit cards, corporate and investment banking and wealth and investment management. We reviewed the existing RabbitMQ architecture and suggested performance enhancements to deliver high availability on a Kubernetes platform in a new data centre. US financial services organisation This financial service firm combines traditional banking services with the offerings of a technology company to provide a comprehensive suite of products in a banking-as-a-platform solution, encompassing lending, payments and risk management. We came onboard to review the existing RabbitMQ architecture and code and to advise on a scaleup strategy to take the bank to the next level. Our Work In The FinTech & Blockchain Space What else has been happening in the FinTech space? Global FinTech investment trends remain strong following on from a record year in 2018 (fuelled by Ant Financial’s massive $14bn deal in China). According to Innovate Finance’s report ‘A Fine Year for Fintech’, investment levels in the first eight months of 2019 were strong – with just under $17bn raised from more than 1,000 deals. According to the EY Global FinTech Index 2019 – there is a 71% UK adoption of FinTech while it’s 87% in both China and India and 46% in the US. The UK Government’s 2019 ‘UK FinTech – State of the Nation’ report, found of the 89,000 finance and insurance firms in the UK, there are over 1,600 UK FinTech companies, a figure set to double by 2030. World FinTech Forum In November we attended the World Fintech Forum in London. Global operators form throughout the FinTech ecosystem came together to debate important and, at times, controversial topics. You can read our blog post on the 10 Key Takeaways from the conference , we’d be happy to hear your thoughts on any of the topics covered. Barclays Rise appearance – building scalable systems Rise is a global community of the world’s top innovators working together to create the future of financial services. We visited them earlier this year to discuss how startups could future proof their applications via building distributed, massively scalable and fault-tolerant systems with high availability. Dominic Perini, our Scalability Architect and Technical Lead, was presenting and you can view his presentation slides for How to Build Systems that Scale on our LinkedIn channel . Working with FinTech Alliance This new online portal for the global FinTech community connects the UK to a worldwide ecosystem in partnership with the UK Government. Fintech Alliance provides a multifaceted digital engagement platform to bring the global FinTech ecosystem together to explore, engage and do business. We’re delighted to be a member of the community from the outset of the project and look forward to working together on knowledge sharing and growth of the network in 2020. 5 Top FinTech Trend Predictions for 2020 1. You need more than just great UX Until recently, traditional financial institutions delivered similar products and services through the same channels at the same cost. Customers were forced to use slow, clunky web apps that made performing simple, everyday functions a nightmare. This was the setting where nimble FinTech startups enjoyed a serious competitive advantage over incumbents by offering slick, mobile-centric user experiences. Great UX is now standard in every new FinTech innovation and product launch. In order to onboard new users in 2020, FinTech companies need to move towards more inventive ways to win business and retain existing customers. Hyper-personalisation, through successful use of the vast amounts of data being collected, will be one of the key competitive differentiators in the industry. 2. Blockchain or DLT use to increase Gartner, the global technology research firm, estimates that the blockchain market will be worth roughly $23 billion by 2023. Blockchain technology holds the power to improve trust, provide transparency and potentially lower costs, reduce transaction times and improve cash flow. Moving forward it won’t just be the giants like JP Morgan, Citigroup and Wells Fargo who are investing serious resources into blockchain. Of course, caution does remain around blockchain initiatives, due to the issues of regulation, scalability and interoperability. Global banker, Arzu Toren, was a guest contributor to our blog considering the big questions in blockchain . BEAM VM, which Erlang runs on, is ideal for blockchain development – our friends at ArcBlock joined us for a webinar on using the BEAM for blockchain and also wrote about the key advantage of using Erlang for blockchain development projects . Ultimately, true advancements will emerge from the intersections of different disciplines. Interfaces of blockchain technologies with other technologies (AI, ML, IoT) need to be explored as the field moves to actual design and engineering. 3. More AI in financial services While Gartner estimates the blockchain market will be worth roughly $23 billion by 2023, the estimated business value created by AI is a whopping $3.9 trillion in 2022. The key to using AI in FS will focus on the ability to create personalisation at scale and in real-time. With the use of data and concerns over security and privacy of paramount importance, when FS companies use AI in the decision-making process they need to factor a human into the loop. This is partly out of regulatory necessity but also because there are inherent fears that, if AI is left to operate autonomously, then something might go wrong. We’ve all seen the Terminator films, right? Well, while I can’t necessarily imagine my Robinhood robo advisor going rogue and chasing me across the US, I do sleep better at night knowing that ultimately there is some human accountability in place when it comes to my money. 4. 5G and IoT set to impact The 5th generation of mobile connectivity promises superfast download and upload speeds as well as more stable connections. 5G mobile data networks started to be deployed in 2019 and availability will expand in 2020. The greatest initial impact will be that all components in the Internet of Things (IoT) universe will be able to connect with each other seamlessly, collecting and transferring more data than ever. Financial institutions will need to understand the business implications of having super-fast and stable internet access everywhere. From a customer experience perspective, virtual assistants will be able to provide more contextual recommendations in real-time. 5. Execution over tech exotica As with startups in any industry, what differentiates those that succeed from the rest is the ability to execute – deploying new apps and digital products rapidly, getting to market in the shortest time possible and being scale ready from the start. This agile mentality is not generally associated with traditional banks and financial institutions but is one that is being increasingly adopted by those incumbents proactively looking to maintain their competitive position. In many instances, leading FinTech companies are not working with new technologies, they are using established battle-tested open-source tech whenever possible to be able to concentrate their resources on areas of their business which will determine their success. It’s this strategic mindset which looks set to offer the best opportunity for longevity and survival in the FinTech space. Summary We are currently experiencing a rate of technological change in banking like never before. Disruptive enabling technology’s use will continue to usher in the next great leap in the growth of FinTech in 2020. New FinTech related investments and innovations for the consumer and enterprise markets will continue to prosper with prototypes and POCs evolving into full scale, consumer facing apps and digital products. Erlang Solutions works with partners, including Vocalink/Mastercard, Danske Bank, Safaricom, Bloomberg and OTP Bank, to prototype, design, build, monitor and maintain hyper-reliable, scalable and concurrent solutions for blockchain, smart contracts, TPS and switching solutions. FinTech Trends in 2021 Report", "date": "2019-12-19"},
{"website": "Erlang-Solutions", "title": "An introduction to RabbitMQ – What is RabbitMQ?", "author": ["Gabor Olah"], "link": "https://www.erlang-solutions.com/blog/an-introduction-to-rabbitmq-what-is-rabbitmq/", "abstract": "Why Rabbit? What is MQ? How can it improve our applications? Why would I want to learn more about it? — These are questions that I asked when I was first introduced to RabbitMQ. I’m Gabor, and I’m now a RabbitMQ engineer and consultant. In my time working with RabbitMQ, I’ve learned that even experienced customers ask these questions. TL;DR > Check out our RabbitMQ product page for use cases, features and to contact our expert consultants. Learn more at RabbitMQ Summit 2021 What problem does RabbitMQ solve? Before we delve into what RabbitMQ is and how to use it, it is worth learning more about the problem domain itself. Communication between different services (a.k.a computers) is an age-old problem. On one hand, there are the different protocols defining the means of transportation and the properties of the communication. Some examples of such protocols include SMTP, FTP, HTTP or WebSockets (to name a few), which are all based on TCP/UDP. They deal with the formatting, reliability and finding the correct recipient of a message. On the other hand, we can explore the communication from the perspective of the message. It exists in one system, then it is transported to another, gets transformed, that is, it has a lifecycle. As it travels from one system to another, we should be aware of where the message is, and who owns it at any given point of time. The communication protocols mentioned above can make sure that the ownership (and the “physical” location) of the message is transferred from one system to the other (although it may take some time to execute this transaction). We can consider the transfer to be a transaction between the two parties while both are present. Most of the time, this active exchange is desirable, e.g. asking for the status of the service and expecting a timely and accurate answer. An example from the physical world would be calling somebody over the phone: 1) we start the call, 2) wait for the other party to answer, 3) have a nice discussion, 4) hang up the phone. But there are other times when we don’t need the answer, we just need the receiver to take ownership of the message and do its job. In this case, we need an intermediary agent, another system to take ownership of the message (temporarily) and make sure that the message reaches its destination. To push the phone example further, the other party is not available at the moment, so we leave a voice message. The voicemail service will notify the intended receiver. This asynchronous (delayed) message delivery is what RabbitMQ provides. Obviously, it can do more than a simple answering machine, so let’s explore some of the options it provides below (If you are interested in learning more about the history of RabbitMQ, I recommend the first chapter of “RabbitMQ in Action” by Alvaro Videla and Jason Williams. It will reveal the answer to why it is named after Rabbits). What is RabbitMQ? RabbitMQ is a free, open-source and extensible message queuing solution. It is a message broker that understands AMQP (Advanced Message Queuing Protocol), but is also able to be used with other popular messaging solutions like MQTT. It is highly available, fault tolerant and scalable. It is implemented in Erlang OTP , a technology tailored for building stabe, reliable, fault tolerant and highly scalable systems which possess native capabilities of handling very large numbers of concurrent operations, such as is the case with RabbitMQ and other systems like WhatsApp, MongooseIM, to mention a few. At a very high level, it is a middleware layer that enables different services in your application to communicate with each other without worrying about message loss while providing different quality of service (QoS) requirements. It also enables fine-grained and efficient message routing enabling extensive decoupling of applications. Use cases To show off the versatility of RabbitMQ, we are going to use three case studies that demonstrate how RabbitMQ is well suited as a black-box managed service approach, as one that integrates tightly with the application enabling a well-functioning micro-service architecture, or as a gateway to other legacy projects. RabbitMQ as a general message-bus When a monolith system is broken down to separate subsystems, one of the biggest problems that needs solving is which communication technology to use. A solution like Mulesoft , or MassTransit can “wire” services by declaring HTTP listeners and senders. This kind of solution treats RabbitMQ as a black box, but is still able to leverage the capabilities of RabbitMQ. As an example of direct communication, let’s use HTTP to “connect” the individual services. While it is a well-supported and solid choice, it has some drawbacks: 1) The discovery of services is not solved. A possible solution is to use DNS. As the system scales and grows, so too does the complexity of finding and balancing this load. RabbitMQ can mitigate the increased complexity of the solution. 2) The communication is ephemeral. Messages are prone to being dropped or duplicated on the network layer. If a service is unavailable temporarily, the delivery fails. RabbitMQ can help in both cases by utilising message queues as a means of transport. Services can publish and consume messages, which decouples the end-to-end message delivery from the availability of the destination service. If a consuming service is temporarily unavailable, unlike HTTP, the message is safely buffered and retained in RabbitMQ, and eventually delivered when the service comes back online. Discoverability is simplified too. All we need to know is where RabbitMQ is and what the queue name is. Although it seems like this just reinvents the problem, this is scalable. The queue name acts as the address of the service. Consuming messages from the queues by the individual services offer a means for scalability, i.e. each queue can serve multiple consumers and balance the load. There is no need to change the queue configuration already built into the services. This moderately static queue configuration pushes RabbitMQ to a middleware layer where a solid design can guarantee a stable service quality in the long term. RabbitMQ as an advanced routing layer for micro-services On the other end of the spectrum is an architecture which is more fluid and adapts to the ever-changing needs of many micro-services. What makes RabbitMQ shine in this environment is the very powerful routing capabilities it provides. The routing logic is implemented in different (so-called) exchange types that can be dynamically created by the application when needed. The destination services create the queues that they wish to consume from, then bind them to exchanges by specifying a pattern for the keys the publishers can use when publishing the message. (Think about these keys as metadata that the exchanges can use to route and deliver the messages to one or more queues.) The routing logic is implemented in different (so-called) exchange types that can be dynamically created by the application when needed. The destination services create the queues that they wish to consume from, then bind them to exchanges by specifying a pattern for the keys the publishers can use when publishing the message. (Think about these keys as metadata that the exchanges can use to route and deliver the messages to one or more queues.) RabbitMQ comes with four useful exchange types that cover most of the use-cases for messaging: 1) Direct exchange. This will deliver the incoming message to any queue whose binding key exactly matches the routing key of the message. If you bind the queues with the queue name as routing keys, then you can think about it as a one-to-one message delivery. It is simple to deliver the same message to multiple queues by using the binding keys for multiple queues. 2) Topic exchange. This will deliver the incoming message to any queue whose wild-card binding key matches the routing key of the published message. Binding keys can contain wild-card matching criteria for a compound routing key. (e.g. the binding key logs.*.error will match the routing keys logs.accounting.error and logs.ui.error ). This enables us to write simple services where the logic is well contained, and the message will arrive to the correct services through the “magic” of RabbitMQ. 3) Fanout exchange. Some messages need to be delivered to all queues, this is where a fanout exchange can be used instead of writing an elaborate multicast logic in the application. With a RabbitMQ fanout exchange, each service binds the appropriate queue to the exchange without need to specify a binding key, and it all happens automatically. If a binding key is specified, the fanout exchange will simply ignore it and still route/broadcast messages to all queues bound to it. 4) Headers exchange. This exchange leverages the structure of AMQP messages and is capable of complex routing based on the headers (including custom ones) of the AMQP message. Headers are metadata attached to each message sent via AMQP. In addition to exchanges, there are other useful features in RabbitMQ which enable the implementation of very complex messaging logic. Some of the most important features include: 1) Custom plug-ins. RabbitMQ is extensible by allowing its users to add plug-ins. Almost every aspect of RabbitMQ is customisable, including the management, authentication and authorisation, back-up solutions, and clustering. 2) Clustering. When a single RabbitMQ server is not enough, multiple RabbitMQ brokers can be connected to work together and scale the system. It can enable RabbitMQ to process more messages or increase resilience to errors. 3) Quality of Service tuning. Time-sensitive message delivery can be helped by attaching a TTL (Time-to-Live) value to either the message or the queue. Timed out messages can be automatically delivered to a Dead-letter queue. Combining ordinary routing logic and these extra features can lead to highly advanced routing logics. Another useful feature is using priority queues where the publisher can assign a priority level to each message. It is also possible to limit the number of unacknowledged messages, which allows for the performance tuning of the consuming services, in this case, RabbitMQ applies a back-pressure mechanism. RabbitMQ integrated into legacy systems In the previous use-case, I mentioned the possibility of using plug-ins to extend the functionality of RabbitMQ. This powerful feature allows RabbitMQ to act as a mediation layer between your RabbitMQ native (AMQP capable) services and other legacy applications. Some notable examples include: 1) Using RabbitMQ as an MQTT broker by simply enabling a plug-in. This opens up the landscape to many IoT technologies. 2) RabbitMQ’s JMS (Java Message Service) plug-in, which allows RabbitMQ to communicate with any JMS capable messaging solution. 3) If your application is using a proprietary protocol for communicating, it is possible to develop a custom plugin to connect to any such services. Conclusion As the above examples demonstrate, there is hardly anything that RabbitMQ can’t communicate with. But as with anything in life, it has a price. Although configuring RabbitMQ is mostly straightforward, sometimes the mere number of features can be overwhelming. If you face any problems with designing, implementing or supporting your RabbitMQ brokers, reach out to our expert team here. Or, if you’d like to kick start your career in one of the most in-demand technologies sign up for our 3-day RabbitMQ training course. Debugging RabbitMQ Want an intuitive system that makes monitoring and maintenance of your RabbitMQ easy? Get your free 45 day trial of WombatOAM now . Our RabbitMQ Capabilities page", "date": "2020-04-3rd"},
{"website": "Erlang-Solutions", "title": "Smart Contracts – How To Deliver Automated Interoperability", "author": ["Dominic Perini and Michael Jaiyeola"], "link": "https://www.erlang-solutions.com/blog/smart-contracts-how-to-deliver-automated-interoperability/", "abstract": "Introduction Here Dominic Perini and Michael Jaiyeola continue on their path exploring the latest in blockchain development. Following the excitement that has surrounded the innovation progressively introduced into the blockchain and distributed ledger technology (DLT) space, we wanted to provide an overview of the automation known as smart contracts. While there are various aspects of smart contracts which are of great potential benefit, with the technology still being relatively young, there are certain challenges of which to be aware. There remains some mysticism surrounding blockchain smart contracts and we, therefore believe, that it is worthwhile for us to take a moment to review the essence of contracts in society before expanding our analysis into some of the technicalities and frameworks of smart contract automation. The Origin Of Contracts The Romans were probably the first to come up with a written form of what was back then referred to as ‘contractus’, a word etymologically derived from a combination of ‘con-’ (together) and ’-trahere’ (draw, derive). This initial formalisation of a contract included obligations to fulfil a set of discrete transactions to enforce a promise or fall back on categories of cancellations so as to withdraw. Yet the need to formally or informally agree on expected social norms dates back much further to ancient civilisation and beyond. Since humans began to first associate they have formed a number of implicit expectations on one another’s behaviour. We could mention among these the spread of shared social ethics, which led to the development of a range of reasonable expectations of behaviour such as not killing or stealing from others. From a general perspective, any form of expected behaviour by an individual or group is either implicitly or explicitly bound to a contract. The judicial system and its enforcement which is triggered in the event of disputes form the basis under which society is regulated. This combines a number of incentives and deterrents which encourage individuals to comply with a commonly shared code of conduct. Explicitly formalised contracts expand on these acceptable social norms to deliver a resolution of known deviations from the expected course of action, some of which may or may not trigger legal penalties as a disincentive or compensation for damages. Smart Contracts and the Decentralisation of Authority History teaches us that rule enforcement has had the tendency to migrate from a peripheral form of control to a more centralised one. As an example, a number of local rules are applied within smaller groups (e.g. family, friends, coworkers, village inhabitants etc.), but in order to resolve disputes society has evolved into broader organisations that exercise a more authoritative form of power (e.g. legal systems, states, superstates, etc.). Recently, however, it appears that there is the desire to break free from what are increasingly perceived as slow, inadequate and occasionally corrupt central authorities. If we ask ourselves what triggers this reverse force, we might find an explanation in examining a number of examples of how interactions that involve a central governing body are perceived negatively from the parties involved: Example 1: A homeowner seeking a court ruling over a housing service supplier in breach of contractual terms might be frustrated about the costs and length of the process required to reach a resolution. This is because the process is often driven by human effort at various degrees of responsibility and in compliance with strict regulations. It is of no surprise that organisations providing these types of services can experience congestion, especially if they need to operate under constrained costs. Example 2: Cross border financial payments and balance settlements can be slowed down by a range of verifications that banks are required to undertake in order to comply with a regulatory framework. Erlang Solutions’ involvement in the FinTech space while working on the development of Vocalink/Mastercard payment switches taught us how a centrally governed financial system is bound to comply with strict regulations and standards. These constraints make it challenging to deliver highly customised interoperability that scales. Example 3: The European Court of Justice (ECJ) acting as a super-national governing body has been rejected by UK Brexiteers. Political and ideological motivations are behind this desire to break free from the central authority which some of the British electorate perceives as acting to the disadvantage of the nation. Example 4: In the aftermath of the 2008 subprime crisis, the transparency and trust towards centrally driven financial institutions have deteriorated significantly. Since then individuals developed an interest in decentralised forms of control over their assets such as that offered by blockchain technology. It is sensible to conclude that the need for performance, interoperability, low costs and decentralisation, as illustrated above, are motivating the eagerness of individuals to migrate towards automated, independent and inexpensive services to resolve localised interactions. These are all aspects that empower them to rapidly predict and determine the evolution of their lives in society. What Is A Smart Contract? A smart contract is an addressable blockchain entity that contains a set of storable data representing a logical state and a set of automated instructions used to alter that state. The instructions allow it also to interact transactionally with other addressable entities and emit events that distributed applications can subscribe to in order to trigger appropriate behaviours. The state, instructions and transactions are all maintained and secured by the underlying immutable blockchain technology. While originally conceived in the context of the theory behind Szabo’s Bit Gold digital currency and digital contracts, it is within the Blockchain / Distributed Ledger Technology (DLT) space that smart contracts have started to get real market traction. They respond to the requirement to automate interactions among peer to peer (P2P) actors that rely on the consensual order of events defined in the blockchain immutable data structure. Effectively they are algorithmic in nature and are following a structure common to a wide variety of computer programs (for instance Javascript, Java, Erlang etc.) and they typically require a Virtual Machine interpreter to drive their execution. Our Work In The Blockchain Space Smart Contract Use Cases What is fascinating about automated algorithms is that their execution within a given context can be predicted. This would effectively accomplish part of the expectations that are intrinsic to legally-binding contracts. If we were able to execute and enforce automated contracts in a reliable and safe context and if their effect could be extended beyond the confined environment of the computational world, they could overlap in functionality with legally binding contracts and therefore represent a good candidate to replace them in the long term. This would be an attractive alternative to the traditional legal enforcement as it involves lower costs and can be operated on top of a decentralised and trustless blockchain network governed by a series of automated incentives. Until now the fulfilment of contractual terms has predominantly relied on a trusted third party. For example, we observe how a centralised judicial system exercises its authority in determining how events evolve and to resolve disputes when agreed obligations are not fulfilled. An interesting case is that of the stocks and shares investment market, which has worked with the support of the stock exchange, as a centralised trusted party. Escrows are also of interest since they can be interpreted as a way to resolve disputes independently away from a central governing body in a similar way to how an automated smart contract would. While these scenarios are representative of how binding agreements are handled nowadays, there is growing speculation that, through smart contracts, a range of contractual terms can be enforced automatically without relying on a trusted third party. With DLT two new elements are introduced:* immutability and *consensus by distribution. They are both valuable instruments that come in handy in the chase for contractual automation and enforcement. Smart contracts rely on these features to support the fulfilment of a set of discrete transactions, with the capacity to automatically resolve a category of cancellations or malicious interactions. Nevertheless, as also happens with ordinary legal contracts, some edge cases are not covered due to the complexity it would require to define them and their associated implementation costs. As a consequence, alongside some expected and well defined cancellation clauses there are a number of “catch-all” state rollbacks, or, in the event that this is not possible, other exit clauses that enforce a penalty to work as a deterrent for participants, so that the so-called “happy-path” is generally preferred and disputes and cancellations are avoided. Also a number of strategies can be introduced to improve security and safety. Edge cases can be tested via generated and simulated conditions, while their execution can be constrained within dedicated safe environments, such as Virtual Machines that enable only a subset of features compared to environments that support fully featured general purpose languages. This has resulted in the creation of a wide spectrum of Domain Specific Languages (DSLs), which, by definition, are designed to support requirements that satisfy domain specific use cases. How Smart Contracts Operate A blockchain smart contract is initially defined as a script such as the one reported in the following example: contract SimpleStorage {\n    uint storedData;\n\n    function set(uint x) public {\n        storedData = x;\n    }\n\n    function get() public view returns (uint) {\n        return storedData;\n    }\n} The SimpleStorage contract comes as the first simple example of the Solidity language. It involves the state data that is represented by an integer that gets stored on chain and two functions, one for setting and another one for getting the stored integer. The steps required to use it are: first to compile it into a bytecode intermediate representation and then to deploy it on chain, an operation that involves a minimal charge. If it gets successfully deployed, a blockchain address is returned to the deployer. Subsequent interactions will use this address in order to access the set and get public functions. As we observe smart contracts interactions are typically triggered by an explicit invocation, as opposed to being awakened once the right conditions that allow the transition to a new state arise. The first example of how the notion of blockchain’s transactional automation started taking place can be seen on the Bitcoin network. The ancestors of what are now called smart contracts, were scripts developed on top of stack-based languages such as FORTH -like scripting in Bitcoin. This is a design that excludes some execution patterns (e.g. circular executions or loops) upfront. This is a non-Turing complete model which is difficult to understand and use. That said, there were distributed applications built on top of Bitcoin that were using this type of scripting such as the online game Spells of Genesis . The Ethereum project introduced a significant enhancement on this scripting language, and managed to obtain popularity by introducing a number of friendlier Turing complete languages such as Solidity whose syntax resembles that of Javascript. The Turing Complete Languages capabilities, instead of excluding loops by design, deter their excessive use by applying minor fees against each execution step. This means that a script inherently exits a loop once the funds allocated for that execution burn out. When it comes to the automation of contractual obligations outside the scope of a single blockchain network, there is also another critical issue to resolve – how to provide reliable support for oracles?. An oracle, as per the definition provided by BlockchainHub , is an agent that finds and verifies real-world/external occurrences and submits this information to a blockchain to be used by smart contracts. Further explanations on how an oracle operates can be found in the blog post Blockchain 2018 – Myth vs. Reality under section 7. As mentioned in this previous post, if a contract needs to access external information or needs to trigger activators in an external environment (frequently this means the real world, or in some cases another digital / virtual environment with which it operates), it must do so while ensuring that the health of the respective environment is preserved. To be more specific, oracles need to be trusted, which in some circumstances requires that a high level of trust gets extended to the external systems. This is essential if we aim to deliver comprehensive solutions that can encourage an increasingly large number of people and organisations to adopt this level of automation and interoperability on top of a blockchain DLT. The alternative, as happens predominantly these days, is to limit the level of automation to the boundaries of the digital world or, in a worst case scenario, a single blockchain. DSLs And GPLs For Smart Contracts Although Solidity and the Ethereum Virtual Machine (EVM) standard is, at the time of writing, the most popular smart contract platform on the market, other solutions are challenging its leadership. This is driven by attempts to deliver better performance, safety and security, while simultaneously extending the set of instructions, and enabling applicability in an increasingly wide range of use cases. Innovation in this area of expertise is proceeding with a tension between two predominant approaches. On one hand there is a trend that aims to introduce general purpose languages (GPLs) on blockchain to enable fully featured interpreters where keeping a high level of safety and security is very demanding. Then there is an approach which aims to simplify and restrict the set of instructions and the complexity of virtual machines in order to guarantee a safer and more secure execution limited to a domain specific use case. Which is where Domain Specific Languages (DSLs) come in. Here is a resource where you can find a comprehensive list of popular smart contract programming languages used at present in the marke – thanks to Sergei Tikhomirov for producing it. When one comes across this constellation of languages it is legitimate to think: ‘well this is definitely a sign of interest and activity’, however, it is also clear that this market hasn’t yet consolidated in its response to user needs (whether these are end-users or enterprises developing distributed applications). Although most of us are in favour of variety, we also understand how developers can get confused about this vast heterogeneity and it is sensible to expect that this translates into an obstacle that prevents the mass adoption of smart contracts. Smart Contract Templates Apart from the choice of language, there is often a significant effort spent on understanding just how to implement what it is needed in a smart contract and to ensure that its implementation complies with security and safety standards. Therefore, distributed application designers and developers typically rely on existing market tested templates. This is a process similar to the adoption of standard contracts in the legal world (e.g. employment contracts normally follow a standard contract in structure). Ethereum, for example, introduced a number of popular templates: ERC20 is designed for minting and allocating fungible tokens such as cryptocurrencies, ERC721 and ERC1155 aim at supporting the production and management of Non-Fungible Tokens (NFT), etc. More templates can be found on the Ethereum Improvement Proposal page some of which have reached the final phase, while others are still considered drafts. Some templates involve the definition of cross-chain Conditional Atomic Swaps, while others implement Ballots to deliver a voting mechanism, or lottery where random raffles take place or when a probability for a random event is simulated (e.g. the online game Dice2 ). Now, while it is true that the use of templates encourages initial adoption, one could argue that in the long term more creative and customised smart contracts will be needed in order to expand the market’s reach and drive proper decentralised interoperability. In fact, if we have to rely on an elite, as the only entity capable of delivering these solutions, we haven’t really resolved much in terms of operating independently and expressing a broader and richer variety of bespoke solutions. Conclusion Just as with Machine Learning, IoT and AI, autonomy and its inherent benefits provide the major value opportunity of blockchain smart contracts. The potential to conduct high-value transactions without the need for a third-party intermediary, such as requiring an estate agent for a property transaction, has revolutionising appeal with substantial benefits to individuals and companies alike. Of course, there will be some groups within society to whom this is a threat in the short to medium term, such as less skilled legal professionals and ‘middle-men’ facilitator type organisations, but this is for governments and other centralised institutions of authority to mitigate against. This case is not unique to smart contracts, it is present with the growth of all the emerging technologies we have just mentioned. On a practical level it must be remembered that, because smart contracts are pure computer program code, the logic imputed into the code is of vital importance. Where smart contract logic is derived from human logic and the legal system legislation commonly used in business, a smart contract can be expected to execute exactly as it has been set up to with all of the benefits and potential threats that this binary type of automation entails. What is most exciting to those currently outside of the blockchain ecosystem is the combination of computer science principles and traditional legal contract thinking necessary to create blockchain smart contracts. It is this fusion which is providing the platform on which to build a competent successor to traditional legal agreements. This post and that previous which examined digital asset ownership have covered two major emerging innovations that blockchain technology has influenced dramatically over recent times. What we believe we are seeing is the coming together of powerful psychological and technological trends which are heralding a new era in both the digital and physical world. Harnessing such powerful potential must be a priority for those actively involved in all parts of the blockchain ecosystem – compromising on safety and security is not an option for mass adoption to be a success. While a lot remains to be explored, we are moving along the way on an expansive journey destined to reshape the global digital and physical markets. At Erlang Solutions we continue to collaborate closely with our partners in researching and delivering innovative solutions to support blockchain projects. This ranges from core blockchain technologies to more specific distributed applications supported by automated smart contracts. Interesting forums to watch to learn more about smart contracts are the NFT London MeetUp and the Museum of Contemporary Digital Art where our architect and co-author Dominic Perini will be launching a series of online workshops aimed at bringing attendees up to speed with best practices, tools and libraries. FinTech Trends in 2021 Report", "date": "2020-06-15"},
{"website": "Erlang-Solutions", "title": "Blockchain No Brainer | Ownership in the Digital Era", "author": ["Dominic Perini"], "link": "https://www.erlang-solutions.com/blog/blockchain-no-brainer-ownership-in-the-digital-era/", "abstract": "Introduction Seven months of intense activity have passed since the release of Blockchain 2018 Myth vs. Reality article . As a follow-up to that blog post, I would like to take the opportunity to analyse in further detail the impact that this new technology has on our perceptions of asset ownership and value, how we are continuously exploring new forms of transactional automation and then conclude with the challenge to deliver safe and fair governance. Since the topic to cover is vast, I have decided to divide it into two separate blog posts, the first of which will cover how the meaning and perception of ownership is changing, while the second will discuss how Smart Contract automation can help deliver safe, fair, fast, low-cost, transparent and auditable transactional interoperability. My intention is to provide an abstract and accessible summary that describes the state-of-the-art of blockchain technology and what motivations have led us to the current development stage. While these posts will not focus on future innovation, they will serve as a prelude to more bold publications I intend to release in the future. Digital Asset Ownership, Provenance and Handling How we value Digital vs. Physical Assets In order to understand how the notion of ownership is currently perceived in society, I propose to briefly analyse the journey that has brought us to the present stage and the factors which have contributed to the evolution of our perceptions. Historically people have been predominantly inclined to own and trade physical objects. This is probably best explained by the fact that physical objects stimulate our senses and don’t require the capacity to abstract, as opposed to services for instance. Ownership was usually synonymous with possession. Let us try to break down and extract the fundamentals of the economy of physical goods: we originally came to this world and nothing was owned by anyone; possession by individuals then gave rise to ownership ‘rights’ (obtained through the expenditure of labour – finding or creating possessions); later we formed organisations that exercised territorial control and supported the notion of ownership (via norms and mores that evolved into legal frameworks), as a form of protection of physical goods. Land and raw materials are the building blocks of this aspect of our economy. When we trade (buy or sell) commodities or other physical goods, what we own is a combination of the raw material, which comes with a limited supply, plus the human/machine work required to transform it to make it ready to be used and/or consumed. Value was historically based on a combination of the inherent worth of the resource (scarcity being a proxy) plus the cost of the work required to transform that resource into an asset. Special asset classes (e.g. art) soon emerged where value was related to intangible factors such as provenance, fashion, skill (as opposed to the quantum of labour) etc. We can observe that even physical goods contain an abstract element: the design, the capacity to model it, package it and make it appealing to the owners or consumers. In comparison, digital assets have a stronger element of abstraction which defines their value, while their physical element is often negligible and replaceable (e.g. software can be stored on disk, transferred or printed). These types of assets typically stimulate our intellect and imagination, as our senses get activated via a form of rendering which can be visual, acoustic or tactile. Documents, paintings, photos, sculptures and music notations have historical equivalents that predate any form of electrically-based analog or digital representations. The peculiarity of digital goods is that they can be copied exactly at very low cost: for example, they can be easily reproduced in multiple representations on heterogeneous physical platforms or substrates thanks to the discrete nature in which we store them (using a simplified binary format). The perceivable form can be reconstructed and derived from these equal representations an infinite number of times. This is a feature that dramatically influences how we value digital assets. The opportunity to create replicas implies that it is not the copy nor the rendering that should be valued, but rather the original digital work. In fact, this is one of the primary achievements that blockchain has introduced via the hash lock inherent to its data structure. If used correctly the capacity to clone a digital item can increase confidence that it will exist indefinitely and therefore maintain its value. However, as mentioned in my previous blog post ( Blockchain 2018 – Myth vs. Reality ) the immutability and perpetual existence of digital goods are not immune from facing destruction, as at present there is a dependence on a physical medium (e.g. hard disk storage) that is potentially subject to alteration, degradation or obsolescence. A blockchain, such as that of the Bitcoin network, represents a model for vast replication and reinforcement of digital information via so-called Distributed Ledger Technology (DLT). Repair mechanisms can intervene in order to restore integrity in the event that data gets corrupted by a degrading physical support (i.e. a hard disk failure) or a malicious actor. The validity of data is agreed upon by a majority (the level of majority varying across different DLT implementations) of peer-to-peer actors (ledgers) through a process known as consensus. This is a step in the right direction, although the exploration of increasingly advanced platforms to preserve digital assets is expected to evolve further. As genetic evolution suggests, clones with equal characteristics can all face extinction by the introduction of an actor that makes the environment unfit for survival in a particular form. Thus, it might be sensible to introduce heterogeneous types of ledgers to ensure their continued preservation on a variety of physical platforms and therefore enhance the likelihood of survival of information. Our Work In The Blockchain Space The evolution of services and their automation In the previous paragraph, we briefly introduced a distinction between physical assets and goods where the abstraction element is dominant. Here I propose to analyse how we have started to attach value to services and how we are becoming increasingly demanding about their performance and quality. Services are a form of abstract valuable commonly traded on the market. They represent the actions bound to the contractual terms under which a transformation takes place. This transformation can apply to physical goods, digital assets, other services themselves or to individuals. What we trade, in this case, is the potential to exercise a transformation, which in some circumstances might have been applied already. For instance, a transformed commodity, such as refined oil, has already undergone a transformation from its original raw form. Another example is an artefact where a particular shape can either be of use or trigger emotional responses, such as artefacts with artistic value. Service transformation in the art world can be highly individualistic (depending on the identity of the person doing the transforming (the artist; the critic; the gallery etc) or the audience for the transformed work. Thus, Duchamp’s elevation (or, possibly, degradation) of a porcelain urinal to artwork relied on a number of connected elements (i.e. transformational actions by actors in the art world and beyond) for the transformation to be successful – these elements are often only recognised and/or understood after the transformation has been affected. Even the rendering from an abstract form, such as with music notation or a record, the actual sound is a type of transformation that we consider valuable and commonly trade. These transformations can be performed by humans or machinery. With the surge of interest in digital goods, there is a corresponding increasing interest in acquiring services to transform them. As these transformations are being automated more and more, and the human element is progressively being removed, even services are gradually taking the shape of automated algorithms that are yet another form of digital asset, as is the case with Smart Contracts. Note, however, that in order to apply the transformation, an algorithm is not enough, we need an executor such as a physical or virtual machine. In Part 2 we will analyse how the automation of services has led to the evolution of Smart Contracts, as a way to deliver efficient, transparent and traceable transformations. Sustainability and Access to resources Intellectual and imagination stimulation is not the only motivator that explains the increasing interest in digital goods and consequently their rising market value. Physical goods are known to be quite costly to handle. In order to create, trade, own and preserve them there is a significant expenditure required for storage, transport, insurance, maintenance, extraction of raw materials etc. There is a competitive and environmental cost involved, which makes access to physical resources inherently non-scalable and occasionally prohibitive, especially in concentrated urban areas. As a result, people are incentivised to own and trade digital goods and services, which turns out to be a more sustainable way forward. For example, let us think about an artist who lives in a densely populated city and needs to acquire a canvas, paint, brushes, and so on, plus studio and storage space in order to create a painting. Finding that these resources are difficult or impossible to access, he/she decides to produce their artwork in a digital form. Services traditionally require resources to be delivered (e.g. raw material processing). However, a subset of these (such as those requiring non-physical effort, for instance, stock market trading, legal or accounting services) are ideally suited to being carried out at a significantly lower cost via the application of algorithmic automations. Note: this analysis assumes that the high carbon footprint required to drive the ‘Proof of Work’ consensus mechanism used in many DLT ecosystems can be avoided, otherwise the sustainability advantage can be legitimately debated. The Generative Approach The affordable access to digital resources, combined with the creation of consistently innovative algorithms has also contributed to the rise of a generative production of digital assets. These include partial generation, typically obtained by combining and assembling pre-made parts: e.g. Robohash derives a hash from a text added to the URL that leads to a fixed combination of mouths, eyes, faces, body and accessories. Other approaches involve Neural Net Deep Learning: e.g. ThisPersonDoesNotExist uses a technology known as Generative Adversarial Network (GAN) released by NVidia Research Labs to generate random people faces, Magenta uses a Google TensorFlow library to generate Music and Art, while DeepArt uses a patented neural net implementation based on the 19-layer VGG network . In the gaming industry we should mention No Man’s Sky , a mainstream Console and PC Game that shows a successful use of procedural generation . Project DreamCatcher also uses a generative design approach that leverages a wide set of simulated solutions that respond to a set of predefined requirements that a material or shape should satisfy. When it comes to Generative Art, it is important to ensure scarcity by restricting the creation of digital assets to limited editions, so an auto-generated item can be traded without the danger that an excess of supply triggers deflationary repercussions on its price. In Blockchain 2019 Part 2 we will describe techniques to register Non Fungible Tokens (NFT) on the blockchain in order to track each individual replica of an object while ensuring that there are no illegal ones. Interesting approaches directly linked to Blockchain Technology have been launched recently such as the AutoGlyphs from LarvaLabs, although this remains an open area for further exploration. Remarkably successful is the case of Obvious Art where another application of the GAN approach resulted in a Generated Artwork being auctioned off for $432,500 . What prevents mass adoption of digital goods Whereas it is sensible to forecast a significant expansion of the digital assets market in the coming years, it is also true that, at present, there are still several psychological barriers to overcome in order to get broader traction in the market. The primary challenge relates to trust. A purchaser wants some guarantees that traded assets are genuine and that the seller owns them or acts on behalf of the owner. DLT provides a solid way to work out the history of a registered item without interrogating a centralised trusted entity. Provenance and ownership are inferable and verifiable from a number of replicated ledgers while block sequences can help ensure there is no double spending or double sale taking place within a certain time frame. The second challenge is linked to the meaning of ownership outside of the context of a specific market. I would like to cite as an example the closure of Microsoft’s eBook store . Microsoft’s decision to pull out of the ebook market, presumably motivated by a lack of profit, could have an impact on all ebook purchases that were made on that platform. The perception of the customer was obviously that owning an ebook was the same as owning a physical book. What Microsoft might have contractually agreed through its End-User License Agreement (EULA), however, is that this is true only within the contextual existence of its platform. This has also happened in video games where enthusiast players are perceiving the acquisition of a sword, or armour as if they were real objects. Even without the game closing down its online presence (e.g. when its maintenance costs become unsustainable), a lack of interest or reduced popularity might result in a digital item losing its value. There is a push, in this sense, towards forms of ownership that can break out from the restrictions of a specific market and be maintained in a broader context. Blockchain’s DLT in conjunction with Smart Contracts, that exist potentially indefinitely, can be used to serve this purpose allowing people to effectively retain their digital items’ use across multiple applications. Whether those items will have a utility or value outside the context and platform in/on which they were originally created remains to be seen. Even the acquisition of digital art requires a substantial paradigm shift. Compared to what happens with physical artefacts, there is not an equivalent tangible sense of taking home (or to one’s secure storage vault) a purchased object. This has been substituted by a verifiable trace on a distributed ledger that indicates to whom a registered digital object belongs. Sensorial forms can also help in adapting to this new form of ownership. For instance, a digital work of art could be printed, a 3D model could be rendered for a VR or AR experience or 3D printed. In fact, to control what you can do with a digital item is per se a form of partial ownership, which can be traded. This is different from the concept of fractional ownership where your ownership comes in a general but diluted form. It is more a functional type of ownership. This is a concept which exists in relation to certain traditional, non-digital assets, often bounded by national laws and the physical form of those assets. For instance, I can own a classic Ferrari and allow someone else to race it; I can display it in my museum and charge an entry fee to visitors; but I will be restricted in how I am permitted to use the Ferrari name and badge attached to that vehicle. The transition to these new notions of ownership is particularly demanding when it comes to digital non-fungible assets. Meanwhile, embracing fungible assets, such as a cryptocurrency, has been somewhat easier for customers who are already used to relating to financial instruments. This is probably because fungible assets serve the unique function of paying for something, while in the case of non-fungible assets there is a range of functions that define their meaning in the digital or physical space. Conclusion In this post we have discussed a major emerging innovation that blockchain technology has influenced dramatically over the last two years – the ownership of digital assets. In Blockchain 2019 – Part 2 we will expand on how the handling of assets gets automated via increasingly powerful Smart Contracts. What we are witnessing is a new era that is likely to revolutionise the perception of ownership and reliance on trusted and trustless forms of automation. This is driven by the need to increase interoperability, cost compression, sustainability, performance (as in the speed at which events occur) and customisation, which are all aspects where traditional centralised fintech systems have not given a sufficient solution. It is worthwhile, however, to remind ourselves that the journey towards providing a response to these requirements, should not come at the expense of safety and security. Privacy and sharing are also areas heavily debated. Owners of digital assets often prefer their identity to remain anonymous, while the benefit of socially shared information is widely recognised. An art collector, for instance, might not want to disclose his or her personal identity. Certainly, a lot more still remains to be explored as we are clearly just at the beginning of a wider journey that is going to reshape global digital and physical markets. At Erlang Solutions we are collaborating with partners in researching innovative and performant services to support a wide range of clients. This ranges from building core blockchain technologies to more specific distributed applications supported by Smart Contracts. Part of this effort has been shared on our website where you can find some information on who we work with in the fintech world and some interesting case studies, others of which remain under the scope of NDAs. This post intentionally aims at providing a state-of-the-art analysis. We soon expect to be in a position to release more specific and, possibly controversial, articles where a bolder vision will be illustrated. Get notifications when more content gets published – you know the drill, we need your contact details – but we are not spammers! FinTech Trends in 2021 Report", "date": "2019-05-28"},
{"website": "Erlang-Solutions", "title": "Fintech 2.0 Incumbents vs Challengers – Banking’s Battle Royale", "author": ["Michael Jaiyeola"], "link": "https://www.erlang-solutions.com/blog/fintech-2-0-incumbents-vs-challengers-bankings-battle-royale/", "abstract": "The fundamental building blocks of finance and financial services (FS) are transforming driven by emerging technologies and changing societal, regulatory, industrial, commercial and economic demands. Fintech or financial technology is changing the FS industry via infrastructure-based technology and open APIs. The venue for last week’s World Fintech Forum was filled with fintechs from across the ecosystem and representatives from big banks. The agenda promised two days of talks that would cover the spectrum of discussion points that are occupying the industry and this blog post will look at the Top 10 Takeaways for those unable to attend the event. Key takeaways Great user experience is vital There’s a lot of regional variation in fintech disruption More challengers and incumbents are partnering Big Tech’s strength is in data Open Banking: regulations proving a boost to fintechs Blockchain: A dramatic increase in levels of investment For Cryptocurrencies – the jury’s still out AI is key Talent recruitment is one of the biggest challenges RegTech enables conforming with the changing regulatory landscape FinTech Trends in 2021 Report 1. The customer really is king Consumers and business customers alike have embraced the idea of on-demand finance, thanks to mobile and cloud computing. Fintech trends show that people are more comfortable managing their money and business online, and they’re less willing to put up with the comparatively slower and less flexible processes of traditional financial services. Present on day one of the conference was Laurence Krieger, COO of SME lender Tide, who views customer experience as the key advantage held by challengers. At Tide, they spotted the gap in SME banking where incumbents were simply offering the same type of products as those offered to retail customers but just repackaged as being for SMEs. Meanwhile, Filip Surowiak from ViaCash looked at how they are bridging the gap between cash and digital cash solutions by addressing the movement away from branch and ATM use to a more convenient model. It is these customer-centric approaches which will both win and retain business for fintechs and incumbents alike. 2. The West is moving at a different pace to Asia While there are over 1,600 UK fintech companies, a figure set to double by 2030 according to the UK Government’s Tech Nation report, it’s emerging economies which lead the global charge in the sector. In the UK there is a 71 percent adoption of fintech while it’s 87 percent in both China and India. Chinese fintech ecosystems have scaled and innovated faster than their counterparts in the West. In Asia there are singular platforms or super apps which combine FS entertainment and lifestyle products, as yet the equivalent does not exist elsewhere. 3. Partnerships are the favoured way to go for incumbents and fintechs Despite recent announcements such as of HSBC’s Kinetic platform and Bo from RBS, for banks to build their own digital solutions takes significant investment and resources with no guarantees of success. It also takes significant capital to acquire a successful competing fintech. Strategic investments and partnerships was the approach most favoured by the incumbents present at Fintech Forum. According to McKinsey’s report, Synergy and Disruption: Ten trends shaping fintech , 80 percent of financial institutions have entered fintech partnerships. Meanwhile, 82 percent expect to increase fintech partnerships in the next three to five years, according to a Department for International Trade report entitled UK FinTech State of the Nation. At the conference BNY Mellon, RBS, Citi Ventures and others were all present and positioning their organisation as being a willing and accessible partner for budding fintech startups. Luis Valdich, MD at Citi Ventures, spoke of their success stories with Pindrop Security and Trulioo and warned us all to avoid being pushed towards a focus on services instead of on the product in collaborations. Udita Banerjee from RBS and Christian Hull from BNY Mellon also painted a welcoming environment in which ambitious startups can succeed. 4. Fintech v Techfin – Big Tech’s data riches During day one’s panel on FS disruption – Stephanie Waismann, CCO from Barclays, outlined the key advantage for incumbents exists in the data which they hold. This may be true, but when it comes to the amount of data available – Big Tech’s GAFA (Google, Apple, Facebook and Amazon) are unrivalled. They appear poised to grab a substantial slice of the pie by leveraging their deep customer relationships and knowledge with which to offer financial products. Joan Cuko, mobile payments analyst form Vodafone, examined Big Tech’s role in the future of FS, classifying them as frenemies and (again) collaboration was highlighted as being the ‘ best path for long-term growth ’. The positioning of fintech challengers to align closer towards the tech part of what they do indicates the importance they attach to this side of their business. Noam Zeigerson, Chief Data Officer at Tandem Bank, poses the rhetorical question of whether they are either a bank or a tech player. Likewise, Tide’s Laurence Krieger sees fintechs as primarily being tech players and financial services providers second. 5. Open Banking’s untapped potential Open Banking’s enabling technology APIs allow non-financial businesses to offer financial services to their clients based on customer banking data is recognised as having revolutionising potential. Sam Overton, Head of Commercial at Bud, discussed the latent power behind Open Banking and how banks and fintechs can approach end consumers to empower them and prove that their data and privacy are not at risk. He emphasised the need for alignment on value issues where it is being approached in a one-fits-all approach and that for real traction there needs to be segmentation for individual user groups such as young parents. 6. Blockchain is here to stay? Although DLT or blockchain’s components – such as cryptographic hashes, distributed databases and consensus-building – are not new. When combined, they create a powerful new form of data sharing and asset transfer, capable of eliminating intermediaries, central third parties and expensive reconciliation processes. Monday’s panel included Keith Bear, Fellow of The Cambridge Judge Business School, whose ‘2nd Global Blockchain Benchmarking Study’ found: The Banking, Financial Markets and Insurance industries are responsible for the largest share of live (enterprise blockchain) networks. Manu Manchal, Managing Director of Consensys, observes the FS landscape as one of price compression and increased competition with blockchain essential to contributing to the ultimate ‘ goal of bringing the cost of providing financial products to zero ’. Noam Zeigerson from Tandem Bank regards blockchain as the ‘ most transformative technology of all time, only surpassed by the internet ’. Soren Mortensen, Director of Global Financial Markets at IBM, summed things up nicely when it comes to blockchain in FS: ‘ everyone acknowledges that the technology is proven – what’s needed is greater value propositions and use cases ’. 7. For Cryptocurrencies – the jury is out OKCoin’s Head of Europe, Gabor Nguy, set out the case for cryptocurrency’s role within FS. While the benefits of blockchain for improving banks’ processes are accepted, cryptocurrency is still seen somewhat as the unruly upstart within much of the financial family. Gabor argued with broad agreement that ‘ digital assets are here to stay ’ – only this week a UK legal panel has defined their recognition of both digital assets and smart contracts. Again, when it comes to the adoption of digital assets, Europe is some way behind in terms of mass adoption when compared to Asia. Our Technical Lead, Dominic Perini’s recent blog post discusses the psychology of digital asset ownership and provenance. Our Work In The FinTech & Blockchain Space 8. AI is the technology ‘must-have’ for banks Whereas for many, blockchain is the technology which holds the biggest potential, the panel on the second day highlighted the role of AI. The global technology research firm, Gartner, estimates that the blockchain market will be worth roughly $23 billion by 2023, as a way of comparison the estimated business value created by AI is $3.9 trillion for 2022. Jeremy Sosabowski, who works with AI for risk analysing at Algo Dynamix, sees it as not being a nice-to-have, but a ‘ must-have for banking ’. The biggest potential for AI lies around middle-office banking involving compliance and risk. Angelique Assaf spoke about Cedar Rose’s use of AI Powered Models for Credit Risk Assessments with a clear message that: ‘ Data is our asset and AI is our tool ’. The key consideration for implementing AI was again focused on the question of To Build or To Buy. With 60 percent of AI talent being absorbed into the tech and FS sectors, Nils Mork-Ulnes, Head of Strategy at AIG, was on hand to provide valuable insight into building an AI centred product team. Soren Mortensen from IBM framed things as AI not actually existing (not until at least 2050) – and what we currently have is Augmented Intelligence. He identified the key driver for successful AI implementation as data availability – it’s quality and appropriateness. Of course, the high profile fallout involving the Goldman Sachs backed Apple Credit Card provides a timely reminder of the nascent nature of both cross-industry collaboration and the use of AI in the form of blackbox algorithms. 9. The human element is still key to success Getting specialist skills, both technical and non-technical, is generally recognised as one of the biggest challenges in the fintech space. Laurence Krieger from Tide explained that, in his experience, the very best graduates now want to move into the fintech/startup world. Meanwhile, Stephanie Waismann from Barclays observed that many banks are looking to recruit form outside of the banking and FS industry for a broader range of skills and are going towards students and universities to help fill gap areas. 10. Regulatory compliance must be at the centre of planning Emerging international standards have mostly taken the form of high-level principles, leaving national implementation (both regulation and supervision) to diverge considerably across jurisdictions and across different FS sectors. The Financial Conduct Authority (FCA) is a key influence in setting standards of regulation globally. On hand to advise eager young startups on the importance of regulatory compliance was Ross Paolino, General Counsel at Western Union. Of course, FS is one of the most highly regulated industries and compliance is becoming of increasing importance whether that be for startups or large institutions. Agreed upon by all was the fact that tech simply moves faster than regulators can regulate – and that the gap will always exist to a greater or lesser extent. He directs us towards sandboxes as the best options for new ideas to be tested. Concluding Thoughts Similarly to what we have witnessed with publishing, financial services are made up of information rather than physical goods and are therefore seen as one of the industries most vulnerable to disruption by software technology. Of course, not all fintech startups are out to hurt banks, and in fact, many services use legacy platforms to bring them more customers. For incumbents the costs of digitisation are substantial, partnering with specialist vendors is the most efficient way to approach implementing changes to each technology layer. Trying to connect the dots between different parts of the community here in the UK and reaching out as a portal for organisations worldwide is Fintech Alliance who are backed by the Department for International trade. They aim to provide access to people, firms and information, including connections to investors, policy and regulatory updates, and the ability to attract and hire workers. In the end, the talks from the stage and taking place during the networking sessions at the World Fintech Forum lived up to expectations and left an overall impression of positivity around the fintech ecosystem. Our work in the FInTech space.", "date": "2019-11-28"},
{"website": "Erlang-Solutions", "title": "FinTech Trends – Disruptors Disrupted", "author": ["Michael Jaiyeola"], "link": "https://www.erlang-solutions.com/blog/fintech-trends-disruptors-disrupted/", "abstract": "The first few months of the 2020s have been nothing if not eventful. The start of a new decade is always a time for optimism so, just as many of us look forward to a new year as an opportunity to change something in our lives, you can reasonably magnify that feeling 10-fold when applying it to beginning a new decade. In this article, I take a look at some of the highlights of FinTech’’s first quarter of 2020. Of course, the COVID-19 pandemic has dominated everyone’s minds and, while I do examine what it means in relation to the FinTech ecosystem, outside of this there has been plenty else of note occurring so far this year too and that is where we’ll start. What Happened in Q1 for FinTech Vodafone leaves Facebook’s Libra to focus on M-Pesa Libra, Facebook’s proposed blockchain digital currency, provided one of the biggest stories of 2019, even though it did seem at one point that the concerns held by governments and central banks could permanently derail the project. Vodafone, the major UK telecommunications provider, joined a growing list of big name companies dropping out of the Libra project which was more unwelcome news for Facebook. Vodafone stated that, as part of their commitment to financial inclusion, they would focus the resources they had planned for Libra on M-Pesa , the digital payment service, a company which we have previously worked with on development projects. The plan was to expand the successful mobile solution to further markets in Africa where it has already enjoyed great success since its launch in 2007. Vodafone did appear to leave the door open to some future involvement with the Libra Association and maybe the revised roll-out plan by Facebook will prove more attractive to partners with its decreased emphasis on the cryptocurrency element of the project. Brexit’s First Big Name Victim? N26 pulled out of the UK citing Brexit as the reason. The German challenger with five million global customers became the first fintech of note to publicly claim that Brexit had directly led to a decision to cease operations within the UK. Financial services ‘passporting’ in its current guise will cease to be possible from the end of this year and N26 opted not to apply for a UK banking licence which would have allowed them to continue operating. However, critics were quick to point out that Brexit may have been a convenient scapegoat to distract from some lacklustre numbers. Since a high profile launch in the UK in October 2018 (the referendum where Leave won was, of course, back in 2016) N26’s performance has been notably poor in comparison to that of established challengers such as Monzo and Starling. Despite Brexit, the UK remains an attractive place for any fintech to operate due to its digitally aware consumers and startup friendly ecosystems. N26 CEO and co-founder Valentin Stalf said at their time of launching: “The UK is one of the most digitally advanced countries in the world.” We have already had the 2020 UK Budget which was generally deemed to be fintech-friendly, encouraging as we approach the end of the transition period. Chancellor Rishi Sunak announced the Government will carry out a review of the FinTech industry to “support growth and competitiveness in the sector” as part of its goal to champion the UK’s lead in encouraging tech talent. The review will be led by Ron Kalifa, vice-chair of Worldpay and non-executive director at the BoE. FinTech Trends in 2021 Report Revolut Kept Making the News The UK based money app joined the list of Euro challengers including Monzo and N26 looking to make waves in the US market where their main competition will be from Chime. They plan to launch some of their basic account features before rolling out the various additions which are already available to their ten million plus European customers. They have partnered with New York based Metropolitan Commercial Bank (MCB). Also, this year we have seen Chief Financial Officer David MacLean quit his role at Revolut after just five months and a new $500 million funding round for what is now the most valued fintech in Europe at $5.5bn. Some have been quick to claim this is another example of FinTech’s price bubble. Of course, the elephant in the room when admiring the sheer scale of some of Revolut’s numbers is that profit, that seemingly, at times, neglected measuring-stick of sustainable business models, is still somewhat far from being achieved. Although a recognised growth strategy, this approach does contrast jarringly with the achievements of other UK profit making fintechs Transfer Wise and Oak North Bank. Q1’s Headline Grabbing FinTech M&A Megadeal Due to the current macroeconomic environment, reduced funding for fintechs will force firms to seek collaboration, investment or acquisition in order to grow and succeed. 2020 has already seen Visa acquire the fintech Plaid for $5.3 billion . Plaid calls itself the “plumbing” behind fintech and provides APIs which connect users’ bank accounts with third party providers such as Wealthfront and Robinhood. The frenetic pace of FinTech M&As over recent months has seen not just incumbents buying fintechs (Visa/Plaid, Intuit/Credit Karma) but fintechs buying banks (Lending Club/Radius Bank) should accelerate during any economic slowing over the next 12 to 18 months. Repercussions of COVID-19 for FinTech – Concerns and Opportunities The biggest event when talking about any industry or pretty much anything thus far in 2020 is the coronavirus or COVID-19 outbreak. What I consider here is its the meaning for the FinTech industry. Clearly not the most important question of the moment but there are other more suitable channels to discover and discuss other topics relating to the broader situation. That having been said, in financial services business as usual will not be as usual anymore. The pandemic has created an immediate operational crisis for FS firms, especially those that have previously elected to ignore the need to digitise. The good news for fintechs is that they are well versed in the art of disruption of traditional fields and should be best placed to adapt to any new normal. Traditional firms will be accelerated towards replacing legacy monolith infrastructure with microservices-based tech stacks with maintaining operational resilience a clear priority in the short-term. Right now, banks and investment firms are being overwhelmed with customer requests, Barclays are paying frontline staff triple overtime to ensure call centres are fully staffed to meet demand. Lending and wealth management fintechs are being inundated with customer requests and are scrambling to increase capacity and resources to stand up to system stress. Warning signs became apparent in this regard early in March thanks to the travails of the US stock trading app Robinhood which experienced its third outage in a week as stock market volatility peaked. The problems led to this statement being published by Baiju Bhatt and Vlad Tenev, Co-Founders and Co-CEOs “Multiple factors contributed to the unprecedented load that ultimately led to the outages. The factors included, among others, highly volatile and historic market conditions; record volume; and record account sign-ups. Our team is continuing to work to improve the resilience of our infrastructure to meet the heightened load we have been experiencing.” This is the kind of situation which must keep any fintech’s top people awake at night. With unquestioning customer loyalty becoming a thing of the past and a plethora of convenient app alternatives available, failure of the technology on which everything relies can prove too much to recover from. Yet there are opportunities and segments within FS that will be less hard hit than others and will play a key part in our economic recovery. With digital-only as the financial industry default legacy, institutions will turn to tech companies for their expertise and know-how. Digital businesses have generally responded faster than traditional firms, helping to deliver government backed credit and by offering COVID-19 specific products. In the UK, Trade Ledger, Wiserfunding, Nimbla, and NorthRow make up a business lending taskforce and provide a platform for lenders to virtually deploy funds to businesses during the crisis. Covid Credit is a POC used to demonstrate loss of income to the UK’s HMRC and is a notable FinTech contribution to address some of the disruption caused by coronavirus. The collaboration between Fronted, 11:FS and Credit Kudos in creating the platform demonstrates the nimbleness of fintechs at this time. Digital currencies and contactless payments will be areas of increased focus with bricks and mortar businesses having been forced to close during lockdowns. The World Health Organisation (WHO) encouraged people to use contactless forms of payments whenever possible in response to COVID-19 in recognition that the hand to hand transfer of physical cash may lead to the virus spreading. Mastercard was one of the leading payment organisations calling for higher contactless limits to be put in place in order to be socially responsible and to help small businesses who may be suffering disproportionately during this crisis. Gregor Dobbie, CEO at Vocalink (Mastercard), recently wrote in depth on how we must retain a human touch to ensure inclusion for groups such as the elderly during the growth in digital options for day to day living. With app downloads for banks like Barclays and Monzo having soared across March it’s difficult to imagine that the majority of these new users will revert to traditional methods of banking and managing their money when and if that becomes possible. Our Work In The FinTech & Blockchain Space Other Business of Q1 2020 Our parent company, Trifork, has worked in the banking business for many years developing some of the very first mobile banking solutions, such as the innovative MobilePay for Danske Bank. Trifork are currently extending their tech solutions into the UK utilising our expertise in building scalable, fault-tolerant systems for fintechs and I have recently written about some of our latest FinTech projects . We have enjoyed a close relationship with the DiT backed Fintech Alliance in the UK and have discussed some exciting joint ventures for the FinTech community as we emerge from lockdown so it was an exciting development for the group when Trifork recently confirmed a similar strategic partnership with Copenhagen Fintech in Denmark. There are a number of interesting FinTech startups and high profile sponsors involved with Copenhagen Fintech and Trifork’s experience in cloud-based development and FinTech innovation, along with a passionate commitment to the tech and startup experience will mean some exciting opportunities and projects soon to come. Find out more about our work in the FinTech space.", "date": "2020-05-1s"},
{"website": "Erlang-Solutions", "title": "10 Unusual Blockchain Use Cases", "author": ["Erlang Solutions"], "link": "https://www.erlang-solutions.com/blog/10-unusual-blockchain-use-cases/", "abstract": "Digital assets have increased in the last few years, and the FinTech industry is set to continue growing throughout 2019. Blockchain obviously falls into this category, with Bitcoin and Ethereum. Crypto startups and established companies are looking to develop their business strategies by incorporating FinTech. By allowing transparency on the ledger, this offers more secure online transactions, which are increasing at a rapid rate. But what unusual blockchain cases have been emerging, in this blog post we will explore some cases that caught our eye. It’s safe to say blockchain’s transparency and accountability is a huge factor and can lend itself to a number of industries in unusual and impressive ways. Here, we look through how: 1 Reducing Identity Theft Forbes magazine states that 2.6 BILLION records were lost or stolen in 2017, and identified that theft accounts for 69% of all data breaches. Using blockchain to protect records offers a new level of security in the form of identification checks such as verifiable user IDs and multi-factor authentication. This means the need for countless passwords and usernames you’ll evitably end up forgetting is redundant. The Civic’s Secure Identity Platform (SIP) is the perfect example of this. The entire blockchain is decentralised, meaning there is no point of weakness and therefore a very limited chance of hackers breaking in. This includes the self-sovereign IDs as mentioned and the removal of countless passwords and paperwork connected to accounts. The result is a single key that is matched to an immutable ledger. Your digital ID can include social security information, social media data, and medical records, but also all the private data you gather from online actions. 2 Buying virtual cats and gaming Blockchain can be used in gaming in general by creating digital and analog gaming experiences. Cryptokitties emerged in 2017 and generated more than $1.3 million in approximately one month. By March 2018, it had raised $12 million . What’s the point? It utilises a game into an economy. By investing in CryptoKitties, players can invest, build and extend their gaming experience. BitPainting is another example where you can create and share your digital art in the form of “crypto-collectables”. A third example is a gaming platform called Chimaera that converts gaming into virtual assets to allow developers to manage, share and trade their games. FinTech Trends in 2021 Report 3 Cannabis Legal cannabis is a booming business in the United States with an estimated $9 billion dollars spent in 2017. This estimate is set to grow, no pun intended. With this amount of money, a cashless solution could offer business owners further security. Transactions are easily trackable and offer transparency and accountability that traditional banking doesn’t. The link between cryptocurrencies and cannabis isn’t a new revelation; it has been used in the industry for a number of years. Cryptocurrencies have been created including PotCoin, DopeCoin, and ParagonCoin. However, none have fully reached their potential. There are some key differences between why blockchain would be more appropriate for the cannabis trade than cryptocurrencies, and this is similar to traditional banking. Campaignlive sums it up perfectly here: “By integrating blockchain into supply chain management, business owners could provide legislators, consumers, and banks with the data they need to build what they need to gain mainstream acceptance: trust.” Transparency is a key benefit to using blockchain within the cannabis industry. But what other advantages does it hold? Blockchain offers anonymity. As legal cannabis is still a fairly new industry, stepping towards the edge of caution is sensible business acumen. Secondly, the tax put in place for legal marijuana in the US can be crippling to small businesses. The account fee for cannabis in California can be up to $60,000 per annum. Blockchain results in less tax because the individual will be taxed as property, not currency. Thirdly, many cannabis consumers aren’t aware of how cryptocurrencies work, offering further security. It makes sense for traditional banking too. Along with various taxes to the individual businesses, the federal government has also put in place rules and regulations for banks. This places a risk on banks investing in the cannabis business, as a result, the California state treasurer John Chiang has proposed opening a public bank dedicated to the cannabis industry. 4 Sharing solar power Siemens has partnered with startup LO3 Energy with an app called Brooklyn Microgrid . This allows residents of Brooklyn who own solar panels to transfer their energy to others that don’t have this capability. Consumers and solar panel owners are in control for the entire transaction. 5 Marriage Same-sex marriage is still banned in 87% of all countries in the world, according to figures from campaignlive . Bearing that in mind, the Swedish sportswear brand Björn Borg have discovered an ingenious way for loved ones to be in holy matrimony, regardless of your sexual orientation, beliefs and the country you live in. But how? Blockchain is stereotypically linked with money but take away those connotations and all you have is an effective ledger that can record events as well as transactions – no finance is necessarily required. It has the ability to record and preserve events without the need for a third party so Björn Borg has put this loophole to extremely good use by forming the digital platform Marriage Unblocked where you can propose, marry and exchange vows all on the blockchain. What’s more, the records can be kept anonymous offering security for those in potential danger. And of course, you can request a certificate to display proudly too! The first couple to do this was David Mondrus and Joyce Bayo in 2014, by engraving their nuptials on the Bitcoin blockchain. The plus side of a blockchain marriage is the flexibility; libertarianism claims that when billionaire Brock Pierce wed his wife last year, they created a contract that could be “renewed, changed and dissolved annually”. Put simply, blockchain offers smart contracts. Whilst this doesn’t hold any legal requirements, everything is produced and stored online. If religion or government isn’t a primary concern of yours, where’s the harm in a blockchain marriage? As Marriage Unblocked says, “no state or religion should control love” . 6 Simplifying the Internet of Things (IoT) Blockchain offers ledgers that can record the huge amounts of data produced by IoT systems, and yep you can guess, it’s the transparency that offers the level of trust that other services cannot offer. Internet of Things is one of the most exciting element to come out of technology; these connected ecosystems can record and share various interactions and blockchain lends itself perfectly to this . It can transfer data and gives identification for both public and private sector use cases. Blockchain can be used for the following: Public sector; infrastructure management, taxes (and other municipal services). Private sector; logistical upgrade, warehousing tracking, greater efficiency, and enhanced data capabilities. There is even a blockchain specifically for IoT which handles machine-to-machine micropayments by the name of Tangle. Tangle is the data structure behind micro-transaction crypto token that is purposely optimised and developed for IoT. It differs from other blockchains and cryptocurrencies by having a much lighter and efficient way to deal with ten of billions of devices. Created by David Sønstebø, Sergey Ivancheglo, Serguei Popov and Dominik Schiener, they included a decentralised peer-to-peer network that relies on a Distributed Acyclic Graph (DAG), which creates a distributed ledger rather than “blocks”. There are no transaction fees, no mining, and no external consensus process. This also secure data to be transferred between digital devices. Serguei Popov has provided a whitepaper on The Tangle if you’d like further information. 7 Improving Supply Chains Blockchain provides real-time tracking that is essential for any companies with a significant number of supply chains. This is incredibly useful for the consumer industry and pharmaceutical industry. In fact, Forbes reports that Walmart partnered with IBM to produce a blockchain called Hyperledger Fabric blockchain to track foods from the supplier to the shop shelf. This allows for accountability and a clear process from the start of the business trail, right to the end. The whole supply chain is clear and allows multiple parties to access the same database whilst providing one truthful point of origin. You can find examples of this all over the web. We particularly enjoy this from origintrail.io . Our Work In The FinTech & Blockchain Space 8 Elections and Voter Fraud Voting on a blockchain offers full transparency and therefore reduces the chance of fraudulent voting. One example of this is the app Sovereign which was created by nonprofit organisation Democracy Earth. This blockchain produces tokens that represent votes rather than money. Another example is Sierra Leone which became the first country to run a blockchain-based election last year with 70% of the pollers using the technology to anonymously store votes in an immutable ledger. This offered instant access of the election results to the public. These results were placed on the Agora’s blockchain and by allowing anyone to view, the government’s aim was to provide a level of trust with its citizens and offers a platform to reduce controversy as well as reducing costs enquired when using paper ballots. The result is a trustworthy and legitimate result that will also limit the amount of the hear’say from opposition voters and parties, especially in Sierra Leone that has had corruption claim in the past. Leonardo Gammar created the Agora blockchain and raises a very good point about switching to blockchain for the next voting platform. With the online security threat, resulting in paper ballots continuing, blockchain again offers transparency from start to finish. Whilst we still have a long way to go, the potential for using a blockchain voting platform is exciting. 9 Healthcare With the emphasis on keeping many records in a secure manner, blockchain lends itself nicely to medical records and healthcare. MedRec is one business using blockchain to keep secure files of medical records by using a decentralised CMS and smart contracts. This also allows transparency of data and the ability to make secure payments connected to your health. Blockchain can also be used to track dental care in the same sort of way. One example is Dentacoin that uses the global token ERC20. It can be used for dental records but also to ensure dental tools and materials are sourced appropriately, whether tools are used on the correct patients, networks that can transfer information to each other quickly and a compliance tool. 10 Luxury items and art selling With the ability to see everything that’s going on, and track the data and transactions within the blockchain, it lends itself nicely to luxury items such as diamonds and art. Deals made on a blockchain reduces the chances of fraudulent behaviour. One business offering this is Everledger ), as well as many other lines of business, that we mention in our post Blockchain Myth vs Reality . Everledger works by verifying provenance and limiting risks of fraud by certifying the assets (for example a piece of artwork) and then storing these records publicly. This offers transparency by displaying the verified ownership of the piece in question. Another example is Chronicled ; a start-up blockchain-based technology for supply chain management to create a registry that tracks every item and places it in a single product line. Whilst fraud is by no means impossible on a blockchain, it does reduce the risks. For example, if a hacker does by a small chance break into the blockchain, it will be impossible for them to resell an item without the blockchain network noticing. It’s safe to say we’ve worked in the FinTech Industry for some while now. Whilst there is a buzz around blockchain, it’s important to note that the industry is well-established, and these surprising cases of blockchain display the broad and exciting nature of the industry as a whole. There are still other advantages to blockchain that we haven’t delved into in this article, such as decentralised apps, banking and the energy market. Perhaps this requires a follow up! The most important element of a blockchain is its transparency, from displaying votes during an election and protecting an expensive painting. Blockchain is also secure, and can aid in new lines of businesses. If you or your business are working on an unusual blockchain case, let us know – we would love to hear about it! Also if you are looking for reliable FinTech or blockchain experts, give us a shout, we offer many services to fix issues of scale !", "date": "2019-03-11"},
{"website": "Erlang-Solutions", "title": "The Future for Erlang Solutions", "author": ["Francesco Cesarini"], "link": "https://www.erlang-solutions.com/blog/the-future-for-erlang-solutions/", "abstract": "Refining our path. Erlang Solutions have recently turned 20, but growing up does not mean getting old. It means being able to adapt to change whilst remaining true to who we are. Looking forward to the next twenty years, the time has come to bring Erlang Solutions to the next level. We listened to our internal teams, community influencers, customers and representatives from the industries we want to build stronger relationships with to help us understand how our brand can serve others better and become more purposeful. With Erlang having grown up from being a programming language to an ecosystem of languages, tools and deployment capabilities, our delivery capabilities have also evolved. This has resulted in a brand refresh which will allow us to bring not only Erlang Solutions but also Erlang, Elixir and other BEAM languages to new audiences whilst helping our existing users make the transition to containerisation, orchestration and other cloud native technologies. What does this mean for you? The core of Erlang Solutions has since its inception been around our capabilities, consultancy and community. Paired together with values around teamwork & knowledge sharing, passion & fun and sustainability makes who we are. That is not going to change. On the contrary, our brand refresh is about committing more of our focus and energy into doing the things we have done best for the past 20 years. What will change is our ability to reach out to new markets in need of scalable and resilient solutions, combining our knowledge of the BEAM with other and new tools and technologies that we are keen to learn and adopt. It also creates a renewed commitment in continuing to grow our community. We stay humble . A brand refresh means a new logo, and as much as we are excited to reveal it today we are also humbled to remember our heritage. Our much loved Erlang Solutions squiggly logo and its Elixir twin have over the years been adopted by the community. Originally derived from the 1942 LM Ericsson logo, it served us well over the past 20 years and we hope it will continue serving the community now.  So don’t be surprised if you see our new Erlang Solutions logo in presence of both the Erlang and Elixir logos at community related activities and events, or on people’s laptops and hoodies. It all stays in the community. Join us in our journey, helping amplify the impact our amazing team has made throughout the years. We opted for evolution instead of revolution. Evolution not only of a brand, but of a programming language, an ecosystem, a community and a customer base. This is to you, us and the next twenty years, Francesco and Erlang Solutions Team", "date": "2021-02-3rd"},
{"website": "Erlang-Solutions", "title": "Take Control Of Your RabbitMQ Queues", "author": ["Ayanda Dube"], "link": "https://www.erlang-solutions.com/blog/take-control-of-your-rabbitmq-queues/", "abstract": "Erlang Solutions offers world-leading RabbitMQ consultancy, support & tuning solutions. Learn more > You’re a support engineer and your organisation uses a 3-node RabbitMQ cluster to manage its inter-application interactions across your network. On a regular basis, different departments in your organisation approach you with requests to integrate their new microservices into the network, for communication with other microservices via RabbitMQ. With the organisation being so huge, and offices spread across the globe, onus is on each department’s application developers to handle the integration to the RabbitMQ cluster, only after you’ve approved and given them the green light to do so. Along with approving integration requests, you also provide general conventions which you’ve adopted from prior experience. Part of the conventions you enforce is that the connecting microservices must create their own dedicated queues on integration to the cluster, as the best approach to isolating services and easily managing them. Unless of course, the microservices would be seeking to only consume messages from already existing queues. So, average message rate across your cluster is almost stable at 1k/s , both from internal traffic, and external traffic which is being generated by some mobile apps publicised by the organisation. Everything is smooth sailing, till you get to a point where you realise that the total number of queues in your cluster is nearing the order of thousands, and one of the three servers seems to be over burdened, using more system resources than rest. Memory utilisation on that server starts reaching alarming thresholds. At this point, you realise that things can only get worse, yet you still have more pending requests for integration of more microservices onto the cluster, but can’t approve them without figuring out how to solve the growing imbalance in system resources across your deployment. Fig 1. RabbitMQ cluster imbalance illustration After digging up on some RabbitMQ documentation , you come to light with the fact that since you’re using HA queues, which you’ve adopted to enforce availability of service, all your message operations only reference your master queues. Microservices have been creating queues on certain nodes at will, implying that the provisioning of queues has been random and unstructured across the cluster. Concentration of HA queue masters on one node significantly surpass that on the other nodes, and as a result, with all message consumptions referencing master queues only, the server with the most queue masters is feeling the operational burden in comparison to the rest. Your load balancer hasn’t been of much help, since whenever you experience a network partition, or purposefully make one of your nodes unavailable for maintenance work, queue provisioning has proceeded uncontrolled on the remaining running nodes. This retains the queue count imbalance upon cluster restoration. A possible immediate solution would be to purge some of the queues, to relieve the memory footprint on the burdened server(s), but you can’t afford to do this as most queued up messages are crucial to all business operations transacting through the cluster. New and existing microservices also can’t continue timelessly creating and adding more queues into the cluster until this problem has been addressed. So what do you do? Well, as of version 3.6.0 , RabbitMQ has introduced a mechanism to grant its users more control in determining a queue master’s location in a cluster, on creation. This is based on some predefined rules and strategies, configured prior to the queue declaration operations. If you can relate with the situation above, or would like to plan ahead and make necessary amendments to your RabbitMQ installation before encountering similar problems, then read on, and give this feature a go. So how does it work? Prior to introducing the queue master location mechanism, declaration of queues, by default, had been characterized by the queue master being located on the local node on which the declare operation was being executed on. This is somewhat very limiting, and has been the main reason behind the inefficient imbalance of system resources on a RabbitMQ cluster when the number of queues become significantly large. Upon introducing this mechanism, the node on which the queue master will be located is now first computed from a configurable strategy, prior to the queue being created. Configurable strategy is key here, as it leverages full control to RabbitMQ users to dictate the distribution of queue masters across their cluster. There are three means by which a queue master location strategy may be configured; Queue declare arguments : This is at AMQP level, where the queue master location strategy is defined as part of the queue’s declaration arguments Policy : Here the strategy is defined as a RabbitMQ policy. Configuration file : Location strategy is defined in the rabbitmq.config file. Once set, the internal execution order of declaring a queue would be as follows; Fig 2. Queue master location execution flow These are the three ways in which a queue master location strategy may be configured, and how the execution flow is ordered upon queue declaration. Next, you may be asking yourself the following question; What are these strategies anyway? Queue master location strategies are basically the rules which govern the selection of the node on which the queue master will reside, on declaration. If you’re from an Erlang background, you’d understand when I say these strategies are nothing but callback modules of a certain behaviour pattern in RabbitMQ known as the rabbit_queue_master_locator . If you aren’t from an Erlang background, no worries, all you need to know is what strategies are available to you, and how to make use of them. Currently, there are three queue master location strategies available; Min-Masters : Selects the master node as the one with the least running master queues. Configured as min-masters . Client-local : Like previous default node selection policy, this strategy selects the queue master node as the local node on which the queue is being declared. Configured as client-local . Random : Selects the queue master node based on random selection. Configured as random . So in a nutshell, this is the general theory behind controlling and dictating the location of a queue master’s node. Syntax rules differ for each case, depending on whether the strategy is defined as part of the queue’s declare arguments, as a policy, or as part of the rabbitmq.config file. NOTE : When both, a queue master location strategy and HA nodes policy have been configured, a conflict could arise in the resulting queue master node. For instance, if one of the slave nodes defined by the HA nodes policy becomes the queue master node computed by the location strategy. In such a scenario, the HA nodes policy would always take precedence over the queue master location strategy . With this knowledge at hand, the engineer in the situation mentioned above would simply enforce the use of the min-masters queue location strategy as part of the queue declaration arguments for all microservices connecting to the RabbitMQ cluster. Or even easier, he’d simply set the min-masters policy on the cluster nodes, using the match-all wildcard for the queue name match pattern. This would ensure that all newly created queues would be automatically distributed across the cluster until there’s a balance in the number of queue masters per node, and ultimately, a balance in the utilization of system resources across all three servers. Going forward At the moment, only three location strategies have been implemented, namely; min-masters , client-local and random . More strategies are yet to be brewed up, and if you feel you’d like to contribute a rule by which the distribution of queues can be carried out to better improve the performance of a RabbitMQ cluster, please feel free to drop a comment. These will go through some rounds of review, and could possibly be implemented and included in near future releases of RabbitMQ . Quick n’ easy experiment I’ll illustrate how the queue master location strategy is put into effect with a simple experiment to carry out on your local machine. We’re going make things easy by making most of the management UI, to avoid the whole AMQP setup procedures like opening of connections and channels, creating exchanges, and so forth. Download and install a RabbitMQ package specific for your platform. If you’re on a UNIX based OS, you can just quickly download and extract the generic unix package , and navigate to the sbin directory. tar xvf rabbitmq-server-generic-unix-3.6.1.tar.xz cd rabbitmq-server-generic-unix-3.6.1/sbin Create a 3-node cluster by executing the following commands; export RABBITMQ_NODE_PORT=5672 && export RABBITMQ_NODENAME=rabbit && ./rabbitmq-server -detached export RABBITMQ_NODE_PORT=5673 && export RABBITMQ_NODENAME=rabbit_1 && ./rabbitmq-server -detached export RABBITMQ_NODE_PORT=5674 && export RABBITMQ_NODENAME=rabbit_2 && ./rabbitmq-server -detached ./rabbitmqctl -n rabbit_1@hostname stop_app ./rabbitmqctl -n rabbit_1@hostname join_cluster rabbit@hostname ./rabbitmqctl -n rabbit_1@hostname start_app ./rabbitmqctl -n rabbit_2@hostname stop_app ./rabbitmqctl -n rabbit_2@hostname join_cluster rabbit@hostname ./rabbitmqctl -n rabbit_2@hostname start_app Enable the rabbitmq_management plugin; ./rabbitmq-plugins enable rabbitmq_management and access the management UI from http://localhost:15672 . Verify that your 3-node cluster was successfully created by checking cluster status; ./rabbitmqctl cluster_status Or, from the management UI Overview page. Fig 3. RabbitMQ cluster nodes Next, navigate to the Queues tab on your management UI, and create 3 qeueues on node rabbit_2@hostname . I prefixed my queues with the node name, i.e. rabbit_2.queue.1 , and created them as follows Fig 4. Add rabbit_2@hostname queues Repeat this procedure for queues rabbit_2.queue.2 and rabbit_2.queue.3 . Repeat step 5 on nodes rabbit_1@hostname and rabbit@hostname , creating 5 and 9 qeueues on each, respectively. If you’ve carried out steps 5 & 6 correctly, your queue listing should be similar to the following; Fig 5. Created queues Or, from the command line, by executing the following; ./rabbitmqctl list_queues -q name pid state resulting in a queue listing alike the following; Ayandas-MacBook-Pro:sbin ayandadube$ ./rabbitmqctl list_queues -q name pid state rabbit_2.queue.3 <rabbit_2@Ayandas-MacBook-Pro.3.1527.1> running rabbit.queue.3 <rabbit@Ayandas-MacBook-Pro.2.4772.0> running rabbit.queue.2 <rabbit@Ayandas-MacBook-Pro.2.4761.0> running rabbit.queue.4 <rabbit@Ayandas-MacBook-Pro.2.4783.0> running rabbit.queue.6 <rabbit@Ayandas-MacBook-Pro.2.4815.0> running rabbit.queue.7 <rabbit@Ayandas-MacBook-Pro.2.4827.0> running rabbit_1.queue.2 <rabbit_1@Ayandas-MacBook-Pro.2.1433.0> running rabbit_1.queue.4 <rabbit_1@Ayandas-MacBook-Pro.2.1455.0> running rabbit.queue.1 <rabbit@Ayandas-MacBook-Pro.2.4751.0> running rabbit_1.queue.1 <rabbit_1@Ayandas-MacBook-Pro.2.1416.0> running rabbit_2.queue.1 <rabbit_2@Ayandas-MacBook-Pro.3.660.1> running rabbit.queue.9 <rabbit@Ayandas-MacBook-Pro.2.4848.0> running rabbit_1.queue.3 <rabbit_1@Ayandas-MacBook-Pro.2.1444.0> running rabbit_2.queue.2 <rabbit_2@Ayandas-MacBook-Pro.3.896.1> running rabbit.queue.8 <rabbit@Ayandas-MacBook-Pro.2.4838.0> running rabbit_1.queue.5 <rabbit_1@Ayandas-MacBook-Pro.2.1476.0> running rabbit.queue.5 <rabbit@Ayandas-MacBook-Pro.2.4794.0> running NOTE: The pid ’s prefix is a good indicator of each queue’s home node. Next, we’re going to create a queue with the min-masters strategy configured, and verify that it’s been created on the correct and expected node. We’ll call our queue MinMasterQueue.1 .Firstly, lets configure the min-masters queue master location policy. We’ll name this policy qml-policy , and set it to be applied to all queues, of names prefixed with MinMasterQueue ; ./rabbitmqctl set_policy qml-policy \"^MinMasterQueue\\.\" '{\"queue-master-locator\":\"min-masters\"}' --apply-to queues With our policy is configured, we can now go on and create our MinMasterQueue.1 queue.So to prove that the min-masters policy does work, we’ll create our MinMasterQueue.1 queue on the node with the most number of queues, i.e. rabbit@hostname (9 queues). With the queue master location policy in effect, the set rabbit@hostname node should be overriden by the node computed by the policy, which has the least number of queues, i.e. rabbit_2@hostname . Let’s proceed! So as mentioned in step 8 , let’s create MinMasterQueue.1 on the node with the most queues, rabbit@hostname , as follows; Fig 6. MinMasterQueue creation Now the moment of truth; let’s verify if the queue was created on the correct node; Fig 7. Min-master queue Or, from the command line by executing the following; ./rabbitmqctl list_queues -q name pid which should yield the following as part of the full list of displayed queues; MinMasterQueue.1 <rabbit_2@Ayandas-MacBook-Pro.3.1995.3> The results are indeed correct. The home node of MinMasterQueue.1 is rightly the one which had the least number queue masters, i.e. rabbit_2@hostname . You can repeatedly execute step 9 , creating more MinMasterQueue.N queues to see this queue master location strategy in effect. The home node of the queues created will interchange from one node to another, depending on the queue masters’ count per node, at each given moment of execution. Fig 8. Min-master queues This is a quick illustration of this mechanism at work. In addition to setting a policy from the command line, as in step 8 , there also other means of defining the queue master location strategy which I illustrate in the next section. Examples Following are some examples of how to configure queue master location strategies. 1. rabbitmq.config Firstly, to set the location strategy from the rabbitmq.config file, simply add the following configuration entry; {rabbit,[ .\n          .\n          {queue_master_locator, <<\"min-masters\">>},\n          .\n          . ]}, NOTE : The strategy is configured as an Erlang binary data type i.e. <<“min-masters”>> . 2. Policy As already seen from our experiment, setting the strategy as a policy on a UNIX environment can be carried out as follows; rabbitmqctl set_policy qml-policy \".*\" '{\"queue-master-locator\":\"min-masters\"}' --apply-to queues This creates a min-masters queue location strategy policy, of name qml-policy , which, from the \".*\" wildcard match pattern, will be applied to all queues created on the node/cluster. You can find more information on defining policies from the official RabbitMQ documentation . 3. Declare arguments I illustrate setting the queue location strategy from declare arguments using three examples; in Erlang , Java and Python . In Erlang , you’d simply specify the location strategy as part of the ‘queue.declare’ record as follows; Args = [{<<\"x-queue-master-locator\">>, <<\"min-masters\">>}],\n  QueueDeclare = #'queue.declare'{queue      = <<\"microservice.queue.1\">>,\n                                  auto_delete= true,\n                                  durable    = false,\n                                  arguments  = Args },\n#'queue.declare_ok'{} = amqp_channel:call(Channel, QueueDeclare), In Java , just create an arguments map, define the queue master location strategy and declare the queue as follows; Map args = new HashMap(); \nargs.put(\"x-queue-master-locator\", \"min-masters\"); \nchannel.queueDeclare(\"microservice.queue.1\", false, false, false, args); Similarly, in Python , using Pika AMQP library, you’d carry out something similar to the following; queue_name = 'microservice.queue.1'\nargs = {\"x-queue-master-locator\": \"min-masters\"}\nchannel.queue_declare(queue = queue_name, durable = True, arguments = args ) You can find complete versions of these snippets here . These are simplified primers which you can build upon. If you have a requirement to implement something more complex, and need some assistance, please don’t hesitate to get in touch ! Erlang Solutions is the world leader in RabbitMQ consultancy, development, and support. We can help you design, set up, operate and optimise a system with RabbitMQ. Got a system with more than the typical requirements? We also offer RabbitMQ c ustomisation and bespoke support.", "date": "2016-03-23rd"},
{"website": "Erlang-Solutions", "title": "Erlang & Elixir DevOps From The Trenches – Why we felt the need to formalize operational experience with the BEAM virtual machine", "author": ["Francesco Cesarini"], "link": "https://www.erlang-solutions.com/blog/erlang-elixir-devops-from-the-trenches-why-we-felt-the-need-to-formalize-operational-experience-with-the-beam-virtual-machine/", "abstract": "Let’s backtrack to the late 90s, when I was working on the AXD301 switch, one of Erlang’s early flagship products. A first line support engineer broke procedure and tried to fix a bug in a live system. They compiled the Erlang module, put it in the patches directory and loaded it in one of the nodes. The patch did not solve the problem, so they deleted the BEAM file but were unaware they had to load the old version again and purge the patched module. So the switch was still running the wrong version of the module. The issue was eventually escalated to third line support, where just to figure out the node was running a different version of the code than originally thought ended up taking a colleague of mine forty hours. All this time was wasted before they could start troubleshooting the issue itself. Year after year, we came across similar incidents, so I started asking myself how we could formalize these experiences in reusable code. Our aim was to ensure no one would ever have to spend 40 hours figuring out that a first line engineer had not followed procedure. At the same time, I wanted to make sure no one had to reinvent the wheel every time they started on a new project. This is how the idea for WombatOAM was born, a standalone Erlang node that acts as a generic operations and maintenance node (O&M for short) for Erlang clusters. Any system with requirements on high availability should follow a similar pattern and approach. Why not formalize it in reusable code? Much of what we do here at Erlang Solutions, is developing, supporting and maintaining systems. Every incident we ever experienced which could have been avoided by analyzing symptoms and taking action has been formalized. Take the AXD301 example. WombatOAM will generate a unique module identifier using the md5 digest of the source code in every beam file, omitting information such as compilation date and attributes which do not affect the execution. If two nodes running the same release have different md5 digests of the same module, we raise an alarm that alerts an operator. If a module is loaded or purged in the system, we log it. If something gets typed into the shell, we log it as well. So not only are we alerted that nodes running the same release have different versions of a module, we also have the audit trail which lead to that state for post mortem debugging purposes. Every system that never stops needs mechanisms for collecting and suppressing alarms , monitoring logs and generating and analyzing metrics . It is one or more subsystem that collects functionality used for monitoring, pre-emptive support, support automation and post-mortem debugging. Applications such as exometer , folsom , elarm , eper , lager and recon will help, but only so far. In the telecom world, this functionality is put in a standalone node to minimize the impact on live traffic, both in terms of throughput and downtime. If the O&M node crashes or is taken offline, the system will still switch calls. This is the operations and maintenance node approach we believe should be adopted by other verticals, as high availability is today relevant to most server side systems. Let’s look at some stories from behind the trenches, and see how a proper O&M system would have reduced downtime and saved $$ in the form of person months of troubleshooting efforts and reduced hardware requirements. Alarms I’ve rarely seen the concept of alarms being properly used outside of telecoms. In some cases, threshold based alarms are applied, but that is where it often stops. A threshold based alarm is when you gather a metric (such as memory consumption or requests per second) and raise an alarm if the node on which it is gathered on reaches a certain upper or lower bound. But the potential of alarms goes beyond monitoring thresholds in collected metrics. The concept is easy; if something that should not be happening is happening, an alarm is raised. When issues, maybe on their own accord, through automation (scripts triggered by the alarm) or human intervention revert back to normal, the alarm is cleared. Your database or network connectivity goes down? Raise an alarm and alert the operator as soon as the system detects it. Latency hits the limits of your SLA? Raise an alarm in your system the second it happens, not when the metrics are collected. Or a process message queue (among millions of processes) is growing faster than the process is able to consume the messages? Once again, raise the alarm. If the network or database link comes back up, latency becomes acceptable or the message queue is consumed, the active alarm is cleared. Processes with long message queues are usually a warning of issues about to happen. They are easy to monitor, but are you doing it? We had a node crashing and restarting over a three-month period at a customer site. Some refactoring meant they were not handling the EXIT signal from the ports we were using to parse the XML. Yaws recycled processes, so every process ended up having a few thousand EXIT messages from previous requests that had to be traversed before the new request could be handled. About once a day, the node ran out of memory and was restarted by hand. The mailboxes were cleared. Our customers complained that at times, the system was slow. We blamed it on them using Windows NT, as we were not measuring latency. We occasionally saw the availability drop from 100% to 99.999% as a result of the external probes running their request right during the crash or when the node was restarting. This was rarely caught, as external probes sent a request a minute that took half a second to process, whilst the node took 3 seconds to restart. So we blamed the glitch on operations messing with firewall configurations. With triple redundancy, it was only when operations happened to notice that one of the machines was running at 100% CPU that we got called in. Many requests going through the system, we thought, but that count was only 10 requests per second. Had we monitored the message queues, we would have picked this the issue immediately. Had we had notifications on nodes crashing, we would have picked the problem up after the event, and had we been monitoring memory usage, we would have seen the cause, leading us to look at the message queue metrics. Or what about checking for file sanity? There was a software upgrade that required changes of the Erlang application Environment variables. The DevOps team upgraded the sys.config file, copying and pasting from a word document, invisible control characters included. Thank you Microsoft! Months later, a power outage caused the nodes to be rebooted. But because of the corrupt sys.config file, the invisible control characters would not parse and the node would crash in the startup phase. Looking at the sys.config file did not make us any wiser, as the control characters were not visible. It took half a day to figure this one out. We now regularly check and parse all boot, app and config files (as well as any dependencies). If one of them got corrupted or changed manually and might prevent the node from restarting, we will raise an alarm. I once did an ets:tab2list/1 call in the shell of a live system, and used the data to solve the problem. A few months later, when a similar issue arose, I instructed a first line support engineer to do the same, forgetting that the shell stores the result of its calls. We coped with two copies of the ets table (which happened to store subscriber data for millions of users) but the third caused the system to run out of memory (oh, had we only been monitoring memory utilization!). Today, WombatOAM monitors the actual memory utilisation of the shell process and raises an alarm should it become unproportionately large. I could go on with war stories for every other alarm we have, describing how some of our customers, not having that visibility, caused unnecessary outages and call-outs. Every alarm we implemented in WombatOAM has a story behind it. Either an outage, a crash, a call from operations, or hours of time wasted looking for a needle in a haystack as we did not have the necessary visibility and did not know where to look. We have 20 alarms in place right now, including checking system limits (many of which are configurable, but you need to be aware you risk reaching them), sanity checks (such as corrupt files, clashing module versions, multiple versions) and the unusual shell history size alarm. I trust you get the point. Oh, and if you are using Nagios, Pagerduty, or other alarming and notification services, WombatOAM allows you to push alarms. Metrics Metrics are there to create visibility, help troubleshoot issues, prevent failure and for post mortem debugging. How many users out there with Erlang and Elixir in production are monitoring BEAM’s memory usage, and more specifically, how much memory is being allocated and used by processes, modules, the atom table, ets tables, the binary heap and the code server? How do you know a leak in the atom table is causing the node to run out of memory? Monitor its size. Or what if the cause is long message queues? You should see the used and allocated process memory increase, which leads you to the sum of all message queues. These metrics allow you to implement threshold-based alarms, alerting the DevOps team the node has utilized 75% and 90% of its memory. It also allows you to figure out where the memory went after the node crashed and was restarted. What about the time the distributed Erlang port hung and requests queued up, causing a net split effect despite all network tests being successful and not raising an alarm? The Distributed Erlang port busy counter would have quickly pointed us to the issue. It is incremented every time a process tries to send data using distributed Erlang, but the port is busy with another job. We hooked up WombatOAM and realised we were getting three million port busy notifications per day! Distributed Erlang was not built to carry the peak load of this particular system. Our customer migrated from distributed Erlang to gen_rpc, and everything worked smoothly ever after. We once spent three months soak testing a system that, contractually, had to run for 24 hours handling 15,000 operations per second sustained. Each operation consisted on average of four http requests, seven ets reads, three ets writes and about ten log entries to file, alongside all of the business logic. The system was running at an average of 40% CPU with about half a gig of memory left over on each node. After a random number of hours, nodes would crash without any warning, having run out of memory. None of the memory graphs we had showed any leaks. We were refreshing the data at 10-second intervals, showing about 400mb of memory available in the last poll right before the crash. We suspected memory leaks in the VM, looked for runaway non-tail recursive functions, reviewed all the code and ended up wasting a month before discovering that seconds prior to the crash, a very rapid increase of processes with unusually high memory spikes and high activity of garbage collection. With this visibility, provided by the system trace, we narrowed this down to a particular operation which, when run on its own, caused a little spike in the memory usage graph. But when many of these operations randomly happened at the same time, the result was a monster wave that caused the VM to run out of memory. This particular issue took up to 20 hours to reproduce. It kept two people busy for a month trying to figure out what happened. When we finally knew what was going on, it took two weeks to fix it. Wombat today increments counters for processes spending too long garbage collecting, processes with unusually high memory spikes, or NIFs or BIFs hogging the scheduler, blocking other processes from executing and affecting the soft real-time properties of the system. If you suspect there is an issue, you can enable notifications and log the calls causing the issues themselves. Already using folsom so you do not need WombatOAM I hear? Think again. Out of the box, folsom will give you a dozen metrics. Use folsom or exometer for your business metrics, measuring application specific values such as failed and successful requests, throughput and latency. And let Wombat gather over a hundred VM specific metrics, with a hundred more depending on which applications you are running in your stack. It will take anyone a few days (or hours if you are a hero programmer) to implement a few instrumentation functions to collect metrics from Cowboy, RabbitMQ, Riak Core or Mnesia. It will however take weeks (sometimes months) to figure out what metrics you actually need and optimize the calls to reduce CPU overheads, ensuring there is no impact on the throughput of the system you are monitoring. When looking for a needle in a haystack, after a crash, you never know what metric you are going to need until after the event. If we’ve missed it, be sure we’re going to add it to WombatOAM as soon as one of our customers points it out. So now, you are not only getting the benefit of our experience, but also that of all other customers. Notifications Notifications are log entries recording a state change. They help with troubleshooting and post mortem debugging. How many times did you have to investigate a fault not knowing if any configuration changes had been made? In the AXD301 example, we would have logged all modules loaded in the nodes, and any commands executed in the Erlang Shell. In our first release of WombatOAM, we used to store information on all system notifications, including the distributed Erlang Port Busy notifications and processes with unusually high memory spikes and log garbage collection pauses. Until a customer started complaining that they were running out of disk space. It was the very same customer who was getting three million port busy messages a day. System notifications are today turned off by default, but as soon as you detect you have a problem, you can turn them on and figure out which process has unusually high memory spikes or what function call is sending data to the distributed Erlang port. High volumes of logs, especially business logs, need to be monitored for size. As every system is different, you will have to configure and fine tune what is pushed. We once had (well, we still do, they are now running WombatOAM) a customer with a several hundred nodes in production, running over thirty different services. The only way for them to notice a node had been restarted was to log on to the server and look for the crash dump file. The only way to know if a process had crashed was to log on to the machine, connect to the shell and start the report browser and search for crash reports. Minor problem. This was an enterprise customer, and developers were not given access to the production machines. Wombat will automatically collect node crash notifications (and rename the crash dump file), as well as pull in error, warning and crash notifications. They are all in one place, so you can browse and search all node restarts and crash reports (and twenty other standard notifications, alongside application specific ones). We handle system logs out of the box, but if you are using logging applications such as the SASL Logger, Lager and the Elixir Logger, you can pull in your business logs and use them to prove your innocence or admit guilt from one location. And if that one location is not WombatOAM, WombatOAM can push them to LogStash, Splunk, DataDog, Graylog, Zabbix or any other log aggregation service you might be using. But it is only Erlang Code! Yes, someone who was trialling WombatOAM actually did say this to us, and suggested they implement everything themselves! You can do all of WombatOAM yourself, and add items after outages or as you get more experience of running beam clusters. Figure out what metrics, alarms and notifications to generate, and add them for SASL, Lager, Elixir Logger, Folsom, Exometer, Mnesia, OSmon, Cowboy, Phoenix, Poolboy, Riak KV, Riak core, Yokozuna, Riak Multi-Datacenter Replication, Ecto, RabbitMQ, MongooseIM (all this at the time of writing, more are being added each month). After all, as they said, it is just code. When done, you can spend your time testing and optimizing your instrumentation functions, chasing BIFs which cause lock contention and optimize your nodes so as to reduce memory utilization and CPU overhead. And when you are done with that, you can start working on the tooling side, adding the ability to check and update application environment variables. Run etop like tools or execute expressions from the browser. Or implement northbound interfaces towards other O&M tools and SAAS providers. Or implement the infrastructure to create your own plugins, optimize connectivity between WombatOAM and the managed nodes, optimize your code to handle hundreds of nodes in a single WombatOAM node, or put in place an architecture and test that it will scale to 20,000 nodes. Honestly? If you want to reinvent the wheel, don’t bother. Just get WombatOAM or come work for us instead! (Shameless plug: The WombatOAM team is recruiting). Wrapping Up Let’s wrap up with a story from our own teams who pretty much caught the essence of Wombat in an area I had not thought of originally. You know a tool is successful when others start using it in ways you had never thought of. Our original focus was operations and DevOps teams, but developers and testers have since used WombatOAM for visibility and quick turnaround of bugs. Our MongooseIM team had to stress test and optimize a chat server running on four nodes, with about 1.5 million simultaneously connected users on four machines. As soon as they started generating load, they got the long message queue alarm on one of the nodes followed by the ets table system limit alarm. Based on this, they figured out Mnesia is the bottleneck, and more specifically, the login operation which uses transactions (hence, the ETS tables). Investigating, they discovered from the graphs that one of the nodes carries twice the load of the other three. They look at the schema and see that tables are replicated on three of the four nodes. How did the above misconfiguration happen? They looked at the notifications and discovered a logged shell command where the table schema was created using nodes() instead of [node()|nodes()], missing out creating a table on the node they had run the operation on. They found and solved the problem in 30 minutes, only to have to jump on to the next one, which was an unusually high memory spike occurring during a particular operation. Followed by issues with the versioning of the MongoDB driver (Don’t ask!). Similar issues, when they did not have the same level of visibility, could have taken days to resolve. In Conclusion It has long been known that using Erlang, you achieve reliability and availability. But there seems to be some common myth that it all happens magically out of the box. You use Erlang, therefore your system will never fail. Sadly, there is no such thing as a free lunch. If you do not want to kiss your five nines goodbye, the hard work starts when designing your system. Monitoring, pre-emptive support, support automation and post mortem debugging are not things you can with easily just bolt on later. By monitoring , we mean having full visibility into what is going on. With pre-emptive support , we mean the ability to react to this visibility and prevent failure. Support automation allows you to react to external events, reducing service disruption resolving problems before they escalate. And post-mortem debugging is all about quickly and efficiently being able detect what caused the failure without having to stare at a screen hoping the anomaly or crash experienced in the middle of the night happens again whilst you are watching. Whilst five nines will not happen magically, Erlang takes care of a lot of accidental difficulties allowing you to achieve high levels of availability at a fraction of the effort compared to other programming languages. WombatOAM will take care of the rest. I think the above is the beginning of the end of a blog post, and if you are still reading, trying WombatOAM out is probably worth more than another demo or walk-through. You never know when you are going to need the information and visibility until it is too late, and unless you are troubleshooting issues, the more you see the better. If you want to read more about systems which never stop, I recommend chapter 16 in my book Designing for Scalability with Erlang/OTP , which covers monitoring and pre-emptive support. You can find it on Safari, BitTorrent or buy it on Amazon or the O’Reilly site. If you use the O’Reilly site, discount code authd gives you 50% off the digital copy and 40% off the printed one. And of course, there is also Erlang in Anger , by Fred Hebert.", "date": "2016-09-14"},
{"website": "Erlang-Solutions", "title": "RabbitMQ Monitoring WombatOAM and the RabbitMQ Management Plugin", "author": ["Ayanda Dube"], "link": "https://www.erlang-solutions.com/blog/rabbitmq-monitoring-wombatoam-and-the-rabbitmq-management-plugin/", "abstract": "1. Introduction If you’re a RabbitMQ user, then you must be accustomed to monitoring and keeping track of the status of your Rabbit installation by use of the native RabbitMQ Management plugin , or, alternatively, using third party monitoring tools such as Sensu , which internally make use of the RabbitMQ Management API for metrics acquisition, to present them on custom UIs. Regardless of the user’s preferred tool, a common aspect which cuts across most of these off-the-shelf tools is full dependency on the RabbitMQ Management plugin. In other words, for you to monitor and manage your RabbitMQ installation via a web interface, the RabbitMQ Management plugin has to be enabled at all times on the RabbitMQ nodes. The downside of this approach lies mainly on the overhead the RabbitMQ Management plugin introduces per each node it is enabled on. The following image depicts the accompanying and required applications introduced on a node when the RabbitMQ Management plugin is enabled; A total of 13 additional applications are required by the RabbitMQ Management Plugin, which aren’t related to, or required to run any of the AMQP operations. Internally, the RabbitMQ Management Plugin creates multiple Erlang ETS tables, which are RAM based, in order to store, aggregate and compute statistics and various RabbitMQ specific node metrics. Unless the hardware has been dimensioned to take this into account, it can place a huge demand on the node’s memory, and could potentially contribute to a number of unknown side effects when traffic load patterns vary from peak to peak. Ironically, a common recommendation in the RabbitMQ community for troubleshooting is to disable and re-enable the RabbitMQ Management Plugin! In an ideal world, RabbitMQ nodes should be dedicated to delivery of the AMQP protocol (queueing and message interchange logic between connected clients). All potential burdensome operations like UI monitoring and management should ideally be taken care of on a completely independent node; deployable on a separate physical machine from the RabbitMQ nodes, for all OAM functions. This is how telecoms systems have addressed monitoring and operations for decades. This inflexibility of the RabbitMQ Management plugin and other monitoring tools dependant on its API brings to light the main strengths and advantages of using our tool WombatOAM [4]. Illustrated below, is the application overhead WombatOAM introduces on a RabbitMQ node; So what is different about the WombatOAM approach of monitoring RabbitMQ? Firstly, only 1 additional application is introduced, which is the wombat_plugin , unlike the 13 additional applications which are introduced by the RabbitMQ Management plugin. The reason behind this is the fact that WombatOAM only requires a single and very lightweight application, which is the wombat_plugin , on the node it’s monitoring, to relay all metrics and events to a separate and independent node, responsible for carrying out all heavy computations and UI related operations. This implies a much less, if not negligible, application overhead on the RabbitMQ node in comparison to that introduced by RabbitMQ Management plugin, which carries out all of its computations and operations on the RabbitMQ node, itself. WombatOAM thus fully leverages distributed Erlang by carrying out its major operations and maintenance functions in a loosely coupled manner, on an independent and separate node. This grants much more freedom to the RabbitMQ nodes to predominantly be focused on carrying out AMQP functions, only , with very little or no chance at all, of experiencing any problems relating to UI operations and maintenance functions. NOTE : The RabbitMQ Management Plugin does attempt to reduce the amount of overhead when used in a cluster setup. Not all nodes in a cluster need the full RabbitMQ Management plugin enabled, but just one. The rest of the nodes need only the RabbitMQ Management Agent plugin enabled, which is a single lightweight application with no additional dependencies. This implies that any problems arising from the heavy resource footprint of the RabbitMQ Management Plugin are likely to be only experienced on 1 out of N cluster nodes. The following results were captured from a benchmark test executed against a RabbitMQ node; first as a standalone node, next, being monitored by WombatOAM, and finally with the RabbitMQ Management plugin enabled (without WombatOAM). This test was carried out on a 2GHz Intel i7 8 core MacBook Pro, with 16G memory. Standalone Node WombatOAM enabled RabbitMQ Management plugin enabled Applications Count 9 10 22 Erlang Process Count 257 287 344 Memory (MB) 145.2 152 186.8 Message Rate (msgs/sec) 18692 18518 18321 It’s clear from the captured results, that the RabbitMQ node experiences less strain, and thus performs much better (higher message rate, for example), when being monitored by WombatOAM, as compared to monitoring using the RabbitMQ Management plugin. Even without the application of traffic, the total process count for example on the RabbitMQ node goes up from 144 to 232 when using the RabbitMQ Management Plugin, as compared to only 174 when using WombatOAM, which is much less overhead. Latency was also captured to be averaging at approximately 237920 microseconds using WombatOAM, as compared to an average of 254705 microseconds with the RabbitMQ Management Plugin enabled. These improvements in message rate and latency become more relevant on an aggregate scale, on cluster wide setups, playing a crucial role on the overall system’s performance. In addition, this same test may also be carried out with WombatOAM being used to monitor a node with the RabbitMQ Management Plugin already enabled, in order to gather and analyse hundreds more related metrics, alarms and notifications from the virtual machine that RabbitMQ runs on. These metrics, such as process memory, atom memory, Mnesia and TCP specific metrics, and much more, are are not available from the RabbitMQ Management Plugin. NOTE : These tests could also yield varied results depending on the hardware platform on which the test RabbitMQ nodes are installed on. 2. Installation You can ask for a free 45 day product trial of the WombatOAM package by contacting us at general@erlang-solutions.com or by filling in the ‘Request a Demo’ form on the product page .  The package will be accompanied with the WombatOAM documentation, and easy to follow installation procedures to get it up and running in a very short space of time. 3. Features Next up, I’ll discuss following WombatOAM features, in the context of RabbitMQ; Metrics Notifications, Events & Alarms Configuration Control Explore WombatOAM has a vast number of other features and capabilities, explained and illustrated in greater detail in the WombatOAM documentation. 3.1. Metrics One of the primary purposes of the WombatOAM RabbitMQ plugin is metrics acquisition and processing from the RabbitMQ nodes it’s connected to, and presenting these metrics on the WombatOAM UI. WombatOAM provides a number of RabbitMQ specific metrics such as publish rate, deliver rate, messages ready, messages unacknowledged, connection related metrics, and so forth. A total of 49 RabbitMQ specific metrics are currently provided by WombatOAM, and additional metrics can easily be developed if required by WombatOAM users. Live metrics Collected metrics Gauge metrics In addition, WombatOAM also provides an extremely useful feature of defining upper and lower bound metric thresholds, from which alarms will be raised when these metric limits are exceeded. This means support engineers are never caught by surprise when certain event limits are reached. 3.2. Notifications, Events & Alarms RabbitMQ users have to manually dig up the log files, copy them across from remote servers, analyse them line by line, in order to get an indication of the events and alarms being raised in their nodes. This can be a tedious and time consuming exercise to carry out, especially when problem resolution is required in a very short space of time on your production environment. However, with WombatOAM installed, events and alarm notifications are presented in a very user friendly manner, on a dashboard, under the Notifications tab, with minimal effort from the user, as compared to manually locating and reading up log files. WombatOAM alarm and event notifications go on to provide even more information than what the native RabbitMQ logs would expose, for example code version discrepancies. Without WombatOAM, many problems, only detectable by using WombatOAM, would only be discovered during post-problem diagnosis, when the RabbitMQ nodes have already suffered from the side effects, and in some cases, may even have gone down. This is the definition of preemptive support, which is a shortcoming of many monitoring systems, except WombatOAM, which excels in this area. The last chapter, 16, of the text, Designing for Scalability with Erlang OTP explores preemptive support in detail, and why it’s extremely crucial in the operations and maintenance activities of any system, (whether or not Erlang based), such as RabbitMQ. Quoting part of the text: Preemptive support automation gathers data in the form of metrics, events, alarms, and logs for a particular application; analyzes the data; and uses the results to predict service disruptions before they occur. An example is noticing an increase in memory usage, which predicts that the system might run out of memory in the near future unless appropriate corrective actions are taken. 3.3. Configuration Another powerful feature which WombatOAM provides is configuration management from the user interface. RabbitMQ is heavily configurable from the rabbitmq.config file, and from the application resource files of all its associated plugins. WombatOAM gives full visibility of all application environment variables, which, in RabbitMQ are all the configuration parameters defined in rabbitmq.config file, as illustrated below; Select the rabbit application and start request; Configuration parameters are displayed WombatOAM also goes the extra mile, to allow users to make in-memory configuration changes during system runtime. This means that with WombatOAM installed, if you’re, for example, using the RabbitMQ queue master location policies[5], you could update the location policy via the WombatOAM as illustrated below. This will be in effect for the queue master location configuration, in memory. NOTE : It must be clarified that not all configuration changes will be applied. Some settings are cached, for example, channels retain channel_operation_timeout configuration in their cache. So the new value will not be applied to already created channels, only to new channels created thereafter. In addition, WombatOAM’s configuration management feature allows for global, cluster wide configuration settings. This is a very useful feature which helps avoid inconsistencies between configuration changes carried out on a single node, as compared to configuration settings on the other cluster nodes. Global configuration changes 3.4. Control WombatOAM grants a high degree of control over the RabbitMQ nodes it’s monitoring by providing an interface to execute Erlang native functions. This is found under the Services tab of the WomabatOAM UI. This means that AMQP equivalent operations such as queue and exchange declarations, or, native RabbitMQ operations like mirror policy definitions, and control operations (like listing of queues, exchanges, connections, and so forth) may be executed from the WombatOAM UI. NOTE : This feature will, in the near future, be replaced by a more user friendly and safer means of executing control commands. This should also help avoid the continuous and repeated re-entry of such commands, if executed in a frequent manner. Below is an example of a control function being executed to acquire and display all exchanges Result being displayed WombatOAM also logs these executed control commands as part of its Notifications , making it possible to carry out minor and/or major audit trail operations, to investigate which commands were executed, and when. This is very useful when, for example, troubleshooting a problem’s cause resulting from human intervention on the RabbitMQ nodes. Other useful RabbitMQ control functions you could execute from the WombatOAM interface are summarized as follows (you can copy, modify and use these as you like): Command Function Declare a queue rabbit_amqqueue:declare(rabbit_misc:r(<<“/”>>, queue, _Queue = <<“test.queue” >>), false, false, [], none). Declare 20 queue begin Queues = 20, L=[{ , } = rabbit_amqqueue:declare(rabbit_misc:r(<<“/”>>, queue, list_to_binary(“test.queue.”++integer_to_list(N))), false, false, [], none) List queues rabbit_amqqueue:list(). Declare exchange rabbit_exchange:declare({resource, _VHost = <<“/”>>, exchange, _XName = <<“test.exchange”>>}, _Type = topic, _Durable = true, _AutoDelete = false, _Internal = false, _Args = []). List exchanges rabbit_exchange:list(). Set mirror policy rabbit_policy:set(_VHost = <<“/”>>, _PolicyName = <<“ha-2-policy”>>, _Pattern = <<“.*”>>, _Definition = [{<<“ha-mode”>>, <<“exactly”>>}, {<<“ha-params”>>, 2}], _Priority = 1, _ApplyTo = <<“queues”>>). Clear policy rabbit_policy:delete(_VHost = <<“/”>>, _PolicyName = <<“ha-2-policy”>>). Create vhost rabbit_vhost:add(_VHost = <<“test.vhost”>>). Authenticate user rabbit_access_control:check_user_pass_login( <<“username”>>, <<“password”>> ). Purge queue rabbit_control_main:purge_queue({resource, _VHost = <<“/”>>, queue, _QName = <<“test.queue”>>}). RabbitMQ Status rabbit:status(). Additionally, WombatOAM also provides control operations which allow users to; Forcefully initiate garbage collection Kill specific processes, using the process identifier (PID) or the registered name as reference Soft purge modules on managed nodes, ensuring no old module versions are held in memory, unless being used by a process. The image below illustrates these control categories; 3.5. Explore WombatOAM has a powerful feature, extremely useful for RabbitMQ support and development engineers, which allows them to explore node processes and ETS tables, (including the rabbit_queue table, which consists of all alive queues on the nodes). The image below depicts the categories which may be explored; Different Explore categories For example, inspecting the process state of an essential process like the vm_memory_monitor is only a matter of navigating to the Explore tab -> Process state category, specifying the process registered name, in this case the vm_memory_monitor , or PID (Process Identifier), and executing the request. Specifying vm_memory_monitor registered name, and executing request Process state of vm_memory_monitor displayed 4. When to use WombatOAM A question often raised regarding WombatOAM and the RabbitMQ Management plugin is; “….which is the right tool to use, and on which occasions?” Both these tools may be viewed as complementary to each other. Despite WombatOAM’s superior capabilities to provide more than just RabbitMQ metrics, but also a vast number of events, alarms, Erlang VM node specifics, and so forth, the RabbitMQ Management plugin does also provide useful features, like, for example, manual publishing/receiving of messages from the UI, plugin specific operations like creation of links and upstreams for the Federation plugin, and so forth. Hence, bearing this in mind, recommendation and best practice is to use both tools in the following manner; WombatOAM , at ALL times, to monitor and keep track of the wellness of the RabbitMQ installation. WombatOAM will introduce very minimal overhead on the RabbitMQ node(s) which it is monitoring, thus chances of end-to-end service interruptions from resource hungry UI operations would be as good as neglible. RabbitMQ Management Plugin , to be enabled only when certain functions and operations are required. For example, specific plugin operations (e.g. Federation plugin configuration), or easy definition of mirroring policies. All these are once off operations, for which the RabbitMQ Management Plugin need not be enabled at all times for. With WombatOAM continually enabled, and the RabbitMQ Management plugin enabled on specific occasions only, when required, common RabbitMQ problems such as nodes raising VM High Watermark memory alarms due to excessive memory usage beyond that permissible, would hardly be ever experienced, since these are attributed by UI operations of a high memory footprint. Unless of course, the root cause is something else not directly related to, nor attributed by, memory hungry UI computations and operations, and also, the basis is not on any hardware limitations on which RabbitMQ is installed on. This is just one major example, amongst an abitrary number of many other problems, which would be alleviated by making use of WombatOAM in this manner. 5. Beyond RabbitMQ Beyond the specific metrics provided by the RabbitMQ plugin, WombatOAM also by default, provides Erlang VM related metrics. This means that WombatOAM gathers more metrics and node specific information from the RabbitMQ nodes like no other monitoring tool available. Additional metrics gathered from the RabbitMQ node are illustrated in the image below; Each of these metric categories are both rich in the information they provide and crucial in monitoring any RabbitMQ installation, and in essence, any Erlang based system. For example, Memory and Mnesia System metrics would be critical for any RabbitMQ installation, as all metadata (queue, exchanges, bindings, and so forth) is stored in Mnesia, along with the fact that RabbitMQ nodes are classified as either DISC or RAM nodes, based on type of Mnesia tables they’re configured to use. Same applies to all other metrics WombatOAM provides; they all play a crucial role for any RabbitMQ (and Erlang system) installation. Adding on, I/O metrics are also provided, which can also reveal vital information regarding the number of permissible client connections when specifications such as scalability come into consideration. And off course, Error Log Notifications can indicate the rate at which errors/exceptions and warnings are being reported and logged to the SASL logs and general logs, respectively, by the RabbitMQ nodes. The magnitudes of these metrics in particular, can be an easy and direct indicator that the node is experiencing problems which have, or have not yet been to be detected by the user. 6. Conclusion Rounding up the discussion, its clear that WombatOAM proves an essential and efficient tool for monitoring any RabbitMQ installation. As already pointed out in Section 4, recommendation is to maintain an instance of WombatOAM continually, monitoring your RabbitMQ installation, and have the RabbitMQ Management Plugin on the other hand, due its resource hungry nature, only enabled for certain periods of time, when required for some of its specific control features, to use once off, and disabling it thereafter. This guarantees that during the majority periods of your RabbitMQ installation’s uptime, your nodes are dedicated to AMQP operations only , and avoid servere UI related operational overheads induced by the RabbitMQ Management Plugin. Per 24hour monitoring cycle, you could for example, employ the RabbitMQ Management plugin for no more than 2 hours only, when required, with WombatOAM monitoring continually throughout the entire cycle. If you would like to receive a copy of this post in the form of a PDF Whitepaper, fill in your details here . References [1] https://www.rabbitmq.com/configure.html [2] https://www.rabbitmq.com/management.html [3] Designing for Scalability with Erlang OTP [4] https://www.erlang-solutions.com/products/wombat-oam.html [5] https://www.erlang-solutions.com/blog/take-control-of-your-rabbitmq-queues.html Erlang Solutions is the world leader in RabbitMQ consultancy, development, and support. We can help you design, set up, operate and optimise a system with RabbitMQ. Got a system with more than the typical requirements? We also offer RabbitMQ customisation and bespoke support.", "date": "2016-10-04"},
{"website": "Erlang-Solutions", "title": "Erlang (and Elixir) distribution without epmd", "author": ["Magnus Henoch"], "link": "https://www.erlang-solutions.com/blog/erlang-and-elixir-distribution-without-epmd/", "abstract": "When you deploy a distributed Erlang application, you’ll most likely have to answer the question “which ports need to be open in the firewall?”. Unless configured otherwise, the answer is: port 4369 for epmd, the Erlang Port Mapper Daemon, and an unpredictable high-numbered port for the Erlang node itself. That answer usually doesn’t translate neatly into firewall rules. The usual solution to that is to use the environment variables inet_dist_listen_min and inet_dist_listen_max (described in the kernel documentation ) to limit the distribution ports to a small port range, or even a single port. But if we’ve limited the Erlang node to a single port, do we really need a port mapper? There are a few potential disadvantages with running epmd: We may want to run epmd under the same user as the Erlang node — but as we’ll see, it can be hard to guarantee that. We may want to configure epmd to listen only on a certain interface — again, that relies on us being the first to start epmd on this host. Anyone who can connect to epmd can list the local Erlang nodes, without any authentication. Connections to epmd are not encrypted, so even if you use distribution over TLS , epmd connections are not protected, and potentially vulnerable to man-in-the-middle attacks. While epmd is very stable, it can happen that it crashes. In that case, any distributed Erlang nodes will keep running but not reconnect to epmd if it comes up again. That means that existing distribution connections will work, but new connections to those nodes cannot be made. It’s possible to fix this situation without restarting the nodes; see Экстренная реанимация epmd (in Russian). So let’s explore how epmd works, and what we can do to run an Erlang cluster without it. How does epmd get started in the first place? Let’s have a look at the code! epmd is started in the function start_epmd in erlexec.c . In fact, epmd is started unconditionally every time a distributed node is started. If an epmd instance is already running, the new epmd will fail to listen on port 4369, and thus exits silently. In fact, that’s what will happen even if the existing epmd instance was started by another user. Any epmd instance is happy to serve Erlang nodes started by any user, so usually this doesn’t cause any problem. So who can connect to epmd? Anyone! By default, epmd listens on all available interfaces, and responds to queries about what nodes are present, and what ports they are listening on. Hearing this tends to make sysadmins slightly nervous. You can change that by manually starting epmd and specifying the -address option, or by setting the ERL_EPMD_ADDRESS environment variable before epmd gets started. This is described in the epmd documentation . That requires that the place where you do this is actually the first place where epmd gets started — otherwise, the existing epmd instance will keep running unperturbed. Why do we need a port mapper daemon? Clearly, epmd is just the middleman. Can we cut out the middleman? We could make every Erlang node listen on a well-known port — perhaps use the port reserved for epmd, 4369, if we’re going to get rid of epmd. But that means that we can only run one Erlang node on each host (of course, for some use cases that might be enough). So let’s specify some other port number. I mentioned inet_dist_listen_min and inet_dist_listen_max earlier. Those two variables define a port range, but if we set them to the same value, we narrow down the “range” to a single port: erl -sname foo \\\n    -kernel inet_dist_listen_min 4370 \\\n            inet_dist_listen_max 4370 That’s all well and good, but we’d also need a way to tell other nodes not to bother asking epmd about the port number, and just use this number instead. And if we have several nodes on the same host, we’d need some kind of configuration to specify the different port numbers for those nodes. Let’s use something else In Erlang/OTP 19.0, there are two new command line options: When you specify -start_epmd false , Erlang won’t try to start epmd when starting a distributed node. -epmd_module foo lets you specify a different module to use for node name registration and lookup, instead of the default erl_epmd . Those are the building blocks we need! I want to use a state-less scheme for this: since the connecting node already knows the name of the node it wants to connect to, I use that as the source of the port number. I pick a “base” port number — why not 4370, one port higher than epmd. Then I extract the number at the end of the “node” part of the node name, such that myapp3@foo.example.com becomes 3 . Then I add that number to the base port number. As a result, I know a priori that the node myapp3@foo.example.com is listening on port 4373. If there is no number in the node name, I treat that as a zero. This means that the nodes myapp3 and myotherapp3 couldn’t run on the same host, but I’m ready to live with that. (Thanks to Luca Favatella for perfecting this idea.) Let’s write a little module for that: -module(epmdless).\n\n-export([dist_port/1]).\n\n%% Return the port number to be used by a certain node.\ndist_port(Name) when is_atom(Name) ->\n    dist_port(atom_to_list(Name));\ndist_port(Name) when is_list(Name) ->\n    %% Figure out the base port.  If not specified using the\n    %% inet_dist_base_port kernel environment variable, default to\n    %% 4370, one above the epmd port.\n    BasePort = application:get_env(kernel, inet_dist_base_port, 4370),\n\n    %% Now, figure out our \"offset\" on top of the base port.  The\n    %% offset is the integer just to the left of the @ sign in our node\n    %% name.  If there is no such number, the offset is 0.\n    %%\n    %% Also handle the case when no hostname was specified.\n    NodeName = re:replace(Name, \"@.*$\", \"\"),\n    Offset =\n        case re:run(NodeName, \"[0-9]+$\", [{capture, first, list}]) of\n            nomatch ->\n                0;\n            {match, [OffsetAsString]} ->\n                list_to_integer(OffsetAsString)\n        end,\n\n    BasePort + Offset. And a module to use as the -epmd_module . One slight complication here is that 19.0 expects the module to export register_node/2 , while from 19.1 onwards it’s register_node/3 . Let’s include both functions to be sure: -module(epmdless_epmd_client).\n\n%% epmd_module callbacks\n-export([start_link/0,\n         register_node/2,\n         register_node/3,\n         port_please/2,\n         names/1]).\n\n%% The supervisor module erl_distribution tries to add us as a child\n%% process.  We don't need a child process, so return 'ignore'.\nstart_link() ->\n    ignore.\n\nregister_node(_Name, _Port) ->\n    %% This is where we would connect to epmd and tell it which port\n    %% we're listening on, but since we're epmd-less, we don't do that.\n\n    %% Need to return a \"creation\" number between 1 and 3.\n    Creation = rand:uniform(3),\n    {ok, Creation}.\n\n%% As of Erlang/OTP 19.1, register_node/3 is used instead of\n%% register_node/2, passing along the address family, 'inet_tcp' or\n%% 'inet6_tcp'.  This makes no difference for our purposes.\nregister_node(Name, Port, _Family) ->\n    register_node(Name, Port).\n\nport_please(Name, _IP) ->\n    Port = epmdless:dist_port(Name),\n    %% The distribution protocol version number has been 5 ever since\n    %% Erlang/OTP R6.\n    Version = 5,\n    {port, Port, Version}.\n\nnames(_Hostname) ->\n    %% Since we don't have epmd, we don't really know what other nodes\n    %% there are.\n    {error, address}. As you can see, most things are essentially stubbed out: start_link/0 is invoked as this module is added as a child of the erl_distribution supervisor. We don’t actually need to start a process here, so we just return ignore . The register_node function would normally connect to epmd and tell it what port number we use. In return, epmd would return a “creation” number. The “creation” number is an integer between 1 and 3. epmd keeps track of the creation number for each node name, and increments it whenever a node with a certain name reconnects. That means that it’s possible to distinguish e.g. pids from a previous “life” of a certain node.Since we don’t have epmd, we don’t have the benefit of it tracking the life span of the nodes. Let’s return a random number here, which has a 2 in 3 chance of being different from the previous “creation” number. port_please/2 gets the IP address of the remote host in order to connect to its epmd, but we don’t care; we use our algorithm to figure out the port number.We also need to return a distribution protocol version number. It has been 5 ever since Erlang/OTP R6 (see the Distribution Protocol documentation ), so that’s simple. Finally, names/1 is called to list the Erlang nodes on a certain host. We have no way of knowing that, so let’s pretend that we couldn’t connect. So far, so good — but we need a way to make sure that we’re listening on the right port. The best way I could think of is to write a new distribution protocol module, one that just sets the port number and then lets the real protocol module do its job: -module(epmdless_dist).\n\n-export([listen/1,\n         select/1,\n         accept/1,\n         accept_connection/5,\n         setup/5,\n         close/1,\n         childspecs/0]).\n\nlisten(Name) ->\n    %% Here we figure out what port we want to listen on.\n\n    Port = epmdless:dist_port(Name),\n\n    %% Set both \"min\" and \"max\" variables, to force the port number to\n    %% this one.\n    ok = application:set_env(kernel, inet_dist_listen_min, Port),\n    ok = application:set_env(kernel, inet_dist_listen_max, Port),\n\n    %% Finally run the real function!\n    inet_tcp_dist:listen(Name).\n\nselect(Node) ->\n    inet_tcp_dist:select(Node).\n\naccept(Listen) ->\n    inet_tcp_dist:accept(Listen).\n\naccept_connection(AcceptPid, Socket, MyNode, Allowed, SetupTime) ->\n    inet_tcp_dist:accept_connection(AcceptPid, Socket, MyNode, Allowed, SetupTime).\n\nsetup(Node, Type, MyNode, LongOrShortNames, SetupTime) ->\n    inet_tcp_dist:setup(Node, Type, MyNode, LongOrShortNames, SetupTime).\n\nclose(Listen) ->\n    inet_tcp_dist:close(Listen).\n\nchildspecs() ->\n    inet_tcp_dist:childspecs(). Mostly stubs here; it’s just the listen/1 function that sets the inet_dist_listen_min and inet_dist_listen_max variables according to our node name, before passing control to the real module, inet_tcp_dist . (Note that while inet_tcp_dist is the default module, it only provides unencrypted connections over IPv4. If you want to use IPv6, you would use inet6_tcp_dist , and if you want to use Erlang distribution over TLS , that would be inet_tls_dist or inet6_tls_dist . Adding that flexibility is left as an exercise for the reader.) And we’re ready! Now we can start two nodes, foo1 and foo2 , and have them connect to each other: erl -proto_dist epmdless -start_epmd false -epmd_module epmdless_epmd_client -sname foo1 erl -proto_dist epmdless -start_epmd false -epmd_module epmdless_epmd_client -sname foo2 System working? (foo2@poki-sona-sin)1> net_adm:ping('foo1@poki-sona-sin').\npong Seems to be! Once more, with Elixir! Of course, since Erlang and Elixir run on the same virtual machine, there is nothing stopping us from doing all of this in Elixir instead. In Elixir, we can put all the code in a single file, and the compiler will compile it into the different modules we require: # A module containing the function that determines the port number\n# based on a node name.\ndefmodule Epmdless do\n  def dist_port(name) when is_atom(name) do\n    dist_port Atom.to_string name\n  end\n\n  def dist_port(name) when is_list(name) do\n    dist_port List.to_string name\n  end\n\n  def dist_port(name) when is_binary(name) do\n    # Figure out the base port.  If not specified using the\n    # inet_dist_base_port kernel environment variable, default to\n    # 4370, one above the epmd port.\n    base_port = :application.get_env :kernel, :inet_dist_base_port, 4370\n\n    # Now, figure out our \"offset\" on top of the base port.  The\n    # offset is the integer just to the left of the @ sign in our node\n    # name.  If there is no such number, the offset is 0.\n    #\n    # Also handle the case when no hostname was specified.\n    node_name = Regex.replace ~r/@.*$/, name, \"\"\n    offset =\n      case Regex.run ~r/[0-9]+$/, node_name do\n    nil ->\n      0\n    [offset_as_string] ->\n      String.to_integer offset_as_string\n      end\n\n    base_port + offset\n  end\nend\n\ndefmodule Epmdless_dist do\n\n  def listen(name) do\n    # Here we figure out what port we want to listen on.\n\n    port = Epmdless.dist_port name\n\n    # Set both \"min\" and \"max\" variables, to force the port number to\n    # this one.\n    :ok = :application.set_env :kernel, :inet_dist_listen_min, port\n    :ok = :application.set_env :kernel, :inet_dist_listen_max, port\n\n    # Finally run the real function!\n    :inet_tcp_dist.listen name\n  end\n\n  def select(node) do\n    :inet_tcp_dist.select node\n  end\n\n  def accept(listen) do\n    :inet_tcp_dist.accept listen\n  end\n\n  def accept_connection(accept_pid, socket, my_node, allowed, setup_time) do\n    :inet_tcp_dist.accept_connection accept_pid, socket, my_node, allowed, setup_time\n  end\n\n  def setup(node, type, my_node, long_or_short_names, setup_time) do\n    :inet_tcp_dist.setup node, type, my_node, long_or_short_names, setup_time\n  end\n\n  def close(listen) do\n    :inet_tcp_dist.close listen\n  end\n\n  def childspecs do\n    :inet_tcp_dist.childspecs\n  end\nend\n\ndefmodule Epmdless_epmd_client do\n  # erl_distribution wants us to start a worker process.  We don't\n  # need one, though.\n  def start_link do\n    :ignore\n  end\n\n  # As of Erlang/OTP 19.1, register_node/3 is used instead of\n  # register_node/2, passing along the address family, 'inet_tcp' or\n  # 'inet6_tcp'.  This makes no difference for our purposes.\n  def register_node(name, port, _family) do\n    register_node(name, port)\n  end\n\n  def register_node(_name, _port) do\n    # This is where we would connect to epmd and tell it which port\n    # we're listening on, but since we're epmd-less, we don't do that.\n\n    # Need to return a \"creation\" number between 1 and 3.\n    creation = :rand.uniform 3\n    {:ok, creation}\n  end\n\n  def port_please(name, _ip) do\n    port = Epmdless.dist_port name\n    # The distribution protocol version number has been 5 ever since\n    # Erlang/OTP R6.\n    version = 5\n    {:port, port, version}\n  end\n\n  def names(_hostname) do\n    # Since we don't have epmd, we don't really know what other nodes\n    # there are.\n    {:error, :address}\n  end\nend When starting Elixir, we need to pass some of the parameters with --erl in order for them to make it through: iex --erl \"-proto_dist Elixir.Epmdless -start_epmd false -epmd_module Elixir.Epmdless_epmd_client\" --sname foo3 Let’s try to ping the two Erlang nodes we started earlier: iex(foo3@poki-sona-sin)1> Node.ping :\"foo1@poki-sona-sin\"\n:pong\niex(foo3@poki-sona-sin)2> Node.ping :\"foo2@poki-sona-sin\"\n:pong All connected, and no epmd in sight! Conclusion This is just one possible scheme for Erlang distribution without epmd; I’m sure you can come up with something else that fits your requirements better. I hope the example code above proves useful as a guide!", "date": "2016-10-26"},
{"website": "Erlang-Solutions", "title": "Scaling RabbitMQ on a CoreOS cluster through Docker", "author": ["Gabriele Santomaggio"], "link": "https://www.erlang-solutions.com/blog/scaling-rabbitmq-on-a-coreos-cluster-through-docker/", "abstract": "Erlang Solutions offers world-leading RabbitMQ consultancy, support & tuning solutions. Learn more > Introduction RabbitMQ provides, among other features, clustering capabilities. Using clustering, a group of properly configured hosts will behave the same as a single broker instance. All the nodes of a RabbitMQ cluster share the definition of vhosts, users, and exchanges but not queues. By default they physically reside on the node where they have been created, however as from version 3.6.1, the queue node owneriship can be configured using Queue Master Location policies . Queues are globally defined and reachable, by establishing a connection to any node of the cluster. Modern architectures often involve container based ways of scaling such as Docker . In this post we will see how to create a dynamic scaling RabbitMQ cluster using CoreOS and Docker : We will take you on a step by step journey from zero to the cluster. Get ready We are going to use different technologies, although we will not get into the details of all of them. For instance it is not required to have a deep CoreOS/Docker knowledge for the purpose of executing this test. It can be executed using your pc, and what you need is: Vagrant VirtualBox Git What we will do: Configure CoreOS cluster machines Configure Docker Swarm Configure RabbitMQ docker cluster Configure CoreOS cluster machines First we have to configure the CoreOS cluster: 1. Clone the vagrant repository: $ git clone https://github.com/coreos/coreos-vagrant\n$ cd coreos-vagrant 2. Use the user-data example file: $ cp user-data.sample user-data 3. Configure the cluster parameters: $ cp config.rb.sample config.rb 4. Open the file then uncomment num_instances and change it to 3, or execute: sed -i.bk  's/$num_instances=1/$num_instances=3/' config.rb 5. Start the machines using vagrant up : $ vagrant up\nBringing machine 'core-01' up with 'virtualbox' provider...\nBringing machine 'core-02' up with 'virtualbox' provider...\nBringing machine 'core-03' up with 'virtualbox' provider… 6. Add the ssh key: ssh-add ~/.vagrant.d/insecure_private_key 7. Use vagrant ssh core-XX -- -A to login, ex: $ vagrant ssh core-01 -- -A \n$ vagrant ssh core-02 -- -A \n$ vagrant ssh core-03 -- -A 8. Test your CoreOS cluster, login to the machine core-01: $ vagrant ssh core-01 -- -A Then core@core-01 ~ $ fleetctl list-machines\nMACHINE     IP      METADATA\n5f676932... 172.17.8.103    -\n995875fc... 172.17.8.102    -\ne4ae7225... 172.17.8.101    - 9. Test the etcd service: core@core-01 ~ $ etcdctl set /my-message \"I love Italy\"\nI love Italy 10. Login to vagrant ssh core-02: $ vagrant ssh core-02 -- -A\ncore@core-02 ~ $ etcdctl get /my-message\nI love Italy 11. Login to vagrant ssh core-03: vagrant ssh core-02 -- -A\ncore@core-03 ~ $ etcdctl get /my-message\nI love Italy As result you should have: 12. Test Docker installation using docker -v : core@core-01 ~ $ docker -v\nDocker version 1.12.3, build 34a2ead 13. (Optional step) Run the first image with docker run : core@core-01 ~ $  docker run ubuntu /bin/echo 'Hello world'\n…\nHello world The CoreOS the cluster is ready, and we are able to run Docker inside CoreOS. Let’s test our first RabbitMQ docker instance: 14. Execute the official RabbitMQ docker image: core@core-01 ~ $ docker run -d --hostname my-rabbit --name first_rabbit -p 15672:15672 rabbitmq:3-management 15. Check your eth1 vagrant IP (used to access the machine) : core@core-01 ~ $ ifconfig | grep -A1 eth1\neth1: flags = 4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n      inet 172.17.8.101  netmask 255.255.255.0  broadcast 172.17.8.255 Go to http://<your_ip>:15672/#/ in this case: http://172.17.8.101:15672/#/ . You should see the RabbitMQ management UI as ( guest guest ): In order to scale up the node above, we should run another container with --link parameter and execute rabbitmqctl join_cluster rabbit@<docker_host_name> . In order to scale down we should stop the second container and execute rabbitmqctl forget_cluster_node rabbit@<docker_host_name> . Turn into more positive. e.g. This is one of the areas where further enhancements on automation would be helpful. We need docker orchestration to configure and manage the docker cluster. Among the available orchestration tools, we have chosen Docker swarm . Before going ahead we should remove all the running containers: core@core-01 ~ $ docker rm -f $( docker ps -a -q ) And the images: core@core-01 ~ $ docker rmi -f $( docker images -q ) Configure Docker swarm Docker Swarm is the native clustering mechanism for Docker. We need to initialize one node and join the other nodes, as: 1. Swarm initialization: to the node-01 execute docker swarm init --advertise-addr 172.17.8.101 . docker swarm init automatically generates the command (with the token) to join other nodes to the cluster, as: core@core-01 ~ $ docker swarm init --advertise-addr 172.17.8.101\n\nSwarm initialized: current node ( 2fyocfwfwy9o3akuf6a7mg19o ) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join \\\n    --token SWMTKN-1-3xq8o0yc7h74agna72u2dhqv8blaw40zs1oow9io24u229y22z-4bysfgwdijzutfl6ydguqdu1s \\\n    172.17.8.101:2377 Docker swarm cluster is is composed by leader node and worker nodes. 2. Join the core-02 to the cluster docker swarm join --token <token> <ip>:<port> (you can copy and paste the command generated to the step 1) : In this case: core@core-02 ~ $ docker swarm join  \\\n--token SWMTKN-1-3xq8o0yc7h74agna72u2dhqv8blaw40zs1oow9io24u229y22z-4bysfgwdijzutfl6ydguqdu1s \\\n172.17.8.101:2377\n\nThis node joined a swarm as a worker. 3. Join the core-03 to the cluster docker swarm join --token <token> <ip>:<port> : core@core-03 ~ $ docker swarm join  \\\n--token SWMTKN-1-3xq8o0yc7h74agna72u2dhqv8blaw40zs1oow9io24u229y22z-4bysfgwdijzutfl6ydguqdu1s  \\\n172.17.8.101:2377\n\nThis node joined a swarm as a worker. 4. Check the swarm cluster using docker node ls : core@core-01 ~ $ docker node ls\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n07m3d8ipj2kgdiv9jptv9k18a    core-02   Ready   Active\n2fyocfwfwy9o3akuf6a7mg19o * core-01   Ready   Active        Leader\n8cicxxpn5f86u3roembijanig    core-03   Ready   Active Configure RabbitMQ docker cluster There are different ways to create a RabbitMQ cluster: Manually with rabbitmqctl Declaratively by listing cluster nodes in a config file Declaratively with rabbitmq-autocluster (a plugin) Declaratively with rabbitmq-clusterer (a plugin) To create the cluster we use the rabbitmq-autocluster plugin since it supports different services discovery such as Consul , etcd2 , DNS, AWS EC2 tags or AWS Autoscaling Groups . We decided to use etcd2, this is why we tested it on Configure CoreOS cluster machines see step 8. Ready to the final round, create the RabbitMQ cluster. 1. Create A Docker network: core@core-01~$ docker network create --driver overlay rabbitmq-network The swarm makes the overlay network available only to nodes in the swarm that require it for a service 2. Create a Docker service: core@core-01 ~ $ docker service create --name rabbitmq-docker-service \\\n-p 15672:15672 -p 5672:5672  --network rabbitmq-network  -e AUTOCLUSTER_TYPE = etcd  \\\n-e ETCD_HOST = ${ COREOS_PRIVATE_IPV4 } -e ETCD_TTL = 30  -e RABBITMQ_ERLANG_COOKIE = 'ilovebeam' \\\n-e AUTOCLUSTER_CLEANUP = true -e CLEANUP_WARN_ONLY = false gsantomaggio/rabbitmq-autocluster Note : The first time you have to wait a few seconds. 3. Check the service list using docker service ls 4. You can check the RabbitMQ instance running on http://<you_vagrant_ip>:15672/#/ most likely http://172.17.8.101:15672/#/ 5. Scale your cluster, using docker service scale as: core@core-01 ~ $ docker service  scale rabbitmq-docker-service = 5\nrabbitmq-docker-service scaled to 5 Congratulations!! You just scaled your cluster to 5 nodes! Since the 3 CoreOS machine are in cluster, you can use all the 3 machines to access the cluster, as: http://172.17.8.101:15672/#/ http://172.17.8.102:15672/#/ http://172.17.8.103:15672/#/ Where you should have: 6. Check the cluster status on the machine: core@core-01 ~ $ docker ps\nCONTAINER ID IMAGE  COMMAND   CREATED   STATUS PORTS  NAMES\nb480a09ea6e2        gsantomaggio/rabbitmq-autocluster:latest   \"docker-entrypoint.sh\"   1 seconds ago       Up Less than a second   4369/tcp, 5671-5672/tcp, \n15671-15672/tcp, 25672/tcp   rabbitmq-docker-service.3.1vp3o2w1eelzbpjngxncb9wur\naabb62882b1b        gsantomaggio/rabbitmq-autocluster:latest   \"docker-entrypoint.sh\"   6 seconds ago       Up 5 seconds            4369/tcp, 5671-5672/tcp, \n15671-15672/tcp, 25672/tcp   rabbitmq-docker-service.1.f2larueov9lk33rwzael6oore Same to the other nodes, you have more or less the same number of containers. Let’s see now in detail the docker service parameters: Command Description docker service create Create a docker service --name rabbitmq-docker-service Set the service name, you can check the services list using docker service ls -p 15672:15672 -p 5672:5672 map the RabbitMQ standard ports, 5672 is the AMQP port and 15672 is the Management_UI port --network rabbitmq-network Choose the docker network -e RABBITMQ_ERLANG_COKIE='ilovebeam' Set the same erlang.cookie value to all the containers, needed by RabbitMQ to create a cluster. With different erlang.cookie it is not possible create a cluster. Next are the auto-cluster parameters: Command Description -e AUTOCLUSTER_TYPE=etcd set the service discovery backend = etcd -e ETCD_HOST=${COREOS_PRIVATE_IPV4} The containers need to know the etcd2 ip. After executing the service you can query the database using the command line etcdctl ex: etcdctl ls /rabbitmq -recursive or using the http API ex: curl -L http://127.0.0.1:2379/v2/keys/rabbitmq -e ETCD_TTL=30 Used to specify how long a node can be down before it is removed from etcd’s list of RabbitMQ nodes in the cluster -e AUTOCLUSTER_CLEANUP=true Enables a periodic check that removes any nodes that are not alive in the cluster and no longer listed in the service discovery list. Scaling down removes one or more containers, the nodes will be removed from etcd database, see, for example: docker service scale rabbitmq-docker-service=4 -e CLEANUP_WARN_ONLY=false If set, the plugin will only warn about nodes that it would cleanup. AUTOCLUSTER_CLEANUP requires CLEANUP_WARN_ONLY=false to work. gsantomaggio/rabbitmq-autocluster The official docker image does not support the auto-cluster plugin, in my personal opinion they should. I created a docker image and registered it on docker-hub. AUTOCLUSTER_CLEANUP to true removes the node automatically, if AUTOCLUSTER_CLEANUP is false you need to remove the node manually. Scaling down and AUTOCLUSTER_CLEANUP can be very dangerous , if there are not HA policies all the queues and messages stored to the node will be lost. To enable HA policy you can use the command line or the HTTP API, in this case the easier way is the HTTP API, as: curl -u guest:guest  -H \"Content-Type: application/json\" -X PUT \\\n-d '{\"pattern\":\"\",\"definition\":{\"ha-mode\":\"exactly\",\"ha-params\":3,\"ha-sync-mode\":\"automatic\"}}' \\\nhttp://172.17.8.101:15672/api/policies/%2f/ha-3-nodes Note : Enabling the mirror queues across all the nodes could impact the performance, especially when the number of the nodes is undefined. Using \"ha-mode\":\"exactly\",\"ha-params\":3 we enable the mirror only for 3 nodes. So scaling down should be done for one node at time, in this way RabbitMQ can move the mirroring to other nodes. Conclusions RabbitMQ can easily scale inside Docker, each RabbitMQ node has its own files and does not need to share anything through the file system. It fits perfectly with containers. This architecture implements important features as: Round-Robin connections Failover cluster machines/images Portability Scaling in term of CoreOS nodes and RabbitMQ nodes Scaling RabbitMQ on Docker and CoreOS is very easy and powerful, we are testing and implementing the same environment using different orchestration tools and service discovery tools as kubernetes, consul etc, by the way we consider this architecture as experimental . Here you can see the final result: Enjoy! Erlang Solutions is the world leader in RabbitMQ consultancy, development, and support. We can help you design, set up, operate and optimise a system with RabbitMQ. Got a system with more than the typical requirements? We also offer RabbitMQ customisation and bespoke support.", "date": "2017-03-22nd"},
{"website": "Erlang-Solutions", "title": "Erlang Garbage Collector", "author": ["Lukas Larsson"], "link": "https://www.erlang-solutions.com/blog/erlang-garbage-collector/", "abstract": "This is an update to our previous blog post, Erlang 19.0 Garbage Collector. With Erlang/OTP 20.0, some things have changed, which is the reason for this updated blog post. Erlang Garbage Collector Erlang manages dynamic memory with a tracing garbage collector . More precisely a per process generational semi-space copying collector using Cheney’s copy collection algorithm together with a global large object space. Overview Each Erlang process has its own stack and heap which are allocated in the same memory block and grow towards each other. When the stack and the heap meet , the garbage collector is triggered and memory is reclaimed. If not enough memory was reclaimed, the heap will grow. Creating Data Terms are created on the heap by evaluating expressions. There are two major types of terms: immediate terms which require no heap space (small integers, atoms, pids, port ids etc) and cons or boxed terms (tuple, big num, binaries etc) that do require heap space. Immediate terms do not need any heap space because they are embedded into the containing structure. Let’s look at an example that returns a tuple with the newly created data. data (Foo) -> Cons = [42|Foo],\n   Literal = {text, \"hello world!\"},\n   {tag, Cons, Literal}. In this example we first create a new cons cell with an integer and a tuple with some text. Then a tuple of size three wrapping the other values with an atom tag is created and returned. On the heap tuples require a word size for each of its elements as well as for the header. Cons cells always require two words. Adding these things together, we get seven words for the tuples and 26 words for the cons cells. The string \"hello world!\" is a list of cons cells and thus requires 24 words. The atom tag and the integer 42 do not require any additional heap memory since it is an immediate . Adding all the terms together, the heap space required in this example should be 33 words. Compiling this code to beam assembly ( erlc -S ) shows exactly what is happening. ...\n    {test_heap,6,1}.\n    {put_list,{integer,42},{x,0},{x,1}}.\n    {put_tuple,3,{x,0}}.\n    {put,{atom,tag}}.\n    {put,{x,1}}.\n    {put,{literal,{text,\"hello world!\"}}}.\n    return. Looking at the assembler code we can see three things; The heap requirement in this function turns out to be only six words, as seen by the {test_heap,6,1} instruction. All the allocations are combined to a single instruction. The bulk of the data {text, \"hello world!\"} is a literal . Literals, sometimes referred to as constants, are not allocated in the function since they are a part of the module and allocated at load time. If there is not enough space available on the heap to satisfy the test_heap instructions request for memory, then a garbage collection is initiated. It may happen immediately in the test_heap instruction, or it can be delayed until a later time depending on what state the process is in. If the garbage collection is delayed, any memory needed will be allocated in heap fragments. Heap fragments are extra memory blocks that are a part of the young heap, but are not allocated in the contigious area where terms normally reside. See The young heap for more details. The collector Erlang has a copying semi-space garbage collector. This means that when doing a garbage collection, the terms are copied from one distinct area, called the from space , to a new clean area, called the to space . The collector starts by scanning the root-set (stack, registers, etc). It follows all the pointers from the root-set to the heap and copies each term word by word to the to space . After the header word has been copied a move marker is destructively placed in it pointing to the term in the to space . Any other term that points to the already moved term will see this move marker and copy the referring pointer instead. For example, if the have the following Erlang code: foo (Arg) -> T = {test, Arg},\n    {wrapper, T, T, T}. Only one copy of T exists on the heap and during the garbage collection only the first time T is encountered will it be copied. After all terms referenced by the root-set have been copied, the collector scans the to space and copies all terms that these terms reference. When scanning, the collector steps through each term on the to space and any term still referencing the from space is copied over to the to space . Some terms contain non-term data (the payload of a on heap binary for instance). When encountered by the collector, these values are simply skipped. Every term object we can reach is copied to the to space and stored on top off the scan stop line, and then the scan stop is moved to the end of the last object. When scan stop marker catches up to the scan start marker, the garbage collection is done. At this point we can deallocate the entire from space and therefore reclaim the entire young heap. Generational Garbage Collection In addition to the collection algorithm described above, the Erlang garbage collector also provides generational garbage collection. An additional heap, called the old heap, is used where the long lived data is stored. The original heap is called the young heap, or sometimes the allocation heap. With this in mind we can look at the Erlang’s garbage collection again. During the copy stage anything that should be copied to the young to space is instead copied to the old to space if it is below the high-watermark . The high-watermark is placed where the previous garbage collection (described in Overview ) ended and we have introduced a new area called the old heap. When doing the normal garbage collection pass, any term that is located below the high-watermark is copied to the old to space instead of the young. In the next garbage collection, any pointers to the old heap will be ignored and not scanned. This way the garbage collector does not have to scan the long-lived terms. Generational garbage collection aims to increase performance at the expense of memory. This is achieved because only the young, smaller, heap is considered in most garbage collections. The generational hypothesis predicts that most terms tend to die young, and for an immutable language such as Erlang, young terms die even faster than in other languages. So for most usage patterns the data in the new heap will die very soon after it is allocated. This is good because it limits the amount of data copied to the old heap and also because the garbage collection algorithm used is proportional to the amount of live data on the heap. One critical issue to note here is that any term on the young heap can reference terms on the old heap but no term on the old heap may refer to a term on the young heap. This is due to the nature of the copy algorithm. Anything referenced by an old heap term is not included in the reference tree, root-set and its followers, and hence is not copied. If it was, the data would be lost, fire and brimstone would rise to cover the earth. Fortunately, this comes naturally for Erlang because the terms are immutable and thus there can be no pointers modified on the old heap to point to the young heap. To reclaim data from the old heap, both young and old heaps are included during the collection and copied to a common to space . Both the from space of the young and old heap are then deallocated and the procedure will start over from the beginning. This type of garbage collection is called a full sweep and is triggered when the size of the area under the high-watermark is larger than the size of the free area of the old heap. It can also be triggered by doing a manual call to erlang:garbage_collect() , or by running into the young garbage collection limit set by spawn_opt(fun(),[{fullsweep_after, N}]) where N is the number of young garbage collections to do before forcing a garbage collection of both young and old heap. The young heap The young heap, or the allocation heap, consists of the stack and heap as described in the Overview. However, it also includes any heap fragments that are attached to the heap. All of the heap fragments are considered to be above the high-watermark and part of the young generation. Heap fragments contain terms that either did not fit on the heap, or were created by another process and then attached to the heap. For instance if the bif binary_to_term created a term which does not fit on the current heap without doing a garbage collection, it will create a heap-fragment for the term and then schedule a garbage collection for later. Also if a message is sent to the process, the payload may be placed in a heap-fragment and that fragment is added to young heap when the message is matched in a receive clause. This procedure differs from how it worked prior to Erlang/OTP 19.0. Before 19.0, only a contiguous memory block where the young heap and stack resided was considered to be part of the young heap. Heap fragments and messages were immediately copied into the young heap before they could be inspected by the Erlang program. The behaviour introduced in 19.0 is superior in many ways – most significantly it reduces the number of necessary copy operations and the root set for garbage collection. Sizing the heap As mentioned in the Overview the size of the heap grows to accommodate more data. Heaps grow in two stages, first a variation of the Fibonacci sequence is used starting at 233 words. Then at about 1 mega words the heap only grows in 20% increments . There are two occasions when the young heap grows: if the total size of the heap + message and heap fragments exceeds the current heap size. if after a fullsweep, the total amount of live objects is greater than 75%. There are two occasions when the young heap is shrunk: if after a young collection, the total amount of live objects is less than 25% of the heap and the young heap is “big” if after a fullsweep, the total amount of live objects is less than 25% of the heap. The old heap is always one step ahead in the heap growth stages than the young heap. Literals When garbage collecting a heap (young or old) all literals are left in place and not copied. To figure out if a term should be copied or not when doing a garbage collection the following pseudo code is used: if (erts_is_literal(ptr) || (on_old_heap(ptr) && ! fullsweep)) { /* literal or non fullsweep - do not copy */ } else {\n  copy(ptr);\n} The erts_is_literal check works differently on different architectures and operating systems. On 64 bit systems that allow mapping of unreserved virtual memory areas (most operating systems except Windows), an area of size 1 GB (by default) is mapped and then all literals are placed within that area. Then all that has to be done to determine if something is a literal or not is two quick pointer checks . This system relies on the fact that a memory page that has not been touched yet does not take any actual space. So even if 1 GB of virtual memory is mapped, only the memory which is actually needed for literals is allocated in ram. The size of the literal area is configurable through the +MIscs erts_alloc option. On 32 bit systems, there is not enough virtual memory space to allocate 1 GB for just literals, so instead small 256 KB sized literal regions are created on demand and a card mark bit-array of the entire 32 bit memory space is then used to determine if a term is a literal or not. Since the total memory space is only 32 bits, the card mark bit-array is only 256 words large. On a 64 bit system the same bit-array would have to be 1 tera words large, so this technique is only viable on 32 bit systems. Doing lookups in the array is a little more expensive then just doing the pointer checks that can be done in 64 bit systems, but not extremely so. On 64 bit windows, on which erts_alloc cannot do unreserved virtual memory mappings, a special tag within the Erlang term object is used to determine if something is a literal or not . This is very cheap, however, the tag is only available on 64 bit machines, and it is possible to do a great deal of other nice optimizations with this tag in the future (like for instance a more compact list implementation) so it is not used on operating systems where it is not needed. This behaviour is different from how it worked prior to Erlang/OTP 19.0. Before 19.0 the literal check was done by checking if the pointer pointed to the young or old heap block. If it did not, then it was considered a literal. This lead to considerable overhead and strange memory usage scenarios, so it was removed in 19.0. Binary heap The binary heap works as a large object space for binary terms that are greater than 64 bytes (from now on called off-heap binaries). The binary heap is reference counted and a pointer to the off-heap binary is stored on the process heap. To keep track of when to decrement the reference counter of the off-heap binary, a linked list (the MSO – mark and sweep object list) containing funs and externals as well as off-heap binaries is woven through the heap. After a garbage collection is done, the MSO list is swept and any off-heap binary that does not have a move marker written into the header words has its reference decremented and is potentially freed . All items in the MSO list are ordered by the time they were added to the process heap, so when doing a minor garbage collection, the MSO sweeper only has to sweep until it encounters an off-heap binary that is on the old heap . Virtual Binary heap Each process has a virtual binary heap associated with it that has the size of all the current off-heap binaries that the process has references to. The virtual binary heap also has a limit and grows and shrinks depending on how off-heap binaries are used by the process. The same growth and shrink mechanisms are used for the binary heap and for the term heap, so first a Fibonacci like series and then 20% growth. The virtual binary heap exists in order to trigger garbage collections earlier when potentially there is a very large amount of off-heap binary data that could be reclaimed. This approach does not catch all problems with binary memory not being released soon enough, but it does catch a lot of them. Messages Messages can become a part of the process heap at different times. This depends on how the process is configured. We can configure the behaviour of each process using process_flag(message_queue_data, off_heap | on_heap) or we can set a default for all processes at start using the option +hmqd . What do these different configurations do and when should we use them? Let’s start by going through what happens when one Erlang process sends a message to another. The sending process needs to do a couple of things: calculate how large the message to be sent is allocate enough space to fit the entire message copy the message payload allocate a message container with some meta data insert the message container in the receiver process’ message queue The process flag message_queue_data , of the receiver process, controls the message allocating strategy of the sender process in step 2 and also how the message data is treated by the garbage collector. The procedure above is different from how it worked prior to 19.0. Before 19.0 there was no configuration option, the behaviour was always very similar to how the on_heap option is in 19.0. Message allocating strategies If set to on_heap , the sending process will first attempt to allocate the space for the message directly on the young heap block of the receiving process. This is not always possible as it requires taking the main lock of the receiving process. The main lock is also held when the process is executing. The possibility for a lock conflict is thus likely in an intensely collaborating system. If the sending process cannot acquire the main lock, a heap fragment is instead created for the message and the message payload is copied onto that. With the off_heap option the sender process always creates heap fragments for messages sent to that process. There are a bunch of different tradeoffs that come into play when trying to figure out which of the strategies you want to use. Using off_heap may seem like a nice way to get a more scalable system as you get very little contention on the main locks, however, allocating a heap fragment is more expensive than allocating on the heap of the receiving process. So if it is very unlikely that contention will occur, it is more efficient to try to allocate the message directly on the receiving process’ heap. Using on_heap will force all messages to be part of on the young heap which will increase the amount of data that the garbage collector has to move. So if a garbage collection is triggered while processing a large amount of messages, they will be copied to the young heap. This in turn will lead to that the messages will quickly be promoted to the old heap and thus increase its size. This may be good or bad depending on exactly what the process does. A large old heap means that the young heap will also be larger, which in turn means that less garbage collections will be triggered while processing the message queue. This will temporarly increase the throughput of the process at the cost of more memory usage. However, if after all the messages have been consumed the process enters a state where a lot less messages are being received. Then it may be a long time before the next fullsweep garbage collection happens and the messages that are on the old heap will be there until that happens. So while on_heap is potentially faster than the other modes, it uses more memory for a longer time. This mode is the legacy mode which is almost how the message queue was handled before Erlang/OTP 19.0. Which one of these strategies is best depends a lot on what the process is doing and how it interacts with other processes. So, as always, profile the application and see how it behaves with the different options. [1] : C. J. Cheney. A nonrecursive list compacting algorithm. Commun. ACM, 13(11):677–678, Nov. 1970. [2] : D. Ungar. Generation scavenging: A non-disruptive high performance storage reclamation algorithm. SIGSOFT Softw. Eng. Notes, 9(3):157–167, Apr. 1984.", "date": "2017-11-30"},
{"website": "Erlang-Solutions", "title": "Lessons FinTech Can Learn From Telecom – Part 2/2", "author": ["Erik Schön"], "link": "https://www.erlang-solutions.com/blog/lessons-fintech-can-learn-from-telecom-part-two/", "abstract": "In the second half of this blog series, Erik Schön looks next at the ‘right tool for the jobs’ that are required for FinTech 3.0.  The Erlang/Elixir/OTP open-source ecosystem for software development has its roots in telecom – an industry that has already resolved many of the new challenges confronting FinTech. Revisit Part One . The Right Tool for the Job One of the most important tools in the telecoms toolbox for real-time, secure, reliable, scalable and interoperable systems that can be developed quickly and operated at low cost, is the open-source programming language and associated frameworks, tools and libraries called Erlang/OTP (Open Telecom Platform) originally developed by the telecom giant Ericsson. The Need: Quick Development of the Right Thing Mike Williams, co-creator of Erlang/OTP, wrote down his credo in 1985 which was then used to guide the development of Erlang/OTP during the 80s and 90s (Däcker, 2009): “Find the right methods – design by prototyping.” “Make mistakes on a small scale – not in a production project.” “It’s not good enough to have ideas – you must also be able to implement them to know that they work.” These insights contributed to making Erlang/OTP suitable for iterative, incremental development with quick feedback from real customers. This also ensured a smooth developer experience making it easier to build the right thing that: customers find valuable , usable and sometimes even desirable ; is feasible from a technical perspective; is viable from a commercial perspective; is reasonable from a societal perspective, e.g. sustainable and ethical. The Need: Real-Time, Secure, Reliable and Scalable Solutions Bjarne Däcker, head of the Ericsson computer science lab and Mike Williams’s manager at the time, formulated the requirements on the new programming language system as follows (Däcker, 2000): Real-time: “Actions to be performed at a certain point in time or within a certain time.” Security: “Stringent quality” Reliability: “Very large software systems … Interaction with hardware … Complex functionality such as feature interaction … Continuous operation for many years … Software maintenance (reconfiguration, etc.) without stopping the system … Fault tolerance both to hardware failures and software errors” Scalability: “Systems distributed over several computers … handling of a very large number of concurrent activities.” Joe Armstrong , co-creator of Erlang/OTP summarised it as “making reliable distributed systems in the presence of software errors”. (Armstrong, 2003). Business Outcomes: Faster, Safer, Better, and, More for Less Over the past 20+ years Erlang/OTP has provided the following business outcomes (Cesarini, 2019): 2x FASTER development of new services thanks to the language’s design, the OTP middleware, the set of frameworks, principles, and design patterns that guide and support the structure, design, implementation, and deployment where e.g. Motorola saw 4-20 times less code compared to traditional languages. 10x BETTER services that are down less than 5 minutes per year thanks to built-in fault tolerance and resilience mechanisms built into the language and the OTP middleware, e.g. software upgrades and generic error handling, as seen by e.g. Ericsson in their mobile network packet routers. 10x SAFER services that are hard to hack and crash through denial of service attacks thanks to the language construction with lack of pointers, use of messages instead of shared memory and immutable state rather than variable assignments, as seen by the number of vulnerabilities compared to other languages (CVE, 2021). 10x MORE users (billions), transactions per second (millions) within milliseconds thanks to a battle-tested virtual machine as seen by e.g. WhatsApp with more than 2 billion users. 10x LESS costs and energy consumption thanks to fewer servers needed where e.g. Bleacher Report were able to reduce their hardware requirements from 150 servers to 5. It has been used in telecom since the 90s by e.g. Vendors: Cisco, Ericsson, Nokia, Motorola, 2600Hz Operators: T-Mobile, Telia Social: WhatsApp and in FinTech. since the mid-00s by e.g. Startups: Rocker , Monara Scaleups: Solarisbank , Brex , SumUp , Kivra Unicorns: Klarna Traditional banks and financial institutions: Mastercard , Goldman Sachs , Bloomberg , OTP Bank Digital currencies: æternity , BlockFi Robert Virding , co-creator of Erlang/OTP formulated the unique value proposition like this: “ Any sufficiently complicated concurrent program in another language [for this job to be done] contains an ad hoc informally-specified bug-ridden slow implementation of half of Erlang.” (Virding, 2008). What about Elixir? Elixir has all the benefits of Erlang with a lower threshold for developers used to traditional programming languages like Ruby since Elixir’s syntax looks much more familiar to them. Additionally, Elixir gives a very smooth developer experience including state-of-the-art libraries e.g. for graphical user interfaces with Phoenix LiveView. Our Elixir Consultancy & Development The Developer Experience Both Erlang and Elixir are easy to learn within a couple of weeks for people with practical experiences and skills equivalent to a computer science degree as well as a curiosity to learn. And, experience shows that they get up to speed within a couple of months which is what it normally takes to understand a new product codebase or business domain (Däcker, 2000). Engineers and developers love the experience of using Elixir and Erlang, and as of today, there are over 50,000 members in over 140 Meetup groups in all continents of the world except Antarctica (Schön, 2021). The Road Ahead The Erlang/Elixir open-source ecosystem is thriving like never before. During 2020-2021 we have seen companies like WhatsApp and Klarna working together on improving the developer experience further and companies like Ericsson evolving the OTP middleware where the next OTP release in May, 2021 is expected to improve the out-of-the-box performance of Erlang and Elixir applications by 30-130% (Larsson, 2020) and reduce the energy consumption by 25% (Cathcart, 2021). And, we haven’t even mentioned the recent and exciting announcement of Elixir optimised for quick development of safe machine learning solutions for new innovative, automated services (Valim, 2021). Conclusion We hope that you now see what FinTech. can learn from telecom and how FinTech. companies can use the Erlang/Elixir/OTP open-source ecosystem to go 2x FASTER, 10x SAFER, 10x BETTER with 10x MORE for 10x LESS – resulting in happy and loyal customers as well as engaged developers. What are your jobs to be done? What tools are you using? How can we help? Visit our FinTech Hub Page or, if you’re ready to find out how we can work together, tell us about your development project or idea . Acknowledgements Kudos to Michael Jaiyeola, for the original idea, helpful pointers, examples and feedback; Noman Azim, for valuable input and concrete examples; Steve Roberts for helpful feedback, insights and examples from a telecom perspective; Phil Harrison for insights and feedback from a FinTech. perspective; Francesco Cesarini for co-creating a generous and welcoming community, for spreading the word and feedback; Joe Armstrong, Robert Virding, Mike Williams and Bjarne Däcker for perseverance, professionalism and respect in co-creating and managing Erlang/OTP. Writer Erik Schön , Managing Director, Erlang Solutions Nordic, is an executive with 20+ years of telecoms experience from global standardisation, system design and managing R&D organisations of up to 400 people at the global telecom giant Ericsson. Erik is a big fan of Erlang since the 90s and his latest book is The Art of Strategy . References Armstrong, Joe (2003). Making reliable distributed systems in the presence of software errors Cathcart, Will (2021), Improving WhatsApp’s server efficiency by 25% Cesarini, Francesco (2019). Which companies are using Erlang, and why? CVE (2021). CVE Security Vulnerability Database Däcker, Bjarne (2000). Concurrent Functional Programming for Telecommunications: A Case Study of Technology Introduction Däcker, Bjarne (2009). CS Lab and all that … Larsson, Lukas (2020). Implement BeamAsm – a JIT for Erlang/OTP . Rubio, Manuel (2019). Which companies are using Elixir, and why? Schön, Erik (2021). Elixir & Erlang Developers Valim, José (2021). Nx (Numerical Elixir) is now publicly available . Virding, Robert (2008). Virding’s First Rule of Programming", "date": "2021-05-19"},
{"website": "Erlang-Solutions", "title": "Lessons FinTech Can Learn From Telecom – Part 1/2", "author": ["Erik Schön"], "link": "https://www.erlang-solutions.com/blog/lessons-fintech-can-learn-from-telecom-part-one/", "abstract": "This two-part blog series looks at the jobs to be done in fintech and argues that the right tool for the job, the Erlang/Elixir/OTP open-source ecosystem for software development, has its roots in telecom – an industry that has already resolved many of the new challenges confronting fintech currently. To ensure you don’t miss part two of this blog and other high-quality content from us you can subscribe to our fintech newsletter here. The Brave New World of Fintech The world of financial services using technology to make individuals’ lives easier and more prosperous is becoming increasingly similar to the world of telecommunications. The simple reason is that since the introduction of the iPhone (a smart telecommunications device also known as a “smartphone”) in 2007, most of our interactions with banks and other financial institutions are through smartphones. This means that we expect the banking experience to be as good as all our other smartphone experiences, be it entertainment or work related, including around the clock availability and responses within fractions of a second. This is backed up by research showing that response times above half a second makes people wonder what is going on and get stressed. Since it is about money and other financial instruments, we also expect all interactions and personal data to be highly secure from e.g. intrusions and theft. The essence of all financial services is trust. You trust banks to not lose your money. If you lose the trust in your bank, you will switch to another bank or at least move parts of your business elsewhere. In more technical terms, we want our financial interactions to be done in real-time and be extremely secure and reliable. The Job to Be Done From a customer perspective, we have just seen how our financial services needs are for real-time, secure and reliable products and solutions. Also, we expect new innovative services that simplify our daily lives, e.g. saving us time and money while growing our financial wealth. One example is instant payments using your smartphone as a digital wallet connected to your bank account where work is ongoing to enable instant person-to-person payments also across national borders. The traditional banks have a track record of security and reliability leading to trust, loyalty and long-standing personal relationships with their customers with e.g. personal banking experts available for financial advice. In the past that trust was established with large imposing buildings signifying they were here to stay.  Over time banks have eliminated buildings – particularly in the branch network in order to reduce costs and as a consequence “trust” has become more intangible. As a traditional bank, you would like to extend the experience to the digital world and at the same time reduce your large cost base which is due to very old hardware and software.  One example is a European bank that developed a new, well-designed and innovative smartphone application that unfortunately did not work since it took up to 30 seconds to fetch customer data from the legacy software system. They realised that their legacy software system was not fit for the future and have since then started to replace the system with future-proof technology, one “elephant carpaccio slice” at a time. If you are a startup providing financial services, your needs are your customers’ needs, so real-time, security and reliability in order to secure their trust and loyalty are still valid. Since you are competing with the incumbent giants, you need a unique, focused offering that can be developed quickly and operated at low cost. Additionally, you want to provide a smooth onboarding experience to acquire new customers quickly. Backbase and Tide offer one minute customer onboarding time, a process that usually requires at least 48 hours to fully vet and comply with regulations. If you are one of the big US technology giants, you may want to consider your particular superpower, e.g. social networking, superb user experience or superior customer understanding, to expand your offering to embrace selected financial services as an integral part of your brand. One example is Apple Pay which extends the real time facility without having a current account or complying with onboarding requirements. Another example is Uber Money with digital wallets and credit and debit cards. If you are one of the clearing houses, banking centrals, credit card companies or central banks, the huge number of financial transactions due to the real-time interactions lead to massive requirements on scalability and resilience of your technical infrastructure since the systems you operate are critical, not only for your specific customers but for society at large. We are seeing experiments with digital currencies run by several central banks in the world, including Sweden’s “Riksbank”, founded in 1668. If you are one of the providers of digital currencies based on blockchain technologies you also face the challenge of minimising the energy consumption for all the computations needed. Considering the engineers and developers who design, deliver and operate the financial systems, they want an environment where you can develop and deploy easily – including smooth programming languages requiring few lines of code supported by libraries, frameworks and tooling – and get quick feedback from both development, test and commercial deployments. Motivated and engaged employees lead to better services and happier and more loyal customers. To summarise: we need real-time, secure, reliable and scalable systems that can be developed quickly and smoothly, operating at low cost with interoperability across borders and currencies. FinTech Trends in 2021 Report The Telecoms World Fortunately, the needs of real-time, secure, reliable, scalable and interoperable systems that can be developed quickly and operated at low cost have already been fulfilled in the telecoms world starting in the 60s and 70s when the telecom operators started deploying the first digital software switches for local, regional and international phone calls. This infrastructure critical for society was designed, developed and tested for global, real-time around the clock operation with billions of users from the very beginning and global standards for interoperability were key enablers. The technology evolution grew very rapidly in the 80s and 90s due to the increased demands for communication on the move using handsets connected to mobile networks, first using national network standards  (the first generation, 1G, for voice calls only), then regional network standards (the second generation, 2G, for voice, text messages and limited data transmission), and, finally global network standards (the 3rd, 4th and 5th generations, 3G/4G/5G, optimised for higher and higher rates of data transmission). This evolution happened in parallel with the evolution of systems for secure and reliable financial transactions. The biggest difference was that the telecom systems had far more transactions to handle due to the real-time nature of phone calls, mobile phone calls and browsing the web whereas the financial world handled this particular challenge through daily batches around midnight where all the financial transactions during the past 24 hours were processed. Finally, telecoms have been commoditised by regulation, standards and global competition and is now a low margin business. There is a similar threat to financial services. In telecom, threats such as Google and others were well known to telecoms yet the telecom operators failed to act and now the value has been taken by the Big US Tech players with a global mindset. To summarise, telecoms have already mastered the challenge of real-time, secure, reliable, scalable and interoperable systems that can be developed quickly and operated at low cost. At Erlang Solutions we believe that a lot of our learnings from this industry can be applied to exciting new projects in the fintech space. Writer Erik Schön , Managing Director, Erlang Solutions Nordic, is an executive with 20+ years of telecoms experience from global standardisation, system design and managing R&D organisations of up to 400 people at the global telecom giant Ericsson. Erik is a big fan of Erlang since the 90s and his latest book is The Art of Strategy . For part two of this blog series and other great fintech content you can subscribe to our newsletter here.", "date": "2021-05-06"},
{"website": "Erlang-Solutions", "title": "How to use RabbitMQ in service integration", "author": ["Gabor Olah"], "link": "https://www.erlang-solutions.com/blog/how-to-use-rabbitmq-in-service-integration/", "abstract": "RabbitMQ, the message broker, is not the first thing that comes to mind when we talk about integrating legacy services. We use high level frameworks to hide the complexity of the underlying plumbing. In this post we will show how the RabbitMQ, the most widely deployed open source message broker, can help with reliable and resilient messaging. What is RabbitMQ? RabbitMQ is a free, open-source and extensible message queuing solution. It is a message broker that understands AMQP (Advanced Message Queuing Protocol), but can also be used with other popular messaging solutions like MQTT. It is highly available, fault tolerant and scalable. It is implemented in Erlang OTP, a technology tailored for building stable, reliable, fault tolerant and highly scalable systems which possess native capabilities of handling very large numbers of concurrent operations. The benefits of Erlang OTP can easily be seen in RabbitMQ and other systems like WhatsApp, MongooseIM, to mention a few. At a very high level, it is a middleware layer that enables different services in your application to communicate with each other without worrying about message loss while providing different quality of service (QoS) requirements. It also enables fine-grained and efficient message routing enabling extensive decoupling of applications. RabbitMQ Connecting Legacy Services When designing system interactions we usually start with a top-to-bottom approach. We identify the components and use the technology available to connect them. If we need to integrate existing systems with newer ones, then it is often infeasible to change the older system to use modern protocols. In cases like this, connector applications can be developed acting as a bridge between the old system and the new. Once we know the components and we have all the translation layers, we can deal with the interconnection. There are two major ways how parts of the system can communicate: Directly (synchronously). When both components are online, they can use an active network connection and a preferred protocol like HTTP, protobuf or even FTP. Using these technologies results in a simple solution with its trade-offs, such as unavailability of multiple services, or having to implement client side buffering, when things go wrong. Indirectly (asynchronously). To mitigate the trade-offs from the direct communication we can introduce a messaging layer into the solution that can retain the messages even if the receiving application is not online. Although this adds complexity to the architecture it removes a lot of edge cases from the applications. Scaling the messaging layer is often easier than introducing scale to the legacy applications. RabbitMQ is a well-suited message broker because it scales well not only with the message load requirements, but also with the business requirements. Finding message brokers that can handle hundreds or thousands or even tens of thousands of messages is easy. But finding a message bus that can cater for several messaging patterns, can handle different protocols (AMQP, MQTT, STOMP, JMS and many more) and can be modified via plugins is much more difficult. Features come with trade-offs, but when RabbitMQ is put into a heterogeneous environment, it provides an easy way to move the messages between virtually any systems. All major frameworks support an AMQP adapter, but even if the component uses a proprietary protocol then it is possible to develop a plugin and extend RabbitMQ’s functionality. Hear from industry leaders at RabbitMQ Summit 2021 Resilient and Reliable Messaging RabbitMQ provides a high degree of availability and consistent messaging if the following three features are used. First we see why it is recommended to run RabbitMQ as a cluster, then how to guarantee that messages are received and kept safe by RabbitMQ and how quorum queues improve on classic mirroring. Clustering Although a single node RabbitMQ provides the highest consistency guarantees, it is not resilient to hardware errors or planned downtimes due to upgrades. If anything happens to the single RabbitMQ instance it means that the service is no longer available system-wide. If any of the message store files gets corrupted on the file system, then messages can be lost. To avoid these problems RabbitMQ can run as a cluster, each node providing the full RabbitMQ functionality. It does not matter for the clients which RabbitMQ node it connects, RabbitMQ hides the complexities of distributing and replicating messages, queues and exchanges. The big question is how many nodes we should use in a cluster. Using two nodes might be tempting, but it only works with systems running in an active-passive nature. RabbitMQ, as we discussed above, runs in an active-active fashion where each node in a cluster provides identical functionality. This means that in case of any problems, RabbitMQ needs a consensus of the majority of the nodes to function. By restricting the number of nodes in a cluster to an odd number, we can guarantee that the cluster will consistently survive the loss of the minority of the nodes (in case of 5 nodes, RabbitMQ can tolerate the loss of 2 nodes.) To enforce that the minority nodes don’t commit changes, pausing them when detecting a network issue can be automated by setting the cluster partition handling mechanism to pause_minority. This is a built-in feature and it not only detects that a network partition occurred but also detects when the network is working again and can restart the nodes paused automatically. RabbitMQ does not require manual intervention to heal. Publisher confirm and acknowledgements RabbitMQ can automagically cluster and heal itself, but due to the distributed nature of things, it requires some cooperation from the clients to guarantee no message loss. When publishing a message to RabbitMQ the clients can only “forget” about the message when it was acknowledged by RabbitMQ. This acknowledgement on the publishing side is called the Publisher Confirm feature. It is more lightweight than the transaction mechanism in AMQP and performs much better under load. This RabbitMQ specific feature is supported by all major client libraries. On the consuming side, the clients should acknowledge the message only when they finished processing the message. This guarantees that once a message is fully processed it is removed from RabbitMQ, but if any problem happens during message processing, the same message can be redelivered to the same or a different client. This gives a good opportunity to introduce horizontal scaling for parts of the system. Quorum Queues The last bit of the puzzle is to make sure that the messages that are in RabbitMQ can survive network errors, hardware failures or planned maintenance. With the introduction of clustering it is possible to distribute the workload among many nodes, but it is also important for introducing resiliency. Quorum queues are the new addition to the RabbitMQ feature family providing an improved solution to queue mirroring. It is based on the state of the art Raft algorithm to manage the integrity of the queue leader and the contents of the queues. Using it together with the publisher confirms, it is guaranteed that messages acknowledged by RabbitMQ will be safe until they are delivered to consumers. Quorum queues improve on one of the major issues of classic queue mirroring: queue synchronisation after a network partition. With the Raft protocol, it is possible to only copy the messages that are missing from the mirror. This reduces the synchronisation network pressure which was a cause for some hard-to-debug RabbitMQ issues in the past. Summary In this blog post, we discussed the major challenges of integrating legacy and non-legacy systems. Introducing a versatile messaging layer like RabbitMQ can provide solutions to lots of problems and applying the three major features highlighted in this blogpost will enable a successful project. Here at Erlang Solutions, we have world leading experts in RabbitMQ and can help with finding solutions to the unique problems of distributed systems. These challenges range from auditing the planned or deployed RabbitMQ solution or designing bespoke solutions with RabbitMQ . Get in touch to find out more about how we can help with your integration project or get your ticket to RabbitMQ Summit 2021 to learn more about why RabbitMQ is trusted by many of the world’s biggest companies.", "date": "2021-05-13"},
{"website": "Erlang-Solutions", "title": "FinTech Client Case Studies – Erlang Solutions and Trifork", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/fintech-client-case-studies-erlang-solutions-and-trifork/", "abstract": "Financial services (FS) institutions are some of the biggest global spenders when it comes to software. The costs of ripping out legacy systems are substantial, so traditional FS firms have identified the use of specialist vendors as the most efficient way to approach digitisation of their technology stacks. Similarly, FinTech startups are looking to leverage external expertise and leave themselves room to focus on the core of their business. Erlang Solutions is part of the Trifork Group of tech companies who work closely together to deliver scalable and highly reliable systems. This blog summarises some of the latest projects we have completed for our FinTech clients across our Erlang Solutions and Trifork teams. You can also visit the dedicated area of our website for more information on our FinTech development solutions . Creating a leading Open Banking platform Client: Spar Nord The enabling technology APIs associated with Open Banking which allow non-financial businesses to offer financial services to their clients based on customer banking data continues to pick up speed, particularly in instant payments, peer-to-peer lending, and credit scoring. One of the major concerns for companies looking to leverage opportunities in this space concerns the safety and security of data. For Spar Nord , this was a fundamental principle in the development of their Open Banking platform. The Challenge The requirement was for a new secure platform to enable the easy on-boarding of FinTech firms who could obtain user consent to access customer data. The platform also needed to allow them to provide services using the Spar Nord API. The Solution In just two months, the solution was launched and the first FinTech company was live with their subscription service. The platform is based on Trifork’s eMobility product for customer consent and focused on user-friendliness and robust security. Third parties can access over 400,000 customers, a banking licence, security platform with consent and dedicated business and tech contacts. The Results Spar Nord was first-to-market with their solution Customers have an up-to-date and innovative digital solution An Identity Management and Secure Service Gateway to allow Spar Nord’s API to be accessible on mobile devices FinTech Trends in 2021 Report Robo Advisors for user investments Client: Sparindex Sparindex work with index investments, passively managed portfolios which are low cost, efficient and have a large spread of risk. The Challenge Sparindex had traditionally marketed their products through banks. They wanted to create a new digital platform that focused on index investments. The solution needed to: Be more visible and available to everyone Allow users to create virtual portfolios with their funds Allow trading on Sparinvest’s platform The Solution Sparinvest and Trifork agreed to kick-off the new cooperation with a Trifork Accelerate® workshop. Following the five-day Accelerate design sprint, Sparindex and Trifork were able to prototype and test their ideas and concepts on real end-users. This led to a rapid proof of concept and ultimately the development of the robo based investment solution (using AI and machine learning processes) providing wealth management following algorithms based on users’ settings and requirements The Results Sparindex officially launched its new digital platform in January 2018. More than 20,000 users are already on the platform and with passive investing continuously evolving, Sparindex expects the number of users to grow remarkably in the coming years. Innovative mobile payment solution Client: Mobile Pay (Danske Bank) Danske Bank and Trifork developed a market-leading mobile payment solution, MobilePay , in only 6 months. The advanced yet easy to use MobilePay app allows you to send and receive money via iPhone and Android. Due to the success of MobilePay, Danske Bank wanted to expand the mobile payment landscape and asked Trifork to be a partner in the creation of WeShare . The Challenge The idea was to deliver a settlement solution providing one place where users can share expenses and communicate with groups of friends about it. The Solution It was important to build a highly secure platform, where scalability had the highest priority. Trifork’s role in the process was focused on native app development for iPhone and Android as well as implementing business and database logic in a .NET framework backend. The Results WeShare is an award winning app that makes it easy to keep track of outlays when a group of friends need to go on a trip or activity. It’s possible to share expenses, send pictures and chat in the same app. The app is responsive, visually appealing and easy to use. Fast-tracking the mortgage loan process Client: Bankdata Sydbank Bankdata is owned by nine major Danish banks including Sydbank and is one of the largest financial technology companies in Denmark. The Challenge Eliminate the unnecessary systems and steps in the mortgage loan process Produce a user-friendly interface for bank advisors Provide an improved and more cost-efficient and transparent mortgage solution The Solution Bankdata decided to involve advisors and customers in the development of the new solution by joining the Trifork Accelerate® programme . The team prototyped and ran user tests before going to production. Trifork’s role as a full-stack technology partner involved taking an outdated legacy system and transforming it into the primary tool for bank advisors. The Results A flexible and scalable platform with seamless integration to Bankdata, Totalkredit and other third-party legacy systems 2,400 bank advisors using the solution 1,200 calculations made every week 2.5 hours reduced time per case Payroll solution raising the bar in user experience Client: ProLøn As one of Denmark’s oldest and most experienced payroll companies, ProLøn’s renowned customer service has helped them differentiate from their competitors along with their wide range of custom and standard solutions. The Challenge The existing outdated data entry system was to be replaced with a cohesive and user-friendly IT framework for more efficient processes, reduced risk of errors and a seamless data flow between different functions. This required: A more advanced and user-friendly system New APIs, enabling deep integration with other IT systems The Solution User insights were at the core of the process to ensure that the new solution would integrate intuitively into the user’s daily workflow. Trifork conducted a pre-project analysis based on interviews with ProLøn’s customers and employees. After completing a successful proof of concept project, the new payroll system successfully went to production. The Results Transparent, convenient and efficient user experience Positive response from customers to the new user-interface 5% increase in conversion rate Erlang powered blockchain Client: æternity æternity is an open-source, distributed computing platform built on decentralised cryptographic P2P technology. æternity partnered with Erlang Solutions to scale its distributed system and peer-to-peer network to bring its blockchain technology to the masses. The Challenge With scalability issues being one of the major barriers to mass adoption of blockchain technology, innovative and revolutionary solutions were required by the æternity team. The Solution æternity needed to handle large volumes of data in parallel. They also needed a unique state channel to enable off-chain verification of data and smart contracts, allowing for all transactions to be independent of each other to increase transaction speed and scalability while also ensuring privacy. The Results Today, æternity uses Erlang for most of its proof of concept development. This includes the blockchain, contract language, VM, oracle and governance mechanisms. The servers with the highest uptime in the world are Erlang based and Erlang/OTP which allows their team to write code that can respond to many requests in parallel without crashing. Our Work In The FinTech & Blockchain Space Cryptocurrency transactions with Microservices Client: Blox Blox was created to make cryptocurrencies easily and safely accessible to a wider audience through a platform that bridged the gap between banks’ traditional financial networks and blockchains’ distributed transaction networks. The Challenge Users of the existing platform had difficulty in securely self-storing their cryptocurrencies. A review of the existing software showed it wasn’t possible to extend the current software stack to scale to millions of transactions per day. The requirement was for a: Solution that provides auditing out of the box Platform to store and manage cryptocurrencies Safe and accessible service to encourage investment The Solution Trifork built event-based applications able to capture every event that occurs in the application using Axon Framework and the Axon Server event store technology stack. Packaging the applications as containerised Microservices allowed quick deployment of new features and the scaling of services independently. The Results Event sourcing and Microservices to track every transaction in milliseconds A solution for users to securely store their crypto-coins and make safe transactions Production of a trading search engine No delays as transactions take place Concluding thoughts As can be seen from our round-up of last month’s World Fintech Forum , two of the most challenging technology issues for FS firms involve; firstly the decision of whether to build or to buy tech solutions and secondly how to source technically skilled employees. These two factors combined make our work and that of Trifork an essential element of our current and future FinTech clients’ success. Trifork has worked in the banking business for many years developing some of the very first mobile banking solutions, like MobilePay and WeShare. While headquartered in the Nordics, as a truly international company, Trifork are currently extending their innovative FinTech solutions into the UK with the help of Erlang Solutions and our expertise in building scalable and fault-tolerant systems. For more high-quality FinTech content you can join our FinTech mailing list.", "date": "2021-01-13"},
{"website": "Erlang-Solutions", "title": "Two-factor authentication in your crypto wallet: are you safe?", "author": ["Viktor Gergely and Bryan Hunt"], "link": "https://www.erlang-solutions.com/blog/two-factor-authentication-in-your-crypto-wallet-are-you-safe/", "abstract": "Cryptocurrency systems and applications are not under central regulation, and the openness of this world implies a large number of attacks against the various digital currencies. When your user authentication is enhanced by means of two or multi-factor authentication (2FA or MFA) in addition to your username and password, you may feel that access to your wallet is secure. Is this necessarily true? * TL;DR In this blog post I will guide you through several attack scenarios and then suggest approaches to mitigate against credential theft. * Learning from history In 2018, there was an incident where user credentials for the MyEtherWallet website were stolen. The attacker used DNS spoofing to convince users to enter their credentials into a malicious duplicate of the actual site. DNSSEC provides some mitigation of these sorts of attack and will likely play a more prominent role in the future, but for the time being the added security is opt-in via web browser configuration which we will not discuss in more detail here. Obviously the aim of the following scenarios is not to encourage malicious activity, rather to show the vulnerabilities of crypto wallet sites where authentication and credential modification are based upon a particular set of approaches. Simple user/password authentication First, let’s take an example whereby an unauthorized user steals another user’s wallet by misrepresenting a server which they control as the login page of the legitimate crypto wallet website. Alice enters the wallet site address in the browser and is redirected to a phishing site which looks exactly like the official one. She may not even see the difference in the browser address bar if the corresponding DNS records were replaced. (Another very common trick is to register a domain name which is very similar to the existing site (typosquatting) but which differs by a single character, for example, elang-solutions.com rather than erlang-solutions.com). Bob, who is operating the phishing site, captures Alice’s username and password and uses them to authenticate to the actual crypto wallet site, as shown in Figure 1. Alice may notice some inconsistency after logging in, but it’s too late. Bob has been granted access to Alice’s wallet already. Adding two-factor authentication Let’s extend the last example with the addition of two-factor authentication (2FA). The system sends a one-time password via SMS to a phone number associated with Alice’s account. The one-time password needs to be supplied by Alice as well as providing her username and password. Two-factor authentication enhances security by ensuring that you not only need to supply a credential but you also (typically) need to be in possession of something, in this case a mobile phone but it could also be a cryptographic token generator, one-time-pad, or a particular email address . As before, Alice is redirected to the phishing site and enters the username and password. Bob forwards Alice’s credentials to the real wallet site. Bob changes the phishing site to ask for the secondary credential. Meanwhile, the official site sends an SMS to Alice containing the one-time password. Alice doesn’t know that she has been redirected, so she enters the code received in SMS on Bob’s site. In common with the example of basic username/password theft, Bob captures this data too, and enters it into the official site, as shown in Figure 2. For the official site, the real identity of the user is transparent, it doesn’t know that Bob has logged in instead of Alice. In fact, the SMS-based two-factor authentication doesn’t add more security in this case. Why is this so? Here we see a typical man-in-the-middle (MITM) attack. From the system’s point of view, the communication channel for authentication is blocked by the attacker. When entering the required username, password and secondary credential, the attacker can emulate all of these activities. Even though the system uses several channels for sending data to the client (web browser, mobile phone), the authentication data from the client is supplied to the system via a single channel. All the credentials supplied by Alice in order for the system to perform authentication traverses the same path: Alice -> phishing site -> Bob -> wallet. How can we overcome this attack, better still, how can we detect, and mitigate against such attacks? A possible solution: using multiple channels Use separate paths for user -> system communication, preferably via encrypted channels. A typical example of this is registration activation via email. The wallet server stores a piece of information internally that should be matched with the information in the email, by clicking on the link in the email for example. When Alice clicks the link, and Bob catches the communication, the attack can still be successful, but if this communication goes through another site, the information will be sent from Alice directly to the wallet which enables Alice (and not Bob) to complete the authentication, as shown in Figure 3. The Elixir-based Phoenix Framework offers several ways to secure your communication channels. You can configure your site to support email sending via SMTP with relatively simple configuration. For security reasons, it is important to store confidential information separately from the source code, and be aware that the security of email sending heavily depends on the settings of the particular SMTP solution. It goes without saying but at the very minimum you should enable TLS transport using certificates, and force all incoming requests to use TLS. This can also be configured for Phoenix. How can we be safer? Even if the login authentication has been hacked, and the hacker can access parts of our private account data, it is generally useful to prevent creating transactions without additional confirmation and authentication from the user. Although creating a phishing site to steal login information seems relatively simple by replicating the design of the official web pages, emulating the account management interface is much more complex, and the hack can easily be detected by the client. It is also possible to restrict the usage of personal data, namely account information and private keys. The client should only retrieve private information connected to the account when needed, without caching it in the server state or cookies. This applies more strictly to the private key: only fetch the key when initiating a transaction. It is also preferred not to store the key in its unencrypted form in memory. If this level of security is insufficient, there is an option to use cold storage, which means that your cryptocurrency wallet is stored offline. Conclusions When providing a service on a public network, you need to prepare for a wide range of different attacks against the weak points in your system. Even in the case of a simple authentication process, there are various candidates for improvement. In order to protect data in cryptocurrency wallets, the user interface must meet the highest possible security requirements. Based on our growing presence and experience in fintech services, we are happy to help you with analyzing and improving the level of security of your existing system, or building a system that complies with the up-to-date security standards. Find out more about what we do in the FinTech and blockchain space. For more high-quality FinTech content you can join our FinTech mailing list. References DNSSEC Deployment Coverage Two-Factor Authentication in Elixir and Phoenix Protect your private keys Cold storage Phoenix SSL configuration Phoenix email sending", "date": "2020-11-26"},
{"website": "Erlang-Solutions", "title": "Receiving messages in Elixir – How to avoid performance issues.", "author": ["Oleg Tarasenko"], "link": "https://www.erlang-solutions.com/blog/receiving-messages-in-elixir-or-a-few-things-you-need-to-know-in-order-to-avoid-performance-issues/", "abstract": "We’re kicking off #ElixirOverload with Oleg Tarasenko’s post on receiving messages in Elixir! What can you do to avoid common mistakes that plague developers? Here, Oleg makes a performance comparison of message processing depending on the process mailbox size, amongst other fantastic insights. This is just the beginning. We are dedicating one week to the brilliant Elixir community with themed content all about Elixir . We have four fresh blog posts with Oleg’s here whetting the appetite. Why have we gone all Elixir? As if there needs to be an excuse to talk Erlang OR Elixir, but in this case, we are offering new Elixir Architecture Sessions at Erlang Solutions. In addition, we are present and counting at ElixirConf US this week with one of our senior developers Claudio Ortolina there in the thick of it listening to fantastic talks over the four days. You can also request Empirical Monkeys training with friend of Erlang Solutions, Rafal Studnicki. If you have any questions for Oleg…and later on for Joe and Bart, you can contact us at general@erlang-solutions.com . Keep up-to-date with all things #ElixirOverload and beyond through our Erlang Solutions Twitter , including Claudio’s #TwitterTakeover at ElixirConf 2018. As you know Elixir programs use processes to run pretty much everything, and in order to communicate between processes, Elixir uses message passing. In this blog post we cover scenarios that could result in degraded messaging performance, which in turn can ruin your overall application performance. Sounds interesting? Of course it does! Find out more below… Sending and receiving messages Messages are sent using the send/2 function and received using the receive do construct. In practice, the simplest way to observe the behaviour of sending and receiving messages is to open up an Elixir shell and execute the following: iex(1)> send(self(), :test_message)\n:test_message The example code shown above will send a :test_message atom to the mailbox of the current shell process. Let’s send several other atoms to ourselves and see what happens when we start reading them back from our process mailbox: iex(2)> send(self(), :test_message1)\n:test_message1\niex(3)> send(self(), :test_message2)\n:test_message2\niex(4)> send(self(), :test_message3)\n:test_message3\niex(5)> :erlang.process_info(self(), :message_queue_len)\n{:message_queue_len, 4} As we can see from the snippet above, every time we send a message to a particular process, it’s stored in that process’ mailbox. We now have 4 messages; lets fetch them from the mailbox using the receive construct! iex(8)> receive do msg -> IO.puts(\"Received message: #{inspect(msg)}\") end\nReceived message: :test_message\n:ok\niex(9)> receive do msg -> IO.puts(\"Received message: #{inspect(msg)}\") end\nReceived message: :test_message1\n:ok\niex(10)> receive do msg -> IO.puts(\"Received message: #{inspect(msg)}\") end\nReceived message: :test_message2\n:ok\niex(11)> receive do msg -> IO.puts(\"Received message: #{inspect(msg)}\") end\nReceived message: :test_message3\n:ok\niex(12)> receive do msg -> IO.puts(\"Received message: #{inspect(msg)}\") end As you can see, messages are received in the same order they were transmitted. You can also see that the last receive blocks the shell, which is left waiting to fetch the next message from the process mailbox. A closer look at the receive block Elixir’s receive macro is used in the following way: receive do\n  pattern1 -> :process_message_pattern1\n  pattern2 -> :process_message_pattern2\n  _  -> :process_catch_all_case\nafter\n 1000 -> :ok\nend This code takes the first message from the mailbox and will then try to match it against all the patterns defined in the receive block. If the first message can’t match both pattern1 and pattern2, it will be matched by the catch all () case, and :process_catch_all_case will be returned in this case. Finally, if the process’ mailbox is empty, the code will block new messages to arrive and continue the execution after the timeout interval (1000 milliseconds) expires. This process can be visualised in the following diagram: Receiving messages with “a priority” Let’s now look at another example of the receive construct: receive do\n  pattern1 -> :process_message_pattern1\n  pattern2 -> :process_message_pattern2\nafter\n timeout -> :ok\nend Despite the visual similarity, there is a tiny difference that will make the code act in a completely different way. With the removal of the catch all case, the following will happen: The first message will be taken from the mailbox The code will try to match the message against pattern1 and pattern2 If one option is a match, the appropriate branch will be evaluated If neither pattern matches, the code will perform steps 1-2 for the entire mailbox, placing all processed but unmatched messages into a temporary space (see more details about the save queue in the following article ). The above algorithm can be visualised in the following diagram: This approach can be used to implement priority-based message processing. Let’s consider the following example: defmodule MailBoxPriorityExample do\n  def run() do\n    pid = spawn(&recv/0)\n    send(pid, :message1)\n    send(pid, :message2)\n    send(pid, :message3)\n    :ok\n  end\n\n  def recv() do\n    receive do\n      :message3 -> IO.puts(\"Got message 3\")\n    end\n\n    receive do\n      :message2 -> IO.puts(\"Got message 2\")\n    end\n\n    receive do\n      :message1 -> IO.puts(\"Got message 1\")\n    end\n\n    :ok\n  end\nend The above code will process mailbox messages in the reverse order: iex(1)> MailBoxPriorityExample.run\nGot message 3\nGot message 2\nGot message 1\n:ok This prioritised receive is also known as a selective receive. The cost of using a selective receives As you can see from diagram 2, the selective receive will scan the entire process mailbox in order to dequeue a matching message. This is not a huge issue when your processes are not under heavy load. However, as soon as other parts of your subsystem are actively sending messages to your process, it can quickly become a bottleneck. Let’s build an example that illustrates the dependency between the mailbox queue length and performance of the selective receive. In order to prepare this experiment we have created the following code snippet: defmodule SelectiveReceive do\n  def setup(num_messages, num_stale_messages) do\n    stats_pid = spawn(fn -> stats(num_messages) end)\n    recv_pid = spawn(fn -> recv(stats_pid) end)\n\n    # Fill recv process with unprocessable messages\n    Enum.map(1..num_stale_messages, fn _ -> send(recv_pid, :unexpected) end)\n\n    # Send regular messages to recv process\n    Enum.each(1..num_messages, fn _ -> send(recv_pid, :expected) end)\n  end\n\n  # Receive :message \n  def recv(stats_pid) do\n    receive do\n      :expected -> send(stats_pid, :ok)\n    end\n\n    recv(stats_pid)\n  end\n\n\n  # Receive messages from receiver, count total time.\n  def stats(num) do\n    ts1 = Time.utc_now()\n\n    Enum.each(1..num, fn _ ->\n      receive do\n        _ -> :ok\n      end\n    end)\n\n    ts2 = Time.utc_now()\n    diff = Time.diff(ts2, ts1, :millisecond)\n    rps = Float.ceil(num / diff * 1000, 2)\n\n    IO.puts(\"Throughput is: #{rps} requests per second\")\n  end\nend The chart below shows a correlation between the size of the mailbox and the decreased throughput of the selective receive. Please note that the numbers can vary from machine to machine. It’s also worth mentioning that the example itself is somewhat synthetic, as the processes were just sending messages without performing any processing (a somewhat unlike real-world scenario). Ok, but what about the Real World? As previously discussed the above example is not representative of real-world scenarios due to its synthetic nature so what about the real-world? The following steps would represent a more typical real life scenario: You have one process which normally handles X requests per second; You have a short spike of incoming messages (due to some unexpected external factor) The queue size grows and now your process is 40% slower The queue size continues to grow, despite the fact that the spike of activity has already finished and causes a further slowdown of the process. Inside OTP? You may think. “Ok, it doesn’t look like I would want to use the selective receive. Why would I want it?”. In reality people are using selective receive for to numerous reasons. Here are some examples of selective receives usage inside OTP: https://github.com/erlang/otp/blob/master/lib/mnesia/src/mnesia_locker.erl#L133-L144 https://github.com/erlang/otp/blob/master/lib/mnesia/src/mnesia_locker.erl#L770-L800 https://github.com/erlang/otp/blob/master/lib/mnesia/src/mnesia_log.erl#L642-L644 https://github.com/erlang/otp/blob/master/lib/mnesia/src/mnesia_checkpoint.erl#L800-L854 https://github.com/erlang/otp/blob/master/lib/sasl/src/release_handler.erl#L1659-L1664 https://github.com/erlang/otp/blob/master/lib/ssl/src/inet_tls_dist.erl#L253-L260 https://github.com/erlang/otp/blob/master/lib/ssl/src/inet_tls_dist.erl#L409-L424 Conclusion Selective receive is an interesting functionality that comes built into Erlang/Elixir. As with every other tool it has both strengths and weaknesses. Selective receive provides some advantages when working with relatively small message boxes (namely prioritised message processing), however, using selective receive without being aware of the potential costs can put your overall application stability at risk. References Elixir Processes from elixir-lang.org: https://elixir-lang.org/getting-started/processes.html", "date": "2018-09-06"},
{"website": "Erlang-Solutions", "title": "20 Years of Open Source Erlang: OpenErlang Interview with Anton Lavrik from WhatsApp", "author": ["Erlang Solutions"], "link": "https://www.erlang-solutions.com/blog/20-years-of-open-source-erlang-openerlang-interview-with-anton-lavrik-from-whatsapp/", "abstract": "May the 20th anniversary celebrations of open sourced Erlang never end! And we don’t intend to slow down. In fact, we are speeding things up as the #OpenErlang party in London fast approaches (you still have time to register and join us on 8th Nov). Our upcoming #OpenErlang interviews will share more insights on how global companies such as WhatsApp and AdRoll achieved the unachievable, with Erlang being their secret weapon. Erlang – WhatsApp secret weapon to conquer the globe! WhatsApp runs on Erlang, and happening to be sponsoring our #OpenErlang London Party in early November. Currently there is 1 billion daily active WhatsApp users, sending 6 billion messages and 4.5 billion photos every day! And there is over 55 billion WhatsApp calls made every day. The ability to process this amount is astonishing and we want to know more about WhatsApp ability to manage their system and provide a smooth experience to their users. Next in our #OpenErlang interviews we host WhatsApp Server Engineer Anton Lavrik who shares with us why he loves Erlang and how it is used at WhatsApp with tremendous success. About Anton Anton came across Joe Armstrong’s PhD thesis on Erlang 15 years ago as part of his own PhD, and he’s been a supporter of the language ever since, having actively used Erlang for over a decade now. Whilst he has been using Erlang for a while now, Anton has worked over numerous domains including embedded and real-time systems, domain-specific languages and programming tools, large-scale data collection and processing systems, custom analytic databases, and analytic stacks. Anton began his programming career in 2001 and has since worked as a Technical Lead for Alert Logic amongst other roles before moving to WhatsApp. About WhatsApp WhatsApp was founded in 2009 by ex-Yahoo! Employees Brian Acton and Jan Koum. After buying an iPhone, Koum quickly realised the gaping hole that WhatsApp would eventually fill and the pair found a developer on RentACoder.com named Igor Solomennikov to turn their idea into reality. The early versions of the app would often crash to the point where Koum was considering packing it in to pursue other ventures. His business partner Acton convinced him to stay and just a few months later in June 2009, Apple launched push notifications which would be vital in the evolution of WhatsApp. Users quickly increased to 250,000. The growth was so sudden that the team decided to change WhatsApp to a paid service (just $1 yearly subscription) as the verification texts were costing the small company too much. By December 2009, you could now send photos via the application as well. Fast-forward to December 2013 – WhatsApp has 400 million active users every month. Fast-forward again to February 2017 – WhatsApp has over 1.2 billion users globally. And of course, it is a free service. Today, we have over 1 billion daily active users – just wow! Some other stats to tantalise the tastebuds courtesy of expandedramblings.com : 450 million daily active users 100 million daily voice calls 70% of users that use WhatsApp on a daily basis 65 billion WhatsApp messages are sent daily There are 1 billion WhatsApp groups 4.5 billion photos are shared through on the app daily 3 million companies use WhatsApp for business purposes. Why WhatsApp uses Erlang Everyone knows WhatsApp – it’s the most popular messaging application that has ever been created – but in terms of the backend? This is often something we don’t think about. WhatsApp will successfully send your message and we all carry on with the rest of our day. WhatsApp uses a surprisingly small amount of engineers for the billions of users it caters to on a daily basis. How do they manage this? Like many applications Erlang is involved in, it becomes the one essential cog that all the smaller cogs revolve around. One of Erlang’s best attributes is concurrency – it is the best multi-tasker out there when it comes to programming languages. Others may try, but they simply can’t run multiple messages and multiple parallel conversations with the efficiency of Erlang. Not only this, but bugs and updates can be fixed and installed without downtime. Erlang was built to solve very specific problems, in particular scaling a large system with it still remaining highly reliable. Those are the properties that make Erlang so appealing. Not only that but programmers love the language – coders can solve problems on the go and fast! WhatsApp has completely monopolised the messaging application industry, and Erlang controls it! Interview Transcript At work with the boss breathing down your neck? Or don’t want to be one of those playing videos out loud on public transport? Here’s the transcript, although not as exciting as the real thing. Anton Lavrik: I came across Erlang about 15 years ago when I was doing research as a part of my PhD and I stumbled across Joe Armstrong PhD thesis. I got really excited because it basically solved a lot of problems that we were trying to solve using arcane and inefficient techniques. WhatsApp started with Erlang and we stuck to it. At WhatsApp, we use Erlang for pretty much everything. We’re essentially running on Erlang. Most of our server code is written in Erlang. It would have been really, really difficult to achieve the same result by using any other existing technology. It’s been such an amazing fit for what we do. We’ve done so much with it. In many ways, we changed the world. We also learned how to use this technology really efficiently and push boundaries. Many alternative technologies that people use for solving these type of problems, they come short in several different areas. They’re much less efficient for solving problems in this domain. People might choose to implement things in lower level languages like C++ where they have more optimisation opportunities, but then they have to implement half of Erlang by themselves, or some of the scripting languages which may offer more rapid development or prototyping, they wouldn’t be able to scale the system. Especially while keeping it reliable. Erlang has an amazing set of really powerful ideas and techniques behind it. A lot of other languages and environments are trying to learn from it and borrow its features, getting a great exposure. Another aspect of it is that many people from technology actually start using Erlang and get attracted by Erlang. They can experience all those great benefits firsthand. What I like about being a software engineer? It’s a really wonderful mix of creativity and solving real problems. [00:02:41] [END OF AUDIO] OpenErlang; 20 Years of Open Sourced Erlang Erlang was originally built for Ericsson and Ericsson only, as a proprietary language, to improve telephony applications. It can also be referred to as “Erlang/OTP” and was designed to be a fault-tolerant, distributed, real-time system that offered pattern matching and functional programming in one handy package. Robert Virding, Joe Armstrong and Mike Williams were using this programming language at Ericsson for approximately 12 years before it went open source to the public in 1998. Since then, it has been responsible for a huge number of businesses big and small, offering massively reliable systems and ease of use. OpenErlang Interview Series As mentioned, this isn’t the first in the #OpenErlang Interview series. We have three more existing videos to enjoy. Robert Virding and Joe Armstrong It only seems fitting to have launched with the creators of Erlang; Robert Virding and Joe Armstrong (minus Mike Williams). Robert and Joe talk about their journey with Erlang including the early days at Ericsson and how the Erlang community has developed. Christopher Price Last week was the launch of our second #OpenErlang Interview from Ericsson’s Chris Price. Currently the President of Ericsson’s Software Technology, Chris has been championing open source technologies for a number of years. Chris chats to us about how Erlang has evolved, 5G standardization technology, and his predictions for the future. Jane Walerud Jane is a serial entrepreneur of the tech persuasion. She was instrumental in promoting and open sourcing Erlang back in the 90s. Since then, she has continued her entrepreneurial activities, helping launch countless startups within the technology sector from 1999 to present day. Her work has spanned across many influential companies who use the language including Klarna, Tobil Technology, Teclo Networks and Bluetail, which she founded herself. Other roles have included Member of the Board at Racefox, Creades AB and Royal Swedish Academy of Engineering Sciences, and a key role in the Swedish Government Innovation Council. Simon Phipps Having become an open source programming language, Erlang was allowed to flourish. It gained a passionate following which has since developed into a close community. Simon Phipps dedicates his time to open source promoting languages such as Erlang through the Open Source Initiative and other similar schemes. Why are open source languages such as Erlang so important? Find out more! Other Erlang Solutions Activities… OpenErlang London Party It’s time to party! We have partnered with WhatsApp and æternity to hold a special Erlang celebration in London this November! Tickets are free and you’re all invited. Sign up here to RSVP and join us for delicious food, free-flowing drinks, and entertainment! 16 lessons I learnt using the BEAM Our October webinar was by Joseph Yiasemides has he talked us through the lessons he has learnt over the years. You may have missed the webinar, but it is available now on YouTube . Sign up to our webinars newsletter to get specific invites to our monthly webinars. If you’re interested in contributing and collaborating with us at Erlang Solutions, you can contact us at general@erlang-solutions.com .", "date": "2018-10-24"},
{"website": "Erlang-Solutions", "title": "20 Years of Open Source Erlang: The OpenErlang Interviews", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/20-years-of-open-source-erlang-the-openerlang-interviews/", "abstract": "Here we are, careering into 2019, and Erlang still remains as strong as ever. What’s the secret? All year we’ve been celebrating #OpenErlang and you have surely come across our amazing video series. We’ve spoken to some of the key enthusiasts that have been championing Erlang/OTP! This includes Anton Lavrik from WhatsApp, probably one of the most prolific companies to use Erlang to date with 1 BILLION active daily users. And how about AdRoll? Staff Engineer Miriam Pena spoke of Erlang participating in over one million actions per second. You can check out the other companies that use Erlang as well. Erlang has the ability to scale tremendously, but it has also understandably made waves in the open source community – the social side of programming. The President of the Open Source Initiative, Simon Phipps, spoke of Erlang’s strong following, and how Microsoft had made a “delicious” U-Turn from the Linux operating system to Erlang. Lastly, we spoke to past and present employees of Ericsson! Jane Walerud was a hugely integral part in making Erlang open source in 1998, whilst current President of Ericsson Software Technology Christopher Price gave us further insight into Ericsson’s 5G development using Erlang. Here we have a look back at all of the #OpenErlang videos that began back in August with Robert Virding and Joe Armstrong and ended with Francesco’s retrospective from 1998 to 2018. The Erlang Creators; Robert Virding and Joe Armstrong We interviewed Robert and Joe during the #OpenErlang Stockholm party, and it felt only right to begin our Erlang journey with those who know it best; the co-creators of Erlang! Mike Williams was absent during our Stockholm Party, but of course there in spirit, and as always Robert and Joe were as enthusiastic about Erlang now as they were in 1998. You can read the full article including the interview, facts about Robert and Joe, and the Erlang journey. The President of Ericsson Software Technology; Christopher Price Chris spends his time developing and promoting open source communities, not only at Ericsson but at OpenStack where he’s on the Board of Directors and the OpenDaylight Project as an Ambassador. These are just his current roles! What was interesting, speaking to Chris, is the development in 5G technology. Ericsson anticipates that by 2023, 20% of the world’s population will be covered by 5G and it intends to get the ball rolling. You can read more about Chris , the legacy of the Ericsson Labs, and what Ericsson is doing including its 5G standardisation. The Serial Tech Entrepreneur; Jane Walerud A “serial tech entrepreneur” is the best way to describe Jane. She has invested in a number of tech startups since the 90s and was instrumental in promoting the fine qualities of Erlang back in 1998 when she campaigned to make it open source. In fact, she used Erlang when founding Bluetail back in 1999 with Robert Virding. Erlang can lend itself to other languages, such as Elixir, which she describes in her #OpenErlang interview. Jane has always been impressed with the robust nature of Erlang. The fact you can run it on multiple devices, and if one breaks? Doesn’t matter. Read more about Jane’s inspiring Erlang journey, and how Erlang went from Ericsson’s “secret weapon” to open source . The Open Source Freedom Fighter; Simon Phipps Simon’s passion is open source. He created the Open Source Initiative back in 1998, in the same month that the term “open source” was coined! Since then, Simon has been involved in so many organisations that champion open source languages, including being the President of the Open Source Initiative. “Software freedom is the essential ingredient for the profitability of companies using Open Source.” as Simon states in his #OpenErlang interview and we couldn’t agree more! Learn more about the OSI and Simon’s work . The Global Engineer; Anton Lavrik Anton has used Erlang for over 10 years and he isn’t interested in stopping now. Currently one of a few server engineers at WhatsApp (they don’t exactly need many, Erlang is that good ;)), you might be tricked into thinking that Anton has his hands full. Due to Erlang’s ability to scan across multiple devices, it’s the perfect solution when dealing with billions of users, messages, videos and photos EVERY DAY. Get the facts and stats surrounding WhatsApp, how such a huge company uses Erlang and why Anton has been using OTP for so long. The Money Maker; Miriam Pena AdRoll has generated over $7 billion in revenue for their customers, resulting in Erlang dealing with 1.5 million actions PER SECOND, says Engineer Miriam. She has worked with Erlang, Elixir, AWS, Python, DynamoDB, Kinesys, and Memcached. She loves working with distributed, high concurrency systems, as you can probably tell! Erlang’s ability to scale regardless of the size of the business is a great asset, and quite poetic considering its very own open source story. Find out more about Miriam’s expertise, AdRoll and read the full transcript at our dedicated blog post . The Erlang Creator Part II; Robert Virding Erlang was created in the mid-80s, and of course, it only became open source over a decade later. We concluded our #OpenErlang interviews with a very special insight from Robert about the importance of Erlang, and all other open source languages that have come after it. As one of the early members of the Ericsson Computer Science Lab, we’re always extremely proud to say that Robert is a Principal Language Expert at Erlang Solutions. The #OpenErlang Parties As mentioned, we’ve been partying in style during 2018 with retro gaming and world-class talks in tow. We’ve visited San Francisco, Stockholm and London, and have arranged webinars and meetups all celebrating this momentous occasion. If you haven’t already seen, our highlights video is a great summary of why the open source community is still flourishing, from those in the thick of it!", "date": "2018-11-30"},
{"website": "Erlang-Solutions", "title": "Twenty Years of Open Source Erlang", "author": ["Francesco Cesarini"], "link": "https://www.erlang-solutions.com/blog/twenty-years-of-open-source-erlang/", "abstract": "Erlang was released as Open Source Tuesday December 8th, 1998. Do you remember where you were that week? I was in Dallas (Texas); one of my many visits helping the Ericsson US branch set up an Erlang team working with the AXD301 switch. Waking up on Tuesday morning, I got the news. The release was uneventful. There was no PR around the release, no build up or media coverage. Just a sparce erlang.org website (handcrafted using vi). An email was sent to the Erlang mailing list , a post made the front page on slasdot , alongside a mention on comp.lang.functional (which Joe dutifully followed up on). There were no other marketing activities promoting the fact that Ericsson had released a huge open source project. My highlight that week was not the release of Erlang, but going to a Marky Ramone and the Intruders gig in a dive in downtown Dallas. Little did I know how Open Source Erlang would affect the tech industry, my career, and that of many others around me. Getting it out of Ericsson What made it all happen? Many of us wanted Erlang to be released as open source, for a variety of reasons. Some of my Ericsson colleagues wanted to leave their current positions, but continue building products with what they believed was a silver bullet. Others wanted to make the world a better place by making superior tools for fault tolerant and scalable systems available to the masses. For Ericsson management, a wider adoption of Erlang would mean a larger pool of talent to recruit from. Jane Walerud was amongst us trying to sell Erlang outside of Ericsson and one of the few who at the time knew how to speak to management; she understood that the time of selling programming languages was over. Håkan Millroth, head of the Ericsson Software Architecture Lab suggested trying this new thing called “Open Source”. Jane, armed with an early version of The Cathedral and the Bazaar paper , convinced Ericsson management to release the source code for the Erlang VM, the standard libraries and parts of OTP. Until Erlang was out, many did not believe it would happen. There was a fear that, at the last minute, Ericsson was going to pull the plug on the whole idea. Open Source, a term which had been coined a few months earlier, was a strange, scary new beast large corporations did not know how to handle. The concerns Ericsson had of sailing in uncharted territory, rightfully so, were many. To mitigate the risk of Erlang not being released, urban legend has it that our friend Richard O’Keefe , at the time working for the University of Otago in New Zealand, came to the rescue. Midnight comes earlier in the East, so as soon as the clocks struck midnight in New Zealand, the erlang.org website went online for a few minutes. Just long enough for an anonymous user to download the very first Erlang release, ensuring its escape. When the download was confirmed, the website went offline again, only to reappear twelve hours later, at midnight Swedish time. I was in Dallas, fast asleep, so I can neither confirm nor deny if any of this actually happened. But as with every legend, I am sure there is a little bit of truth behind it. The Dot Com Bubble Era Adoption in first few years was sluggish. Despite that, the OTP team, lead by Kenneth Lundin was hard at work. In May 1999, Björn Gustavsson’s refactoring of the BEAM VM (Bogdan’s Erlang Abstract Machine) becomes the official replacement of the JAM (Joe’s Abstract Machine). Joe had left Ericsson a year earlier and the BEAM, whilst faster, needed that time to make it production ready. I recall the excitement every time we found a new company using Erlang/OTP. Telia, the Swedish phone company, was working on a call center solution. And One2One – the UK mobile operator – had been initially using it for value added services, expanding its use to the core network. IdealX in Paris, did the first foray into messaging and XMPP. Vail System in Chicago and Motivity in Toronto were using it for auto dialler software. And Bluetail, of course, had many products helping internet service providers with scalability and resilience. The use of Erlang within Ericsson’s core products continued to expand. This coincided with my move to London in 1999, where I increasingly came across the need for Erlang expertise within Ericsson. Erlang Solutions was born. Within a year of founding the company, I had customers in Sweden, Norway, Australia, Ireland, France, the US, and of course, the UK. In 2000, we got our first non Ericsson customer; training, mentorship and a code review for IdealX in Paris. It was the Bluetail acquisition by Alteon Web Systems for $152 million (a few days later Alteon were acquired by Nortel), which sent the first ripples through the Erlang community. An Ericsson competitor developing Erlang products! And a generation of successful entrepreneurs who had the funds to get involved in many other startups; Synapse, Klarna and Tail-f being some of them. Soon after the Bluetail success comes the dot com crash, the industry went into survival mode, and then later recovery, mode. The crash, however, did not affect academics who were moving full steam ahead. In 2002, Prof. John Hughes of Chalmers University managed to get the Erlang Workshop accredited by SIGPLAN and the ACM. We didn’t really know what this all meant, but were nonetheless, very proud of it. The ACM SIGPLAN Erlang Workshop in Pittsburgh (Pennsylvania) was the first accredited workshop. Here, a PhD student from Uppsala University named Richard Carlsson presents the Erlang version of try-catch to the world. In September 2004, Kostis Sagonas, from Uppsala University hijacks the lightning talks at the ACM SIGPLAN Erlang workshop in Snowbird (Utah) and gives the first public demo of Dialyzer. He runs it on a code base from South African Teba Bank. It was the first of many amazing tools he and his students contributed to the ecosystem. Erlang had for a long time been used to teach aspects of computer science in many Universities all over the world. This in turn lead to research, Master Thesis and PhD projects. The workshop provided a forum for academics to publish their results and validate them with industrial partners. Downloads from the erlang.org site kept on increasing, as did adoption. In 2003, Thomas Arts, program manager at the IT University of Gothenburg, invites me to teach an Erlang course to his undergraduate class. Prof. John Hughes, despite already knowing Erlang, wanted to learn about it from someone who had used it in production, so he joins the class. One morning, he shows up to class tired, having pulled an all nighter. He had developed the first version of Erlang QuickCheck, which he was dutifully using to test the course exercises. That was the start of Quviq and a commercial version of QuickCheck, a property based testing tool second to none. I ended up teaching at the IT University for a decade, with over 700 students attending the course. Getting Into Messaging During the dot com crash, Alexey Shchepin starts work on an XMPP based instant messaging server called ejabberd. After working on it for three years, he releases version 1.0 December 1st, 2005. Facebook Chat forks it, rolling out a chat service to 70M users. At around the same time, Brian Acton and Jan Koum founded WhatsApp, also based on a fork of Ejabberd. As forking Ejabberd was all the hype, MongooseIM did the same, becoming a generic platform for large scale messaging solutions. In May 2006, RabbitMQ is born, as we find out that work was underway to define and implement a new pub/sub messaging standard called AMQP. RabbitMQ is today the backbone of tens of thousands of systems. By the end of the decade, Erlang had become the language of choice for many messaging solutions. The Multi-Core Years It was not only Universities innovating during the dot com recovery. In May of 2005, a multi-core version of the BEAM VM is released by the OTP team, proving that the Erlang concurrency and programming models are ideal for future multi-core architectures. Most of the excitement was on the Erlang mailing list, as not many had realised that the free lunch was over. We took Ejabberd, and just by compiling it to the latest version of Erlang, got a 280% increase in throughput when running it on a quad-core machine. In May 2007, the original reel of the 1991 Erlang the Movie was anonymously leaked from a VHS cassette in an Ericsson safe and put on the erlang.org site, eventually making its way to YouTube. Still no one has publically taken responsibility for this action. The world, however, finally understood the relief felt by those still under Ericsson NDA that none of the computer scientists featured in the film had given up their day jobs for a career in acting. The film got a sequel in 2013 , when a hipster tries to give Erlang cool look. This time, the curpruit releasing it is identified as Chicago resident Garrett Smith . In 2007, Programming Erlang by Joe Armstrong is published by the The Pragmatic Programmers. The following year, in June 2008, I held the first paper copy of Erlang Programming ; a book Simon Thompson and I had spent 18 months writing. At the time, an O’Reilly book was the seal of approval that emerging programming languages needed, giving way to many other fantastic and diverse range of books in many languages. The book launch party happened in conjunction with the first commercial Erlang conference, the Erlang eXchange in London June 2008. It was not the first event, as Bjarne Däcker, the former head of the Ericsson Computer Science Lab, had for almost a decade been running the yearly Erlang User Conference in Stockholm. But November in Sweden is cold, and the time had come to conquer the world. The Erlang eXchange gives way to the first Erlang Factory, taking place in March 2009 in Palo Alto (California). Much more exotic, though equally beautiful locations. For the first time, the European Erlang community meet their American peers. We all got on like a house on fire, as you can imagine. At the conference, Tony Arcieri presents Reia , a Ruby flavoured version of Erlang running on the BEAM. Who said that a Ruby like syntax is a bad idea? Other presenters and attendees that year had stellar careers as entrepreneurs and leaders in the tech space. An Erlang user in the US at the time was Tom Preston Werner. He was using it to scale the Ruby front-end of a social coding company called Github. In November of 2009, when in Stockholm for the Erlang User Conference, I introduced him and Scott Chacon to the OTP team. They spent an afternoon together, prompting the OTP team to move the development of Erlang to github, making it its main repository. Conferences spread all over the world. Events have been held in Amsterdam, Bangalore, Berlin, Buenos Aires, Brussels, Chicago, (many places I can not spell in) China, Krakow, Los Angeles, Paris, Moscow, Mexico City, Milan, Munich, New York, Rome, San Francisco, St Andrews, Tel Aviv, Vancouver, Washington DC and many many other places. The Cappuccino Years In 2010, I teach my first graduate class at Oxford University. Erlang was picked for the Concurrency Oriented Programming course. It was also the year Bruce Tate’s Seven Languages in Seven Weeks was released. It was through this book where one of Rails’ core committers, José Valim, realized that Erlang was ahead of everyone in the concurrency race because it also tacked distribution. In January 2011, the first commit in the Elixir repo happens. The results are presented the following year at the Krakow Erlang Factory, reaching version 1.0 in September 2014. Like all successful languages, he was trying to solve a problem, namely bringing the power of Erlang to wider communities, starting with Web. The time was right. In January 2012, WhatsApp announce that by modifying FreeBSD and the BEAM, they achieved 2 million TCP/IP connections in a single VM and host. Their goal was to reduce operational costs, running a scalable service on a hardware footprint that was as small as possible. These results were applicable to many verticals, the Web being one of them. The same month as the WhatsApp announcement, a group of companies pool together knowledge, time and resources to create the Industrial Erlang User Group. They worked with Ericsson to move Erlang from a derivative of the Open Source Mozilla Public License to the Apache Licence, contribute to the dirty scheduler, get bug tracking tool launched, fund a new erlang.org site, launch Erlang Central, and worked together with an aim of setting up a foundation. Elixir Comes of Age In July 2014, Jim Freeze organises the first Elixir Conference in Austin (Texas). There were 106 attendees, including keynote speaker Dave Thomas’ dog. Chris Mccord presented Phoenix, rising from the ashes. Robert Virding and I are part of the lineup and I recall my message loud and clear: just because you know Ruby, don’t believe them when they tell you that learning Elixir is easy. Your challenge will be thinking concurrently. The main idea behind Elixir is concurrency, and knowing how to deal with it is critical to the success of the project. A year later, in August 2015, Phoenix 1.0 was released. It had the same effect Rails had on Ruby, bringing people to Elixir. Now, you didn’t need to master concurrency to get it! Nerves came along soon after, moving Elixir away from being a language solely for the web. At Elixir Conf, I spoke about the book I was co-authoring with Steve Vinoski, Designing For Scalability with Erlang/OTP. At the time, it was available as a beta release. Little did I know that I had to wait for June 2016 to hold to a paper copy. The last four chapters, which should have been a book on their own, ended up taking 1.5 years to write. The main lesson to others writing a book is that if your partner tells you “you are going to become a father”, you have 8 months to finish the book. The alternative is you ending up like me, attending the release party a few days before your second child is due. The book was dedicated to Alison, Peter and our baby bump. Baby bump was born in early July, bringing truth to the Erlang saying that “you do not truly understand concurrency until you have your second child”. The Erlang Ecosystem Throughout 2016, Elixir adoption kept on growing. Conference talks on Lisp Flavoured Erlang and Effene – two other languages on the BEAM – revealed they had code running in production. New experimental ports kept appearing on our radar; the era of a language was over. As with .net, containing C#, F#, Visual Basic and others or the JVM ecosystem encompassing Java, Scala, Clojure, Groovy, to mention but a few. The same happened with Erlang and the BEAM, prompting Bruce Tate to coin the term Erlang Ecosystem. Alpaca, Clojerl, Efene, Elixir, Elm-beam,, Erlog, Erlua, Fez, Joxa, Lisp Flavoured Erlang and Reia, which alongside Erlang and Elixir, are opening an era of interaction and cooperation across languages. Together, we are stronger and can continue evolving! In December of 2018, the paperwork for the Erlang Ecosystem Foundation was submitted, setting up a non profit whose goal is to foster the ecosystem. I am looking forward to more languages on the BEAM gaining in popularity, as we improve interoperability, common tools and libraries. And as the demand for scalable and fault tolerant systems increases, so does the influence of Erlang’s constructs and semantics in the new languages within and outside the ecosystem. I hope this will set the direction for the next 20 years as a new generation of technology leaders and entrepreneurs spreading their wings. The Future In 2018, at Code BEAM Stockholm conference discovering the Erlang Ecosystem (formerly known as Erlang User Conference), Johan Bevemyr from Cisco announces they ship two million devices per year with Erlang applications running on them. That blew the audience away, as it meant that 90% of all internet traffic went through routers and switches controlled by Erlang. Erlang powers Ericsson’s GPRS, 3, 4G/LTE and if recent job ads are anything to go by, their 5G networks. Or IoT infrastructure through VerneMQ and EMQ, the most popular MQTT and CoAP brokers. Erlang powers not only the internet and mobile data networks, it is the backbone of tens of thousands of distributed, fault tolerant systems. Switches billions of dollars each day through its financial switches and even more messages through its messaging solutions. You can’t make this stuff up! These are just some of my personal highlights from the last 20 years. In it all, there is a realisation that we are far from done. Joe Armstrong, in 1995, told me Erlang will not be around forever. Some day, he said, something better will come along. Fast forward to December 2018, I am still waiting, with an open mind, for that prophecy to come true. Whatever it is, there is no doubt Erlang will be a heavy influence on it. A big thank you to Joe, Mike and Robert for making that first phone call, and to Bjarne for enabling it. A shout out to Jane who, by getting it outside of Ericsson, ensured its survival. You all started something which has allowed me to meet, work and learn with amazing and talented people using a technology that we are all passionate about. It has given us a platform enabling many of us to drive the innovation forward for the next 20 years (at least)!", "date": "2018-12-08"},
{"website": "Erlang-Solutions", "title": "Concurrency in Computing with Joe Armstrong", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/lets-talkconcurrency-with-joe-armstrong/", "abstract": "We launched our #TalkConcurrency campaign last week with a fantastic interview with one of the founding fathers of concurrency; Sir Tony Hoare. This week we continue with the co-creator of Erlang, Joe Armstrong, as he walks us through his experiences with concurrency since 1985. In the mid 80s, whilst working at the Ericsson Computer Science laboratory, Joe and his colleagues were looking for an approach to developing fault-tolerant and scalable systems. This resulted in the Erlang style concurrency as we know it today. During our visit to Cambridge University’s Department of Computer Science, we asked Joe, Tony, and Carl to discuss their experiences of concurrent systems and the future direction of concurrent software development. The panel discussion will follow, but for now, here is an insightful discussion about how concurrency models have developed, and where they will be heading in the future with Joe Armstrong. About Joe Armstrong Joe made his name by co-creating Erlang alongside Robert Virding and Mike Williams in the 1980s at the Ericsson Computer Science Labs. Before that, he was debugging programs in exchange for beer whilst studying at University College London. He later received a Ph. D. in computer science from the Royal Institute of Technology (KTH) in Stockholm, Sweden in 2003. Joe is the author of a number of key books on the topic of Erlang and beyond this including Concurrent Programming in Erlang, Programming Erlang: Software for a Concurrent World and Coders At Work. You can read and watch more about Joe’s Erlang journey via our #OpenErlang campaign. [Transcript] Joe Armstrong: My name is Joe Armstrong. I’m here to tell you about a style of concurrent programming that we evolved from about 1985. My introduction to this was when I started working at Ericsson and I got the problem of trying to figure out how to build a fault-tolerant system. At the time, Ericsson built large telephone exchanges that had hundreds of thousands of users, and a key requirement in building these systems was that they should never go down. In other words, they had to be completely fault-tolerant. There was actually a fairly long history of this. Ericsson started building systems like this in about 1974, in the mid ‘70s. By the time I came along, which was around about 1985, they were a world-leading manufacturer of large fault-tolerant systems. I got a job in the computer science lab to see how we could program these systems in the future. Initially, I wasn’t really interested in concurrency as such, I was interested in how you make fault-tolerant systems. A characteristic of these systems were that they handle hundreds of thousands of telephone calls at the same time. If you imagine a system that has 100,000 subscribers talking to each other, you could view this as being 50,000 pairs of people talking to each other. Obviously, there is concurrency in the way the problem is set up. If we have 100,000 people using a telephone exchange, we have 100,000 parallel activities going on. The natural way to model this is with 100,000 processes grouped into pairs. That’s 100,000 people talking, it’s 50,000 pairs of two people. It seemed a natural way to describe this. There was also a tradition of doing this using multiple processes. The system that I first looked at or the one that was being used in Ericsson consisted of two processors. One with an active processor that was doing all the work, the second was a standby processor that immediately took over if the first processor failed. This was the starting point. That was back in 1985. We were in a computer science lab and I started trying to describe this in a number of different programming languages. There was a multi-programming language project to try and model this in different languages. Sooner or later, I stumbled upon Smalltalk . The way that Smalltalk described things in terms of objects and messages seemed very good, but it wasn’t a true type of concurrency I was interested in. The problem with Smalltalk was that if things failed, there wasn’t really a good failure model, and it wasn’t really concurrent. I went over to try to model failure. Actually, through some accidental circumstances, I was introduced to Prolog , and I thought I could model all of this in Prolog. Prolog had a very bad failure model. If a computation fails in Prolog, you just get a saying, “No.” Not very good. I slowly modified this. Over a period of three or four years, from about 1985 to 1989, we developed this style of programming which became this language called Erlang. During the time, this project grew from myself to including Robert Virding and Mike Williams. We evolved this style of programming that is now called Erlang. In 1990, I think it was a sort of hallelujah moment, we went to a conference in Bournemouth and it was about how to program distributed systems. At the time, everybody was building tightly-coupled distributed systems. It was rather embarrassing because, at the end of each talk, we took turns in sticking up our hand and asking the embarrassing question, “What happens if one of the nodes fail?” These people would say, “Well, we would just assume the nodes aren’t going to fail.” The answer to the question was, “Well, the whole system doesn’t work.” We would shrug our shoulders and say, “Well, yet I know a system that doesn’t work.” We were in this rather strange position of thinking, “Hang on. The rest of the world is completely wrong and we are right. We’re doing it the right way.” I had always viewed failure as being central to a problem. We cannot assume when we’re building a big system that the individual nodes will not fail. I had viewed building systems as building them from a lot of independent components, and that any one of those components could fail at any point in time and we have to live with that. Once you’ve split the world into parallel components, the only way they can talk to each other is through sending messages. This is almost a biological, a physical model of the world. When a group of people sit and talk to each other, you can view them as having independent models in their heads and they talk to each other through language. Language is messages and what’s in their brain is a state machine. This seemed a very natural way of thinking. I’m also minded to think that the original Von Neumann and Turing always thought that computation to be viewed as a biological process of communicating machines. It seemed a very natural way to model things. In fact, it seems rather strange recording a set of talks about the origin of concurrency because the way the whole world works is concurrent. If we look at the internet, it’s billions of devices all connected together, all using message passing, and all with private memories. The Internet of Things and the Web consists of loads of nodes with private memory communicating through message passing. To me, it is very strange that that model breaks down when you get to individual applications inside a computer. It seems very strange to not have concurrency. This is a funny state of affairs! In the world of programming, the vast majority of all programming languages make it very easy to write sequential programs and very difficult to write concurrent programs. My goal was to make it very easy to write concurrent programs. Consequently, it might be a bit more difficult to write sequential programs. Of course, when multi-cores came along, what we had done then mapped very well onto parallel programs. Up to that point, concurrent programs were actually sequential programs that were interleaved rather quickly in an operating system. When multi-cores came along, the possibility emerged to execute those programs in parallel, so we were immediately able to take advantage of parallel cores. In fact, that’s probably the reason why Erlang has spread in the last 15 to 20 years because of the way it scales naturally onto massive multi-core computers. What were the key points that we learned perhaps in the early days of Erlang? This is the period from about 1985 to 1989, this is a period when we did a lot of experiments. I think what we tried to do was structure the system into primitives and libraries. We had to choose primitives that made it easy to write the libraries. What a normal program will do is use the libraries. For example, there are many very, very difficult problems like leadership election or maintaining write append buffers between processes. They turn out to be very difficult to program, so they’re done in the libraries, they’re not done in the kernel of the language. The work in the late ’80s was to identify which primitives we had to have in the language in order to build the libraries. This was not at all obvious. One primitive that came– It’s actually an idea of Mike Williams, was the notion of a link. That extended our handling across remote nodes. The idea was you could link two processes together. If you’ve got one process here, another process here, you could put a link between them. The meaning of the link was that if one of the processes failed, the other process would receive a message saying that the remote process had failed. This link mechanism was the enabling factor that allowed us to write a lot of libraries on top of that. What users of Erlang or Elixir will see, I think it’s called supervision trees where we link together collections of processes and build them into trees, but the underlying mechanism is the link mechanism. That’s I think one of the things we learned there was how to build these primitives. In fact, we tried lots of different primitives. We tried to implement buffers between processes and things like that. It turned out that these are very difficult to implement in a virtual machine, so we stripped down the virtual machine to the primitives that we needed. I think one of the things we learned is that in order to implement a concurrent language, you have to do three things at a very primitive level. Message passing should be extremely fast, context switching should be extremely fast, and there should be a built-in error processing mechanism. Without that, it’s really impossible to build languages like this. That’s what’s built into the virtual machine and gets inherited by all languages that are implemented on top of this virtual machine. [00:10:16] [END OF AUDIO]", "date": "2019-01-30"},
{"website": "Erlang-Solutions", "title": "Concurrency panel discussion with Sir Tony Hoare, Joe Armstrong, and Carl Hewitt", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/lets-talkconcurrency-panel-discussion-with-sir-tony-hoare-joe-armstrong-and-carl-hewitt/", "abstract": "When considering the panel to discuss concurrency, you’d be pushed to find a higher calibre than Sir Tony Hoare, Joe Armstrong, and Carl Hewitt. All greats within the industry and beyond, over the past couple of weeks, we’ve been releasing their individual interviews; a storyboard into the lifeline of concurrency and models over the past few decades. Here we have the full panel discussion, hosted by Francesco Cesarini, about their experience in the concurrency field, and where they see concurrency heading in the future. [Full Panel Discussion Transcript] QU 1 – What problems were you trying to solve when you created actors concurrent sequential processes and the Erlang type of concurrency respectively? Francesco Cesarini: Concurrent programming has been around for decades. Concurrency is when your multiple events, your code snippets or programs are perceived to be executing at the same time. Unlike imperative languages, which uses routines or object-oriented languages, which use objects. Concurrency oriented languages use processes, actors, agents as the main building blocks. Whilst these concurrency foundations have remained the same and stable, the problems we’re solving today in the computer science world have changed a lot compared to when these concepts were originally put together in the ‘70s and ’80s. Back then, there was no IoT. There was no web, there were no massive multi-user online games, video streaming, and automated trading or online transactions. The internet has changed it all and in doing so with these changes; it has helped propel concurrency into future mainstream languages. Today we’re very fortunate to have Professor Tony Hoare, Professor Carl Hewitt and Dr. Joe Armstrong; three visionaries who in the ’70s and ’80s helped lay the foundations to the most widely spread concurrency models as we know them today. So, welcome and thank you for being here. Interviewees: Thank you. Francesco: The first question I’d like to ask is, what problems we’re trying to solve when you created actors, concurrent sequential processes and the earliest type of concurrency respectively? Carl Hewitt: I think the biggest thing that we had was, we had some early success with Planner, right? There were these capability systems running around, there was functional programming running around. The most important realisation we came to was that logic programming and functional programming couldn’t do the kind of concurrency that needed to be done. Joe Armstrong: You’re right. Carl: At the same time, we realised it was possible to unify all these things together so that the functional programs and logic programs, all these digital things were special cases of just one concept for modeling digital computation, you can get by with just one fundamental concept and that was the real thing. Of course, we thought, “Well, there’s plenty of parallelisms out there, there are all these machines, we’ll make this work.” The hardware just wasn’t there at the time and the software wasn’t there at the time but now we’re moving into a realm of having tens of thousands of cores on one chip and these aren’t wimpy GPU cores. These are the real things with extremely low latencies among them, so we’ll be able to achieve latencies between actors passing messages in the order of 10 nanoseconds with good engineering and we’re going to need that for the new class of applications that we’re going to be doing, which is scalable intelligent systems. There’s now this enormous technology race on. Francesco: What inspired CSP? Tony Hoare: It was the promise of the microprocessor. The microprocessors were then fairly small and they all had rather small stores and they weren’t connected to each other but people were talking about connecting large numbers of microprocessors mainly in order to get the requisite speed. I based CSP design on what would be efficient, controllable and reliable programming for distributed systems of that kind. So that’s a basic justification for concentrating on a process that didn’t share memory with other processes, which certainly makes the programming a great deal simpler. The problem at that time was the cost of connecting the processes together, the cost and the overhead. The devices for doing this were based quite often on buffered communication, which involves local memory management at each node. I knew that since you had to call a software item to perform communication that the overhead would just escalate as people thought of new and clever things as people always do with software, don’t they? I wanted the hardware instruction for output and for input to be built into the machine code, in which the individual components were programmed. Now, a measure of the success of the transputer, which with the efforts of David May was implemented some years later, ’85 as opposed to ’78. He got the overhead for communication so low, that if you wanted to program an assignment even, you could program it by forking another process, a process which performs an output of the value to be assigned and another process for inputting the value that is intended to be assigned, use communication for that and then join the two processes again. All within a factor of 10 to 20 ordinary instruction cycles, which was way above anything that any other hardware system could touch because the communication was synchronised, it was possible to do it at the hardware level. There was another reason for further pursuing the synchronised communication, that was I was studying the formal semantics of the language by describing how the traces of execution of each individual process were interleaved. If you have synchronised communication they behave like a sort of zip fastener, where each zip links in with a single zap and the train of synchronisations forms a clear sequence with interleaving only occurring in the gaps between the synchronised communications. A combination of practice and theory seemed to converge on making synchronised communication the standard. Of course, I realised you very often need buffered communication but that isn’t very difficult to implement on a very low overhead communication basis by just setting up a finite buffer as a process in the memory of the computer, which mediates between the outputting process and the inputting process. Francesco: You picked synchronous message passing because it was fast enough and it solved the problem? Tony: Fast enough? It was as fast as it could possibly be. I’m talking about what is now 10 nanoseconds, that’s the sort of speed you need to be built right into the software. Francesco: Exactly. Not only but the solution was much simpler, which is perfect. Joe, what about you? Joe: I started from a different angle, I wanted to build fault tolerant systems and pretty soon I realised that you can’t make a fault tolerant system on a computer, because I think in the entire computer might crash, so I needed lots of independent computers, I’d read your CSP book and played with transputer and I thought, “This is great, this sort of lockstep [mimics thumps]. How does it work in a context where the message passing is not internal?” It’s to remote load, I did want it to be remote in case the thing crashed and I couldn’t get this synchronous. I was a physicist and I’m thinking, “Messages take time”, and they propagate through space, there’s no guarantee it gets there. If you send a message to something else and it doesn’t come back, you don’t know if the communication is broken or if the computer is broken, even if it gets there and the computer receives it, the computer might sort of not do anything with it, so you really can’t trust anything basically. I just want to model what’s going on in the real world and I’m thinking, “It’s the key”, I have read your book and this observational equivalence sort of struck through with this– I thought, “It was the most important principle in computer science.” Basically, we’ve got black boxes that communicate but we shouldn’t care what programming language they’re written in. Provided, they obey the protocol, so I thought this, it was central that we wrote down the protocol. Because we couldn’t formally prove it in the sense you would want to do inside one system, I thought, “Well we’ve got to just dynamically check everything.” So, we need to build a world where there are parallel processes communicating through message passing and I thought they cannot have shared memory because if they have shared memory and the remote going to be the crash. If you don’t want dangling pointers that you can’t dereference, so that was a sort of guiding principle. I didn’t know about the actor stuff at the time and I don’t know, how else can you build systems? Carl: That’s right [chuckles]. Joe: We are four people. We’ve got our state machines and we’re sending messages to each other. We don’t, actually, know if the message has been received. Carl: That’s right. Joe: And this and this. I thought because I used to be a physicist and I’m thinking, the program and the data have got to be at the same point in space-time for the computation to occur. I thought, why are people just moving the data and not the programs. We could move both of them to some intermediate point in the middle to perform the computation there. I think, in part of the system we can use strong algebras there, lock step when it’s very beautiful. Another part of the system we can’t, it seems to be a mix between mathematics and engineering. The mathematics can be applied to part of the system, and best engineering practice can be applied to other parts of the system, a delicate balance between the two. I was pursuing the engineering aspects of this and just trying to make a language to make it relatively easy to do this. I thought we’re treading on a minefield. There are sudden bits of terribly complex software like leadership election and terribly complicated, then there are bits that are terribly easy. It struck me rather strange that there was this paradox of the things that are terribly simple in sequential language that are impossible in concurrent languages, then there’s the other way around, the fix is terribly simple in concurrent language, impossible in sequential languages. Tony: I agree with you completely about the central importance of all of the buffered messaging and indeed that it has some nice mathematical properties that they’ve synchronised messaging doesn’t have. But the synchronised message, the paradigm really has another reason, and that is to create the input and output as a single atomic action. I think, atomic actions in the sense of actors and in the sense of petri nets too, I think. Carl: That was the other thing that mystified us because we were thinking, well if you want to be like that the thing that was done in the old sequential computation by Turing and Church. There was a universal model. They nailed that thing. We wanted to do the same thing for concurrency, so we thought well the only possible way to do that is to base it on physics because no matter what they do, you can’t get around physics. [laughs] Tony: I agree, yes. Carl: This put constraints on and also we wanted to be distributed. We thought, well, okay. Also, we wanted to be multi-core, so that means if it’s distributed on the IoT that there’s no in-between place to buffer them. The message leaves here before arrives there, right? We can’t synchronise these IoT things, so the fundamental communication primitive has to be asynchronous and unbuffered. If you want to have a buffer that’s just another actor. You do puts and gets on your buffer right, and sure. Tony: How you got this all upside down. [laughter] Tony: Look at the actual physics, the electronics, it’s all local. If you have a 10 nanosecond communication time on-chip and you don’t take advantage of it by doing synchronised communication then your overhead is good. You can’t use it for everything. Basically, both of them are necessary, which is fundamental…so shall we say postpone discussion? Carl: You see we don’t have, 10 nanoseconds. It’s only average. In some cases, okay it’s going to take us a long time to get a message across a chip, but we have to through compactifying garbage collectors and get the locality and so on. The average, it’s only 10 nanoseconds, but when a core on one side of the chip sends a message to the other side of the chip, goes to this fantastically complicated interconnect. It’s like the internet on a chip between these two cores. Again, the way that they build these things, there is no buffer. You assemble a message in one core and you give it to the communication system and it appears on the other side and there’s no buffer. Joe: I think, we don’t really want two different ways to program. Carl: That’s right. Joe: If you got the world wide web, if I’ve got a process in Cambridge that’s talking to one in Stanford, its messages, I write it this way, I’ve had to send and receive that. I have to allow for failure, my message might not get through. And suddenly, if they collapse this program onto a single processor, where they’re both in the same place, I don’t want to change the way I program. I want to write exactly the same thing. I don’t want two different mental models. I can use mutexes and I can use all these fancy things, but I don’t want to– [crosstalk] Where I can see that– Carl: Have you seen the chip? Now with having 10,000 cores on the chip, the core on the other side of the chip might as well be in Los Angeles, get ready. It’s distributed programming on a single chip– [crosstalk] Joe: And WiFi and things like this are going to change that as well when we have– [crosstalk] Tony: When you can build the buffered communication with a 10 nanosecond average delay, I will come around to your point of view. Carl: Oh, we can do that [laughs] Average, the trick is the average. In some cases, it’s going to be– It could be seconds, that’ll be very few of them. Tony: That’s why I think the Erlang idea of distinguishing local from remote is so important and they both exist, which is fundamental. I’m not going to argue about it. If you take remote as fundamental, well, you’re welcome to it as long as you’ve been my transaction. The things that happen locally, really do happen in a way that no other agent in the system can detect anytime or state in which some of the actions have happened itself or not. Carl: I am absolutely good. Inside an actor, it’s not visible to the other actors. Joe: That’s right, but I thought that the function call really is like a black box you use. Tony: Absolutely. Yes. Joe: You’re sending a message into this thing that doesn’t know, and you get a return value. Only it’s got different semantics because exactly once, is trivial, I mean that’s how it works, but exactly once doesn’t work in a distributed system, it’s at most once to at least once. You have all the funny impossibility things happening then. It’s funny that this is different local [crosstalk] in a different model. Tony: I think there’s– I’ll bring in one of my other buzzwords, abstraction. Modern systems are built out of layers of class declarations. A class declaration can itself be used called by classes higher up, up the hierarchy of abstraction. What the method calls to a lower class are treated theoretically. Theoretically, though may not be implemented in the same way as transactions when reasoning in the higher-level class. They will be implemented by method bodies, which are far from atomic in the lower class. Each class has an appropriate level of granularity at which it regards certain groupings of the actions as being inseparable in time. At the same time, it produces nonatomic things, which are just method bodies, which simulate the atomicity at higher levels. Now the simulation can be very good because the one restriction about disjointness that I would like to preserve is that each level doesn’t share anything with the levels above and below, which I think to be practical programs, perhaps you could check this, would regard as a reasonable thing of declaring it– Francesco: By the way, it’s correct, layering abstraction. Joe: I entirely agree with you, but the question that interests me is, what happens when the system becomes huge? Because if you got this little tight system, you could prove anything you want. Tony: That’s right. Joe: You may or may not be able to prove things about, but the idea of the real practice is– [crosstalk] Tony: The real payoff comes when– [crosstalk] Joe: Really big. How I know when– Tony: Bigger the better. Joe: Yes, but imagine it’s changing more rapidly, than you can prove its properties. Imagine– [crosstalk] Tony: The reason why you could– Joe: Imagine it’s always inconsistent. Tony: I don’t have to imagine these things– [crosstalk] Joe: [crosstalk] any attempt to make it consistency possible. Tony: Do you know Hoare’s great saying about inside every large program, there’s a small program trying to get out. Carl: Yes, but you never find it. [laughter] Tony: You never find because you didn’t put it there at the very top-level. of abstraction. Everything, you will have a very powerful atomic action. You can write a small program which describes at a large scale, what a very large program was. Joe: This is a thing that really scares me, are people developing large applications that they don’t understand. Then they get so complex and put it inside the black box and seal it [crosstalk] layers, so you end up with gigabytes of stuff. Tony: Who talked about sealing it? What is the part of that program that changes most frequently? The top layers change. You can change, they have an interface. Well defined interface [crosstalk]. Joe: Many programs don’t have well-defined interface. They should have, I entirely agree. Carl: These intelligent systems don’t work that way. They’re not like operating systems, okay? These ontologies have a massive amount of information. The layering doesn’t work anymore for these massive ontologies. They’re just chock-full of inconsistency. [laughs] [00:21:17] [END OF AUDIO] QU 2 – Is there anything forgotten which should be known, or anything which you feel has been overshadowed, which is important? Francesco: Is there anything forgotten which should be known, or anything which you feel has been overshadowed, which is important? I think maybe one or two key points. Joe: I could talk. I have lectures that have gone for hours. Francesco: [laughs] I wish we had hours. Joe: I think we seem to have forgotten that things can be small. This way of decomposing systems into small things that I can reason about. There seems to be this belief that things have to be big. If you look at web pages, and something like that, it will download 200 kilobytes of compressed JavaScript to do what? We don’t know. [laughter] You want to create a button or something. Somebody will include a CSS framework of 200 kilobytes to make a button. That’s like two lines of JavaScript. We’ve forgotten that things should be small, we’ve forgotten Linda Tuple Spaces, we’ve forgotten hypertext. I keep going back to Ted Nelson and Xanadu and the ideas there. We’ve forgotten that hypertext should be– The link should be two directional not one directional. Carl: That’s a scam. [laughter] Joe: Now you’ve got me going. Now it becomes interesting. Francesco: Anything from CSP, which you feel has been omitted or forgotten, which would help us today? Tony: If there is, I’m sure I’ve forgotten it. [laughter] I do think this- the new factor, which I hope will become more significant, has been becoming more significant, and that is tooling for program construction. A good set of tools, which really supports the abstraction hierarchy that I’m talking about, and enables you to design and implement the top layers first by simulation, of course, of the lower layers, it’s the sort of stub technique that it will actually encourage programmers to design things by talking about its major components first. The second thing is that the tools must extend well into the testing phase. As you will know, large programs these days are subject to changes daily. Every one of those changes has to be consistent with what’s gone before and correct not to introduce any new strange behaviours. I use a Fitbit. Changes are just extraordinary. Joe: Why do I have to change the software once a day? Tony: No, they’re a little bit less frequent. I have to exercise once a day. I think that’s the problem. Joe: People keep telling me this, you’ve got to upgrade your operating system. Then they say, ‘Well, that’s because of security things.“ I don’t really have much confidence in them. [laughter] If they said we have to change it once every 20 years, I could believe that it was reliable, but telling me that I have to change it once every six weeks is crazy. Tony: You need an overall development and delivery system in which you can reliably deliver small changes to large programs on a very frequent basis. Joe: Without breaking everything. Tony: Without breaking everything. Well, I think I have a rather unpleasant dream, I think it is, that you’ll get your customers to do the testing. The beta testing has always been a useless technique. Well, you set up a sandbox in which you deliver the new functionality. If it fails in any way, you go back to the old functionality, you treat it as a large scale transaction, you go back and you run the old functionality for that customer and you report it. That report gets straight to the person who is responsible for that piece of code in the form of a failed trace. Joe: Why don’t they do that? Should have done it 20 years ago. Tony: Because it’s actually not very easy. Joe: You get a black box and record it, you know like a flight recorder on a plane. Tony: The screens were not really powerful enough to do a large scale trace. In the case of concurrency, you mustn’t use logs. Those logs of individual threads are a pain to correlate when you do get the communication. You’ve got to use in fact, a causality diagram and arbitrary network and the software to manipulate those on large scale I think will take some time to develop. [00:05:06] [END OF AUDIO] QU 3 – Linear structures vs causal structures Francesco: Even with concurrency, you need to be able to extract the linear execution of your program from process-to-process. It’s something which there is work being done on it and structure deal. Tony: I would say you have to analyse the causal structure, not the linear structure. Francesco: That’s true. Carl: I think that we’ve forgotten, which we knew in the early days of intelligent systems, is that these systems are to be isomorphic with large human organisations. These complex intelligent systems are going to run on very much say, the principles that Stanford runs. You say, “Well, what are the specifications for Stanford University?” Well, we have principles and we have ethics and we have guidelines, but you really don’t have formal specifications. Anything you think is going to work for programs, for launch programs, that wouldn’t work for something like Stanford University, it’s not going to work because they’re basically isomorphic. Therefore, I think that what we do is we do keep logs for these things. Stanford keeps records of all kinds, and that’s so that if something goes wrong, we can look back and try to see how we can do better in the future and also to assess accountability and responsibility. That is the fundamental thing, is that that’s going to be the structure of these large scale information systems that we are constructing. Francesco: My key points I’m taking home here are, simplicity, where you need to have small programs or programs which become complex but the units are small. It makes sense to see a process, an actor or an agent as maybe one of the building blocks, which is small, it’s containable, it’s manageable. The second point is I think the importance of no shared memory, correct me if I’m wrong. This no shared memory approach then brings us into both distribution and scalability and multi-core. Those are the key points I’m taking home. Joe: I think one of the things we’ve forgotten, is the importance of protocols and not describing them accurately. We build a load of systems, but we don’t write down what the protocols between them are, and we don’t describe the ordering of messages and things like that. You would think that it would be easy to reverse engineer, a client-server application just by looking at the messages but you can trace the messages then you say, “Where’s the specification that says what the order should be?” There is no specification. Then you have to guess the ordering means. Tony: One can use finite state machines [crosstalk] specifying these things. CSP would. Joe: People don’t, that’s the problem. In fact, all the international RFCs are pretty good. Tony: I think I would have a small concession to make to sharing. You’re allowed to share something between two processes at most. Obvious examples are the communications channel. What’s the use of that if you can’t share it between the outputter or in the inputter. Now, if you have a hierarchical structure like I’ve been describing, the behaviour of a shared object is programmed as a process inside the lower level class, so that even if you only use the same programming language, it’s a highly non-deterministic structure, which is a different context I used to call a monitor, which accepts procedure calls, accepts communications from all sides. Everybody who uses it has to register and has to conform to an interface protocol which governs the sequence which makes the sharing safe in a sense, which is important at the higher level and implemented in the lower level. Carl: The fundamental source of indeterminacy in these systems, you have all these zillions of actors sending messages. Is the order in which the messages are received, that’s where the arbitration occurs in the system. If you have something, for example, Tony and this work that was done by Tony, if you’re on our readers, writers scheduler, you’ve got these read messages and write messages coming in from the great world out there. You’re sitting here defending this database, you’re scheduling it so that it’s never the case that there are two writers in the database, and there’s never the case that there are a reader and a writer. You’re sitting here taking these requests from all covers you don’t know who’s going to be read and writing this database, and you’re scheduling of that. You have your own internal variables and then he must be kept very private the number of readers and the number of writers you’ve got in the database for example. The indeterminacy is in these messages that are coming in from the outside world which you are then scheduling for the database. That is the funnel and irrevocable source of the indeterminacy. Tony: I agree. That’s why it’s built into CSP. The fundamental choice, construction allows you to wait for the first of two messages to arrive. Cut it down to two, two doesn’t scale but just bear with me for a bit! The great advantage of this is that if you have to order the receipts of these two messages, you will double the waiting time. If you went for two things at the same time, you wait twice as fast. Carl: Well, here’s the actors who found that these two messages coming in. You take them in the order in which they’re received. If you want to process them in a different order inside, the idea is you don’t want to have a queue of things waiting inside of it. The ideal, but you don’t want to have that, you want to take everything that comes inside so that you can then properly schedule the order in which you process it. It’s like your mail, you take the mails that come in, you may not want to pay to pay the first bill that comes in. You’ll process it later but you take it as it comes in because that’s much more efficient. Joe: What Erlang does, it’s every practice’s got a mailbox, incoming messages just end up in the mailbox in order and the program gets an interrupt say, “Hey, there’s something in the mailbox,” and then it can do what the hell it likes, it just spits them out. “I want to take that one out, take that one out. I’m going to go to sleep again.” Carl: That’s an excessive amount of overhead. Joe: But it makes the programming a lot easier. Carl: I don’t think so, because you can program is much more easily if you take it all inside as it arrives and doesn’t have this separate cable out there. Joe: But then you have an MxM-state machine… Carl: Well, it’s not so bad for readers, writers. Francesco: It depends on the problem you’re solving, very much. [00:07:53] [END OF AUDIO] QU 4 – Why is concurrent at scale today still done with legacy languages that have concurrency bolted on as an afterthought. What do we need to do to change this? Francesco: Why is concurrent at scale today still done with legacy languages that have concurrency bolted on as an afterthought. I think concurrency needs to be designed into the language from scratch. It’s very very hard to write the framework and bolt it on, what do we need to do to change this? Joe: Darwinism. [laughter] Carl: Well– Joe: Survival of the fittest. Carl: There is this often it’s a new project. Okay, like the moon project or heaven forbid, the Manhattan Project or the icon project that enables new things to be brought in because otherwise because capitalism is basically a very incremental hill-climbing process. The most sensible financial thing for capitalists to do is to bolt something on because you get the most rapid buck for the least investment in the short term but then you end up with monsters like C++ [laughs] and things like that but if you just keep pursuing that path. I think that because we’re now engaged in this giant race among these nations to create these scalable intelligence systems and they’re good at creating these large projects to do that there is some opportunity now for innovation because that’s not the standard hill climbing. Joe: I think hardware changes precede software. I think if you kick this hardware the same, you get an S-shaped curve, you get rapid development in the beginning and then you get up the top end and nothing much happens and then new hardware comes on and suddenly there’s a lot of change. So Erlang is billions and billions of times faster than it was but that’s due to clock speeds, it’s not due to clever programming. Carl: Well the clocks aren’t going up I know. We’re now faced with two fundamental evolution’s having thousands of powerful cores on one chip. Joe: Yes. Carl: Also having all these IoT devices, those are two huge hardware changes. Joe: I always thought that gigabyte memories and certainly I view petabytes memories when they come to me like an atomic bomb because that they are just– if you imagine the combination of petabytes memories with LiFi and communication at tens of gigabits per second but the combination and like 10,000 Cray-Ones and a little thing like your fingernail everywhere in every single light bulb that’s like an atomic bomb hitting software. What we’re going to do with it, nobody’s got a clue. Carl: Well that’s the thing is a stacked carbon nanotube chips that they’re working on now aren’t going to give us these thousands of cores on a chip. Also, they make the memory at the same stuff they make the processor out of. It’s different from now we make it the DRAMs out of different stuff that we make the processes for it so we can’t combine them. Joe: I was completely blown away a couple of weeks ago. I saw a newspaper article about farm bots and suddenly this company made three little robots. One was a seed planting robot, tiny little thing. It will go around and plant seeds. Then there was a watering robot. Walked around and looked at the seeds. Then there was the weeding robot that had a pair of scissors on the bottom. It went around snipping the things and suddenly this realisation that farming could we– We could watch every single seed individually and the amount of energy to do so I thought was claiming was like 5% of the energy of ploughing, using a plough is terribly inefficient use. When we’ve got computing at this scale, we can tackle traditional problems in completely different ways and we have to do that for the benefit of mankind not to build things to feed your cat when you’re out. To improve the efficiency of farming and things like that. It’s amazing. Francesco: What you didn’t know is that the farm bot was actually powered by Erlang. Speaker 4: No, I didn’t. [laughter] Joe: It was open source and all you need is a 3D printer and you can print these things have them running around in your garden. [00:04:16] [END OF AUDIO] QU 5 – The future of current programming and immutability Francesco: I think there a lot of claims about the future from current programming languages. Some people claim that there’ll be a lot of features taken from functional programming languages. The first kind of feature which comes to mind is immutability. Carl: The essential thing about the actors is that they change. They get all their power of the concurrency, is because they change. Now, the messages they send between each other are immutable because they have to exist as photons and there’s no way to change the photons in route. By definition, the messages are immutable but the actors have to change. They get all their power of modularity from over the functional programming, is because they do change. They change a lot, which the functional programming can’t do, right? Francesco: Yes, but it’s only the actors which can change their own data. Carl: They changed it, that’s right. [crosstalk] Francesco: From the outside, yes. Carl: As our friends say, change comes from within. You can’t change me but you can send me a message so I can change myself. Francesco: It’s a form of isolation and I think these are ideas which come from functional programming but they’ve also been heavily influenced from over-programming. I think Alan Kay’s objects, objects don’t share memory, and objects communicate with message passing. Carl: You should mention Kristen Nygaard and Ole-Johan Dahl for that. All: Yes. Tony: I think this is a crucial argument. If you’re writing programs that interact with the real world, you’ve got to construct a model of the real world inside the computer, as it’s done just universally in the design of real-time systems. The real world has things called objects and the objects do sit in a certain place, more or less. They can move around but the movement of objects, the existence of objects, the sequentiality of the actions performed by the same object, these are features of the real world. The objects change. Functional programming doesn’t address the real world. I think functional programs are wonderful. [chuckling] I really, really admire functional. If I had my choice, I’d always use functional programming. Carl: You don’t have a choice. [laughter] Carl: You can’t do the readers/writers scheduling as a functional program. It just makes no sense, it can’t do it. The scheduler has to be continually changing its internal state, as the read and the write messages come in. It’s got to be buffering up the reads and buffering up the writes and letting some reads – It’s just always changing and you can’t do that. Joe: Alan Kay said, the big thing about object oriented programming was the messages. It was the messaging structure that was the important bit to know. That was what had been lost and of course, then the next thing comes, we’ve got your immutable messages, which I totally agree with. Then, we need some kind of notation to write down the sequences of allowed messages, which you got in CSP and which people think to ignore. A state machine in CSP describing the allowed sequencing of messages. Carl: The only thing about the actor model was to minimise sequentiality as much as possible. Sequentiality is evil. You have arbitration in front of the actor, in terms of the order in which it’s going to take the messages in because that’s irreducible. As soon as an actor takes a message in, it wants to run everything inside of itself in the parallel, to the extent that it can. That is its goal, the maximum amount of internal parallelism inside an actor. [00:03:53] [END OF AUDIO] QU 6 – Can a solution with share states be made robust and safe? And can a solution which communicates with measured passing be made fast? Francesco: Can a solution with share states be made robust and safe? Joe: No. Carl: You mean shared memory in which you do assignments, on loads and stores? No way. Francesco: A second question is can a solution which communicates with message passing be made fast? All: Yes. Carl: Yes, but only if you have the right kind of processors in it. That respect Tony was a pioneer with a transputer, of realising that in order to do this at speed, you have to have the hardware that’s suitable and the hardware previously was not. We’re going to have to do that again. The RISC processor is not suitable. We have to do better than that. Francesco: What is the implication for the futures of software development? Tony: I think to capture– The test for capturing the essence of concurrency is that you can use the same language for the design of hardware and of software because the interface between those will become fluid. You’ve got to have a hierarchical design philosophy in which you can program each individual 10 nanoseconds at the same time as you program over a 10-year time span. Sequentiality and concurrency enter into both those scales. Bridging the scale of granularity of time and space is what every application has to do. The language can help do that. That’s a real criterion for designing the language. [crosstalk] Carl: Each semicolon hurts performance because you have to finish up the thing that’s before the semicolon before you can start the thing after the semicolon. The ideal concurrent program has no semicolons. [laughs] No sequentiality. Tony: I played with functional programming… Carl: No, no, it still has to do the state change, but it has to have these macro state change things like in queuing and dequeuing and allowing guys in the queues to proceed. This macro things so that you don’t have to spray your program full of semicolons, but still have the state change. It’s not functional. Joe: I have played with some of these highly concurrent languages. I have played with Strand which was highly concurrent and it was terrible because they have a problem with the opposite. If you created too many parallel processors, so something rather a surprise of this tiny thing you’ve created 6 million parallel processes to do virtually nothing- [crosstalk] Tony: There is a wonderful way of controlling concurrency. If you got a concurrency problem, try and make it more sequential. Anyway– [laughter] Carl: Maybe that’s being religious. [laughter] Tony: I would say it’s all in my religion which is that if you have programmed or unprogrammed components, there are two ways of composing them. One sequentially, which requires that all the causal chains go forward from one to the other, another backwards. The other in which the causal chains can go between both operands. You have to tell the programmer that he is the person who has to worry about deadlocks. Some actually– Carl: I think we’ve solved the deadlock block problem by the following mechanism. Whenever an actor sends a request to another actor, the system says, “Okay, we’re keeping statistics on what’s going on.” We don’t get a response back within the certain number of standard deviations, then the program that issued the request is throwing an exception, “Too long, it too long.” Right? Now, you can try again, but a program will never deadlock, right? It will always terminate. [laughs]. Joe: We’ve done that for 30 years. Carl: Fair enough. Okay, he’s already got the solution. Joe: In fact, with deadlock is– Talked to Francesco, I said, “I’ve only been hit in the face by deadlock once or twice in 30 years because we use Carl’s mechanism. On the other hand, you do have the nasty problem with– The message doesn’t come back within this time. Then the time comes and then it comes just after that. You’ve got a lot of messing around and throw it away. That’s another tricky problem then. Carl: It is, that’s right. Tony: Well, the problem was solved in the same way in the transputer language Occam, which every time you waited for something you could specify a time limit. It’s responsibilities put on the programmer to manage deadlock in that way. I was deprecated that way of managing the deadlocks, but I think it’s going to be inevitable anyway. Joe: I remember with Occam the abstractions were great but the transputer didn’t do fair scheduling. When you’re waiting for things, some of the things sort of lower down weren’t fairly scheduled. Carl: You don’t want to put the burden on the programmer to specify the amount of time. You should say, it’s like you don’t want to put the program on the business of doing the garbage collection with freeze. You want the system to handle it automatically, therefore it will be keeping the statistics and the number of standard deviations that it’s taken in the past. Joe: Of course, what you said about timeouts? Tony, I gave a talk about Erlang and you were in the audience and you had one question, you said, “How do you choose the value for the timeout?” You have immediately hit on the key. Carl: The answer is, don’t put the burden on the garbage collection, you put the burden on the system to keep your statistics and throw the exception. Tony: At the level of the abstraction hierarchy, which you are now living, you choose a level which is appropriate. Joe: And I must say the Telecoms people actually did it very well because they have two protocols. They have remote procedure calls that are known to terminate very quickly. You send a message to something, immediate answer back. Therefore it’s okay to busy wait for that. That’s fine. The second one is that you know that it’s going to take a long time, so send an acknowledgement back. Then you know you’ve got to wait a long time. The protocol designers sort of have to think «which of these two cases should I use so that it’s very explicit?» Tony: Absolutely the right answer. Joe: All of Telecoms protocols sees that. Virtually none of the software systems use that. Tony: In the concurrent system, you have a concept of a transaction, an atomic event which stretches across more than two components. That is a very important idea for which there are many implementations, and therefore, I don’t know. People are reluctant to put into programming languages. Joe: A remote procedure call should actually say, I send you a message and the answer I should get back, uh, either immediately to get that one of two things, is either here’s the answer, or I’m going to give you the answer within 10 seconds. You should tell me how long you think it’s going to take. Tony: This is built into Occam because if you just didn’t mention anything, it wouldn’t assume to. Carl: Yes, but transactions have never been successful for distributed systems and now everything is a distributed system, including what’s going on in a chip. I have my doubts the transactions are going to be a part of the feature of concurrency- Tony: At a low level. Carl: -but within an actor when you get some message it tries to do that. Even then, it’s got the problem between any pair of instructions that can be blown away. Francesco: Yes, that’s how we achieve scale. Any transactions are basically serialised through process or an actor. Then, you need to—however—make sure that you’ve got the fault tolerance around it in case that you lose an actor- Joe: That’s right. Exactly, yes. Francesco: -because you then need to replay. That’s done in a different layer. You’re actually hiding the complexity away from the programmers. Joe: I was going to say, if you had good clock synchronisation down to– Say your IDs have got clock synchronisation down to 100 nanoseconds? Carl: If you can cross a chip, that’s good enough. Joe: No, no, but across the world. If you have really good– Whatever the granularity of time synchronisation, if you think you can trust that, a lot of problems would go away but it’s very difficult. Tony: Levels of granularity– Joe: We could use supernovas and stars, and measure the time- Carl: Google’s pursuing that and now, it’s causing them tremendous amounts of problem. They thought that they can rely on that global time synchronisation. They find that they can’t, that there’s a tail, right? The time synchronisation was cutting off that tail causing unreliability problems. Now that you’re going back to what Tony was talking about, namely, the causal model because message passing like the semicolon also moves irreversibly forward in time. It creates a chain of messages from here to there, that is irreversible. Joe: I used to work with astronomy and the astronomers could get clock synchronisation down to about a nanosecond. If you could propagate that out or – of course you can’t – but that’s the best you could probably do. Tony: Just accept that you have to live that different levels of granularity, but you don’t want to import all the problems of the lower levels every time you write a higher level thing. Higher level things tend to be slower because they’re implemented in terms of lower level things, and therefore the inefficiency of the implementation at the high levels which is where the real application oriented actions happen are relatively not quite so sensitive to overhead as the lower level. Carl: Yes, I agree with Tony. Recently now that we have these IoT devices, we have to have something since an unseen actor is going to live or might have a distributed implementation but an actor then is for a group of IoT devices, like the IoT devices in your house, right? You need to group that as a new unit of abstraction, you and your IoT devices is now a citadel. We had to do that. Now currently we have firewalls which are just terrible. We need a new level of synchronisation, new level security, these citadels which protect a unit of IoT devices and people, and from the Internet, from the bandits on the Internet and they have to be grouped together. Then within that, they use cryptographic protocols between the IoT devices so you know you really can trust what’s happening. Joe: I was going to say what do you think about distributed protocols where you deliberately slow everything down? For example Bitcoin, that’s the fundamental design of things. Well, we’ve got to propagate this to the entire world. That will take 10 seconds and therefore we have to slow every computation down so that it takes 10 seconds. [laughter] If we get faster processors we will make the computation more difficult but it still takes 10 seconds. Carl: If your business model is to make things slower, your competitor is going to beat you. [00:11:46] [END OF AUDIO] QU 7 – What has been the most disappointing developments in concurrency in the last few decades? Francesco: What is the development, I think, which has most depressed you in the last 10, 15 years, 30 years? Joe: Interested or depressed? Francesco: Depressed you. That made you sad, made you angry. Joe: The bitcoin proof of work algorithm… Carl: No, no. It’s the mass surveillance that Snowden revealed. Right? That is really being done, surveillance is being done on a totally amazing level. The amount of information that companies and the intelligence agencies are collecting on us is just astounding. The question is, will they get everything? Because we’re about to all be wearing the holo glasses in 10 years or so because the replacement for our cell phones and that they have a backdoor into holo glasses. They see and hear, everything that you see, hear and do. It’s an absolutely terrifying prospect, but you can’t resist it. You’ll have to use them in your job. Right now I can’t be functional in my life if I gave this up. I would no longer be competent, right? I can no longer coordinate with people, I couldn’t get my job done. The same will be true with the holo glasses once they get them lightweight, like once that Tony wears. Then they don’t make you look like a bug-eyed monster like the current entertainment ones do, right? That’s happening because the companies in Silicon Valley have the prototypes and big companies will be shipping it in just a couple of years. Tony: Well, the central level interference in elections referenda is even more horrific because it really is very easy now to buy votes. When this happened in the Roman Republic, people got rich enough to buy votes. The Republic failed and certainly couldn’t maintain a democracy. I think the political implications are dreadful. [00:02:00] [END OF AUDIO] QU 8 – Concurrency going mainstream Francesco: We’ll be seeing a concurrency oriented programming becoming mainstream. It’s an excellent idea. Carl: It has to. It has to. That’s right. If anything we see the default applications, the default system is going to become an intelligence system, because now we’re going to have the capability to do it. In order to get the response times down, like the people doing with the glasses, you’d think that if you got a server on the internet, you think you’re doing pretty good if you’re giving a 100 millisecond response time. Well, the Holo glasses, they laugh at 100-milliseconds. They talk about 10 [laughs]. That puts an enormous force on how fast the thing has to perform and the only way to do it is with the concurrency. Joe: Now, I think we’re going to go to the sort of structure the brain has. When I was working at Ericsson, you look at how mobile phones are made. I think they’ve got a video codec and an audio codec. The brain has got this visual codecs and it’s got the audio-visual part of the brain with specialised hardware for that. If you look at the sort of chips we build, there was a lot of confusion. There a lot of different video codecs. Then somebody would say, “This is the best codec and we’ll build that in hardware, and this is the best audio codecs.” Then the speech recognition. These become standard components. You bake them into a tiny little chip wired it up with a lot of memory and very fast communications. Then I think the development stops then until we get new generation of chips that have neural network chips that are very, very fast, but that will change how we program. [00:01:43] [END OF AUDIO] QU 9 – What are your views on blockchain and decentralised web? What role do you see concurrency playing? Francesco: What are your views on blockchain and say Solid, you know, Sir Tim Berners Lee’s decentralised web? What role do you see concurrency playing in both? Carl: Blockchains are very slow and they’re easily hacked like, for example, in Bitcoin, the Chinese bit-miners own the majority of the bitcoins and so they can outvote anybody else then, right? That won’t work. The other thing is that we’ve learned that performance is enormously important, and you have competition, and you have to have a business model to have any effect on the world. So unless Solid can compete in the business model and in performance, then it won’t matter. Even if it has great nice ideas, like it was once thought, I disagree with Joe, in that blockchain was a great idea but blockchains completely don’t scale so it was absolutely necessary in order to have a scalable web is to use one-way links. For example, actor addresses don’t have back pointers, because it would just completely kill performance. What if I’m an actor, there might be some popular actor and there might be millions of actors that have its address that could send it a message, but that one guy can’t be held responsible for knowing everybody who has its address. The scalability has now become a crucial issue and that’s a driving force for concurrency because concurrency is the only way to get the scale and performance. Joe: I think deployment is a problem because even if somebody made an open source privacy application, it needs 50 million users to take off. Apple and Google and everybody have dominated this way of deploying something to hundreds of millions of people. Carl: That’s right. Joe: It’s very difficult to break, the first one to get a hundred million users wins basically. Carl: You have to have a business model. I think that for the citadels like each home has a citadel when you get the internet, the business model again it’s going to be advertising because how do you compete with free. There’s a business to be had between your citadel, matching your citadel with merchants that want to sell to you matching you up with them there is a business there, which is basically some of the advertising business. If somebody would build a citadel based on that, then they could fund the whole thing out of advertising as Google does currently with a centralised model. The problem is that we have is how do you bootstrap that, how do you get a big player to make the conversion because it completely scares them because it’s contra to their current business model. Joe: What I don’t know it’s the asymmetry in knowledge, so Google knows everything about us but we know nothing about Google. When people start to realise that that asymmetry can be used for political purposes and economic purposes that they will demand– Maybe maybe something like AT&T was split up. Why isn’t Google being split up? Why doesn’t the European Union have something like Google to deploy its services? Carl: Exactly, but note they have toxic knowledge, having access to our sensitive information in their servers is actually going to be very bad for them, because once the people in England realise that the Americans have all this intimate knowledge in their data centres of British citizens, they realise that’s a national security risk and that’s for example why Uber was kicked out of China. The Chinese government didn’t want to have a foreign company to know about the travel habits of the citizens of Beijing so they bottom out. Storing sensitive information is actually toxic to these companies. They just don’t realise it yet because they’re going to get the view that now they’re being forced to store all the information in each country like you have to store the Chinese citizens’ information in China, and then you have to be domiciled in China which means you’ve just been broken up you can’t be an international company. Not only that, if you’ve got the sensitive information in your data centres, then I’ll send you at the security service of your country you want to come and say, “Look. I want to have it.” Then they discover they don’t they just have the bits, they want to have your toolchain. If you’re your Google or Microsoft, the only way they can manage that is to use your toolchain so then they have this little building inside your company. That’s a pain in the tail, they have two companies that they have to get bits from. They want you to standardise your stack and then the company because it’s got this sensitive information is becoming a prisoner of the government because now the government wants the information. Francesco: We’ve gone from concurrency to resilience to scale to kind of social-political area and they’re all linked together. Carl: That’s right. Francesco: There’s no doubt about it. [00:05:15] [END OF AUDIO] QU 10 – How would you sum up the future of concurrency in one sentence? Francesco: How would you sum up the future in one sentence? Joe: I don’t know, I always imagine a historian in 2-300 years’ time writing the history of this period. It would just be like the Dark Ages, the ages of confusion. Will it end with computer failures that kill millions of people or will it transition into something that is for the benefit of mankind? We don’t know at the moment and I don’t know how long it will be before we know. Maybe we will know in 20 years’ time or 50 years’ time but at the moment, it’s very confused. I don’t know where we’re going. Tony: I don’t really have anything to say about the distant future. I would like to go back to a point by making a suggestion about security, which is enforced by runtime checks. The way that security is enforced at the moment is by sandboxes. If we extend the idea of abstraction downwards, then we get the idea that you can specify security protocol by interrupting the progress of the higher level users and checking that they conform to the protocols in real time all the time. Conceptually, we’re reusing the same concept of layering. You can then have, obviously, what I might call dungeons of security where you’re digging underneath the program to check that it’s satisfying protocols which are believed by people to prove things, will implement your desires as to what can and cannot happen. Carl: We’re now embarked on the most complex engineering project that we have ever done. That is to build the technology stack for these scalable intelligent systems. The Chinese minister of Sciences said they think they can do it by 2025. The only way to build them is to use massive concurrency. It gives you the performance, the modularity, the reliability, and the security that you need. The big question is, what will they now be used for? We want to use them for things like pain management, which is a huge problem in the US, is to have pain management without opioid addiction. Our solution is to use these scalable intelligent systems. They could be used for other things. They could actually become the basis of universal mass surveillance. We are at a turning point. Tony: Why can’t we use things that don’t scale? That seems very hard. Carl: The economics demand it. if it’s not scalable– Tony: I’m not forbidding from using scalable techniques, but not all the ordinary people who work at most at two levels of abstraction and scale use the same concepts which are inappropriate to use at the highest levels. Carl: This technology stack for these things, as you say, they’re all these levels, they’re different abstractions, et cetera. These are complex beasts. Francesco: This leaves some food for thought. Thank you so much for being part of this. All: Thank you. [00:03:55] [END OF AUDIO]", "date": "2019-02-19"},
{"website": "Erlang-Solutions", "title": "ex_rabbit_pool open source AMQP connection pool", "author": ["Simon Benitez"], "link": "https://www.erlang-solutions.com/blog/ex_rabbit_pool-open-source-amqp-connection-pool/", "abstract": "Background A couple of months ago we started writing an open-source continuous delivery system which we called Buildex. Buildex is implemented as a collection of microservices which: – Monitor Github repositories – Respond to changes by sending notifications over AMQP (RabbitMQ) – Trigger software builds running in Docker containers – Expose credentials (SSH keys) to containers as volume mounts. Buildex was born of the frustrations and limitations we faced when attempting to debug production failures in other build systems. Buildex was also influenced by security, privacy, and pricing concerns we encountered while using some of the popular commercial SaaS offerings. Buildex Component overview The principal components of Buildex are: The Poller , which uses the excellent Tentacat Github API to poll GitHub for new tags, the Builder , which checks out and builds the project inside Docker containers, and the Datastore which uses ueberauth OAuth to delegate authorization to Github, and allows users to configure projects for the continuous integration pipeline. The following diagram provides a high-level overview of the Buildex micro-services and supporting infrastructure. Although we could have used distributed Erlang for everything, we wanted to have the ability for operations and develops to easily monitor inter-service communication and define message replication as they pleased, for example with logging or integration with 3rd party services (email, auditing, manually triggering builds, etc). We decided to use RabbitMQ for inter-service communication, which in turn led to our need for a higher level connection handling and channel management library. ex_rabbit_pool As part of the work to support these services, we created a RabbitMQ connection management and pooling library called ex_rabbit_pool . ex_rabbit_pool is built upon the Elixir library AMQP which is an Elixir wrapper for the RabbitMQ client rabbitmq-erlang-client , and the Erlang resource pooling library poolboy . In this post, we will take you through the lessons learnt and steps taken to implement ex_rabbit_pool , but first, a quick explanation of channels and connections. Channels and connections Connections, as in TCP connections, to RabbitMQ, are expensive to create and a finite system resource, Channels, are a lightweight abstraction over connections. Quoting the RabbitMQ documentation , “AMQP 0-9-1 connections are multiplexed with channels that can be thought of as ‘lightweight connections that share a single TCP connection’”. In the Erlang world, you will typically assign a channel per worker process, on other platforms you would assign a channel per-thread. Basic steps involved in using AMQP Open a connection\n{:ok, connection} =  AMQP.Connection.open(host: \"localhost\", port: 5672)\nOpen a channel with that connection\n{:ok, channel} = AMQP.Channel.open(connection)\nBind the channel to a queue via an exchange\nAMQP.Queue.bind(channel, \"test_queue\", \"test_exchange\")\n(Listening) Subscribe to the queue\nAMQP.Queue.subscribe (channel, \"test_queue\", fn(payload, meta) -> IO.puts(\"Received: #{payload}\") end)\n(Sending) Publish message to queue\nAMQP.Basic.publish(channel, \"test_exchange\", \"\", \"Hello, World!\" The process tree A supervision tree diagram is a good way to get an overview of a BEAM system, so let’s start by examining the supervision tree of ex_rabbit_pool . – PoolSupervisor supervises the ConnectionPool – ConnectionPool manages a collection of ConnectionWorker processes – Each ConnectionWorker manages a RabbitConnection – ConnectionWorker uses the RabbitConnection to create a pool of RabbitChannels The principal components are the PoolSupervisor and RabbitConnection. We will examine the implementation of both components over the course of the following sections. Defining the PoolSupervisor First, we define a top-level supervisor, PoolSupervisor , which will be responsible for managing the connection pool. PoolSupervisor is intended to be started within an application so we leave the start-up management to the application developer (here’s how we manage the pool supervisor in Builder ). PoolSupervisor provides an exported function, start_link/1 which takes as arguments both the RabbitMQ connection parameters and connection pool configuration. defmodule ExRabbitPool.PoolSupervisor do\n  use Supervisor\n\n  alias ExRabbitPool.Worker.SetupQueue\n\n  @type config :: [rabbitmq_config: keyword(), rabbitmq_conn_pool: keyword()]\n\n  @spec start_link(config()) :: Supervisor.on_start()\n  def start_link(config) do\n    Supervisor.start_link(__MODULE__, config)\n  end\n\n  @impl true\n  def init(config) do\n    children = []\n    opts = [strategy: :one_for_one]\n    Supervisor.init(children, opts)\n  end\nend if you are not familiar with poolboy you can read a good introduction over at Elixir School, continuing on, here is the pool configuration that we will use: [\n  rabbitmq_conn_pool: [\n    name: {:local, :connection_pool},\n    worker_module: ExRabbitPool.Worker.RabbitConnection,\n    size: 2,\n    max_overflow: 0\n  ]\n] Note the attribute worker_module which is a GenServer module, instances of which will be managed as the pooled resource, in this case, RabbitConnection is the GenServer in charge of connecting to RabbitMQ. We now extract the RabbitMQ and pool configuration from the start_link/1 parameters. AMQP provides helpful defaults for managing its connections so we only need to pass the RabbitMQ configuration to AMQP. We do so, and configure poolboy to manage our connection pool: rabbitmq_conn_pool = Keyword.get(config, :rabbitmq_conn_pool)\nrabbitmq_config = Keyword.get(config, :rabbitmq_config, [])\n{_, pool_id} = Keyword.fetch!(rabbitmq_conn_pool, :name)\n:poolboy.child_spec(pool_id, rabbitmq_conn_pool, rabbitmq_config) Let’s take a look at the full implementation: defmodule ExRabbitPool.PoolSupervisor do\n\n  use Supervisor\n\n  alias ExRabbitPool.Worker.SetupQueue\n\n  @type config :: [rabbitmq_config: keyword(), rabbitmq_conn_pool: keyword()]\n\n  @spec start_link(config()) :: Supervisor.on_start()\n  def start_link(config) do\n    Supervisor.start_link(__MODULE__, config)\n  end\n\n  @impl true\n  def init(config) do\n    rabbitmq_conn_pool = Keyword.get(config, :rabbitmq_conn_pool)\n    rabbitmq_config = Keyword.get(config, :rabbitmq_config, [])\n    {_, pool_id} = Keyword.fetch!(rabbitmq_conn_pool, :name)\n    children = [\n      :poolboy.child_spec(pool_id, rabbitmq_conn_pool, rabbitmq_config)\n    ]\n    opts = [strategy: :one_for_one]\n    Supervisor.init(children, opts)\n  end\nend We continue by defining the worker_module responsible for handling the connection to RabbitMQ. The worker_module will hold the connection, a list of multiplexed channels to RabbitMQ, a corresponding list of monitors (we will explain their purpose later in this post), an adapter (so we can plug a stub implementation later on for testing purposes) and the configuration so we can configure some parts of our application dynamically. Implementing the RabbitConnection With these attributes in place, we create a State module with an associate state struct to represent the internal state of our RabbitConnection GenServer. defmodule ExRabbitPool.Worker.RabbitConnection do\n  use GenServer\n\n  defmodule State do\n    @type config :: keyword()\n\n    @enforce_keys [:config]\n    @type t :: %__MODULE__{\n            adapter: module(),\n            connection: AMQP.Connection.t(),\n            channels: list(AMQP.Channel.t()),\n            monitors: [],\n            config: config()\n          }\n\n    defstruct \n              adapter: ExRabbitPool.RabbitMQ,\n              connection: nil,\n              channels: [],\n              config: nil,\n              monitors: []\n     end\n\n  def start_link(config) do\n    GenServer.start_link(__MODULE__, config, [])\n  end In the init/1 callback, we send an asynchronous: connect message so the connection to RabbitMQ will be initialised separately without blocking the GenServer startup phase (supervisors create the child processes sequentially, and expect them to start very quickly). We also trap exits so all linked connections and multiplexed channels can be restarted by this worker when they crash. Take a look at our default adapter for the full implementation . def init(config) do\n  Process.flag(:trap_exit, true)\n  send(self(), :connect)\n\n  # split our options from the RabbitMQ client adapter\n  {opts, amqp_config} = Keyword.split(config, [:adapter])\n  adapter = Keyword.get(opts, :adapter, ExRabbitPool.RabbitMQ)\n\n  {:ok, %State{adapter: adapter, config: amqp_config}}\nend Now we implement our first GenServer callback, handle_info to handle the: connect message. Within the handler, we open the connection to RabbitMQ using the adapter. In the case where we raise an error, we can schedule another retry or stop the GenServer. In order to retry, we attempt to reconnect to RabbitMQ asynchronously using Process.send_after(self(), :connect, 1000) . If the connection is established successfully we create and start the required RabbitMQ channels, create another pool inside our worker for those created channels and link them to our process. We need to link the channels to our process so that if a client closes a channel or a channel crashes we can respond by creating another in order to maintain the channel pool at the same size, we then store them in the RabbitConnection state for later reuse Connecting to RabbitMQ and opening the channels def handle_info(:connect, %{adapter: adapter, config: config} = state) do\n    case adapter.open_connection(config) do\n      {:error, reason} -> schedule_connect(config)\n        {:noreply, state}\n      {:ok, %{pid: pid}} ->\n        true = Process.link(pid)\n        num_channels = Keyword.get(config, :channels, 1000)\n        channels =\n          do_times(num_channels, 0, fn ->\n            {:ok, channel} = start_channel(adapter, connection)\n            true = Process.link(channel.pid)\n\n            channel\n          end)\n        {:noreply, %State{state | connection: connection, channels: channels}}\n    end\n  end\n\n  @spec do_times(non_neg_integer(), non_neg_integer(), (() -> any())) :: [any()]\n  defp do_times(limit, counter, _function) when counter >= limit, do: []\n\n  defp do_times(limit, counter, function) do [function.() | do_times(limit, 1 + counter, function)] end When creating the channels we need to ensure the connection process is alive or return an error instead then we open a channel using our client adapter and return it’s result @spec start_channel(module(), AMQP.Connection.t()) :: {:ok, AMQP.Channel.t()} | {:error, any()}\n  defp start_channel(client, connection) do\n    if Process.alive?(connection.pid) do\n      case client.open_channel(connection) do\n        {:ok, _channel} = result ->\n          Logger.info(\"[Rabbit] channel connected\")\n          result\n\n        {:error, reason} = error ->\n          Logger.error(\"[Rabbit] error starting channel reason: #{inspect(reason)}\")\n          error\n\n        error ->\n          Logger.error(\"[Rabbit] error starting channel reason: #{inspect(error)}\")\n          {:error, error}\n      end\n    else\n      {:error, :closing}\n    end\n  end Now we have a pool of connections to RabbitMQ and each connection has a pool of channels that clients can check out and check-in again, but at this moment we haven’t yet implemented those features, let’s do that now. For the checkout_channel handler, we also need to handle some edge cases. Firstly, the case when we are still unable to connect to RabbitMQ. In such a situation we need to tell the client to retry later with a {:error,:disconnected} result. Secondly, the situation where there are no channels in the pool, this can happen when channels are already checked out by other clients – in such a situation we have a couple of options, either we can use the async/reply pattern to block and wait a period of time for the new channel to be created or we can return {:error, :out_of_channels} which is simpler and pushes the retry handling decision to the user, to retry later or fail immediately. After we have covered the edge cases when checking out a channel, we can proceed with our implementation of the actual checkout which does the following: it will monitor the client which is claiming the channel, this way if a client crashes we can return the claimed channel back to the pool so another client can reuse it, return {:ok, channel} and then save the monitor reference with the assigned channel into the RabbitConnection state for safe keeping. def handle_call(:checkout_channel, _from, %State{connection: nil} = state) do\n  {:reply, {:error, :disconnected}, state}\nend\n\ndef handle_call(:checkout_channel, _from, %{channels: []} = state) do\n  {:reply, {:error, :out_of_channels}, state}\nend\n\ndef handle_call(\n      :checkout_channel,\n      {from_pid, _ref},\n      %{channels: [channel | rest], monitors: monitors} = state\n    ) do\n  monitor_ref = Process.monitor(from_pid)\n\n  {:reply, {:ok, channel},\n    %State{state | channels: rest, monitors: [{monitor_ref, channel} | monitors]}}\nend We now implement the functionality to return a channel back into the pool, doing so requires the following steps: – Remove the channel from the list of channels – Remove the monitor from the client holding the connection – Unlink the channel from our connection process – Delete the monitor from the RabbitConnection state list of monitored processes – Close the channel – Start a new channel You may notice that we are stopping and creating a new channel every single time we return a channel into the pool, and this is because when a client uses a channel it can change its state, that means, channels are stateful, that’s why we need to create a new channel to replace the old one so we don’t have weird errors if we were reusing channels @impl true\n  def handle_cast(\n        {:checkin_channel, %{pid: pid} = old_channel},\n        %{connection: conn, adapter: adapter, channels: channels, monitors: monitors} = state\n      ) do\n    # only start a new channel when checkin back a channel that isn't removed yet\n    # this can happen when a channel crashed or is closed when a client holds it\n    # so we get an `:EXIT` message and a `:checkin_channel` message in no given\n    # order\n    if find_channel(pid, channels, monitors) do\n      new_channels = remove_channel(channels, pid)\n      new_monitors = remove_monitor(monitors, pid)\n\n      case replace_channel(old_channel, adapter, conn) do\n        {:ok, channel} ->\n          {:noreply, %State{state | channels: [channel | new_channels], monitors: new_monitors}}\n\n        {:error, :closing} ->\n          # RabbitMQ Connection is closed. nothing to do, wait for reconnection\n          {:noreply, %State{state | channels: new_channels, monitors: new_monitors}}\n      end\n    else\n      {:noreply, state}\n    end\n  end\n\n  defp find_channel(channel_pid, channels, monitors) do\n  Enum.find(channels, &(&1.pid == channel_pid)) ||\n    Enum.find(monitors, fn {_ref, %{pid: pid}} ->\n      pid == channel_pid\n    end)\n  end\n\n  defp replace_channel(old_channel, adapter, conn) do\n    true = Process.unlink(old_channel.pid)\n    # omit the result\n    adapter.close_channel(old_channel)\n\n    case start_channel(adapter, conn) do\n      {:ok, channel} = result ->\n        true = Process.link(channel.pid)\n        result\n\n      {:error, _reason} = error ->\n        error\n    end\n  end We now have a reasonably complete connection worker, but we still need to implement error handling for crashing connections, channels crashing/closing and exceptions raised within the clients. Implementing a handler for crashed connections As we already have our worker process linked to the RabbitMQ connection process, we will receive a message corresponding to {:EXIT, pid, reason} if the connection process terminates. We pattern match to ensure the failing process pid is the same as the connection process pid, discard the connection and attempt to schedule reconnection in the background using Process.send_after/3 . def handle_info({:EXIT, pid, reason}, %{connection: %{pid: pid}, config: config} = state) do\n  Process.send_after(self(), :connect, 1000)\n  {:noreply, %State{state | connection: nil}}\nend In the case where the connection crashes and we have channels linked to the connection process, we will receive messages informing us about the crashed channels. We must handle the connection crash in two ways, firstly where the connection already crashed and is now nil, and secondly where the connection remains active but a channel crashed or was closed. Handling a crashed channel Let’s implement the first, where the connection already crashed. We pattern match on the nil connection, then we remove the crashed pid from the channels list and remove any monitor associated with that process identifier. Done . def handle_info(\n      {:EXIT, pid, reason},\n      %{connection: nil, channels: channels, monitors: monitors} = state\n    ) do\n  Logger.error(\"[Rabbit] connection lost, removing channel reason: #{inspect(reason)}\")\n  new_channels = remove_channel(channels, pid)\n  new_monitors = remove_monitor(monitors, pid)\n  {:noreply, %State{state | channels: new_channels, monitors: new_monitors}}\nend\n\ndefp remove_channel(channels, channel_pid) do\n  Enum.filter(channels, fn %{pid: pid} ->\n    channel_pid != pid\n  end)\nend\n\ndefp remove_monitor(monitors, channel_pid) when is_pid(channel_pid) do\n  monitors\n  |> Enum.find(fn {_ref, %{pid: pid}} ->\n    channel_pid == pid\n  end)\n  |> case do\n    # if nil means DOWN message already handled and monitor already removed\n    nil ->\n      monitors\n\n    {ref, _} = returned ->\n      true = Process.demonitor(ref)\n      List.delete(monitors, returned)\n  end\nend Handling the second case when the connection remains open but a channel crashed is the same as handling the case of a crashed connection with the additional requirement that we need to create another channel, link it to our worker, and add the channel to the pool. @impl true\n  def handle_info(\n        {:EXIT, pid, reason},\n        %{channels: channels, connection: conn, adapter: adapter, monitors: monitors} = state\n      ) do\n    Logger.warn(\"[Rabbit] channel lost reason: #{inspect(reason)}\")\n    # don't start a new channel if crashed channel doesn't belongs to the pool\n    # anymore, this can happen when a channel crashed or is closed when a client holds it\n    # so we get an `:EXIT` message and a `:checkin_channel` message in no given\n    # order\n    if find_channel(pid, channels, monitors) do\n      new_channels = remove_channel(channels, pid)\n      new_monitors = remove_monitor(monitors, pid)\n\n      case start_channel(adapter, conn) do\n        {:ok, channel} ->\n          true = Process.link(channel.pid)\n          {:noreply, %State{state | channels: [channel | new_channels], monitors: new_monitors}}\n\n        {:error, :closing} ->\n          # RabbitMQ Connection is closed. nothing to do, wait for reconnections\n          {:noreply, %State{state | channels: new_channels, monitors: new_monitors}}\n      end\n    else\n      {:noreply, state}\n    end\n  end Now we have everything covered for handling connection and channel errors, crashes and closes, but we still need to implement the logic for when a client being monitored crashes without returning the channel back to the pool, in this case, we should remove the client from the monitors list and return the channel to the active channels list. @impl true\n  def handle_info(\n        {:DOWN, down_ref, :process, _, _},\n        %{channels: channels, monitors: monitors, adapter: adapter, connection: conn} = state\n      ) do\n    monitors\n    |> Enum.find(fn {ref, _chan} -> down_ref == ref end)\n    |> case do\n      nil ->\n        {:noreply, state}\n\n      {_ref, old_channel} = returned ->\n        new_monitors = List.delete(monitors, returned)\n\n        case replace_channel(old_channel, adapter, conn) do\n          {:ok, channel} ->\n            {:noreply, %State{state | channels: [channel | channels], monitors: new_monitors}}\n\n          {:error, :closing} ->\n            # RabbitMQ Connection is closed. nothing to do, wait for reconnection\n            {:noreply, %State{state | channels: channels, monitors: new_monitors}}\n        end\n    end\n  end Now that we have covered everything related to connection and channel handling we need to implement an API for our library. This will require us to add some convenience functions for our GenServer and create an API layer to perform the work of getting a connection worker out of the connection pool and executing a function in the context of a channel for us. Firstly, we define functions for checking channels in and out of the pool: def checkout_channel(pid) do\n  GenServer.call(pid, :checkout_channel)\nend\n\ndef checkin_channel(pid, channel) do\n  GenServer.cast(pid, {:checkin_channel, channel})\nend Having done so, we then proceed to create our library API, where we define functions for retrieving a connection worker from the pool, and to execute a function in the context of a channel. When checking out a connection worker we don’t care about isolating access to each process – we use a pool purely in order to spread load (pool config strategy :fifo). The supplied function will receive one of: – A tuple containing :ok and a channel or – An error tuple which the user can deal with as they please. We also define the basic functions for checking in and out a channel manually for a connection worker. defmodule ExRabbitPool do\n  alias ExRabbitPool.Worker.RabbitConnection, as: Conn\n\n  @type f :: ({:ok, AMQP.Channel.t()} | {:error, :disconected | :out_of_channels} -> any())\n\n  @spec get_connection_worker(atom()) :: pid()\n  def get_connection_worker(pool_id) do\n    conn_worker = :poolboy.checkout(pool_id)\n    :ok = :poolboy.checkin(pool_id, conn_worker)\n    conn_worker\n  end\n\n  @spec with_channel(atom(), f()) :: any()\n  def with_channel(pool_id, fun) do\n    pool_id\n    |> get_connection_worker()\n    |> do_with_conn(fun)\n  end\n\n  def checkout_channel(conn_worker) do\n    Conn.checkout_channel(conn_worker)\n  end\n\n  def checkin_channel(conn_worker, channel) do\n    Conn.checkin_channel(conn_worker, channel)\n  end\n\n    defp do_with_conn(conn_worker, fun) do\n    case checkout_channel(conn_worker) do\n      {:ok, channel} = ok_chan ->\n        try do\n          fun.(ok_chan)\n        after\n          :ok = checkin_channel(conn_worker, channel)\n        end\n\n      {:error, _} = error ->\n        fun.(error)\n    end\n  end\nend With this code implemented, we now want to quickly verify our code in the Elixir interactive console IEX, but before we do so, we’ll need access to a running RabbitMQ instance. Docker makes this trivial to do. Let’s work our way through the required steps. First, we pull the RabbitMQ Docker image from the Docker hub: docker pull rabbitmq:3.7.7-management Then we run the RabbitMQ image in another terminal in the foreground, mapping both its web management interface on port 15672 and its message port 5672, to the host loopback interface. docker run --rm --hostname bugs-bunny --name roger_rabbit -p 5672:5672 -p15672:15672 rabbitmq:3.7.7-management After waiting for RabbitMQ initialization to complete, we proceed to copy the following code into the Elixir console in order to verify that everything works as expected: First the configuration: rabbitmq_config = [channels: 1]\n\nrabbitmq_conn_pool = [\n  name: {:local, :connection_pool},\n  worker_module: ExRabbitPool.Worker.RabbitConnection,\n  size: 1,\n  max_overflow: 0\n] Then we create an instance of the PoolSupervisor: {:ok, pid} =\nExRabbitPool.PoolSupervisor.start_link(\n    rabbitmq_config: rabbitmq_config,\n    rabbitmq_conn_pool: rabbitmq_conn_pool\n  ) And finally, we verify everything is working by publishing a message to the AMQP queue “ex_rabbit_pool” via the same channel we published it on. ExRabbitPool.with_channel(:connection_pool, fn {:ok, channel} ->\n  queue = \"ex_rabbit_pool\"\n  exchange = \"my_exchange\"\n  routing_key = \"example\"\n  {:ok, _} = AMQP.Queue.declare(channel, queue, auto_delete: true, exclusive: true)\n  :ok = AMQP.Exchange.declare(channel, exchange, :direct, auto_delete: true, exclusive: true)\n  :ok = AMQP.Queue.bind(channel, queue, exchange, routing_key: routing_key)\n  :ok = AMQP.Basic.publish(channel, exchange, routing_key, \"Hello World!\")\n  {:ok, msg, _} = AMQP.Basic.get(channel, queue, no_ack: true)\n  IO.puts(msg)\nend) The message “Hello World!” is printed to the console. Tada! The library works. If you want to see an example of a real-life consumer, take a look at the implementation of the Buildex Jobs Consumer . Future work and improvements There are some nice enhancements we would like to make and would be very happy to accept quality contributions. All feedback is welcome, this would be a really nice Elixir project to make your first open source contribution! – Moving the monitors logic to ETS – Implementing backoff algorithms for handling reconnections – Overflow support for the channels pool – Support for async/reply channel checkout – A circuit breaker to fail fast after some amount of retrials to connect to RabbitMQ Feel free to explore the source code of this blog post here and give it a try! Credits Special thanks to Bryan Hunt for his help editing this blog and reviewing all the code that was produced while constructing this library, also special thanks to our RabbitMQ experts for reviewing our code and ensuring we were following the best practices when handling connections and channels inside our library. Our work with RabbitMQ.", "date": "2019-04-2nd"},
{"website": "Erlang-Solutions", "title": "Remembering Joe, a Quarter of a Century of Inspiration and Friendship", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/remembering-joe-a-quarter-of-a-century-of-inspiration-and-friendship/", "abstract": "I first came across the name Joe Armstrong in 1994, when I bought the first edition of Concurrent Programming in Erlang, a book he co-authored. Our first interaction happened in 1995, when I was looking for a company interested in sponsoring my Master’s thesis. I dialled the number for Ellemtel Utvecklings AB, home of the Ericsson computer science laboratory, asking to be connected to Joe Armstrong. Getting a Hello, my unprepared opening line was Skall jag ta det på Engelska, or would you prefer if I took it in Swedish? A silent pause was followed by laughter, the same laughter many of us have come to associate with Joe. Shouting at each other through the walls Internships at the computer science lab were an immersion in enthusiasm and creativity. The lab had an academic, non conformist, almost anti-establishment, feel to it. Segregated to the corner of the building, pipe and cigarette smoke coming from some of the rooms. I am sure the cleaning staff were asked to “forget” the department existed, or maybe, they did not dare venture in there after hours. ISO 9000 reviews (and office moves) were a conspiracy to get Joe (and Robert) to clean their desks. But what mattered was not how neat your desk was, but the drive to innovate with an aim to further our understanding in computer science. You did not use an existing web server, you wrote your own so as to understand how HTTP worked and suggest improvements and extensions. You did not take Ericsson’s high security for granted, you found your way around firewalls and tried to outsmart the highly paid system administrators. When I got distributed Erlang working between my University and Ericsson UNIX accounts, I asked Joe if this was right. He walked in the hall and laughed loudly. It was his call for a show and tell, sharing knowledge with his curious colleagues who quickly make their way to his office. Sometimes, there was no need to make it there in person. Mike and Robert were always given the office next to his, so they could shout at each other through the walls. Ahead of his time Brainstorming sessions with Joe were the best. We once spent a good part of an afternoon finding a solution to a problem. I ended up working into the early hours of the morning on the implementation, and indeed, it worked. When Joe walked into the office the following day, he pops his head into my room and says You know the solution we came up with yesterday? Forget about it, it will not work. My jumping up and down saying it does work, it works, here it is, was fruitless. He replied, no, we were wrong , and walked off. He was often ahead of his time, in both thoughts and problems he was solving, and had probably discovered some edge and borderline cases to our solution which was too frivolous to share. This is where you learnt to filter his ideas, take those you understood or believed in and parked those you did not like or understand. Solution to a problem Sometimes, you would be walking outside his office, he would wave you in and share one of his daily epiphanies, hacks or articles and academic papers on a one-on-one basis. One of these hacks happened in conjunction with a major Erlang release in 1995. They had snuck in List Comprehensions and Funs in the language (product management had prioritized other features) and Joe showed me how they allowed you to implement clean and easy to maintain code, hiding recursive patterns and isolating side effects in a single location. Showing me the shortest implementation of QuickSort I had ever seen, he mentions you can solve the Eight Queens problem in six lines of code. I spent two nights trying to figure out a solution in my head, to no avail. Weary of a third sleepless night, I ask Joe for a solution only to be told: I have no idea, I never solved it myself. Look on the internet. WebCrawler and Lycos, whom had been around for a year, failed to provide a solution. Whilst I love Joe to bits, that particular day, I could have strangled him. I think he was used to it (mentee students wanting to strangle him), as he claimed that we have always mistreated geniuses ahead of their time. Guttenberg was bankrupted, Galileo escaped the death penalty when put on trial, whilst Darwin’s theories were mocked by many. Fast forward to ten – fifteen years ago, governments and corporations alike were, instead of embracing peer to peer, persecuting those who built services on top of them. Ok, governments and corporations might not have strangled anyone, but they sure did sue them or try to put them in jail. Joe was disgusted by their treatment (he was the one who showed me how bittorrent worked), but he did smile when I told him one of the Pirate Bay founders claimed in an interview he was going to use his time in jail to learn Erlang. Leaving Ericsson In December 1998, a few days after Erlang was released as open source, Joe and some of his Computer Science Lab colleagues left to found Bluetail, later acquired by Alteon Websystems, who in turn got acquired by Nortel. We were both very busy building products these years, and met mainly at conferences and workshops. I do recall being told of the buzz at Nortel when Erlang Solutions launched its first website, and when the dotcom bubble burst, receiving a call from Pekka, a colleague of his, saying he, Joe, Robert, Jane and many others in the team had just been fired! It did not take long for Nortel, in 2003, to start advertising for Erlang developers with ten years of experience, not realising they had recently let go seven of the ten people who at the time fit the bill. Nortel would not have hired me, at the time I only had 8 years of Erlang. You’ve got to love random acts of management. Seeing his redundancy as an opportunity, he decided to make a stint in academia, working on his PhD through SICS, the Swedish Institute of Computer Science. Making reliable distributed systems in the presence of software errors is a must read for anyone trying to understand the rationale behind systems you write once and run forever. Folklore (Joe after a few beers) has it that after a couple of years at SICS, he walks into his supervisor’s office and submits the full thesis. His supervisor looks up surprised and says, This is not how it is supposed to work. I should be giving you feedback as you write it. Oh , goes Joe, Let me know what you think. After SICS, Joe returns to Ericsson, completing the full circle. In 2014, becoming an Adjunct Professor at KTH, continuing to inspire students through his magical ability to pique their curiosity. Just like he inspired me back in 1995. Quest to learn Joe knew that programming languages were not about popularity or beauty contests. They were all about solving problems and progressing the industry. He was just as excited about Rich Hickey, Don Syme or Simon Peyton-Jones’s success stories, and wished Haskell, F# and Clojure to do as well. More recently, he got all excited about Sylvan Clebsch’s Ponylang. When we had dinner with the Go team, he enthusiastically explained Erlang’s concurrency error handling mechanisms to Ken Thompson. He was encouraging Ken to integrate similar semantics in Go. Erlang will not be around forever, he once told me in the 90s. Something better will come along. I don’t think we realised back then, that whatever will come along, is going to be heavily influenced by his work! There were obviously languages he did not like, C++, Java and JavaScript (read Node.js), backed by well motivated reasons. But despite not liking them, he wanted to meet the inventors behind these languages in his quest to learn more, understand their motivation and share his ideas. He asked me to try and get Brendan Eich to speak at the Erlang Factory in San Francisco and was hoping to meet Bjarne Stroustrup at Code Mesh in London. I once had to drag him out of Google in Mountain View when James Gosling was in his office and we happened to walk outside. Joe asked me if I thought it would be impolite for him to just go in and introduce himself, to which I suggested (for the sake of our host) that it might be better not to, and instead, get a proper introduction. The Trio, or even Quartet! We are talking about Joe, but let’s not forget that for a good part of his career, he was part of a team together with Robert and Mike. They were led by a patient Bjarne, who gave them free rein in solving telecom-related and being interested in what, and not how. It was the space they needed to innovate. I am not sure any of them on their own have been able to create Erlang, but together, they were able to leverage each other’s strengths and succeeded at creating both Erlang and OTP. I tried multiple times to get them on stage together to show the dynamics of the trio, but it was always scripted (even Joe’s parts!). I succeeded the third time, at the 2015 San Francisco Erlang Factory Keynote, From WhatsApp to Outer Space , where the last 15 minutes of the talk, they go off script and start asking each other questions, to the roaring laughter of the audience. It shows Joe to be the inventor, Robert to be the one interested in aesthetics and Mike the finisher and end user with industry experience. It was wonderful to see their team dynamics, friendship and bonds still in place and going strong, twenty years on. Beyond Erlang In more recent years, Joe started talking about topics beyond Erlang. His Strangeloop keynote, The Mess We’re In , focused on one of his pet peeves in the software industry. As a result of computers becoming faster, software seems to become more complex, and hence, slower. He refers to the laws of physics, something the software industry has tried to defy for decades. Joe (a physicist by training) applied the laws of physics to computer science and distributed systems. Synchronously passing messages, shared memory or attempting to share data faster than the speed of light. If you hear a programmer say you can’t do this, it defies the laws of physics, you now know where their quote comes from. One of the many projects he was planning on doing after retirement was to interview his heroes, and if he got enough interviews, publish the results in a book. He suggested interviewing Alan Kay on stage at Code Mesh in London in 2016. I have never seen a conference audience so mesmerised. A similar reaction happened in the Let’s #TalkConcurrency panel discussion in November 2018, where we were able to get Sir Tony Hoare, Carl Hewitt and Joe Armstrong to discuss the past, present and future of concurrency models. Joe was originally supposed to run the interview, but as many of us felt he had just as much to say as Tony and Carl, we got him on the panel instead. I am glad we did, as we covered three different, but overlapping approaches to concurrency, each created to solve a different problem. Travelling to Cambridge with Joe for the recording, it was obvious he was not well. His lungs were at 60% capacity, and he often ran out of breath. The pulmonary fibrosis was evident. But we all hoped they would be able to keep it under control. Let’s #TalkConcurrency Panel Discussion, from left: Joe Armstrong, Francesco Cesarini, Sir Tony Hoare, Carl Hewitt Lung Research On Saturday April 20th, 2019, I get the dreaded WhatsApp message that Joe had just passed away. Just the day before, the news was more positive, they had narrowed down the diagnosis and had adapted the treatment accordingly. Unfortunately, it was too late. Joe leaves behind his wife, Helen, his children Claire and Thomas and two cats, Zorro and Daisy, who figure in various programs. Joe had named a previous generation of cats Wittgenstein and Schopenhauer but reality, in the form of Helen, renamed them; they became known as Tubby and Tiger. He also leaves behind many friends, colleagues, students and followers who will continue his work, spreading his ideas and ensuring they evolve and keep on getting embedded in mainstream programming practices. We are all glad Joe got to see how his work has impacted the world around him, and how Erlang Style Concurrency and OTP are being adopted in the realm of distributed, fault tolerant systems which have to scale on multi-core architectures. Basically, the immediate future. Thank you Joe for being yourself. Thank you Helen for supporting him in doing what he loved the most. And thank you Claire and Thomas for helping bring up an older brother called Erlang. As the old saying goes, no one truly understands concurrency until they have their second child (or cat). Helen and Claire kindly requested donations to be made to lung research, with Claire starting a fundraiser on Facebook: https://www.facebook.com/donate/312270252802913/312270286136243?sfns=mo In the US there’s the American Lung Association: https://www.lung.org/get-involved/ways-to-give/ In the UK British Lung Foundation: https://www.blf.org.uk/donate In Sweden Hjärt-lungfonden: https://www.hjart-lungfonden.se/Stod-oss/ Remembering Joe <3 #RememberingJoe with credits to the wonderful community – many thanks for sharing <3 Many thanks and photo credits to: @samaaron; @puredanger; @rolodato; @bentanweihao; @MachinesAreUs; @bltroutwine; @danielozcps; @christian_fei; @bryan_hunter; @EuenLopez; @cacorriere; @t_sloughter; @acscherp; @strangeloop_stl; @janjiss; @zhanghu; @MarioAquino; @MakisOtman; @perlhack; @colrack; @aodanne; @gsarwate; @scrogson; @gar1t; @RogerSuffling; @gturitto; @lelff Special thanks to @michaelbridge for the feature photograph.", "date": "2019-04-23rd"},
{"website": "Erlang-Solutions", "title": "Introducing Telemetry", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/introducing-telemetry/", "abstract": "We need monitoring “Let it crash” has been a long-running mantra in the BEAM world. While it might be misinterpreted, there is some merit to it – our software will do unexpected things, and more often than not, the only viable choice is to crash and start over. But simply restarting parts of our application is not sufficient – we should understand what was the cause of the error, and handle it properly in the future releases. We also need to know that the error occurred at all and how it affected our customers! To enable both of these things, we need a way to introspect and analyze our app’s behaviour at runtime – we need monitoring. Telemetry is a new open source project aiming at unifying and standardising how the libraries and applications on the BEAM are instrumented and monitored. It is a suite of libraries, developed for the last couple of months by Erlang Solutions in collaboration with the Elixir core team and other contributors. But with existing metrics projects such as exometer and folsom , which have both served the community well over the years, why would we need yet another solution? It might start to feel like in the popular comic strip: By design, Telemetry does not try to cover every use case out there. Rather, it provides a small and simple interface for instrumenting your code, and allows anyone to hook into the instrumentation points at runtime. This enables modularity – projects like Ecto or Plug only need to rely on the core library , and engineers building applications can use the data exposed by those libraries for monitoring their systems. Let’s dive a little bit deeper into the rationale behind Telemetry and its design. The tree has fallen in the forest.. At the core of Telemetry lies the event. The event indicates that something has happened: an HTTP request was accepted, a database query returned a result, or the user has signed in. Each event can have many handlers attached to it, each performing a specific action when the event is published. For example, an event handler might update a metric, log some data, or enrich a context of distributed trace. This becomes extremely convenient when libraries emit Telemetry events. Usually, we don’t write our own web framework or database client, we use an existing package. The library can provide the instrumentation data via events, and our code can handle it in a way that suits our needs. The only thing we need to do is to implement the handlers and attach them when our application starts! For example, Ecto since version 3.0.0 publishes an event on each database query. We could write a handler which logs whenever a total query time exceeds 500ms: defmodule MyApp.Monitoring do\n  require Logger\n\n  def log_slow_queries(_event, measurements, metadata, _config) do\n    if System.convert_time_unit(measurements.total_time, :native, :millisecond) > 500 do\n      Logger.warn(\"Query #{inspect(metadata.query)} completed in #{measurements.total_time}ms\")\n    end\n  end\nEnd Here measurements and metadata are properties of a single event – each Telemetry event carries these values. This handler needs to be attached at runtime, for example when our application starts. Assuming that our Ecto repo is called MyApp.Repo , we attach the handler using the code below: :telemetry.attach(\n  \"slow-query-logger\",\n  [:my_app, :repo, :query],\n  &MyApp.Monitoring.log_slow_queries/4,\n  :no_config\n  ) We specify the name of our handler (which needs to be unique across the system), the event we are attaching it to, the function to be invoked each time the event is emitted, and the handler config which is passed as the last argument to our handler on every invocation. ..and there was no one around to hear it Because Telemetry is designed to have a small performance footprint, there is almost no cost associated with including it even in the most popular Elixir libraries – it is already used by Ecto and Plug, and it is coming to Phoenix soon. Telemetry requires only a single ETS lookup when an event is published, and all handlers are executed synchronously in the process emitting the event, which means that there are no bottlenecks and single points of failure in the whole library. Ecosystem Apart from the core Telemetry library , which provides the interface for emitting and handling events, we have built additional tools addressing common use cases related to monitoring. Telemetry.Poller allows you to perform measurements periodically and emit them as Telemetry events. When you include the library in your project, the default Poller process is started, publishing measurements related to the Erlang VM, like memory usage and the length of run queues. Telemetry.Metrics provides a bunch of functions for declaring metrics based on the events. For example, the definition counter(\"my_app.repo.query\") means that we want to count how many database queries were made by Ecto. Apart from the counter, Telemetry.Metrics also defines some other aggregations: sum, last value (sometimes referred to as gauge) and distribution. It also supports multi-dimensional metrics via tags and unit conversions. After the metrics are declared, they need to be fed to the reporter, which attaches relevant event handlers and forwards metrics to the monitoring system of choice at runtime. Currently, there are two reporters available on Hex: one for Prometheus and one for StatsD . What’s next? Telemetry core reached a stable API, and now is the right time for including it in libraries, so that their users can benefit from exposed instrumentation data. But we cannot do that on our own – Telemetry is a community run project, and without contributors it won’t be able to flourish. So we encourage everyone – Telemetry has had an active group of contributors from the very early days of the project and we would love you to be part of its growth and adoption too. If you want to get involved, you can integrate your own library with Telemetry, build a reporter for Telemetry.Metrics , or give us your feedback on APIs and documentation. Currently, we’re improving the performance of the core, as well as extending and polishing the Poller and the Metrics. Weare also working on making the existing reporters more performant and stable. The next big thing we have planned is a metrics dashboard for Phoenix – imagine generating a new Phoenix project, and having a basic dashboard with metrics served by your endpoint, without setting up any external systems. Telemetry allows us to do this and much more in the near future! Monitor your Erlang, Elixir and RabbitMQ systems Are you currently using Erlang, Elixir or RabbitMQ in your stack? Get full visibility of your system with WombatOAM, find out more and get a free trial on our WombatOAM page. Links – beam-telemetry GitHub organization – Telemetry core on GitHub and HexDocs – Telemetry.Poller on GitHub and HexDocs – Telemetry.Metrics on GitHub and HexDocs Learn more about our expert consultants Code & Architecture Reviews Want actionable advice on how to improve, optimise and future proof your system? Our experts will conduct and in-depth review providing a comprehensive report filled with actionable improvements. Learn more today. Learn more Consulting Let our experts guide your team to create a system that meets your commercial needs including higher uptime, less physical infrastructure requirements, less vulnerabilities and a system that is quicker to update. Learn more Development Need an extra set of hands? Our experts can be embedded to your team to ensure no deadlines or details are missed. Learn more", "date": "2019-04-30"},
{"website": "Erlang-Solutions", "title": "Using CircleCI for Continuous Integration of Elixir projects.", "author": ["Attila Nohl"], "link": "https://www.erlang-solutions.com/blog/using-circleci-for-continuous-integration-of-elixir-projects/", "abstract": "Continuous integration is vital to ensure the quality of any software you ship. Circle CI is a great tool that provides effective continuous integration. In this blog, we will guide you through how to set up CircleCI for an Elixir project. We’ve set up the project on GitHub so you can follow along. In the project we will: Build the project Run tests and check code coverage Generate API documentation Check formatting Run Dialyzer check. You can follow the actual example here . Some inspiration for this blog post comes from this blog and this post . Prerequisites: For this project you will need an account on CircleCI and GitHub. You also need to connect the two accounts. Lastly, you will need Erlang/OTP and Elixir installed. Create an Elixir project To begin with, we need a project, for this demonstration we wanted to use the most trivial possible example, so we generated a classic ‘Hello World’ program. To create the project, simply type the following into the shell: mix new hello_world We also added a printout, because the generated constant-returning function can be optimised away and that will confuse the code coverage checking. Add code coverage metric Code coverage allows us to identify how much of the code is being executed during the testing. Once we know that, we can also understand what lines are not executed because these lines could be where bugs can hide undetected or that we have forgotten to write tests for. If those lines of code are unreachable or not used, they should obviously be removed. To get this metric, I add the excoveralls package to our project (see here for details). Beware that even if you have 100% code coverage, it does not mean the code is bug-free. Here’s an example (inspired by this): defmodule Fact do\n\n  def fact(1), do: 1\n  def fact(n), do: n*fact(n-1)\nend If the test executes Fact.fact(10) , we get 100% test coverage, but the code is still faulty, it won’t terminate if the input is e.g. -1 . For a new project, it should be easy to keep the code coverage near 100%, especially if the developers follow test-driven principles. However, for a “legacy” or already established project without adequate test coverage, reaching 100% code coverage might be unreasonably expensive. In this case, we should still strive to increase (or at least not decrease) the code coverage. To run the tests with code coverage, execute: mix coveralls.html In the project root directory ( hello_world ). Apart from the printout, a HTML report will be generated in the cover directory too. Add Dialyzer checking Dialyzer is a static analyzer tool for Erlang and Elixir code. It can detect type errors (e.g. when a function expects a list to be an argument, but is called with a tuple), unreachable code and other kinds of bugs. Although Elixir is a dynamically typed language, it is possible to add type specifications (specs) for function arguments, return values, structs, etc. The Elixir compiler does not check for this, but the generated API documentation uses the information and it is extremely useful for users of your code. As a post-compilation step , you can run Dialyzer to check the type specifications in addition to writing unit-tests, you can regard this type checking as an early warning to detect incorrect input before you start debugging a unit-test or API client. To enable a Dialyzer check for this code, I’ll use the Dialyzer mix task (see this commit for more details). Dialyzer needs PLT (persistent lookup table) files to speed up its analysis. Generating this file for the Elixir and Erlang standard libraries takes time, even on fast hardware, so it is vital to reuse the PLT files between CI jobs. To run the dialyzer check, execute ‘ mix dialyzer ’ in the project root directory ( hello_world ). The output will be printed on the console. The first run (which generates the PLT file for the system and dependencies) might take a long time! Add documentation generation Elixir provides support for written documentation . By default this documentation will be included in the generated beam files and accessible from the interactive Elixir shell (iex). However, if the project provides an API for other projects, it is vital to generate more accessible documentation. We’re going to use the mix docs task to generate HTML documentation (see this commit for more details). To generate documentation, execute: ‘mix docs’ In the project root directory ( hello_world ). The documentation is generated (by default) in the docs directory. Ensure code formatting guidelines Using the same code style consistently throughout a project helps readers understand the code and could prevent some bugs from being introduced. The mix tool in Elixir provides a task to check that the code is in compliance with the specified guidelines. The command to do this is: mix format --check-formatted In the project root directory ( hello_world ). Push the project to GitHub Now that the project is created, it needs to be published to allow CircleCI to access it. Create a new repository on GitHub by clicking New on the Repository page, then follow the instructions. When the new repository is created, initialize the git repository and push it to GitHub: cd hello_world\ngit init\ngit add config/ .formatter.exs .gitignore lib/ mix.exs README.md test/\ngit commit -m “Initial version” -a\ngit remote add origin <repo-you-created>\ngit push -u origin master Integrate the GitHub repository to CircleCI Login to circleci.com Click on “Add projects” Click on “Set Up Project” for hello_world For now skip the instructions to create the .circleci/config.yml file (we’ll get back to this file later), just click on “Start Building” The build will fail, because we didn’t add the configuration file, that’s the next step. Configuring the CircleCI workflow As our requirement states above, we’ll need 5 jobs. These are: Build: Compile the project and create the PLT file for Dialyzer analysis. Test: Run the tests and compute code coverage. Generate documentation: Generate the HTML and associated files that document the project. Check code format: The mix tool can be used to ensure that the project follows the specified code formatting guidelines. Execute Dialyzer check: run the Dialyzer mix task using the previously generated PLT file The syntax of the CircleCI configuration file is described here. The next section describes the configuration required to setup the above five jobs, so create a .circleci/config.yml file and add the following to it: Common preamble version: 2.1\njobs: The above specifies the current CircleCI version. The jobs (described in the next sections) should be listed in the configuration file. The build step The configuration for the build step: build:\n    docker:\n    - image: circleci/elixir:1.8.2\n        environment:\n        MIX_ENV: test\n\n    steps:\n    - checkout\n\n    - run: mix local.hex --force\n    - run: mix local.rebar --force\n\n    - restore_cache:\n        key: deps-cache-{{ checksum \"mix.lock\" }}\n    - run: mix do deps.get, deps.compile\n    - save_cache:\n        key: deps-cache-{{ checksum \"mix.lock\" }}\n        paths:\n            - deps\n            - ~/.mix\n            - _build\n\n    - run: mix compile\n\n    - run: echo \"$OTP_VERSION $ELIXIR_VERSION\" > .version_file\n    - restore_cache:\n        keys:\n            - plt-cache-{{ checksum \".version_file\" }}-{{ checksum \"mix.lock\" }}\n    - run: mix dialyzer --plt\n    - save_cache:\n        key: plt-cache-{{ checksum \".version_file\"  }}-{{ checksum \"mix.lock\" }}\n        paths:\n            - _build\n            - deps\n            - ~/.mix In this example, I’m using 1.8.2 Elixir. The list of available images can be found here. The MIX_ENV variable is set to test, so the test code is also built and more importantly, the PLT file for Dialyzer will be built for test-only dependencies too. Further build steps: The actual build process checks the code, fetches and installs hex and rebar locally. Restore the dependencies from cache. The cache key depends on the checksum of the mix.lock file which contains the exact versions of the dependencies, so this key changes only when actual dependencies are changed. The dependencies and built files are saved to the cache, to be reused in the test and documentation generating steps, and later when CI runs. Build the actual project. Unfortunately, this result cannot be reused, because the checkout steps produce source files with current timestamp, so in later steps, the source files will have newer timestamps than the beam files generated in this step, this will lead to mix compiling the project anyway. The dialyzer PLT file depends on the Erlang/OTP and Elixir versions. Even though I’ve fixed the Elixir version, it is possible that the Erlang/OTP in the Docker image is updated and in that case, the PLT file would be out of date. As the CircleCI caches are immutable, there’s no way to update the PLT file, for these cases, you’ll need a new cache name for new cache contents. Unfortunately, not all environment variable names can be used in CircleCI cache names, so I needed to use a workaround here: create a temporary .version_file which contains the Erlang/OTP and Elixir versions and use its checksum in the cache name along with the checksum of the mix.lock file (which contains the exact versions of all dependencies). So as long as we have the exact same dependencies and versions, we can reuse the PLT file safely, but as soon as anything changes, we get to use a new PLT file. The test step The configuration for the test step: test:\n    docker:\n    - image: circleci/elixir:1.8.2\n\n    steps:\n    - checkout\n    - restore_cache:\n        key: deps-cache-{{ checksum \"mix.lock\" }}\n    - run: mix coveralls.html\n\n    - store_artifacts:\n        path: cover\n        destination: coverage_results Obviously, you need to use the same docker image as you use in the build step. There’s no need to explicitly configure the MIX_ENV environment variable because the mix job will set it. The test is fairly straightforward: checkout the code from the repository, fetch the dependencies from the cache, then run the coverage check job. The coverage report is generated in the cover directory. It is stored as an artifact, so you can check the results via a browser on the CircleCI page. If the tests itself fail, the output of the step will show the error message, and the job itself will fail. The documentation generation step The configuration for the documentation generation step: generate_documentation:\n    docker:\n    - image: circleci/elixir:1.8.2\n        environment:\n        MIX_ENV: test\n\n    steps:\n    - checkout\n    - restore_cache:\n        key: deps-cache-{{ checksum \"mix.lock\" }}\n    - run: mix docs\n\n    - store_artifacts:\n        path: doc\n        destination: documentation Once again, the same docker from the build step needs to be used and setting up the MIX_ENV variable is important, otherwise, the dependencies might be different from the dev environment to the test environment. The documentation generation is fairly straightforward: checkout the code from the repository, fetch the dependencies from the cache (which contains the documentation generating task), then run the documentation generating the job. The documentation is generated in the doc directory. It is stored as an artifact, so you can check the results via a browser on the CircleCI page. The dialyzer step The configuration for the dialyzer step: dialyzer:\n    docker:\n    - image: circleci/elixir:1.8.2\n        environment:\n        MIX_ENV: test\n\n    steps:\n    - checkout\n    - run: echo \"$OTP_VERSION $ELIXIR_VERSION\" > .version_file\n    - restore_cache:\n        keys:\n            - plt-cache-{{ checksum \".version_file\" }}-{{ checksum \"mix.lock\" }}\n    - run: mix dialyzer --halt-exit-status Much like the last step, the docker image needs to match the one used for the build. Ensure that the MIX_ENV variable is correct. The workaround mentioned above is required to find the cache with the right PLT file, then executing Dialyzer is a simple command. If Dialyzer finds an error, it will return with a non-zero exit code, so the step will fail. The format checking step The configuration for the format checking step: format_check:\n    docker:\n    - image: circleci/elixir:1.8.2\n        environment:\n        MIX_ENV: test\n\n    steps:\n    - checkout\n\n    - run: mix format --check-formatted This is really simple, we don’t even need any cached files, just run the check. Piecing it all together CircleCI executes a workflow which contains the above steps. This is the configuration for the workflow: workflows:\n  version: 2\n  build_and_test:\n    jobs:\n    - build\n    - format_check:\n        requires:\n            - build\n    - generate_documentation:\n        requires:\n            - build\n    - dialyzer:\n        requires:\n            - build\n    - test:\n        requires:\n            - build The workflow specifies that the four later steps depend on the build test, but they can be executed simultaneously. The whole configuration file can be seen on GitHub . When the configuration file is ready, add it to git: git add .circleci/config.yml And push it to GitHub: git push origin master CircleCI will automatically detect that a new version was committed and will execute the configured jobs. Conclusion Continuous integration is essential in a fast moving project. The developers need feedback as soon as possible because the earlier a bug is found, the earlier and cheaper it is to fix it. This blog presents a simple, but effective setup that can run this vital part of an Elixir project. Want more fantastic Elixir inspiration? Don’t miss out on Code Elixir , a day to learn, share, connect with and be inspired by the Elixir community. Want to learn about live tracing in Elixir? Head to our easy to follow guide .", "date": "2019-06-25"},
{"website": "Erlang-Solutions", "title": "Which companies are using Erlang, and why? #MyTopdogStatus", "author": ["Francesco Cesarini"], "link": "https://www.erlang-solutions.com/blog/which-companies-are-using-erlang-and-why-mytopdogstatus/", "abstract": "Once upon a time, Cisco, Ericsson, Klarna, Goldman Sachs, T-Mobile, WhatsApp, Amazon and many other top companies kept a secret. Erlang was that badly kept secret. Many have heard of it, but few realise that it controls vast amounts of infrastructure, including the fixed and mobile networks we use on a daily basis. It was monumental when Cisco revealed that it ships 2 million devices per year running Erlang at the Code BEAM Stockholm conference in 2018. This translates to 90% of all internet traffic going through routers and switches controlled by Erlang. And have you heard about Ericsson ? It has Erlang at the core of its GPRS, 3G, 4G and 5G infrastructure. With a market share of 40%, there’s a high probability a program written in Erlang assigned the IP address your smartphone is using today (amongst other things). Since being released as open source, Erlang has been spreading beyond Telecoms, establishing itself in other verticals such as FinTech, Gaming, Healthcare, Automotive, IoT and Blockchain. How can a technology created to switch phone calls make its way into these verticals? And why should you not only care but consider using it for your server-side development? Erlang and Elixir are the top dogs in any tech stack and it’s time we let more people and companies know about the BEAM technologies. We are kicking off with this first blog being part of the #MyTopdogStatus series, to showcase Erlang’s success stories. We will follow with the Elixir focus next- so you have the bigger picture of both technologies! A Solution to a Problem One of the first things I try to find out when I meet a programming language inventor is what problem were they trying to solve when creating the language. The range of answers is fascinating, and often reveals if it is -or is not- fit for purpose. Ask any of the co-inventors of Erlang and you will get a unanimous response. They were trying to understand how to better build the next generation of telecom systems, the only systems back in the late 90s which had to be scalable, fault-tolerant, predictable and maintainable. They never set off with the intention of inventing a programming language, but the solution to their problem happened to be just that. Programming languages successfully created to solve a problem are the ones to keep an eye out for, as they are, without a doubt, the right tool for the job. If the answer to why a language was invented is experimenting with esoteric computer science concepts, listen, learn, but beware. At the time of Erlang’s conception, telecom systems were the only systems which had to be scalable, handle peak loads predictably and never fail. The internet changed it all. When Erlang was released as open source, telecom grade resilience and scale gave way to web-scale, which in turn gave way to mobile applications and connected devices, which, through the inception of 4G and 5G gave way to IoT. How was this transition possible, and why was it so seamless? If you are switching phone calls, SMSs, publish/subscribe, instant and mobile messages, credit card transactions, money, stocks, medical records, online gaming command sequences, blockchain propagation or telemetry data, the business logic is still the same. And whilst it might be ok to lose the odd SMS, or message, or for a device to be offline for hours and not transmit any data, it is absolutely not cool to lose a financial transaction which involves money, stocks or cryptocurrencies. Basically, what changes in all these systems are the tradeoffs in scalability, reliability and consistency of the data, all items which are programming language agnostic and taken care of in the business logic of the system. Everything else remains the same. There is no better example of Erlang’s reliability than the English National Health Service (NHS) . The NHS obviously requires high availability and handles over 65 million record requests every day through its Spine, a centralised point allowing the exchange of information across national and local systems. Using Riak (written in Erlang), the NHS has managed 99.999% availability for over five years. From health to the advertising industry – AdRoll receives an average of 500,000 real-time bid requests per second (spikes are many times that), with a substantial financial value attached to each. They use Erlang/OTP to manage the real-time bidding platform on the Amazon EC2 platform, pairing up advertisers with users in milliseconds. Many countries rely on Erlang for their immediate payment switches, allowing instant bank transfers and bill payment services. Vocalink, a Mastercard company , is shipping financial switches implemented in Erlang. And they are far from being alone. Other areas where Erlang is used in anger includes private and public blockchain , payment and credit card gateways, banking APIs and more traditional infrastructure management. If you scratch under the surface, you will find an Erlang team in all of the major banks and financial institutions. And those few not using it will be using RabbitMQ, CouchDB, RiakKV or one of the other open source Erlang applications. The investment banking giant, Goldman Sachs , use Erlang in part of its hedge fund trading platform. The Goldman Sachs platform is a very low latency (microseconds) event-driven market data processing, strategy, and order submission engine. Erlang is used to help them deliver real-time changes in response to market conditions. What about massively multi-user online gaming? Nintendo’s Switch has sold over 34 million consoles; the system uses an Erlang based messaging system to handle millions of concurrent connections. Other popular gaming companies such as Riot Games, use Erlang too. League of Legends had up to 7.5 million concurrent players at a time, with an Erlang messaging system allowing them to all chat simultaneously, without interruptions. Even one of the world’s most used online dating apps used Erlang. Grindr used Erlang in its stack to manage it’s 3.2 million daily active users to handle up to 2000 messages per second. Moving to the BEAM helped them maintain a reliable system with a minimal number of outages. What’s in a Programming Language? A programming language, no matter how good it is, is on its own only of limited use. Combine it with a powerful optimised runtime and middleware which abstracts the scalability and reliability of your system, your developers get the foundation of a very powerful ecosystem. The Erlang ecosystem is not just a programming language. It is a family of programming languages, a virtual machine as powerful as an operating system and a set of middleware libraries which abstracts many of the recurring (and tricky) problems you have to deal with when working with scale and resilience. The Language Itself Let’s focus on the language itself. When we talk about the Erlang programming language, we mean the language semantics. Language semantics includes aspects taken from functional, logic and concurrent programming, providing a higher level of abstraction resulting in less code which is more expressive and easier to maintain. – ultimately reduces cost for a business. The language semantics puts the concurrency model in its core, making processes the main building block. Processes provide a natural way to reason around the problem, facilitating the implementation of systems where a lot of things are happening at the same time. This is best described by Joe Armstrong’s tenets: The world is concurrent. Things in the world don’t share data. Things communicate with messages. Things fail. Picture this model applied to us, humans. Humans are concurrent entities who do not share brains. They speak to each other through asynchronous messages, sometimes on top of each other. Humans receiving the message process them and store a copy of whatever they believe is relevant to them. And, sometimes, humans fail, but the ones around them continue with their assigned tasks whilst new humans are created, or the humans that failed are repaired. Now model this world in a programming language. It is as simple as that! BT Mobile (formerly T-Mobile) in the UK uses this approach for many of their core systems. One of them is the Third-Party Gateway, a system written to handle all machine to human SMSs. This includes alert services such as traffic, financial and weather alerts, TV voting, competitions and reminders. Millions of messages are sent and received each day, with many spikes in traffic during TV votes and special promotions. Each SMS being sent or received is a process. The developer needs to reason around sending and receiving a single SMS, with scale handled through the creation of new processes. Online betting is another industry with extreme spikes and loads to manage, a busy day for betting agencies is like Black Friday at Amazon, but the prices change every second. Many online betting companies, including bet365 , use Erlang to manage 100’s of thousands of concurrent users. Sometimes, I hear complaints that the Erlang way of thinking requires a serious mind shift. The problem is not the mind-shift; it is unlearning the unnatural models other programming languages have taught us and going back to basics using Joe’s tenents. If you understood them, understanding Erlang will be just as easy. The OTP Middleware OTP is a set of frameworks, principles, and patterns that guide and support the structure, design, implementation, and deployment of Erlang systems. The innovation in OTP which other languages are copying are the abstract principles used to describe the software architecture. Processes are given a design pattern such as servers, finite state machines, event handlers or supervisors, all packaged in reusable libraries. These libraries have built-in support for debugging, software upgrade and generic error handling. They also abstract and take care of all tricky edge cases which occur with concurrent programming, providing a solid and tried approach to problem-solving. It makes the code easier to understand and maintain, reduces maintenance costs and stops developers from reinventing a square wheel. Motorola funded a study which involved rewriting a telecommunication system used by the emergency services from C++ to Erlang, focusing on productivity gains. Depending on how you calculated, the code reduction in the Erlang system achieved a result of 4-20 times less code. The 20 times reduction assumed the OTP libraries to be part of the Erlang standard libraries, which they are. As C++ did not have a generic OTP, the original project had to implement a good part of it. That seems to be the norm with any project dealing with concurrency at scale. The norm also being that this implementation is often bug-ridden. Development on the first OTP release was started alongside some of Ericsson’s major projects, including their broadband solutions and foray into packet-based switching. One of the first projects to use OTP was the AXD301 switch, an ATM switch which allowed generic networks to also be used for telephony. Telecom providers around the world, including BT’s 21st-century network, stopped routing calls through a dedicated network and instead, started routing them in a common backbone. Any long-distance calls you made in the UK were routed using Erlang through an AXD301 switch. The productivity gains of 4 – 20 times less code are best visible with WhatsApp . When they got acquired by Facebook in 2014, they had 450 million active users, 70% of which were active on a daily basis. Their traffic record at the time was December 31st, 2013. In 24 hours, they sent 54 billion messages, twice the number of estimated SMSs sent that day. The engineering team consisted of 32 people, of which only ten worked on the server-side. The server-side team developed new features, maintained existing ones and supported the whole system. Basically, they were also the ones who got woken up in the middle of the night if something went wrong. What the WhatsApp acquisition highlighted was how Erlang enhances programmer productivity and scalability on a very modest hardware footprint, and the low maintenance costs these systems entail. A small team was able to develop, support, maintain and scale a system with hundreds of millions of users, which has today scaled to billions. Using OTP helps the developer avoid accidental complexities: things that are difficult because they picked inadequate tools. This, we know, always enhances productivity and reduces maintenance costs. But other problems remain difficult, irrespective of the programming tools and middleware. Those are the difficulties one should focus on. The Virtual Machine Erlang uses a Virtual Machine (VM) when executing code; an approach similar to NodeJS, Java and Ruby. Programs are compiled to low-level instructions called Byte Code. The BEAM is the most commonly used Erlang VM which executes this byte code. It can be seen as an operating system running in a container, a virtual instance, on another operating system or directly on the bare metal. The BEAM has, for the last two decades, been optimised and under heavy load rendered predictable for the types of problems Erlang is good at solving. It is capable of concurrently handling millions of processes, ensuring each process displays soft real-time properties and is fairly treated. Throughput remains constant irrespective of load. If your system handles 100,000 requests per second, it will take a second per request if 100,000 are being served simultaneously. If the number of requests increases to 200,000, throughput will remain the same, but latency will increase to 2 seconds. What is important is that all requests are served with no degradation of throughput, even when spikes happen. With many other Virtual Machines, an increase in requests often leads to a degradation of service, possibly grinding to a halt. Not with the BEAM, as it was built for scale. As processes do not share memory, memory management is done on a per-process basis. Schedulers allow a process to execute a predefined number of instructions before suspending it, ensuring fair execution. If the process hadn’t completed its tasks, it will be scheduled to continue executing after all other processes have had their go. This will result in a constant execution time for all requests, unlike the Java Virtual Machine, where garbage collecting affects all requests going through the system at that particular point in time or NodeJS, where a request will execute until completion, blocking other requests on that thread until it is done. The RubyVM has not had decades of engineering resources put into it, and as a result, will not scale. There are no other Virtual Machines around with the properties of the BEAM, making it unique in its capability to predictably handle massively concurrent spikes of traffic. It is not the fastest VM, but it is the most stable and predictable one. How do the advantages of using the BEAM apply in practice? Imagine a VM handling a million TCP/IP connections, each managed by a process serving a customer, where load share and CPU allocation for each process is fair and constant. This equates to a predictable user-experience, where a million users can be served on a single instance. All the programmer has to worry about is developing code to handle a single user (a process), and the VM will automatically scale it to millions. And if a request fails because of corrupt data or a bug in the software, all other requests will continue executing independently of it, isolating the failure. WhatsApp achieved 2 million TCP/IP connections (each managed by a process) on a single BEAM instance back in 2012, using the connections to serve messages and notifications whilst minimising its hardware footprint. This, in turn, greatly reduced their infrastructure costs and the size of the team needed to maintain it. Similar numbers were achieved using Elixir and the Phoenix framework in 2015. From messaging, vertical scalability on a single node we move to the web. Bleacher Report , a news app and website focusing on sports, had a similar impact on operational and infrastructure costs when changing Virtual Machine. When they migrated from Ruby to the BEAM, they were able to reduce their hardware requirements from 150 servers to 5! And Quicken Loans launched a new mortgage offering with a Super Bowl ad , knowing with confidence that their servers would handle the traffic with ease. In fact, they had trouble load testing it with traditional tools, in the end, they needed to fall back to using Erlang to load test Erlang. You can copy OTP and its libraries, but if it does not run on the BEAM, you can not emulate the semantics. This is why using OTP ported to .net or the JVM will not work. On the other hand, many languages are being ported to the BEAM VM, making full use of OTP, Elixir being the most popular. But let’s not forget Luerl, Lisp Flavoured Erlang, Efene and at least a dozen more. Is it still a badly kept secret? In this blog post, we are just scratching the surface. Many companies are using Erlang to power their server-side infrastructure. They are doing it to reduce development costs whilst ensuring their systems scale and are resilient. So resilient that their end-users, often focusing on the glitzy app or website, do not even know it is there. Isn’t it time you looked into Erlang as well? Find out more about Erlang.", "date": "2019-09-11"},
{"website": "Erlang-Solutions", "title": "Which companies are using Elixir, and why? #MyTopdogStatus", "author": ["Manuel Rubio"], "link": "https://www.erlang-solutions.com/blog/which-companies-are-using-elixir-and-why-mytopdogstatus/", "abstract": "How do you choose the right programming language for a project? As a developer or a key-decision maker you could decide based on your personal preference, or what is fashionable at the time, but if you’re not considering real experiences and use cases, you may end up making a decision that becomes difficult to execute, expensive to complete and a nightmare to maintain. As Francesco mentioned in the Erlang blog of our #MyTopdogStatus series, you could start by asking why a language was created. This will help ensure that you’re choosing a language designed to fix a problem suited to your needs. On a number of occasions, José Valim has stated he wanted to give the power and productivity of Erlang to more developers. Another great tip for choosing a language is to look under the hood and see what powers it. For that, the previously mentioned Erlang blog gives you a great insight into the OTP and Virtual Machine (VM) that helps make Elixir the powerful language that it is. Lastly, you can look at case studies to see how other companies have used the language, why they’ve chosen it, and what they were able to achieve. In practice, it’s not easy to find this information, as we still don’t know exactly how many companies use Elixir or their exact use case. Luckily, we can refer to www.elixir-companies.com or openly ask the community. You could even subscribe to the newsletter of Plataformatec who funded the initial creation of Elixir and always share fantastic stories about the language in use. And we hope this blog post will be helpful too! Simply put, here is a checklist of common benefits that a company gets from using Elixir: Need to grow or scale – If you have a service that you expect to be used by millions, then you’ll benefit from the reliability and scalability of the BEAM VM. Handling a lot of incoming requests – Even if you don’t have millions of users, if all of your users make intense use of the system and generate a high volume of requests, you will need a system built for concurrency. Easy to develop and maintainable – One of the priorities of Erlang and Elixir is keeping the code simple. As a result, one of the advantages most companies will see is ease of usage and fast development for everything from fixing bugs to adding new features. Now, without further ado, let’s look at who’s using Elixir and how. Event handling by Pinterest Have you heard that Pinterest are using Elixir? Well, now you have. They use it to handle the routing of the events generated in their system. Think about all of the actions taking place on Pinterest in a day. How many do you think there are? The answer is around 30 thousand events per second that need to be processed. In addition, they have over 200 million active users. Keep in mind, 20 years ago, it would have been impressive to achieve 10 thousand concurrent connections. Luckily, the way the BEAM works lets us handle and deal with huge spikes in concurrent connections in a way that would be very difficult to replicate in other technologies. Elixir is a good example of a language which takes advantage of the BEAM VM, which is why it was a great choice for Pinterest. Not only were they able to handle the demands of their users, but they were also able to significantly reduce the lines of code needed by simplifying it and clustering it. The end result being, they were able to half the number of servers needed, down to just 15, allowing their solution to scale up with simple, maintainable code and lower operating costs. And, it’s better for the environment too. Faster distributed data by Moz Pro Companies often look for alternatives to SQL for data storage. In the case of Moz Pro, one of the leading SEO analytics companies in the world, they decided to replace their traditional MySQL databases with a distributed indexing data system built in Elixir. In doing this, they managed to get responses of 50ms, significantly quicker than the previous +800ms. Elixir’s ability to be distributed in different nodes while keeping transparent communication between the connected nodes makes it easy to create this kind of solution, improving service speed, no matter how much data you need to store. The BEAM can handle millions of processes per node. That helps to create cache systems, as well get rid of other external solutions like Redis or Memcached. In addition, the BEAM provides elements like DETS, ETS and Mnesia which help to create distributed data systems with ease. Proxying requests by Lonely Planet This example shows the ability of an Elixir and the Phoenix Framework to create a project that can handle massive spikes and loads from their millions of users while managing the flow of requests to external sites like booking.com and HostelWorld. Elixir is a great solution for creating systems to manage a high volume of requests to external resources. It allows you to create a pool of workers to access external APIs and keep back-pressure systems to avoid overwhelming your partner websites, which is an essential part of a successful system of this nature. In addition, the Phoenix Framework is a very complete system to build extremely complex websites. It makes extensibility, documentation and avoiding boiler-plate issues easy by minimising the lines of code needed. This allows you to focus on the implementation of integrations with third-party platforms. GraphQL by Financial Times The Financial Times made fantastic use of the Absinthe framework in Elixir, and Elixir’s meta-programming ability to create DSLs (Domain Specific Languages). Ellis Pritchard, the Senior Software Developer at the time said: “I found developers picked up Elixir basics very rapidly if there is an existing framework of code to work within at first since there are patterns they can spot and replicate. Good developers love learning new things, and are adept at switching between languages and frameworks; plus the learning curve for Elixir, certainly within an existing code base, seems low”. They use Elixir for microservices in their system. You can check their GitHub account for more details and take advantage of the elements they share which are battle-tested in production. Mobility Services by Toyota Connected Toyota Connected launched its Mobility Service Platform (MSPF) which connects their cars, allowing them to send real-time events. Functional uses for this include sharing information on traffic patterns or driver behaviours. It is a platform that is expected to have millions of connected vehicles sending events to the cloud regularly. This kind of concurrent traffic makes Elixir an obvious choice for the project. In addition, Toyota Connected features an API that third-parties can use to create apps and website integrations. This allows third parties to take advantage of the data and create a better user experience for their drivers. A great example of this is the Hui service in Hawaii, which uses the Toyota Connected system to run a keyless car-share service. Custom Sports News by Bleacher Report Bleacher Report is a sports news service that serves 1.5 billion pages per month to its users. They offer a customised service to ensure push notifications are relevant to each individual. That in itself is difficult because it means you cannot cache the content between users. Much like Lonely Planet, Bleacher Report also has to consider how they send requests to third-parties (mainly Google and Apple) to reach their users without limiting the throughput or overwhelming their providers. When transitioning from Ruby on Rails to Elixir they were able to shift from using 150 servers, down to 5 servers, all while increasing response time for their 200m+ daily push notifications. Microservices for Games by SquareEnix The increase in massive-multiplayer online gaming has seen a huge shift in the way games are developed. Now, digital environments must consider the way they can handle millions of concurrent players at once. To handle this, SquareEnix is using Elixir for authenticating players, in-game communication and the CMS (Content Management System). These are services which are common to several titles because they rely on the same company and are usually a critical point of convergence for the traffic of the system. Elixir helps provide a healthy throughput, dealing with all of the input requests and releasing the session, authentication and profile data from other services without collapsing the system. Chat for gamers by Discord Another huge use for Elixir in gaming is the chat systems. Discord, a leading provider of chat for gaming have built their systems in Elixir. Their app can allow huge groups (up-to 300 thousand in theory) to join the same voice call. You can read about how they upgraded to be able to handle 11 million concurrent users here . ECommerce by PepsiCo The ECommerce team at PepsiCo told ElixirConf US 2019 “We view Elixir – which is a core part of our software stack – as a nimble and reliable building block for powering critical business solutions.” It is fantastic to see companies of PepsiCo’s size proudly speaking about the benefits of Elixir. We hope to add many more companies of their size to this list over the coming years. Is it time for you to join the pack? Leading companies in a variety of industries are already taking advantage of Elixir. They’re doing it to reduce the cost of their physical hardware, improve the reliability of their site when dealing with high volumes of concurrent users, to scale up data while reducing the time to access it, to allow for fault-tolerant third-party integration, even when dealing with high volumes of traffic and to create reliable micro-services, even when dealing with extremely high numbers of request. Even if the above requirements aren’t needed for your current project, you can still benefit from the future-proofing, ease of use, available documentation and tooling that Elixir offers. Our consultancy work with the elixir programming language.", "date": "2019-09-19"},
{"website": "Erlang-Solutions", "title": "Web scraping with Elixir", "author": ["Oleg Tarasenko"], "link": "https://www.erlang-solutions.com/blog/web-scraping-with-elixir/", "abstract": "Introduction Businesses are investing in data. The big data analytics market is expected to grow to $103 billion (USD) within the next five years. It’s easy to see why, with everyone of us on average generating 1.7 megabytes of data per second. As the amount of data we create grows, so too does our ability to inherent, interpret and understand it. Taking enormous datasets and generating very specific findings are leading to fantastic progress in all areas of human knowledge, including science, marketing and machine learning. To do this correctly, we need to ask an important question, how do we prepare datasets that meet the needs of our specific research. When dealing with publicly available data on the web, the answer is web scraping. What is web scraping? Wikipedia defines web scraping as: Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis. (See: https://en.wikipedia.org/wiki/Web_scraping ). Web scraping with Elixir Web scraping can be done differently in different languages, for the purposes of this article we’re going to show you how to do it in Elixir, for obvious reasons ;). Specifically, we’re going to show you how to complete web scraping with an Elixir framework called Crawly. Crawly is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival. You can find out more about it on the documentation page. Getting started For the purposes of this demonstration, we will build a web scraper to extract the data from Erlang Solutions Blog. We will extract all the blog posts, so they can be analyzed later (let’s assume we’re planning to build some machine learning to identify the most useful articles and authors). First of all, we will create a new Elixir project: mix new esl_blog --sup Now that the project is created, modify the deps` function of the mix.exs file, so it looks like this: #Run \"mix help deps\" to learn about dependencies.\n\n    defp deps do\n    [\n      {:crawly, \"~> 0.1\"},\n    ]\n  end Fetch the dependencies with: mix deps.get , and we’re ready to go! So let’s define our crawling rules with the help of… spiders. The spider. Spiders are behaviours which you create an implementation, and which Crawly uses to extract information from a given website. The spider must implement the spider behaviour (it’s required to implement the parse_item/1 , init/0`, `base_url/0 callbacks). This is the code for our first spider. Save it in to file called esl.ex under the lib/esl_blog/spiders directory of your project. defmodule Esl do\n  @behaviour Crawly.Spider\n\n  @impl Crawly.Spider\n  def base_url() do\n    \"https://www.erlang-solutions.com\"\n  end\n\n  @impl Crawly.Spider\n  def init() do\n    [\n      start_urls: [\"https://www.erlang-solutions.com/blog.html\"]\n    ]\n  end\n\n  @impl Crawly.Spider\n  def parse_item(_response) do\n    %Crawly.ParsedItem{:items => [], :requests => []}\n  end\nend Here’s a more detailed run down of the above code: base_url(): function which returns base_urls for the given Spider, used in order to filter out all irrelevant requests. In our case, we don’t want our crawler to follow links going to social media sites and other partner sites (which are not related to the crawled target). init(): must return a KW list which contains start_urls list which Crawler will begin to crawl from. Subsequent requests will be generated from these initial urls. parse_item(): function which will be called to handle response downloaded by Crawly. It must return the Crawly.ParsedItem structure. Crawling the website Now it’s time to run our first spider: Start the interactive elixir console on the current project, and run the following command: Crawly.Engine.start_spider(Esl) You will get the following (or similar) result: 22:37:15.064 [info] Starting the manager for Elixir.Esl 22:37:15.073 [debug] Starting requests storage worker for Elixir.Esl… 22:37:15.454 [debug] Started 4 workers for Elixir.Esl :ok iex(2)> 22:38:15.455 [info] Current crawl speed is: 0 items/min 22:38:15.455 [info] Stopping Esl, itemcount timeout achieved So what just happened under the hood? Crawly scheduled the Requests object returned by the init/0 function of the Spider. Upon receiving a response, Crawly used a callback function (parse_item/1) in order to process the given response. In our case, we have not defined any data to be returned by the parse_item/1 callback, so the Crawly worker processes (responsible for downloading requests) had nothing to do, and as a result, the spider is being closed after the timeout is reached. Now it’s time to harvest the real data! Extracting the data Now it’s time to implement the real data extraction part. Our parse_item/1 callback is expected to return Requests and Items. Let’s start with the Requests extraction: Open the Elixir console and execute the following: `{:ok, response} = Crawly.fetch(\"https://www.erlang-solutions.com/blog.html\")` you will see something like:\n\n{:ok,\n %HTTPoison.Response{\n   body: \"<!DOCTYPE html>\\n\\n<!--[if IE 9 ]>\" <> ...,\n   headers: [\n     {\"Date\", \"Sat, 08 Jun 2019 20:42:05 GMT\"},\n     {\"Content-Type\", \"text/html\"},\n     {\"Content-Length\", \"288809\"},\n     {\"Last-Modified\", \"Fri, 07 Jun 2019 09:24:07 GMT\"},\n     {\"Accept-Ranges\", \"bytes\"}\n   ],\n   request: %HTTPoison.Request{\n     body: \"\",\n     headers: [],\n     method: :get,\n     options: [],\n     params: %{},\n     url: \"https://www.erlang-solutions.com/blog.html\"\n   },\n   request_url: \"https://www.erlang-solutions.com/blog.html\",\n   status_code: 200\n }} This is our request, as it’s fetched from the web. Now, let’s use Floki in order to extract the data from the request. First of all, we’re interested in “Read more” links, as they would allow us to navigate to the actual blog pages. In order to understand the ‘Now’, let’s use Floki in order to extract URLs from the given page. In our case, we will extract Read more links. Using the firebug or any other web developer extension, find the relevant CSS selectors. In my case I have ended up with the following entries: iex(6)> response.body |> Floki.find(“a.more”) |> Floki.attribute(“href”) [“/blog/a-guide-to-tracing-in-elixir.html”, “/blog/blockchain-no-brainer-ownership-in-the-digital-era.html”, “/blog/introducing-telemetry.html”, “/blog/remembering-joe-a-quarter-of-a-century-of-inspiration-and-friendship.html”, Next, we need to convert the links into requests. Crawly requests is a way to provide some sort of flexibility to a spider. For example, sometimes you have to modify HTTP parameters of the request before sending them to the target website. Crawly is fully asynchronous. Once the requests are scheduled, they are picked up by separate workers and are executed in parallel. This also means that other requests can keep going even if some request fails or an error happens while handling it. In our case, we don’t want to tweak HTTP headers, but it is possible to use the shortcut function: Crawly.Utils.request_from_url/1 Crawly expects absolute URLs in requests. It’s a responsibility of a spider to prepare correct URLs. Now we know how to extract links to actual blog posts, let’s also extract the data from the blog pages. Let’s fetch one of the pages, using the same command as before (but using the URL of the blog post): {:ok, response} = Crawly.fetch(“ https://www.erlang-solutions.com/blog/a-guide-to-tracing-in-elixir.html” ) Now, let’s use Floki in order to extract title, text, author, and URL from the blog post: Extract title with: Floki.find(response.body, \"h1:first-child\") |> Floki.text\n\nExtract the author with:\n  author =\n      response.body\n      |> Floki.find(\"article.blog_post p.subheading\")\n      |> Floki.text(deep: false, sep: \"\")\n      |> String.trim_leading()\n      |> String.trim_trailing() Extract the text with: Floki.find(response.body, \"article.blog_post\") |> Floki.text Finally, the url does not need to be extracted, as it’s already a part of response! Now it’s time to wire everything together. Let’s update our spider code with all snippets from above: defmodule Esl do\n  @behaviour Crawly.Spider\n\n  @impl Crawly.Spider\n  def base_url() do\n    \"https://www.erlang-solutions.com\"\n  end\n\n  @impl Crawly.Spider\n  def init() do\n    [\n      start_urls: [\"https://www.erlang-solutions.com/blog.html\"]\n    ]\n  end\n\n  @impl Crawly.Spider\n  def parse_item(response) do\n    # Getting new urls to follow\n    urls =\n      response.body\n      |> Floki.find(\"a.more\")\n      |> Floki.attribute(\"href\")\n\n    # Convert URLs into requests\n    requests =\n      Enum.map(urls, fn url ->\n        url\n        |> build_absolute_url(response.request_url)\n        |> Crawly.Utils.request_from_url()\n      end)\n\n    # Extract item from a page, e.g.\n    # https://www.erlang-solutions.com/blog/introducing-telemetry.html\n    title =\n      response.body\n      |> Floki.find(\"article.blog_post h1:first-child\")\n      |> Floki.text()\n\n    author =\n      response.body\n      |> Floki.find(\"article.blog_post p.subheading\")\n      |> Floki.text(deep: false, sep: \"\")\n      |> String.trim_leading()\n      |> String.trim_trailing()\n\n    text = Floki.find(response.body, \"article.blog_post\") |> Floki.text()\n\n    %Crawly.ParsedItem{\n      :requests => requests,\n      :items => [\n        %{title: title, author: author, text: text, url: response.request_url}\n      ]\n    }\n  end\n\n  def build_absolute_url(url, request_url) do\n    URI.merge(request_url, url) |> to_string()\n  end\nend Running the spider again If you run the spider, it will output the extracted data with the log: 22:47:06.744 [debug] Scraped \"%{author: \\\"by Lukas Larsson\\\", text: \\\"Erlang 19.0 Garbage Collector2016-04-07\" <> … Which indicates that the data has successfully been extracted from all blog pages. What else? You’ve seen how to extract and store items from a website using Crawly, but this is just the basic example. Crawly provides a lot of powerful features for making scraping easy and efficient, such as: Flexible request spoofing (for example user-agent rotation) Item validation, using a pipeline approach Filtering already seen requests and items Filtering out all requests which are coming from other domains Robots.txt enforcement Concurrency control HTTP API for controlling crawlers Interactive console, which allows to create and debug spiders Our consultancy in the elixir programming language .", "date": "2019-09-20"},
{"website": "Erlang-Solutions", "title": "How to build a machine learning project in Elixir", "author": ["Grigory Starinkin and Oleg Tarasenko"], "link": "https://www.erlang-solutions.com/blog/how-to-build-a-machine-learning-project-in-elixir/", "abstract": "Introduction to machine learning Machine learning is an ever-growing area of interest for developers, businesses, tech enthusiasts and the general public alike. From agile start-ups to trendsetting industry leaders, businesses know that successful implementation of the right machine learning product could give them a substantial competitive advantage. We have already seen businesses reap significant benefits of machine learning in production through automated chat bots and customised shopping experiences. Given we recently demonstrated how to complete web scraping in Elixir, we thought we’d take it one step further and show you to apply this in a machine learning project. The classic algorithmic approach VS machine learning The traditional approach has always been algorithm centric. To do this, you need to design an efficient algorithm to fix edge cases and meet your data manipulation needs. The more complicated your dataset, the harder it becomes to cover all the angles, and at some point, an algorithm is no longer the best way to go. Luckily, machine learning offers an alternative. When you’re building a machine learning-based system, the goal is to find dependencies in your data. You need the right information to train the program to solve the questions it is likely to be asked. To provide the right information, incoming data is vital for the machine learning system. You need to provide adequate training datasets to achieve success. So without further adieu, we’re going to provide an example tutorial for a machine learning project and show how we achieved success. Feel free to follow along. The project description For this project, we’re going to look at an ecommerce platform that offers real-time price comparisons and suggestions. The core functionality of any ecommerce machine learning project is to: Extract data from websites Process this data Provide intelligence and suggestions to the customer Variable step depending on actions and learnings Profit One of the most common problems is the need to group data in a consistent manner. For example, let’s say we want to unify the categories of products from all men’s fashion brands (so we can render all products within a given category, across multiple data sources). Each site (and therefore data source) will likely have inconsistent structures and names, these need to be unified and matched before we can run an accurate comparison. For the purpose of this guide, we will build a project which : Extracts the data from a group of websites (in this case we will demonstrate how to extract data from the harveynorman.ie shop) Train a neural network to recognise a product category from the product image Integrate the neural network into the Elixir code so it completes the image recognition and suggests products Build a web app which glues everything together. Extracting the data As we mentioned at the beginning, data is the cornerstone of any successful machine learning system. The key to success at this step is to extract real-world-data that is publicly available and then prepare it into training sets. For our example, we need to gather the basic information about the products (title, description, SKU and image URL etc). We will use the extracted images and their categories to perform the machine learning training. The quality of the trained neural network model is directly related to the quality of datasets you’re providing. So it’s important to make sure that the extracted data actually makes sense. We’re going to use a library called Crawly to perform the data extraction. Crawly is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival. You can find out more about it on the documentations page . Or you can visit our guide on how to complete web scraping in Elixir . Now that is explained, let’s get started! First of all, we will create a new Elixir project: mix new products_advisor --sup Now the project is created, modify the deps function of the mix.exs file, so it looks like this: # Run \"mix help deps\" to learn about dependencies.\n  defp deps do\n    [\n      {:crawly, \"~> 0.1\"},\n    ]\n  end Now, fetch all the dependencies: mix deps.get , and we’re ready to go. For the next step, implement the module responsible for crawling harveynorman.ie website. Save the following code under the lib/products_advisor/spiders/harveynorman.ex defmodule HarveynormanIe do\n @behaviour Crawly.Spider\n\n require Logger\n @impl Crawly.Spider\n def base_url(), do: \"https://www.harveynorman.ie\"\n\n @impl Crawly.Spider\n def init() do\n  [\n   start_urls: [\n    \"https://www.harveynorman.ie/tvs-headphones/\"\n   ]\n  ]\n end\n\n @impl Crawly.Spider\n def parse_item(response) do\n\n    # Extracting pagination urls\n  pagination_urls =\n   response.body |> Floki.find(\"ol.pager li a\") |> Floki.attribute(\"href\")\n\n    # Extracting product urls\n  product_urls =\n   response.body |> Floki.find(\"a.product-img\") |> Floki.attribute(\"href\")\n\n  all_urls = pagination_urls ++ product_urls\n\n    # Converting URLs into Crawly requests\n  requests =\n   all_urls\n   |> Enum.map(&build_absolute_url/1)\n   |> Enum.map(&Crawly.Utils.request_from_url/1)\n\n  # Extracting item fields\n  title = response.body |> Floki.find(\"h1.product-title\") |> Floki.text()\n  id = response.body |> Floki.find(\".product-id\") |> Floki.text()\n\n  category =\n   response.body\n   |> Floki.find(\".nav-breadcrumbs :nth-child(3)\")\n   |> Floki.text()\n\n  description =\n   response.body |> Floki.find(\".product-tab-wrapper\") |> Floki.text()\n\n  images =\n   response.body\n   |> Floki.find(\" .pict\")\n   |> Floki.attribute(\"src\")\n   |> Enum.map(&build_image_url/1)\n\n  %Crawly.ParsedItem{\n   :items => [\n    %{\n     id: id,\n     title: title,\n     category: category,\n     images: images,\n     description: description\n    }\n   ],\n   :requests => requests\n  }\n end\n\n defp build_absolute_url(url), do: URI.merge(base_url(), url) |> to_string()\n\n defp build_image_url(url) do\n  URI.merge(\"https://hniesfp.imgix.net\", url) |> to_string()\n end\n\nend Here we’re implementing a module called HarveynormanIe which triggers a Crawly.Spider behavior by defining its callbacks: init/0 (used to create initial request used by the spider code to fetch the initial pages), base_url/0 (used to filter out unrelated urls , e.g. urls leading to the outside world) and parse_item/1 (responsible for the conversion of the downloaded request into items and new requests to follow). Now for the basic configuration: Here we will use the following settings to configure Crawly for our platform: config :crawly,\n  # Close spider if it extracts less than 10 items per minute\n closespider_timeout: 10,\n  # Start 16 concurrent workers per domain\n concurrent_requests_per_domain: 16,\n follow_redirects: true,\n  # Define item structure (required fields)\n item: [:title, :id, :category, :description],\n  # Define item identifyer (used to filter out duplicated items)\n item_id: :id,\n  # Define item item pipelines\n pipelines: [\n  Crawly.Pipelines.Validate,\n  Crawly.Pipelines.DuplicatesFilter,\n  Crawly.Pipelines.JSONEncoder\n ] That’s it. Our basic crawler is ready, now we can get the data extracted in a JL format, sent to a folder under the name: /tmp/HarveynormanIe.jl Crawly supports a wide range of configuration options, like base_store_path which allows you to store items under different locations, see the related part of the documentation here . The full review of Crawly’s capabilities is outside of the scope of this blog post. Use the following command to start the spider: iex -S mix\nCrawly.Engine.start_spider(HarveynormanIe) You will see the following entries amongst your logs: 6:34:48.639 [debug] Scraped \"{\\\"title\\\":\\\"Sony MDR-E9LP In-Ear Headphones | Blue\\\",\\\"images\\\":[\\\"https://hniesfp.imgix.net/8/images/detailed/161/MDRE9LPL.AE.jpg?fit=fill&bg=0FFF&w=833&h=555&auto=format,compress\\\",\\\"https://hniesfp.imgix.net/8/images/feature_variant/48/sony_logo_v3.jpg?fit=fill&bg=0FFF&w=264&h=68&auto=format,compress\\\"],\\\"id\\\":\\\"MDRE9LPL.AE\\\",\\\"description\\\":\\\"Neodymium Magnet13.5mm driver unit reproduces powerful bass sound.Pair with a Music PlayerUse your headphones with a Walkman, \"<> ... The above entries indicate that a crawling process is successfully running, and we’re getting items stored in our file system. Tensor flow model training To simplify and speed up the model training process, we are going to use a pre-trained image classifier. We will use an image classifier trained on ImageNet to create a new classification layer on top of using a transfer learning technique. The new model will be based on MobileNet V2 with a depth multiplier of 0.5 and an input size of 224×224 pixels. This part is based on the Tensorflow tutorial on how to how to retrain an image classifier for new categories. If you followed the previous steps, then the training data set has already been downloaded (scraped) into a configured directory (/tmp/products_advisor by default). All the images are located according to their category: /tmp/products_advisor  \n├── building_&_hardware  \n├── computer_accessories  \n├── connected_home  \n├── headphones  \n├── hi-fi,_audio_&_speakers  \n├── home_cinema  \n├── lighting_&_electrical  \n├── storage_&_home  \n├── tools  \n├── toughbuilt_24in_wall_organizer  \n├── tv_&_audio_accessories  \n└── tvs Before the model can be trained, let’s review the downloaded data set. You can see that some categories contain a very small number of scraped images. In cases with less than 200 images, there is not enough data to accurately train your machine learning program, so we can delete these categories. find /tmp/products_advisor -depth 1 -type d \\\n        -exec bash -c \"echo -ne '{}'; ls '{}' | wc -l\" \\; \\\n    | awk '$2<200 {print $1}' \\\n    | xargs -L1 rm -rf This will leave us with just 5 categories that can be used for the new model: /tmp/products_advisor  \n├── headphones  \n├── hi-fi,_audio_&_speakers  \n├── tools  \n├── tv_&_audio_accessories  \n└── tvs Creating a model is as easy as running a python script that was created by the Tensorflow authors and can be found in the official tensorflow Github repository : TFMODULE=https://tfhub.dev/google/imagenet/mobilenet_v2_050_224/classification/2\n\npython bin/retrain.py \\  \n    --tfhub_module=$TFMODULE \\  \n    --bottleneck_dir=tf/bottlenecks \\  \n    --how_many_training_steps=1000 \\  \n    --model_dir=tf/models \\  \n    --summaries_dir=tf/training_summaries \\  \n    --output_graph=tf/retrained_graph.pb \\  \n    --output_labels=tf/retrained_labels.txt \\  \n    --image_dir=/tmp/products_advisor On the MacBook Pro 2018 2.2 GHz Intel Core i7 this process takes approximately 5 minutes. As a result, the retrained graph, along with new label categories, can be found in the configured locations (tf/retrained_graph.pb and tf/retrained_labels.txt in this example), these can be used for further image classification: IMAGE_PATH=\"/tmp/products_advisor/hi-fi,_audio_&_speakers/0017c7f1-129f-4fa7-a62b-9766d2cb4486.jpeg\"\n\npython bin/label_image.py \\\n    --graph=tf/retrained_graph.pb \\\n    --labels tf/retrained_labels.txt \\\n    --image=$IMAGE_PATH \\\n    --input_layer=Placeholder \\\n    --output_layer=final_result \\\n    --input_height=224 \\\n    --input_width=224\n\nhi-fi audio speakers 0.9721675\ntools 0.01919974\ntv audio accessories 0.008398962\nheadphones 0.00015944676\ntvs 7.433378e-05 As you can see, the newly trained model classified the images from the training set with 0.9721675 probability of belonging to the “hi-fi audio speakers” category. Image classification using Elixir Using python a tensor can be created using the following code: import tensorflow as tf\n\ndef read_tensor_from_image_file(file_name):\n    file_reader = tf.read_file(\"file_reader\", input_name)\n    image_reader = tf.image.decode_jpeg(\n        file_reader, channels=3, name=\"jpeg_reader\")\n    float_caster = tf.cast(image_reader, tf.float32)\n    dims_expander = tf.expand_dims(float_caster, 0)\n    resized = tf.image.resize_bilinear(dims_expander, [224, 224])\n    normalized = tf.divide(resized, [input_std])\n    sess = tf.Session()\n    return sess.run(normalized) Now let’s classify the images from an Elixir application. Tensorflow provides APIs for the following languages: Python, C++, Java, Go and JavaScript. Obviously, there’s no native support for BEAM languages. We could’ve used C++ bindings, though the C++ library is only designed to work with the bazel build tool. Let’s leave the mix integration with bazel as an exercise to a curious reader and instead take a look at the C API, that can be used as native implemented functions (NIF) for Elixir. Fortunately, there’s no need to write bindings for Elixir as there’s a library that has almost everything that we need: https://github.com/anshuman23/tensorflex As we saw earlier, to supply an image as an input for a tensorflow session, it has to be converted to an acceptable format: 4-dimensional tensor that contains a decoded normalised image that is 224×224 in size (as defined in the chosen MobileNet V2 model). The output is a 2-dimensional tensor that can hold a vector of values. For a newly trained model, the output is received as a 5x1 float32 tensor. 5 comes from the number of classes in the model. Image decoding Let’s assume that the images are going to be provided encoded in JPEG. We could write a library to decode JPEG in Elixir, however, there are several open source C libraries that can be used from NIFs. The other option would be to search for an Elixir library, that already provides this functionality. Hex.pm shows that there’s a library called imago that can decode images from different formats and perform some post-processing. It uses rust and depends on other rust libraries to perform its decoding. Almost all its functionality is redundant in our case. To reduce the number of dependencies and for educational purposes, let’s split this into 2 simple Elixir libraries that will be responsible for JPEG decoding and image resizing. JPEG decoding This library will use a JPEG API to decode and provide the image. This makes the Elixir part of the library responsible for loading a NIF and documenting the APIs: defmodule Jaypeg do\n  @moduledoc \n  Simple library for JPEG processing.\n\n  ## Decoding\n\n   elixir\n  {:ok, <<104, 146, ...>>, [width: 2000, height: 1333, channels: 3]} =\n      Jaypeg.decode(File.read!(\"file/image.jpg\"))\n\n\n\n  @on_load :load_nifs\n\n  @doc \n  Decode JPEG image and return information about the decode image such\n  as width, height and number of channels.\n\n  ## Examples\n\n      iex> Jaypeg.decode(File.read!(\"file/image.jpg\"))\n      {:ok, <<104, 146, ...>>, [width: 2000, height: 1333, channels: 3]}\n\n\n  def decode(_encoded_image) do\n    :erlang.nif_error(:nif_not_loaded)\n  end\n\n  def load_nifs do\n    :ok = :erlang.load_nif(Application.app_dir(:jaypeg, \"priv/jaypeg\"), 0)\n  end\nEnd The NIF implementation is not much more complicated. It initialises everything necessary for decoding the JPEG variables, passes the provided content of the image as a stream to a JPEG decoder and eventually cleans up after itself: static ERL_NIF_TERM decode(ErlNifEnv *env, int argc,\n                           const ERL_NIF_TERM argv[]) {\n  ERL_NIF_TERM jpeg_binary_term;\n  jpeg_binary_term = argv[0];\n  if (!enif_is_binary(env, jpeg_binary_term)) {\n    return enif_make_badarg(env);\n  }\n\n  ErlNifBinary jpeg_binary;\n  enif_inspect_binary(env, jpeg_binary_term, &jpeg_binary);\n\n  struct jpeg_decompress_struct cinfo;\n  struct jpeg_error_mgr jerr;\n  cinfo.err = jpeg_std_error(&jerr);\n  jpeg_create_decompress(&cinfo);\n\n  FILE * img_src = fmemopen(jpeg_binary.data, jpeg_binary.size, \"rb\");\n  if (img_src == NULL)\n    return enif_make_tuple2(env, enif_make_atom(env, \"error\"),\n                            enif_make_atom(env, \"fmemopen\"));\n\n  jpeg_stdio_src(&cinfo, img_src);\n\n  int error_check;\n  error_check = jpeg_read_header(&cinfo, TRUE);\n  if (error_check != 1)\n    return enif_make_tuple2(env, enif_make_atom(env, \"error\"),\n                            enif_make_atom(env, \"bad_jpeg\"));\n\n  jpeg_start_decompress(&cinfo);\n\n  int width, height, num_pixels, row_stride;\n  width = cinfo.output_width;\n  height = cinfo.output_height;\n  num_pixels = cinfo.output_components;\n  unsigned long output_size;\n  output_size = width * height * num_pixels;\n  row_stride = width * num_pixels;\n\n  ErlNifBinary bmp_binary;\n  enif_alloc_binary(output_size, &bmp_binary);\n\n  while (cinfo.output_scanline < cinfo.output_height) {\n    unsigned char *buf[1];\n    buf[0] = bmp_binary.data + cinfo.output_scanline * row_stride;\n    jpeg_read_scanlines(&cinfo, buf, 1);\n  }\n\n  jpeg_finish_decompress(&cinfo);\n  jpeg_destroy_decompress(&cinfo);\n\n  fclose(img_src);\n\n  ERL_NIF_TERM bmp_term;\n  bmp_term = enif_make_binary(env, &bmp_binary);\n  ERL_NIF_TERM properties_term;\n  properties_term = decode_properties(env, width, height, num_pixels);\n\n  return enif_make_tuple3(\n    env, enif_make_atom(env, \"ok\"), bmp_term, properties_term);\n} Now, all that’s left to do to make the tooling work is to declare the NIF functions and definitions. The full code is available on github . Image resizing Even though it is possible to reimplement the image operation algorithm using Elixir, this is out of the scope of this exercise and we decided to use C/C++ stb library, that is distributed under a public domain and can be easily integrated as an Elixir NIF. The library is literally just a proxy for a C function that resizes an image, the Elixir part is dedicated to the NIF load and documentation: static ERL_NIF_TERM resize(ErlNifEnv *env, int argc,\n                           const ERL_NIF_TERM argv[]) {\n  ErlNifBinary in_img_binary;\n  enif_inspect_binary(env, argv[0], &in_img_binary);\n\n  unsigned in_width, in_height, num_channels;\n  enif_get_uint(env, argv[1], &in_width);\n  enif_get_uint(env, argv[2], &in_height);\n  enif_get_uint(env, argv[3], &num_channels);\n\n  unsigned out_width, out_height;\n  enif_get_uint(env, argv[4], &out_width);\n  enif_get_uint(env, argv[5], &out_height);\n\n  unsigned long output_size;\n  output_size = out_width * out_height * num_channels;\n  ErlNifBinary out_img_binary;\n  enif_alloc_binary(output_size, &out_img_binary);\n\n  if (stbir_resize_uint8(\n        in_img_binary.data, in_width, in_height, 0,\n        out_img_binary.data, out_width, out_height, 0, num_channels) != 1)\n    return enif_make_tuple2(\n      env,\n      enif_make_atom(env, \"error\"),\n      enif_make_atom(env, \"resize\"));\n\n  ERL_NIF_TERM out_img_term;\n  out_img_term = enif_make_binary(env, &out_img_binary);\n\n  return enif_make_tuple2(env, enif_make_atom(env, \"ok\"), out_img_term);\n} The image resizing library is available on github as well. Creating a tensor from an image Now it’s time to create a tensor from the processed images (after it has been decoded and resized). To be able to load a processed image as a tensor, the Tensorflex library should be extended with 2 functions: Create a matrix from a provided binary Create a float32 tensor from a given matrix. Implementation of the functions are very Tensorflex specific and wouldn’t make much sense to a reader without an understanding of the context. NIF implementation can be found on github and can be found under functions binary_to_matrix and matrix_to_float32_tensor respectively. Putting everything together Once all necessary components are available, it’s time to put everything together. This part is similar to what can be seen at the beginning of the blog post, where the image was labelled using Python, but this time we are going to use Elixir to leverage all the libraries that we have modified: def classify_image(image, graph, labels) do\n    {:ok, decoded, properties} = Jaypeg.decode(image)\n    in_width = properties[:width]\n    in_height = properties[:height]\n    channels = properties[:channels]\n    height = width = 224\n\n    {:ok, resized} =\n      ImgUtils.resize(decoded, in_width, in_height, channels, width, height)\n\n    {:ok, input_tensor} =\n      Tensorflex.binary_to_matrix(resized, width, height * channels)\n      |> Tensorflex.divide_matrix_by_scalar(255)\n      |> Tensorflex.matrix_to_float32_tensor({1, width, height, channels})\n\n    {:ok, output_tensor} =\n      Tensorflex.create_matrix(1, 2, [[length(labels), 1]])\n      |> Tensorflex.float32_tensor_alloc()\n\n    Tensorflex.run_session(\n      graph,\n      input_tensor,\n      output_tensor,\n      \"Placeholder\",\n      \"final_result\"\n    )\n  end classify_image function returns a list of probabilities for each given label: iex(1)> image = File.read!(\"/tmp/tv.jpeg\")\n<<255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 255,\n  219, 0, 132, 0, 9, 6, 7, 19, 19, 18, 21, 18, 19, 17, 18, 22, 19, 21, 21, 21,\n  22, 18, 23, 19, 23, 18, 19, 21, 23, ...>>\niex(2)> {:ok, graph} = Tensorflex.read_graph(\"/tmp/retrained_graph.pb\")\n{:ok,\n %Tensorflex.Graph{\n   def: #Reference<0.2581978403.3326476294.49326>,\n   name: \"/Users/grigory/work/image_classifier/priv/retrained_graph.pb\"\n }}\niex(3)> labels = ImageClassifier.read_labels(\"/tmp/retrained_labels.txt\")\n[\"headphones\", \"hi fi audio speakers\", \"tools\", \"tv audio accessories\", \"tvs\"]\niex(4)> probes = ImageClassifier.classify_image(image, graph, labels)\n[\n  [1.605743818799965e-6, 2.0029481220262824e-6, 3.241990925744176e-4,\n   3.040388401132077e-4, 0.9993681311607361]\n] retrained_graph.pb and retrained_labels.txt can be found in the tf directory of the products-advisor-model-trainer repository that was mentioned earlier on in the model training step. If the model was trained successfully, tf directory should be similar to this tree: /products-advisor-model-trainer/tf/  \n├── bottlenecks  \n├── retrained_graph.pb  \n├── retrained_labels.txt  \n└── training_summaries The most probable label can easily be found by the following line: iex(6)> List.flatten(probes) |> Enum.zip(labels) |> Enum.max()\n{0.9993681311607361, \"tvs\"} Learn more So there you have it. This is a basic demonstration of how Elixir can be used to complete machine learning projects. The full code is available on the github . If you’d like to stay up-to-date with more projects like this, why not sign up to our newsletter? Or check out our detailed blog on how to complete web scraping in Elixir . Or, if you’re planning a machine learning project, why not talk to us, we’d be happy to help. Our work with the Elixir programming language.", "date": "2019-10-23rd"},
{"website": "Erlang-Solutions", "title": "RabbitMQ Mirrored Queues Gotchas", "author": ["Szymon Mentel"], "link": "https://www.erlang-solutions.com/blog/rabbitmq-mirrored-queues-gotchas/", "abstract": "Mirrored Queues are a popular RabbitMQ feature that provides High Availability (HA). HA, in this context simply means that RabbitMQ nodes in the cluster can fail and the queues will still be available for the clients. However, the HA queues can lead to some unexpected behaviour in failure scenarios or when combined with specific queue properties. In this blog post, we share three examples of these unpredictable behaviours that we have come across in RabbitMQ. This blog will help us explain some of the intricacies of HA queues. In doing so, we’ll also demonstrate how one can analyze the behaviours of a RabbitMQ cluster on a laptop or a single machine using common tools. Thus the next chapter briefly discusses the requirements to do so and scripts in the assisting repository that allow us to test the presented cases. Setup If you want to reproduce the examples from the post you will need the following dependencies installed: Make Git Docker Python3 Pipenv Rabbitmq-perf-test 2.8.0 Wireshark (optional) All the scenarios are based on a 2-node cluster consisting of RabbitMQ Docker containers - rmq1 and rmq2 - running Rabbit in version 3.7.15 . Both containers expose ports 5672 (AMQP) and 15672 ( management plugin ) which are mapped to 5672/15672 and 5673/15673 for rmq1 and rmq2 respectively. In other words, once you set up the cluster, AMQP port for rmq1 is available at amqp://localhost:5672 and the management interface at http://localhost:15672 . The cluster is started with make up and tore down with make down . The up command will start the containers, attach them to a rmq network and install the following policy: To see the logs run make logs1 to attach the output of the rmq1 container. Also, Python scripts are in use, thus pipenv install and pipenv shell need to be run to install the Python dependencies and start a shell within the python virtualenv respectively. Auto-delete property for an HA queue A queue in RabbitMQ can have the auto-delete property set. A queue with this property will be deleted by the broker once the last consumer unsubscribes. But what does this mean in a distributed environment where consumers are connected to different nodes and queue slaves are promoted to masters on failure? Let’s explore this example by setting up an environment for testing. Run make up which will spawn and cluster the RabbitMQ containers. The command should finish with an output similar to the below: Cluster status of node rabbit@rmq1 ...  \n[{nodes,[{disc,[rabbit@rmq1,rabbit@rmq2]}]},  \n{running_nodes,[rabbit@rmq2,rabbit@rmq1]},  \n{cluster_name,<<\"rabbit@rmq1\">>},  \n{partitions,[]}, {alarms,[{rabbit@rmq2,[]},{rabbit@rmq1,[]}]}] Now we want to create a Mirrored Queue with the master at node rmq2 and the slave at rmq1 . The queue should have the auto-delete property set. For this purpose, we will use the PerfTest tool that we will connect to the second node and make it act as a producer. It will create haq queue (which matches the policy), bind it to the direct exchange with the key routing key and start producing 1 message per second: # producer at rmq2\nperf_test/bin/runjava com.rabbitmq.perf.PerfTest \\\n--uri amqp://localhost:5673 \\\n--producers 1 \\\n--consumers 0 \\\n--rate 1 \\\n--queue haq \\\n--routing-key key \\\n--auto-delete true Throughout the example, we assume perf_test is installed in the ./perf_test directory . As the producer is running, the queue should appear in the management UI and messages should be piling up: Now let’s connect a consumer to our queue. Again, PerfTest will be our tool of choice but this time it will be used as a consumer attached to the first node ( rmq1 ): # consumer at rmq1 \nperf_test/bin/runjava com.rabbitmq.perf.PerfTest \\ --uri amqp://localhost:5672 \\\n--producers 0 \\ \n--consumers 1 \\ \n--queue haq The perf_test output should reveal that the messages are flowing: # consumer at rmq1  \nid: test-114434-950, time: 73.704s, received: 1.00 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs  \n# producer id: test-113752-488, time: 487.154s, sent: 1.0 msg/s At this point, we have the following setup: Now, let’s see what using the auto-delete property on HA queues can lead to in a failure scenario. Let’s introduce some turbulence into the cluster and disconnect rmq2 from the network between the nodes: make disconnect2 Underneath a docker network disconnect command is run that detaches the node from the rmq network. This should result in the consumer draining all the messages (as the producer is at the disconnected node and no new messages arrive at the queue). After approximately 10s the nodes should report that they cannot see each other. Below is a log from rmq2 where all this happens: 2019-06-07 10:02:20.910 [error] <0.669.0> ** Node rabbit@rmq1 not responding ** ** Removing (timedout) connection ** \n2019-06-07 10:02:20.910 [info] <0.406.0> rabbit on node rabbit@rmq1 down 2019-06-07 10:02:20.919 [info] <0.406.0> Node rabbit@rmq1 is down, deleting its listeners \n2019-06-07 10:02:20.921 [info] <0.406.0> node rabbit@rmq1 down: net_tick_timeout But what happened to our queue? If you look at the management interface of rmq2 (the node with the producer, http://localhost:15673/#/queues , user/password is guest/guest ) the queue is gone! This is because once the netsplit has been detected, no consumers were left at rmq2 and the queue got deleted. What’s more, all the messages that made it to the master queue on rmq2 before the netsplit was detected are lost – unless Publisher Confirms was in play. The Producer would simply not receive confirms for the messages that were not accepted by the mirror. The queue should still be present at rmq1 : If we switch our producer to rmq1 the message flow should go back to normal: # producer at rmq2 switched to rmq1 perf_test/bin/runjava com.rabbitmq.perf.PerfTest \\ -uri amqp://localhost:5672 \\\n--producers 1 \\ \n--consumers 0 \\ \n--rate 1 \\ \n--queue haq \\ \n--routing-key key \\ \n--auto-delete true Also, note that our slave at rmq1 was promoted to master: 2019-06-07 10:02:22.740 [info] <0.1858.0> Mirrored queue 'haq' in vhost '/': Slave <rabbit@rmq1.3.1858.0> saw deaths of mirrors <rabbit@rmq2.1.1096.0>\n2019-06-07 10:02:22.742 [info] <0.1858.0> Mirrored queue 'haq' in vhost '/': Promoting slave <rabbit@rmq1.3.1858.0> to master As we are left with one copy of the queue with one consumer, if that last consumer is disconnected, the entire queue will be lost. Long story short: if you set up your HA queue with the auto-delete property, then in a fail-over scenario replicas can be deleted because they may lose consumers connected through other nodes. Using either ignore or autoheal cluster partition handling will keep the replicas. In the next section, we discuss the same scenario but with consumer cancel notification in use. Before moving on to the next section, restart our environment: make down up . The consumer is notified about the fail-over Now have a look at a slightly different scenario in which a consumer specifies that it wants to be notified in case of a master fail-over. To do so, it sets the x-cancel-on-ha-failover to true when issuing basic.consume ( more here ). It differs from the previous example in that the consumer will get the basic.cancel method from the server once it detects the queue master is gone. And now, the client can handle this method as it wishes. E.g. it can restart the consumption by issuing basic.consume again, it could even reopen the channel or reconnect before resuming the consumption. Even if it crashes, upon receiving the basic.cancel , the queue will not be deleted. Note, the consumption can be resumed only once a new master is elected. For the same reasons as the previous example, the replica at another node ( rmq2 ) will be gone. You can test all these consumer behaviours with the consume_after_cancel.py script. Below we will go through a case where the consumer will resume consumption after getting the basic.cancel . At the beginning, set up the cluster with make up and a producer at rmq2 - just as we did in the previous section: # producer at rmq2\nperf_test/bin/runjava com.rabbitmq.perf.PerfTest \\\n--uri amqp://localhost:5673 \\\n--producers 1 \\\n--consumers 0 \\\n--rate 1 \\\n--queue haq \\\n--routing-key key \\\n--auto-delete true Next, connect our new consumer to rmq1 (remember to pipenv install and pipenv shell as mentioned in the Setup): ./consume_after_cancel.py --server localhost --port 5672 --after-cancel reconsume When the messages start flowing (they look meaningless), disconnect the second node from the cluster: make disconnect2 That should stop our consumer for a few moments, but it will resume consumption – since there’s a netsplit there are no messages to consume. Thus, we need to move the producer to the 1st node: # producer at rmq2 switched to rmq1 perf_test/bin/runjava com.rabbitmq.perf.PerfTest \\ -uri amqp://localhost:5672 \\\n--producers 1 \\ \n--consumers 0 \\ \n--rate 1 \\ \n--queue haq \\ \n--routing-key key \\ \n--auto-delete true At this point, the message will be flowing again. When studying these corner cases Wireshark and its AMQP dissector comes in very handy as it is crystal clear what the client-server conversation looks like. In our particular case, this is what can be seen: As a simple explanation: using the x-cancal-on-ha-failover gives a piece of extra information indicating that a fail-over has happened which can be acted upon appropriately – e.g. to automatically subscribe to the new master. We are then left with one consumer, and all the replicas without consumers being cleared. HA queues with automatic synchronization If we take a closer look at our HA policy (see the Setup section) it specifies the queue synchronization as automatic : \"ha-sync-mode\":\"automatic\" It means that when a node joins a cluster (or rejoins after a restart or being partitioned) and a mirror is to be installed on it, all the messages will be copied to it from the master. The good thing about this is that the new mirror has all the messages straight after joining, which increases data safety. However, it comes with a cost: when a queue is being synchronized, all the operations on the queue are blocked. It means that for example, a publisher cannot publish and a consumer cannot consume. Let’s have a look at an example in which we publish 200K messages to the haq queue, then attach a slow consumer and finally restart one node. As the policy specifies that the queue is to have 2 replicas with automatic synchronization, the restart will trigger messages to be copied to keep the replicas in sync. Also, the \"ha-sync- batch-size\":2 set in the policy indicates that RabbitMQ will synchronize 2 messages at a time – to slow down the synchronization and exaggerate “the blocking effect”. Run make up to setup the cluster. Then publish the messages: # 10*20K = 200K \n./perf_test/bin/runjava com.rabbitmq.perf.PerfTest \\\n--uri amqp://localhost:5672 \\\n--producers 10 \\\n--consumers 0 \\\n--queue haq \\\n--pmessages 20000 \\\n--auto-delete false \\\n--rate 1000 Check the queue is filled up ( http://localhost:15672/#/queues ) When it’s ready we can start the slow consumer which will process 10 msgs/s with a prefetch count of 10: ./perf_test/bin/runjava com.rabbitmq.perf.PerfTest \\\n--uri amqp://localhost:5672 \\\n--producers 0 \\\n--consumers 1 \\\n--queue haq \\\n--predeclared \\\n--qos 10 \\\n--consumer-rate 10 Then restart the rmq2 ( make restart2 ) and observe both the consumer’s logs and the rmq1 logs. In the consumers’ logs you should see some slowdown in receiving of messages: id: test-145129-551, time: 20.960s, received: 10.0 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs\nid: test-145129-551, time: 21.960s, received: 10 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs\nid: test-145129-551, time: 23.058s, received: 10 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs\n--> id: test-145129-551, time: 31.588s, received: 0.82 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs \nid: test-145129-551, time: 32.660s, received: 83 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs \nid: test-145129-551, time: 33.758s, received: 10 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs \nid: test-145129-551, time: 34.858s, received: 10 msg/s, min/median/75th/95th/99th consumer latency: 0/0/0/0/0 μs The marked line shows that the consumer was waiting for nearly 10 seconds for a message. And the RabbitMQ logs can explain why: 2019-06-04 13:36:33.076 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 200000 messages to synchronise 2019-06-04 13:36:33.076 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: batch size: 2 \n2019-06-04 13:36:33.079 [info] <0.3039.0> Mirrored queue 'haq' in vhost '/': Synchronising: mirrors [rabbit@rmq2] to sync 2019-06-04 13:36:34.079 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 22422 messages \n2019-06-04 13:36:35.079 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 43346 messages 2019-06-04 13:36:36.080 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 64512 messages \n2019-06-04 13:36:37.084 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 82404 messages 2019-06-04 13:36:38.095 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 104404 messages \n2019-06-04 13:36:39.095 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 128686 messages 2019-06-04 13:36:40.095 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 147580 messages \n2019-06-04 13:36:41.096 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 164498 messages 2019-06-04 13:36:42.096 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: 182408 messages \n2019-06-04 13:36:42.961 [info] <0.1557.0> Mirrored queue 'haq' in vhost '/': Synchronising: complete If we compare timestamps of the first line, indicating the start of the synchronization, with the last line, indicating the end of the synchronization, we can see it took nearly 10 seconds. And this is exactly when the queue was blocked. In real life, obviously, we would not have the ha-sync-batch-size set to 2 – it is by default set to 4096. But what happens in real life is that there can be multiple queues, longer than 200K, with larger messages. All of that can contribute to a high volume of traffic and prolonged synchronization time resulting in queues that remain blocked for longer. The takeaway is: make sure you are in control of the length of your queues if automatic synchronization is in use. This can be achieved by limiting the length of queues or setting TTLs for messages . Delayed Publisher Confirms If you’re concerned about data safety, you probably need Publisher Confirms . This mechanism makes the broker send an ACK for each successfully accepted message. But what does successfully accepted mean? As per the documentation : For routable messages, the basic.ack is sent when a message has been accepted by all the queues. For persistent messages routed to durable queues, this means persisting to disk. For mirrored queues, this means that all mirrors have accepted the message. As we are discussing HA queues, the statement saying that an ACK is sent when ALL the mirrors have accepted the message is the interesting one. To see how this can cause complications, let’s consider the following setup based on the RPC messaging pattern: We have a server that subscribes to haq_jobs queue on which it expects messages with integers in their body. Once it gets a message, it computes a Fibonacci number for that integer and sends the reply back to the reply queue stores it in the reply_to property of the original message. The client in turn (denoted as P/C in the picture), when invoked, will send an integer to the haq_jobs queue and wait for the reply on the temporary queue. It opens two connections: one for publishing the request and one for getting the response. The reply queue is exclusive as it is meant to be used by just one consumer. What is more, the message is sent in a separate thread which then waits for the ACK. In other words, sending a message and waiting for the reply are independent thread-wise. Now we’re ready to start the cluster: make up. Then in a separate shell run the server: /fib_server.py. Finally, try the client: ./fib_client.py 7 \n[2019-06-06 14:20:37] Press any key to proceed... \n[2019-06-06 14:20:37] About to publish request='7' \n[2019-06-06 14:20:37] Message ACKed\n[2019-06-06 14:20:37] Got response=13\n./fib_client.py 7 \n[2019-06-06 14:20:39] Press any key to proceed... \n[2019-06-06 14:20:39] About to publish request='7' \n[2019-06-06 14:20:39] Got response=13 \n[2019-06-06 14:20:39] Message ACKed As you may have noticed, in the second invocation of the client, we got the first response and then the ACK for the request. That may look weird, but this is due to the fact that the confirms are sent asynchronously once all the mirrors accept the message. In this particular case, it turned out that the Fibonacci server and RabbitMQ were quicker to produce the result and deliver it than the cluster to replicate the request and send the confirmation. What can we conclude from this? Never rely on the confirm to come before the reply in the RPC (Remote Procedure Call) pattern – although they should come very close to each other. The last statement, that the ACK and the reply come shorty after one another, doesn’t hold true in a failure scenario where the network/node is broken. To simulate that, let’s clean the environment with make down and set the net_ticktime to 120s to make Rabbit detect net-splits slower. This will give us time to perform the actions after we break the network. It has been set to 10s in the previous experiments to make them run faster, specifically to make the net-split detection run fast. Edit the conf/advanced.config as follows: This is needed for the second experiment in this section as it gives us time to perform the required actions; it doesn’t affect the first experiment. It has been initially set to 10 so that we could go through the tests in the previous sections fast. Modify the conf/advanced.config so that the option is set to 120s instead of 10: [ \n {kernel, [{net_ticktime, 120}]}\n]. Now we can run the cluster, make up , and start the server and client as previously. But leave the clients at the request to press a key and stop for a moment to reflect on the current state of the system: *we have the Fibonacci server connected and waiting for the requests, *we have 2 connections opened from the client: one to publish and one to consume, *all the queues are set up. Now run make disconnect2 to disconnect rmq2 from the cluster. At this point, you have between 90 to 150 seconds (as the net_ticktime is set to 120s) to press any button to publish the message. You should see that the server will process it and you will get the response but the ACK for the request won’t be delivered until you recover the net-split with make connect2 (which you have to do within the given time-range). [2019-06-06 14:21:32] Press any key to proceed... [2019-06-06 14:21:38] About to publish request='7' [2019-06-06 14:21:38] Got response=13\n[2019-06-06 14:22:05] Message ACKed \nIn the above listing, the ACK arrived ~30s after the actual response due to the net-split. Remember: one cannot expect that a message will be ACKed by the broker before the reply arrives on the reply queue (RPC pattern) – as we’ve just seen the ACK can get stuck due to some turbulence on the network. Summary Undoubtedly Mirror Queues provide an important feature of RabbitMQ – which is High Availability. In simple words: depending on the configuration, you will always have more than one copy of the queue and its data. As a result, you are ready for a node/network failure which will inevitably happen in a distributed system. Nonetheless, as illustrated in this blog post, in some specific failure scenarios or configuration, you may observe unexpected behaviours or even data/queue loss. Thus it’s important to test your system against potential failures and scenarios which are not (clearly) covered by the documentation. If you’re interested in learning more about HA in RabbitMQ, talk to us at the RabbitMQ Summit in London or contact us today for a RabbitMQ health check . Monitor your Erlang, Elixir and RabbitMQ systems Are you currently using Erlang, Elixir or RabbitMQ in your stack? Get full visibility of your system with WombatOAM, find out more and get a free trial on our WombatOAM page. Want to learn more? For more information on High Availability in RabbitMQ, watch this video from our Erlang Meetup in Krakow. Our work with RabbitMQ.", "date": "2019-10-25"},
{"website": "Erlang-Solutions", "title": "How to debug your RabbitMQ", "author": ["Kacper Mentel"], "link": "https://www.erlang-solutions.com/blog/how-to-debug-your-rabbitmq/", "abstract": "What you will learn in this blog. Our RabbitMQ consultancy customers come from a wide range of industries. As a result, we have seen almost all of the unexpected behaviours that it can throw at you. RabbitMQ is a complex piece of software that employs concurrency and distributed computing (via Erlang), so debugging it is not always straightforward. To get to the root cause of an unexpected (and unwanted) behaviour, you need the right tools and the right methodology. In this article, we will demonstrate both to help you learn the craft of debugging in RabbitMQ. The problem of debugging RabbitMQ. The inspiration for this blog comes from a real-life example. One of our customers had the RabbitMQ Management HTTP API serving crucial information to their system. The system relied on the API heavily, specifically on /api/queues endpoint because the system needed to know the number of messages ready in each queue in a RabbitMQ cluster. The problem was that sometimes a HTTP request to the endpoint lasted up to tens of seconds (in the worst case they weren’t even able to get a response from the API). So what caused some requests to take so much time? To answer that question, we tried to reproduce the issue through load testing. Running load tests We use a platform that we created for MongooseIM to run our Continuous Load Testing . Here are some of the most important aspects of the platform: all the services that are involved in a load test run inside docker containers the load is generated by Amoc ; it’s an open source tool written in Erlang for generating massively parallel loads of any kind (AMQP in our case) metrics from the system under test and Amoc site are collected for further analysis. The diagram below depicts a logical architecture of an example load test with RabbitMQ: In the diagram, the left-hand side, shows a cluster of Amoc nodes that emulate AMQP clients which, in turn, generate the load against RabbitMQ. On the other side, we can see a RabbitMQ cluster that serves the AMQP clients. All the metrics from both the Amoc and RabbitMQ services are collected and stored in an InfluxDB database. Slow Management HTTP API queries We tried to reproduce the slow queries to Management HTTP API in our load tests. The test scenario was fairly simple. A bunch of publishers were publishing messages to default exchange. Messages from each publisher were routed to a dedicated queue (each publisher had a dedicated queue). There were also consumers attached to each queue. Queue mirroring was enabled. For concrete values, check the table below: That setup stressed the Rabbit servers on our infrastructure. As seen in the graphs below: Every RabbitMQ node consumed about 6 (out of 7) CPU cores and roughly 1.4GB of RAM except for rabbitmq-1 which consumed significantly more than the others. That was likely because it had to serve more of the Management HTTP API requests than the other two nodes. During the load test /api/queues endpoint was queried every two seconds for the list of all queues together with corresponding messages_ready values. A query looked like this: http://rabbitmq-1:15672/api/queues?columns=name,messages_ready Here are the results from the test: The figure above shows the query time during a load test. It’s clear that things are very slow. The median equals 1.5s while the 95, 99, 999 percentiles and max reach 20s . Debugging Once the issue is confirmed and can be reproduced, we are ready to start debugging. The first idea was to find the Erlang function that is called when a request to the RabbitMQ Management HTTP API comes in and determine where that function spends its execution time. If we were able to do this it would allow us to localise the most time expensive code behind the API. Finding the entrypoint function To find the function we were looking for we took the following steps: looked through the RabbitMQ Management Plugin to find the appropriate “HTTP path to function” mapping, used the Erlang tracing feature to verify if a found function is really called when a request comes in. The management plugin uses cowboy (an Erlang HTTP server) underneath to serve the API requests. Each HTTP endpoint requires a cowboy callback module, so we easily found the rabbit_mgmt_wm_queues:to_json/2 function which seemed to handle requests coming to the /api/queues . We confirmed that with tracing (using a recon library that is shipped with RabbitMQ by default). root@rmq-test-rabbitmq-1:/rabbitmq_server-v3.7.9# erl -remsh rabbit@rmq-test-rabbitmq-1 -sname test2 -setcookie rabbit  \nErlang/OTP 21 [erts-10.1] [source] [64-bit] [smp:22:7] [ds:22:7:10] [async-threads:1]  \n\nEshell V10.1  (abort with ^G)  \n(rabbit@rmq-test-rabbitmq-1)1> recon_trace:calls({rabbit_mgmt_wm_queues, to_json, 2}, 1).  \n1  \n\n11:0:48.464423 <0.1294.15> rabbit_mgmt_wm_queues:to_json(#{bindings => #{},body_length => 0,cert => undefined,charset => undefined,  \n  has_body => false,  \n  headers =>  \n      #{<<\"accept\">> => <<\"*/*\">>,  \n        <<\"authorization\">> => <<\"Basic Z3Vlc3Q6Z3Vlc3Q=\">>,  \n        <<\"host\">> => <<\"10.100.10.140:53553\">>,  \n        <<\"user-agent\">> => <<\"curl/7.54.0\">>},  \n  host => <<\"10.100.10.140\">>,host_info => undefined,  \n  media_type => {<<\"application\">>,<<\"json\">>,[]},  \n  method => <<\"GET\">>,path => <<\"/api/queues\">>,path_info => undefined,  \n  peer => {{10,100,10,4},54136},  \n  pid => <0.1293.15>,port => 53553,qs => <<\"columns=name,messages_ready\">>,  \n  ref => rabbit_web_dispatch_sup_15672,  \n  resp_headers =>  \n      #{<<\"content-security-policy\">> => <<\"default-src 'self'\">>,  \n        <<\"content-type\">> => [<<\"application\">>,<<\"/\">>,<<\"json\">>,<<>>],  \n        <<\"vary\">> =>  \n            [<<\"accept\">>,  \n             [<<\", \">>,<<\"accept-encoding\">>],  \n             [<<\", \">>,<<\"origin\">>]]},  \n  scheme => <<\"http\">>,  \n  sock => {{172,17,0,4},15672},  \n  streamid => 1,version => 'HTTP/1.1'}, {context,{user,<<\"guest\">>,  \n               [administrator],  \n               [{rabbit_auth_backend_internal,none}]},  \n         <<\"guest\">>,undefined})  \nRecon tracer rate limit tripped. The snippet above shows that we enabled tracing for rabbit_mgmt_wm_queues:to_json/2 first, then we manually sent a request to the Management API (using curl; not visible on the snippet) and which generated the trace event. That’s how we found our entrypoint for further analysis. Using flame graphs Having found a function that serves the requests, we can now check how that function spends its execution time. The ideal technique to do this is Flame Graphs . One of its definitions states: Flame graphs are a visualisation of profiled software, allowing the most frequent code-paths to be identified quickly and accurately. In our case, we could use flame graphs to visualise the stack trace of the function or, in other words, which functions are called inside a traced function, and how much time it takes (relatively to the traced function’s execution time) for these functions to execute. This visualisation helps to identify suspicious functions in the code quickly. For Erlang, there is a library called eflame that has tools for both: gathering traces from the Erlang system and building a flame graph from the data. But how do we inject that library into Rabbit for our load test? Building a custom RabbitMQ docker image As we mentioned previously, all the services in our load testing platform run inside docker containers. Hence, we had to build a custom RabbitMQ docker image with the eflame library included in the server code. We created a rabbitmq-docker repository that makes it easy to build a docker image with modified RabbitMQ source code. Profiling with eflame Once we had a modified RabbitMQ docker image with eflame included, we could run another load test (specifications were the same as the previous test) and start the actual profiling. These were the results: We ran a number of measurements and had two types of result as presented above. The main difference between these graphs is in rabbit_mgmt_util:run_run_augmentation/2 function. What does that difference mean? From the results of the previous load tests and manual code analysis, we know that there are slow and fast queries. The slow requests can take up to twenty seconds while the fast ones only take a few. It confirms the query time chart above with: 50 percentile about 1.5s but 95 (and higher percentiles) equaling up to 20s. Moreover, we manually measured execution time of both cases using timer:tc/3 and the results were consistent. This happens because there is a cache in the Management plugin. When the cache is valid, the requests are served much faster as the data has already been collected, but when it’s invalid, all the necessary information needs to be gathered. Despite the fact that the graphs have the same length in the picture, they represent different execution times (fast vs slow). Hence it’s hard to guess which graph shows which query without actually taking a measurement. The first graph shows a fast query while the second shows a slow one. In the slow query graph rabbit_mgmt_util:augment/2 -> rabbit_mgmt_db:submit_cached/4 -> gen_server:call/3 -> … the stack takes so much time because the cache is invalid and fresh data needs to be collected. So what happens when data is collected? Profiling with fprof You might ask “why don’t we see the data collection function(s) in the flame graphs?” This happens because the cache is implemented as another Erlang process and the data collection happens inside the cache process . There is a gen_server:call/3 function visible in the graphs that makes a call to the cache process and waits for a response. Depending on the cache state (valid or invalid) a response can come back quickly or slowly. Collecting data is implemented in rabbit_mgmt_db:list_queue_stats/3 function which is invoked from the cache process. Naturally, we should profile that function. We tried eflame and after several dozens of minutes this is the result we got: eheap_alloc: Cannot allocate 42116020480 bytes of memory (of type \"old_heap\"). The Erlang heap memory allocator tried to allocate 42GB of memory (in fact, the space was needed for garbage collector to operate) and crashed the server. As eflame takes advantage of Erlang Tracing to generate flame graphs it was, most probably, simply overloaded with a number of trace events generated by the traced function. That’s where fprof comes into play. According to the official Erlang documentation fprof is: a Time Profiling Tool using trace to file for minimal runtime performance impact. That’s very true. The tool dealt with collecting data function smoothly, however it took several minutes to produce the result. The output was quite big so there are only crucial lines listed below: (rabbit@rmq-test-rabbitmq-1)96> fprof:apply(rabbit_mgmt_db, list_queue_stats, [RA, B, 5000]).  \n...\n(rabbit@rmq-test-rabbitmq-1)97> fprof:profile().  \n...\n(rabbit@rmq-test-rabbitmq-1)98> fprof:analyse().  \n...\n%                                       CNT        ACC       OWN  \n{[{{rabbit_mgmt_db,'-list_queue_stats/3-lc$^1/1-1-',4}, 803,391175.593,  105.666}],  \n { {rabbit_mgmt_db,queue_stats,3},              803,391175.593,  105.666},     %  \n [{{rabbit_mgmt_db,format_range,4},            3212,390985.427,   76.758},  \n  {{rabbit_mgmt_db,pick_range,2},              3212,   58.047,   34.206},  \n  {{erlang,'++',2},                            2407,   19.445,   19.445},  \n  {{rabbit_mgmt_db,message_stats,1},            803,    7.040,    7.040}]}. The output consists of many of these entries. The function marked with the % character is the one that the current entry concerns. The functions below are the ones that were called from the marked function. The third column ( ACC ) shows the total execution time of the marked function (the functions own execution time and callees) in milliseconds. For example, in the above entry the total execution time of the rabbit_mgmt_db:pick_range/2 function equals 58,047ms. For a detailed explanation of the fprof output check the official fprof documentation . The entry above is the top level entry concerning rabbit_mgmt_db:queue_stats/3 which was called from the traced function. That function spent most of its execution time in rabbit_mgmt_db:format_range/4 function. We can go to an entry concerning that function and check where it spent its execution time accordingly. This way, we can go through the output and find potential causes of the Management API slowness issue. Reading through the fprof output in a top-down fashion we ended up with this entry: {[{{exometer_slide,'-sum/5-anonymous-6-',7},   3713,364774.737,  206.874}],\n { {exometer_slide,to_normalized_list,6},      3713,364774.737,  206.874},     %\n [{{exometer_slide,create_normalized_lookup,4},3713,213922.287,   64.599}, %% SUSPICIOUS\n  {{exometer_slide,'-to_normalized_list/6-lists^foldl/2-4-',3},3713,145165.626,   51.991}, %% SUSPICIOUS\n  {{exometer_slide,to_list_from,3},            3713, 4518.772,  201.682},\n  {{lists,seq,3},                              3713,  837.788,   35.720},\n  {{erlang,'++',2},                            3712,   70.038,   70.038},\n  {{exometer_slide,'-sum/5-anonymous-5-',1},   3713,   51.971,   25.739},\n  {garbage_collect,                               1,    1.269,    1.269},\n  {suspend,                                       2,    0.151,    0.000}]}. The entry concerns exometer_slide:to_normalized_list/6 function which in turn called two “suspicious” functions from the same module. Going deeper we found this: {[{{exometer_slide,'-create_normalized_lookup/4-anonymous-2-',5},347962,196916.209,35453.182},\n  {{exometer_slide,'-sum/5-anonymous-4-',2},   356109,16625.240, 4471.993},\n  {{orddict,update,4},                         20268881,    0.000,172352.980}],\n { {orddict,update,4},                         20972952,213541.449,212278.155},     %\n [{suspend,                                    9301,  682.033,    0.000},\n  {{exometer_slide,'-sum/5-anonymous-3-',2},   31204,  420.574,  227.727},\n  {garbage_collect,                              99,  160.687,  160.687},\n  {{orddict,update,4},                         20268881,    0.000,172352.980}]}. and: {[{{exometer_slide,'-to_normalized_list/6-anonymous-5-',3},456669,133229.862, 3043.145},\n  {{orddict,find,2},                           19369215,    0.000,129761.708}],\n { {orddict,find,2},                           19825884,133229.862,132804.853},     %\n [{suspend,                                    4754,  392.064,    0.000},\n  {garbage_collect,                              22,   33.195,   33.195},\n  {{orddict,find,2},                           19369215,    0.000,129761.708}]}. A lot of the execution time was consumed by orddict:update/4 and orddict:find/2 functions. These two combined accounted for 86% of the total execution time. This led us to the exometer_slide module from the RabbitMQ Management Agent Plugin . If you look into the module, you’ll find all the functions above and the connections between them. We decided to close the investigation at this stage because this was clearly the issue. Now, that we’ve shared our thoughts on the issue with the community in this blog, who knows, maybe we’ll come up with a new solution together. The observer effect There is one last thing that is essential to consider when it comes to debugging/observing systems – the observer effect . The observer effect is a theory that claims if we are monitoring some kind of phenomena the observation process changes that phenomena. In our example, we used tools that take advantage of tracing. Tracing has an impact on a system as it generates, sends and processes a lot of events. Execution times of the aforementioned functions increased substantially when they were called with profiling enabled. Pure calls took several seconds while calls with profiling enabled several minutes. However, the difference between the slow and fast queries seemed to remain unchanged. The observer effect was not evaluated in the scope of the experiment described in this blog post. A workaround solution The issue can be solved in a slightly different manner. Let’s think for a while if there is another way of obtaining queues names corresponding to the amount of messages in them? There is the rabbit_amqqueue:emit_info_all/5 function that allows us to retrieve the exact information we are interested in – directly from a queue process. We could use that API from a custom RabbitMQ plugin and expose a HTTP endpoint to send that data when queried. We turned that idea into reality and built a proof of concept plugin called rabbitmq-queue-info that does exactly what’s described above. The plugin was even load tested (test specification was exactly the same as it was with the management plugin; from earlier in the blog). The results are below and they speak for themselves: Want more Want to know more about tracing in RabbitMQ, Erlang & Elixir? Check out WombatOAM, an intuitive system that makes monitoring and maintenance of your systems easy. Get your free 45 day trial of WombatOAM now . Apendix Version 3.7.9 of RabbitMQ was used in all the load tests mentioned in this blog post. Special thanks go to Szymon Mentel and Andrzej Teleżyński for all the help with that publication. Our work with RabbitMQ.", "date": "2019-11-07"},
{"website": "Erlang-Solutions", "title": "The big questions for enterprise blockchain.", "author": ["Arzu Toren"], "link": "https://www.erlang-solutions.com/blog/the-big-questions-for-enterprise-blockchain/", "abstract": "Meet Arzu Toren, Finance Industry expert and banking academic. Arzu Toren is a Global Banker with 20 years of experience working in international and Turkish commercial banks in the areas of asset management, trade finance, debt instruments and business development. She is currently continuing her independent research to drive value through the application of blockchain and for the blockchain ecosystem. In this blog, Arzu offers an up-to-date take on the key discussion points in the field of blockchain. Blockchain for good. Blockchain is a game-changing technology which is due to have fundamental impacts on enterprises as it can transform moving, storing, lending, exchanging value, funding, investing and managing risk. It is moving 3Ps – people, process and paperwork. In the digital era, data is power and blockchain is a universal bus to carry the data by ensuring its validity. Like every radical innovation, blockchain encapsulates hurdles for adoption and diffusion. It is complicated in legal, regulatory, financial and operational aspects and its challenges need technical, behavioural, educational and business solutions. Long-term adoption is due to take time with a gradual transition. The process is not linear, you learn along the way and reducing complexity is the key. Blockchain should not be considered as a magic stick but as one of the technologies that will be a part of the next-generation infrastructure and developed according to needs. Distributed ledger based on blockchain is a form of database; it will not make decisions for enterprises, they need to learn how to use it to create value. Blockchain is expanding and moves quickly in new businesses. Resistance, lack of education, issues of interoperability and scalability are the main reasons why it is moving slowly in some incumbent and large organizations. The ideal implementation steps of blockchain in enterprises are: choosing the right use case selecting the right platform starting small for testing purposes The end-customer doesn’t need to know how blockchain works, they don’t even need to know the word ‘blockchain’. While technology-side is the enabler and business-side is the main driver, the primary focus should be on the use case and outcome. Interoperability Blockchain requires a solid legal foundation to allow for interoperability. Regulations are rules imposed by an authority representing the interests of the public while governance embodies a set of rules imposed by the participants of a system to protect their own interests. Blockchain faces major uncertainties in both fields. Internationally accepted regulations need to be developed and new principles need to be introduced in order to incorporate blockchain to the market infrastructure, comply with cross border & domestic guidelines and identify the legal ownership of documents. All these arrangements need careful engineering and take time. Blockchain is designed to support networks and requires cooperation to flourish. If the ecosystem is not connected, blockchain is not much of a use. Enterprises, innovators and regulators need to collaborate to make blockchain work in practice. In the case of consortiums, the answers to questions like ‘Who has jurisdiction when a network is spread over a large number of regions and companies?’ need to be clear. A blockchain would also need a governing body, which can be challenging when stakeholders have divergent interests. Moreover, industry alignment is required in major issues such as design and interaction principles. Scalability, Security & Resilience To make full use of blockchain, high standards are required for security, robustness, and performance. Resolving challenges such as network capacity, transaction speed, verification process and data limits would be critical. Without scalability, higher energy costs could eliminate the benefits of any blockchain. If the technology does not receive sufficient market scale, the shared benefits will be muted and there will only be a marginal improvement over today’s structure. Blockchain needs to be integrated into existing technology to be usable and affordable. Replacing legacy systems with a new technology is complex with a very low tolerance for error. Moving the existing contracts and documents to blockchain based methodology requires the execution of a significant set of migration tasks. Operational risk of transition needs to be minimized. Getting to decentralization might be easier starting from scratch compared to making the transition from central to decentralized services. Furthermore, although irreversibility is one of the features of blockchain, it might become a challenge and needs to be managed carefully. The identity of each party to a blockchain-enabled transaction is protected by cryptography techniques but enterprises will want to ensure that information isn’t revealed to competitors through analysis of their information. In its current state, blockchain requires above-average computer literacy to use effectively, which acts as a barrier to entry for enterprises that are interested in applications but do not know where to begin. Additionally, there is no guarantee that blockchain will never be subject to any cybersecurity attacks; the system is robust but there is still risk. Centralised vs Decentralised DLTs for Real-World Production At the highest level, the blockchains can be categorized into three groups: Public (Permissionless) Blockchains – A public blockchain is a platform where anyone in the world can read, write, send transactions and participate in the consensus process provided they are able to show the proof of work. Public blockchains are open-source, fully decentralized and don’t have any initial costs. Private (Permissioned) Blockchains – A private blockchain allows only the owner to have the right to access or make any changes on the network. It is similar to existing infrastructure where the owners have the power to change the rules or participants based on needs. Private blockchains have quite high initial capital and maintenance expenses. Hybrid Blockchains – A hybrid blockchain would be a mix of both public and private blockchains where the abilities could be extended to different nodes based on pre-set criteria. All running costs need to be met by the participating organizations. The blockchains that would capture the most attention in the first place are private or hybrid blockchains as they are more specialized, efficient and compatible with existing systems. The most effective approach for employment of private blockchains is to treat their features like a catalog and tailor different combinations for different use-cases. However, private blockchains are very similar to distributed databases already used by enterprises and unless they integrate with public blockchains where the audit trail is more secure and control over transactions is not in the hands of trusted nodes, there is a question mark whether blockchain could have a drastic effect. In the early applications, some enterprises test blockchain internally to manage their business units. A large group with subsidiaries around the world may benefit from an internal blockchain to synchronise data and systems across group companies, AML & KYC policies and payments. Moreover, they could learn the technology, decide if it is strategically fit and test if they can expand to their customer related products as the second phase.", "date": "2019-11-13"},
{"website": "Erlang-Solutions", "title": "FinTech and Blockchain an Interview with Keith Bear from the CCAF", "author": ["Michael Jaiyeola"], "link": "https://www.erlang-solutions.com/blog/fintech-and-blockchain-an-interview-with-keith-bear-from-the-ccaf/", "abstract": "During last year’s London FinTech Week and following his appearance on the panel for the ‘Disruptive Forces in FinTech’ discussion, we were pleased to have some time to reflect on the current shape of the industry and the role of blockchain technology within it with Keith Bear from Cambridge University’s Centre for Alternative Finance (CCAF). Previously, Keith was the Global Leader for financial markets at IBM, working with banks and market infrastructure firms worldwide. His most recent work was on blockchain deployment in financial markets and FinTech innovation. Keith is now a Fellow at Cambridge University’s Centre for Alternative Finance and is also on the advisory board of three FinTechs and mentors at the Barclays/Techstars Rise accelerator. This was of particular interest to us given our expertise in FinTech and blockchain technology. Can you tell us about the aims of the Centre for Alternative Finance and your role there? Keith Bear – The department sits within the University’s Business School and has three core elements: Research – survey-based research, we have just published our recent blockchain survey Global Crypto Assets Regulatory Landscape to favourable reviews. Our next survey examines enterprise networks, it’s a deep dive into production networks. Collaboration – with the likes of the World Bank, the IMF and others in the area of FinTech, blockchain and crowdfunding peer-to-peer lending. Client projects – working with various financial institutions to leverage our insights and research. Thoughts on blockchain performance strategies, methods of consensus and centralisation Keith Bear – In this area there are significant differences between the private permissioned world and the public non-permissioned world. We recently released our tool for measuring the Bitcoin network’s energy consumption CBECI . This really illustrates the downsides of this area of the technology because of the high energy requirements involved. However, some people may see it as value for money when considering the value of Bitcoin for those who are invested in it and see the future transformative potential. Consensus methods are less of a problem for private permissioned networks because of dealing with permissioned known actors. The direction of Ethereum is publicly stated as Proof of Stake an alternative to achieving consensus, but it’s a difficult model because of all the complexities currently involved. With regards to centralised strategies for blockchain – it very much depends on how various technologies will evolve over time and it’s a little hard to predict at the moment what the future trends will be. At the CCAF we are working on an enterprise blockchain analysis at the moment, where we are finding that organisations are using elements of blockchain technology but where parts of the architecture are being centralised and so not fully embracing the fully distributed state found with Bitcoin and Ethereum. Questions over the extent of decentralisation depend upon individual use cases especially in permissioned cases where centralisation is necessary for some level of control to meet regulatory obligations etc. Blockchain and environmental sustainability Keith Bear – The CCAF Bitcoin power index shows the high levels of consumption which vary depending on the mining tools employed. On the one hand concerns about energy consumption can be countered by using blockchain as a force for good, i.e. where the technology can be used for environmental causes. One example of this can be found with a FinTech that’s part of the Rise Accelerator programme Sparkchange.io who focus on a tokenised approach to carbon allowances. Whether the [environmental] cost is justified or not justified depends on which side of the blockchain industry you are on. Blockchain identity tracking vs secret identities Keith Bear – Again it all depends on the type of networks which we are looking at. For private permissioned blockchains identity is important to financial institutions because of AML/KYC requirements. Bitcoin operates at the other extreme where, although individuals cannot be directly identified, their transactions can be. Therefore, you can start mapping individuals based on transactional movements on the network. In this respect it is less opaque than cash. For FinTechs, the choice is clear, in that you must either operate wholly within the regulatory environment or try and survive outside of it, usually where regulation isn’t yet well defined or formed. The obligations depend on the jurisdiction in which you’re operating and Libra has been a good trigger for opening up the debate over regulation. If you’re operating outside of the regulatory environment it can only be a matter of time until the authorities intervene because, since Libra, there is a lot more regulatory debate and focus than was the case previously. The Libra question Keith Bear – The Libra white paper made for interesting reading, my main thought is that it’s very, very early days and the regulatory comments already made mean there are still many questions to be answered on how and where it will operate. It is less a question about the technology but over how it will practically work. I don’t think anyone would dispute the idea of offering financial inclusion to the world’s 1.7 billion unbanked, maybe there’s some hesitancy over whether Facebook is the ideal organisation to be offering the solution. However, the set-up of the Libra Association is a positive step to help provide a level playing field. It will be interesting to see how things evolve. The key result so far is that it has stimulated debate amongst the regulators of what needs to be done. Interoperability of blockchain Keith Bear – This a very important issue, especially in the enterprise permissioned world. Our survey of 60+ production networks has found that they are using many different technologies but that they may well eventually want to connect. For example, We.Trade in Europe signing an MOU with eTrade Connect, an equivalent network based in Hong Kong. So interoperability is already a significant factor in the business application for blockchain using FinTechs and not just a technical question of how it can be done. There are a number of projects working on interoperability such as HyperLedger Quilt with Ripple. As blockchain networks mature and offer more value to society, how they join up and work together will become increasingly important. The best strategy for FinTech startups – disrupt or collaborate? Keith Bear – There are examples across the board from disruptive models to partner models such as with Transfer Wise who want to work with as well as compete with banks. It’s really dependent upon particular use cases and finding a route to scale. The future regulatory environment for FinTechs Keith Bear – Regulation is a key focus of our research. There’s a big range of different perspectives based on jurisdiction especially when looking at what is in place now and what may be required in the future. The use of regulatory sandboxes like GFIN (Global Financial Innovation Network) which is backed by the FCA provides safe environments for FinTechs to work through their propositions and initiatives. Our work in the blockchain space .", "date": "2020-02-17"},
{"website": "Erlang-Solutions", "title": "RabbitMQ Quorum Queues explained – what you need to know.", "author": ["Lajos Gerecs"], "link": "https://www.erlang-solutions.com/blog/rabbitmq-quorum-queues-explained-what-you-need-to-know/", "abstract": "Introduction to Quorum Queues In RabbitMQ 3.8.0 , one of the most significant new features was the introduction of Quorum Queues. The Quorum Queue is a new type of queue, which is expected to replace the default queue (which is now called classic ) in the future, for some use cases. This queue type is important when RabbitMQ is used in a clustered installation, it provides less network intensive message replication using the Raft protocol underneath. Usage of Quorum Queues A Classic Queue has a master running somewhere on a node in the cluster, while the mirrors run on other nodes. This works the very same way for Quorum Queues, whereby the leader, by default, runs on the node the client application that created it was connected to, and followers are created on the rest of the nodes in the cluster. In the past, replication of queues was specified by using policies in conjunction with Classic Queues. Quorum queues are created differently, but should be compatible with all client applications which allow you to provide arguments when declaring a queue. The x-queue-type argument needs to be provided with the value quorum when creating the queue. For example, using the Elixir AMQP client 1 , declaring a Quorum Queue is as follows: Queue.declare(publisher_chan, \"my-quorum-queue\", durable: true, arguments: [ \"x-queue-type\": \"quorum\" ]) An important difference between Classic and Quorum Queues is that Quorum Queues can only be declared durable, otherwise the following error message is raised: :server_initiated_close, 406, \"PRECONDITION_FAILED - invalid property 'non-durable' for queue 'my-quorum-queue' After declaring the queue, we can observe that it is indeed a quorum type on the Management Interface: We can see that a Quorum queue has a Leader, this roughly serves the same purpose as it did for the Classic Queue Master. All communication is routed to the Queue Leader, which means the queue leader locality has an effect on the latency and bandwidth requirement of the messages, however the effect should be lower than it was in Classic Queues. Consuming from a Quorum Queue is done in the same fashion as other types of queues. New for Quorum Queues Quorum queues come with a few special features and restrictions. They can not be non-durable, because the Raft log is always written to disk, so they can never be declared as transient. They also do not support, as of 3.8.2, message TTLs and message priorities 2 . As the use case for Quorum Queues is data safety, they also cannot be declared as exclusive, which would mean they get deleted as soon as the consumer disconnects. Since all messages in Quorum Queues are persistent, the AMQP “delivery-mode” option has no effect on their operation. Single Active Consumer This is not unique to Quorum Queues, but it’s still important to mention, that even though the Exclusive Queue feature was lost, we gain a new feature that is even better in many ways and was a frequently requested enhancement. Single Active Consumer enables you to attach multiple consumers to a queue, while only one of the consumers is active. This lets you create highly available consumers while ensuring that at any moment in time, only one of them receives messages, which until now was not possible to attain with RabbitMQ. An example of how to declare a queue with the Single Active Consumer feature in Elixir: Queue.declare(publisher_chan, \"single-active-queue\", durable: true, arguments: [ \"x-queue-type\": \"quorum\", \"x-single-active-consumer\": true ]) The queue with the Single Active Consumer setting enabled is marked as SAC. In the image above, we can see that two consumers are attached to it (two channels executed Basic.consume on the queue). When publishing to the queue, only one of the consumers will receive the message. When that consumer disconnects the other one should take exclusive ownership of the stream of messages. Basic.get , or inspecting the message on the Management Interface, can not be done with Single Active Consumer queues. Keeping Track of Retries, Poison Messages Keeping a count of how many times a message was rejected is also one of the most requested features for RabbitMQ, which has finally arrived with Quorum Queues. This lets you handle the so-called poison-messages more effectively than before, as previous implementations often suffered from the inability to give up retrying in the case of a stuck message, or had to keep track of how many times a message was delivered in an external database. NOTE: For Quorum Queues, it is best practice to always have some limit on the number of times a message can be rejected. Letting this message reject count grow forever can lead to erroneous queue behaviour due to the Raft implementation. Using Classic Queues, when a message was requeued for any reason, with the redelivered flag being set, what this flag essentially means is, “the message may have been processed already”. This helps you to check if the message is a duplicate or not. The same flag exists, however it was extended with the x-delivery-count header, which keeps track of how often this requeueing has occurred. We can observe this header on the Management Interface: As we can see, the redelivered flag is set and the x-delivery-count header is 2. Now your Application is better equipped to decide when to give up retrying. If that is not good enough, you can now define the rules based on the delivery count to send the message to a different Exchange instead of requeuing. This can be done right from RabbitMQ, your application does not have to know about the retrying. Let me illustrate this with an example! Example: Re-routing rejected messages! Our use case is that we receive messages which we need to process, from an application which, however, may send us messages which cannot be processed. The reason could either be because the messages are malformed, or because the application itself cannot process them for some reason or another, but we do not have a way to notify the Sending Application of these errors. These errors are common when RabbitMQ serves as a message bus in the system and the Sending Application is not under the control of the Receiving Application Team. We then declare a queue for the messages which we could not process: And we also declare a fanout exchange, which we will use as a Dead Letter Exchange: And bind the unprocessable-messages queue to it. We create the application queue called my-app-queue and corresponding policy: We can use either Basic.reject or Basic.nack to reject the message, we must use the requeue property set to true . Here’s a simplified example in Elixir: def get_delivery_count(headers) do case headers do :undefined -> 0 headers -> { _ , _, delivery_cnt } = List.keyfind(headers, \"x-delivery-count\", 0, {:_, :_, 0} ) delivery_cnt end end receive do {:basic_deliver, msg, %{ delivery_tag: tag, headers: headers} = meta } -> delivery_count = get_delivery_count(headers) Logger.info(\"Received message: '#{msg}' delivered: #{delivery_count} times\") case msg do \"reject me\" -> Logger.info(\"Rejected message\") :ok = Basic.reject(consumer_chan, tag) _ -> \\ Logger.info(\"Acked message\") :ok = Basic.ack(consumer_chan, tag) end end First we publish the message, “this is a good message”: 13:10:15.717 [info] Received message: 'this is a good message' delivered: 0 times 13:10:15.717 [info] Acked message Then we publish a message which we reject: 13:10:20.423 [info] Received message: 'reject me' delivered: 0 times 13:10:20.423 [info] Rejected message 13:10:20.447 [info] Received message: 'reject me' delivered: 1 times 13:10:20.447 [info] Rejected message 13:10:20.470 [info] Received message: 'reject me' delivered: 2 times 13:10:20.470 [info] Rejected message And after it was delivered three times it is routed to the unprocessed-messages queue. We can see on the Management Interface that the message is routed to the queue: Controlling the members of the quorum Quorum queues do not automatically change the group of followers / leaders. This means that adding a new node to the cluster will not automatically ensure that the new node is being used for hosting quorum queues. Classic Queues in previous versions handled adding queues on new cluster nodes through the policy interface, however this could pose problematic as cluster sizes were scaled up or down. An important new feature in the 3.8.x series for Quorum Queues and Classic Queues, is the in-built queue master rebalancing operations. Previously this was only attainable using external scripts and plugins. Adding a new member to the quorum can be achieved using the grow command: rabbitmq-queues grow rabbit@$NEW_HOST all Removing a now stale, for example deleted, host from the members can be done through the shrink command: rabbitmq-queues shrink rabbit@$OLD_HOST We can also rebalance the queue masters so that the load is equal on the nodes: rabbitmq-queues rebalance all Which (in bash) will display a nice table with statistics on the number of masters on nodes. On Windows, use the --formatter json flag to get a readable output. Summary RabbitMQ 3.8.x comes with a lot of new features. Quorum Queues are just one of them. They provide a more understandable, in some cases less resource intensive, new implementation for achieving replicated queues and high availability. They are built on Raft and support different features than Classic Queues, which fundamentally, are based on the custom Guaranteed Multicast protocol 3 , (a variant of Paxos,). As this type and class of queues are still quite new, only time will tell if they become the more used and preferred queue type for the majority of distributed installations of RabbitMQ in comparison to their counterparts, the Classic Mirrored Queues. Until then, use both as best fitting for your Rabbit needs! 🙂 Need help with your RabbitMQ? Our world-leading RabbitMQ team offers a variety of options to suit your needs. We have everything from health checks, to support and monitoring, to help you ensure an efficient and reliable RabbitMQ system. Or, if you’d like to have full visibility of your RabbitMQ system from an easily readable dashboard, why not take advantage of our free trial to WombatOAM .", "date": "2020-02-20"},
{"website": "Erlang-Solutions", "title": "Using RabbitMQ for microservice architecture success – Guest Blog by API Fortress API", "author": ["Simone Pezzano"], "link": "https://www.erlang-solutions.com/blog/using-rabbitmq-for-microservice-architecture-success-guest-blog-by-api-fortress-api/", "abstract": "API Fortress is an automated continuous testing platform for APIs that can be deployed in-house or on the cloud. Thier list of clients includes GoPro, Getty Images and MuleSoft (just to name a few). We had the pleasure of speaking to their CTO and co-founder, Simone Pezzano to discuss the journey they underwent to choosing RabbitMQ for their microservices architecture, why they felt it was the right tool for the job and how they’ve used its capabilities to build a succesful platform. The result is great example of how and why to move to RabbitMQ for a long-term harmonious micro-service architecture with improved performance. API Fortress and RabbitMQ – The discovery phase The initial concept of API Fortress was born in 2014. They were working with APIs for various companies, and realized that there was no system to test and monitor the APIs without writing a test suite in code. After searching the market, they realized there was a definite need for a better, more modern approach to testing and monitoring, so they started building our platform. In the early days, as they were gaining customers, they gathered critical insight about the most common testing and monitoring use cases. One of the top use cases involved a transition from legacy monolithic systems to a more contemporary microservices architecture. This trend had dovetailed with the explosion of APIs and API management platforms like Apigee, Mulesoft, Mashery, WSO2, and Kong, etc. In 2016, they decided to undergo that change, and converted their platform into an entirely microservices-based architecture. During that process, they realized that it was very important that the platform leverage a tool capable of seamlessly orchestrating asynchronous messaging across heterogeneous components. This tool would need to indicate the importance of each message, pointing to whether the message represented a notification or an important task to execute. During setup, it would need to help us scale our microservices-based platform as much as needed horizontally for a multi-tenant cloud system but with negligible overhead, which was highly desirable for our situation. Additionally, it would need the tool to be able to deploy with our platform on-premises. Their search for the right tool led them to RabbitMQ. Not only would RabbitMQ let them reach the goals of API Fortress, several unique features of RabbitMQ opened a whole new set of possibilities. In fact, they planned several key architectural improvements based on the new capabilities found in RabbitMQ, including: RabbitMQ’s powerful message routing settings allowed us to better shape the flow of events throughout our platform. By increasing the efficacy of each message, we reduced the complexity caused by message replication, and subsequent remedial actions in case of failure. Standardized AMQP protocol and STOMP support allowed us to create message queues that were not just internal, but also usable by our customers when reacting to certain events with their internally built software in any language. High configurability of Queues and Exchanges using the default client libraries in RabbitMQ allowed us to delegate 100% of the setup operations to the pieces of software that actually use the queues, reducing the uncertainty introduced by custom maintenance sequences. The costs and skill requirements for day-to-day maintenance dropped to virtually zero. RabbitMQ provided all of these features and more with unparalleled stability and a minimal resource footprint. Whether powering our cloud platform with thousands of messages being exchanged, or our Docker on-premises deployment, RabbitMQ is the component helping to drive our platform’s success. The API Fortress Architecture To help better understand how RabbitMQ solved many past problems for API Fortress, but then also opened our architecture to new possibilities, we can take a deeper dive into how RabbitMQ infiltrates into all aspects of the API Fortress platform in the following: While it may seem a little intimidating in its complexity, the key factor to understand is that RabbitMQ literally coordinates the majority of all API Fortress microservices. Without going into too much detail, here are several factors that should be highlighted on that point. Microgateway-to-Storage RabbitMQ gives a quick solution to allow the microgateway to not care how the messages are delivered by simply asking RabbitMQ to take care of the payloads. This approach allows the Storage Microservice to only accept as much load as it can possibly bear, leveraging RabbitMQ’s QoS as it writes the payloads to the database. Scheduler-to-Engines Tests may fail to execute properly when accepted by one of the engines, however, the .ack-.nack system in RabbitMQ makes sure that if it happens, the test can immediately be requeued to execute later. Combined with the the QoS system, we can accurately decide how much load each engine is allowed to accept, making sure the platform does not get congested. To add some extra spice to this, each message from the scheduler can be configured with a given execution priority, which is a very desirable option, especially when thousands of tests are scheduled every five minutes, and some of them are more time-sensitive than others. Finally, as tests get accepted and acknowledged at a somewhat relatively slow rate, RabbitMQ Flow Control can apply back pressure to the Scheduler so that the queue maintains a reasonable size. Engines-to-Downloaders Downloaders are microservices that perform I/O operations on behalf of an engine. While this communication is generally solved with an HTTP call, some downloaders may live on-premises behind a firewall. RabbitMQ comes to the rescue offering us two solutions: An RPC-like functionality that allows an entity (in this case, the engine) to ask for something (to the downloader) and then expect a response A STOMP connector that allows us to expose a dedicated queue as a websocket. The combination of these two features allows us to deploy the downloader on-premises, initiating a stable channel from the inside of the third-party datacenter and bypassing inbound firewall rules. Engines-to-Notification Multiple aspects of a test execution may need to be notified to users in a variety of ways. In our system, the Mailer sends emails while the Connector triggers Web Hooks. Third-party notification systems can also be added in the mix, with the internal notification system completing the picture. RabbitMQ Fan-out exchanges allow multiple notification systems to subscribe to the messages they’re interested in and just receive them as they are made available. Notification-to-Engine Sometimes, notification systems may need certain types of extra information to complete their jobs, e.g., email recipients. Because, by design, they don’t have access to the database, notification systems leverage the same RPC-like system we previously mentioned to ask the Engines for the information they’re missing without the need to discover what engines are available, where they live, and how healthy they are, etc. In Conclusion API Fortress undertook the same journey as many of their customers. Everyday, thousands of customers leverage the API Fortress platform as they transform from a monolithic architecture to microservices. By leveraging RabbitMQ, much like their customers who are working towards event-driven architectures and other patterns, they have established the foundation to grow and expand their capabilities without sacrificing performance. To learn more about the automation of continuous API testing, check out the API Fortress website. Discover more about our work with RabbitMQ.", "date": "2020-03-11"},
{"website": "Erlang-Solutions", "title": "RabbitMQ’s Anatomy – Understanding Topic Exchanges", "author": ["Bartosz Szafran"], "link": "https://www.erlang-solutions.com/blog/rabbits-anatomy-understanding-topic-exchanges/", "abstract": "Topic Exchange Intro/Overview In RabbitMQ (and AMQP in general), an exchange is the abstraction and routing entity to which messages are published to, from external connecting applications. There are four kinds of exchange types, one of which is the TOPIC exchange. Amongst these four exchange types, the TOPIC exchange offers the most flexible routing mechanism. Queues are bound to TOPIC exchanges with the routing key. This is a pattern matching process which is used to make decisions around routing messages. It does this by checking if a message’s Routing Key matches the desired pattern. A Routing Key is made up of words separated by dots, e.g.: floor_1.bedroom.temperature . The Binding Key for Topic Exchanges is similar to a Routing Key, however there are two special characters, namely the asterisk * and hash # . These are wildcards, allowing us to create a binding in a smarter way. An asterisk * matches any single word and a hash # matches zero or more words. Here are examples of patterns to match on messages from: – all devices on first floor in bedroom: floor_1.bedroom.* , – all devices on first floor floor_1.# . It’s clear that using a Topic Exchange allows for much simpler and more specific Routing. Now let’s take a look at how a Topic Exchange actually works. Trie Understanding how a trie data structure is a key to understanding how a Topic Exchange works. The trie data structure is a tree that holds ordered data and is typically used for storing string values. Each node in the tree represents a prefix of a string and holds links to child nodes, which share the same prefix. The child nodes are addressed with the character following the prefix for the node. This data structure allows us to search for specific strings independent of the structure’s size. The characters of a string are used to traverse the tree while searching it. The trie is used for storing Binding Keys in a Topic Exchange. A Binding Key is split by dots and all the string parts are used as pointers to the next node. Every time a new binding is added to a Topic Exchange, the trie associated with it is updated. And each time a new message has to be routed, the trie is queried to look for the message’s destinations. Implementation of the trie A Topic Exchange trie is implemented on top of Mnesia, the built-in distributed database of Erlang. Nodes and edges of a trie are stored in rabbit_topic_trie_node and rabbit_topic_trie_edge tables respectively. #topic_trie_node{\n    trie_node = #trie_node{\n        exchange_name,\n        node_id\n    },\n    edge_count,\n    binding_count\n}\n#topic_trie_edge{\n    trie_edge = #trie_edge{\n        exchange_name,\n        node_id, % parent\n        word\n    },\n    node_id % child\n} In this case, trie_node or trie_edge records are the primary keys used to identify records. Both nodes and edges are assigned to one particular Topic Exchange by specifying an exchange_name field in primary key. Nodes are also used to identify bindings, because they are not stored directly in the nodes table you will need to obtain them using node_id from the rabbit_topic_trie_binding table. Edges store information about the connections between parent and child nodes. Edges also contain the part of the Binding Key (word) , which is used to traverse the tree. Therefore, traversing through the tree requires a sequence of Mnesia queries. Topic Exchange internals An Exchange type is created by implementing the rabbit_exchange behaviour. In the context of tries in a Topic Exchange, the are two interesting operations. Namely, add_binding/3 and route/2 , the first implements the adding of a new binding to the structure and the latter is used to determine target for routing. Binding Operation The arguments needed to create a binding are: – source Exchange – Binding Key – destination Every trie starts with the root node,this represents the empty Binding Key. It makes sense, as an empty string is a prefix for any string. The first operation is pretty straightforward – the Binding Key has to be split by dots . and stored in a list. For example the key “a.b.c” will be transformed to [“a”, “b”, “c”] . Let’s call the list Words for later. It will be used for traversing the data structure. Then, recursively the tree is traversed down, starting with root as a current node. Repeat until the Words list is empty. 1.1. Take the head part of Words list and query Mnesia for child matching it. 1.2. If node is found, use it as a new current node and go to 1.1 with the rest of Words list. Otherwise go to 2. Create child nodes using rest of the Words list. When Words list is exhausted, create a rabbit_topic_trie_binding for the current node. It signals that there are bindings associated with it. Here is an example binding operation. Let’s assume there is a Topic Exchange with two existing bindings: floor_1.bedroom.temperature and floor_1.# . our example trie structure would look like this: Let’s add a new binding with the Binding Key floor_1.bedroom.air_quality . First we split it with dots: [floor_1, bedroom, air_quality] . There are already keys floor_1 and bedroom, but the latter one is missing. Therefore a new node has to be created. Then, the rest of the key [air_quality] is used to create nodes. Finally a new binding is associated with the newly created node and we have a structure that looks like this: As you can see, to insert the new node, three read queries were executed to retrieve the edge between the last pair of nodes: {root, floor1}, {floor_1, bedroom} and {bedroom, air_quality} . However, the latter edge was not present, so 2 write operations were executed: the first updates the edge_count for the bedroom node and the second inserts a new edge. At this point the trie structure is ready to create the final node. Therefore, another two write operations will need to be completed: – One to create the actual node, which corresponds to the given Binding Key, – The second to create the entry in rabbit_topic_trie_binding , which bounds the node with the destination for messages. The Mnesia tables used here are an ordered_set type, which means that it is implemented with a binary search tree. Thus, both read and write operations have complexity O(log(n)), where n is the size of the table. It can be observed, that first phase of traversing through the trie requires: read operation when, a node exists write operation when a node is not existing. The final phase of inserting the actual binding requires two extra operations. In a worst case scenario, where there are no nodes and all of them need to be created, the complexity is O(n*2*log(m) + 2*log(k)) , where n is the length of the Binding Key, m is the number of nodes in the table and k is number of actual bindings. The m and k are global, so the efficiency of queries depends on the global number of bindings/nodes, not just for the given Exchange. For simplicity it is assumed that the number of edges and nodes are equal, because in this structure (number of edges) = (number of nodes – 1). Routing Operation Routing happens when a new message arrives to a Topic Exchange. The trie structure needs to be queried using the Routing Key associated with the message. However, traversing through the trie is not straightforward, as wildcards * and # need to be taken in to account. As with a binding operation, the beginning of the Routing Key is split by dots. Again, let’s call it Words list [ Word | RestW ] = Words . The process starts with a root node. Then the algorithm of discovering the binding is a recursive exploration of the tree in three ways: – Look for # in child nodes. If node is found, it is considered as new root and new scans are started with all remaining parts of Words, e.g: if Words is [a,b,c] , then start searching again with [a,b,c], [b,c], [c] and [] . – Look for * in childs nodes. If node is found, continue with found node as a root and RestW. – Look for Word in child nodes. If node is found, continue with found node as a root and RestW. Exploring will be finished for all cases when all pathways in Words is exhausted. The last step in this process is to look for extra child nodes connected through any of the hash # wildcards. This has to be done because the # wildcard stands for “zero or more”. So here is an example of the new search algorithm. Let the Topic Exchange have the following bindings: floor_1.*.air_quality, floor_1.bedroom.air_quality, floor_1.bathroom.temperature . Now, let’s examine the routing for message published with the Routing Key floor_1.bedroom.air_quality , which will match all the bindings. Here is the trie representation, where the current node is marked as blue and the number on the node represents the number of bindings. The first step is to find out if there is a hash # child node. in the example above it is not present. Then, the asterisk * child node is queried, but it is also not present. Finally, the algorithm will find a node, matching the head of Words list – floor_1: Now, the algorithm will consider the blue node as a new root and start again. Again, there is no hash # child, but an asterisks is found. Then, the head of the Words list is consumed and the algorithm moves down: Here there is only one option available – air_quality: The Words list is exhausted, so the current node is a result. There is one extra step – the hash # child node has to be queried again, because it also accepts empty lists. However, it is not found, so only the current blue node is considered to be a result. Let’s mark the found node with a green and get back to previous node: The node was found using an asterisk, but there is one step left. It has to be checked to see if there is a bedroom child node. And, in this case there is one: There is one word left and the child node is present: The Words list is empty again, so current is result: The final step is to query for any bindings associated with the bindings we found. According to the numbers on the found nodes, there are two bindings. They are the final result of the route/2 function. Now, let’s consider another example, with hash bindings present. There are three bindings present: #.air_quality , floor_1.# and floor_1.bedroom.air_quality.# . Again the floor_1.bedroom.air_quality Routing Key will be used: In this example we have found a hash node. This will cause the algorithm to go down to that node with all available Routing Key parts: Let’s emphasise this again: the current node was reached via # edge, so the algorithm visits the current blue node 4 times with different Words lists. They are presented on the figure. One of the Words list is an empty one [] , so this node is also appended to the results. There is no floor_1 or bedroom edge going out of this node, but there is air_quality one. So, the algorithm goes to the leaf node using the third Words list. Then: The current Words list is empty, so this node is also a result of the search. There are no hash child nodes, so the current branch is finished. The algorithm will go back to the root node: The only option for the algorithm to go down is the head of the Words list: Again there is a hash child node, so it needs to be visited with all tails of the Words list. Three times in this case: One of the Words lists is empty, so the current blue node is appended to the result list. As there are no child nodes, the algorithm goes back: Now the algorithm will go down two times consuming all remaining words. Let’s jump directly to it: The Words list is empty, so the current node is also part of the result. However there is also a hash # child node. According to the algorithm, if any node is considered as a result, its child hash nodes are matched. So finally there are 5 nodes found: The final step is to find bindings associated with found nodes. The final results of route/2 function are all 3 bindings. In term of complexity it is hard to estimate precisely. The algorithm does 3 queries for each node. The # nodes result in duplication of query paths, as it starts the whole algorithm with all remaining parts of Words. However all operations are depending on two factors – the Words list length and the total number of nodes existing in the table. So assuming the worst case, where the bindings are: #, #.#, #.#.# ... k*# , we can see that each level will run with all possible combinations of Words, some of them will be visited many times with exactly the same Words. Then, the first node is visited n times, second is visited sum(1,n) , third sum(1,sum(1,n)) and so on. We can rewrite it as: The total number of operations is k1+k2+…+kk. When this recursive equation is unwrapped, every new level contains two times more multiplications than the previous one. The level k will contain 2k multiplications. It will be dominant in terms of complexity, so we can bound the complexity by O(2k*n*log(m)), where k is maximum trie depth, n is the length of the Words and m is the total number of nodes. However, the above example is extreme and bindings like #.#.# make no sense. Then, the average complexity would be close to O(nlog(m)), because it makes no sense to put two subsequent hash # parts of the key. The overhead introduced by a single hash node should not be significant, because in such case the traversing trie with different Words stops after the hash edge. Evaluation This section will cover the performance of a Topic Exchange. The experiments will demonstrate the characteristics of a Topic Exchange under different conditions. Synthetic tests Two experiments will be presented, in order to experimentally confirm or reject the assumptions made in the previous Routing Operation section: First, the relation between the number of bindings and routing operation time. The bindings are fixed and the routing key length is adjusted. The linear dependency is expected. Secondly, the routing key length is fixed and the number of bindings is varying. Both experiments are performed under following conditions: – Tests are made on single RabbitMQ node. – Routing time is measured by checking time of evaluation rabbit_exchange_type_topic:route/2 . – Measurements are made 50 times and average results are presented on the figures. – The Bindings Keys are random, ensuring that there are no two subsequent hashes in any Binding Key. – The Binding Key part has 70% chance to be a word, 20% chance to be an asterisk and 10% to be a hash. – The Routing Keys are created from existing Binding Keys – For example the Routing Key with length n , will be created from existing Binding Key with the length n . Any hashes or asterisks, are replaced by random strings. It ensures that operation must traverse through at least n levels of trie. The above figure presents the results of the three experiments. Now, let’s slightly modify the conditions to visualize the impact of a # hash key in Trie structure. There is only one binding added, which is just two subsequent hashes #.# . Then, the performance will look like this: The red curve bends, as we expected. When there are more # bindings on the query path, the relation between the Routing Key length and query time is no longer linear. This effect can also be observed in 10k bindings series – the green curve also bends slightly. This can be explained in the same way – there are more Bindings Keys starting with a #, this increases query time for all queries. Let’s check it in the RabbitMQ Management UI: Here we have roughly 50 bindings like the ones above, if we replace them we will see a more linear relation and get a better overview of the impact of the performance from our hash # as seen below: Again, the time to find relevant routes has improved. Now, let’s examine the way the number of bindings impacts query time. As we explained in the previous section, a logarithmic relation is expected: This example also follows the expected behaviour. All the bindings are stored in a single Mnesia table. Querying any node has its own complexity. Where there are more entries in the table, the query time grows. As the table has an ordered_set type, the query time has logarithmic complexity, what is actually observed. Summing up, the previous experiments align to the theory we started with. The expectations about the impact of Route Key length and number of bindings to the routing operation time was confirmed. The huge impact of hash a # wildcard has also been confirmed and the scale of it was presented. Real world example The two previous examples measured the time of a single query. While this is still valuable, it does not necessarily reflect a real world use case. The test is synthetic and focuses on a single query, but is a Topic Exchange performance overhead also observable when the overall performance of RabbitMQ is taken into account? This section will present a performance evaluation of RabbitMQ integration to MongooseIM . MongooseIM is Erlang Solutions’ highly scalable, instant messaging server. The RabbitMQ component in MongooseIM simply reports each users’ activity, which may be: – User became online/offline – User sent/received message Only sent/received messages will be discussed in this case. The Routing Key of the message activity follows simple pattern <username>.{sent, received} . In order to evaluate the performance of the component, there was a load test designed. There were simulated XMPP clients connecting to the MongooseIM server. The simulated clients were exchanging messages with each other. Each message generated events, which were published to RabbitMQ. Then, we had a number of AMQP clients connecting to RabbitMQ, to consume generated events. This is the outline of the experiment’s architecture: For the purpose of this post, only results which were directly connected to a Topic performance were covered. Let’s define the key performance indicator as the Time To Delivery, the amount of time between a message being sent by a XMPP user and being received by Consumer of RabbitMQ’s queue. This value will be presented in the figures to follow. Tests conditions were as follows: – 90k XMPP users – 10k AMQP consumers – ~ 4,8k payloads/s from MongooseIM to RabbitMQ – Payload size ~120B – topic exchange with Binding Keys like user_N.* – 20k bindings in total In this case, the performance is presented in the following graph. It shows the 95th – 99th percentiles of the Time To Delivery, as well as the maximum observed Time To Delivery in a given time window. The latter test had similar condition. The only difference was different Exchange type: – 90k XMPP users – 10k AMQP consumers – ~ 4,8k payloads/s from MongooseIM to RabbitMQ – Payload size ~120B – direct exchange with Binding Keys like user_N.chat_msg_sent, user_N.chat_msg_recv – 40k bindings in total Under those condition performance was better, which is illustrated on following figure. While the previous section showed the performance characteristics of a Topic Exchange, those examples provide an overview on a bigger scale. Both tests had identical characteristics apart from the exchange type being direct or topic (and consequently number of bindings). However, the difference in performance is significant in favor of the Direct Exchange. It allowed us to effectively decrease the Time To Delivery, which is a factor of efficiency in the case of the presented tests. Summary Now you’ve seen the basics of the Topic Exchange internals, a brief overview of its implementation, a theoretical overhead introduced by traversing the trie structure, as well as some performance evaluation. As it was observed, a Topic Exchange is not the fastest method and there are many factors which may influence its performance. However it is not true that Topic Exchanges are slow. In fact, they are generally fast in a typical RabbitMQ usage. These test conditions were specific. If there are a few bindings or the trie depth is not deep, the Topic Exchange overhead is usually negligible. Still, it is important to understand the underlying mechanism, as the example with MongooseIM’s RabbitMQ component presented – using different Exchange types resulted in a significant improvement in performance. Learn more about our work with RabbitMQ.", "date": "2020-03-20"},
{"website": "Erlang-Solutions", "title": "How data drives MongooseIM.", "author": ["Jan Cieśla"], "link": "https://www.erlang-solutions.com/blog/how-data-drives-mongooseim/", "abstract": "To make decisions about how to steer your open source project, you need to know whether you’re on the right path or if you need to course-correct. That’s why you need to equip your project with a proper compass and barometer to help you navigate. Gathering the metrics of your project can be one of the tools that can help you gain more insight into how your project is being used. When used wisely, data can help open source maintainers understand how users are responding to new functionality. This allows you to prioritise the work you do and understand how the features are being used. You can identify less used functionalities that might not need as much support. Most importantly, data can turn suspicions and opinions into facts which help to improve your project, leading to more satisfied users. In this blog post, we are taking a deep dive into the new MongooseIM feature that was introduced in version 3.6. Now, system metrics are gathered to analyse the trends and needs of our users, improve MongooseIM, and let us know where to focus our efforts. This blog post is devoted to explaining what the new metrics are, what they help us achieve, why we’ve done it, and how you can manage and customise them at your end. Why do we do it? Knowing how our product is used is critical for us to identify the core value it brings to the users. It points us in the direction in which to expand it and show us how to target our further efforts in developing it. The collected data only has statistical relevance and is automatically anonymised before it is processed any further. Each MongooseIM randomly generates a Cluster ID that is attached to the reports. A sample report showing mod_vcard backends usage from our CI builds can be found below. Such reports can show us how we approach testing different configuration scenarios. This can be contrasted with real-world metrics that are gathered. Based on these reports, we can see the frequency of different backends being used (or not used) with mod_vcard. These comparisons can tell us that the LDAP backend was not widely used for the past month; no user installation reported this configuration. It is important to note that such reporting is not yet painting a full picture of the MongooseIM ecosystem. The metrics feature has just been introduced and is mostly showing fresh installations/upgrades. It might take some time to draw decisive conclusions from long-running deployments. Where can you see the information gathered? You can view all the information that is shared in two different ways. The log file system_metrics_report.json contains the most recent report that was sent. Additionally, you can configure the Tracking ID to use your own Google Analytics account and have a view of your MongooseIM status in that dashboard. How can you configure this service? To ensure full transparency, you will notice a log message that is generated on every MongooseIM node start (unless the metrics service is configured with the report option) to show that the functionality is enabled. We wanted to notify you that the metrics are gathered, and you have the right to withdraw consent at any time without limiting the functionality of the product. This feature is provided as a “service”. To be operational, it needs to be added to the list of services as shown below: Example configuration {service_mongoose_system_metrics, [\n                                   report,\n                                   {intial_report, 300000},\n                                   {periodic_report, 108000000}\n                                  ]\n} The metrics are first reported shortly after the system startup and later at regular intervals. These timers are configurable using the initial_report and periodic_report parameters. The default values are 5 minutes for the initial report and 3 hours for the periodic one. These reporting intervals can be changed depending on the configuration parameters. Removing the service_mongoose_system_metrics entry from the list of services will result in the service not being started. Metrics will not be collected and shared. It will generate a notification that the feature is not being used. The notification can be silenced by setting the no_report option explicitly. For more details regarding service configuration, please see Services section in our documentation. What information are we gathering? When introducing this feature, it is crucial for us to be fully transparent as to what information is being gathered. In general, we capture data on how MongooseIM is being used, its version and the chosen feature set. We only report the names of known modules and APIs that are part of the open source product. All additional customisations are simply counted without disclosing any specific details. The full list of information that is being gathered is listed below: MongooseIM node uptime. MongooseIM version. The number of nodes that are part of the MongooseIM cluster. Generic modules that are part of the open source project and are in use. Some modules report what database they use as a backend. Number of custom modules – without disclosing any details, we are just curious to see if there are any. Number of connected external XMPP components. List of configured REST APIs that are part of the open source project. XMPP transport mechanisms like, TCP/TLS, WebSockets or BOSH. Geographical Data – Google Analytics is providing several geographical dimensions, such as City, Country, Continent. These values are derived from the IP address the data was sent from. You can learn more about Googles Geographical Data here for more details. How do I configure additional and private Tracking ID’s in Google Analytics? The data is gathered and forwarded to Google Analytics. The user can add custom Google Analytics Tracking ID in the MongooseIM configuration and see all incoming events that are related to their own system metrics. For more details on how to create or sign in to the Google Analytics account, please see Get Started with Analytics. The Tracking ID is a property identification code that all collected data is associated with. It determines the destination where the collected data is sent. To create a new Tracking ID, please follow the steps below: Go to the Admin tab of your user dashboard. Create a new account with + Create Account. Add new property with + Create Property. Within the new property go to Tracking Info > Tracking Code. The Tracking ID can be found in the top left corner of the section and has the following format UA-XXXX-Y. Example configuration A new Tracking ID can be added to the list of options as follows: {service_mongoose_system_metrics, [\n                                   report,\n                                   {intial_report, 300000},\n                                   {periodic_report, 108000000},\n                                   {tracking_id, UA-XXXX-Y}\n                                  ]\n}", "date": "2020-04-09"},
{"website": "Erlang-Solutions", "title": "Why Elixir is the Programming Language You Should Learn in 2020", "author": ["Manuel Rubio"], "link": "https://www.erlang-solutions.com/blog/why-elixir-is-the-programming-language-you-should-learn-in-2020/", "abstract": "What should you look for when choosing to learn a new programming language? The answer might vary depending on what your project or career goals are, but as a basic starting point, you want a language that: Is fun and easy to use Has the ability to meet modern user demands Has rewarding career progression Has an active and supportive community Has a range of helpful tooling Has frameworks to allow full-stack development Has easily accessible documentation Ensures you grow as a programmer. Over the course of the article, we’ll show you how Elixir rates in relation to all of the above dot points. Before we start, it’s worth letting you know a little bit about me. I like to consider myself a polyglot developer. Over the years I have worked with Elm, Lua, Rust, Dart, Go, Kotlin, Scala, C, C++, Perl, Ruby, PHP, Python, Java, JavaScript, Erlang and Elixir. If I were to only pick one language to learn as a brand new developer in 2020, it would be Elixir. Why Elixir is fun and easy to use Fun might not be the only important consideration, but it is one that should not be underestimated. Elixir’s syntax shares a lot of similarities with Ruby; both are high-level, readable, fun programming languages. As a senior developer from Pinterest once said of his decision to transition to Elixir: “”I thought, ‘Wow, it’s as fun as Ruby, but it has some chops — it actually performs really well.’” Personally, I love the ability to write less code. It is a language that is expressive enough to be understandable and fast to read. For example, using the Tesla library, I could implement integration with a provider like Paypal in 20 lines of code. I will be happy to schedule a hands-on workshop or a 1-on-1 call if there is a need for dive into this library. How Elixir has the ability to meet modern usage demands We all heard of the digital transformation, right? As populations grow, and more services move online there is increased pressure on systems to be able to handle huge spikes with billions of concurrent users. Elixir runs on the BEAM VM. A system built for concurrency. As a result, it can handle these user spikes with ease. For those interested in what concurrency is, and how it works, we have a blog comparing the JVM and BEAM VM, which explains it well. Pinterest and Bleacher Report are just two of widely popular and big companies who moved to Elixir in order to help solve issues of scalability. In the case of Bleacher Report, one of our clients, they were able to move from 150 servers, down to 5 servers. Not only did they reduce there physical infrastructure requirements, they were also able to handle more traffic with faster times. A language that can be relied upon for scalable, fault-tolerant will always be sought out by high profile companies. Elixir’s rewarding career progression Elixir is a growing language which is being adopted at a significant rate. As a result, Elixir developers have the opportunity to work for wide spectrum of companies such as PepsiCo, Pinterest, Lonely Planet or MBTA. And the list is continuing to grow. Check out this list for companies added in the last 12 months alone. The supportive and active Elixir community As you might expect with any new technology or programming language, the Elixir community started out small. Nowadays Elixir has a thriving community that continues to grow, with fantastic events such as ElixirConf in Europe and in the US, EMPEX, Code Elixir LDN and Gig City Elixir, as well as Meetups all around the world. This community means the language is continuing to grow and evolve with new tooling, and that there is always someone to provide inspiration or help find solutions to a problem. Elixir’s range of useful tooling Tooling makes languages more versatile and tasks easier. They mean that you don’t have to reinvent the wheel with every new use case. Elixir already has a number of powerful tools, including Phoenix LiveView which enables developers to build real-time, front-end web applications without writing a line of JavaScript or Crawly which makes web crawling and Data Scraping in Elixir easy. Not only does Elixir have a wide-variety of tooling available, but as we mentioned earlier, there is also a passionate community of people continually creating new tools and improving the ones available. Elixir frameworks allow for full-stack development Given its scalability performance and its origins in Erlang, it is no surprise that Elixir is a popular backend choice. As mentioned above, Phoenix LiveView has provided an easy, time-efficient and elegant way for Elixir developers to produce front-end applications. Finally, the Nerves framework allows for embedded software development on the hardware end. As a result, Elixir is a language that can be adopted right throughout the tech stack. This doesn’t just make it an attractive choice for businesses; it also opens up the door for where the language can take you as a developer. Elixir’s easily accessible documentation A language and community where documentation is valued is one that makes knowledge sharing easy. The high standard of documentation in the Elixir community makes it easier for you to learn the language, but also, make it easier for anyone to contribute to its improvement and growth. Learning Elixir can make you a better programmer in other languages There are a number of stories , from people who come from object-oriented languages, who found that the process of learning Elixir made them a better programmer in their language of choice. Learning a new purely functional programming paradigm forces you to examine the way you approach programming. In doing so, it can help you identify habits that can be improved and introduce you to new ways of thinking about problems. This fresh perspective is invaluable for all future development you do, regardless of what language you are working with. For our friends using Ruby, this is particularly true, as the Elixir syntax will be familiar to you, allowing you an easy transition into functional, concurrent programming. Everyone will have different motivation and considerations for what programming language to choose. But these are the reasons I would recommend you learn Elixir in 2020, for the years ahead. Ready to get started in Elixir? There are many ways to start. First, you should head to the getting started page of the official Elixir website . We also offer a number of free downloadable packages for Elixir. To get started in the community ElixirForum is a great place to start, as well as searching the #Elixirlang and #MyElixirStatus hashtags on Twitter. Find out more about what we do with the Elixir language.", "date": "2020-05-3rd"},
{"website": "Erlang-Solutions", "title": "Optimizing GraphQL with Dataloader", "author": ["László Hegedüs"], "link": "https://www.erlang-solutions.com/blog/optimizing-graphql-with-dataloader/", "abstract": "Throughout my professional career, I’ve had the chance to work with a few programming languages from different paradigms. Some of them were a joy to use, others, were bloated with features that could make even the simplest code unreadable. A little over two years ago I decided to give Erlang a try. Soon after, I started working at Erlang Solutions, and after a couple of months of intensive coding in Erlang I was introduced to Elixir. I’ve been working as an Elixir developer since then. Over the last fourteen months I have been working with a client on mobility applications. We developed several parts of the backend of a corporate car sharing platform and built an application from scratch to conduct and analyze test drives at dealerships around the World. Development of the former had already started before we got there, so the tech stack was pretty much decided. We developed a handful of services exposing GraphQL APIs. For a number of reasons Elixir was the right fit for this purpose. Using the Absinthe library to craft the APIs was another good choice by their team. However, multiple services had to communicate with each other and making a responsive UI requires optimized queries whenever possible. We learned a lot from the experience which we will share for you today in this blog. When we started working on the second application, we had more flexibility in choosing the tools. We decided to stick with a GraphQL API and used Absinthe, but paid attention to how we write the resolver functions. In the end, our API was blazing fast, because most of our resolvers ran only a handful of queries. Designing a usable API is art, as is optimizing the backend that serves that API. In this blog post – and hopefully some upcoming ones – I’ll give you a few hints on what to avoid or what to strive for when working with Absinthe. We cannot always affect how other applications/services work, but we can do our best to make our service as fast as possible. There are plenty of resources out there to help you get started with Absinthe, and the number of tutorials on how to use Dataloader are growing, but it is a recurring topic on Slack. I hope to give you an insight into how it works and how you can make use of it in your project. What is Dataloader in a nutshell? In short, Dataloader is a tool that can help your application fetch data in an optimal way by implementing batching and caching. Dataloader in Elixir In the Elixir world, it is provided by a library and integrates well with Ecto and Absinthe. Note that it’s not a silver bullet, your queries using Dataloader rely on the optimal implementation of the underlying library for the specific data source. We are not going into details on how to use Dataloader on its own. Please consult the official documentation for that. Instead, let’s dive into using Dataloader in Absinthe. Using Dataloader in Absinthe Resolvers For these examples, we are going to use rather simple schemas to avoid getting lost in the details. Assume we are building a database where we keep track of companies and their employees. One employee belongs to exactly one company, but each company may have multiple employees. The Ecto schemas may be defined as: schema \"employees\" do field(:name, :string)\n  field(:email, :string)\n  belongs_to(:company, Company) end schema \"companies\" do field(:name, :string)\n  has_many(:employees, Employee) end And the corresponding object definitions in Absinthe GraphQL may look like: object(:employee) do field(:id, non_null(:string))\n  field(:name, non_null(:string))\n  field(:email, non_null(:string)) end object(:company) do field(:id, non_null(:string))\n  field(:name, non_null(:string)) end This is all good until we want to resolve the employees in the company. A naïve field definition of it may be: object(:company) do field(:id, non_null(:string))\n  field(:name, non_null(:string))\n\n  field :employees, non_null(list_of(:employee)) do resolve( fn company, _args, _info -> employees = Ecto . assoc(company, :employees) |> Repo . all()\n\n      {:ok, employees} end ) end end Similarly, we can add the field :company to the employee object as: field(:company, non_null(:company)) do resolve( fn employee, _args, _info -> company = Ecto . assoc(employee, :company) |> Repo . one()\n\n    {:ok, company} end ) end If we now query a company through GraphQL and also ask for the employees on that field, then our backend will perform two SQL queries. This is not bad at all, but imagine a case where we have ten employee results. Moreover, each result asks for its own company field, which causes several duplicate queries to Ecto. query($id: ID!) {\n  company(id: $id) {\n    id\n    name\n    employees {\n      id\n      name\n      company {\n        id\n        name\n      }\n    }\n  }\n} This may not happen in this exact form, but it helps us imagine what happens when the same associated object has to be resolved for a list of results. We have to make several queries to Ecto for this to be answered. One query to resolve the company in the root, one query for each employee and one additional query for resolving the company of each employee. Twenty-one queries overall; we’re facing the infamous n+1 problem. This is where Dataloader comes to the rescue. Adding Dataloader The documentation of Absinthe is a good starting point for using Dataloader with an Ecto data source. In short, if we want to use Dataloader in our resolvers we have to do two things: Add a dataloader struct to the resolution context Add Absinthe.Middleware.Dataloader to the list of plugins in our schema We can create a dataloader struct with Dataloader.new/1 . After that, we’ll have to add sources to dataloader with Dataloader.add_source/3 . If we have an Ecto repository (let’s call it Repo ), we can add it as follows: def context(ctx) do loader = Dataloader . new() |> Dataloader . add_source(Repo, Dataloader . Ecto . new(Repo))\n\n  Map . put(ctx, :loader, loader) end Let’s not forget to add the Dataloader plugin: def plugins() do [Absinthe . Middleware . Dataloader | Absinthe . Plugin . defaults()] end Absinthe provides a convenient way of using Dataloader in our resolvers. We just have to import Absinthe.Resolution.Helpers in our schema and we can use the dataloader/1 function to resolve fields using the Repo datasource. Our Absinthe object definitions become: object(:employee) do field(:id, non_null(:string))\n  field(:name, non_null(:string))\n  field(:email, non_null(:string))\n\n  field(:company, non_null(:company)) do resolve(dataloader(Repo)) end end object(:company) do field(:id, non_null(:string))\n  field(:name, non_null(:string))\n\n  field(:employees, non_null(list_of(:employee))) do resolve(dataloader(Repo)) end end With these modifications, resolving the graphql query above requires only three Ecto queries. That is a significant improvement. The query function One of the useful features of the Dataloader.Ecto source is that we can pass a query function to it which can be used for filtering or processing parameters that are common to many fields, for example, pagination arguments. def context(ctx) do loader = Dataloader . new() |> Dataloader . add_source(Repo, Dataloader . Ecto . new(Repo, query: & Repo . dataloader_query / 2))\n\n  Map . put(ctx, :loader, loader) end Where we can define Repo.dataloader_query/2 to process parameters related to pagination and also leave room for extending it easily. def dataloader_query(queryable, params) do queryable |> paginate(params[:paginate]) end def paginate(query, nil), do : query def paginate(query, params) do from d in query,\n    limit: ^ params[:limit],\n    offset: ^ params[:offset] end Note that so far we haven’t needed to write any Ecto queries, because we used the dataloader/1 helper from Absinthe.Resolution.Helpers . Unfortunately, we’re not done yet. If you assume we added the paginate parameter to the employees field on the company object. object(:company) do field(:id, non_null(:string))\n  field(:name, non_null(:string))\n\n  field(:employees, non_null(list_of(:employee))) do arg(:paginate, :pagination_input)\n\n    resolve(dataloader(Repo)) end end This will work, if we only have one company, but as soon as we make two company queries with the same pagination parameter (for example, {\"paginate\": {\"limit\": 10, \"offset\": 0}} ) for the employees in one single GraphQL query, we’ll see an anomaly. query($id1: ID!, $id2: ID!, $paginate: PaginationInput) {\n  res1: company(id: $id1) {\n    id\n    name\n    employees(paginate: $paginate) {\n      id\n      name\n    }\n  }\n  res2: company(id: $id2) {\n    id\n    name\n    employees(paginate: $paginate) {\n      id\n      name\n    }\n  }\n} We only see employees for one of the companies, while the list of employees for the other one is empty. This happens because the Ecto Dataloader tries to fetch the employees for both companies with a single query that includes an order_by on the company_id field. This works well when we don’t want to add any limit or offset parameters. One workaround for this is to modify the argument list, which will force the loader to make separate queries for each company. field(:employees, non_null(list_of(:employee))) do arg(:paginate, :pagination_input)\n\n  resolve(\n    dataloader(Repo, fn company, args, _info -> {:employees, Map . put(args, :company_id, company . id)} end )\n  ) end Here we made use of a dataloader key function that should return a resource ( :employees in this case) and a list of arguments that are passed on to our Repo.dataloader_query/2 function. Since the :company_id is different for the two companies, the keys for the dataloader cache will be different. One limitation of this solution is that Dataloader will have to make separate Ecto queries for each company. If this causes performance issues, then regular resolvers or rather batch resolvers may be implemented with optimal queries. The query function is also useful if we want to have more flexibility, for example, filtering or ordering results. For this, we’ll have to extend our query function to process extra parameters. In general, I like to write query helpers that take a queryable object as their first parameter and return a queryable. For example (assuming we have a status field on the employee): def where_active(employee) do from e in employee,\n    where: e . status == \"active\" end If we extend the query function ( Repo.dataloader_query/2 ) as below, we will be able to use these helpers easily: def dataloader_query(queryable, params) do queryable |> paginate(params[:paginate]) |> apply_query(params[:query], params[:query_args] || []) end def apply_query(queryable, nil, _query_args), do : queryable def apply_query(queryable, query, query_args) do apply(query, [queryable | query_args]) end And we can now resolve only active employees on companies if we specify query in the resolver: field(:employees, non_null(list_of(:employee))) do arg(:paginate, :pagination_input)\n\n  resolve(\n    dataloader(Repo, fn company, args, _info -> args = args |> Map . put(:company_id, company . id) |> Map . put(:query, & Blog . Dataloader . Employee . Query . where_active / 1)\n\n      {:employees, args} end )\n  ) end Of course, other filtering is also possible, and we can also construct our queries based on the GraphQL parameters that the resolver receives in args . Note that now :query becomes part of the key that is used in the dataloader cache, so querying the active and non-active employees of the same company in one GraphQL query might require two database queries. Dataloader.KV So far, we have only seen examples of how to use Dataloader.Ecto . But what if we need to collect data from another service to respond to the GraphQL query? We can use Dataloader.KV to retrieve and cache data based on keys. For this, we will need a function that receives two parameters. The first parameter is a batch key that groups together different objects. We will return to this shortly. The second parameter is a list, usually a list of objects or IDs from which the required data is to be retrieved. The function should return a map where the keys are the elements of this list, and the associated values are the corresponding retrieved data. For example, if we store the addresses of employees in a different service, we may write a loader function as follows: def fetch_addresses(_batch_key, employees) do ids = Enum . map(employees, & &1 . id)\n\n  results = call_to_another_service(ids)\n\n  employees |> Map . new( & {&1, lookup_result_for_employee(&1, results)}) end This function receives a list of employees and returns a map with each employee mapped to an address. How the call to another service and the lookup are completed are simply implementation details, but for the solution to be optimal, the other service should support querying data in batches (for all IDs at once instead of one-by-one). To use this function as a load function for a Dataloader.KV source we may change the context function in the schema as follows. def context(ctx) do loader = Dataloader . new() |> Dataloader . add_source(Repo, Dataloader . Ecto . new(Repo, query: & Repo . dataloader_query / 2)) |> Dataloader . add_source(:address, Dataloader . KV . new( & Address . fetch_addresses / 2))\n\n  Map . put(ctx, :loader, loader) end Then we can resolve the address field for each employee using the :address dataloader source: object(:employee) do # ... existing fields here field(:address, non_null(:string)) do resolve(dataloader(:address)) end end As you can see, we did not use the batch key in our loader, which means we will handle all employees in one batch. This is usually fine. The batch key can be useful if we intend to pass on certain arguments to the other service or refine the results. Perhaps we have a user token that we intend to supply for the service to check whether the user has the necessary access rights: object(:employee) do # ... existing fields here field(:address, non_null(:string)) do resolve(\n      dataloader(:address, fn _employee, _args, %{context: %{user_token: token}} -> {:address, %{user_token: token}} end )\n    ) end end Then make the load function handle the additional arguments: def fetch_addresses({:address, %{user_token: _token} = args}, employees) do ids = Enum . map(employees, & &1 . id)\n  results = call_to_another_service(ids, args)\n\n  employees |> Map . new( & {&1, lookup_result_for_employee(&1, results)}) end Note that for each different batch key we have to make a call to the other service, so we have to be careful when specifying the arguments. For example, if we pass in a unique ID (e.g., employee.id ), then we lose the advantage of batching, the function is called for each employee. In general, constructing the batch key provides flexibility, but it can also hide important details in the code. Use with caution. More control over dataloader In some cases, we may want to have more control over how we want to handle loading and post-processing data. In Absinthe.Resolution.Helpers there’s an on_load/2 function that takes a dataloader struct and a callback. It is useful when we have to obtain information that relies on data that can be retrieved by dataloader. The following example is not likely to appear in a real-world scenario, but it demonstrates how we can make use of the on_load function : object(:company) do # ... other fields here field(:number_of_distinct_addresses_of_employees, non_null(:integer)) do resolve( fn company, _args, %{context: %{loader: loader}} -> loader |> Dataloader . load(Repo, :employees, company) |> on_load( fn loader_with_employees -> employees = Dataloader . get(loader_with_employees, Repo, :employees, company)\n\n        loader_with_employees |> Dataloader . load_many(:address, %{}, employees) |> on_load( fn loader_with_addresses -> addresses = Dataloader . get_many(loader_with_addresses, :address, %{}, employees)\n\n          {:ok, length(Enum . uniq(addresses))} end ) end ) end ) end end Notice how we took advantage of the fact that we can embed on_load calls to optimize fetching the results. First, we tell dataloader to load the employees, then we use those employees to load their addresses. Finally, we fetch the addresses and count how many unique ones there are. In general, this kind of resolver is useful when we want to move data one (or more) level up the tree with or without aggregation. In one project, I used the same solution to retrieve telematics data of vehicles to be aggregated and displayed on certain trips taken with those vehicles. Both the vehicles and telematics data needed to be queried from other services. Conclusion Dataloader is a powerful tool when it comes to optimizing queries, but we have to be aware of its limitations. We saw a few simple examples to get up and running with Dataloader and Absinthe. This is only the tip of the iceberg, and I am hoping to follow up with some more advanced tricks and tips. Find out more about our Erlang and Elixir consultancy.", "date": "2020-05-11"},
{"website": "Erlang-Solutions", "title": "Optimising for Concurrency: Comparing and contrasting the BEAM and JVM virtual machines", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/optimising-for-concurrency-comparing-and-contrasting-the-beam-and-jvm-virtual-machines/", "abstract": "The success of any programming language in the Erlang ecosystem can be apportioned into three tightly coupled components. They are: 1) the semantics of the Erlang programming language , on top of which other languages are implemented 2) the OTP libraries and middleware used to architect scalable and resilient concurrent systems and 3) the BEAM Virtual Machine tightly coupled to the language semantics and OTP. Take any one of these components on their own, and you have a runner up. But, put the three together, and you have the uncontested winner for scalable, resilient soft-real time systems. To quote Joe Armstrong, “You can copy the Erlang libraries, but if it does not run on BEAM, you can’t emulate the semantics”. This gets enforced by Robert Virding’s First Rule of Programming, which states that “Any sufficiently complicated concurrent program in another language contains an ad hoc informally-specified bug-ridden slow implementation of half of Erlang.” In this post, we want to explore the BEAM VM internals. We will compare and contrast them with the JVM where applicable, highlighting why you should pay attention to them and care. For too long, this component has been treated as a black box and taken for granted, without understanding the reasons or implications. It is time to change that! Highlights of the BEAM Erlang and the BEAM VM were invented to be the right tool to solve a specific problem. They were developed by Ericsson to help implement telecom infrastructure handling both mobile and fixed networks. This infrastructure is highly concurrent and scalable in nature. It has to display soft real-time properties and may never fail. We don’t want our Hangouts calls on our mobile with our grandmothers dropped or our online gaming experience of Fortnite to be affected by system upgrades, high user load or software, hardware and network outages. The BEAM VM is optimised to solve many of these challenges by providing fine-tuned features which work on top of a predictable concurrent programming model. Its secret sauce are light-weight processes which don’t share memory, managed by schedulers which can manage millions of them across multiple cores. It uses a garbage collector which runs on a per-process basis, highly optimized to reduce any impact on other processes. As a result, the garbage collectors do not impact the overall soft real time properties of the system. The BEAM is also the only widely used VM used at scale with a built-in distribution model which allows a program to run on multiple machines transparently. Highlights of the JVM The Java Virtual Machine (JVM) was developed by Sun Microsystem with the intent to provide a platform for ‘write once’ code that runs everywhere. They created an object oriented language similar to C++, but memory-safe because its runtime error detection checks array bounds and pointer dereferences. The JVM ecosystem became extremely popular with the Internet-era, making it the de-facto standard for enterprise server applications. The wide range of applicability was enabled by a virtual machine that caters for a wide range use cases, and an impressive set of libraries catering for enterprise development. The JVM was designed with efficiency in mind. Most of its concepts are an abstraction of features found in popular operating systems such as the threading model which maps to operating system threads. The JVM is highly customisable, including the garbage collector (GC) and class loaders. Some state-of-the-art GC implementations provide highly tunable features catering for a programming model based on shared memory. The JVM allows you to change the code while the program is running. And, a JIT compiler allows byte code to be compiled to the native machine code with an intent to speed up parts of the application. Concurrency in the Java world is mostly concerned with running applications in parallel threads, ensuring they are fast. Programming with concurrency primitives is a difficult task because of the challenges created by its shared memory model. To overcome these difficulties, there are attempts to simplify and unify the concurrent programming models, with the most successful attempt being the Akka framework . Concurrency and Parallelism We talk about parallel code execution if parts of the code are run at the same time on multiple cores, processors or computers, while concurrent programming refers to handling events arriving to the system independently. Concurrent execution can be simulated on single threaded hardware, while parallel execution cannot. Although this distinction may seem pedantic, the difference results in some very different problems to solve. Think of many cooks making a plate of Carbonara pasta. In the parallel approach, the tasks are split across the number of cooks available, and a single portion would be completed as quickly as it took these cooks to complete their specific tasks. In a concurrent world, you would get a portion for every cook, where each cook does all of the tasks. You use parallelism for speed and concurrency for scale. Parallel execution tries to solve an optimal decomposition of the problem to parts that are independent of each other. Boil the water, get the pasta, mix the egg, fry the guanciale ham, grate the pecorino cheese 1 . The shared data (or in our example, the serving dish) is handled by locks, mutexes and various other techniques to guarantee correctness. Another way to look at this is that the data (or ingredients) are present, and we want to utilise as many parallel CPU resources as possible to finish the job as quickly as possible. Concurrent programming, on the other hand, deals with many events that arrive at the system at different times and tries to process all of them within a reasonable time. On multi-core or distributed architectures, some of the execution is run parallel, but this is not a requirement. Another way to look at it is that the same cook boils the water, gets the pasta, mixes the eggs and so on, following a sequential algorithm which is always the same. What changes across processes (or cooks) is the data (or ingredients) to work on, which exist in multiple instances. The JVM is built for parallelism, the BEAM for concurrency. They are two intrinsically different problems, requiring different solutions. The BEAM and Concurrency The BEAM provides light-weight processes to give context to the running code. These processes, also called actors, don’t share memory, but communicate through message passing, copying data from one process to another. Message passing is a feature that the virtual machine implements through mailboxes owned by individual processes. The message passing is a non-blocking operation, which means that sending a message to another process is almost instantaneous and the execution of the sender is not blocked. The messages sent are in the form of immutable data, copied from the stack of the sending process to the mailbox of the receiving one. This is achieved without the need for locks and mutexes among the processes, only a lock on the mailbox in case multiple processes send a message to the same recipient in parallel. The immutable data and the message passing enable the programmer to write processes which work independently of each other and focus on the functionality instead of the low-level management of the memory and scheduling of tasks. It turns out that this simple design not only works on a single thread, but also on multiple threads on a local machine running in the same VM and, using the built in distribution, across the network with a cluster of VMs and machines. If the messages are immutable between processes, they can be sent to another thread (or machine) without locks, scaling almost linearly on distributed, multi-core architectures. The processes are addressed in the same way on a local VM as in a cluster of VMs, message sending works transparently regardless of the location of the receiving process. Processes do not share memory, allowing you to replicate your data for resilience and distribute it for scale. This means having two instances of the same process on two separate machines, sharing state updates among each other. If a machine fails, the other has a copy of the data and can continue handling the request, making the system fault tolerant. If both machines are operational, both processes can handle requests, giving you scalability. The BEAM provides the highly optimised primitives for all of this to work seamlessly, while OTP (the “standard library”) provides the higher level constructs to make the life of the programmers easy. Akka does a great job at replicating the higher level constructs, but is somewhat limited by the lack of primitives provided by the JVM, allowing it to be highly optimised for concurrency. While the primitives of the JVM enable a wider range of use cases, they make programming distributed systems harder as they have no built in primitives for communication and are often based on a shared memory model. For example, where in a distributed system do you place your shared memory? And what is the cost of accessing it? Scheduler We mentioned that one of the strongest features of the BEAM is the ability to break a program into small, light-weight processes. Managing these processes is the task of the scheduler. Unlike the JVM, which maps its threads to OS threads and lets the operating system schedule them, the BEAM comes with its own scheduler. The scheduler starts, by default, an OS thread for every core and optimises the workload between them. Each process consists of code to be executed and a state which changes over time. The scheduler picks the first process in the run queue that is ready to run, gives it a certain amount of reductions to execute, where each reduction is the rough equivalent of a command. Once the process has either run out of reductions, is blocked by I/O, is waiting for a message or completes executing its code, the scheduler picks the next process in the run queue and dispatches it. This scheduling technique is called pre-emptive. We mentioned the Akka framework many times, as its biggest drawback is the need to annotate the code with scheduling points, as the scheduling is not done at the JVM level. By removing the control from the hands of the programmer, soft real time properties are preserved and guaranteed, as there is no risk of them accidentally causing process starvation. The processes can be spread around the available scheduler threads and maximise CPU utilisation. There are many ways to tweak the scheduler but it is rare and needed only for edge and borderline cases, as the default options cover most usage patterns. There is a sensitive topic that frequently pops up regarding schedulers: how to handle Natively Implemented Functions (NIFs). A NIF is a code snippet written in C, compiled as a library and run in the same memory space as the BEAM for speed. The problem with NIFs is that they are not pre-emptive, and can affect the schedulers. In recent BEAM versions, a new feature, dirty schedulers, was added to give better control for NIFs. Dirty schedulers are separate schedulers that run in different threads to minimise the interruption a NIF can cause in a system. The word dirty refers to the nature of the code that is run by these schedulers. Garbage Collector Modern, high level programming languages today mostly use a garbage collector for memory management. The BEAM languages are no exception. Trusting the virtual machine to handle the resources and manage the memory is very handy when you want to write high level concurrent code, as it simplifies the task. The underlying implementation of the garbage collector is fairly straightforward and efficient, thanks to the memory model based on immutable state. Data is copied, not mutated and the fact that processes do not share memory removes any process interdependencies, which, as a result, do not need to be managed. Another feature of the BEAM is that garbage collection is run only when needed, on a per process basis, without affecting other processes waiting in the run queue. As a result, the garbage collection in Erlang does not ‘stop-the-world’. It prevents processing latency spikes, because the VM is never stopped as a whole – only specific processes are, and never all of them at the same time. In practice, it is just part of what a process does and treated as another reduction. The garbage collector collecting process suspends the process for a very short interval, often microseconds. As a result, there will be many small bursts, triggered only when the process needs more memory. A single process usually doesn’t allocate large amounts of memory, and is often short lived, further reducing the impact by immediately freeing up all its allocated memory on termination. A feature of the JVM is the ability to swap garbage collectors, so by using a commercial GC, it is also possible to achieve non-stopping GC in the JVM. The features of the garbage collector are discussed in an excellent blog post by Lukas Larsson . There are many intricate details, but it is optimised to handle immutable data in an efficient way, dividing the data between the stack and the heap for each process. The best approach is to do the majority of the work in short lived processes. A question that often comes up on this topic is how much memory the BEAM uses. Under the hood the VM allocates big chunks of memory and uses custom allocators to store the data efficiently and minimise the overhead of system calls. This has two visible effects: 1) The used memory decreases gradually after the space is not needed 2) Reallocating huge amounts of data might mean doubling the current working memory. The first effect can, if really necessary, be mitigated by tweaking the allocator strategies. The second one is easy to monitor and plan for if you have visibility of the different types of memory usage. (One such monitoring tool that provides system metrics out of the box is WombatOAM ). Hot Code Loading Hot code loading is probably the most cited unique feature of the BEAM. Hot code loading means that the application logic can be updated by changing the runnable code in the system whilst retaining the internal process state. This is achieved by replacing the loaded BEAM files and instructing the VM to replace the references of the code in the running processes. It is a crucial feature for no downtime code upgrades for telecom infrastructure, where redundant hardware was put to use to handle spikes. Nowadays, in the era of containerization, other techniques are also used for production updates. Those who have never used it dismiss it as a less important feature, but it is none-the-less useful in the development workflow. Developers can iterate faster by replacing part of their code without having to restart the system to test it. Even if the application is not designed to be upgradable in production, this can reduce the time needed for recompilation and redeployments. When not to use the BEAM It is very much about the right tool for the job. You need a system to be extremely fast, but are not concerned about concurrency? Handling a few events in parallel, and having to handle them fast? Need to crunch numbers for Graphics, AI or analytics? Go down the C++, Python or Java route. Telecom infrastructure does not need fast operations on floats, so speed was never a priority. Aided with dynamic typing, which has to do all type checks at runtime means compiler time optimizations are not as trivial. So number crunching is best left to the JVM, Go or other languages which compile to native. It is no surprise that floating point operations on Erjang, the version of Erlang running on the JVM, was 5000% faster than the BEAM. But where we’ve seen the BEAM shine is using its concurrency to orchestrate number crunching, outsourcing the analytics to C, Julia, Python or Rust. You do the map outside the BEAM and the reduce within the BEAM. The mantra has always been fast enough. It takes a few hundred milliseconds for humans to perceive a stimulus (an event) and process it in their brain, meaning that micro or nano second response time is not necessary for many applications. Nor would you use the BEAM for microcontrollers, it is too resource hungry. But for embedded systems with a bit more processing power, where multi-core is becoming the norm, you need concurrency, and the BEAM shines. Back in the 90s, we were implementing telephony switches handling tens of thousands of subscribers running in embedded boards with 16mb of memory. How much memory does a RaspberryPi have these days? And finally, hard real time systems. You would probably not want the BEAM to manage your airbag control system. You need hard guarantees, something only a hard real time OS and a language with no garbage collection or exceptions. An implementation of an Erlang VM running on the bare metal such as GRiSP will give you similar guarantees. Conclusion Use the right tool for the job. If you are writing a soft real time system which has to scale out of the box and never fail, and do so without the hassle of having to reinvent the wheel, the BEAM is the battle proven technology you are looking for. For many, it works as a black box. Not knowing how it works would be analogous to driving a Ferrari and not being capable of achieving optimal performance or not understanding what part of the motor that strange sound is coming from. This is why you should learn more about the BEAM, understand its internals and be ready to finetune and fix it. For those who have used Erlang and Elixir in anger, we have launched a one day instructor-led course which will demystify and explain a lot of what you saw whilst preparing you to handle massive concurrency at scale. The course is available through our new instructor lead remote training, learn more here . We also recommend The BEAM book by Erik Stenman and the BEAM Wisdoms , a collection of articles by Dmytro Lytovchenko.", "date": "2020-05-11"},
{"website": "Erlang-Solutions", "title": "Performance testing the JIT compiler for the BEAM VM", "author": ["Lukas Larsson"], "link": "https://www.erlang-solutions.com/blog/performance-testing-the-jit-compiler-for-the-beam-vm/", "abstract": "Erlang JIT In early September, our colleague, Lukas Larsson, announced the pull request for BEAMAsm, a just-in-time compiler for the Erlang Virtual Machine. Lukas worked on the development of this feature in conjunction with the OTP core team at Ericsson. The JIT compiler is a huge addition to the BEAM community. Two of its most impressive features are the speed improvements it delivers, with the JIT compiler offering anything from 30% to 130% increase in the number of iterations per second. The second thing that it offers is the perf integration which allows developers to profile where bottlenecks are in production. In this blog, we’ll take you through how we conducted the benchmarking and performance testing of the JIT using a RabbitMQ deployment. Find out more about our RabbitMQ services here . If you’d like to see the talk from Code BEAM V, watch the video below. RabbitMQ benchmarks Like any performance comparison, the first thing we need to do is to conduct benchmark testing for RabbitMQ. To do this, we need to create a docker image with the Erlang interpreter and a separate one with the JIT. Below is what such a docker file could look like. FROM docker.pkg.github.com/erlang/otp/ubuntu-base\n\nRUN apt-get update && apt-get install -y rabbitmq-server git linux-tools-generic\n\n## Enable us to connect from outside the docker container without auth\nRUN echo \"loopback_users = none\" >> /etc/rabbitmq/rabbitmq.conf\n\nENV MAKEFLAGS=-j4 \\\n        ERLC_USE_SERVER=yes \\\n        ERL_TOP=/buildroot/otp \\\n        PATH=/otp/bin:$PATH\n\nWORKDIR /buildroot\n\n## Download Erlang/OTP with JIT support\nRUN git clone https://github.com/erlang/otp && cd otp && \\\n        git checkout 4d9f947ea71b05186d25ee346952df47d8339da6\n\nWORKDIR /buildroot/otp/\n\n## Build Erlang/OTP with JIT support\nRUN ./otp_build setup -a --prefix=/otp/ && make install\n\n## Build Erlang/OTP without JIT support\nRUN make install FLAVOR=emu\n\nUSER rabbitmq\n\nCMD \"rabbitmq-server\" Then we build it as follows below: docker build -t rabbit . Then we can start RabbitMQ with the JIT: docker run -d -e ERL_FLAGS=\"\" -p 5672:5672 -p 15672:15672 We can also start it with the interpreter. docker run -d -e ERL_FLAGS=\"-emu_flavor emu\" -p 5672:5672 -p 15672:15672 RabbitMQ PerfTest We then use RabbitMQ PerfTest to measure the difference in the number of messages per second: Single Queue Performance > docker run -d -e ERL_FLAGS=\"-emu_flavor emu\" -p 5672:5672 -p 15672:15672 rabbit\n476fba6ad56c5d8b34ceac9336b035737c021dee788f3f6c0d21b9309d67373e\n> bin/runjava com.rabbitmq.perf.PerfTest --producers 1 --consumers 1 --queue q1 --flag mandatory --qos 300 --confirm 100\nid: test-104559-735, starting consumer #0\nid: test-104559-735, starting consumer #0, channel #0\nid: test-104559-735, starting producer #0\nid: test-104559-735, starting producer #0, channel #0\nid: test-104559-735, time: 1,000s, sent: 13548 msg/s, returned: 0 msg/s, confirmed: 13449 msg/s, nacked: 0 msg/s, received: 13433 msg/s, min/median/75th/95th/99th consumer latency: 404/5648/7770/11609/14864 µs, confirm latency: 241/3707/5544/9435/11787 µs\nid: test-104559-735, time: 2,000s, sent: 20558 msg/s, returned: 0 msg/s, confirmed: 20558 msg/s, nacked: 0 msg/s, received: 20569 msg/s, min/median/75th/95th/99th consumer latency: 919/4629/6245/9456/16564 µs, confirm latency: 440/3881/4493/6971/9642 µs\nid: test-104559-735, time: 3,000s, sent: 26523 msg/s, returned: 0 msg/s, confirmed: 26530 msg/s, nacked: 0 msg/s, received: 26526 msg/s, min/median/75th/95th/99th consumer latency: 1274/3689/3970/4524/5830 µs, confirm latency: 495/3617/3842/4260/4754 µs\nid: test-104559-735, time: 4,000s, sent: 25835 msg/s, returned: 0 msg/s, confirmed: 25827 msg/s, nacked: 0 msg/s, received: 25834 msg/s, min/median/75th/95th/99th consumer latency: 1946/3830/4124/4451/5119 µs, confirm latency: 1852/3760/4051/4415/5275 µs\nid: test-104559-735, time: 5,000s, sent: 25658 msg/s, returned: 0 msg/s, confirmed: 25658 msg/s, nacked: 0 msg/s, received: 25659 msg/s, min/median/75th/95th/99th consumer latency: 556/3840/4110/4646/7325 µs, confirm latency: 1611/3727/4020/4474/5996 µs\n....\nid: test-104559-735, time: 15,000s, sent: 25180 msg/s, returned: 0 msg/s, confirmed: 25177 msg/s, nacked: 0 msg/s, received: 25182 msg/s, min/median/75th/95th/99th consumer latency: 843/3933/4152/4543/5517 µs, confirm latency: 863/3898/4110/4495/9506 µs\n^Ctest stopped (Producer thread interrupted)\nid: test-104559-735, sending rate avg: 24354 msg/s\nid: test-104559-735, receiving rate avg: 24352 msg/s\n> docker stop 476fba6ad56c5d8b34ceac9336b035737c021dee788f3f6c0d21b9309d67373e The above results are for a single queue performance when under heavy load. We see that the average sending and receiving rates are both 24k. If we complete the same tasks with the JIT compiler we get the following results: > docker run -d -e ERL_FLAGS=\"\" -p 5672:5672 -p 15672:15672 rabbit\n993b90ab29f662b45ad4cab7d750a367ac9e5d47381812cad138e4ec2f64b2a3\n> bin/runjava com.rabbitmq.perf.PerfTest -x 1 -y 1 -u q1 -f mandatory -q 300 -c 100\n...\nid: test-105204-749, time: 15,000s, sent: 39112 msg/s, returned: 0 msg/s, confirmed: 39114 msg/s, nacked: 0 msg/s, received: 39114 msg/s, min/median/75th/95th/99th consumer latency: 1069/2578/2759/3101/4240 µs, confirm latency: 544/2453/2682/2948/3555 µs\n^Ctest stopped (Producer thread interrupted)\nid: test-105204-749, sending rate avg: 36620 msg/s\nid: test-105204-749, receiving rate avg: 36612 msg/s\n> docker stop 993b90ab29f662b45ad4cab7d750a367ac9e5d47381812cad138e4ec2f64b2a3 Which is 36K msgs/second already, we’ve seen a 45% increase in the number of messages per second. Multi-Queue Performance With a 45% increase in a single queue performance, it’s time to test how the JIT compares when looking at the multi-queue performance. Below are the results for the Erlang interpreter. bin/runjava com.rabbitmq.perf.PerfTest -x 150 -y 300 -f mandatory -q 300 -c 100 --queue-pattern 'perf-test-%d' --queue-pattern-from 1 --queue-pattern-to 100\n...\nid: test-105930-048, time: 60,396s, sent: 39305 msg/s, returned: 0 msg/s, confirmed: 39293 msg/s, nacked: 0 msg/s, received: 39241 msg/s, min/median/75th/95th/99th consumer latency: 44909/352133/503781/742036/813501 µs, confirm latency: 36217/352381/491656/714740/821029 µs\nid: test-105930-048, sending rate avg: 37489 msg/s\nid: test-105930-048, receiving rate avg: 37242 msg/s Here you can see 37,489 messages sent per second and 37,242 received. Now let’s test that benchmark against the JIT: bin/runjava com.rabbitmq.perf.PerfTest -x 150 -y 300 -f mandatory -q 300 -c 100 --queue-pattern 'perf-test-%d' --queue-pattern-from 1 --queue-pattern-to 100\n...\nid: test-105554-348, time: 60,344s, sent: 50905 msg/s, returned: 0 msg/s, confirmed: 50808 msg/s, nacked: 0 msg/s, received: 50618 msg/s, min/median/75th/95th/99th consumer latency: 32479/250098/359481/565181/701871 µs, confirm latency: 16959/257612/371392/581236/720861 µs\n^Ctest stopped (Producer thread interrupted)\nid: test-105554-348, sending rate avg: 47626 msg/s\nid: test-105554-348, receiving rate avg: 47390 msg/s Again, we have a significant boost in performance, this time there are 30% more messages sent and received. Profiling We can use perf to profile what is happening under-the-hood in RabbitMQ. We then need to add some permissions to run the perf tool. There are many ways to do that, the simplest (though not the most secure) is to add --cap-add SYS_ADMIN when starting the container. I’ve also added +S 1 because that makes the perf output a little easier to reason about and +JPperf true to be able to disassemble the JIT:ed code. > docker run -d --cap-add SYS_ADMIN -e ERL_FLAGS=\"+S 1 +JPperf true\" -p 5672:5672 -p 15672:15672 rabbit\n514ea5d380837328717 Then we can run perf in an exec session to the container: > docker exec -it 514ea5d380837328717 bash\n> perf record -k mono --call-graph lbr -o /tmp/perf.data -p $(pgrep beam) -- sleep 10\n[ perf record: Woken up 57 times to write data ]\n[ perf record: Captured and wrote 15.023 MB /tmp/perf.data (54985 samples) ]\n> perf inject --jit -i /tmp/perf.data -o /tmp/perf.data.jit\n> perf report -i /tmp/perf.data If you run the above, while the RabbitMQ PerfTest is running, it will display as seen in the image below: The default report from perf sorts the output according to the accumulated run-time of a function and its children. So from the above, we can see that most of the time seems to be spent doing bif calls closely followed by the gen_server2:loop and gen_server2:handle_msg . It makes sense for gen_server2 to be at the top of accumulated time as that is where most of RabbitMQ’s code is called from, but why would the accumulated time for bif be more extensive? The answer is in the usage of lbr ( last branch records ) as the call stack sampling method. Since the lbr buffer is limited, it does not always contain all the call stack of a process, so when calling bifs that have a lot of branches, perf loses track of the calling process and thus cannot attribute the time to the correct parent frame. Using lbr has a lot of drawbacks and you have to be careful when analyzing its finsing, however, it is better than having no call stack information at all. Using the information from the lbr we can expand the call_light_bif function and see which bifs we are calling and how what each call takes: Here we can see that a lot of accumulated time is spent doing erlang:port_command , ets:select and erlang:port_control . So the RabbitMQ application is busy doing tcp communication and selecting on an ets table. If we wanted to optimize RabbitMQ for this benchmark, it would be a good start to see if we could eliminate some of those calls. Another approach to viewing the perf report is by sorting by time spent in each function without its children. You can do that by calling perf as follows: > perf report --no-children -i /tmp/perf.data The functions that we spent the most time on here are do_minor (which is part of the emulator that does garbage collection), make_internal_hash (the hashing function for terms used internally that does not have to be backwards compatible)and eq (the function used to compare terms). If we expand make_internal_hash and eq we see something interesting: The top user of both functions is erlang:put/2 , i.e. storing a value in the process dictionary. So it would seem that RabbitMQ is doing a lot of process dictionary updates. After some digging, I found that these calls come from rabbit_reader:process_frame/3 and credit_flow:send/2 which seem to deal with flow control within RabbitMQ. A possible optimization that could be done here is to use the fact that literal keys to erlang:put/2 are pre-hashed when the module is loaded. That is instead of using a dynamic key like this: erlang:put({channel, Channel}, Val) you use a literal like this: erlang:put(channel, Val) and the emulator will not have to calculate the hash every time we put a value in the process dictionary. This is most likely not possible in this specific case for RabbitMQ, but it’s worth keeping in mind for further investigations. We’re excited about the JIT compiler and its use cases. If you’ve put it to use and can share some insight, get in touch with us, as we would be happy to co-organise a webinar on the JIT compiler! If you’d like to more help understanding how the JIT compiler can improve the performance of your RabbitMQ platform, Phoenix application, or any other Erlang/Elixir system, get in touch we’re always happy to help!", "date": "2020-05-11"},
{"website": "Erlang-Solutions", "title": "How to write a Phoenix PubSub adapter. Tutorial example based on EventStore", "author": ["Erlang Solutions"], "link": "https://www.erlang-solutions.com/blog/how-to-write-a-phoenix-pubsub-adapter-tutorial-example-based-on-eventstore/", "abstract": "In distributed systems there is usually a need for the asynchronous transmission of messages to one or more services or processes. If you have used Phoenix you might have discovered that it provides a flexible way of solving this problem through a built-in pubsub framework called Phoenix PubSub . Currently, it officially supports pubsub based on PG2 and Redis. It uses so called adapters to provide a pluggable interface for different pubsub implementations. In this blog post, we are going to show you the main steps of implementing an adapter for Phoenix PubSub. The example is written using version 2.0.0. A full implementation of the adapter which is based on EventStore can be found on Github at laszlohegedus/phoenix_pubsub_eventstore . The code discussed in this post is available on the master branch, while a version that works with phoenix_pubsub 1.1.2 is on branch v1.1. To learn more about Elixir and related technologies you might want to check out ElixirConf EU Virtual taking pace 18-19 June. Phoenix.PubSub.Adapter in a nutshell A Phoenix PubSub adapter has to implement a few callbacks that are specified in the behaviour Phoenix.PubSub.Adapter : node_name(adapter_name) child_spec(keyword) broadcast(adapter_name, topic, message, dispatcher) direct_broadcast(adapter_name, node_name, topic, message, dispatcher) Node name This function should return the node name as an atom or a binary. We did not discover too many uses for it, apart from the module Phoenix.Tracker and its implementations. In most cases the following implementation should suffice: def node_name(nil), do : node() def node_name(configured_name), do : configured_name Child spec This function is used to generate the child spec for our adapter. Note that it is a default implementation for each GenServer, so usually it is not necessary to overwrite it. Broadcast The function broadcast is called when a message is broadcasted through Phoenix.PubSub.broadcast . The first paramater adapter_name is derived from the name we specify for the PubSub. We set the name (as an atom or module name) of the PubSub system when we initialize it. Note that the name of the PubSub is treated as a valid (not necessarily existing) module name, so it is better to follow the corresponding naming convention. The name of the adapter will come from the PubSub name with the suffix .Adapter added. The topic and message parameters are self explanatory. The dispatcher is a module that is responsible for the local delivery of messages. It implements a dispatch/3 function that will forward the messages to the subscribed processes. Direct Broadcast Direct Broadcast is similar to broadcast with an additional node_name parameter. When direct_broadcast is called, the message should only be broadcasted to subscribers on a given node. EventStore adapter We will walk through a possible implementation of a Phoenix PubSub adapter that uses EventStore to distribute the messages between nodes. This gives us a solution that does not depend on Erlang/Elixir distribution. Additionally, we’ll have an event log stored in case further analysis is needed. Note that I did not perform any load tests on this solution and it is not production-ready, mainly a proof of concept and an aid for demonstration. I mentioned above that we are going to use the latest master of phoenixframework/phoenix_pubsub since it is cleaner and easier to use than the previous versions. Phoenix.PubSub In order to know how our adapter should work, it is worth looking into the code of the module Phoenix.Pubsub . It is well documented and clean, so it doesn’t take too long to understand what each function does. The latest master version of Phoenix Pubsub makes use of Registry . Each subscription is an entry under the corresponding key in the registry associated with our PubSub adapter. That is, when we call Phoenix.PubSub.subscribe(pubsub, topic, opts \\\\ []) , a new entry is added to the registry with Registry.register(pubsub, topic, opts[:metadata]) . Duplicate subscriptions are allowed, but they will lead to duplicate delivery of messages. Unsubscribing from a topic removes all entries for the process under that topic. The main functionality we are going to deal with is Phoenix.Pubsub.broadcast and the similar Phoenix.Pubsub.direct_broadcast . Whenever these functions are called, two main things happen: 1) The broadcast or direct_broadcast function is called on the corresponding PubSub adapter and 2) if successful, the message is dispatched to local processes through the default or overridden dispatch method. This means that the main goal of our adapter’s broadcast function is to make sure that the message gets delivered to the other nodes. In the case of a direct_broadcast the message should only be received by the subscribers on the given node and not others. The implementation First, we create a GenServer called Phoenix.PubSub.EventStore so we can easily stitch it into the PubSub supervision tree. We want to give the user the flexibility to specify which EventStore to use. To do this, we will expose an eventstore option to pass the desired EventStore module to the PubSub. We are going to store this in the state along with the name of the current instance of the pubsub adapter (the option name in the pubsub config). We will need both in the future. In order to use our PubSub we have to add it to our supervision tree. This can be done as follows: {Phoenix . PubSub,\n  [name: MyApp . PubSub,\n   adapter: Phoenix . PubSub . EventStore,\n   eventstore: MyApp . EventStore]\n} Then storing the desired values can be done in the GenServer’s init callback: defmodule Phoenix . PubSub . EventStore do @behaviour Phoenix . PubSub . Adapter use GenServer def start_link(opts) do GenServer . start_link(__MODULE__, opts, name: opts[:adapter_name]) end def init(opts) do {:ok,\n     %{\n       eventstore: opts[:eventstore],\n       pubsub_name: opts[:name]\n     }} end #... implementation will come here ...# end Note the difference between opts[:name] and opts[:adapter_name] . The former is the name of the PubSub as a whole and is reserved for the Registry. Publishers use it when broadcasting messages. We can use opts[:adapter_name] as the name of our GenServer. The implementation is fairly simple. We have to make sure that our adapter can be used to broadcast messages to all subscribers. For this, we will make use of the event store. Distributing a message as an event The first thing our GenServer has to do is append a new message to the event store when broadcast/4 is called. def broadcast(server, topic, message, dispatcher) do GenServer . call(server, {:broadcast, topic, message}) end def handle_call(\n      {:broadcast, topic, message},\n      _from_pid,\n      %{eventstore: eventstore} = state\n    ) do event = %EventStore . EventData{ ... }\n\n  res = eventstore . append_to_stream(topic, :any_version, [event])\n\n  {:reply, res, state} end This is where we have to decide how we want to wrap the message inside an %EventStore.EventData{} struct. An easy solution is to serialise the message as one field. For this, we’ll have to introduce a struct that will hold this field: defmodule Phoenix . PubSub . EventStore . Data do defstruct [:payload] end Then our message can be written as: event = %EventStore . EventData{\n  event_type: \"Elixir.Phoenix.PubSub.EventStore.Data\",\n  data: %Phoenix . PubSub . EventStore . Data{\n    payload: Base . encode64(:erlang . term_to_binary(message))\n  }\n} Note that EventStore converts the data to JSON and if we only used :erlang.term_to_binary then we would likely have invalid JSONs, so an additional base64 encoding is required. You may wonder why we need to convert the payload to a binary. This is needed because JSON cannot differentiate between atoms and strings, so each atom would appear as a string in the published message. If we want consistency, we have to make sure to serialise and deserialise the payload. Another way to partially solve this would be to force the user to use structs when sending messages. That way EventStore would be able to do ser-des. Note that the keys remain atoms, but any value that was originally an atom will be converted to a string, so some post processing is still required. Unless the messages are huge the base64 encoded binary should suffice. Handling events, local distribution Now that the events are in the event store, any process that is subscribed to corresponding topics will receive them. First, we have to make sure that our GenServer ( Phoenix.PubSub.EventStore ) subscribes to all topics ( \"$all\" ). If you also want to use an event store for a different purpose, it’s best to have a separate one for pubsub. We can easily do the subscription by sending a message to self() , then handling it in handle_info right after the server starts. def init(opts) do send(self(), :subscribe)\n\n  {:ok,\n   %{\n     eventstore: opts[:eventstore],\n     pubsub_name: opts[:name]\n   }} end #...# def handle_info(:subscribe, %{eventstore: eventstore} = state) do eventstore . subscribe(\"$all\")\n\n  {:noreply, state} end def handle_info({:subscribed, _subscription}, state), do : {:noreply, state} We use a transient subscription, because we do not care about previous messages. The event store will reply with a {:subscribed, subscription} message, which we’ll also have to handle. After this, the server will start receiving {:events, events} messages. Note that when a message is broadcast on a node, it will be distributed to local subscribers by Phoenix.PubSub right after our adapter returns from broadcast/4 as seen in the implementation: # from Phoenix.PubSub # def broadcast(pubsub, topic, message, dispatcher \\\\ __MODULE__) when is_atom(pubsub) and is_binary(topic) and is_atom(dispatcher) do {:ok, {adapter, name}} = Registry . meta(pubsub, :pubsub)\n\n  with :ok <- adapter . broadcast(name, topic, message, dispatcher) do dispatch(pubsub, :none, topic, message, dispatcher) end end So we have to make sure that a local message is not dispatched twice. For that we can add a unique ID (I went with UUID.uuid1() ) to the process state: def init(opts) do send(self(), :subscribe)\n\n  {:ok,\n   %{\n     id: UUID . uuid1(),\n     eventstore: opts[:eventstore],\n     pubsub_name: opts[:name]\n   }} end Now, we can just add the id to the event before publishing it into the event store. I chose to put it in the metadata field, but we could also wrap it inside data . Although it’s best to keep this information separate from the actual message. Finally, we’ll have to change the handle_call for :broadcast and add the id to the event: event = %EventStore . EventData{\n  event_type: \"Elixir.Phoenix.PubSub.EventStore.Data\",\n  data: %Phoenix . PubSub . EventStore . Data{\n    payload: Base . encode64(:erlang . term_to_binary(message))\n  }\n  metadata: %{source: id}\n} Where the value of id comes from the state. Now when we recive an event we know where it came from and we can decide whether it is needed to be dispatched to local subscribers. def handle_info({:events, events}, state) do Enum . each(events, & local_broadcast_event(&1, state))\n\n  {:noreply, state} end defp local_broadcast_event(\n       %EventStore . RecordedEvent{\n         event_type: \"Elixir.Phoenix.PubSub.EventStore.Data\",\n         data: %Phoenix . PubSub . EventStore . Data{\n           payload: payload\n         },\n         metadata: metadata,\n         stream_uuid: topic\n       },\n       %{id: id, pubsub_name: pubsub_name} = _state\n     ) do case metadata do %{\"source\" => ^ id} -> # This node is the source, nothing to do, because local dispatch already # happened. :ok\n\n    _not_local -> # Otherwise broadcast locally message = :erlang . binary_to_term(Base . decode64(payload))\n\n      Phoenix . PubSub . local_broadcast(\n        pubsub_name,\n        topic,\n        message\n      ) end end That’s it. This should give us enough to have a simple implementation of Phoenix Pubsub using EventStore. Note that the implementation of direct broadcast is still missing, which I solved by adding the destination node to the metadata field and extending the function local_broadcast_event to handle it. I also added support for handling the dispatch field during broadcasts. For my complete implementation consult laszlohegedus/phoenix_pubsub_eventstore . Find out more about our work with the Elixir programming language .", "date": "2020-05-15"},
{"website": "Erlang-Solutions", "title": "Elixir Community Tools: StreamData", "author": ["Martin Gausby"], "link": "https://www.erlang-solutions.com/blog/elixir-community-tools-streamdata/", "abstract": "Intro In his ElixirConf US 2018 keynote, José Valim announced that what the core team had set out to do with Elixir was now present in the language, and that new projects, ideas, and developments belong to the ecosystem, and should be explored by the community. In this blog series, we will highlight some of the interesting packages that have emerged in the Elixir ecosystem, describe why they exist and how they can help us build Elixir applications. It is important to stress that a mention of a package is not necessarily an approval from Erlang Solutions. There might be occasions where a given package might not be the right choice for your application, but we do provide code review services and are able to augment your team with highly skilled consultants, so do get in touch. With that out of the way; In this first installment, I will take a look at the StreamData project started by Andrea Leopardi. StreamData will help us generate random data that we can use to test our applications. Raison d’être StreamData is a framework for generating pseudo-random data based on specifications, which are defined using “generators” that can be combined together to create more complex generators. Generators are created using the functions found in the StreamData toolbox. That truly is a mouthful, and one might ask: Why would I need randomly generated data for test purposes? After all, I am interested in testing based on a known input for my application, or algorithm, to produce an expected output! To answer that we have to look at the second part that StreamData provides us, ExUnitProperties, which gives us ExUnit helpers for defining test cases that fall within the category of “property based testing.” So, we have a framework for defining generators that will produce random data based on specifications, and helpers that enable us to define property based tests in ExUnit. Let us explore that; first we will explore data generation by defining a data generator, and then we will use that data generator to define a test case. Generating data StreamData is available on Hex, and as I write this, the current version is “0.5.0.” Let us create a “Playground” application using mix, and add {:stream_data, \"~> 0.5.0\"} as a dependency. Once we run our trusty mix deps.get it should be available to us. Running our Playground project in an interactive Elixir shell we will see that typing StreamData . (dot) and pressing tab will produce a long list of functions, and some of them have names such as integer, float, string , etc, mapping to data types we are familiar with from Elixir. So let us try to generate an integer: iex> StreamData.integer()  \n#StreamData<45.14801001/2 in StreamData.integer/0> Hmm, not the output we expected. Instead of getting an integer, we got some kind of data structure; This is a generator! The generators in StreamData implement the Enumerable protocol, meaning we can use functions from the Enum-module to produce our data. iex> StreamData.integer() |> Enum.take(5)  \n[-1, -2, -2, -4, 1] As the name “StreamData” suggests we can also use functions found in the Elixir Stream module. Applying a Stream.map to our pipeline we could, for instance, get rid of all the negative numbers: iex> StreamData.integer() |> Stream.map(&abs(&1)) |> Enum.take(5)  \n[1, 2, 2, 3, 3] abs/1 will take the arithmetical absolute value of number; a fancy way of saying it will ignore the minus if the number is negative. NB: If we used the map function from the Enum module our interactive Elixir shell would hang. This is because it will work on the entire list, eagerly grabbing all the elements from the stream, which is infinite! The map function found in Stream will work on one element at a time, so this combined with the Enum.take function, which takes a finite number of elements resulting in a finite list of elements. One interesting thing to notice is that the values produced get more “crazy” and “extreme” the more we ask from our generator. If we experiment with dropping a lot of elements we can see this in action: iex> StreamData.integer() |> Stream.drop(100_000) |> Enum.take(1)  \n[-52] This is true for all the data types StreamData can produce; iex> StreamData.float() |> Stream.drop(1_000) |> Enum.take(1)  \n[-1.243273519341557e30]  \niex> StreamData.string() |> Stream.drop(1_000) |> Enum.take(1)  \n[\"9DE^nC*:?k4S~\\\\7xWdW lt`Y_6HC]a>R@pkRXX96iw7K~~*Z\\\\1\"] NB: I have reduced the number of elements we drop for these examples, because it is a bit more computation intensive to produce floats and strings than it is to produce an integer. Even though we are dropping the values they are still computed. The fact that the values start out small and become bigger, and more “crazy”, is very important for when we get into the property based testing aspect of StreamData. But first, let us explore composing generators together. Elixir has some compound data types such as lists, maps, and tuples. They can all contain keys and values consisting of other terms, so being able to generate these data types and specifying what type their keys and what type the values will have will be very helpful; to do this in StreamData we have some functions that will work together with other StreamData functions to “compose” the desired output. For instance, we got the StreamData.list_of-function, that will take a generator and generate lists containing elements produced by the given generator. iex> StreamData.list_of(StreamData.integer()) |> Enum.take(5)  \n[[1], [], [2, 2, 1], [-1], []] As we can see this will produce lists of integers of varying length, and sometimes it will be empty. The list_of function takes the integer function and composes it into a generator that will produce exactly that, a list of integers. We could compose that with the nonempty function and get a new generator that will never produce an empty list. Very handy in some situations! iex> list_of_integers = StreamData.list_of(StreamData.integer())\n#StreamData<54.14801001/2 in StreamData.list_of/1>\niex> StreamData.nonempty(list_of_integers) |> Enum.take(5)\n[[0], [-1], [3, -1], [2], [-4, -3, 2, -3, 0]] Again; notice that the output gets “crazier” the more data we request from the stream, and in my “crazy” terminology: If you are a list, to get crazy, means that you will grow in size. StreamData makes it possible to adjust how much a list should grow by passing in options, but that is outside the scope of this article. Notice though that the inside of the list generators gets “crazy” as well. It is craziness all the way down. Dipping our toes in property based testing Let us put all this randomness to good use. So far, we have a generator that can produce random lists containing random integers. We know the list will start out with small values, and get a greater degree of complexity the more values we request from the generator. These are properties that come in very handy when we get into property based testing. How does property based testing differ from regular unit testing? In unit testing, we call into our implementation with some input values that we come up with, and we will assert on the resulting values. In the unit test approach, we use a known input and test that against the expected output, which is also known. This is a good approach to testing, and it will get us so far, but it requires us to anticipate edge cases, and it can become very repetitive to set up assertions. In property based testing, we look for a “property that should hold true” for the thing we want to test. This sounds scary, and it really is, but it is very powerful and can find edge cases that would be very hard to anticipate. Let us take a list as an example, and demonstrate how to test that a reverse list function works as expected. The property of a list being reversed is…that the order of the elements in the list is in the reverse order of its input. So, if we give an input to the reverse list function and test if the result is different from the input like this: defmodule PlaygroundTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  describe \"list\" do\n    property \"should not be the same when reversed\" do\n      check all input_list <- list_of(integer()) do\n        refute input_list == Enum.reverse(input_list)\n      end\n    end\n  end\nend Will show us that our assumption and property had a minor flaw. When run, it will produce the following output: 1) property list should not be the same when reversed (PlaygroundTest)\n     test/playground_test.exs:8\n     Failed with generated values (after 0 successful runs):\n\n         * Clause:    input_list <- list_of(integer())\n           Generated: []\n\n     Refute with == failed, both sides are exactly equal\n     code: refute input_list == Enum.reverse(input_list)\n     left: []\n     stacktrace:\n       test/playground_test.exs:10: anonymous fn/2 in PlaygroundTest.\"property list should not be the same when reversed\"/1\n       (stream_data 0.5.0) lib/stream_data.ex:2102: StreamData.check_all/7\n       test/playground_test.exs:9: (test)\n\nFinished in 0.04 seconds\n1 property, 1 failure\n\nRandomized with seed 205049 Studying this we can see that we failed spectacularly at our first run. Our generator produced an empty list, and of course the result of reversing an empty list is an empty list. We need to adjust our test model. What happens if we add the StreamData.nonempty/1 to the data generator? Well, we will learn that it sometimes generates lists containing the same value; reversing the list [0, 0] will result in [0, 0] . We need to think deeper. Reversing a list is a reversible operation. So if we reverse a list, and then reverse it again, then we should end up with the initial list. Let us try that instead: defmodule PlaygroundTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  describe \"list\" do\n    property \"reversing twice should result in the initial input\" do\n      check all input_list <- list_of(integer()) do\n        assert input_list == input_list |> Enum.reverse() |> Enum.reverse()\n      end\n    end\n  end\nend Presto. This should result in a successful test run. The observant reader would notice that the indentity function (a function that simply returns its input, fn (x) -> x end ) would pass this property as well! We have a couple of options for tackling this problem, where the easiest might be to use the StreamData.uniq_list_of generator instead of the StreamData.list_of , as it would ensure the elements in the lists will never repeat. As a result reversing the list should result in a list that is different from the input. Another option could be to mix and match property based testing, and regular unit testing, and write a test that makes the assertion that a list with two different elements, :foo and :bar, indeed becomes :bar and :foo when reversed—the two approaches to testing lives perfectly well side-by-side, and accommodate each other quite well. We encourage you to copy the example into a project and make it more robust, and please share your findings with the community. A couple of things to notice. Once we have used the ExUnitProperties module (provided by StreamData) a “property”-macro will be available to us in the test DSL (domain specific language). It is similar to the test-macro, but it knows about the check macro, that we in this case instruct to check “all” (meaning a lot of different input lists), and all this will be tested in the assert (or refute) in the check-body. Also, in the examples we use list_of and integer without specifying the StreamData module; these have been imported when we used ExUnitProperties . There is one last thing I would like to show you. I have been talking a lot about data getting crazier the more we ask from a given data generator. Would that make our test output “crazy” if our property test finds a failure deep within the run? Not necessarily! StreamData supports “shrinking,” which means that once it has found a failure it will attempt to “shrink” the input that made the test fail, so it can present the minimal input needed to break the property. Let me demonstrate with a crazy example, where we use our own reverse implementation that has a weird bug which ignores instances of the number 15: defmodule PlaygroundTest do\n  use ExUnit.Case\n  use ExUnitProperties\n\n  # An intentionally broken reverse function\n  defp broken_reverse(list) when is_list(list) do\n    # kick off the recursion\n    broken_reverse(list, [])\n  end\n\n  # Base case\n  defp broken_reverse([], acc), do: acc\n\n  # Recursive cases:\n\n  # BUG: For some reason we throw away the value 15 when we see it!\n  # Thanks to jlouis for providing this very random integer:\n  # - https://twitter.com/jlouis666/status/1260563927534112768\n  defp broken_reverse([15 | remaining], acc) do\n    broken_reverse(remaining, acc)\n  end\n\n  # put the head into the accumulator and recurse\n  defp broken_reverse([head | remaining], acc) do\n    broken_reverse(remaining, [head | acc])\n  end\n\n  describe \"crazy list\" do\n    property \"input should be the same if reversed twice\" do\n      check all input_list <- list_of(integer()) do\n        assert input_list == input_list |> broken_reverse() |> broken_reverse()\n      end\n    end\n  end\nend Which will result in this helpful test report: 1) property crazy list reversing twice should result in the initial input (PlaygroundTest)\n     test/playground_test.exs:29\n     Failed with generated values (after 20 successful runs):\n\n         * Clause:    input_list <- list_of(integer())\n           Generated: [15]\n\n     Assertion with == failed\n     code:  assert input_list == input_list |> broken_reverse() |> broken_reverse()\n     left:  [15]\n     right: []\n     stacktrace:\n       test/playground_test.exs:31: anonymous fn/2 in PlaygroundTest.\"property crazy list reversing twice should result in the initial input\"/1\n       (stream_data 0.5.0) lib/stream_data.ex:2148: StreamData.shrink_failure/6\n       (stream_data 0.5.0) lib/stream_data.ex:2108: StreamData.check_all/7\n       test/playground_test.exs:30: (test)\n\nFinished in 0.05 seconds\n1 property, 1 failure In this case it had 20 successful runs, and on the twenty-first run it produced a list that resulted in an error. It shrunk the input until it found the smallest possible input that can reproduce the error, a list containing the number 15, [15] . This makes it much easier to find the bug and correct it. Feel free to play around with it—it really is magic—and if you need random values a bunch of helpful people have provided some here: https://twitter.com/gausby/status/1260561299181838336 Notice that a value which is too high, such as 115, in our “bug” might go unnoticed. This is because the generator will not get to produce a value anywhere near that range before it decides it has generated enough tests to be confident that the property holds true. Luckily we can adjust the size of the test space, but keep in mind that a bigger test space will result in a longer run time. A good way to get around that is to have a small test space on the local test environment, such that tests run very fast while we develop, and go a bit crazier on a continuous integration server, where we can allow the test suite to run a bit longer and be a bit more thorough. How to set this all is described in the StreamData documentation. As we see, creating property based testing requires some deep thought, and it is a skill that needs to be learned, but it is a very strong testing strategy when applied correctly. StreamData is a framework that integrates very well with the Elixir ecosystem, but is only one of the many options out there. Another option is PropEr (and PropCheck , which provides a wrapper for Elixir). The creator of StreamData, Andrea Leopardi gave the talk Property-Based testing is a mindset at ElixirConf EU 2018 , and this is a very good introduction to both topics. If property based testing has caught your interest we would like to suggest the book Property-Based Testing with PropEr, Erlang, and Elixir by Fred Hebert from the Pragmatic Bookshelf. Find out more about our Elixir programming language projects.", "date": "2020-05-28"},
{"website": "Erlang-Solutions", "title": "Erlang Solutions partners with Humio to provide streaming log management to our customers", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/erlang-solutions-partners-with-humio-to-provide-streaming-log-management-to-our-customers/", "abstract": "Achieve real-time observability and the power to log everything to answer anything in real time. Erlang Solutions, the world’s leading provider of Erlang and Elixir solutions, and Humio , the only log management platform enabling complete observability from streaming logs in real time, self-hosted or in the cloud, will collaborate to help customers across Europe understand and explore complex IT environments and keep them secure. Combining the technical and consulting capabilities of Erlang Solutions with Humio’s modern log management platform enables the clients of both partners to accelerate their growth with the ability to easily log everything — structured or unstructured — in real time and at massive scale. “We look forward to collaborating with Erlang Solutions to deliver streaming observability at scale to organizations looking to gain efficiencies with managing complex, distributed systems. Humio’s real-time log management platform combined with Erlang Solution’s transformative consulting approach makes it possible for companies to maximize end-to-end system performance and dramatically improve user experiences,” stated Mortem Gram, EVP at Humio. Exploring Humio’s platform with Erlang Solutions gives users instant and full visibility into complex, distributed systems, backed by direct access to an extended team of scalability experts. “We are delighted to be working with Humio in providing industry-leading log management solutions to our customers. Our customers come to us because of our deep expertise in building highly scalable, distributed systems and the addition of Humio to our portfolio further enhances that capability”, shared Stuart Whitfield, CEO of Erlang Solutions. Visit the Humio website for more information, https://www.humio.com/ About Humio Humio’s log management platform offers the lowest total cost of ownership, industry-leading unlimited plans, minimal maintenance and training costs, and remarkably low compute and storage requirements. Humio is the only log management solution that enables customers to log everything to answer anything in real time — at scale, self-hosted or in the cloud. Humio’s modern, index-free architecture makes exploring and investigating all data blazing fast, even at scale. Founded in 2016, Humio is headquartered in London and backed by Accel and Dell Technologies Capital. For more information, visit www.humio.com and follow @MeetHumio on Twitter. Find out more and how to partner with us.", "date": "2020-06-22nd"},
{"website": "Erlang-Solutions", "title": "Scaling a Mongoose: How scalable is the MongooseIM XMPP server?", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/scaling-a-mongoose-how-scalable-is-the-mongooseim-xmpp-server/", "abstract": "How does MongooseIM it scale? When talking about servers, this question is asked over and over again, and, MongooseIM i s no exception. How does it scale? It scales well, this we know. We’ve done a lot of load tests in various environments, we’ve set up multiple production clusters, some of them handling a substantial load. But, more specifically, how does it scale? It is a difficult question and requires a careful insight (never say that in a TV interview, it is the worst answer you can possibly give). But, it really is. It depends on so many factors – underlying hardware, usage pattern, enabled extensions, database backend, integrations… We’d love to have a definite answer to the question, but is that even possible? 215 km/h This is what my car spec says is its top speed is. Does that mean I can drive it that fast? First of all, no because I’m not going to try. Even if I did, I certainly won’t make it, for a variety of reasons – the car is not brand new, driving conditions are never perfect, I have regular tyres which are designed for security and not for speeding, etc. What the manufacturer is really saying is (a) you won’t go faster than that no matter what you do, and (b) something like 180km/h is absolutely fine (if legal). So let’s follow this approach and find out what the “top speed” of MongooseIM is. Along the way, we’ll also examine how it behaves when we expand the hardware, both horizontally and vertically, what is limiting its growth, and some other interesting aspects. Setup We ran our tests on AWS – it is the most commonly used general-purpose cloud environment. Results from there can serve as a benchmark. A set of ansible scripts provisioned an EC2 instance, installed and configured MongooseIM. Then we launched the “client” EC2 instances one by one, each establishing a number of XMPP connections pretending to be real clients. We proceeded with it until either new connections started failing or the end to end time to delivery shot up, then we terminated the whole test. The MongooseIM server was “plain vanilla” – the goal was to test just the main functionalities, like handling connections and routing messages. If a cluster of servers were used, clients would connect randomly to any of the nodes available – there was no load balancer to get in the way. Client behaviour was also rudimentary – a single “client” would establish a stream, authenticate and then keep the connections open, sending one short message every minute and receiving whatever came in. Messages contained timestamps, so that we could track end-to-end time-to-delivery. First results Instance types After experimenting with a number of instance types, we found out that the c5 line is a perfect balance. With our assumed usage pattern, this hardware profile provides just the right combination of memory and CPU power. Memory-optimised instances of a similar size offers a similar performance, while being much more expensive – there is not much to gain by adding more memory. Also, results from running memory-optimised instances were unstable, since the CPU usage of MongooseIM had many spikes which may break the tests at any time. Here are some examples: Message frequency Our imaginary average user sends one message per minute – with 137k connections on c5.large instance, this means that MongooseIM is routing about 2.3k messages per second. What if our users become more active, or less active – how does it change the maximum load we can handle? Less messages means the CPU has less work to do, but memory pressure stays the same. The expectation, therefore, is, that when the traffic is low, the maximum number of connections should not depend on the traffic. This is because, in that scenario, the memory is the limiting factor. On the other hand, if the load increases, at some point, it should overload the CPU and the maximum number of connections should start to fall. Our tests confirmed this, and also proved that the default usage pattern of one message per minute is just about the breaking point. Below it the limit stays the same. Go slightly above it and the limit begins to fall. Testing scalability on two dimensions Vertical Now that we know that c5 line provides a perfect balance, how will using a more powerful variant help? Again, is it linear? Does doubling the digit before letter “x” actually double the load we can handle? Is there a limit to it? This was tricky, because there are many OS-level limits and Erlang VM settings which can stop your node from accepting more connections. For details on how we configured our servers, consult MongooseIM documentation available online. We ran our test on larger instance types, up to c5.9xlarge. At that level, MongooseIM was able to handle almost 2.5 million connections, passing 45 thousand messages per second. And no issues were found, so there doesn’t seem to be a real hard limit. However, since this was already far more than we were aiming for, and running these tests incurs a considerable expense, in terms of both time and money, we chose to stop here. This is not the end, though, chances are that some day we will try to push it even further. Horizontal How does MongooseIM scale horizontally? Is it linear, and if so, how many users each new node can handle? Can we scale ad infinitum, or is there a limit above which there are diminishing returns or even a failure? We wanted to find out, and it was by far the most time consuming part of the whole research. We were assembling MongooseIM clusters on c5.large instances and stress-testing them until they failed. We went on and on, and scaling was almost linear, with the slope of the line fluctuating about 51 thousands (meaning each new node increased the cluster’s capability by 51 thousands connections, with intercept being about 80 thousands). And so it went, until we reached fifteen nodes, at over 1.2 million users and 22 thousand messages per second, no limit was in sight. At this point we decided to jump ahead and try twenty nodes. This proved to be too much. Connections between the nodes were timing out every now and then, causing netsplits and inconsistencies in the Mnesia database, and the cluster was basically not functional. The reason for this is the way Mnesia and, more generally, Erlang distribution works. Each clustered Erlang node maintains active connections to all the other nodes, so the number of connections grows at an increasing rate. With twenty nodes, there were already 190 connections to maintain and exchange data over. User sessions in MongooseIM are kept in the Mnesia database, which is replicated to all the nodes to ensure strict consistency at all times. Every new connection then triggers a replication mechanism spanning all the nodes and possibly all the internode connections. With this number of nodes, traffic can become quite substantial, no wonder it becomes unstable. Why not both? Since we didn’t discover any limit in the vertical scalability, and horizontal clustering is linear (up to a reasonable number of nodes), then by combining the two, we could expect to be able to handle really large numbers – from what we know so far, ten million is well within reach. Taking costs into account, we decided to stop the load tests at this point. Exploring this path is one of the options for the future. Cost Last but not least, how much does running a MongooseIM cost, and how do the various clustering strategies work cost-wise? Because horizontal scaling is linear with a fixed part, running a smaller cluster is somewhat more cost-efficient than expanding the number of nodes. Vertical scaling, in turn, is almost exactly proportional – the AWS price list is designed in such a way that an instance which is twice as expensive can handle almost exactly two times more client connections. The rule of thumb for economic clusters would then be: make a cluster of four or five nodes, and scale vertically if needed. And here we have to reiterate that actual results, from a customised server with its particular usage pattern may vary widely. Keep in mind also that in real life, it is not only MongooseIM you have to scale. Database backends or other external services, which are normally shared by all the instances, have their own scaling patterns, and pricing. Third dimension – federation If the load is hard to handle for a single cluster, you can also go beyond that – there is no rule saying that all users have to use the same data centre. With federation, you can set up multiple data centres, each under their own domain, thus multiplying the possible load your whole system can handle. It’d also give you the extra benefit of bringing your servers closer to your geographically distributed uses. Federation also has its idiosyncrasies, but it is beyond the scope of this article (chances are we will get back to it in one of the future installments).", "date": "2020-07-10"},
{"website": "Erlang-Solutions", "title": "Applying Scrum to a system of fluid teams working on an open-source product", "author": ["Katarzyna Kraus"], "link": "https://www.erlang-solutions.com/blog/applying-scrum-to-a-system-of-fluid-teams-working-on-an-open-source-product/", "abstract": "The modern IT workplace is weird. You get a desk that sometimes has a chair, but often also wheels or a button that makes it go up and down. There are often bean bags involved, bosses that say they are not really bosses, complimentary fruit and candy, engagement programs, and people constantly pushing you to “self-actualise” and grow. Your projects often come with a lot of trust and freedom, but that also means that project structures are fluid, and there is a lot of responsibility for individuals to set up the direction. On the spectrum of freedom and security, everything serves the most important goal – adapting to change. A lot has been said about how Agile and Scrum differ from other approaches to organising work. They are born out of experiencing first hand how detrimental it can be to follow ill-fitting methods that work well in other industries. At their core, they are about creating effective communication channels – between team members, teams and the market, different stakeholders and the various people responsible for orchestrating the process. A culture and environment that promotes sharing, listening and taking ownership for your work is required for this approach to shine and foster dynamic progress and satisfaction of all involved. It would be tempting to say that following the Scrum guide verbatim is a silver bullet for running all IT projects, and many would argue it is. In this short article, I would like to tell you the story of how we took the spirit of Agile, measured it against our needs and then iteratively made it our own. Did we manage to reach the shore of a procedure that works for us? For now – yes. Will we stick with it? I have no idea – come back and check in with us in a few months. One thing is for sure, if we are not, it will be born out of a test and learn approach which has led us to something that fits our way working better. Mongoose wrangling – you have to be agile! MongooseIM is an open source product developed by a commercial company. 100% of the code is available to the public; everyone can use it, customise it, contribute, point out bugs, raise feature requests. At the same time, the core development team works not only on developing the product but also on assisting companies that want to create custom extensions, optimise their installations, scale or adjust their setup. This means we have two groups contributing to MongooseIM – the open source community and Erlang Solutions developers. The second group can’t really be considered a monolith, since some of them only work with the open source product while others assist the company’s customers with different projects. In short, what we have on our hands is a system of fluid teams . By definition a fluid team consists of “experts from disparate functions and geographies who must get a temporary project or task up and running, sometimes with completely different priorities, beliefs and values”. As challenging as it may sound, this form of organisation is typical for modern IT, it has been widely accepted that nowadays “this is how we roll”. What we have at MongooseIM goes a step further – it is not just a fluid team, it’s a system of fluid teams. We all work on MongooseIM in one way or another, but our goals, priorities and procedures are often very different, since what is important for a single client is not always a must for the entire product (and vice versa). Different projects follow different procedures; they follow different guidelines and assumptions and sometimes they even look for solutions to contradicting problems. We slip in and out of different roles and teams, and the direction of the entire system is forged somewhere in between these parallel interests. This also means implementing Scrum is tricky. You have to remember that Scrum is not only valuable, it’s also costly. You spend a lot of time and effort on inspection – meetings, analysis, discussions and syncing. It’s all an investment you make to empower your team, allow them to learn, minimize certain risks and make sure you generate value, but in order to have a return on that investment, you have to be consistent in your actions. For a team to be considered totally “scrummy” it has to meet a few criteria. It has to be cross-functional, self-organising, it can’t have any hierarchies, and it has to be small. Ideally it should have between 3 and 9 team members. Many of these assumptions make little sense when talking about an open source project like MongooseIM. For example – who do we consider to be a part of the team? Everyone who contributes to the project? If so, there is no way of determining how many people that is or to coordinate their efforts. So do we define it as all Erlang Solutions employees working on MongooseIM? Some of them are embedded in customer teams, making it near impossible to have them conform to Scrum ceremonies and requirements. The cost of abiding by the client’s procedure and our Scrum would make them unable to do any actual work. Perhaps we should narrow down our focus to just the core development team? Very few members will be with the team for more than 2 consecutive sprints, there will be sprints with just 2 or 3 members, followed by sprints with 9 or more, we will constantly pay the cost of onboarding new people and relearning and restructuring day-to-day work. In short – it will be very hard to let the team members do their work, learn from each other, form long term plans, and benefit from what Scrum offers. Taking all of that and more into consideration, for us, there was little sense in going “full Scrum”. To be able to progress and not lose our minds, we had to remain Agile, but also, we had to make it our own. Building a frame Let’s skip the part where we start with an eager team, an inexperienced Scrum master and a willingness to do everything just as the Scrum guide says. That was our starting point, but it is not exactly the basis of what we do now. For me that elusive basis was best expressed by Tomasz Włodarek at one of his amazing workshops. To paraphrase the thought, he encourages management to steer away from creating a workflow as a set of rules, or set pipelines with different steps and stages. You do need those to create predictability and thus stability, but at the heart of how you operate, there should be something else. Your work culture should be built on two sets of objects later filled by your self-organising teams. Those two are the frames and attractors. You build these by answering two simple questions – “what are the things we want to discourage?” and “what are the things we want to encourage?”. The answers will never be complete and might change over time, but they need to be communicated to the team. For our team, there were a few things we did not want. We did not like it when we were swamped with ad hoc requests. We did not like it when people started a lot of tasks and never finished any. We did not like external factors interfering with our goals. We did however, and still do, love our product. Working with Erlang, XMPP and MongooseIM is something the team is genuinely excited about. We love to learn, explore, come up with new ideas, we love the open source culture and commercial projects we get to work with. It is exciting to get to work with customers who have massive, running installations and then assist a hobbyist writing his own MongooseIM module for fun. It is great seeing in action how people use and build upon our work. Luckily this state of mind aligned perfectly with what our management wanted – a highly professional, efficient team always growing their capabilities, ready to tackle any task. The initial attractors and frames we’ve set reflected that: We wanted to encourage knowledge sharing – so we created channels to share the lessons we’ve learnt and award people who actively engage in assisting others. We wanted the team to have agency – so we made sure that both our backlog and pipeline of commercial projects is transparent (everyone can volunteer, contribute or comment) and that everyone is encouraged to submit their ideas on what to do next. This also eliminated many of the bottlenecks we have run into in the past. We wanted the team to have the comfort of working on their goals without interruptions – so we’ve introduced work iterations and separated the development team from other teams. The process that crystallized around those assumptions gets more and more refined over time, but these add up to an agreed-upon basis of all the changes we introduce. What does that mean in practice? Let’s perhaps start with what we have in common with Scrum. We have sprints with all the prescribed Scrum ceremonies that produce potentially shippable increments. We do Daily Scrum, Sprint Planning, and Sprint Retrospectives. Our backlog is always growing, and the Release Owner orders it based on their best judgement and value calls. Have you noticed that? The first difference? We do not have a Product Owner. We tried and we are very grateful to our past Product Owners for all their contributions, passion and the great ideas they brought to inspire the team. What became apparent after working with these talented individuals is that if your product is an open source project with an elaborate business model built around it, traditional POs do not get enough space to shine. What has worked for us is actually an expression of the self-organising principle of Scrum – we put things up for a vote. Every release ends with a small “political” campaign where members of the development team present their ideas about what the focus of the next release should be. We have a few meetings discussing the ideas, some people contribute to the existing proposals, others form their own. We’ve had elections with multiple candidates and elections where one candidate asked us to vote on release themes and priorities to create guiding principles for the upcoming months. The result is a vision everyone understands and an uncanny level of transparency and insight into the plan. And, since everyone has contributed to it, we all feel responsible, engaged and encouraged to take ownership of the work that needs to be done. The Release Owner has extra powers (they decide on priorities and can overrule a tied vote) but also extra responsibilities (managing external stakeholders including the marketing team, gathering extra information, attending many meetings that the other team members can skip). As they say – with great power comes little time to do some actual coding yourself. The Backlog Grooming is of course an ongoing process. Everyone can add stuff to the backlog at any point, but it is the Release Owner who decides the order. We do hold backlog grooming meetings to which the team should bring their ideas, preferably in the form of PBIs we can all see, discuss and ask questions about. Top priority, groomed items get placed on the top of the list and are likely to be included in the next sprint’s backlog. So far we are not that far from a traditional Scrum with a PO who is part of the team. Where is the key difference? Let’s switch the perspective and ask – if you happen to be an Erlang solutions employee and part of the MongooseIM core team: does this mean you are on the sprint? Not always. Does it mean that you are a part of the sprint work? Most yes, no, maybe, kinda, can you clarify what you mean? We have people who join the sprint. At Sprint Planning they state their availability and we populate the sprint backlog based on their estimates and capacity. There are people who know they will have some availability but have no idea how much. For them, we have a neat Kanban board with community support items, research, occasional marketing requests, side projects that can bear fruit in the form of estimable PBIs for the Scrum team. There are people on commercial projects who are invaluable to us and the way we plan. They have hands-on experience with live implementations of MongooseIM, they have battle stories, insights, ideas, they know the pain points and have a unique perspective on the work. It is critical for them to know what is happening on the open source product and it is critical for the Scrum team to know what’s up with them. The two latter groups are not included in the sprint capacity and we make sure we do not depend on them to complete the sprint work, but their feedback is welcome and assistance is invaluable. Another important thing is that the division between the groups is strict only from the single sprint perspective and in no way permanent. There are commercial projects that last months and others that are completed within days. People who technically are not on the sprint are still invited to join our meetings, listen in and join slack conversations – they are much more than external stakeholders. The result of all this? We are still “scrummy” (all the roles, rules, ceremonies and artifacts are there and we get to benefit from them). We are inclusive and responsive to specific needs of team members, external stakeholders and projects. We are highly communicative and flexible. We have a structure that gives us predictability but also the capacity to change. Cool tricks There are many supplementary techniques we use in our work to make sure the attractor/frame setup is something we actually live and breath and not just sometimes brag about. Here are some examples that can inspire you to come up with ideas to introduce to your own project/team/organization. Feedback. Creating a culture of feedback is not just a way to inspect and adapt but also to provide people with a sense of agency. We’ve introduced many tools team members can use to share their thoughts with different levels of formality. Let’s, for example, explore Pigeon Post – a form where you can send a short note and a cookie (or not) to people who did something notable. The person not only gets to enjoy the cookie but can also include these notes in their appraisal process as evidence of what a great teammate they are. It’s a fun way to express appreciation to your colleagues and point out specific behaviours we value (or not – it’s not just for praise and positive feedback). Other systems we have, allow people to explicitly raise points and introduce ideas that are then included in the office-wide processes and day-to-day work. Most of the mechanisms we use in the project pipeline or for knowledge sharing purposes, stem from suggestions from the team. One important rule to keep in mind, is that when you ask for feedback or ideas, it’s on you to act on the response. Whether it’s a suggestion box, a retrospective action point, informal conversation or an official request, you have to have a timely reply. If nothing else, make a short presentation listing what you’ve done to address the ideas, preferably no later than 3 months after they have landed on your desk. If you don’t, you are not really encouraging your team to engage, rather just proving that what they say does not matter. Knowledge sharing. That is the number one reason for most of the Pigeon post cookies. “I loved your report on Helm – you get a cookie”, “I really enjoyed your Lessons Learned – here’s a cookie”, “Your workshop was awesome – One cookie coming right up!”. Developers love learning, love exploring new ideas, sharing the knowledge, discussing stuff. If your team members are afraid to speak up, unwilling to share or reluctant to talk, something has gone terribly wrong and needs to be addressed right now. Perhaps they are afraid of ridicule? Work on how safe they feel. Perhaps they feel they don’t have enough time to “waste” on knowledge sharing? There’s something wrong with the workflow. Perhaps they do not see how, where or when to let people know they have something to share? Create a Lessons Learned series! The format that worked for us is a company-wide library of presentations from different people and teams sharing stuff they find interesting. It ranges from project post-mortems, lessons learned from different projects, panel discussions to presenting a pet project or favourite technology. Whatever it is, it is welcomed, encouraged and rewarded. Not by a “knowledge-sharing bonus” or an official letter of praise but by attention and appreciation from the rest of the team. This is not only how we build expertise, but also ourselves as teachers and experts. Candy. How do you make people actually follow up on all these values and ideas? A lot has been said about what drives us , how to build or destroy motivation. My favourite tool is humour, empathy and showing people you actually like them (the prerequisite here is that you actually do). Let’s explore an example. MongooseIM Scrum Team has daily standups, but other teams and standalone consultants have separate, internal meetings to worry about. We would like them to write short standup notes every morning to let us in on what they’re doing, but we do not want to enforce it or annoy them with the request (they are very busy people!). The solution? Candy. Everyone who shares what they are doing on a given day, gets a personal visit from the Scrum Master carrying a bag of candy. They get to pick one. Ridiculous, right? No one would change their behaviour for such a small bribe. “If you want me to write standups, just make it part of my job description, we are all adults here, do not insult my intellect!” But it’s not a bribe. It’s a nice gesture, an occasion to talk for a minute without big disruptions, to quickly see how the other person is doing. It’s small and it’s fun. After a while everyone sees the benefit of the standups and they write it regardless of the reward, but the candy stays. You get to learn everyone’s favourite flavour, you get people requesting different types of chocolate, finally you get a petition asking for a “Healthy jar” with nuts as an alternative to candy. Some people choose either the candy or nuts depending on how their day went – what a great conversation starter you just earned! Your team gets closer, gets friendlier, gets engaged. Of course this technique will not work for every team, and I am not advocating for getting everyone off their healthy and balanced diet. The tool you choose for this type of exercises can be whatever you see fit, but it should: Not be too valuable or attached to salary/bonuses/promotions (or it will provoke people to try to cheat the system, see if they can break it – they are developers afterall) Be easily accessible and of similar value (we use candy for all kinds of exercises and mini-procedures) Not cause too much formality or fuss (it should be a nice gesture, a conversation starter not a potential black market currency). Want some other great ideas on how to engage people and make them enjoy their work more? Ask your team! The takeaway? The modern IT workplace is weird. It makes you think of things, observe the nature and craziness that is life. All of a sudden you see how complex systems emerge from simple rules and how trying to design or foresee something can sometimes only make things worse. Before working in IT, you might think that being proactive is better than being reactive, that defending against failure, planning for all possible cases is the smart way to go. The more you observe nature, Erlang or self-organizing teams, the more certain you will be that that’s wrong. Evolution is smarter than revolution or any sort of planning, it’s more mature, natural, it allows order to emerge painlessly. Embracing your team’s potential, allowing mistakes and experimentation helps expose their strength, building an environment that fosters creativity where traditionally it was limited and curbed. No one person can design a perfect system or process for a diverse team facing challenges that are hard to foresee. What you can do is listen, encourage and adapt. It’s more fun that way.", "date": "2020-08-20"},
{"website": "Erlang-Solutions", "title": "The sound of Erlang: How to use Erlang as an instrument", "author": ["Aleksander Lisiecki"], "link": "https://www.erlang-solutions.com/blog/the-sound-of-erlang-how-to-use-erlang-as-an-instrument/", "abstract": "The sound of Erlang When most people think of Erlang, they think of enormous business platforms handling high volumes of concurrent users. That’s often accurate, but we wanted to show off a less conventional but fun project that you can do in Erlang. Sam Aaron’s Sonic Pi is a wonderful code-based music creation and performance tool. It’s a brilliant innovation that makes use of Erlang, and it could be a big part of the future of music. You can see an example of what Sonic Pi is capable of here. Also, you can join us at Code Mesh V conferecne on 5-6 November for Sam’s tutorial – Introduction to Live Coding Music With Sonic Pi. We were inspired by this and decided to go one step further and show off how you can code music in Erlang directly. Requirements This is a table of all the software used with their versions. The theory behind the music Before we make music with Erlang, it’s worth explaining some of the underlying theory behind what sound actually is; this will come in handy later. Sound is a vibration that propagates as an acoustic wave. Frequency is the number of occurrences of a repeating event per unit of time. Its basic unit is Hz which is the number of occurrences per second. f = 1 / T The period is the duration of time of one cycle in a repeating event. The period is the reciprocal of the frequency . where: f is a frequency in Hz T is a period in seconds The simplest way of generating a wave is by providing a sine wave with the given frequency. Computers work in a discrete domain while sine waves work in a continuous domain; therefore, sampling is used to convert material from a continuous domain to a discrete one. Sampling sound is the process of converting a sound wave into a signal. The more samples we approximate, the better our sound quality will be, but this will also make the file in which we store the approximations larger. If you want to learn more about sampling I recommend watching this video. Starting a project Start by creating a new project. Since the script is small, I will use the escript template: rebar3 new escript the_sound_of_erlang Let’s check if the project worked correctly: cd the_sound_of_erlang\nrebar3 escriptize\n./_build/default/bin/the_sound_of_erlang\nmkdir out If everything has gone to plan, then you should see the following output: $ rebar3 escriptize\n===> Verifying dependencies...\n===> Compiling the_sound_of_erlang\n===> Building escript...\n\n$ ./_build/default/bin/the_sound_of_erlang\nArgs: [] Well done! First wave Lets now generate an example wave. To do this we can just: wave () -> [math: sin (X) || X <- lists: seq (1, 48000)]. To save a generated wave, we need to transform a list of floats to binary representation and write this binary to a file. save (Filename, Wave) -> Content = lists: foldl ( fun (Elem, Acc) -> << Acc / binary, Elem / float >> end , << \"\" >> , Wave),\n    ok = file: write_file (Filename, Content). Call the above 2 functions in the main/1 function as follows: main (_) -> Wave = wave (), save (\"out/first_wave.raw\", Wave),\n    erlang: halt (0). Build the script binary with: rebar3 escriptize And run it with: ./_build/default/bin/the_sound_of_erlang A new file called out/first_wave.raw should show up in a repository. Then use ffplay to listen to the result: ffplay -f f64be -ar 48000 out/first_wave.raw The options given are: -f f64be means that input format is 64-bit big-endian float -ar 48000 means that the input audio sampling rate is 48000 per second You can listen to the result in the video below. It’s not a pleasant sound (yet), but it’s a start. For the sake of convenience, let’s try to play the resulting sound from the script: play (Filename) -> Cmd = \"ffplay -f f64be -ar 48000 \" ++ Filename,\n    os: cmd (Cmd). and add it to the end of main/1 : main (_) -> Wave = wave (),\n    Filename = \"out/first_wave.raw\", save (Filename, Wave), play (Filename),\n    erlang: halt (0). Now we can recompile the script and run it with: rebar3 escriptize && ./_build/default/bin/the_sound_of_erlang We can also convert a raw file to a .mp3 format with: ffmpeg -f f64be -ar 48000 -i out/first_wave.raw out/first_wave.mp3 That will enable it to be played with any music player. We have managed to generate a wave, save it to a file and play it. Tuning in Now that we can make sound let’s improve our wave, so it’s not just any random sound, but a fixed frequency for a given amount of time. To have several samples played in a given amount of time, we need to multiply the sample rate times and sound duration, since the number of samples is an integer we should round the multiplication result. NumberOfSamples = round(SampleRate * Duration) The sinus period is 2 * PI, because we know that and the sample rate, we can calculate how long each signal step will last for a given frequency Hz . Step = Hz * 2 * math: pi () / SampleRate Knowing the number of samples and the Step, we can map the time domain to a signal as follows: frequency (Hz, Duration, SampleRate) -> Signals = lists: seq (1, round(SampleRate * Duration)),\n    Step = Hz * 2 * math: pi () / SampleRate,\n    [ math: sin (Step * Signal) || Signal <- Signals ]. Let’s now modify a wave/0 function to get a sound of 440 Hz played for 2 seconds with a sampling rate of 48000 samples per second: wave () -> frequency (440, 2, 48000). I will change the Filename in the main/1 function to Filename = \"out/2Sec440Hz.raw\", Just add it to a repository. You can listen to the result below. Let’s play the new sound and compare it with the same frequency of sound from YouTube. To me, they sound identical. rebar3 escriptize && ./_build/default/bin/the_sound_of_erlang We can now try playing two sounds with different frequencies and lengths of times, but we need to flatten the list of signals to make a list of signals from a list of lists of signals. Filename = \"out/2Sec440HzAnd1Sec500Hz.raw\",\n... wave () -> lists: flatten ([ frequency (440, 2, 48000)\n      , frequency (500, 1, 48000)\n    ]). You can listen to the result below. Frequency to note We can play a given frequency for a given amount of time, but how do we make music out of that? From here we can see that the frequency of an A4 note is 440 Hz which is also known as pitch standard . We can also lookup all other needed notes in the same way, but there is an alternative called the frequency ratio of a semitone, and it is equal to the twelfth root of two 2 ** (1 / 12) . To calculate A#4 which is 1 semitone higher than A4 we just multiply 440 * (2 ** (1/12)) = 466.16 and, after comparing it to the table the value is A#4 the correct corresponding frequency. Let’s try translating the maths into code: At the top of the file just below the module directive, we can add a macro for pitch standard and extract the sampling rate. -module(the_sound_of_erlang).\n\n-define(PITCH_STANDARD, 440.0).\n-define(SAMPLE_RATE, 48000).\n\n-export([main / 1]). Now we need to use the macro for the sampling rate in the code. Let’s modify frequency/3 to frequency/2 : frequency (Hz, Duration) -> Signals = lists: seq (1, round( ? SAMPLE_RATE * Duration)),\n    Step = Hz * 2 * math: pi () / ? SAMPLE_RATE,\n    [ math: sin (Step * Signal) || Signal <- Signals ]. Do not forget to change the calls of the frequency function by removing the last argument: wave () -> lists: flatten ([ frequency (440, 2)\n      , frequency (500, 1)\n    ]). and play/1 as follows: play (Filename) -> StrRate = integer_to_list( ? SAMPLE_RATE),\n    Cmd = \"ffplay -f f64be -ar \" ++ StrRate ++ \" \" ++ Filename,\n    os: cmd (Cmd). The following function takes the number of semitones to be shifted and returns a frequency of a shifted sound: get_tone (Semitones) -> TwelfthRootOfTwo = math: pow (2, 1.0 / 12.0), ? PITCH_STANDARD * math: pow (TwelfthRootOfTwo, Semitones). We need to introduce one more concept, beats per minute, which is the base time unit for a note to be played. Each note is played in a given number of beats and the number of beats per minute is fixed, so we can calculate how long each beat lasts (in seconds) by dividing 60 by beats per minute. Let’s introduce a new function for that: beats_per_minute () -> 120. beat_duration () -> 60 / beats_per_minute (). We can generate notes for a given amount of time with the following function: sound (SemitonesShift, Beats) -> frequency ( get_tone (SemitonesShift), Beats * beat_duration ()). Let’s try it out by providing the following wave: wave () -> lists: flatten ([ sound (SemiTone, 1) || SemiTone <- lists: seq (0, 11)\n   ]). And play it by recompiling and running the script. I saved my output as \"out/increasingSemitones.raw\" . You can listen to the below. I only need some of these notes to play my songs, but you might need more, so I provided them in the semitones_shift/1 function. Let’s provide a helper function for easier sound notation: note (Note) -> SemitonesShift = semitones_shift (Note), get_tone (SemitonesShift). semitones_shift (c4) -> - 9; semitones_shift (c4sharp) -> - 8; semitones_shift (d4flat) -> - 8; semitones_shift (d4) -> - 7; semitones_shift (d4sharp) -> - 6; semitones_shift (e4flat) -> - 6; semitones_shift (e4) -> - 5; semitones_shift (f4) -> - 4; semitones_shift (f4sharp) -> - 3; semitones_shift (g4flat) -> - 3; semitones_shift (g4) -> - 2; semitones_shift (g4sharp) -> - 1; semitones_shift (a4flat) -> - 1; semitones_shift (a4) -> 0; semitones_shift (a4sharp) -> 1; semitones_shift (b4flat) -> 1; semitones_shift (b4) -> 2; semitones_shift (c5) -> 3; semitones_shift (c5sharp) -> 4; semitones_shift (d5flat) -> 4; semitones_shift (d5) -> 5; semitones_shift (d5sharp) -> 6; semitones_shift (e5flat) -> 6; semitones_shift (e5) -> 7; semitones_shift (f5) -> 8; semitones_shift (f5sharp) -> 9; semitones_shift (g5flat) -> 9; semitones_shift (g5) -> 10; semitones_shift (g5sharp) -> 11; semitones_shift (a5flat) -> 11; semitones_shift (a5) -> 12. and modify slightly the sound/2 function as follows: sound (Note, Beats) -> frequency ( note (Note), Beats * beat_duration ()). To use the more convenient, newly created note/1 function instead of get_tone/1 . Now we can try out the sounds played by modifying the wave/0 function as follow: wave () -> lists: flatten ([ sound (Note, 1) || Note <- [\n           c4, c4sharp, d4flat, d4, d4sharp, e4flat,\n           e4, f4, f4sharp,g4flat, g4, g4sharp,\n           a4flat, a4, a4sharp, b4flat, b4\n       ]\n]). I will save the result in \"out/increasingNotes.raw\" file. You can listen to the result below. ADSR When you listen to the increasing notes, you will notice that there is a very strange tick or thudding sound as the note changes. This is because the sound increases and decreases too rapidly. To resolve this issue, we can implement ADSR which stands for *A*ttack *D*ecay *S*ustain *R*elease and works by modifying the sound amplitude (volume) according to the following chart: For the sake of simplicity, it is enough to only implement the Attack and Release components because we already have the Sustain part. To implement the Attack, we will consider a sequence of numbers smaller or equal to 1 that will be generated in the following way: attack (Len) -> [ min (Multi / 1000, 1) || Multi <- lists: seq (1, Len)]. An example of this kind of list may look like this: [0.001, 0.002, ... 0.999, 1, 1, 1, ..., 1] We can also generate the Release a lazy way: release (Len) -> lists: reverse ( attack (Len)). The release function generates the following list: [1, 1, 1, ..., 1, 0.999, ..., 0.002, 0.001] Now we need to slightly modify the frequency/2 to adjust the sound volume: frequency (Hz, Duration) -> Signals = lists: seq (1, round( ? SAMPLE_RATE * Duration)),\n    Step = Hz * 2 * math: pi () / ? SAMPLE_RATE,\n    RawOutput = [ math: sin (Step * Signal) || Signal <- Signals ],\n    OutputLen = length(RawOutput),\n    lists: zipwith3 ( fun (Attack, Release, Out) -> Attack * Release * Out end , attack (OutputLen), release (OutputLen), RawOutput). I saved the result as out/increasingNotesASR.raw . Now when you rebuild and run the script, you will hear a smooth transition as we pass between the notes. You can listen to the result here . Get ready to face the music Now we will try to play the actual song. Let’s modify the wave/0 function as follows: wave () -> lists: flatten ([ sound (f4, 0.5)  \n    , sound (d4, 0.5)  \n    , sound (d4, 0.5)  \n    , sound (d4, 0.5)  \n    , sound (g4, 2)  \n    , sound (d5, 2)  \n    , sound (c4, 0.5)  \n    , sound (b4, 0.5)  \n    , sound (a4, 0.5)  \n    , sound (g5, 2)  \n    , sound (d5, 1)  \n    , sound (c4, 0.5)  \n    , sound (b4, 0.5)  \n    , sound (a4, 0.5)  \n    , sound (g5, 2)  \n    , sound (d5, 1)  \n    , sound (c4, 0.5)  \n    , sound (b4, 0.5)  \n    , sound (c4, 0.5)  \n    , sound (a4, 2)  \n    , sound (d4, 1)  \n    , sound (d4, 0.5)  \n]). Also, change the beat per minute to 120. The reasoning behind setting given beats per second can be found here but it is out of the scope of this article so I will not go into further details. beats_per_minute () -> 120. You can recompile and run the script or just listen to the result below. The result can be saved to out/StarErlang.raw. I hope you recognize the melody I picked, it is the Star Wars Theme. Last but not least, let’s introduce an Erlang behaviour for a melody. Create a new file src/melody.erl and define a melody behaviour. -module(melody).\n\n-type note () :: c4 | c4sharp | d4flat | d4 | d4sharp | e4flat | e4 |\n                f4 | f4sharp | g4flat | g4 | g4sharp | a4flat | a4 |\n                a4sharp | b4flat | b4 | c5 | c5sharp | d5flat | d5 |\n                d5sharp | e5flat | e5 | f5 | f5sharp | g5flat | g5 |\n                g5sharp | a5flat | a5.\n-type duration () :: float().\n\n-callback beats_per_minute () -> non_neg_integer ().\n\n-callback sounds () -> { note (), duration ()}. Each melody may have different beats per minute, so this is the first function is needed to describe a song and the second function is the notes the song consists of. There are two types to be introduced: note() which is one of the possible notes (aka sound frequencies) and the duration() which is a float saying how many beats the sound of a given frequency will last. To use a song defined in a different module with a slightly simplified notation, let’s add a new macro which will store the module name in which the song is defined: -define(SONG, star_wars_main_theme). and modify wave/0 and beats_per_minute/0 functions to use it: beats_per_minute () -> ? SONG: beats_per_minute (). wave () -> RawSounds = ? SONG: sounds (),\n    Sounds = lists: map ( fun ({Note, Duration}) -> sound (Note, Duration) end , RawSounds),\n   lists: flatten (Sounds). This will not work yet as there is no star_wars_main_theme module defined, so create a file src/songs/star_wars_main_theme.erl and implement the melody behavior: -module(star_wars_main_theme).  \n\n-behaviour(melody).\n\n-export([sounds / 0, beats_per_minute / 0]). beats_per_minute () -> 120. sounds () -> [\n    ,   {d4, 0.5}\n    ,   {d4, 0.5}\n    ,   {d4, 0.5}\n    ,   {g4, 2}\n    ,   {d5, 2}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 0.5}\n    ,   {g5, 2}\n    ,   {d5, 1}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 0.5}\n    ,   {g5, 2}\n    ,   {d5, 1}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {c4, 0.5}\n    ,   {a4, 2}\n    ,   {d4, 1}\n    ,   {d4, 0.5}\n    ,   {g4, 2}\n    ,   {d5, 2}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 0.5}\n    ,   {g5, 2}\n    ,   {d5, 1}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 0.5}\n    ,   {g5, 2}\n    ,   {d5, 1}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {c4, 0.5}\n    ,   {a4, 2}\n    ,   {d4, 1}\n    ,   {d4, 0.5}\n    ,   {e4, 1.5}\n    ,   {e4, 0.5}\n    ,   {c4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 0.5}\n    ,   {g4, 0.5}\n    ,   {g4, 0.5}\n    ,   {a4, 0.5}\n    ,   {b4, 0.5}\n    ,   {a4, 1}\n    ,   {e4, 0.5}\n    ,   {f4sharp, 1}\n    ,   {d4, 1}\n    ,   {d4, 0.5}\n    ]. You can listen to the result below. We have also provided a few more examples. Sound of Silence Super Mario Bros. We hope you enjoyed this tutorial, If you want to see the full code go to https://github.com/aleklisi/The-Sound-Of-Erlang we’re looking forward to hearing your examples of your favourite song, played in Erlang. You can share them with us on Twitter. Sources: https://en.wikipedia.org/wiki/Frequency https://en.wikipedia.org/wiki/Sound https://en.wikipedia.org/wiki/Sampling_(signal_processing) https://youtu.be/f53m72uLa2I https://pages.mtu.edu/~suits/notefreqs.html https://en.wikipedia.org/wiki/Twelfth_root_of_two https://courses.lumenlearning.com/suny-musicappreciationtheory/chapter/introduction-to-tempo/ https://youtu.be/FYTZkE5BZ-0", "date": "2020-09-08"},
{"website": "Erlang-Solutions", "title": "What’s new in MongooseIM 4.0 – The friendly Mongoose", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/whats-new-in-mongooseim-4-0-the-friendly-mongoose/", "abstract": "Hello from the team at MongooseIM It’s been busy four months. As most of us were locked in our homes, we decided to put it to use and prepare a really special release. We introduced a new configuration format, structured logging and many new features and extensions that add up to the product we are proud to share with you. MongooseIM has always empowered users to create a customised, owned chat application without the struggle of building one from scratch, now we’ve made these amazing features even more accessible and easy to use. Friendly to developers with TOML configuration We want everyone to be able to benefit from MongooseIM, and so it was a rude awakening to hear the configuration described as ‘the trenches of the Somme’ by one of our users. Given we love Erlang, we hadn’t considered that its configuration might be a barrier for some developers. Once we read the feedback we knew that had to change. In the release of 4.0 we are introducing a new configuration format. For that task we’ve decided to go with TOML. Thanks to its merits we managed to get rid of most of the nested lists and tuples that sometimes were followed by a comma and at other times by a dot. We have simplified the syntax and disassembled the trenches while keeping the required configuration capabilities. Friendly to Kubernetes with Helm Charts We all like to have the installation procedure as simple as installing a package. So we’ve made it possible to install MongooseIM and MongoosePush on Kubernetes through the Helm Chart. You can find the Helm Charts of our organisation at the link below: https://artifacthub.io/packages/search?page=1&org=mongoose Friendly to DevOps with structured logging In MongooseIM 4.0 we’re introducing structured logs. This can help to have more precise and clearer structure when we query events that are logged. This is a tool you may need not often, but when you do need it, you’ll be so glad you have it because it makes it significantly easier to find exactly what you’re looking for. If you are not yet familiar with the new OTP logger and structured logs we recommend having a look at this https://ferd.ca/erlang-otp-21-s-new-logger.html blogpost. Friendly for users with video and voice calling With the new release, we added the implementation for XEP-0215: External Service Discovery which assists in discovering information about services external to the XMPP network. The main use-case is to help discover STUN/TURN servers to allow for negotiating media exchanges. So if you want to have a video/voice call using MongooseIM and Conversations now you can. You can use MongooseICE as a STUN/TURN relay, configure MongooseIM with mod_extdisco enabled and start having video calls between connected users. For more details on how to use and setup mod_extdisco and our STUN/TURN server stay tuned to our future blog posts. Friendly for everyone with improvements to MongoosePush We’ve released a new MongoosePush. In the 2.1 release you will find: OpenAPI Specs Phoenix as the Web Framework Structured Logs, logfmt and JSON formatters Metrics: Exometer to Telemetry, Multidimensional metrics Many improvements in the testing pipeline For more information on the new MongoosePush, please have a look at the release notes https://github.com/esl/MongoosePush/releases/tag/2.1.0 Friendly for managers with AMOC 2.1 Load testing We released AMOC 2.1. This release focuses on the REST API, which is now powered by OpenAPI Specifications generated by openapi-generator. We’ve also significantly reworked the RestAPI so you can upload files with simple put requests. With the newly introduced documentation API for scenarios you can now check what the scenario is about before running it. Finally, the execution API was updated and now you have full control of options such as starting, stopping scenario, adding, removing users. This makes load testing even easier so you can demonstrate the value of MongooseIM to your management team. Let’s be friends! So if you ever considered MongooseIM for your product or a project but you didn’t choose it for some reason, it’s time to give it a try. It’s the most robust, scalable and now easiest to configure Instant Messaging solution available on the market. Learn more about how MongooseIM stacks up against the competitors in terms of key considerations like costs and features in our complete guide to choosing a messaging app. Or, explore the MongooseIM page. One last word from your friends at MongooseIM After working hard to get the new release live, we wanted to show off a little creative spirit. Here’s the MongooseIM’s team summary of MongooseIM 4.0 as inspired by the theme song to Friends ! So no one told your MongooseIM 4.0 was gonna be this way When your app’s won’t scale, you’re broke Your XMPP life’s DOA It’s like you’re always stuck with a single node When it hasn’t been your day, your week, your month Or even your year, but MongooseIM will be there for you (When the rain of messages starts to pour) MongooseIM will be there for you (When you like to configure with TOML) MongooseIM will be there for you (‘Cause structured logs are for people too)", "date": "2020-10-14"},
{"website": "Erlang-Solutions", "title": "How to use Lua for flexible configurations in Erlang and Elixir", "author": ["Manuel Rubio"], "link": "https://www.erlang-solutions.com/blog/how-to-use-lua-for-flexible-configurations-in-erlang-and-elixir/", "abstract": "When I need to configure something in a complicated way, I find myself reviewing the embedded language that provided the server to create a flexible configuration. In Redis, you can improve the performance of requests, in Nginx, you can improve the handling of incoming requests, FreeSwitch offers alternatives for performing the same tasks using different embedded languages. Even in a software like TheGimp, you can add your own code to make edit images. Among the embedded languages, JavaScript and Lua are the most commonly used languages. JavaScript is very well known to the Erlang community because it was integrated (as a port, it is not implemented on top of Erlang) in popular products such as CouchDB and Riak. But I think the more exciting option, raised by Erlang Co-Creator, Robert Virding, is to implement Lua on top of Erlang, which can be used as an embedded language. Why? Let’s take a look. Complex Configuration Usually, when tasked with fitting the definition of a behaviour we would like to configure, we would create an algorithm in a simple language such as Lua. This saves us from performing activities like: defining the configuration to fit with all of the cases, reading and transmitting that information to be prepared for use, writing a specific code to handle that standardised information. The kind of implementation is used frequently. It is easy to think of examples you’re likely to come across in day-to-day life. For example, supermarket offers which have multiple dependencies, commissions for salespeople which might feature variable ranges and percentages based on the type of sale, amount of sale or tax brackets, even SMS, emails or HTTP requests could be considered examples. To demonstrate this, let’s look at an example of developing a load balancer. This is a simple Erlang project using cowboy as a dependency, and depending on the headers and other information from the HTTP request, we can send it to the different web servers we have available and configured. Based on the above premise we can write the configuration as follows: {load_balancer, [\n    {servers, [\n        {odin, \"1.1.1.1\", [\n            {in, method, [post]},\n            {'>', <<\"Content-size\">>, 10000},\n            {in, <<\"Accept\">>, [<<\"json\">>]}\n        ]},\n        {thor, \"1.1.1.2\", [\n            {in, method, [get, post]},\n            {'==', http_version, <<\"2\">>}\n        ]},\n        {balder, \"1.1.1.3\", [\n            {in, method, [get, post]}\n        ]}\n    ]}\n]}. As you can see, we have to define a 3-tuple system with the operation in the first element and the two operators as the following elements inside of the tuple. In addition, we are occasionally handling the second element as the header name (if it is a binary), but at other times it’s the method we use to perform the request (using the atom “method”) and other times still, the HTTP version is used to gather the information. The problem is that we have no closed specifications. We could add more elements or even change the meaning of them. What if we want to use logical modifiers like “and” and “or” to join the checks instead of assuming they are always using “and”? This change will add more complexity to our configuration and more complexity means more possibilities for making mistakes. At the moment, if the configuration is wrong or is adding something that is not granted, it is up to us to trigger the corresponding error and point to where it is to make it easier to fix. As you can imagine, that is not an easy thing to do if you are handling it in the runtime. Lua saves the day! It’s not unconventional to think about configuration in terms of a specific code. At this point, Lua code could be put in charge of the definition because it is based on Lua semantics. We only need the information for the configuration and running of the snippet to give us the desired behaviour we want to plug into the correct place. For example, the previous configuration could be written as: local odin = \"1.1.1.1\"\nlocal thor = \"1.1.1.2\"\nlocal balder = \"1.1.1.3\"\n\nlocal method = http.method()\nlocal size = http.header(\"Content-size\")\nlocal accept = config.split(http.header(\"Accept\"), \", \")\nlocal httpver = http.version()\n\nif method == \"post\" and size > 10000 and member(\"json\", accept) then\n    return odin\nelseif config.member(method, {\"get\", \"post\"}) and httpver == \"2\" then\n    return thor\nelseif config.member(method, {\"get\", \"post\"}) then\n    return balder\nend As you can see, we are able to optimise and fix the code to suit our needs, it is shorter and clearer than the original configuration and, most importantly, we can now test and check to be sure it is compiling correctly. The important thing to keep in mind is that the configuration code must include the functions which are going to be needed to handle the request. In the example above, we are using functions like http_version() , http_header(\"...\") or even split(...) and member(...) . These functions should be provided to the interpreter. Depending on the underlying language you are using, you can develop these functions in Erlang or Elixir. Of course, the interpreter also has other functions available, we only need to provide the specific functions that are required for our business logic. In addition to improving the performance, using these functions we are also improving the security because it has not been able to access functions which are not used. You can check the file luerl_sandbox.erl where it is removing the access to the functions which use the underlying operating system. Where the code dwells? Inserting the Lua code into the configuration can be a little tricky. To avoid this, I recommend putting these scripts into the priv directory as a normal Lua file (using the extension .lua) this could even be done inside of a database if we are handling the configuration in an automated way using a key/value storage configuration such as etcd. The most important thing to keep in mind before running that code is to have a specific task which helps you to parse it and ensure the code is correct. One solution is to conduct a testing phase to ensure that the configuration is not breaking or negatively impacting other parts of the system. For example, in the previous code, we would write a couple of libraries, one called utils for the functions needed for strings and tables and another called http needed for the HTTP functions. An example would be: -module(luerl_lib_utils).\n-export([load/1, install/1]).\n\n-include_lib(\"luerl/include/luerl.hrl\").\n\nload(St) ->\n    luerl:load_module([<<\"utils\">>], luerl_lib_utils, St).\n\ninstall(St) ->\n    luerl_heap:alloc_table(table(), St).\n\ntable() ->\n  [\n    {<<\"split\">>, #erl_func{code = fun split/2}},\n    {<<\"member\">>, #erl_func{code = fun member/2}}\n  ].\n\nmember([Entry, Table], St) ->\n  #table{a = Array} = luerl_heap:get_table(Table, St),\n  Result = array:foldl(fun\n    (_, V, false) when V =:= Entry -> true;\n    (_, _, Acc) -> Acc\n  end, false, Array),\n  {[Result], St}.\n\nsplit([String, Sep], St) ->\n  {[string:split(String, Sep, all)], St}. As you can see, we are implementing a couple of functions which will be available on our Lua interface under the utils package. To load these functions, we have to run the function load which is exported in the previous module. Then we can load the file: {ok, Form, St2} = luerl:loadfile(\"config.lua\", St1). This step will help us obtain the forms that can be run in the following step. We will also obtain the modified state. To do this we just need to run the code: luerl:eval(Form, St2). The output for this code should be the return performed by the Lua code in an ok-tuple if everything was correct, or an error-tuple if something went wrong. Scaling up Lua scales up because it is built on top of Erlang. This means Lua is using the same processes as Erlang does, it also means we are not using ports to communicate with the Lua interpreter, we have the Lua interpreter running on Erlang. This makes a great difference in comparison to JavaScript because if you are handling millions of requests and all of them require a JavaScript snippet, this can cause a bottleneck very quickly if you have to limit the number of ports or requests. On the other hand, Lua is using native Erlang functions when it calls the functions we want to provide for the interpreter. That makes a clear improvement, saving us from performing data serialisation or transformation. Conclusions Using a language for flexible configuration gives us the possibility to create an easy interface to provide configurations, reduce the amount of code we need to write, improve the maintenance without jeopardising the performance of the system. At the moment, you can use Lua as we have explained during the article or you can jump into PHP if you need to process text or templates on top of Erlang or Elixir. Alternatively you can join the community and provide other solutions which help us build better fit-for-purpose software. Let us know if you need help building your system with one of these solutions. We look forward to hearing from you.", "date": "2020-11-10"},
{"website": "Erlang-Solutions", "title": "Erlang Solutions partners with RE2 Programming to deliver Erlang, Elixir & RabbitMQ Online Training and to Grow the Community in South East Asia", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/erlang-solutions-partners-with-re2-programming-to-deliver-erlang-elixir-rabbitmq-online-training-and-to-grow-the-community-in-south-east-asia/", "abstract": "Erlang Solutions, the world’s leading provider of Erlang and Elixir solutions, and RE2 Programming, an IT training organisation dedicated to helping developers in the Asian market reap the benefits of the BEAM, will work together to offer world-class online training in a format and time zone that caters to developers in South East Asia. By partnering with Erlang Solutions, they’ll have access to the expert trainers as well as Erlang Solutions’ experience delivering interactive online training, taught by some of the most experienced developers in the community, including language co-creators like Robert Virding. “We are delighted to be partnering with RE2 Programming to bring our world class Elixir and Erlang training courses to an Indian audience in a format and at times that are suited to their needs. We have significant experience providing training globally, and our new partners will help us extend that into the new Indian marketplace. Erlang and Elixir are currently experiencing unprecedented demand and developers with those skills are building some of the world’s most scalable, reliable and fault tolerant systems” – stated Stuart Whitfield, CEO of Erlang Solutions. The benefits to both parties are clear with RE2 Programming able to benefit from the most experienced training team on the BEAM. “Both Erlang Solutions and RE2 Programming share a common goal of spreading Erlang and Elixir expertise across the globe, helping individuals, as well as companies, achieve their maximum potential. There is a high demand for Erlang and Elixir developers that companies find difficult to hire for. The skillset is in high demand but there is a gap between experienced professionals able to implement them. To help the Erlang and Elixir community grow across the Asian market and bring this badly kept secret to the engineering professionals on the eastern side of the globe, RE2 Programming has partnered with Erlang Solutions to deliver their world-class Elixir and Erlang training courses. This partnership is of key importance for both Elixir and Erlang communities and will immensely benefit the Indian IT industry. The aim is to spread the love for BEAM technologies across the Asian market; with the initial launch in India” – shared Hetal Vora, CEO of RE2 Programming. About Erlang Solutions Erlang Solutions is a tech consultancy that builds transformative, fault-tolerant solutions for the world’s most ambitious companies. By providing user-focused consultancy, high tech capabilities and connection to diverse communities, Erlang Solutions has built strong and long-lasting relationships with clients including WhatsApp, Klarna, Motorola, PepsiCo, Cisco, Ericsson and Adidas Runtastics to name a few. About RE2 Programming RE2 Programming’s vision is to help IT professionals thrive in their careers as well as fulfil the demand and supply gap for Erlang and Elixir engineers across the world through world class training delivered by Erlang Solutions. They aim to spread the love for BEAM technology to the eastern world, helping the future IT Industry evolve and grow. The founder of RE2 Programming Hetal V. identified the need for introducing functional programming to the Asian markets. RE2 Programming has been set up to bring the best practices for building technology that is highly scalable, resilient and fault tolerant. Find out more about partnership opportunities .", "date": "2020-11-17"},
{"website": "Erlang-Solutions", "title": "Introducing Caramel – An Erlang Backend for the OCaml compiler that provides a fast type-checker for BEAM-based technologies", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/introducing-caramel-an-erlang-backend-for-the-ocaml-compiler-that-provides-a-fast-type-checker-for-beam-based-technologies/", "abstract": "Type-checking for the BEAM is one of the most common barriers for adoption. From comments on our LinkedIn posts to threads on HackerNews, it is something that we know can make people nervous. For those in the Erlang and Elixir community, tools like Dialyzer have done a great job of meeting some of the core requirements where type-checking would be necessary, but it’s also understandable that they’re not the right tool for every user. As a result, there has been a substantial increase in the number of languages and frameworks that looks to address this need. At Code BEAM V, Anton Lavrik announced that Facebook would be releasing their own version of statically-typed Erlang. Our friend Louis Pilford has also developed Gleam, another fast and performant language that works on the BEAM VM and offers static typing. Why Static-Typing Erlang is a dynamically and strongly typed language that is famous for allowing teams to reduce the lines of codes needed in their system. As a result, the teams required for many Erlang systems have been small (just think of Whatsapp managing 900 million users with a backend team of only 50 server-side developers). However, as the need for scalable, distributed systems has grown (and with it, the demand for technologies that offer the benefits of BEAM-based technologies) the interest in Erlang and Elixir from broader teams, with systems built in a mix of technologies has also grown. The growing demand for BEAM based technologies has led to an increased desire to make it easier to catch errors at the time of compiling before they enter the production system. In theory, this could allow developers to create the ultimate crash proof systems, with the type checker catching everything before production and the BEAM VM, offering all the ‘let it crash’ benefits for handling failure. Enter Caramel – the OCaml backend for the BEAM The newest approach to type-checked Erlang has been created by our colleague Leandro Ostera. This began as a hobby project put together in a matter of weeks to help him understand why it is so hard to offer statically typed Erlang. As more people found out about the project, it’s popularity rapidly grew, and with good reason. Caramel now offers a highly expressive type-system and a blazingly fast type checker. As a result, your team can rule out entire classes of errors by using types that are closer to your domain. Who is likely to benefit most from Caramel As you can imagine, a system like this will be highly beneficial for those dealing with complex state machines. Caramel allows for a type-level representation of your business domain logic this in turn rules out illegal representations, making them entirely unrepresentable. Using Caramel, you will have a tool that tells you instantly if you are about to do something which you should not. Caramel will also make fearless refactoring of large codebases significantly easier to achieve. Why OCaml Rather than reinvent the wheel when implementing the type-checker for Erlang, Leandro chose to leverage off of the existing OCaml language with over 25 years of development. OCaml was originally designed for systems where correctness was critical. As a result, it has an extremely powerful type system that has benefitted from significant time (we’re talking millions of hours) and financial (millions of dollars) investment to produce the right tool for the job. As well as its type checker, Caramel will allow Erlang developers to use OCaml’s powerful module system, which will help with code reuse, enforcing codebase conventions and large scale refactors. Caramel features The first public release of Caramel in October features: caramelc the Caramel compiler erlang an OCaml library for handling Erlang code, including an AST, lexer, parser, and printer The Caramel compiler Caramel can compile a large portion of the OCaml language, in particular most of the immutable functional OCaml, and into idiomatic Erlang code. Erlang library for OCaml The Erlang library currently includes support for the vast majority of the Standard Erlang language, according to the grammar shipped with Erlang/OTP as of version 24. It includes a series of modules to make it easy to construct Erlang programs from OCaml, as well as deconstruct them and operate on them. What’s next: Given the core components that currently exist within Caramel – there is an exciting and ambitious road map ahead. In the future, we could see a situation where any Erlang or OCaml code can be translated and implemented interchangeably, making the world-class strengths of both languages effortlessly available to anyone who is familiar with either language. To learn more about how you can help make that happen head to Caramel on Github. Until then, if you’d like to talk to our expert team about how we can build reliable, scalable solutions to meet your needs, get in touch. We’re always happy to help.", "date": "2020-11-18"},
{"website": "Erlang-Solutions", "title": "A guide to tracing in Elixir!", "author": ["Gabor Olah"], "link": "https://www.erlang-solutions.com/blog/a-guide-to-tracing-in-elixir/", "abstract": "Greetings! I’m Gabor, an Erlang programmer. I like tracing, and I use it whenever I need it. When I started working with Elixir, I asked the obvious question, how do I do tracing in Elixir-land? The Elixir community have improved the user experience for so many things (mix vs pre-rebar3, macros vs parse-transforms, etc, etc, ad-infinitum). Naturally, I was looking forward to using all the great tools for tracing in Elixir. Although there are some good tools, namely Rexbug and Tracer , I was a bit surprised to see that there is very little talk about tracing. I couldn’t find all the great blog posts, tutorials explaining the battle stories, and the wonders of tracing. There are some, but this powerful feature of the BEAM virtual machine seems to be somewhat underrepresented in the Elixir community. In this blog post, I will discuss “what is tracing”, and “when is it appropriate to use tracing?” and provide a high-level overview of the available tracing tools for the BEAM. What is tracing? Tracing is defined by Wikipedia as the following: “tracing involves a specialised use of logging to record information about a program’s execution.” This definition is not only useful but also provides an idea of where tracing fits in the spectrum of logging and debugging. Logging is an invaluable tool to see what is happening in our system, it can give insights to errors and bugs. But this narrow definition has a big flaw, the places where we choose to emit log messages are hard-coded into our program logic. Choosing where to insert log messages is entirely subjective, dependent upon assumptions and prior experience. Some of the time the logging statement is inserted into a useful location, but if not, you will have to recompile your code and insert extra log statements, this can be a very tedious activity (and is the norm in the vast majority of computer languages). In the pathological scenario, gratuitous use of log statements can even become a significant drain on system IO (process 500 records, generate 50000 lines of logging statements). Logging can be useful in order to explain what the system is doing, but is less helpful for understanding system behaviour in an unexpected failure state. At the other end of the spectrum is debugging with the traditional step-by-step debugger such as the GNU debugger (gdb). This requires no modification of your code, we observe the program as it executes and the data as it evolves one step at a time. This is a fine way to observe how an algorithm works or how a data structure changes. But it is very problematic in a distributed system because many operations are handled by transient processes with short timeouts, taking a single example, gen_server has a call timeout of 5000 milliseconds, and there are very few Elixir or Erlang systems which are not making significant use of OTP to implement system functionality. The key here is observing the system while it is working without stopping the world. Does that sound impossible? Here comes tracing to the rescue. Tracing is a window through which we can observe the machinations of the system. Starting with top-level function calls and message passing with configurable stack traces, right the way through to monitoring the behaviour of the scheduler, port interaction, and garbage collection. In this regard, tracing is very close to a step-by-step debugger, but it turns the debugging steps into a stream of log messages which we can analyse at our leisure in order to determine what was happening in the system at a given point in time. Myths If you have good monitoring, then you don’t need tracing. Let us assume that diligent monitoring is already in place. Tracing is by no means a substitute for logging. But monitoring has the same problems as logging. If you know beforehand what are the parts to monitor for every possible event, then you will probably never need tracing. But for any sufficiently complex application, you cannot monitor everything. So there will be many parts of the system, which are not measured. This is where tracing comes very handy. It is not safe to trace in production. Doing anything with production carries a certain amount of risk. This is also true for tracing. You should be careful, as it has a minimal amount of overhead. You need to understand what tracing does and how it affects certain parts of the system. Probably you want to test the tracing activity in a test environment first, but saying it is not safe is not true. With due diligence, you get a lot of ad-hoc logging for free by leveraging the capabilities of the BEAM. Elixir’s success is partly because it successfully leveraged the concurrency and distribution features of the BEAM. This is another free meal once you learn to utilise it. You should change the architecture so that you don’t need tracing. If you have the power to do so, you have probably found the right solution. But there are many systems where upgrading the system isn’t straightforward. Just think about a system that needs to be certified before it can be put into production. “Being fast and breaking things” is not allowed everywhere. These are usually the environments where observation tools like tracing are most utilised. But you don’t need to be in a highly regulated environment to enjoy the benefits. Starting a trace takes less time than deploying a change, it can save time when it is needed most… when things are on fire. I use OpenTracing, I’m covered. OpenTracing is a really good way to keep track of requests in your whole product including front-end, back-end and infrastructure. But it requires you to instrument your code. It is good to track the business specific part of the code, but again, it has the same problem of logging. It is not ad-hoc. You can’t just easily add extra deeper layers, or see the system internals of the BEAM. If you use sampling, then capturing the event you are looking for is not even guaranteed. Unfortunately, it shares the same name, but the use-case is not exactly the same. How to use tracing Let us outline a strategy that makes tracing a non-painful experience regardless of whether it is a development, test or production system. 1. Form a theory Using tracing without a clear plan is a recipe for disaster. So first and foremost, you need an idea what are you tracing for and what output you expect. To have that you need to read the code you are about to trace and form some understanding of how that code interacts in the system. Do you expect the function to be called only a few times? Do you expect the function arguments to be large? Do you expect the process to exist, or will it be created after starting tracing? Do you expect the function not to crash? Do you expect the process not to be scheduled out between two function calls? Do you expect the garbage collection to be quick? These are some questions that you can answer by using tracing, but if your expectations are far off from the truth, then the performance characteristics of tracing can cause some headaches. If the function is called many times, instead of every once in a while, you need an automatic mechanism that stops the tracing before it causes any problems, or trace into a file with wrapping in place. Or you expect a function to return properly, but it crashes and doesn’t generate a return trace entry. In this case, you need to capture the crash by setting the right trace flags. These are considerations you should make before starting the trace, it might be your only chance for a rare case to collect evidence. You can also use tracing to understand the code better. It might not seem to be intuitive, but in this case, I still recommend you try to form and prove a theory of system behaviour. Don’t trace for everything, but one thing at a time. Remember, Elixir (and Erlang) applications are built with resiliency in mind. Some errors are expected and not necessarily reported by the application. Catching these cases might be more difficult than anticipated. 2. Choose the right tool There are many tracing tools out there. Yes, even for Elixir. The higher level tools will cover 99% of your use case, but you can always defer to the lower level tools that might be a bit more difficult to control. The tracing facilities are built into the VM, so Elixir developers have a harder time to translate their code to the one the BEAM understands. This is a price paid for the nice and efficient syntax, but in reality, it is not that far off. A good starting point is rexbug which is a wrapper for Redbug . It not only gives a convenient interface to use with Elixir (it translates the Elixir syntax to Erlang, and calls Redbug), but it also provides a lot of safeguards to protect us from making a big mistake and prevents the trace messages overloading the node. It is also possible to gather the trace entries from more than one node if we have a distributed application. Imagine a cluster where a certain function is executed on a random node as a means of load balancing. You can log into one node, start the tracer there and see what is happening regardless of which node executes it. Although the matching syntax of rexbug is convenient and easy to use, sometimes it is necessary to fine tune the tracer for better performance or safer operation. In this case, dbg comes to the rescue. It has no built-in safeguards but has the ability to use all the mighty powers of match specifications to restrict what can be traced. You can trace messages sent or received with optional filters, you can also specify which specific processes you want to trace. You can also choose an output file or a network port as a destination for the trace entries. Some of this functionality is exposed to rexbug, but not all. The number of problems you can solve with this tool is so vast that you hardly ever need to use the built-in trace BIFs from the erlang module. 3. Analyse the result It is possible to customise the output of the tracer tools to ease the analysis phase. This is the time when you verify your initial assumptions and find evidence in the trace logs. You can inspect the function arguments, return values, scheduling logic, garbage collection cycles, the interaction between processes and port drivers. Sometimes it is a good idea to visualise the events by post-processing the output by e.g. python and matplotlib. Sometimes the frequency of events is the clue you are looking for. If you are searching for a needle in the haystack, then it is a good idea to trace to a file (or more files with wrapping) and log even gigabytes of data. Later you can use a custom tool to filter through all that noise. You can create your own trace analysing tool by writing a trace client that reads from a file. You can also do this filtering during tracing, by writing a tracer module (in Erlang or Elixir) and use that as an on-the-fly filtering mechanism. If you are into graphical interfaces, then erlyberly is another option. Written in Java, it provides a GUI which you can use for tracing. erlyberly can connect to a remote Erlang node or cluster and direct the output messages to the GUI. It is quite convenient if you want your trace commands to survive node restarts. Lastly, if you have a commercial BEAM-focused monitoring system like WombatOAM deployed, you can execute the tracing commands in WombatOAM so you don’t even need to manually connect to your node. Learn more You can learn about tracing and debugging systems built on the BEAM from the excellent book “Erlang in Anger” by Fred Hebert . I recommend checking out the tools I mentioned in this post.", "date": "2019-06-06"},
{"website": "Erlang-Solutions", "title": "How to ensure your Instant Messaging solution offers users privacy and security", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/how-to-ensure-your-instant-messaging-solution-offers-users-privacy-and-security/", "abstract": "Concerns around privacy and security have become a big talking point this year. There have been a number of major Instant Messaging providers who have been criticised for the way that their apps collect, store and share the information of their users. In amongst the mountains of data collected most apps will receive information about users interests, behavioural patterns and location. Users’ privacy concerns have caused an unexpected and unwanted complication for many companies using enterprise versions of these chat applications. Firstly, if customers are turning away from the specific chat provider a business has chosen to adopt, it can create a barrier between the company and their customers. Secondly, for businesses in regulated industries such as FinTech or Healthcare, a chat provider’s ability to change their privacy and security terms and conditions is an unacceptable risk. The best way for a business to be sure that their enterprise instant messaging solution is secure and meets the privacy demands of its users is to have control of the privacy and security setting of the chat application. With most out-the-box solutions, control over privacy and security is rigid and dictated by the chat provided. Privacy by default, customisable by design. MongooseIM is built with maximum user privacy levels as the default, which means anyone using a standard implementation of MongosseIM has an extremely private, secure chat server that has been approved by some of the most stringent regulatory boards. On top of that, you control your chat application giving you the ability to format the settings to suit your company’s needs and your users. Below are a list of privacy considerations built into MongooseIM. Minimise and limit The minimise and limit principle regards the amount of personal data gathered by a service. The general principle here is to take only the bare minimum required for a service to run instead of saving unnecessary data just in case. If more data is taken out, the unnecessary part should be deleted. Luckily, MongooseIM is using only the bare minimum of personal data provided by the users and relies on the users themselves to provide more if they wish to – e.g. by filling out the roster information. Moreover, since it is implementing XMPP and is open source, everybody has an insight as to how the data is processed. Hide and protect The hide and protect principle refers to the fact that user data should not be made public and should be hidden from plain view disallowing third parties to identify users through personal data or its interrelation. We have tackled that by handling the creation of JIDs and having recommendations regarding log collection and archiving. What is this all about? See, JIDs are the central and focal point of MongooseIM operation as they are user unique identifiers in the system. As long as the JID does not contain any personally identifiable information like a name or a telephone number, the JID is far more than pseudo-anonymous and cannot be linked to the individual it represents. This is why one should refrain from putting personally identifiable information in JIDs. For that reason, our release includes a mechanism that allows automatic user creation with random JIDs that you can invoke by typing ‘register’ in the console. Specific JIDs are created by intentionally invoking a different command (register_identified). Still, it is possible that MongooseIM logs contain personally identifiable information such as IP addresses that could correlate to JIDs. Even though the JID is anonymous, an IP address next to a JID might lead to the person behind it through correlation. That is why we recommend that installations with privacy in mind have their log level set to at least ‘warning’ level in order to avoid breaches of privacy while still maintaining log usability. Separate and aggregate The separate principle boils down to partitioning user data into chunks rather than keeping them in a monolithic DB. Each chunk should contain only the necessary private data for its own functioning. Such a separation creates issues when trying to identify a person through correlation as the data is scattered and isolated – hence the popularity of microservices. Since MongooseIM is an XMPP server written in Erlang, it is naturally partitioned into modules that have their own storage backends. In this way, private data is separated by default in MongooseIM and can be also handled individually – e.g. by deleting all the private data relating to one function. The aggregation principle refers to the fact that all data should be processed in an aggregated manner and not in one focused on detailed personal cases. For instance, behavioural patterns should be representative of a concrete, not identifiable cohort rather than of a certain Rick Sanchez or Morty Smith. All the usage data being processed by MongooseIM is devoid of any personally identifiable traits and instead tracks metrics relevant to the health of the server. The same can be said for WombatOAM if you pair it with MongooseIM. Therefore, aggregation is supported by default. Privacy by default It is assumed that the user should be offered the highest degree of privacy by default. This is highly dependant on your own implementation of the service running on top of MongooseIM. However, if you follow our recommendations laid out in this post, you can be sure you implement it well on the backend side, as we do not differentiate between the levels of privacy being offered. The Right of Access As users privacy concerns go, so too does the likelihood that a user will request to see the data that your chat application has stored on them. With MongooseIM we have put a lot of effort in order to make the retrieval as painless as possible for system administrators that oversee the day to day operations. That is why we have developed a mechanism you can start by executing the retrieve_personal_data command in order to collect all the personal and derivative data belonging to a user behind a specific JID. The command will execute for all the modules no matter if they are enabled or disabled. Then, all the relevant data is extracted per module and is returned to the user in the form of an archive. In order to facilitate the data collection, we have changed the schemas for all of our MAM backends. This has been done to allow a swift data extraction since up till now it was very inefficient and resource hungry to run such a query. Of course, we have prepared migration strategies for the affected backends. The Right to be Forgotten The right to be forgotten is another one that goes alongside the right of access. Each user has the right to remove their footprint from the service. Since we know retrieval from all the modules listed above is problematic, removal is even worse. We have implemented a mechanism that removes the user account leaving behind only the JID. You can run it by executing the “unregister” command. All of the private data not shared with other users is deleted from the system. In contrast, all of the private data that is shared with other users – e.g. group chats messages or PubSub flat nodes – is left intact as the content is not only owned by one party. Logs are not a part of this action. If the log levels are set at least to ‘warning’, there is no personal data that can be tied to the JIDs in the first place so there is no need for removal. Long term peace of mind The recent privacy concerns of major messaging provider has created an unwanted hurdle for many businesses, but one that can be easily overcome. They do however serve as a good example of one of the broader problems with choosing an out-the-box, software-as-a-service provider for your chat solutions. Most third party products are offered as one-size-fits all, meaning any changes made by the owner will impact your account, this creates an uncontrolled liability for your business. MongooseIM offers an easily manageable alternative to software-as-a-service providers. With us, you’ll be able to utilise an extremely robust, battle-tested messaging server, that has been used by many of the world’s most used chat applications. In doing so, you’ll have a solution that is scalable, reliable but customisable and owned by your business. Want to learn MongooseIM? Visit our training to page to find out how you can empower your team to utilise our awesome, open source instant messaging server for the needs of your organisation.", "date": "2021-01-18"},
{"website": "Erlang-Solutions", "title": "The continuing headaches of distributed programming", "author": ["Andy Oram"], "link": "https://www.erlang-solutions.com/blog/the-continuing-headaches-of-distributed-programming/", "abstract": "Why scaling and parallelism remain hard even with new tools and languages Despite ground-breaking advances in languages and utilities that purport to simplify parallel or distributed computing, designing a system that supports modern commerce and other high-priority applications remains one of the most difficult tasks in computing. Having dealt with such systems for too long–I wrote tests and documentation for a Unix pthreads implementation at a manufacturing firm in the 1980s, for instance, and edited O’Reilly’s PThreads Programming book in 1996–I have gotten so familiar seeing the same difficulties over and over that I’ve learned to look for them in every distributed system I hear about. And I recently started to wonder why there had been so limited progress in the field after I was assigned a cluster of books on languages (Erlang, Haskell, Scala, OCaml), messaging (ZeroMQ), and DevOps/scheduling tools (Mesos, ZooKeeper). I expect language and tool designers to deal with the difficulties of distributed computing. What surprises me is that users out in the field–application developers and operations personnel–have to do so as well. The complexities are not completely hidden by the tools. And I think I have identified the reason: efficient and robust distributed computing requires careful tuning of your architectural parameters. These parameters must be exposed to the programmers or operations people. At least a few of the following issues will intrude into their decisions: How many nodes you want for each function in your application (front-end, application back-end, database, load balancing, etc.) The memory and CPU capacity of each node The geographic distributions of nodes and which share a data center How to partition and replicate your data, the number of replicas needed, and the distribution of nodes How many systems at each site should be alive simultaneously to handle availability and load How many nodes constitute a quorum when replicating state or electing a leader What consistency models (eventual, causal, strong, etc.) you want during data replication The recovery policy in case of failures or unresponsive systems Data maintenance policies, such as whether to cache the intermediate results of calculations or recalculate them when necessary The demands on operations pile up fast, and a lot of configuration is required no matter how easy the tool-makers try to render it. This article discusses the design of such systems. Languages facilitate distributed computing–but only to a degree Erlang, Go, and several other languages tout their solutions for people writing for multiple cooperating nodes. Support for distributing computing has certainly been refined in these languages, particularly through the message-passing model, which now seems almost universal. Good old threads and multiprocessing have been relegated to the most performance-critical applications on single systems. Applications spanning nodes can’t benefit from the data sharing that threads and processes on a single system can exploit, and even the blessings of sharing data on a single node run into headaches because of cache consistency issues–along with threads’ notorious vulnerability to hard-to-debug errors. It’s definitely convenient to use a single programming model to cover communication among both local and remote processes, so messaging wins out. Its asynchronous communication allows the same mechanisms to be used for both local and remote processes transparently. You can write code that runs on a single node, and with little or no changes, distribute the code to a cluster of nodes. Asynchronous messaging can even be used through the Network Time Protocol (NTP) to synchronize clocks on different computers, so that when they do have to engage in shared memory activities (such as writing files to a shared disk), their views of the external world are roughly in agreement. But processes still have to deal with backed-up queues, child processes that fail, and child processes that simply never return a result because of process failure, programming error, or networking partitioning. Programmers have to work with protocols such as back pressure and policies enforced by supervisor nodes. The very existence of ZooKeeper (for implementing distributed consensus and guaranteeing agreement across application processes) demonstrates how complicated distributed computing remains. It would be unreasonable to expect a programming language to deal with these issues, and in fact they don’t. The Erlang developers realized some time ago that something on top of Erlang was required to deal with distributed computing, so they developed the OTP library discussed in the O’Reilly book Designing for Scalability with Erlang/OTP . This library provides basic calls for starting and restarting processes, recognizing and propagating error messages, and similar supervisor responsibilities. The distinction between what’s appropriate for language semantics and what’s appropriate in a library depends on the architectural considerations I mentioned before. Policy decisions such as the following aren’t appropriate to build into a language: Whether to give up on a process that failed (because you don’t need its results or cannot recover from its corrupt state) or restart it How long to wait before deciding that a node has vanished How many nodes to run for fault-tolerance and scalability Whether to set up a single monitor for all subprocesses, or separate monitors for your user interactions and back-end databases Those are the sorts of decisions relegated by Erlang to the OTP library. OTP has built the functionality to deal with the issues of launching processes, joining them, and handling failures into a set of generic templates so that the programmers can concentrate more on the business logic of their particular applications. A useful overview of policy decisions in distributed systems introduces some of these topics–and even a treatment of that length couldn’t cover everything. But at a certain level of mission-critical application deployment, even OTP is insufficient. In order to avoid the need for a human staff person to hang around 24/7 and monitor thousands of nodes, you want automated ways to send user requests to a pool of servers, and sophisticated ways to duplicate data in such a way that it’s safe even if multiple nodes fail. These requirements call for more than a library–they need a network of cooperating separate processes. Riak Core and Scalable Distributed Erlang are discussed in the book as extra tools to fill the gap. These tools expose the detailed configuration details mentioned earlier in this article so that a programmer can make the tough, architecture-specific, environment-specific choices that allow an application to scale and stay highly available. Unrelated processes The tools I’ve discussed so far in this article, for all their diversity, take one easy way out: they assume that all processes have a common ancestor or at least were started by a single operator. Most, such as ZeroMQ and Erlang/OTP, rely on a tree structure of supervisors going back to a single root. Unrelated Erlang processes and microservices can find each other through a central registry called Gproc . Bluetooth and other wireless technologies allow device discovery on local networks, part of the technologies for the Internet of Things. ZooKeeper is also more flexible, using a coordination and voting algorithm in the tradition of Paxos to coordinate independent peers. A scheduling system used internally at Google allows multiple schedulers to start jobs on shared computing resources, with potentially different policies for determining when jobs of varying lengths should run. The paper does not explain how the system users pass it information on the expected time requirements or latencies of the jobs they submit. Shared resources means conflicts will occur and schedulers must use transactions to deal with jobs that require too many resources to finish. Few systems try to support a totally peer-to-peer environment of different processes with different owners–a goal that was already recognized as nearly intractable when I wrote about it in 2004. Unknown processes can “plug in” to a distributed environment if everybody is communicating over a local area network. Broadcasts on the network allow systems to discover each other and choose leaders. That’s how Apple’s Bonjour and Microsoft’s CIFS let you start up a printer and have other systems automatically communicate with it. Outside a LAN, the coordination of unrelated processes requires out-of-band communications, such as when I tweet the URL of a video I want you to view. There are also specialized servers that use DNS lookups or related methods to register and offer up unknown services to clients seeking them. In summary, various cloud solutions and schedulers take on important burdens such as job monitoring, fair division of resources, auto-scaling, and task-sharing algorithms such as publish/subscribe, but the configuration issues mentioned at the beginning of this article still require intervention by a programmer or operations person. Which Way to Turn? I’ve been watching the distributed computing space since the 1980s. The fundamental problems don’t go away and aren’t solved by throwing more CPU or faster networks at them. Distributed computing forces computers to reflect the complexity of life itself, with many interacting factors over various periods of time. Go meditate in a forest for half an hour, and you will understand more about the problems of distributed systems (and other things) than I can convey through text. My friend and stalwart author Francesco Cesarini reminds me that computing has made many advances over the past few decades. One, as I mentioned, is the large-scale abandonment of shared member in favor of message-passing. Not all shared resources can be given up, though. Every production system rests on some shared data store, such as a relational database, and these data stores present their own partitioning and high-availability challenges. The computer field has significantly advanced in understanding the trade-offs between consistency and availability, illustrated by all the discussion around CAP and all the data systems designed with different these trade-offs. Network failures are being understood and handled like other software exceptions. Cesarini says, “Some frameworks that try to automate activities end up failing to hide complexity. They limit the trade-offs you can make, so they cater only to a subset of systems, often with very detailed requirements.” Cesarini says, “Some frameworks that try to automate activities end up failing to hide complexity. They limit the trade-offs you can make, so they cater only to a subset of systems, often with very detailed requirements.” So go ahead and pick your tool or language–you may well be picking another one two years from now. But rest comfortably in the knowledge that whatever challenges you’re struggling with, and whatever solutions you find, you’ll be reusing ideas that were thought up and probably implemented years ago.", "date": "2016-06-18"},
{"website": "Erlang-Solutions", "title": "Programming languages for online betting: an investigation of Go, Erlang, and Elixir", "author": ["Stuart Whitfield"], "link": "https://www.erlang-solutions.com/blog/programming-languages-for-online-betting-an-investigation-of-go-erlang-and-elixir/", "abstract": "“The right tool for the job” is a popular adage in the online betting industry . In my time at Erlang Solutions, I’ve learned that there are three key reasons why Erlang and Elixir are often the right tool for the job in online gambling and betting: superior concurrency, scalability, and reliability. Online betting is a booming industry, both in terms of technology and revenue. Case in point; bet365, the world’s leading bookmaker has more than 22m customers, takes up to three-quarters of its £1.5bn revenues come from international markets. Extreme scalability, concurrency, and reliability is a must to keep bet365’s engines turning. bet365 lives by the mantra the right tool for the job , and runs a mix of Erlang, Elixir, and Go in production. So when and where should you choose Erlang, Elixir or Go in your betting stack? Concurrency At any one time bet365’s systems are serving many 100,000s of users live odds and results, while managing multiple backend data streams. In peak times, like the Super Bowl or the Grand National, the number of users swells by an order of magnitude. This is an awesome feat of concurrent engineering. When it comes to concurrency, Erlang and Elixir, both built on the BEAM virtual machine, excel. They are both concurrency oriented functional languages, built to handle vast numbers of users at the same time. Users of Erlang across verticals as diverse as telecoms, adtech; financial payments; massive multiplayer online role playing gaming; and social media have all exploited its ability to provide impressive concurrency. Go also offers good support for concurrency, especially when comparing it to Ruby, Python, or C++. However, is not an alternative to Erlang or Elixir for backends where availability and low latency for high numbers of concurrent requests is required, as in online betting. Scalability Modern betting infrastructures demand massive scalability, speed, and fault tolerance to run smoothly. Without built-in scalability, operators can be left to rely on additional hardware to scale vertically, which is not a sustainable nor a cost effective approach. Erlang and Elixir’s concurrency go hand-in-hand with their massive scalability. If Erlang isn’t the best tool for every job your system has, a distributed architecture that can plug into Erlang-based products can make your betting system quick to deploy and scale elastically. Tools built in Erlang, such as Riak KV , scale out, up and down predictably. Say goodbye to the headache of emergency hardware and unpredictable system performance at peak load. When you pit Go’s requests per seconds against the likes of Ruby on Rails or Django, Go returns some impressive benchmarks performing 3x better. Go can scale to hundreds of thousands with relative ease, in much the same way that Erlang and Elixir scale to millions. Reliability Like the stock market, downtime for an online betting operator has immediate financial and reputational consequences. For online sports betting the provision of a ultra-reliable, real-time service is now a priority for bookmakers and punters alike. Just look at the booming inplay market for sports events. In this world, pauses of any kind, for system failure, garbage collection, or queuing backlogs, are not acceptable. Online betting stacks must handle their constant torrents of data without impacting the system’s processes, or end users. Erlang and Elixir’s no-shared memory approach allows processes to terminate without corrupting the state of other requests being executed in parallel. This allows a ‘fail and recover’ architecture where failure is isolated and does not affect the system. As a result, Erlang and Elixir achieve “five nines” availability at a fraction of the effort of other programming languages. This makes Erlang and Elixir ideal to build critical gambling and betting systems on. Go is tooled very well, but some of its automated triggers can cause errors, jeopardising server code that is supposed to reply immediately. This in turn can harm its overall reliability in comparison to Erlang and Elixir. Learn more about our work with our Online Betting partners, such as bet365 and William Hill.", "date": "2017-10-10"},
{"website": "Erlang-Solutions", "title": "Build a complete iOS messaging app using XMPPframework-tutorial-part 1", "author": ["Andres Canal"], "link": "https://www.erlang-solutions.com/blog/build-a-complete-ios-messaging-app-using-xmppframework-tutorial-part-1/", "abstract": "YAXT??! Yet another XMPP tutorial? Well, this is going to be another tutorial, but I’m going to try to make it a little bit different. This is an XMPP tutorial from an iOS developer’s perspective. I’ll try to answer all the questions I had when I started working in this area. This journey is going to go from no XMPP knowldege at all to having a fully functional instant messaging iOS app using this cool protocol. We are going to be using the super awesome (yet overwhelming at the beginning…) XMPPFramework library, and the idea is also to also mix in some iOS concepts that you are going to need for your app. What’s XMPP? From Wikipedia: Extensible Messaging and Presence Protocol (XMPP) is a communications protocol for message-oriented middleware based on XML. This basically means XMPP is a protocol for exchanging stuff. What kind of stuff? Messages and presences. We all know what messages are, but what about presences? A presence is just a way of sharing a “status”, that’s it. You can be ‘online’, ‘offline’, ‘having lunch’, or whatever you want. Also there’s another important word: Extensible meaning it can grow. It started as an instant messaging protocol and it has grown into multiple fields for example IoT (Internet of Things). And last, but not least: every piece of information we are going to exchange under this protocol is going to be XML. I can heard you complaining but… Come on, it’s not that bad! Why do we need XMPP? Why not just REST? Well what other options do we have? On the one hand, a custom solution means building everything from scratch, that takes time. On the other hand, we have XMPP, a super tested technology broadly used by millions of people every day, so we can say that’s an advantage over a custom approach. Everytime I talk about XMPP, someone asks me ‘Why not just REST?’. Well, there is a misconception here. REST is not a protocol, it’s just a way of architecting a networked application; it’s just a standarized way of doing something (that I love btw). So let’s change the question to something that makes more sense: “Why not just build a custom REST chat application?”. The first thing that comes to my mind is what I already explained in the previous paragraph, but there is something else. How do I know when someone has sent me a message? For XMPP this is trivial: we have an open connection all the time so, as soon as a message arrives to the server, it will send us the message. We have a full-duplex. On the other hand, the only solution with REST is polling. We will need to ask the server for new messages from time to time to see if there is something new for us. That sucks. So, we will have to add a mechanism that allows us to receive the messages as soon as they are created, like SSE or WebSockets. There is one more XMPP advantage over a custom REST chat application. REST uses HTTP, an application level protocol that is built on top of a transport level protocol: TCP. So everytime you want to use your REST solution, you will need HTTP, a protocol that is not always available everywhere (maybe you need to embed this in a cheap piece of hardware?). Besides, we have XMPP built on top of TCP that’s going to be always available. What’s the basic stuff I need to know to get started? Well, you know a lot already but let’s make a list. Lists are always good: XMPP is built on top of TCP. It keeps an open connection all the time. Client/Server architecture. Messages always go through a server. Everything we send and receive is going to be XML and it’s called Stanza. We have three different types of stanzas: iq, message and presence. Every individual on the XMPP network is univocally identified by a JID (Jabber ID). All the stanzas are cointained in a Stream. Let’s imagine the Stream as a white canvas where you and the server write the stanzas. Stream, iq, message and presence are the core of XMPP. You can find everything perfectly detailed in RFC6120 XMPP can be extended to accomplish different stuff. Each extension is called XEP (XMPP Extension Protocol). What’s a JID? Jabber ID (JID) is how we univocally identify each individual in XMPP. It is the address to where we are going to send our stanzas. This is how a JID looks like: localpart : This is your username. domainpart : Server name where the localpart resides. resourcepart : This is optional, and it identifies a particular client for the user. For example: I can be logged in with andres@erlang-solutions.com on my iPhone, on my Android and on my mac at the same time… So all these will be the same localpart + domainpart but different resourcepart I’m sure you have already noticed how similar the JID looks to a standard email address. This is because you can connect multiple servers together and the messages are rooted to the right user in the right server, just as email works. Pretty cool, right? Sometimes you will see we have a JID with just the domain part. Why?! Because it’s also possible to send stanzas to a service instead of a user. A service? What’s a service?! Services are different pieces of an XMPP server that offer you some special functionality, but don’t worry about this right now, just remember: you can have JIDs without a localpart. What’s a Stanza? Stanza is the name of the XML pieces that we are going to be sending and receiving. The defined stanzas are: <message/> , <presence/> and <iq/> . <message/> This is a basic <message/> stanza. Everytime you want to send a message to someone (a JID), you will have to send this stanza: <message from='andres@erlang-solutions.com/iphone' to='juana@erlang-solutions.com' type='chat'>\n    <body>Hey there!</body>\n</message> <iq/> It stands for Info/Query. It’s a query-action mechanism, you send an iq and you will get a response to that query. You can pair the iq-query with the iq-response using the stanza id. For example, we send an iq to the server to do something (don’t pay attention to what we want to do… you just need to know there is an iq stanza and how the mechanism works): <iq to='erlang-solutions.com' type='get' id='1'>\n  <query xmlns='http://jabber.org/protocol/disco#items'/>\n</iq> And we get back another iq with the same id with the result of the previous query: <iq from='erlang-solutions.com' to='ramabit@erlang-solutions.com/Andress-MacBook-Air' id='1' type='result'>\n    <query xmlns='http://jabber.org/protocol/disco#items'>\n        <item jid='muc.erlang-solutions.com'/>\n        <item jid='muclight.erlang-solutions.com'/>\n        <item jid='pubsub.erlang-solutions.com'/>\n    </query>\n</iq> <presence/> Used to exchange presence information, as you could have imagined. Usually presences are sent from the client to the server and broadcasted by it. The most basic, yet valid presence, to indicate to the server that a user is avaiable is: <presence/> After a sucessfull connection, you are not going to receive any <message/> until you make yourself available sending the previous presence. If you want to make yourself unavailable, you just have to send: <presence type=\"unavailable\"></presence> If we want to make the presences more useful, we can send something like this: <presence>\n      <status>On vacation</status>\n</presence> What’s a Stream? Before answering this, let’s refresh our mind. What’s a Unix socket? From Wikipedia: A socket is a special file used for inter-process communication. These allows communication between two processes. So a socket is a file that can be written by two processes (in the same computer or in different computers in the same network). So the client is going to write to this file and server too. Ok, but how is a socket related to a Stream? Well, we are going to be connected to a server using a socket, therefore we are going to have a ‘shared file’ between the client and the server. This shared file is a white canvas where we are going to start writting our XML stanzas. The first thing we are going to write to this file is an opening <stream> tag! And there you go… that’s our stream. Perfect, I understand what a stream is, but I still don’t understand how to send a message to the server. Well, the only thing we need to do to send a message is writting a <message/> stanza in our shared file. But what happens when the server wants to send me a message? Simple: it will write the message in the ‘shared file’. Are we ok so far? I’m sure at this point you have questions like: “What?! An active TCP connection open all the time? I’m used to REST! How am I going to do that?!” ​ Easy, you don’t have to care about that any more! That’s why we are going to use the library, and it will take care of that. “You said nothing about how to connect to the server!” Believe me, you don’t have to care about this either. If we start adding all this info, we are going to get crazy. Trust me, I’ve been there. “What about encrypted messages? We need security! How are we going to handle this?” Again, you don’t have to care about this at this point. Baby steps! You just need to be able to answer: “What’s XMPP?”, “How do you send a message?”, “How do you change your status in XMPP?”, “How do you ask something to the server?”, “What’s a Stream?”. If you can answer all that, you are WAY better than me when I started. All the concepts we described so far are the core of XMPP.  To find out how to get started with XMPPFramework, how to connect to the server and authenticate a user, go to PART 2 !", "date": "2016-11-30"},
{"website": "Erlang-Solutions", "title": "Build a complete iOS messaging app using XMPPFramework – Part 2", "author": ["Andres Canal"], "link": "https://www.erlang-solutions.com/blog/build-a-complete-ios-messaging-app-using-xmppframework-part-2/", "abstract": "First steps: XMPPFramework Build a complete iOS messaging app using XMPPFramework is a tutorial that shows you how to build a fully functional instant messaging iOS app using the very cool XMPPFramework protocol and Swift3. In this part, we are going to get our hands dirty! To recap on the theory, or if you just landed here randomly, have a quick read through the first part , then get your Xcode ready and let’s start! In this issue we are going to be integrating the library to our project, creating a connection with the server and authenticating. The XMPPFramework library is the most used XMPP library for iOS and macOS. At the beginning it may be a little bit overwhelming but after a few days working with it you will learn to love it. Installing the library Let’s create a brand new Xcode project and install the library. In this tutorial we are going to be using Swift 3. The easiest way to integrate XMPPFramework to the project is using CocoaPods . Let’s create our Podfile using the pod init command in the folder where our . xcodeproj lives. There are thousands of forks but the maintained one is the original: robbiehanson/XMPPFramework . So let’s add the pod to our Podfile and remember to uncomment the use_frameworks! . use_frameworks!\n\ntarget 'CrazyMessages' do\n    pod 'XMPPFramework', :git=> 'git@github.com:robbiehanson/XMPPFramework.git', :branch => 'master'\nend Then pod install and CocoaPods is going to do its magic and create a .xcworkspace with the library integrated. Now we just need to import XMPPFramework in the files we want to use the library and that’s it. Starting to build our Instant Messaging app The most important thing in an XMPP application is the stream, that’s where we are going to “write” our stanzas, so we need an object that is going to hold it. We are going to create an XMPPController class with an XMPPStream : import Foundation\nimport XMPPFramework\n\nclass XMPPController: NSObject {\n    var xmppStream: XMPPStream\n\n    init() {\n        self.xmppStream = XMPPStream()  \n    }\n\n} We are dealing with a highly asynchronous library here. For every action we are going to have a response some time in the future. To handle this XMPPFramework defines the XMPPStreamDelegate . So implementing that delegate is going to help us answer lots of different questions like: “How do I know when XMPP has successfully connected?”, “How do I know if I’m correctly authenticated?”, “How do I know if I received a message?”. XMPPStreamDelegate is your friend! So we have our XMPPController and our XMPPStream , what do we need to do now? Configure our stream with the hostName , port and ourJID . To provide all this info to the controller we are going to make some changes to the init to be able to receive all these parameters: enum XMPPControllerError: Error {\n    case wrongUserJID\n}\n\nclass XMPPController: NSObject {\n    var xmppStream: XMPPStream\n\n    let hostName: String\n    let userJID: XMPPJID\n    let hostPort: UInt16\n    let password: String\n\n    init(hostName: String, userJIDString: String, hostPort: UInt16 = 5222, password: String) throws {\n        guard let userJID = XMPPJID(string: userJIDString) else {\n            throw XMPPControllerError.wrongUserJID\n        }\n\n        self.hostName = hostName\n        self.userJID = userJID\n        self.hostPort = hostPort\n        self.password = password\n\n        // Stream Configuration\n        self.xmppStream = XMPPStream()\n        self.xmppStream.hostName = hostName\n        self.xmppStream.hostPort = hostPort\n        self.xmppStream.startTLSPolicy = XMPPStreamStartTLSPolicy.allowed\n        self.xmppStream.myJID = userJID\n\n        super.init()\n\n        self.xmppStream.addDelegate(self, delegateQueue: DispatchQueue.main)\n    }\n} Our next step is going to actually connect to a server and authenticate using our userJID and password , so we are adding a connect method to our XMPPController . func connect() {\n    if !self.xmppStream.isDisconnected() {\n        return\n    }\n\n   try! self.xmppStream.connect(withTimeout: XMPPStreamTimeoutNone)\n} But how do we know we have successfully connected to the server? As I said earlier, we need to check for a suitable delegate method from XMPPStreamDelegate . After we connect to the server we need to authenticate so we are going to do the following: extension XMPPController: XMPPStreamDelegate {\n\n    func xmppStreamDidConnect(_ stream: XMPPStream!) {\n        print(\"Stream: Connected\")\n        try! stream.authenticate(withPassword: self.password)\n    }\n\n    func xmppStreamDidAuthenticate(_ sender: XMPPStream!) {\n        self.xmppStream.send(XMPPPresence())\n        print(\"Stream: Authenticated\")\n    }\n} We need to test this. Let’s just create an instance of XMPPController in the AppDelegate to test how it works: try! self.xmppController = XMPPController(hostName: \"host.com\",\n                                     userJIDString: \"user@host.com\",\n                                          password: \"password\")\nself.xmppController.connect() If everything goes fine we should see two messages in the logs but of course that’s not happening, we missed something. We never told to our xmppStream who was the delegate object! We need to add the following line after the super.init() self.xmppStream.addDelegate(self, delegateQueue: DispatchQueue.main) If we run the app again: Stream: Connected\nStream: Authenticated Success! We have our own XMPPController with a fully functional and authenticated stream! Something that may catch your attention is how we are setting our delegate, we are not doing: self.xmppStream.delegate = self Why not? Because we can “broadcast” the events to multiple delegates, we can have 10 different objects implementing those methods. Also we can tell what’s the thread where we want to receive that call, in the previous example we want it in the main thread. Getting a Log In Our app is super ugly, let’s put on some makeup! We have nothing but an XMPPController and a hardcoded call in the AppDelegate . I’m going to create a ViewController that is going to be presented modally as soon as the app starts, that ViewController will have the neccesary fields/info to log in to the server. I’m going to create a LogInViewControllerDelegate that is going to tell to our ViewController that the Log in button was pressed and that’s it. In that delegate implementation we are going to create our XMPPController , add the ViewController as delegate of the XMPPStream and connect! extension ViewController: LogInViewControllerDelegate {\n\n    func didTouchLogIn(sender: LogInViewController, userJID: String, userPassword: String, server: String) {\n        self.logInViewController = sender\n\n        do {\n            try self.xmppController = XMPPController(hostName: server,\n                                                     userJIDString: userJID,\n                                                     password: userPassword)\n            self.xmppController.xmppStream.addDelegate(self, delegateQueue: DispatchQueue.main)\n            self.xmppController.connect()\n        } catch {\n            sender.showErrorMessage(message: \"Something went wrong\")\n        }\n    }\n} Why are we adding ViewController as a delegate of XMPPStream if our XMPPController alreay has that delegate implemented? Because we need to know if this connection and authentication was successfull or not in our ViewController so we are able to dismiss the LogInViewController or show an error message if something failed. This is why being able to add multiple delegates is so useful. So as I said I’m going to make ViewController to comform to the XMPPStreamDelegate : extension ViewController: XMPPStreamDelegate {\n\n    func xmppStreamDidAuthenticate(_ sender: XMPPStream!) {\n        self.logInViewController?.dismiss(animated: true, completion: nil)\n    }\n\n    func xmppStream(_ sender: XMPPStream!, didNotAuthenticate error: DDXMLElement!) {\n        self.logInViewController?.showErrorMessage(message: \"Wrong password or username\")\n    }\n\n} And that’s it! Our app can log in to our server as I’m showing here: Logging! We’ve been talking a lot about XMPP, stanzas and streams… but is there a way I can see the stream? Yes SR! XMPPFramework got us covered! XMPPFramework ships with CocoaLumberJack , a pretty well known logging framework. We just need to configure it, set the logging level we want and that’s it. Logs are going to start showing up! Configuring CocoaLumberjack This is a really simple task, you just need to add to your func application(application: UIApplication, didFinishLaunchingWithOptions ... method the following line (remember to import CocoaLumberjack ): DDLog.add(DDTTYLogger.sharedInstance(), with: DDLogLevel.all) I’m not going to paste here all the connection process log because it makes no sense to try to understand what’s going on at this stage of our learning. But I think showing what some stanzas look like is a good idea. To do this I’m going to be sending messages from Adium . I’m going to send this <message/> : <message to=\"test.user@erlang-solutions.com\">\n    <body>This is a message sent from Adium!</body>\n</message> Let’s see how it looks like when it reaches our app: <message xmlns=\"jabber:client\" from=\"iamadium@erlang-solutions.com/MacBook-Air\" to=\"test.user@erlang-solutions.com\">\n   <body>This is a message sent from Adium!</body>\n</message> Let’s send a <presence/> from Adium: <presence>\n    <status>On vacation</status>\n</presence> We are receiving: <presence xmlns=\"jabber:client\" from=\"iamadium@erlang-solutions.com/MacBook-Air\" to=\"test.user@erlang-solutions.com\">\n   <status>On vacation</status>\n</presence> No doubts at all right? We send something and we receive it on the other end! That’s it! Test Time! I want to be sure that you are understanding and following everything and not just copy and pasting from a tutorial (as I usually do 🙊). So if you are able to answer these questions you are on a good track! Why am I sending a presence after successfully authenticating? What happens if I don’t send it? What happens if I write a wrong server URL in the Log In form? How do I fix this problem if there is a problem… How do I detect if suddenly the stream is disconnected from the server? (maybe a network outage?) How do I detect if the user/password was wrong? If you need help leave a message! The sample project is on Github! The next part is going to be on Roster , and if I will have space I would also like to add sending and receiving messages. I’ve been kind of super busy lately so I’m not sure when I’m going to be able to deliver the next issue but I’ll try to work on it as soon as I have some free minutes to spare! PS: Also take a look at MongooseIM , our XMPP based open source mobile messaging platform.", "date": "2017-01-12"},
{"website": "Erlang-Solutions", "title": "The complete guide to Instant Messaging and in-application chat", "author": ["Erlang Admin"], "link": "https://www.erlang-solutions.com/blog/the-complete-guide-to-instant-messaging-and-in-application-chat/", "abstract": "What you need to know about Instant Messaging and chat applications Have you got the message? Chat is a critical feature for almost every business, in virtually every industry. Now, more than ever, digital communication is relied upon to share information and keep our contacts and users in touch. We’ve created bespoke chat applications for use cases as varied as large scale medical or digital health providers, industry-leading financial service providers and modern dating apps. For business-to-consumer uses, chat is a great way to turn your app or business into a community, keeping users engaged and adding a social element to your applications. On the other hand, in the B2B space, chat applications can be used to increase collaboration and productivity. In fact, external research conducted by one of our clients TeleWare found that instant messaging was the most in demand feature for a financial service app. In this blog, we’ll look at some of the key considerations for an Instant Messaging service as well as the must-have features of the modern chat application and how MongooseIM 4.0 stacks up to deliver what you need. Build vs buy One of the first considerations a company needs to make when implementing a chat offering is whether to use an out-of-the-box product-as-a-service or software-as-a-service offering or build your own chat. Below we weigh up the pros and cons of each approach. Benefits of buying The key benefits of an out-the-box purchase solution is that you are able to deploy quickly. The bigger players in this space often offer a comprehensive set of integrations and require little to no development from your team. They also provide users with a familiar user-interface, which means they’re incredibly quick for anyone to learn how to use. All of this means you can be up-and-running quickly with the peace-of-mind that you’re using a tried and tested solution. Cons of buying Both product-as-a-service and software-as-a-service options create the ongoing overhead of a subscription fee by their very nature. Over time, this cost inevitably adds up, making it a more expensive offering. Another drawback is that bought options are designed as one-size-fits-all products and seldom offer flexibility for bespoke features and changes. These options offer next to no control and data ownership is often shared. This makes it hard for your users to control their privacy and hard for your chat solution to meet any needs other than the most vanilla offering.The customer service and support can also be variable. All of this creates a huge potential for complication if something stops functioning in what is essentially a blackbox solution. Benefits of building Building provides you with the flexibility to create a specific chat solution for your needs and own every step in the functionality. In theory, building can be more affordable over the long-term as it reduces the ongoing costs of a software-as-a-service offering. An owned solution also minimises the risk of major changes in your chat application no longer being compitable with the rest of your application. Cons of building When building goes wrong, it is the most costly option, with high upfront and ongoing maintenance costs. Building your own chat application can run into difficulties when the app starts to scale (which is exactly when you want them least). Lastly, building something bespoke means there is no support or community to help you troubleshoot. The MongooseIM way MongooseIM is a massively scalable, battle tested, open-source chat server that has been proven to be able to handle 10’s of millions of connections with ease. With MongooseIM you have the freedom and flexibility to use the open-source product and develop it to your needs, or you can engage our experts to build bespoke features for you. Our team also offers the peace-of-mind of support should you ever need it. This gives you the freedom and flexibility to develop and own your chat solution without the cost or risk of starting from scratch. The most desired features in a chat application With over a decade’s experience in building chat applications, we know the features required to ensure a success, taking everyone from the end-user to the DevOps team into consideration. Below is a list of the most used and desired features and how MongooseIM stacks up. Real-time chat It goes without saying that a chat application should allow users to reliably send and receive messages in real-time. MongooseIM’s scalability ensures that no matter what the spikes or loads of your user-base is, no important message will be lost in transit. Push notifications Push notifications are one of the most valuable parts of a modern chat application. Even if your user is not logged into the application, they’ll still be informed of the message. For B2C applications, that increases the chances of bringing them back to your app and for B2B applications, it ensures no important message goes missed, without requiring your team to be logged into a chat application at all times. MongooseIM has an in-house developed push notification management system, MongoosePush, which is designed to integrate with MongooseIM to easily enable push notifications for your chat app. External integrations MongooseIM rarely works alone, usually it is coupled with other microservices. We offer a rest API that these services can call, and an event pusher for MongooseIM to notify them, thus providing a two-way communication with other microservices over the REST API. API An easy to use API makes your chat application faster and easier to embed and integrate into your chat. We offer a REST API, which is simple, modern and easily understood by most developers. This can be used for both backend integration and client / service development. Multi-user Chat Group chat is one of the most popular features in social settings, and one of the most in-demand features for business collaboration. MongooseIM offers a multi-user chat functionality that is reliable and seamless for users whilst minimising demands on your server. We also provide a light-weight implementation of multi-user chat, tailored for mobile devices. File Transfer and sharing For a majority of use cases, allowing users to share and transfer files makes a chat more usable, keeping them engaged on your platform longer. MongooseIM uses an out-of-band transfer method which reduces the workload on the server side whilst still enabling an easier to use experience for users to share files within the chat application. Batch permission Batch permissions allow for privacy and control of access to information. MongooseIM uses access control lists to offer this functionality. Our chat applications have been approved by regulatory bodies in the health care and financial services worldwide. Contact management As an application built in XMPP, MongooseIM uses the tried and tested mod_roster functionality to allow for users to manage and customise their address books within the chat application. History and version control If something goes wrong, history and version control is vital. Having access to previous versions means you always have a proven version to fall back on. MongooseIM has a public history of its source code which you have access to at all times. Contact sharing Contact sharing from within a chat application encourages connections between groups of users, helps to grow user bases and increase collaboration. Four key MongooseIM integrations Instant Messaging and Kubernetes Kubernetes has become an extremely popular platform-agnostic deployment tool and has powerful cloud management automation. The MongooseIM Helm Chart makes it easy to install MongooseIM and MongoosePush to Kubernetes. Structured Log Management for chat solutions Humio is a modern log management tool that provides complete observability to your team. Our new structured logging allows you to integrate with log management tools just like Humio to identify, prevent and resolve bottlenecks, poor usage patterns in production and other recurring issues in your system. Instant Messaging metrics WombatOAM is another tool to help you understand what is going on under-the-hood in your system. WombatOAM specialises in giving you visibility on the metrics of your system so you can identify trends and prevent problems arising. This includes allowing you to create automated alarms based on customisable performance metrics such as CPU usage. Aysnchrounous message delivery In complex systems RabbitMQ can be used as an asynchronous message broker. MongooseIM is able to handle the instant messaging between users’ smartphone while RabbitMQ connects these devices to other software systems. Make sure your users get the message MongooseIM 4.0 has just been released. In this release, we’ve gone a step further to ensure an easy to use product for developers, users and a DevOps alike. Explore the changes on GitHub If you need help with the perfect chat solution for your needs, talk to our team of scalability experts. We’re always happy to help.", "date": "2020-10-13"}
]