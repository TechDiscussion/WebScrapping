[
{"website": "Novoda", "title": "Android P Slices: the missing documentation — part 1", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/android-p-slices-missing-documentation-part-1/", "abstract": "Android P introduced the Slice s API, but with only sparse Javadocs and no indication of what they could be used for. After some digging and with a lot of speculation, we present you with the missing documentation for Slice s! What are Slice s? The easiest, and somewhat correct, way to think of Slice s is as RemoteViews v2. Now, that is not a completely correct definition, but will help you understand what we’re dealing with here. If you’ve ever developed a homescreen widget, or created a custom Notification , you’ll know what RemoteViews are. If you haven’t, RemoteViews are the only way to display your own UI inside of another process before Android P — and they’ve been with us since API 1. In that perspective, Slice s are just another way to display your UI inside of another app or process. There are also some very tangible differences with RemoteViews though, and we’ll see what they are in the next installment of this series. A slice is a way to declare a static piece of information and provide indications on how it should be presented to users. It is a serialisable piece of data, not a direct representation of the UI like a layout XML is. Slices also allow for extension with the SliceSpec mechanism, which allows both providers and consumers of slices to negotiate a format they both know. But first, we need to cover the big question: Why are Slice s being introduced? ⚠️ WARNING: intense speculation from here onwards! I believe the answer is: Google Assistant and Google Feed . The reason being that, while the current state of slice rendering doesn’t look finished or polished in any way, I can clearly recognise the same UI patterns that the Assistant and Feed use: Google Assistant An equivalent Slice These admittedly do not look exactly the same, but if you disregard styling, the building blocks are clearly there. Another example: Google Assistant An equivalent Slice They aren’t exactly the same, but the structure is close enough to at least suggest that Slice s could be the foundation for an hypothetical upcoming Assistant API that apps could use to surface their content in Assistant answers, or in the Google Feed, on devices. Again this is pure speculation, but the signs are pointing in that direction. Who can use slices? Everyone can! Although the Slice API has been introduced in the Android P Developer Preview 1, there is a support library component as well which exposes the same public API through the androidx.slice package: Slices provides a framework for apps to embed templated content from other apps. slices-builders contains methods to build content in a template format. slices-view contain methods to present that content. Source: Recent Support Library Revisions — Revision 28.0.0 Alpha 1 Now, Android being Android, when I say \"everyone\" I obviously mean \"everyone on API 24+\" (that is Android 7.0 Nougat), which is the minSdkVersion for the compat package. That said, it's still very nice to see that it's supported on a 2-years old Android version right at launch, although this leaves out still a lot of users and developers, since for many minSdkVersion 19 is still a dream. This article looks at things from the compat point of view as it’s certainly the most interesting and because it ships with the sources and javadoc, which makes it easier to work with the compat library version. That library simply wraps the platform implementation on P anyway, converting the slices back and forth between the platform version and the compat version. At the current state of things, slices are still pretty broken. Let’s see what the ideal flow is, and then we’ll be able to identify where things fall apart. Journey of a Slice , end to end A slice can come from anywhere on a device; any app can act as a slice provider , and any app can consume slices from any other app, including itself, by acting as a slice host : The slice provider app is any app that exposes a SliceProvider , which is a fancy ContentProvider that takes in a Uri (or an explicit Intent ) and emits a Slice . The slice host app is any app that uses SliceManager to bind, pin or otherwise interact with a SliceProvider via a Uri (or an explicit Intent ). Both the provider and host apps will need to: Have compileSdkVersion 'android-P' and targetSdkVersion 'P' Have a dependency on either 'com.android.support:slices-view:28.0.0-alpha1' or 'com.android.support:slices-builders:28.0.0-alpha1' , depending on their role Accessing slices Assuming there is at least one slice provider app installed, a host app will need to know the URI (or, we’ll imply from now on, an explicit intent) for a slice to be able to display it. There are several ways to obtain slices to display: A one-off retrieval calling sliceManager.bindSlice() . Signal interest for updates about a URI by using sliceManager.pinSlice() — this does not actually provide you with updates, you'll still need to call bindSlice() manually. Pinning does not persist across reboots. I am not entirely sure why you would need this as a separate API, as pinning in and by itself is not particularly useful; it’s needed for observing changes, but you still need to fetch the content. It looks like it may have been exposed just so that SliceLiveData can access it. Register a changes callback with sliceManager.registerSliceCallback() , which will also pin the slice for you, and provides a callback with fresh Slice s as they are emitted. Use SliceLiveData to obtain a LiveData<Slice> that you can use to effortlessly update your app as new data is emitted. The SliceLiveData also pins the slice for you. Solution number 4 is by far the easiest, but unfortunately due to an issue in the current release it does not work; pinning is currently broken and if you use it in any form, your app may crash with something like java.lang.SecurityException: Call pinSlice requires full slice access . Besides this little growing pain, the SliceManager and SliceProvider do an excellent job at managing all the permissions for you. Contrary to what error messages may say, you do not need to declare that your app uses the BIND_SLICE permission, nor to define an explicit permission for your SliceProvider in the manifest. In fact, if you add permissions manually you may hit issues, so I would recommend against doing that, even if leaving an exported and unprotected content provider is not a generally advisable strategy. Once a host app gets hold of a Slice , the only thing left to pass it on to a SliceView : val slice = // ...\nsliceView.setSlice(slice) The first time a host app tries to access another app's slices, the SliceView will show a — very crude — notice that the user needs to grant permission to the app to access slices: The user can tap the notice to show a permission dialog controlled by the platform or compat slices framework, where they can grant or deny permission to access a certain app's, or all apps', slices: After that, if the host is observing for slice updates, they will receive the actual slice content and can display it on their SliceView . Or at least, that's how it's supposed to work. Unfortunately, a bug in the current version means that when the SliceView displays the permission slice, it fails to make it clickable, so hosts cannot display the actual slice content since they never get to ask for permission. The permission slice does, however, contain the pending intent that should be triggered as the SliceAction of its first sub-item. You could then, until this is fixed, proceed to identifying permission requests based on a few characteristics: private fun Slice.isPermissionRequest(appName: String): Boolean {\n  // Permission slices have only one item, whose action is a PendingIntent to\n  // show the permission granting activity (which shows a dialog). We cannot\n  // easily check the contents of the action PendingIntent, so for now we just\n  // assume that if it's not null it's good enough. We'll check other signals\n  // afterwards to verify if it's really a permission slice.\n  if (items.count() != 1 || items[0].action == null) {\n    return false\n  }\n  // Permission slices contain one slice item, which contains a slice item which is\n  // a FORMAT_TEXT with a fixed format\n  val subItems = items.first().slice.items\n  val firstSubItem = subItems.firstOrNull()\n  return firstSubItem?.format == android.app.slice.SliceItem.FORMAT_TEXT &&\n    firstSubItem.text == \"$appName wants to show Slice Provider slices\"\n} This code assumes the slice provider you are connecting to has the name Slice Provider . Currently there is no way to discover slice providers, their names, or any details about them, so you have to hardcode the name, or use a laxer check on the text. Edit: After this post was written, this issue has been addressed, so you can now discover Slice providers by doing the following: fun getSliceProviders(): List<ResolveInfo> {\n    val intent = Intent().apply {\n        addCategory(\"android.app.slice.category.SLICE\")\n        action = Intent.ACTION_VIEW\n    }\n    return getPackageManager()\n            .queryIntentContentProviders(intent, PackageManager.GET_META_DATA or PackageManager.MATCH_ALL)\n} Thanks to Andrew Pietila for pointing this out! And then, based on that detection, could hack your way to the permission dialog: internal fun SliceManager.requestPermission(sliceUri: Uri, activity: Activity) {\n  try {\n    val intentSender: IntentSender? = bindSlice(sliceUri)!!.permissionRequestPendingIntent.intentSender\n    activity.startIntentSenderForResult(intentSender, REQUEST_CODE_PERMISSIONS, null, 0, 0, 0)\n  } catch (e: IllegalArgumentException) {\n    Toast.makeText(\n      activity,\n      \"Looks like we've hit a bug in Slices, likely fixed in alpha2. \" +\n        \"Uninstall the app and reboot the device/emulator.\",\n      Toast.LENGTH_LONG\n    ).show()\n  }\n} There is a try-catch in there because the slices framework in some cases can get confused and lose the permission state, therefore we need to caution ourselves. It is a rare case, and I could not reproduce it reliably, but better safe than sorry. Once this bug is fixed, you won't need all this anymore. Styling a slice I am sad to say, there's very, very little you can do to control the appearance of a slice inside of a SliceView . Unfortunately this seems to be by design, but I have put in a feature request to get more customisation options on the rendering side. One alternative approach would be to create a new view implementation that can render slices, basically reimplementing the androidx.slice.builders package almost in its entirety. Rendering slices is no trivial matter and I would recommend not embarking in such an adventure unless there really is no alternative, and a very stringent requirement. Very, very few host apps should ever need to do this. I could imagine the Google Assistant being one of these; it could either implement a different way to render slices so they adapt to its UI design, or an hypothetical SDK on the provider side could simply declare a new set of supported SliceSpec s which the Assistant app knows how to render properly, sidestepping the built-in list slice and subitems. What you can theme with the existing API is: Override the tint colour, using SliceView.setTint() or the tint attribute in the view style — by default, slices use the host's colorAccent All images that are not big images (e.g., a LARGE_IMAGE in a grid cell) It is not possible not to tint icons in slices, unfortunately It is not possible to define a per-item tint, all items are tinted with the same colour The tint is applied as a color filter on the images, which means the tinting mode is SRC_ATOP — this means that translucent tints lose their alpha and become fully opaque Set the display mode, using SliceView.setMode() The supported modes are MODE_SMALL , MODE_LARGE , and MODE_SHORTCUT MODE_LARGE is the default, uses a large template (i.e., a RecyclerView that can contain multiple rows of various types) MODE_SMALL will only show the first item, be it a list row or a grid item MODE_SHORTCUT will only show a shortcut using the slice icon, title and action (or fallbacks if those aren't available) Make the slice scrollable or not scrollable, using SliceView.setScrollable() This only applies to MODE_LARGE , since it's the only mode that shows more than one item Set the title and subtitle colours for items using the titleColor and subtitleColor attributes in the view style Their defaults are the theme's textColorPrimary and textColorSecondary , respectively There is no corresponding API in code for this Set the text size for various items, using the respective attributes in the view style: headerTitleSize , headerSubtitleSize , titleSize , subtitleSize , gridTitleSize , gridSubtitleSize There is no corresponding API in code for this A few growing pains Currently, there’s a series of bugs and limitations that I have spotted in the slice framework implementation. I have added to the appendix all the issues I have reported so far to the Android P Preview issue tracker . If you are building an app that uses slices today , you will want to check that list out to know what to expect. In the next part of the series In the second part of this series is an explanation of how to create a SliceProvider to provide slices to other apps, how a slice is structured, and a few more issues and tricks. Besides that, we'll try to come up with some ideas on how slices could be used to create rich SDKs that allow apps to integrate pre-built pieces of UI they don't need to, or should control. You can find the source code for the sample app accompanying this series on GitHub . Android P Slices: the missing documentation — part 2 Appendix: list of issues encountered in P DP1 and 28.0.0-alpha1 While developing the sample app, I hit several issues with the first preview and alpha build of slices. Here’s a rundown of the ones that concern an app that wants to host slices: Anything that implies pinning a slice makes the host app crash ( #75001463 ) Running on Android P DP1, you get a dialog that informs that a bunch of gray listed methods are being called ( #75001461 ) If you inspect the logcat you can see a bunch of messages complaining about access to hidden methods; this seems connected to some LiveData internals calling gray listed methods You can have a header, but if you don't, the first item becomes automagically a header, and you can't avoid it ( #74917014 ) Except for the defaults of having a slightly larger text size, a header looks exactly the same as a normal list row Slice actions are not displayed ( #74889520 ) If you're in large mode then the slice actions should be displayed in the header, but this is not documented anywhere If you're not, they're supposed to show in a LinearLayout right below whatever is the slice content that is being displayed When you use a GridBuilder you would expect to get a grid out of it, but what you get is a one-line horizontal list of items in a list row ( #74889524 ) The list of items can only host up to 5 items, including a \"see more\" item There is no way to create a scrollable carousel with these items All \"see more\" items look exactly the same as any other item, so I am not sure why you would need one ( #74889525 ) If your slice contains an Input Range (basically, a slider), then its action will be triggered immediately when the slice is bound to the view ( #75004842 ) Input Ranges must define an action, so that's an unavoidable issue You could work around it using sliceView.setOnSliceActionListener() and ignoring the first action after binding if it has an EventInfo.actionType of type ACTION_TYPE_SLIDER but there is no way to suppress actions from being carried out, only to know they happened Until the bug is fixed it's recommended not to use Input Ranges There is no programmatic way to know if you have the user permission for accessing slices from a certain app, or any app ( #74889527 ) There is also no programmatic way to request permissions to show slices except at the point where the first interaction happens via the SliceManager , and there is no way to provide a custom onboarding UI leading to the permissions request dialog You can work around it and hack your way to your goal, but that's not really recommended unless you have no alternatives The built-in “permission needed” slice is not clickable ( #74917009 ) There is no built-in discovery mechanism for slices, and no way to list all the slice providers on the system ( #75245750 ) This also means there is no direct way to check if a certain slice provider is available without going to the implementation detail of slice providers being content providers and using the package manager to check But that is limited in that there is no way to check if a content provider is a slice provider without knowing its authority and adding it to a whitelist There is no easy way to provide support for custom SliceSpec s in the SliceView — what is built in to it is all you can have ( #74917012 ) There are no hooks into the slice rendering process There are no hooks into the slice views and their styling except what outlined in the previous section A list row with only a titleItem, or endItem image makes the SliceView crash ( #75001462 ) This can be worked around by also assigning an empty title to it: RowBuilder(listBuilder).setTitle(\"\") ... A bunch of necessary APIs are marked as restricted to the library/library group, but are necessary to use unrestricted APIs ( #75001467 ) androidx.slice.widget.SliceView.SliceMode , androidx.slice.widget.EventInfo.SliceRowType , androidx.slice.widget.EventInfo.SliceActionType , androidx.slice.widget.EventInfo.SliceButtonPosition , androidx.slice.Slice.SliceHint , androidx.slice.SliceItem.SliceType , androidx.slice.builders.GridBuilder.ImageMode , androidx.slice.builders.MessagingSliceBuilder A bunch of toString() s in public API classes, which makes little sense to me While the slice items rendering is rather inflexible, but some tricks can be applied to get a similar result to what you might want. We'll see a bunch of them in the next part of this mini-series. Thanks a lot to everyone that helped me making this article more readable and interesting, and to those who helped me dig into Slices!", "date": "2018-03-16"},
{"website": "Novoda", "title": "GOTO Conference Berlin 2017", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/goto-conference-berlin-2017/", "abstract": "The 5th edition of the GOTO Berlin conference took place on the 14th-17th November. We didn't want to miss this opportunity to learn from the some of the best in the software industry so we went along to attend some of the workshops and talks at the event. The GOTO Conferences are created for developers, by developers with events in Chicago, Amsterdam, Copenhagen and Berlin. Having an office in Berlin we didn’t want to miss the opportunity of attending this event which took place in the Berlin Congress Center. The Conference took place a block away from Alexanderplatz On the first day of GOTO Berlin there were only workshops running, there was a wide variety of topics and all of them sounded really interesting. Here’s a summary of the workshops we attended. Working Effectively with Legacy Code by Michael Feathers We were excited about the workshop since we had greatly enjoyed the book by Michael Feathers with the same name. The content was half presentation style, half coding exercises. The most interesting part was hearing about Michael’s experience and learnings from decades of software engineering and consulting. The coding examples had just the right size to focus on a certain aspect of refactoring and testing. All in all we really enjoyed that day, thanks Michael. Volker Leck and Tobias Heine Data Science & Analytics for Developers by Phil Winder I really enjoyed this workshop. It was a great introduction to practical data science and machine learning problems. The instructor gave a lot of good tips that would help an aspiring data scientist. During the workshop we followed the following modules: Introduction. A short introduction to the python programming language . Basics. Fundamental terms, such as mean , variance , entropy , etc. and how to calculate them. Data. We learned how to use the most common data science python libraries, which included pandas , matplotlib and numpy among others, to normalise and plot data in different ways so that we can get as most insights from it as possible and find out which features are important and which ones aren't. Regression. Here's where the Machine Learning part began and it got quite fun as we started using scikit-learn to play with linear classifiers, such as linear regression and support vector machines to solve linear problems. Generalisation. Where we learned about learning curves, overfitting, underfitting and how to visualise them to make sure we can understand when we're making mistakes. Nearest Neighbours. In this module we got into the unsupervised learning algorithm known as k-nearest neighbours , which can be used to crete simple recommendation algorithms, and more generally, to find 'similar' stuff. Clustering. This was the last module and we learned how to use agglomerative clustering and dendograms to figure out the structure of our data and find what features represent a cluster. Phil using the whiteboard to explain the ins and outs of data science These modules were distributed as a list of Jupyter notebooks and they were a great way to get started in solving practical problems using data science techniques and some machine learning algorithms. Xavi Rigau On the second and third days there were plenty of talks split in 5 tracks each day (10 in total) and each track had a specific topic (i.e. Connected World, Agile, Programming Languages, etc.). Here are a the talks that stood out for us. Drones on Broadway by Raffaello D'Andrea Professor D’Andrea gave a talk on how they created a breed of interactive and autonomous flying machines that have successfully been operated within Broadway shows and concerts. It was interesting to learn about the technical challenges his teams had to solve in order to make drones fault-tolerant and safe to operate above a crowd of people. An amazing talk about the state of robotics, with lots of compelling little demo videos and first-hand experience and learnings. Volker Leck Number Crush by Hannah Fry Hannah Fry presented how we can use mathematics to identify and explain a lot of behaviour and social patterns in our societies. She explained how we can model real life problems with mathematics, so that we can create formulas that estimate how likely it is for a couple to break up, based on the number of arguments in the relationship. She also showed really cool projects she worked on, such as one that showed how the public transport in London moved during a 24-hour period. Hannah presenting the science behind long-lasting relationships This was a really inspiring talk, would totally recommend watching some of Hannah's videos on YouTube . Xavi Rigau To sum up, we definitely enjoyed attending the GOTO Conference, and we’re looking forward to attending the next edition.", "date": "2018-03-15"},
{"website": "Novoda", "title": "6 Winning Monetisation Strategies", "author": ["Daniel Blatchford"], "link": "https://blog.novoda.com/6-winning-monetisation-strategies/", "abstract": "We’ve all been there:  the CEO has a great idea and just wants to get started with building, without any agreement on the goals, or even who this new feature or product will be targeting. The focus is on launching the product as quickly as possible, so that the money can start pouring in. But monetisation is a tricky beast. Get it right, and you have an audience of happy users, who are willing to pay for your product. But misprioritise revenue targets over other Key Performance indicators (KPIs), such as active or repeat users, and your audience numbers quickly drop, along with your revenue. There is no right answer regarding how a product generates revenue, or when that is appropriate. For some, it will make sense to acquire a significant number of users before pursuing revenue, while for others, generating revenue will be one of the first things that needs to be achieved in order for the app to succeed.  And there will, of course, be some apps that never monetise, such as public sector apps. As a quick TL;DR on the subject of monetisation: as with most things, your approach needs to based on your knowledge of your product, your audience, the competition and your business goals. There simply isn’t a one-size-fits-all solution, and your success will largely depend on your research, understanding and testing. But I’ve outlined six of the most common strategies below - they were chosen on the basis of those we explored with CCleaner, a project that saw us increasing revenue by 388% over 12 months.  Each is worth considering when it comes time to put together your own monetisation strategy. First, let’s deal with the basics. So what is a monetisation strategy anyway? Simply put, it’s a plan to generate revenue for your product. Like with any plan, it’s not something that is fixed - it should be flexible enough to develop with the product, the market the product exists in and its users. Goals can and will change over time, and so strategies need to evolve to continuously achieve the goals they’re designed to target. How do we measure the success or failure of a monetisation strategy? To enable us to learn from our hypotheses and adapt our approach, we first need to define appropriate KPIs, which in turn, requires us to understand our market and users thoroughly. With CCleaner, we examined the available data to determine that Total Cleans, Cleans per User, Average Ratings and Revenue were all KPIs. We also defined the countries that we wanted to perform the tests in initially. Your KPIs will, of course, be specific to each product - a news app will probably want to measure how many articles each user reads, while a fitness app might want to know the average miles run per workout to figure out how dedicated their user is to their fitness. Regardless of the KPIs you choose, regular assessment is crucial. For CCleaner, we performed monthly check-ins, where we reviewed the data with the key stakeholders and decided on the next steps of the rollout. Once we were satisfied that our strategy wasn’t negatively impacting our KPIs, we would expand to the next country, until we eventually achieved the global rollout we planned. 6 classic monetisation strategies Since there’s more than one solution, it’s worth carefully considering all of the  strategies below. Your decision should be based on your knowledge of your product and your audience, which will you allow you to prioritise these, and perhaps consider a mixed solution (more on this later). 1. Advertising This is likely to be the first revenue stream to test, as it’s often far simpler to introduce a freemium model (a free ad-supported version and paid-for version without ads) as a tester to a larger monetisation strategy. There are simple solutions to test this relatively quickly in your app, such as Google's AdMob. Implementing Google's Native Ads Advanced is an effective solution to monetise your user base with simple banner ads. This was our first experiment with CCleaner. Thanks to our status as a Google Certified Agency, we were able to get access to a beta program of native ad integration - advertising that is shown using native UI components, meaning faster, less intrusive ads.    Using Firebase, we implemented Native Ads Express behind Remote Config. This allowed us to control which users, using which devices, were targeted within each country. This was crucial, since Google had recently disabled a crucial function of the app for newer Android devices. By disabling advertising for these users, we didn’t negatively impact their experience, which allowed us to maintain our rating - a KPI that we were closely tracking. Remote Config was essential to help us limit the impact of the advertising. Going one step further, we were able to A/B test things like changing the colour of the ‘Upgrade to CCleaner Pro’ button. This enabled quick testing of the colour, as well as the location and type of ad that was most effective in terms of revenue, or least impactful on other KPIs. These kind of expertise are the reason that we’re part of Google Developers Agency Program, which is further explained in this video. There are, of course, many different ad networks that can, and should, be implemented if advertising is core to your monetisation strategy. The Chinese market, for instance, has ad networks specifically tailored for that market, which are designed to work within the various restrictions that advertising in this heavily regulated country entails. As our CCleaner advertising rollout progressed, we started to experiment with Facebook advertising, in addition to AdMob, and found that the Facebook adverts had a far higher eCPM than the AdMob ads. (eCPM is the ‘effective cost per mille’, which is calculated by the ad revenue generated by a banner or campaign, divided by the number of ad impressions of that banner or campaign, expressed in units of 1,000.)  As a result of that, Facebook advertising was prioritised in the second iteration of the advertising strategy. Pros: For apps with a large user base, ad revenue can be substantial and is automated, thanks to the ad networks available. This isn’t a revenue strategy that needs to run alone. Displaying adverts naturally lends itself to offering a non-ad based Pro version, allowing you to upsell your users to a more valuable proposition. Cons: Advertising comes in many forms, from intrusive pre-rolls to more subtle or relevant banner adverts. With that comes the potential to annoy users, and ultimately negatively impact app store ratings, which is why this is such an important KPI to measure. Mitigating this is a question of experimentation, as well as finding the right ad platform for your audience. Tailoring the types of ads that you allow to appear within your app can be a time consuming process. As well as ensuring you’re not advertising your competitors, you may also want to consider if there are any types of ads that are you don’t want shown, such as for religion, smoking or alcohol. 2. One time in-app purchase This often comes in the form of an an upgrade to a Pro tier, which offers more features, or fewer/no adverts. This is something we implemented for CCleaner, offering a Pro version that had both more features and was ad-free. It was crucial to make sure that we didn’t degrade the experience for those that were already regularly converting to Pro, so we sliced our market into: Countries where users were more likely to upgrade to Pro. Countries where users were least likely to upgrade to Pro. We initially tested our advertising/Pro strategy with the second group, tracking our chosen KPIs over time, while slowly enabling ads for more users in those countries, and then beyond. The KPIs we tracked were: Average Revenue Per Paying User (ARPPU) Average Value per Transaction Revenue by country Cumulative Average Rating Installs on Active Devices Uninstalls by Device We made sure that these didn’t dip to a predefined level, rolling out advertising slowly. Eventually, we achieved worldwide rollout. Pros: This approach is quick to implement, so it’s easy to test your market. A one-off payment, when the proposition is right, can generate income quickly. And experimenting with what that proposition is can be easily A/B tested, in terms of things like the price point you set, and the way you frame the offering. Cons: A one-time payment is just that: one time only. This caps your revenue, with the only route to more being based on acquiring new users - and depending on your market and your product, you may reach the limit of that quicker than you’d like. 3. Subscription model As the name suggests, subscriptions, if implemented correctly, allow for a recurring, periodic payment. This can be more complicated to implement, but has a longer-term benefit. When considering the former, it’s worth asking yourself if you’re able to do the work that comes with offering a subscription service. For instance, will you offer refunds? Will your users expect more regular updates if they’re paying monthly (or otherwise)? How would this affect your existing development pipeline?  Do you have the resources to cope with all of the above? Pros: It’s increasingly common for apps to offer a subscription model, and it’s something that users are comfortable with - as long as they feel like they’re getting value for money. Apple actively encourages subscriptions, offering a 85/15 revenue split, versus the standard 70/30 split for non-subscription apps. A regular revenue stream makes for a more predictable financial outlook, so planning, development and committing to marketing efforts becomes easier. You can incentivise people to upgrade for longer easily, offering a discount on 6 or 12 month subscriptions. Cons: As mentioned, this strategy is more complex to build. It also requires ongoing commitment from users, so you need to have a plan for keeping users engaged. If you’re collecting subscriptions from all users on a certain day, you will see your revenue uplift only once per month. 4. Gamification This can take many forms, but overall, involves defining the goal you’d like to pursue - app session length, for instance - and a game to drive user behaviour to meet that goal. This isn’t a standalone monetisation strategy - rather it’s based on advertising revenue, and/or incentivising people to upgrade to Pro. One example is creating an in-app digital currency, which can be earned by watching an advert, for example. These 'coins' can then be exchanged for rewards, such as limited access to Pro features, or going ad-free for a limited time. Pros: You only have to look at the app store to see that humans love games, and are easily incentivised through gamification. This makes it a great way to showcase your Pro tier -  you’ll get lots of people trialling it, who can then be upsold to the paid version. This keeps users engaged within the product's ecosystem, as they work to unlock levels, collect coins or whatever else you’ve created to gamify the experience. Cons: This can be large and complex,  and requires a degree of creativity. It’s not uncommon to require a dedicated team to own this in its entirety. 5. Invitation rewards As the name suggests, this involves offering rewards to users who successfully entice friends or family to join the app. Again, this isn’t directly a monetisation strategy. Rather, it answers another KPI: user numbers. Depending on the app, these new users can be monetised through advertising etc - although for many just increasing the user base (and therefore increasing sales) is the aim. It’s a strategy that is popular with extremely well-funded start-ups, who are engaged in a land grab for customers in a new market. Uber, for example, gives each customer account credit for each new customer they introduce, although the amount has decreased over time, as the service becomes more ubiquitous. Pros: As evidenced by Uber et al, this approach can quickly aid growth for new products into a market. Cons: Paying for user acquisition is a strategy with an expiration date - it simply can’t be sustained forever - and you need deep pockets to begin with. 6. All of the above? But why settle for only one solution? With CCleaner, we found that users across different countries reacted differently to advertising and in-app purchases (IAP), so opting for a hybrid approach was the best solution for this product. We tested this with CCleaner, in territories where users were more likely to upgrade to Pro, validating our hypothesis that a mixture can work best. We found that we could drive IAP by displaying adverts, while still monetising free users. Interestingly, this approach was particularly effective in the German market, where we not only saw a spike in ad revenue following the initial enabling for all users, but a general increase in IAP from users who didn’t want adverts, but until that point had chosen not to upgrade to Pro. This combined monetisation approach allowed us to take advantage of both models, while not overwhelming us. Thanks to Firebase, we were able to test things like the placement of the adverts, and the most effective colour of the ‘Go Pro’ button, to fully optimise each model to increase revenue as much as possible. And the results were impressive: in-app upgrade increased by 211% worldwide, and overall revenue, including ad sales, increased by 388% . Ultimately, this success was as much down to research and testing as it was the quality of the app itself. By rolling out advertising slowly and in a controlled manner, first to the markets that were contributing the least financially, negative impacts were minimised. This, in turn, allowed us to use agile methodologies to iterate on our findings, so that by the time we came to use advertising in high IAP value countries, we were several versions into the strategy. The result? The rating of the app was maintained at 4.4 stars worldwide over the 12 months we spent rolling out the advertising worldwide.", "date": "2018-09-20"},
{"website": "Novoda", "title": "Novoda Named as a Top Global Service Provider", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/top-global-service-provider/", "abstract": "Novoda is a top mobile app development company according to Clutch. As the app development space becomes more crowded, the need for quality products has grown. At Novoda, we strive to create measurable results with high-quality design and strategy. Our clients are at the core of what we do – and we devote time and resources to understanding their business needs and goals . We’re proud of our commitment to creating the best products . This dedication to our clients allowed us to be recognized among Clutch’s annual list of the Top 1,000 Global Service Providers, as well as a top mobile app developer in the UK. Clutch is a B2B ratings and reviews firm in Washington, D.C.. Clutch’s methodical approach to client reviews puts a company’s ability to deliver at the forefront, and we are proud to be included as an industry leader in their recent Global Press Release among other top mobile app development companies. We were also named as a top UK mobile app development company on The Manifest , a site that focuses on providing business insights and how-to guides for readers. We are so grateful to our clients who provided such complimentary, detailed reviews about their finished products and our services. One client called us their “go-to people when we need top-quality work done well and done quickly. ” We appreciate the time they took to work with Clutch and get our reviews featured on the site. Interested in reading the full reviews? Check out our profile on Clutch to see where we rank and what we can deliver. If you like what you see there, let us know and let’s see how we can work together to create something great.", "date": "2018-12-20"},
{"website": "Novoda", "title": "What IS real? Augmented Reality Testing with ARKit", "author": ["Dimitris Karavias"], "link": "https://blog.novoda.com/augmented-reality-testing-with-arkit/", "abstract": "AR has been one of the up and coming technologies of the last few years, already known as the amazing tech behind Pokémon Go, Snapchat's animated emojis, and Instagram's 3D stickers. Let’s walk through the unique challenges it presents to QA and the testing process. Introduction AR was one of the technologies that caught my eye in 2017 but I never got to try it out first-hand. When I was presented with the opportunity to work on an internal research project that involved AR and Machine Learning I jumped at the task enthusiastically. It's worth clarifying where this testing effort fits within the testing pyramid. If you haven't heard of it before, the testing pyramid is a guideline on the ratio of the different test levels we include in our project: In this post we're looking at the higher levels. While we do consider Integration Tests in the \"Solutions\" section by using the feature point clouds, we only performed exploratory UI testing. Unit tests would be done the same way since we're still adding Swift code. As a QA I naturally decided to first have a look at the test support Apple provides. The search was brief as there isn’t any yet (other than some debug options, which we’ll cover in the “Solutions” section). That’s ok though since testing is about one’s mindset, not the toolset. I followed this tutorial and made a simple app to get familiar with ARKit. Together with my fellow Novodans we then got working on building the app  as described earlier in this series . The end goal was a simple one:  add a wireframe to an object within an ARSCNView . The object itself is recognised using Machine Learning but that is a separate topic . For this post we’ll consider the AR side of the app only. Testing Challenges All testing is done on physical devices as simulators cannot run the Camera app. The first thing that struck me is that there is no UI in our Demo App - at least, not in the typical sense. Once we've launched it the only UI elements are the 2 buttons at the bottom of the screen: One of these bananas doesn't exist! The only user inputs are touches and the only events are the addition and removal of 3D models. Accessibility is thus already an issue that we’d have to look into before considering a production version. The second observation is that testing AR is more time consuming than traditional testing activities. One needs to get up, manually try different scenes, objects, lighting conditions, etc. Compared to typing values in a text field this is physically slower and much less consistent. Consistency in particular is an area of concern. As per Apple’s Developer Documentation: ARKit does not guarantee that the number and arrangement of raw feature points will remain stable between software releases, or even between subsequent frames in the same session Since feature points form the basis of the app’s view of the world it becomes clear that we simply can’t automate our efforts using XCUITest. If you need an introduction to XCUITest head over to our Getting started with XCUITest post. Every screenshot could potentially be different since the view is built upon constantly changing input data (our scene) despite our best efforts and this is just for a static viewpoint, we didn’t even try a moving setup. Putting aside everything else the biggest challenge is simply the lack of maturity of the field. Most developers are working with the APIs for the first time and trying to figure out the do’s and don'ts along the way. We’re probably a long way from solid testing frameworks, good practices, or reliable CI. Solutions Let’s try to address the above difficulties. “No UI” isn’t necessarily an issue. If anything, we have fewer inputs to worry about. Screenshot comparison on a physical device is a no-go due to the scene consistency issues mentioned above. However, ARKit provides us with the ARPointCloud . If we capture the full set of points we can use that to provide consistent input. The full set of points can be accessed using one line: self.sceneView.session.currentFrame?.rawFeaturePoints?.points This still just provides a single capture but we can then save consecutive sets of points to model the scenes we want to test. Printing a full Point Cloud gives something like this, where every float3 is a detected point in 3D space: These correspond to the highlighted points here: To get the points to show on the screen, you’ll need to enable them by adding the debug options to the viewDidLoad method of your view controller: sceneView.debugOptions =\n[ARSCNDebugOptions.showFeaturePoints] You can also add the 3D World Origin marker to the above which represents the 3 axes starting from the [0,0,0] point in the current scene using this: sceneView.debugOptions =\n[ARSCNDebugOptions.showFeaturePoints, \nARSCNDebugOptions.showWorldOrigin] The Origin marker is particularly useful when experimenting with newly imported 3D models which might not get added at the expected location. This could be due to an accidental offset or scaling issue, since 3D modelling tools often use measurement units different to those used by Xcode, as mentioned in the Designing for AR part of this series. The stats and fps counter at the bottom of the screen can be enabled with this line: sceneView.showsStatistics = true With the above data in place we could validate our app’s behaviour by asserting that the wireframe always gets added to the same point given a specific Point Cloud. We didn’t get to implement this solution as part of our experiment as we invested our time in building the app and learning more about AR but watch this space for our first attempts at automated testing based on Point Clouds. Further opportunities In this highly recommended talk (requires a Ministry of Testing free membership) BJ Aberle predicts that future AR testing will be done with game engines. He anticipates test environments built in Unity and Unreal. For me this is fascinating and makes perfect sense. By doing so the uncertainty of the real world is controlled but at the same time we have entire 3D worlds instead of just point clouds. There would probably be a lot of work involved in order to pass the Unity/Unreal scene as an input to ARKit but the opportunity to define the testing standards & methodologies for a whole new class of applications is part of what makes these experiments so exciting! From there automated testing (both scripted and exploratory) becomes possible which then allows for CI. There’s another advantage of using game engines: At companies like Novoda we are always looking to improve remote collaboration. For the purposes of this project, pairing remotely on ARKit development involved one person trying things on an iPhone and either reporting back verbally or having to share their screen via QuickTime, which meant disconnecting from Xcode. Not an optimal experience. Imagine entering a 3D world and just interacting with the scene together with your teammates.  Now, let’s think about accessing that world on a VR platform. The VR world is considered “reality”, and a QA will be testing the Augmented Reality objects which are “not-real”. We are currently starting to look into this type of integration between Unity & ARKit and will report back with our findings. Conclusions In this post of our AR series we looked at the testing challenges & opportunities presented by the exciting field of Augmented Reality. The code for the demo app used can be found on Novoda’s GitHub and you can also check our ARDemoApp repo , where you can import your own models into an AR Scene without having to write a line of code. Any comments or questions? Hit me up on Twitter @KaraviasD", "date": "2018-03-06"},
{"website": "Novoda", "title": "Getting Started with ARKit", "author": ["Berta Devant (IOS Developer)"], "link": "https://blog.novoda.com/getting-started-with-arkit/", "abstract": "Apple’s ARKit API, makes the exciting world of Augmented Reality available to every iOS developer, but where do you get started? Come with us on an Augmented Reality journey to build an AR solar system and learn how to make your first ARKit application. This post is from the multi-part series on ARKit, where we talk about designing for AR, building a demo app, exploring and testing many of the features of ARKit. We previously wrote on designing 3D models for AR apps . Introduction AR is the core technology behind amazing apps such as Pokémon Go, Snapchat's animated emojis, and Instagram's 3D stickers. Apple’s announcement of ARKit at WWDC in 2017 has already resulted in some impressive software, with a mix of fun and practical apps providing something for everyone. We wanted to have the opportunity to play around with it and see what incredible things we could build with it. Over the past year Novoda have been investigating the features of ARKit, seeing what we could built and what were the limitations of the technology, we had tons of fun building things with it and wanted to share some of our findings. Setting a house as a hat is best way to test location placement, they say We will be using a custom 3D model created in the design part of this series for this demo. Even if you cannot create your own custom model, you could use the simple AR cube [1] that Apple provides or download a model from SketchUp or Google's Poly The first thing to understand is how AR perceives the world through the device camera: it translates the camera input into a scene composed of planes, light sources, a virtual camera, and Feature Points. ARKit recognizes notable features in the scene image, tracks differences in the positions of those features across video frames, and compares that information with motion sensing data. The result is a high-precision model of the device’s position and motion that also analyzes and understands the contents of a scene. If you want a more in depth analysis I highly recommend you read this page About Augmented Reality by Apple or watch their WWDC 2017 talk on ARKit . I would aslo recommend to watch Understanding ARKit Tracking and Detection talk and ARKit2 video from WWDC 2018. How a model with planes and light source looks on Xcode. This will be added to an AR scene With this World Tracking and Plane Detection ARKit is able to create Feature Points , Feature Points are used in ARKit to place models on the scene and to have the models anchored to their \"surroundings\". As Apple explains: These points represent notable features detected in the camera image. Their positions in 3D world coordinate space are extrapolated as part of the image analysis that ARKit performs in order to accurately track the device's position, orientation, and movement. Taken together, these points loosely correlate to the contours of real-world objects in view of the camera. Using ARView and ARSCNView To build the AR app we followed a series of tutorials AppCode ARKit Introduction , AppCoda ARKit with 3D objects , Pusher building AR with ARKit and MarkDaws AR by Example , as well as the documentation on AR classes Apple provides. Since most of the basic setup has already been covered by Apple and by other tutorials we will not post all the code here, just go through some of the logic, issues and solutions we found along the way. All the source code for this and all the following posts related to this project can be found on our GitHub . The first decision to make when creating an ARKit project is whether to use a standard one-view app template or the AR template Apple provides. We have tried both and found little difference when it came to simple apps/demos. The AR template is set up to use storyboards and has a pre-configured ARSCNView with a model of a plane. If you like playing around with working code before you write your own, we would recommend the AR template, especially as it comes with some clear explanatory comments. Alternatively, if you like having control of every piece of code it is obviously better to start from scratch. For this demo we used the template and storyboards but even if you create the project from scratch you should be able to follow along. There are some key points every AR app needs: You will need an ARSCNView . Most people name their instance sceneView . This is where all the AR magic happens. You can set it to occupy the whole screen or simply as a part of the UI. You need to implement the ARSCNViewDelegate protocol which includes the methods used to render the models into the View. The sceneView controller will implement this protocol and be the delegate of the View. sceneView.delegate = self ARConfiguration needs to be set up with the type of plane tracking you want (horizontal is the default) and then added to the sceneView session run() method to actually start the AR scene. ARSession override func viewWillAppear(_ animated: Bool) {\n        super.viewWillAppear(animated)\n        \n        // Create a session configuration\n        let configuration = ARWorldTrackingConfiguration()\n        configuration.isLightEstimationEnabled = true;\n        \n        // Run the view's session\n        sceneView.session.run(configuration)\n    } On viewWillDisappear we pause the sceneView session to stop the world tracking and device motion tracking the phone performs while AR is running. This allows the device to free up resources. override func viewWillDisappear(_ animated: Bool) {\n        super.viewWillDisappear(animated)\n        \n        // Pause the view's session\n        sceneView.session.pause()\n    } This is the basic configuration you need for every AR scene. None of this code will add any AR object just yet though, only set up the view. Apple’s pre-made template then sets up a scene directly by initialising it with a model asset at launch. That is straightforward and works well if you simply want to open the app and have a model appear in the scene. If you want to let the user choose where to place the object (for example by tapping) then you’ll need to put in a little more work. Before we move forward I highly recommend you add this to the viewDidLoad method of your view controller: sceneView.showsStatistics = true\n    sceneView.debugOptions = [ARSCNDebugOptions.showFeaturePoints,\n    ARSCNDebugOptions.showWorldOrigin] Enabling these options will allow you to see the recognized Feature Points and the XYZ axes of the AR scene. If there is any bug with your model these features are one of the few ways you can debug AR. We’ll dig deeper into how you can test and debug AR and ML applications in an upcoming article of this series. With Feature points options on debug enabled you are able to see what ARKit is recognising as you move around your plane aka yellow dots You can also have the world origin showed, this is especially good for debugging position based models since the beginning of the 3 lines is where the AR scene considers 0,0,0. Knowing where that is in \"real world\" space can help you figure out why your model does not seem to appear on the screen. Now for the fun part: adding your 3D model to the sceneView !  Instead of creating a scene with an asset you can create a SCNNode then place that node onto the sceneView at a specific point. We are using nodes here instead of SCNScene because a SCNScene object occupies the entire sceneView , but we want our model in a specific point of the scene. To create the SCNode we first load a temporary SCNScene with an asset and then save the scene’s childNode as the node we are going to use. We do this because you can't initialize a node with an asset but you can load a node from a loaded scene if you search for the node by name. [2] guard let scene = SCNScene(named: assetPath) else {\n      return nil\n}\nlet node = scene.rootNode.childNode(withName: assetName, recursively: true) Note that AssetName here is not the fileName of the asset but rather the node name of the model itself. You can find what nodeName your model has just by opening the .dae or .scn file in XCode, and toggling the Scene Graph view, which will reveal the layer list of the file. How to set up the name of the node on the scene After getting the node, the next step is adding it to the scene. We found two different ways to do it, and choosing one or the other depends on how you want your app to work. First, we need to know where to render our model within the 3D world. For our demo we get the location by getting the user tap CGpoint from the touchesBegan method: guard let location = touches.first?.location(in: sceneView) else { return }\n\nlet hitResultsFeaturePoints: [ARHitTestResult] = sceneView.hitTest(location, types: .featurePoint)\n\nif let hit = hitResultsFeaturePoints.first {\n  let finalTransform = hit.worldTransform\n} Getting a location CGPoint and translating it into a float_4x4 matrix with the worldTransform method. The location variable we are getting from the above example is a 2D point which we need to position in the 3D AR scene. This is where the Feature Points mentioned above come into play. They are used to extrapolate the z-coordinate of the anchor by finding the closest Feature Point to the tap location. sceneView.hitTest(location, types: .featurePoint) You can also use the cases .existingPlaneUsingExtent and .estimatedHorizontalPlane to get the positions of the planes when using planeDetection This method gives us an array of the closest ARHitTestResult , sorted by increasing distance from the tap location. The first result of that array is therefore the closest point. We can then use the following let transformHit = hit.worldTransform that returns a float4x4 matrix of the real world location of a 2D touch point. Plane Detection Now that we have the location of the touch in the 3D world, we can use it to place our object. We can add the model to the scene in two different ways, choosing one over the other depends on how we have set up our ARSession and if we have planeDetection enabled. That is because if you run your configuration with planeDetection enabled, to either horizontal or vertical detection, the ARSCNView will continuously detect the environment and render any changes into the sceneView . When you run a world-tracking AR session whose planeDetection option is enabled, the session automatically adds to its list of anchors an ARPlaneAnchor object for each flat surface ARKit detects with the rear-facing camera. Each plane anchor provides information about the estimated position and shape of the surface. We can enable planeDetection on viewWillAppear when adding a ARWorldTrackingConfiguration to the ARSession : configuration.planeDetection = .horizontal So while planeDetection is on we can add a new node into the scene by creating a new SCNode from our Scene object and changing the node's position, a SCNVector3 , to where we want the model to be on the view. We will then add this node as part of the childNode of the sceneView , and since planeDetection is enabled the AR framework will automatically pick up the new anchor and render it on the scene. let pointTranslation = transformHit.translation\nlet node = scene.rootNode.childNode(withName: assetName, recursively: true)\nnodeModel.position = SCNVector3(pointTranslation.x, pointTranslation.y, pointTranslation.z)\nsceneView.scene.rootNode.addChildNode(nodeModel) Here the transformHit from before has been used with the .existingPlaneUsingExtent and .estimatedHorizontalPlane cases instead of .featurePoints , since planeDetection is enabled To get the correct node position we will need to use the transformHit float4x4 matrix we created before and translate it to an float3 . To do that translation we used an extension that translates our float4x4 matrix into a float3 . extension float4x4 {\n  var translation: float3 {\n  let translation = self.columns.3\n  return float3(translation.x, translation.y, translation.z)\n  }\n} Tada 🎉 we just successfully added a 3D model into an AR Scene! Anchoring Having the app continuously detect the plane is quite resource heavy. Apple recommends disabling planeDetection after you are done detecting the scene. But as we mentioned before, if planeDetection is not enabled the ARScene won't pick up your newly added childNode and render it on to the sceneView . So if you want to be able to add new nodes and models to a scene after you are done detecting planes you will need to add a new ARAnchor manually. To create an ARAnchor from the tap location we will use the same transformHit float4x4 matrix we created before — without needing to translate it this time, since ARAnchors and ARHitResults use the same coordinate space. guard let location = touches.first?.location(in: sceneView) else { return }\n\nlet hitResultsFeaturePoints: [ARHitTestResult] = sceneView.hitTest(location, types: .featurePoint)\n\nif let hit = hitResultsFeaturePoints.first {\n  let finalTransform = hit.worldTransform\n}\n\nlet anchor = ARAnchor(transform: finalTransform)\nsceneView.session.add(anchor: anchor) By adding the new anchor by ourselves instead of relying on the Session Configuration we trigger the renderer() function from the delegate that will return the node to be rendered for a particular anchor. func renderer(_ renderer: SCNSceneRenderer, nodeFor anchor: ARAnchor) -> SCNNode? {\n  guard !anchor.isKind(of: ARPlaneAnchor.self) else {\n      return nil\n  }\n  let node = scene.rootNode.childNode(withName: assetName, recursively: true)\n  node.position = SCNVector3Zero\n  return node\n } We need to double check if the anchor triggering the render function is the anchor we just added and not an ARPlaneAnchor . With this in place our model will be rendered at the tap location of the sceneView just as seamlessly as when we had planeDetection enabled. Tada 🎉 we just successfully added a 3D model into an AR Scene! Conclusions To summarise, in this post we went through the basics of Augmented Reality and Apple’s ARKit. We applied the lessons learned and crafted an application that adds our 3D models to the world using two different methods. The code for this demo can be found on Novoda’s GitHub and you can also check our ARDemoApp repo , where you can import your own models into an AR Scene without having to write a line of code. If you enjoyed this post make sure to check the rest of the series! [3] Have any comments or questions? Hit us up on Twitter @bertadevant @KaraviasD You can create a simple AR box by creating a node and specifying its geometry to be a box, you can also then change the color of said node to see it better on the view since the default is white. ↩︎ If you are using a complex model/scene conmposed of different nodes you should can different SCNNode and add them to the sceneView together. This would allow you to be able to use each node as a seperate entity from the model or scene. For example to you could animate or change tetxures of just one node instead of the entire scene or model. ↩︎ This post was updated in January 21st 2019 to reflect new findings and updates made by Apple to their API. ↩︎", "date": "2019-01-21"},
{"website": "Novoda", "title": "Designing 3D models for AR apps (ARKit)", "author": ["Chris Basha"], "link": "https://blog.novoda.com/designing-for-ar-with-arkit/", "abstract": "At Novoda we’ve been playing around with Apple’s ARKit features, and we had tons of fun building demos with it, even though a few colleagues were wondering why we were walking around holding our phones up. Now we want to share our findings with you. Getting started with 3D modeling To begin creating your first 3D model you will need a 3D modeling app. There’s a plethora of options out there, some of the most popular ones being Blender, 3DS Max, and Cinema 4D (C4D). For the purpose of this post, we’ll be creating our 3D models in C4D, but you can use whichever software you prefer, as long as you’re able to export your models in Collada 1.4 format (.dae extension). The first important thing that we want to do when we open C4D is go to the Project Settings in the bottom right panel, and change the scale to meters, instead of the default centimeters. This is because ARKit’s unit of length is meters, and unless we use the same units, things will appear disproportionate in the real world compared to the size we create them in C4D. Now that we got that out of the way, we can start designing the objects that we’re going to add into our world. Prior to this project I had no knowledge of modeling 3D objects at all. I spent a few days watching tutorial videos on YouTube to get familiar with C4D itself, what goes into creating an object (such as texturing and lighting), as well as the terms that are often used in 3D modeling. Getting into the 'how' of creating 3D models would be a bit of an overkill, but most importantly, out-of-scope for this post. If you've already got the skills and the time to create 3D models, go ahead and use your own! For the sake of this tutorial we’ll use a couple of simple objects; this banana , which we’ll use in the AR app, and this low-poly tree to illustrate a few important steps in our workflow later. TurboSquid is a good place to find paid and free 3D objects. Google also recently launched Poly , a library of 3D objects which you can download for free. Preparing for export At this point in the process there’s a few things to keep in mind—if we were to export this object and plug into our AR app, we would see an all-white projection of it without any kind of texture, and that’s no good. Textures need to be ‘baked’ into the model in order to be visible in the AR app. We’ll do that in a second. Let’s set the anchor point of the object on its bottom; i.e. where it is supposed to touch the surface. If we don’t do this, the object might appear like it’s hovering over the surface as we move the device around. That’s because the object isn’t properly anchored to the surface. The low-poly tree will help illustrate this point better. After positioning the anchor point correctly, let’s make sure to set the object’s coordinates to 0, 0, 0 on the X, Y, Z axis accordingly—namely, the origin point. Another thing to keep in mind is to avoid using any sort of lighting in C4D. The best time to start thinking about lighting your scene is during development time in Xcode. Adding a light source to the scene in Xcode will allow us to have more control over its properties through ARKit’s APIs, as well as the ability to add lighting to a scene that hosts multiple objects. Just like in real life, objects do not carry a light source with them wherever they go, but are lit by external light sources. Doing this will make your project more granular, and thus easier to maintain. Using a light source will also allow us to cast shadows onto a surface, making objects look like they belong in the real world, rather than looking like stickers on your device’s screen. This will be taken care of in the next post where we continue designing for AR. We are almost done preparing our model for the AR app. Now it’s time to ‘bake’ the texture so the AR app can pick it up. Hold on, what does it mean to ‘bake a texture’? Baking essentially generates an image file that contains the texture of every polygon of an object, called a UVW Map. UVW maps are three dimensional (hence the three U, V, and W coordinate identifiers), which allows them to wrap onto complex geometries. In the case of our banana, this is an unnecessary step as the author already provides us with a texture, but this is something that in most cases you will have to do for your own 3D objects. Back to the low-poly tree again! To bake a texture, we will select all of our layers and group them into a single unified object, then select Bake Object in the Objects menu. Check Single Texture and Replace Objects , PNG format, and your desired export location. A size of 1024x1024px seemed to maintain good details on the model when viewed through the AR app. Now the model consists only of the texture we just baked, making all other materials redundant. Don’t worry if it looks blurry; that’s C4D downscaling the texture for faster inline rendering. Before we export, it’s a good idea to clean up the file from any unused layers and materials. To remove any unused materials, right-click on one, then select Remove Unused Materials . It’s now time to export our model! Generally, ARKit supports many files types, the most popular being .dae, and .obj along with it’s .mtl file. Generally, .dae is preferred due to its ability to support animations and the fact that they don't need supportive files (like the .mtl file). If you go the File menu and then Export, you’ll notice C4D supports both these options, but you might also notice that there are two .dae type exports—COLLADA 1.4 and 1.5. The latter wasn’t working for us, so we exported all our models in COLLADA 1.4. Apple recently introduced the .usdz format, which is a format based on Pixar's USD format and packages the model, textures, and animations in one single file. Apple has provided a tool in Xcode to convert the above mentioned file formats into .usdz, but for the scope of this post we will focus on using our usual formats. When exporting to .dae enable the following options in the export window: The .dae file has a reference to the UVW Map created earlier, or in this case the texture given with the 3D model. If you change the name of the texture file, you should also update the .dae file, too. You can easily do so by opening the file with any text editor. The texture is usually referenced in the first few lines of the code, but if you can’t find it, look for something ending in .png (or whichever file format you previously exported the UVW map to). Animating Animating 3D objects has proven to be easier than we thought—the .dae format is able to store animations done in C4D, and there’s no extra steps involved to make them play. Easy as that! Animating in C4D is similar to After Effects, so if you’re familiar with this software you won’t find it very difficult to adapt. Admittedly, C4D feels very old and clunky, and this process can turn out to be very slow and frustrating at times. To export the animation, don’t forget to check the final “Export animation” box in the Collada 1.4 export window. ARKit will pick up the animation automatically and will play it in a loop, just like a gif. We will talk about implementing this animation in the AR in the next post. Conclusion That’s about all you need to know to get started designing for AR! We went through using a 3D modeling app to create our object, texturing, and exporting in the right formats. In the next design post, we will talk about lighting the model appropriately, adding shadows, and supporting animations. In the next post of the series , Berta Devant walks you through how to create a basic demo AR app which will allow you to use 3D models in an augmented reality environment. Got any comments or questions? Hit me up on Twitter @BashaChris", "date": "2019-01-14"},
{"website": "Novoda", "title": "Making AR more precise with CoreML", "author": ["Berta Devant (IOS Developer)"], "link": "https://blog.novoda.com/arkit-coreml/", "abstract": "ARKit allows you to create and play with augmented realities, creating a new way for users to interact with digital content and the world around them. But what if you could have your model not only augment reality but also interact with and react to the changes around itself? Placing a cube wireframe around the plant was our end result This post is from the multi-part series on ARKit, where we talk about designing for AR, building a demo app, exploring and testing many of the features of ARKit. We previously wrote on Adding the finishing touches to 3D models in Xcode . In the previous post in this series we played around with AR and added a model to a scene. While doing so we wondered if there was a way to add a model to a specific object of the world. So we turned to Machine Learning to see if by recognizing the scene we could place a 3D model seamlessly into an environment. Furthermore we wanted to create a recipe for adding ML capabilities to an app that doesn’t require specialized knowledge of image recognition, Neural Networks, or Machine Learning in general. We experimented on CoreML with a TensorFlow model, as well as the YOLO framework . Our goal was to find the most clearly identified object within the scene and automatically add a 3D model to it. Read on to find out about our experiences with the different approaches we tried, and each one’s comparative advantages. For an easy to read introduction to Machine Learning check out this post by Alex Styl on understanding machine learning Exploring our options We decided to work with pre trained Machine Learning models and the 3D models created in the first part of this series Designing for ARKit . The focus of our investigation for this demo was not training a machine learning model but rather seeing how easily you can implement machine learning into an AR app and how straightforward CoreML is to use on iOS. Core ML delivers blazingly fast performance with easy integration of machine learning models enabling you to build apps with intelligent new features using just a few lines of code. [1] With Core ML, you can integrate trained machine learning models into your app. Apple Documentation Another important distinction before we move on is the difference between image recognition and object detection. Image recognition is a machine learning term for when a model is trained to identify what an image contains: Image recognition is the process of identifying and detecting an object or a feature in a digital image or video. MathWorks Image detection app from the CoreML udacity course Object detection on the other hand is the process of a trained model detecting where certain objects are located in the image. Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class in digital images and videos. Wikipedia This is what the TinyYolo CoreML by Matthijs Hollemans model output looks like All the pre trained models Apple gives us for CoreML are built for image identification instead of object detection, so we knew that we had to convert an object detection model to CoreML. [2] For this exercise we also used Apple's Vision Framework since it processes images and would prepare input from ARSCNView for the model. Apply high-performance image analysis and computer vision techniques to identify faces, detect features, and classify scenes in images and video. We decided to use CoreML because its implementation is straightforward once the model has been imported into the app. CoreML reads as a regular Swift class which means it can be implemented by mobile developers without having to worry about machine learning concepts. That makes it a really powerful framework, the caveat is that your trained model might end up being a large file that a user will have to download with the app (using disk space on the phone) but since the model lives on the phone it also allows the machine learning functionalities to be used on the device instead of through a cloud. Doing so allows for object detection to happen even without an internet connection. During WWDC 2018 Apple announced that their Vision Framework could now recognize and detect objects on live capture camera, making it perfect for this tutorial without the need to use external machine learning models or conversions. Because of that we updated the code that goes with this blog to use Vision to test how it worked, but also left the existing code and documentation for how to use a converted tensorflow model called Tiny YOLO. You can check out the new Vision Model working on our github . For both machine learning models, the 3D model used and how is placed on the ARScene is the same. Using a Tiny YOLO model Apple provides different ways to convert a model to CoreML.  Options include a Python package called CoreML Tools , maintained by Apple itself,  which converts any python model to CoreML; Apache MXNet , maintained by Apache, which can train MXNet models and then convert them to CoreML, or the recently added TensorFlow converter , maintained by Google, which allows us to convert some of the best known TensorFlow models into usable CoreML. We looked into converting a tensorFlow model into CoreML using the converter but found that the graph for the object detection model was not optimized to be understood by the converter. While considering different object detection models we found this article by Matthijs Hollemans in which he uses a YOLO framework, converts it to CoreML and implements it to an ios app. [3] YOLO is a neural network made up of 9 convolutional layers and 6 max-pooling layers, but its last layer does not output a probability distribution like a classifier would. Instead the last layer produces a 13×13×125 tensor. This output tensor describes a grid of 13×13 cells. Each cell predicts 5 bounding boxes (where each bounding box is described by 25 numbers). We then use non-maximum suppression to find the best bounding boxes. The YOLO model converted to CoreML will get an input of an image and output the 13x13 cells of every objects it detects. We then need to convert this grid into CGPoint coordinates for our scene. In his post Matthijs goes over how to convert this 25 numbers to actual Floats values needed for the prediction. Read the post and take a look at his code if you want to understand YOLO and CoreML a little better. We used his code, with his permission, to predict where our object is located.There are few differences between our implementation and his, notably that we used an ARScene as the input image, and that in our case the output is not an array of bounding boxes, but a single box marking the most prominent object in the scene. Our first step was importing the TinyYOLO ML model into the app and setting up the TinyYolo class , this class converts the grid into a Prediction structure. Once we had this set up we started editing the View Controller code from the ARKit app . On the ARKit app we were using user tap events to see where to place the 3d object and now we want to use the point given by the machine learning model to place the object there. The first thing we need to do is pass the current AR scene view to the model so it can analyze it and find the mostprominent object on the screen. let pixelBuffer = sceneView.session.currentFrame?.capturedImage However if we pass that pixelBuffer straight into the YOLO model it will return an error, not a prediction. This happens because the model expects a specific size image as input, meaning we need to scale the image to fit the model input (which in this model is 416x416). To get the input processed we could manually scale the image to a 416x416 or we could wrap the model objects into a Vision request. Vision, as I mentioned before, processes images for Machine Learning models and will scale our input image,pass it through the CoreML model and get the prediction results. guard let visionModel = try? VNCoreMLModel(for: yolo.model.model) else {\n      print(\"Error: could not create Vision model\")\n      return\n}\nlet request = VNCoreMLRequest(model: visionModel, completionHandler: visionRequestDidComplete) We set up a vision model as a wrapper of our CoreML model and then create a Vision Request at the start of the app. Once we want to perform the request to the model we create a VNImageRequestHandler with the pixelBuffer we want to use and perfom the request. let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer)\ntry? handler.perform([request]) Before we can perform the request there are still a couple of things we need to set up. One is to perform the request on a background thread so it does not actually block the UI of the app and the second is to get the Dispatch Semaphore to wait. We initialized the semaphore with a value of 2 to avoid possible repeated calls to the model before we get the results. let semaphore = DispatchSemaphore(value: 2)\nsemaphore.wait() You will need to create the dispatch semaphore at the beginning of the app and then set it to wait before performing the first request and before going into the background thread. Once the model has finished processing the image we will get the result prediction on our visionRequestDidComplete ; we can then get the results as a multi array value and pass that to the Yolo class to get the outline CGRect boxes of the objects detected. if let observations = request.results as? [VNCoreMLFeatureValueObservation],\n   let features = observations.first?.featureValue.multiArrayValue {\n\n   let boundingBoxes = yolo.computeBoundingBoxes(features: features)\n   showOnMainThread(boundingBoxes)\n} computeBoundingBoxes is a method created by Matthijs Hollance that accompanies the TinyYolo model. He explains the transformation on his blog Using Object Detection in Vision We now need to change how we get the location of the model but can maintain the rest of the AR code. Instead of getting a CGPoint from the touch location we get a CGRect from the model of where the object is and use those points against ARKit's Hit Points to find where we should place the model on a 3D AR scene. To do any of the AR code we will need to return to main thread and interact with the UI, but before we do that we should free the semaphore allowing for other requests to be completed if necessary: self.semaphore.signal() guard let scaledRect = yolo.scaleImageForCameraOutput(predictionRect: prediction.rect, viewRect: self.view.bounds) else {\n     print(\"could not scale the Point vectors\")\n     return\n}\n\nguard let model = arViewModel.createSceneNodeForAsset(nodeName, assetPath: \"art.scnassets/\\(fileName).\\(fileExtension)\") else {\n     print(\"we have no model\")\n     return\n}\nlet scaledPoint = CGPoint(x: scaledRect.origin.x, y: scaledRect.origin.y)\nif let hitPoint = arViewModel.getHitResults(location: scaledPoint, sceneView: sceneView, resultType: [.existingPlaneUsingExtent, .estimatedHorizontalPlane]) {\n     let pointTranslation = hitPoint.worldTransform.translation\n     model.position = SCNVector3(pointTranslation.x, pointTranslation.y, pointTranslation.z)\n     sceneView.scene.rootNode.addChildNode(model)\n} Once we are back to the main thread and ready to place the model on the scene we will first need to scale back the CGRect prediction into the proportions of the sceneView . This scaling code was done by Matthijs Hollance on his github code assuming the scene where the image comes from is full screen of a phone. Once we have the scaled CGRect position of the prediction we create the model, get a hit point of where the model is supposed to go [FOOTNOTE] and use that hit point to change the model position and add it to the sceneView.rootNode And just like that you can place a box wireframe inside a potted plant. That in itself doesn’t seem very useful or impressive, but we can use the same project set up to calculate the space between two objects and place a model there only if there is enough space, or using the code to automatically place a new color on a wall or floor while avoiding the objects that get in the way, etc. There a lot of possibilities you can get from combining both frameworks, not only making your AR apps smarter but also helping make your Machine Learning apps more visual. Conclusions To summarise, in this post we went through the basics of CoreML and expanded onto the usage of ARKit by having it place models around existing objects. The code for this demo can be found on Novoda’s GitHub and you can also check our ARDemoApp repo , where you can import your own models into an AR Scene without having to write a line of code. Have any comments or questions? Hit us up on Twitter @bertadevant @KaraviasD Core ML is optimized for on-device performance, which minimizes memory footprint and power consumption. Running strictly on the device ensures the privacy of user data and guarantees that your app remains functional and responsive when a network connection is unavailable. ↩︎ This post was updated on January 2019 to reflect that Apple has announced Object Detection using Vision. The code on the demos has also been updated. Novoda’s GitHub ↩︎ Matthijs is an incredible person and developer and you should take a look at his full project , blog and twitter . ↩︎", "date": "2018-02-19"},
{"website": "Novoda", "title": "Adding the finishing touches to 3D models in Xcode", "author": ["Chris Basha"], "link": "https://blog.novoda.com/adding-the-finishing-touches-to-3d-models-in-xcode/", "abstract": "In this post, we will continue our intro into designing for AR by adding our models into the demo AR app and apply lighting, shadows, and animations! This post is from the multi-part series on ARKit, where we talk about designing for AR, building a demo app, exploring and testing many of the features of ARKit. We previously wrote on getting started with ARKit . A rather important thing to mention is that there’s a small amount of coding involved in designing for AR. Despite the fact that this might sound scary to some, I believe it’s our responsibility as designers to account for the entire lifespan of our creations through the journey of building a product, even though they might be far from our immediate reach. I’m strongly in support of going the extra mile and making an effort to learn new working methods, tools, or even a programming language, in order to ensure perfect delivery of our designs. Lighting the model in Xcode If you've followed our previous post, by now you should have a running AR demo app. Let’s start by copying the .dae file as well as the UVW map into the art.scnassets folder. If the files are exported correctly, we should see our model in Xcode, along with its texture, in a 3D environment similar to the one of C4D. This is the time to add a light source to our object. In Xcode’s Object Library panel on the bottom right, you can see a few types of light sources that you can add to your scene. A Directional light shining from 80–90 degrees overhead seems to be the most realistic one. In addition to the main Directional light, you may choose to add a secondary Directional light to illuminate certain parts of the object that aren’t directly illuminated by the main Directional light, in order to avoid completely black areas. Adjust the intensity of the light source until you get something that looks relatively realistic. Adding shadows A crucial part of making virtual objects appear as if they’re part of the real world is to include shadows. Drop shadows—i.e. shadows that are cast onto the surface where the object is resting on—is what makes it look tangible and creates the illusion that it is in fact anchored to a surface. To add a shadow, while still having the main Directional light selected, go into the Attribute Inspector panel and make sure Casts shadows is enabled, and the mode is set to Deferred . So now that the light source can cast shadows, we need a surface on which to cast these shadows. From the Object Library, add a Plane object to your scene, which essentially is a surface without any thickness. Let’s adjust the size and placement of the Plane object to be right under your model (or put 0, 0, 0 on the object’s xyz coordinates), and now we should see its shadow cast on the Plane. Now there’s a very obvious problem; the Plane object has a texture, but the end goal is to have our model’s shadow cast on whatever surface is resting on, not a surface embedded in the model itself. Fixing that is as easy as going to the Material Inspector while having the plane selected, and simply unchecking all color channels from Write To Color property under the Transparency settings. In addition to that, choose Constant on the Lighting Model dropdown menu. Now we should see a shadow under the 3D object while the surface is invisible, which is what we were going for. The surface might also appear darkened, but this effect won’t be visible in AR. Xcode might ask you to convert the .dae file to a .scn file. That’s okay for now, but it might prove to be a problem if your models also animate. However, there is a way to fix that. We’ll be exploring that in a bit. The default settings will cast a hard shadow on the surface, or in other words a shadow with clear lines. A more diffused shadow will look more realistic in most lighting conditions. To adjust diffusion, go back to the Attributes Inspector while having the Directional Light selected, and adjust the shadow sample radius and sample count until you get something that looks good. You can also decrease the transparency of the shadow colour for an optimal look. Lighting based on environment The final step to achieving basic realism with ARKit is lighting your model in accordance with the environment. Imagine if you placed an object in a relatively dark room—the object would look quite dark too, right? The opposite goes for objects placed in a bright environment. This is something we have to take into account when designing for AR. The ARKit API calculates and provides a Light Estimate value, which is the estimated luminance of a frame in lumens. self.sceneView.autoenablesDefaultLighting = false;\n\nlet estimate: ARLightEstimate!\nestimate = self.sceneView.session.currentFrame?.lightEstimate Now that we have an estimated luminance value of our real world lights, it’s easy to adjust the intensity of our virtual light source. This way we ensure that there’s consistent lighting between the model and the environment around it. let light: SCNLight!\nlight = self.lightNodeModel?.light\nlight.intensity = estimate.ambientIntensity The model is now ready to be placed in the real world! Supporting animations Everything that has been mentioned above applies to .dae files that include animations in them. And that's great… until we try to add a plane and Xcode asks us to convert the file to .scn, and as soon as we accept, the animation doesn’t play anymore. For some reason, the .scn format isn’t maintaining the animations of the .dae file. So we have to find another way to add our light sources and shadows, while still maintaining the animations of the model. A fairly small amount of coding is required. Adding a light source and plane to your .dae file won’t require us to convert the file to .scn, but altering these objects’ properties will. Who’s to say we can’t do that programmatically, though? let plane = self.planeNodeModel?.geometry!\n\nplane?.firstMaterial?.writesToDepthBuffer = true\nplane?.firstMaterial?.colorBufferWriteMask = []\nplane?.firstMaterial?.lightingModel = .constant This piece of code is the equivalent of unchecking all colour channels from the drawing the texture of the plane and setting the lighting model to constant, thus drawing only the shadows of any objects cast on it. For that to work, the plane must have a material applied to it. C4D doesn’t apply a material to a new geometry by default, so make sure you apply a basic material, (it doesn’t matter how it looks, as it’s not going to be visible). Your model is now lit properly, anchored to the surface realistically, while also doing all sorts of crazy dance moves. In closing Now we've learned everything you need to know in order to design for a basic AR app. Previously we learned to create 3D models and export them properly, and now we're able to add them in an AR app and place them in the world adequately. Any comments or questions? Hit me up on Twitter @BashaChris", "date": "2019-01-28"},
{"website": "Novoda", "title": "Moving forward with Kotlin - Part 1: Properties", "author": ["Pablisco"], "link": "https://blog.novoda.com/kotlin-properties/", "abstract": "Originally published here: https://medium.com/@pablisco/moving-forward-part-1-properties-d8f695b3c812 Most of us, who are lucky enough to be able to work with Kotlin often come from other programming languages. The vast majority also come from the beautiful and verbose world of Java. However, things are different on the other side, and it’s not uncommon to fall for the convenience of bringing our old baggage with us because we are used to it and feels comfortable. We don’t have to. We can also think of it as moving to a new home. Packing all our old stuff and, in the process, getting rid of clutter we didn’t even realise we had at the back of that dusty shelf. Let’s have a look, in this series of articles, at some of the things that could help you find extra space in your new home in the Kotlin neighbourhood. Properties With Java, in-memory data “storage” comes in four shapes: variables, parameters, fields, and constants, in order of scope. With Kotlin, parameters are similar with a few variations. They are final (so they cannot be assigned a new value), can have a default value (so we can omit them when calling it) and their names matter (as we can use them when invoking). Now, for the other three, things are more different. As in, they are all defined as properties. Remember: we are in a different paradigm now. When we use the val and var keywords, we are defining properties. Not variables. Not fields. Not Constants. Just properties. So, what is a property? We can define Properties as the characteristics of an object. They are always obtainable, we can read them with a getter, and sometimes they also accept changes, we can change them when they have a setter defined. In the Java world, we have the concept of Beans, which contains properties. The convention here is to provide a getXxx (or isXxx ) for each of them and a setXxx if we want them to be writable. Enforcing greater encapsulation of information inside objects. In Kotlin, as in many other languages, we have the concept of properties embedded in the language itself. Kotlin properties To understand how Koltin defines properties, we have to think of them as one or two functions and not as simple data references. That said, under the hood, the compiler makes optimisations that may inline such functions when they are not required. When we assign a val with some data: val userName = \"Alice\" We are creating a property with a backing field behind it. To all effects, we can consider it something like this written in Java: private String userName = \"Alice\"\npublic String getUserName() {\n  return userName;\n} At first, it feels strange and dirty to have a property like this on an object and to access it directly. We’ve programmed ourselves to think of it as a bad practice. Repeat with me: “We must always keep fields private and have an accessor method”. Well, yeah, in Java it is a bad practice. However, why do we do this? The reason for this hard rule is because we want to keep that value encapsulated if we later want to change the way in which we obtain the final value. Well, with Kotlin properties, we can still do that. We know that properties are just functions to read or write data. So at any point, we can override how we retrieve the value of the property. There are several ways to do this. We can add a getter to our property: val prefix = \"user\"\nval userName = \"Alice\"\n  get() = \"$prefix-$field\" Here we have the actual value accessible as a field inside of the getter (as well as in the setter, for var) but allows us to change what we do before returning it to the caller. In Java, this would look like this: private String userName = \"Alice\"\npublic String getUserName() {\n  return prefix + \"-\" + userName;\n} If we require it, we can also drop the backing field altogether and define it as a standalone getter: val userName\n  get() = userSource.fetchUser() In both cases it still looks like the same property to any component using it, so we can keep our code encapsulated at the same time as being able to keep our code terse. Also, in comparison to Java, we can keep accessor functions close to the property definition, as opposed to having the field at the top of the class and the method after the constructor. Setters Similarly to getters, we can define how we want to set a value. var userName = \"user-Alice\"\n  set(value) { field = \"user-$value\" } This way we can ensure that the value set to our property always follows the given rules in the setter. In Java we would do this like: private String userName = \"user-Alice\"\npublic String getUserName() {\n  return userName;\n}\npublic void setUserName(String userName) {\n  this.userName = \"user-\" + userName;\n} If we have a property that we want to be readable but we also want to change it internally we can modify the setter: var userName = \"Alice\"\n  private set(value) { field = \"user-$value\" } Moreover, if we don’t want to change the way we assign the property, but we want to restrict access to the setter, we can also add a visibility modifier to it but leave it without a body. var userName = \"Alice\"\n  private set Meaning that externally we can see the userName as a read-only property but if we have the same scope of that setter, we can set it, which saves having to have private property and a separate public one just referencing the latter. Changing the visibility modifier to something like internal or protected can help with API design where we want to be able to change properties of objects we are sending out to the world, but not to allow others to change them. Delegation There is yet another way to declare properties: using delegation. As we know, properties are just functions so, in Koltin, we can delegate these functions to an object: val userName by UserSource To do this, UserSource can extend either ReadOnlyProperty or ReadWriteProperty . The first one requires a getValue function which acts as the getter. On the other hand, ReadWriteProperty also requires a setValue to act as a setter. class UserSource(...) : ReadOnlyProperty<Any, User> {\n  override operator fun getValue(\n    thisRef: Any, \n    property: KProperty<*>\n  ): User = TODO(\"Not implemented.\")\n} Extending from one of these interfaces is optional as these operator functions can be defined without inheritance too: class UserSource(...) {\n  operator fun getValue(\n    thisRef: Any, \n    property: KProperty<*>\n  ): User = TODO(\"Not implemented.\")\n} Property delegation is rather powerful as it gives us extra details about the property when it is accessed or assigned. For example, we can get the name of the property, which can be useful when declaring extension properties. Property delegation in action As an example of delegation, on Android we can define a type that represents properties inside a Bundle : object StringBundleProperty {\n    operator fun getValue(\n        thisref: Bundle, \n        property: KProperty<*>\n    ): String? = thisref.getString(property.name)\n    operator fun setValue(\n        thisref: Bundle, \n        property: KProperty<*>, \n        value: String?\n    ) = thisref.putString(property.name, value)\n} Then we can declare an extension property over a Bundle : val Bundle.sessionId by StringBundleProperty The name of the property ( \"sessionId\" in this case) is used any time the property accessed or set on a Bundle predictably. However, we need to make sure that we don’t expose them to too much of our code. One way to limit access to them is to make the file or class private. Another one, among many, would be to create a custom scope: object UserDataScope {\n    val Bundle.sessionId by StringBundleProperty\n} Where we can jump into when needed: with(UserDataScope) {\n    val sessionId = bundle.sessionId\n} Helping us keep the Bundle's properties clear from global extensions. It may not be evident at first but, even data defined inside a block of code (aka variables in Java) is also a property in Kotlin. I’ve personally tried not to call them variables in the past to make it pertinent that they follow the same patterns as any of the other scopes: member, companion, and global/package. Member properties They are part of a type and can be defined either as part of a constructor or anywhere in the type, which complicates things when thinking about where to place them in the class when they are private. The consensus in Java is to put private fields at the top of the class and private methods underneath the constructors and public methods (and, often after they get used). Constructors are a completely different story so we’ll ignore them for this case. However, it’s still not clear where to put private properties. Since, in reality, they are functions (and sometimes don’t have a backing field) then where should we put them? We are not going to get into details as it depends too much in each situation. Companion properties One of the things that we don’t have in Kotlin is static scopes. However, we can create singleton objects which are close enough. A particular case of those is companion objects. class UserSource(...) {\n  companion object {\n    val defaultUser = User()\n  }\n} These can be accessed directly from the type as if it had a static scope as this companion object singleton is shared between all the instances of the class. Although it may feel like we should use upper snake case (using underscores), there is no particular reason why these should follow such convention as we are still talking about properties. Global/package properties Finally, if we declare a property in a file, it becomes “free” from other components as a first-class citizen, which means that it lives in the packages where the file is defined and not inside of a type. When compiled, we can find it inside of the object singleton that Kotlin creates for each file, but the language hides that fact from us. If we declare a property const val then it becomes an actual constant that the compiler can inline. So-called constants in Java are often just static fields. In both cases, only when they are a String or a primitive type they benefit of the compiler inlining. That’s why in Kotlin, constants can only be either a string or a primitive value. In Kotlin, since these are more strict constants and purely immutable, I think that it seems entirely appropriate to use upper snake case to name them. However, all depends on what your team decides in the end. Conclusion We shouldn’t think of properties in Kotlin in the same way as variables, fields or constants. Properties are an incredibly versatile feature from the language. To all purposes, we can see them as functions rather than object references. Hopefully, this has helped you to get a better understanding of how some of the features from Kotlin properties work. #Spam If you like this content and want to know when I post the next part or have a chat, don't forget to follow me on here (Medium) and on Twitter: https://twitter.com/pablisc0 Also, if you want to extend your Kotlin knowledge in the functional side of things come and have a look at our courses at: https://twitter.com/functionalhub", "date": "2019-01-27"},
{"website": "Novoda", "title": "Is Flutter ready for prime-time?", "author": ["Richard Tolley"], "link": "https://blog.novoda.com/flutter/", "abstract": "In late 2018 Google's Flutter cross-platform development tool reached version 1.0. In this article we ask whether Flutter is suitable for production apps. About a year ago, Novoda published an article comparing the various mobile cross-platform solutions then on offer. Now Flutter is no longer a beta product, we have decided to take a deeper look at it. This article covers the following areas: what Flutter is, the Flutter ecosystem, the Dart language, existing apps developed in Flutter, and how much Flutter is affected by the issues that have troubled cross-platform development in the past. What is Flutter? Flutter is a cross platform technology created by Google, which allows developers to create iOS and Android apps from the same codebase. It has a strong focus on rapid UI development. Unlike other cross-platform solutions which use established languages such as JavaScript (React Native, Cordova) or C# (Xamarin), Flutter is built on top of the new and relatively unknown Dart language. Dart is a modern language which allows 'Just In Time' compilation (JIT). This enables Flutter’s killer feature, ‘stateful hot reloading’.  During development, Flutter apps can be rebuilt with minor code changes while they are still running. This radically increases the speed of development, and allows designers and developers to pair together to develop UIs in real time. Flutter provides its own UI libraries for iOS and Android, allowing developers to produce apps which are visually indistinguishable from native ones. This contrasts with older cross-platform solutions such as Xamarin and Cordova, which offer a somewhat sub-standard user experience. How large is the Flutter ecosystem? The Dart package manager lists around 2000 Flutter-compatible packages written in the current version of Dart. There are packages implementing SDKs for many common APIs, although naturally the offering is not as complete as for native apps yet. This means that many third party services can be integrated with Flutter already without writing additional code There is no easily accessible roadmap for the future of Dart on the project Github page, so it is difficult to say whether the next version of the language will also contain changes affecting existing packages. There is a roadmap for Flutter development for 2019, and it’s good to see that the team is prioritising key gaps in functionality, such as the lack of decent support for maps. There is an active and rapidly growing Flutter community on Stack Overflow: Dart is also the second most used language at Google internally ( source ). The increase in Flutter question views seems to have happened at the expense of React Native and Xamarin, according to this graph ( source ): This suggests that the developer community is in general more excited by Flutter than by  other cross-platform solutions, which means it likely to be an easier sell to a development team than React Native or Cordova. While announcing Flutter 1.0 , the Flutter team mentioned the Flutter repository is also one of the most active on Github, and that around 250,000 developers have used the platform. The Flutter showcase site lists a variety of apps that have been implemented using Flutter. Most are fairly small in scale, and many seem to be developer side projects, although there are a couple of high-profile ones (notably the Chinese retailer Alibaba, and a video streaming app produced by Tencent). Learning Dart I found the experience of learning Dart straightforward. The syntax of the language is similar to other C based languages, and will offer few surprises to anyone familiar with Java or Swift. There are a few idiosyncrasies and pitfalls, as with any new technology. Dart provides libraries for unit and UI testing, which means it is easy to produce high-quality, well-tested code in Flutter projects, meaning your Flutter project doesn’t just have to be a throw-away prototype but could form the basis of a production-level app. While there are not many developers on the market with Dart skills, I don't feel that this is a serious impediment to staffing a Flutter team: the learning curve for the language is so shallow that most developers from a native mobile or web background should be able to pick it up in a few days. Case studies Part of the problem with assessing Flutter at the moment is that not many large apps have been implemented using it. Despite this, several developers have written about their experience building apps in Flutter. This section provides a survey of their findings. Hamilton The Hamilton app is a companion for the popular musical about the American Revolution. The agency that developed the app, GoPosse, are very pleased with Flutter, because it allowed them to develop more features for the app than would otherwise have been possible in the time available. They plan to recommend it to future clients. They note that in the rare cases where Flutter doesn't support a native feature, it is easy enough to drop in a native screen (the app has a few camera-based features, but in general doesn't integrate with many device-specific capabilities). Google AdWords Flutter is also used by the Google AdWords team for both their web and mobile offerings, where it has delivered considerable time savings. However this particularly scenario (Dart being used on multiple platforms) is not a common one outside Google. Easy Diet Diary Gary Hunter ported his Easy Diet Diary app to Flutter and wrote an article describing the experience. He found it easier to develop reusable UI components in Flutter than in iOS, and believes Flutter development to be faster than iOS/Android overall. He also had some positive things to say about the responsiveness and culture of the Flutter community. CARTUNE Tetsuhiro Ueda implemented a large scale app using Flutter. He chose Flutter over React Native and Xamarin, and in his article notes that he found UI development in Flutter to be rapid, but that the platform still has rough edges. Ueda also cites the fact that Flutter is open source as a key advantage - it is always possible to debug the Flutter framework itself, if necessary (and of course, if you have time). Xianyu (Alibaba) The Chinese company Alibaba recently used Flutter to build some key screens in a mobile app for their Xianyu secondhand trading platform, which has over 50 million users. In a Google post on pitching Flutter to clients Wm Leler states that Flutter reduced the time Alibaba required to develop new features from a month to two weeks. The Alibaba team have also produced a video on adopting Flutter , But it appears the path was not completely smooth: in a post on Flutter optimization, they remark: \"In computing, as in much of life, any given method can see a lot of use before its latent flaws reach a decisive impasse. For Alibaba, discovering one such flaw in software development kit Flutter meant the difference between success and failure in the group’s recent work\" The key issue was with capturing high memory images of native screens (such as video frames or screenshots). While Alibaba were able to solve this problem, the fact that they ran into it in the first place highlights the risks a company takes if it chooses to use a relatively young cross-platform framework such as Flutter. The article discusses how to synchronize Flutter bug fixes from the public master to a forked version of Flutter - also possibly a red flag, since one of the reasons AirBnb cited for dropping React Native is that they had to maintain their own fork of it. The consensus seems to be that Flutter makes sense if your app doesn't rely too heavily on native capabilities. Developing Flutter apps that will need plugins is possible, but likely won’t be much quicker than writing two native solutions. Does Flutter offer a solution to previous issues with cross-platform development? Issues with previous cross-platform solutions. Native app developers tend to have a highly sceptical view of cross-platform development. This is often partly because they have been burned by it before, and partly because they are sceptical of the underlying technology. I last worked with a cross-platform technology (Cordova) in 2013, and found that it struggled to deliver on its promises: we spent a great deal of time writing platform specific plugins, effectively negating a lot of the time savings the technology purported to offer. This is a fairly representative experience: in the annual Stack Overflow developer survey for 2018 Cordova and Xamarin came first and second in the the 'Most Dreaded Frameworks, Libraries, and Tools' category. More recently, Facebook's React Native has been touted as a cross-platform silver bullet. It gained some traction a year or so ago, with various high-profile companies (AirBnb, Udacity, Walmart) choosing to implement at least part of their apps with it. Unfortunately, its reputation was somewhat tarnished when several of these companies subsequently abandoned the platform, and in the case of AirBnb and Udacity , published blog posts explaining why. Despite this, both companies had a lot of positive things to say about React Native. Their reasons for abandoning it were mostly around the limitations of JavaScript, and shortcomings in the developer experience. Does Flutter solve these problems? Many of the problems afflicting previous cross-platform solutions were at root due to the fact that they used JavaScript. In their explanation of why they chose to drop React Native, the AirBnb team mentioned that JavaScript's weak type system and the issues around the React Native to native bridge were major problems). Flutter implements all functionality natively and therefore isn't vulnerable to performance issues caused by communication between native and JavaScript code. Also, the Dart language is statically typed, and doesn't suffer from a lot of the issues that afflict JavaScript. IDE support is generally good, and the standard of the tooling (at least in Android Studio) is high. On the other hand, the fact that Dart is a new language is also a disadvantage. React Native devs have access to the vast library of code provided by the npm package manager - the Dart equivalent is much smaller. Nonetheless, Flutter does seem to be a step forward from React Native technically speaking. But it needs to gain more traction before it can be considered a better solution than React Native in all respects. Most modern cross-platform solutions such as React Native and Flutter can clearly deliver on the promise of rapid UI development; even those companies that have adopted and then discarded React Native acknowledge this. The problem is that this is only part of the story - this is merely the requirement to 'get a foot in the door'. Even relatively 'minor' issues can have a disproportionate effect on development time, especially when the whole team is learning a new technology. Conclusions Flutter is currently suitable for prototyping and less complex production projects, but not for large-scale apps, unless you are willing to take on a considerable amount of risk and have the development resources and institutional support to work around the issues you will probably run into. For most projects, this would likely outweigh the key benefit of using Flutter in the first place, which is to save money and time. Nevertheless, the first production apps in Flutter are beginning to emerge, and the platform certainly seems to be gaining popularity rapidly, at least in the developer community. Looking at things from a slightly longer term perspective, the quality of the underlying technology makes it likely that effort put into developing plugin/packages for Flutter/Dart would be worthwhile. I'm cautiously optimistic that it could be used for larger scale projects in the near future, as soon as there is reliable package support for more common APIs in Dart. When exactly this might be is an open question, but I'd suggest taking another look in a year to six months.", "date": "2019-01-24"},
{"website": "Novoda", "title": "QA Testing Myths", "author": ["Dimitris Karavias", "Jonathan Taylor (Head of QA)", "Sven Kroell"], "link": "https://blog.novoda.com/qa-testing-myths/", "abstract": "7 popular misconceptions we just had to address Quality Assurance is a field that is currently going through a soul searching phase. People with the exact same title might have completely different ideas about their responsibilities, and those on the outside have even more disparate opinions about what those tester-people do. While we are interested in what QA is, we thought it would be fun to list what we think it is not. QA breaks things QA don’t break the software. They break illusions about the software. Unless someone is writing the product’s features or managing infrastructure they can’t, by definition, break things. QA vs the world A more generalised version of the first myth. Within teams QA can be seen as the people who point out developers mistakes, frustrate management by stopping releases, and cause a bottleneck in the development flow. These are signs of malfunctioning teams which require systemic efforts from everyone involved. “Just do automation on the side” Automation is a full time development job, treating it as a part time one is what makes so many automation efforts fail. Developers are expected to only work on development and continuously hone their craft, why would test developers be any different? “We have manual QAs you will pair with and in a month they will be writing tests” First, the whole “automation vs manual testing” debate has already been addressed a number of times so we won’t go deeper into that. A good professional will use the right tool for the job, upskilling as needed, instead of finding the job that accommodates for their tool. The big issue here is unrealistic expectations for people to be able to independently code tests without going through the learning process that other developers did. The unknown unknowns of the managers and those expected to skill up mean that no realistic estimate can be done since learning largely depends on the person’s interest in the topic and lots of practice. We’re not really involved in setting the requirements, we just test There’s few contributions for a QA to make that are more valuable than analysing the requirements. Defects found in testing are estimated to be about 10 times more expensive to fix than those found in the requirements phase. Often, this is expressed as \"We’d love to be more involved in requirements setting but we have so many bugs to test and retest\". Well, you will keep having them until you get involved in the earlier stages. QA’s main job is:Automated UI Testing, Exploratory Testing, Finding bugs, any other testing activity Our job is to provide information to the business about the state of the system and highlight potential risks we see coming. How that is done depends on the situation. In reality the actual verification and/or automation usually takes much less than 50% of our time. QA can somehow be a secondary concern QA can be the team experts in how to make things verifiable from the start, how to find the breaking cases, teaching other developers to do these things better so the whole team can become good at writing tests, and automate the heck out of it. Furthermore QA is often the bridge between the business and the tech team since they know both the product and the technical issues it’s facing. Why just let a dev sit there and bash out whatever scripts they can in isolation instead of gaining so much more? Thanks to Jonathan Taylor, Sven Kröll and Jon Reeve for their inputs!", "date": "2019-02-19"},
{"website": "Novoda", "title": "The ultimate guide to AR Design, ARKit, CoreML and more!", "author": ["Chris Basha", "Berta Devant (IOS Developer)", "Dimitris Karavias"], "link": "https://blog.novoda.com/the-ultimate-guide-to-ar-design-arkit-coreml-and-more/", "abstract": "Over the course of a few months, various people at Novoda have experimented with technologies such as Augmented Reality, Machine Learning, Image Recognition and more. We've written a lot about these topics and wanted to gather all our blog posts in one place for your ease! You can always find us on Twitter if you have any comments and questions! Berta Devant , Chris Basha , Dimitris Karavias Designing 3D models for AR apps (ARKit) The guide begins with an introduction to designing 3D models and making sure they're exported in a proper way, ready to go into the AR app. Getting Started with ARKit Next, Berta Devant walks you through how to create a basic demo AR app which will allow you to use 3D models in an augmented reality environment. Adding the finishing touches to 3D models in Xcode Back to design, Chris Basha illustrates a few methods that will make your 3D models look as realistic as possible with a few lines of code. Making AR more precise with CoreML Berta talks about how we can identify objects in an AR scene through Machine Learning capabilites, without requiring specialised knowledge of complex frameworks. What IS real? Augmented Reality Testing with ARKit Dimitris Karavias explains the challenges and solutions he came across while performing UI and Unit testing to the AR app. Feature photo by Patrick Schneider on Unsplash", "date": "2019-02-26"},
{"website": "Novoda", "title": "Unleashing the potential of Design Sprints for your business", "author": ["Margarida Aleixo"], "link": "https://blog.novoda.com/design-sprints/", "abstract": "Have you ever wondered what design sprints are? How can they benefit your business or if they’re the right thing for you? Here you can find an overview of what to expect going through a design sprint and what you get at the end. What's a Design Sprint? One of the techniques we use at Novoda is Design Sprints. Set over the course of five days, they're a great way to explore product ideas in a collaborative team environment. In just one week, you'll work together in a cross-functional team (stakeholders, product owners, designers, and engineers) to create a solution and gain unique feedback from real people through user testing. A classic Design Sprint structure consists of five phases: Understand, Sketch, Decide, Prototype, and Validate. In each phase, we tackle the problem in the same way we build a product, in a multi-disciplinary team. Understanding In the Understand Phase, the team comes together to research and really understand all angles of the sprint goal. This ensures a shared understanding of domain knowledge, business goals and customer insights across the team. Sketching In the Sketching Phase, team members brainstorm solutions individually before sharing ideas with the team. This approach encourages creativity and participation from every team member, ensuring a range of ideas from each discipline. Deciding In the Deciding Phase, the team lays all the ideas on the table and make decisions about the most relevant to develop into a prototype. These ideas will then be defined and outlined for the next stage. Prototyping During the Prototyping Phase, the team creates a prototype of the chosen solution, mapping out an agreed user journey before creating a prototype that can later be tested with users. Validating In the Validation Phase, the team observes users interacting with their solution for the first time. Feedback on the ideas can be collected along with any usability issues to inform ideas for future improvements. Concluding In the Concluding Phase, the Sprint Lead will create a Design Sprint report to celebrate and showcase the deliverables ready to share with team and stakeholders. Why do we love them? As you might know, we love team collaboration, so it doesn't come as a surprise when we say that we love Design Sprints. But why do we love it so much? Collaboration There is no better place to see beautiful collaboration and creativity come into play than during a design sprint when the team puts their thoughts and energy towards solving a problem. Collaboration is also the key to any successful design sprint. Working towards a collective goal The great thing about design sprints is that everyone in that room is working towards the same goal, everyone wants to learn/make the same thing, everyone is there together and that creates a great and exciting environment where everyone is in sync with what they want to achieve. Quick learning The amazing quick learnings you get after going through each of the above phases. For example, in the ‘Understand’ phase you’d get to know why we got together and what you want to build. When you need to understand if something works, if it’s the right decision for your product or for your users, this is the way to fully understand and act accordingly. In a week you will test and learn if an idea is worth pursuing, without waiting for months or until the product is in the user’s hands. How can they benefit you? Design Sprints will allow your team to tackle fresh new challenges with clear goals to ensure focus and impact for an effective working future. The structured format allows multi-disciplinary teams to work together to explore solutions from a variety of perspectives. Overall, they provide the opportunity for you to explore a wide range of ideas and then test chosen ideas with users. A strong sprint brief will help you to stay focused and finish the week with a proposed solution. The sprint itself will take a working week, and should be full of new information, new ideas, lots of open discussion and active collaboration from all participants. By the end of the sprint, you have a clickable prototype of the proposed solution, which has been tested with users. Observations and feedback are concluded and improvements recommended for future development. You can then use the knowledge and results of this design sprint to inform a more detailed project management proposal. What do you get at the end? Following the end of the Design Sprint, it’s time to focus on the Design Sprint report. This is then presented to external stakeholders. So at this point, you’ll have a unified understanding of Design Sprints, a clickable prototype, design assets, a Design Sprint report with presentation and handover. You'll also walk out with a backlog full of stories that can be prioritised to inform future ideas and excitement about the future and you'll have an idea what you're team can do together and the great potential of team collaboration. As a Product Designer, I’ve tried and tested many different techniques within many projects and I highly recommend Design Sprints for all the reasons above, but mainly because they’re highly customizable depending on teams, projects and different problems.", "date": "2019-02-28"},
{"website": "Novoda", "title": "Mirror, Smart Mirror on the wall…", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/mirror-smart-mirror-on-the-wall/", "abstract": "Smart mirrors come from the idea of bringing technology into every-day, not-so-smart objects: the traditional mirrors. Turned off, it just reflects the image of the person in front of it, but when turned on, more useful information can be seen as an overlay on the reflected image. Let’s have a look how to build one yourself. How to build one Two-way mirror For this project a traditional mirror is not enough, what we are looking for is a two-way mirror : these are special panels which allow the light to pass from the rear outwards, and also reflect the light off the front, depending on the light condition on the other side. If the other side is bright, we will see through, like a normal glass. On the other hand, if the other side is dark, we will just see our reflected image. An interesting aspect about this property is the fact that it is very localised, so for example if we put a screen on the other side of the two-way mirror, we will see our reflected image in the dark parts of the screen, while the bright ones will \"pass through\" and we will see them overlaid to our mirror. This is the key idea behind a smart mirror. It is possible to find two-way mirrors from different stores online, like for example this or this . Display In order to present information on the mirror we need a display. This will be placed behind the mirror and the bright areas of the image will be visible from the viewer point of view, while the dark ones will just reflect as a traditional mirror. Ideally the display should cover the whole area of the mirror, so that it will be possible to cover it all with overlaid information. This can became clearly very expensive, so another solution is to limit the \"smartness\" of our mirror to one area only, using the smaller screen of a tablet or a smartphone, for example. Frame Depending on the size of the mirror and the place where it will be placed, we might want to add a frame around it, in order to cover the screen on the back, but most importantly to shield the rear from light. As it's easy to imagine, once the device is installed it won't be possible to interact with it using the touch screen. It will be good to consider then an easy way to remove it from the mirror, in case we will need to access some of the device settings later. In our projects we attached the devices to a panel in the back of the frame which could be easily removed. Remember that the device will be always-on, so consider a hole in order to allow a stable power connection. Completely cover the back As mentioned before, in order for the two-way mirror to reflect properly, it is important for the rear side to be as dark as possible. Shielding the rear section of the mirror from the light is especially important in order to hide the device and all the cables, as well as to guarantee the best reflection to the front. Example applications Droidcon Berlin In Droidcon Berlin we presented a variation of our smart mirror configured like in a typical domestic environment, displaying useful information like the weather forecast or the agenda for the day. On top of this we added a minimal Twitter client which displayed real time tweets containing a given hashtag related to the conference. The app we created was running on an Android tablet placed in the the lower left corner, in order to not cover the reflected image with the displayed information. Makefest For Liverpool MakeFest we considered the expected audience, mainly composed by families with young children. With that in mind we created a more interactive configuration: we used face recognition to detect when someone was looking at the mirror and displayed different animations and smiling images depending on the happiness of the person in front of it. For this mirror we used a different Android tablet placed at the center of the frame, in order to display the looking eyes and the other animations in the middle of the mirror. Novoda London Office Another variation we are working on is for the Novoda London office. This one will be the biggest one so far, sporting a 170x80 cm mirror. For such a big mirror a tablet would have been clearly too small, so we decided to use the 22” LCD panel from one of our office monitors, connected to an Android HDMI dongle. The wooden frame used was a specifically made one, deeper than a standard one in order to mask entirely the cables behind. At the moment the mirror is just displaying time and weather, but we are going to integrate it with more services, especially calendar-related ones. This would allow us to display information about people in the office, like upcoming birthdays and planned meetings. Future concepts Mirrors are to be primarily used in our homes. We look at them in several occasions, while grooming in the morning or while brushing our teeth in the evening. They all act as a very personal element of our daily routine. A smart mirror should keep this characteristic, allowing a per-user-tailored experience. This could be done using face recognition or proximity detection to identify the person in front of it and then displaying the most relevant information for that user. These information should be then aware of the user’s schedule, proposing different sets of information depending if the user is leaving for work, if she has a day off, or if she just came back home in the evening. The interaction with the smart mirror should be as minimal as possible, allowing user to get the right information at the right time, without even asking for it. When thinking about the user interface for such a device, we need to remember that touch interactions will be most likely not available. Voice recognition is one of the more obvious alternative, but gesture detection could be another option, if some kind of user interaction is needed. Conclusions As you can see, replacing one of the traditional mirrors you already have with a smart one is not difficult, and it can be the perfect opportunity to give a new life to some of the old smartphone and tablets that were collecting dust in a drawer. We are curious to see how your smart mirror will look like! A huge thanks to Alexandros for the collaboration in the making of the mirrors! The app we created for both MakeFest and Droidcon Berlin is completely open-source, and it's available at https://github.com/novoda/spikes/tree/master/MagicMirror For more Novoda open-source project, have a look at http://novoda.github.io/", "date": "2016-07-12"},
{"website": "Novoda", "title": "Developing like a pro — portrait only apps", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/portrait-only-apps/", "abstract": "When writing an Android application there comes a time when the client / business utters those dreaded words. \"Let's support only portrait for phone and landscape for tablets\" they might add ... \" just for now \". After all the debates and your passion to prove this is the wrong decision sometimes it just has to be done. This blog post will discuss the hidden pitfalls of this decision including unexpected crashes and explain how you can work around these to make a solid, well functioning app that you can change to supporting both orientations in an instant. I admit I am not the UI guy, I fail at making things shiny and at tweaking the UI views to get it just right. I much prefer the 'back end' of the application the code that makes it run with shiny caches and performant loading. Therefore this post is not going to be hovering around resource buckets, or layouts. So when I say supporting both orientations in an instant . I mean from this perspective and yes you would likely have to do further work to improve your alternative orientation view design - not todays problem. What is a configuration change device configurations can change during runtime (such as screen orientation, keyboard availability, and language). When such a change occurs, Android restarts the running Activity (onDestroy() is called, followed by onCreate()). [1] Device configuration changes are bad news for your activity it is killed and a fresh one created in its place. However this new activity knows nothing about your data in the last activity unless you implement onSaveInstanceState and restore this state when the new activity is created. When you lock an app to landscape or portrait not noticing these device configuration changes will have a big impact on your app users and therefore your app store ratings. Put another way ; when an app is locked to portrait onSaveInstanceState will not be implemented as the developer has never seen a config change and so they have not seen the bugs they have to fix. Your activity rotating - even if this is by accident as you lean your hand to the side is the most obvious tool you have to highlight glitches or app crashes in your code from config changes. An activity rotating is a change of device configuration. There are other instances of a configuration change that you aren't too likely to bump into whilst developing but WILL happen when in the wild i.e. when you release your app. For instance somebody plugging in a keyboard or secondary screen, changing the global font scale, putting the device in a car dock or changing the device language. These are rare events as you develop but as your user base grows they become more prevalent. If you have activity rotation disabled you aren't going to be noticing configuration change bugs unless you actively test for them. So what can we do to deal with this, the client wants a portrait only app, we want to not glitch/crash on configuration change. Lets talk about some tools and techniques to bridge the divide here. orientation only locked by 180° Allow your users to be free! Just because the business only wants to support tablet landscape orientation doesn't mean you can't support orientation changes. Introducing sensorLandscape . sensorLandscape - Landscape orientation, but can be either normal or reverse landscape based on the device sensor. Added in API level 9. [1:1] There are both sensorPortrait and sensorLandscape . This means you will get configuration changes you just have to spin your device around a full 180 degrees. This benefits the user as well if they have some crazy display or pass there tablet to a friend upside down. Unfortunately on phones the 180° portrait view is disabled by default (enabled by default on tablets) so most phones won't support this, but some will so no harm having it on! This can be seen here , here and here in the AOSP source code. As an example of changing your AndroidManifest.xml: <activity\n      android:name=\"com.novoda.MyActivity\"\n      android:screenOrientation=\"sensorLandscape\" /> Side note: there is a funny old bug (typo) that has now been fixed where you had to use sensorPortait (missing the R) depending on the Android version you targeted. [1:2] orientation not locked whilst developing Don't lock the application orientation unneccesarily. What about only locking it as part of the release process? Development builds are only seen by the team, developers, testers. Therefore the business rule of orientation portrait only applies when the customer sees the app. Keeping it unlocked whilst you develop allows you to test orientation change and find those pesky rotation bugs. Another advantage here is the business / clients can see the application in both orientations, back at the start of this post when we talked about the debate for not locking. Sometimes seeing is believing, watch as a stakeholder naturally holds the phone in landscape to interact with a certain view and then whisper to them that users won't be able to do that ;-) the power of example. Two ways you can enable this: Using different flavors of the application for release & development Checking the BuildConfig.DEBUG flag Using flavors you can replace parts of your AndroidManifest.xml file. You can [1:3] . You could also keep a single manifest and just change the screenOrientation value using an integer in the corresponding flavor resources. Below is a brief example of the former. /src/devFlavor/AndroidManifest.xml <activity\n      android:name=\"com.novoda.MyActivity\"\n      android:screenOrientation=\"unspecified\" /> Then override it on release /src/releaseFlavor/AndroidManifest.xml <activity\n      android:name=\"com.novoda.MyActivity\"\n      android:screenOrientation=\"portrait\" /> You can also do it programmatically in your Activity using the debug flag. [1:4] . @Override\n    protected void onCreate(Bundle savedInstanceState) {\n      if(BuildConfig.DEBUG) {\n\t\tactivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_UNSPECIFIED);\n      } else {\n\t\tactivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);\n      }\n      super.onCreate(savedInstanceState);\n    } developing with 'don't keep activities' I was trying to find the official documentation for this developer option but I don't think any exists. Other docs state you can use this option to simulate low memory pressure on your Activity (which is true). Here I show how it can also simulate a config change (from our perspective). To toggle on this option go to. Settings > Developer Options > Don't keep activities What this will do is when navigating through your app and back to an Activity you have previously seen. It will force this activity to be recreated. Therefore you can think of this like a configuration change when this option is turned on on your device. Once an activity is no longer in the foreground and visible to you, it will have its onSaveInstanceState called. When you navigate again to this activity (most likely with the back button, or calling finish() on the current activity). The original will be recreated and be passed the Bundle that was saved. Put another way, if your activity has onStop called on it, with this option toggled on it will also have onSavedInstanceState and onDestroy called. When navigating back onCreate will be called with the restore bundle. You can use Don't keep activities to develop in an environment where you have to consider saved state more often. Especially for the QA out there this is gold mine for finding bugs that the developer has no clue how to reproduce until you explain you have this option turned on . Use it to encounter scenarios you would not usually see whilst developing as discussed, but remember the users in the wild WILL see these cases! This is why it is imperative they are coded for, giving your app a seamless experience. Conclusion Configuration changes are a great tool to find bugs in your application. Having your application locked to one orientation does not mean you don't have to cater for these bugs, it just means they are more hidden and harder to reproduce. Therefore there are two take away points 1: Locking to portrait never means it will be faster to develop an application from a back end perspective. 2: If locking an application consider other ways you can reproduce configuration issues just as easily. Doing this will give you a sturdy, hardened application that will fare well in the wild. You will get less strange and erroneous crash reports and you have some sure ways to attempt to reproduce these bugs if you see them. Further, your application is ready to be enabled for both orientations. Just you know drop in some designs, add a resource bucket, ship it! http://developer.android.com/reference/android/app/Activity.html#setRequestedOrientation(int) ↩︎ ↩︎ ↩︎ ↩︎ ↩︎", "date": "2016-03-08"},
{"website": "Novoda", "title": "#myhomescreen, Pt. II", "author": ["Ryan Bateman (Head of Product)"], "link": "https://blog.novoda.com/myhomescreen-pt-ii/", "abstract": "myhomescreen is a series of blog posts that shows off some of Novoda’s homescreens, highlighting apps we use and love, games we like playing, and sometimes our own small projects. Natasha Borsos (HR Manager): My home screen is like my life—a lot going on at once. I have a very varied job role—and am very active outside work, so I like to keep as many handy things on my home screen as possible. I have the weather displayed so when I wake up in the morning I can quickly plan my outfit based on what will be going on outside (Hat? Umbrella? Coat?). I keep the apps I use most in folders. One for Travel (Google maps and City Mapper—which is probably my most used app), another for social media—Instagram / Twitter / Linkedin / Hipchat / Skype / Snapchat / Swarm. It sounds like a lot, but I manage several Twitter accounts and use most of these for work/events/networking regularly. I also have a fitness folder—currently using Moves to keep track of how many miles I walk in a day—I prefer walking to tubes/buses when I can help it. Besides the folders I keep spotify handy (I listen to music everywhere I go) and my messaging apps. I am guilty of generally having a cheesey motivational quote in the background—a little reminder every day. ^_^ Sebastiano Poggi (Pixel Pusher): Like Ryan’s, my homescreen is my temple. No frills, few icons, and I use Muzei as well. I wrote a simple plugin of my own, too, that shows pics from Romain Guy’s repertoire . My poor Nexus 5 is undergoing some delicate restoration after an accident that left it shattered, so I’m on an even less-frills setup on my Moto G. The only widget I have is FWeather , which I wrote as well. The folders I have don’t contain anything peculiar, with the exception of Whatsapp+ , a customisable mod of Whatsapp that I’ve been using for quite some time. My Whatsapp looks almost fully Material. Another app I use is Toshl , a really nice finance tracker. I found it to be the least cumbersome and most visually pleasant around a couple of years ago, and I stuck with it. The free version gets you enough to do your basic money math. The app and the website do need a fresh coat of paint after all these years, but it’s on its way. Ferran Garriga (Senior Android Developer): Google Now is amazing so I have it on the top of the screen, Google Music on the bottom as I listen to music the whole day long, though I combine it with Spotify for mood playlists; plus SleepTimer . I use Ryanair , Easyjet , Vueling and British Airways apps for digital tickets (paper no more!), and Citymapper is amazing (it tells you which platform your train's arriving at and whether there are any delays). For tracking news, I love Pocket , using it alongside the desktop Chrome plugin for saving websites for later reading. Along with Google Play Newsstand , Mashable , SmartNews (experimenting with it), TED and BBC News . I use Sunrise as a calendar app, but I use the Cal (from Any.do) widget. Xperia Z3 for its amazing battery life and apt-x bluetooth codec and, as opposed to my previous Nexus 5, a reasonable screen size. I use the official Google Launcher instead of the Xperia one due limited use of folders in its latest version. I cannot wait for the 'L' upgrade early next year!", "date": "2014-11-06"},
{"website": "Novoda", "title": "Developer Mood, Android M and Android Auto", "author": ["Friedger Müffke (Co-Founder Berlin)"], "link": "https://blog.novoda.com/developer-mood-android-m-and-android-auto/", "abstract": "Recently, I made my mother proud as I was cited in the internationally known German newspaper “Frankfurter Allgemeine Zeitung”. I’d like to add some comments to it. Photo: Gregor Fischer The article “Streben nach mehr Freiheit für Android Apps” (issue 07 July 2015, page T4) was about the mood in the (European) Android community, Google’s strategies around Android and some vaguely described features of Android M and Android Auto. Unlike the article’s subtitle may suggest: “Developers feel patronised by Google”, I want to clarify that I do not feel patronised by Google. The author claims that many developers do not like the fact that Google provides new features only in Google Play services, that developers have to use Google Play services involuntarily and that Google is layering pressure on developers who do not use Google’s services. Did I miss anything or are there really many developers that feel like that? An unease that I can think of is the effort that developers have to bring up to cater for Google’s offerings and the needs of consumers who can’t use their Android Wear or Android Auto devices with their Amazon Fire? This is something for the EU commission to investigate . I’d like to go into more detail regarding the technical parts mentioned in the article. The automatic backup system introduced in Android M was used to highlight Google’s greed for user data. There does, however, not appear to be much difference between the existing backup service and the new one in Android M . Both, the old and new service use the configured Backup Transport method to transfer app data from and to the device. Hence, nothing has changed when it comes to support by alternative backup apps (with service action TRANSPORT_HOST ). The way how the backup of app data is configured has changed to become an easier, zero-code feature - probably because many developers are too busy to look into this part of their application. If, however, you take the time for your apps' backup procedures you see that you can even encrypt the app data before transferring it off the device. As a second example for Google’s bullish acquisition of user data, the author uses Android Auto . It is true that cars produce a lot of interesting data for car manufacturers, service providers, insurances and other players. However, Android Auto is technically nothing more than a communication protocol between Android devices and the head unit of the car infotainment system. #####Android Auto Protocol It is important to understand that having Android Auto in the car does not necessarily mean that any Android code runs in the car, see for example Genivi platform . The data that can be collected by using Android Auto is the same as the data collected on any Android device. If there is an update of Android Auto in the future which allows for the sending of more data from the car to Android devices then it is up to the car manufacturer to decide which data. Once opened up, the data will obviously be available to other mobile platforms and the web. If this happens at all - here I agree with the author - is still to be seen. Feel free to contact us with your comments and thoughts and also if you are interested in implementing new Backup Transport services or building apps for Android Auto with us at Novoda.", "date": "2015-07-09"},
{"website": "Novoda", "title": "Making Icepick: The Good, The Bad and The Ugly", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/the-making-of-icepick-the-good-the-bad-and-the-ugly/", "abstract": "Annotation processing. You have heard about it at some point. You have used either Dagger or AndroidAnnotations in your projects and now you know there’s some magic happening at compile time, but did you dare to dig down further? If you did you may have started digging, but soon found yourself lost in these huge frameworks. So you may have moved to something smaller, like Butterknife , to get a better understanding on the mechanics. Perhaps you still are puzzled by the magic. Well, if any of the above is true and you’re curious to know how to write your own annotation processing library, then you’re on the same boat I was on a couple of weeks ago when I started writing Icepick . Do you want to help the Android community fight boilerplate code? Here’s what you need to know. Writing an Annotation Processing library is a lot less complicated than you may think. First you need to declare your annotation: Above states that Annotations can only be used on Fields and after having been processed, can be safely discarded during compilation. You can specify a path to your own Processor in the META-INF.services/javax.annotation.processing.Processor file inside your resources path. This will tell the javac compiler that it must invoke this service when it is compiling classes. The easiest way to implement Processor is to subclass AbstractProcessor , a superclass which provides most of the functionalities needed. You can use either @SupportedAnnotationTypes or getSupportedAnnotationTypes() to declare the annotations supported by your processor. During the compilation phase, the processor is created and its process() method will be called. Don’t worry too much about the two parameters types, as you usually start the process method in this idiomatic way: This reads like “For each annotation supported by this processor, get all the elements where this annotation has been used”. Since the IcicleProcessor is only responding to a single annotation we can get rid of the for loop and get all the elements with roundEnv.getElementsAnnotatedWith(Icicle.class) . Now we are in the java.lang.model.element territory, a wilderness of mysterious classes that deal with elements of the Java language. Let’s quickly smuggle in what we need and leave this creepy place as soon as possible. If we want to generate code which saves and restores instance state we must collect the field name and type, along with the name of the containing class. Before getting these values we must validate the element modifiers but we cannot manipulate the field through our helper classes if it is either private , static or final . If we group the elements by their containing class, we can iterate through each class to generate an helper dual that eliminates the boilerplate code. Everything required for compile time is now provided. But if you think that our work it’s over, you’re wrong. We can use the generated classes just like any other class, but I think it’d be a mistake to do so. We don’t want to wait for the compilation phase to generate classes that we can’t see at the moment. We want to hide them behind another API that will make the call transparent to the end user. If we use reflection we can get a reference of the helper method and execute it. This way not only we avoid referencing the helper class directly, but we can hide the calls Bundles.saveInstanceState() and Bundles.restoreInstanceState() in a parent class so we only have to write it once. Pretty neat, huh? Well, if life were that easy we would have seen more and more annotations helping developers writing less code. But as in movies, no hero is a true hero until he has to face a badass enemy. Since its code is executed at compile time, a Processor is extremely hard to debug. I’m not saying it’s impossible, but it’s hard enough to discourage most developers. You want to get your code right so that you don’t need to debug it. And the only way to avoid debugging is testing. Funnily enough, Processors are also pretty hard to test. In a certain way, they share most of the problems that we have in testing Android applications. You do not create a Processor directly, just like you do not create Activities. This prevents you from passing it additional collaborators or dependencies. You must rely on a ProcessingEnvironment that holds a reference on pretty much anything you need, much like Activities rely on Context to reach to every other objects. Both ProcessingEnvironment and Context are God Objects extremely hard to mock and thus even more dangerous. These problems share a common solution. The class which is hard to test should do as few things as possible, while delegating most of the logic to other objects easier to test. Let’s look at the IcicleWriter for example. Is it the processor’s responsibility to create a helper class or can extract a more focused object which can deal with this problem? Since it’s mainly doing string manipulation this object will be extremely easy to test and at lest we can assume that the code generation part is correct. Another candidate ready to be extract in a separate class is the logic that determine which is the correct method to invoke on the Bundle to save or restore our fields. My first implementation of the IcicleConverter made heavy use of the Elements and Types utilities that you can get from the ProcessingEnvironment to reason over classes, generic collections and inheritance. Testing and mocking classes inside the java.lang.model.element will not make your life easier, so I was extremely unsatisfied by the overall result. Then I realized that if I can work on the class names instead of the class types I can code 99% of the behavior just doing String manipulation, a testing paradise. For the last 1% of the behavior (that is, a class that implements the Pacelable or Serializable interface) I could hide Elements and Types in a very simple IcicleAssigner and mock it when needed. Finally, even Bundles can be written test-first. It is just a matter of extracting an internal object that does not use static methods and keep it hidden from the external caller. This way, the caller only knows about Bundles , whose job is to enforce type check on the callers, while BundleInjector can be constructed and tested without any restriction. You can start your tests trying to call a simple class by name and slowly work your way until you can correctly call a method from the helper classes. There is a special character in this story. Sometimes he may look like a foe, although his intentions are good. That’s the new Android Build System . Since I’m a big fan of Gradle I was eager to switch to the new Android Studio IDE. And I must say, I was extremely impressed by how easy it was to deploy to Maven Central using Gradle (and it could be even easier with the Gradle Nexus Plugin ). However, as soon as I tried out Icepick in the wild, a problem became apparent. The library compiles against Android API 14 because it makes use of Fragments to enforce type checking and this was causing conflicts on older version of Android. The build tools documentation explicitly states that if you’re using the Android API on your library you should use the Gradle android-library plugin. Switching from java to the android-library plugin means that you have to deploy your artifact as an AAR and that you don’t have access to a unit test sourceSet (you can only run instrumentation tests against a device). Thankfully, creating a provided scope in Gradle is not that complicated. The Gradleware team is not particularly keen on supporting the provided scope, but this solution works well for the moment. I’m looking forward to see the Andorid tools getting better and better. Keep up the good work guys!", "date": "2013-08-12"},
{"website": "Novoda", "title": "Arte Mobile Relaunch with Novoda, nxtbgthng & GOOD", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/arte-mobile-relaunch-with-novoda-nxtbgthng/", "abstract": "Novoda are contributing to Berlin’s bubbling tech scene by currently building a new office in Mitte . We are excited to share one of our first Berlin based projects which is also a European partnership marrying the best of native development for both Android and iOS. ARTE viewers can look forward to a a beautiful, useful and of course Androidy mobile product developed by Novoda for the prestigious French-German TV channel. In a partnership with the talented nxtbgthng and Studio GOOD , this application will be part of a relaunch for iPhone, iPad and Android coming later this year. Together with our passionate, multi disciplined team we have all the ingredients to deliver a great mobile experience for a forward thinking video broadcast channel. Full press release: English and German .", "date": "2013-07-12"},
{"website": "Novoda", "title": "Wutson: Exploring Design And Development Workflows", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/londroid-wutson/", "abstract": "At this month's Londroid, Qi and I presented Wutson - an app we've been working on since the start of this year. We share our workflow describing how we worked together, which tools we find most valuable, and tips for improving the dev-design process. Wutson is your own personal TV guide, an app to help you find what's on. We decided to make it because none of the existing options we found felt quite right: some had thoughtful features and UX but looked dated and carried ads, while others looked brilliant but had confusing UX and awkward user journeys. Development started around the time that Android TV was launching - Wutson would be a perfect app for the living room: very visual, really simple and of course, it's the right place for a TV guide. This year, we've also been quite focused on exploring accessibility for Android in some of our newer projects; a lot of the considerations that would make an app usable for TV would also help make the app accessible, so this was an ideal case to experiment and learn. We started by looking at existing apps - two of the most popular were SeriesAddict and SeriesGuide . SeriesGuide is a great example of an app with a gorgeous UI; it's the one I currently use. Unfortunately some simple tasks are not intuitive (searching for a show) or not possible (looking for the episode description for a show you don't yet track). SeriesAddict is a popular app in France backed by the BetaSeries API. It doesn't follow the most up-to-date UI patterns, but has some well designed features like the watchlist, a view which presents the oldest five episodes that you haven't marked as watched for every show you're tracking. Qi and I made a list of all the features from these apps (and more), then identified the ones we felt were most important to include in an initial release of Wutson, mapping them together as user journeys to form a rough information architecture. After the initial project bootstrap tasks were complete, it's time to start working on features. We follow a process that's similar to the one we use at Novoda: Plan Scribbles/prototype Visual design and specs Implement visual refinements Review (then back to 1) While we tend to use Atlassian JIRA at work, for our personal project, Qi and I chose to use Trello . Our Trello board is made up of four columns: a to-do list Qi's current tasks my current tasks completed tasks Our planning meetings would begin with a review of the previous week's work, running through the completed tasks column and either archiving the ticket, or moving it back into the to-do list, if it wasn't done-done . Then we'd run through the remaining tasks, deleting the ones that weren't going to be worked on in the near-ish future, and selecting others to move to our respective columns with the intent to start these tasks next. By planning and reviewing together (instead of separately) we're able to: organise tasks so we don't block each other give feedback to contribute to the direction the product is taking receive praise and feedback after presenting the work we've spent time and effort on completing We presented our workflow for the search feature: Based on the user flow, Qi is able to produce quickly scribble out the basic structures page by page. The Search Overlay and Search Results screens are shown above. For this stage, there's no need to worry about the details - it doesn't matter what text is displayed, what colour the app bar is, which images to use, and it doesn't even matter if you can't even draw straight lines; they're meant to be pen & paper scribbles. While Qi is working on these, I'm keeping myself busy by setting up the data side. During the planning stage, we already discuss what the feature should include, so I know which APIs I'll need to hit. On Wutson, I used a combination of Retrofit and RxJava to work quickly. I should have also started to write tests at this point - it's one of the things we do daily at Novoda but I regret not doing for Wutson. It's never too late though! When Qi is ready with some scribbled screens, we'll meet again to discuss them: Now is the time we identify all the assets I'll need to implement the design, including overlays for focused and pressed states, icons in various sizes, image placeholders, and font files - that is, we make a list of all the things I need from Qi. In terms of accessibility, we identify which components can fall under a single content description. To ensure we have keyboard/trackball/switch access ( non-touch mode ), we take decisions on which elements are core to the screen. For example, we might hide the star icon here if the user is in non-touch mode, because it'll make navigation through the list take two clicks per item instead of one. We must be careful not to remove functionality though - there is an affordance to track a show on a different screen (show details). By doing these now, the app is already TV ready - after spending around 8 months with a Nexus Player, I'm certain when I say I'd rather have a completely functional app with similar UI to the phone version than none at all. Qi will cut up the scribbles into slices and I'll put them in the app. Implementing it like this for now means: Qi can work on visual design (mock ups) while I work on functionality I can consider adding acceptance tests (Espresso) while everything is bare bones I can ensure I've added focus and press states for all interactive elements, even if they're only temporary ones; I should be able to swap the assets later To test focus states, you need a device with a dpad or trackball, a USB-OTG adapter to plug a keyboard in, or an emulator ( you don't use 'focusable in touch mode' ). The first doesn't really exist with recent versions of Android and the second is feasible but annoying. I tend to use Genymotion virtual devices: I can use keyboard to enable non-touch mode, so I can check all focus/press states it has a resizable window screenshots/screen recording very fast I create a 360x640 px device with 160 pixel density (MDPI) to match Qi's output. Apart from Android's font rendering, this makes it easy to spot differences between the designs and the implementation. I'll create a device at my minSdkVersion, and one at targetSdkVersion which is especially useful when you're theming or styling anything to do with AppCompat. So while I'm working on implementing the scribbles, Qi's working on making the scribbles pretty: Qi explains that Sketch is currently her favourite tool for visual design, particularly loving the vast improvements on loading/running speed (over Photoshop/Illustrator). The Hover Guide allows you to see the distances from your selected element to other elements: Sketch also uses symbols. You create a symbol (e.g. an icon) and use it in multiple places, but when you change the symbol, it updates all the other instances. A similar feature exists for styles, applying the same set of attributes (like text colour, font, etc.) to multiple text boxes: Sketch itself is available for a 30 day free trial (Mac only) and there are plenty of design kits online to get you started. Getting designs over to the development team in a format that's useful to them used to be a hassle. We'd have to produce, for each screen, a PDF showing the margins and paddings, colours, text styles and dimensions. Here's an example that Dave produced while we were working on the Sun Mobile app: It's not great. For designers, it's a waste of time, and it's easy to miss things. As a developer, the spec sheet is very noisy - and flipping between different projects meant having to get used to the spec sheets from different designers. Zeplin is program which hopes to have the solution to redlines/spec sheets. It's available as a web app and for designers using Sketch, producing a Zeplin project is as simple as using \"Cmd+E\" to export their artboards. Zeplin provides an interactive portal into what we used to get as static spec sheets - we can now query for the information we need, when we need it: It does require care from designers though - Zeplin isn't clever enough to know what information we need - as developers, we know there's a difference between what the user can see on the screen and what views/view groups make up that screen. These look the same: But in Sketch, Qi has to add an invisible bounding box around the numbers so Zeplin can pick it up: Once we have the designs exported to Zeplin, it's time to update the app from scribbles to match the design. I bought this whiteboard from Amazon and it's glorious. You need to get the thin pens too, the fat ones are not useful. I draw the part I need to implement as a lo-fi sketch; even though Zeplin hides a lot of information by default, I still think it's a lot of noise, and by making a list, I find it easier to get from A to B. Ta-da! Next steps will include releasing a beta via Google+ communities and exploring a smart TV app. Zeplin have generously offered a discount code for Londroid attendees! Use ZEPLINLOVESLONDROID to get 25% off for 3 months until 26th of August. Keep following this blog to stay updated!", "date": "2015-07-28"},
{"website": "Novoda", "title": "Debugging Memory Leaks on Android (for Beginners)", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/debugging-memory-leaks-on-android-for-beginners-programmatic-heap-dumping-part-1/", "abstract": "This is the first post in a series about debugging Android memory issues—subsequent posts will deal with using Eclipse Memory Analyzer to inspect heap dumps, as well as looking at common Android-specific memory consumption pitfalls. We’ll start with Programmatic Heap Dumping . The contents of this post ought to be useful to developers new to the Android platform (regardless of Java experience) as it details Android-specific APIs and the use of Android SDK utilities to successfully interoperate with the standard JVM toolchain (e.g. hprof-conv which converts Dalvik heap dumps into the J2SE HPROF format, expected by Eclipse Memory Analyzer; MAT ).  Once this has been accomplished, the process of debugging leaks is more or less identical for Android applications and standard Java code running on a traditional JVM. What’s a Heap? All we really need to know about the heap for the purposes of this post is that it's a slab of VM-managed memory into which (most) Java objects are allocated—certainly all of the objects we're likely to care about.  When we're talking about the heap size of a particular object—how much of the heap it occupies—we talk of Shallow Size (or Shallow Heap ) and Retained Size (or Retained Heap ). The shallow size is the amount required for an object itself, while the retained size is the amount required for an object and all of the objects it refers to (i.e. via instance attributes). A heap dump is a portable (i.e. file-based) representation of a VM's heap at a particular point in time. Obtaining Heap Dumps On Demand Interactively getting an Android process to dump its heap is trivial with DDMS (there's a Dump HPROF File button). That's not our concern; we're focused on obtaining dumps at particular points in a program's execution by using the android.os.Debug API, which is an approach we might want to take if pathological memory consumption is peaking at points which aren't easily identified through interactive use. That said, our example is fairly contrived—an in-app Button which triggers a heap dump is isomorphic to the Dump HPROF File DDMS feature—figuring out when to trigger the dumping is entirely application-specific—the code samples are to illustrate how to do it, using a triggering mechanism likely to be familiar to anybody who's written an interactive Android application. Here's an example of a View.OnClickListener which accepts a String data directory, and writes a heap dump within it, when the onClick method is invoked: Below is an excerpt of an Activity which attaches the above listener to a Button , and passes in a sensible value for the path prefix, so that the dump ends up somewhere useful (note that this method will overwrite an existing file at the supplied path): When the Dump Heap button is activated, we'll be able to retrieve the file from the Android device/emulator by executing the following command on the host machine (assuming $ANDROID_HOME/platform-tools is in $PATH ): % adb pull /data/data/com.novoda.example.MemoryLeaker/MemoryLeaker.dalvik-hprof\n226 KB/s (9160365 bytes in 17.973s) If we plan to do anything with the heap dump, we'll need to convert it to the J2SE format, as outlined above, using the hprof-conv binary, in $ANDROID_HOME/tools (which ought to be in $PATH ): % hprof-conv MemoryLeaker.dalvik-hprof MemoryLeaker.hprof\n% ls -lh MemoryLeaker.hprof\n-rw-r--r--  1 moe  staff   9.2M 18 Apr 15:33 MemoryLeaker.hprof Grabbing a Heap Dump When Things Go Wrong Alternatively, if we're dealing with a worst-case scenario and have absolutely no idea what's causing the leakage, or lack the patience to babysit the application until it falls apart, we can install a handler which'll catch an OutOfMemoryError and try to write a snapshot of heap use at that point.  Alternatives would be using either Activity.onLowMemory or Application.onLowMemory , which have vaguely defined semantics, and are not as definitively catastrophic as an uncaught OOM (from Application's documentation: \"this is called when the overall system is running low on memory, and would like actively running process to try to tighten their belt. While the exact point at which this will be called is not defined…\" ). Back to Uncaught exception handlers: these are per-Thread, which is clear from the entrypoint— Thread's setUncaughtExceptionHandler instance method.  Below is an example of a Thread.UncaughtExceptionHandler suitable for passing in: And the code to set this up at Application startup: Next Steps The next post in this series will detail how to use MAT to find likely culprits in a heap dump which has been fetched and converted using the above method.", "date": "2013-04-19"},
{"website": "Novoda", "title": "Minimal Android ZeroMQ Client/Server Example", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/minimal-zeromq-client-server/", "abstract": "While researching possible technologies for implementing a publish/subscribe-based task distribution system for long-running, semi-autonomous/intelligent agents (with Android as a supported platform), ZeroMQ emerged as a sensible choice of message transport, given the exploding complexity of AMQP (Pieter Hintjens' August 2008 steering committee missive sums up my misgivings). ZeroMQ vs. JeroMQ The ZeroMQ/jzmq Android build instructions look pretty onerous, particularly for pre-3.0 versions (due to a libuuid dependency)— JeroMQ (a pure Java implemention)—seemed like a natural choice. I haven't benchmarked anything like a real-world use case, though there are some promising numbers published by the JeroMQ authors. Since the JeroMQ and jzmq (0.2.0 series) API structures are functionally identical, there shouldn't be any real cost associated with making a choice for exploratory reasons (barring showstopping bugs). The code snippets below adumbrate a minimal working example of a ZMQ client/server in request/response mode, running in a single process (an Android application). There are a dearth of Android-specific ZMQ examples around—I've endeavoured to use Android-specific concurrency abstractions where they seem idiomatic. Note that the purpose of this example/tutorial is to provide examples of how to issue a request and process a response using a trivial message format with ZMQ—while all of the work is happening in a single process, the server & client threads can be viewed as separate applications; trivial adaptations of the Hello World request/response example applications ( hwclient & hwserver ) which are ubiquitous in ZMQ tutorials. Code Let's start with a Handler which takes an instance (conforming to a trivial interface) to which it dispatches message events, and a key name identifying the String datum within the corresponding Message's Bundle which holds the message payload. Predictably, the interface isn't particularly jazzy. The long-running server, which blocks on reads and instantly responds with a reversed version of the message payload is defined as a Runnable , taking a Handler which is used to communicate the content of incoming messages to interested parties. The client/request-side is implemented as an AsyncTask which re-does all of its setup when run. AsyncTask.execute takes a String to use as the message body, and the constructor, as above, takes a Handler to which received messages are dispatched (after being bundled). UI The UI is incredibly simple, and the layout XML doesn't bear repeating. The three pieces of information required to make the Activity code intelligible are: text_message is the EditText at the top. button_send_message is the Button beside it. text_console is the large TextView console below. Wiring Everything Together Notes: The static methods in Util aren't particularly useful - the implementations have been omitted to reduce clutter. No Activity -lifecycle contingencies have been made in the above code. From the AsyncTask documentation : “When first introduced, AsyncTasks were executed serially on a single background thread. Starting with DONUT , this was changed to a pool of threads allowing multiple tasks to operate in parallel. Starting with HONEYCOMB , tasks are executed on a single thread to avoid common application errors caused by parallel execution.”  Both seem like strange choices to me. In any case, this example doesn't use THREAD_POOL_EXECUTOR , so client tasks won't run concurrently. Your application's manifest ( AndroidManifest.xml ) will have to request android.permissions.INTERNET if you're using the TCP transport, as in this example. Alternatives MQTT makes sense on a couple of axes, though it may be a slightly curious choice given the heterogeneity of clients in this use-case (it'd be easier to defend if clients were exclusively Android, or mobile-only).  It may well be the subject of a follow-up blog-post. ZeroMQ is used more in the wild, and it may be more flexible in case any strange requirements emerge (for example, if we were to develop an in-browser Javascript task consumer which required the proxying of ZMQ messages over HTTP, then the existence of middleware-factories like Mule might make this easier).  There's an example of what looks something like ZMQ over HTTP via Mule although I haven't looked at it (Mule, nor that example) in depth.  The word “middleware” makes me a little queasy.", "date": "2013-04-03"},
{"website": "Novoda", "title": "Maintaining String Resources with Translation in Mind", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/maintaining-string-resources-with-translation-in-mind/", "abstract": "Translation and localization is a vital part of any growing app. It is, however, time-consuming—often being carried out in disjointed phases and as such it’s all too often left until the last minute. If you’re not managing your String resources with this fact in mind, you could find yourself with delays in the development (and ultimately the release) of your application. At Novoda, we’ve slowly converged onto a common approach to managing our String resources that acknowledges this real-world issue and allows us to be flexible with when and how translations are supplied. We’ve found it very helpful for accommodating most translation efforts and have been using it on all projects, even if we don’t believe an application will be translated right now . The approach itself is fairly simple and consists of 3 rules. For all user-facing text use String resources (in both XML layouts or in code as appropriate). Never use String resources that contain String literals in your code/XML layouts directly Use resource aliases to provide one level of abstraction between user-facing text and IDs Let’s illustrate this with a concrete example. Here’s a simple Textview label displaying “Hello world” that’s being shown in an Activity named ‘Initial’: <TextView\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"wrap_content\"\n  android:text=\"@string/initial_welcome_message\" /> The @string/initial_welcome_message resource would not be the String literal “Hello world”. Instead, it would be a reference to another resource in a the strings file named after the screen/domain (in this case, res/strings-initial.xml , after our Activity). <string name=\"initial_welcome_message\">@string/hello_world</string> This @string/initial_welcome_message resource then references the @string/hello_world String resource, which is inside the file holding all translated strings. It’s this resource that holds actual text (in this case, “Hello World”) and has literal naming (“hello_world”): res/strings-localised.xml <string name=\"hello_world\">Hello world</string> This layer of indirection allows you to change the referenced resource ID that initial_welcome_message points to as you refactor your code. You therefore wouldn’t need to alter the hello_world String, which it may have been sent for translation. These IDs would remain stable and so prevent conflicts when the translation is completed and the resulting String need to be merged back in. Once translation has started, the process would be fairly simple: Don’t delete or rename IDs/values in strings-localised.xml ; instead add new ones as necessary Only clean up (unused) String resources when it’s safe (if it ever is safe, checking the process of the translation team) And that’s it. This approach allows for easy replacement as translated text becomes available without any major refactoring of XML layouts or code, meaning your development process is a lot safer. I don’t only like it because it makes string translation easier, I like it for the separation of concerns and the use of domain concepts to segregate the string resources into different files, allowing faster development and code maintenance when adding/changing features. Translations is a cheeky bonus, nice! Paul Blundell (For a little more detail, and some more concrete examples, here’s a longer description of this process .)", "date": "2015-05-12"},
{"website": "Novoda", "title": "ShowcaseView Customisation and Consecutive Showcases", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/showcaseview-customisation-and-having-consecutive-showcases/", "abstract": "When creating your apps, sometimes you may be targeting a set of users that won’t be so familiar with the Android UX, or sometimes your UI is that rad , so you need to tell someone how to navigate. Thanks to this cool guy Espian , things have become much easier. If you instantly just want the sauce, see the link below. Check the readme as it has a dependency on the ShowcaseView library. Here’s a ShowcaseView Example and a quick look at what I’m talking about: Now the library is great, but like many open source libraries there is a lack of documentation. So I have written a sample project that shows you how to fully customise these “showcase views”, using layouts and themeing. The library itself has recently had an update that was supposed to make it easier to add a showcase view to your app, it kinda worked but also made it harder to customise and to realise that you can customise it. The one thing I couldn’t find in the library was a way to add multiple Showcases to one Activity. I wanted it to show one view, and right when you click ok it will show you the next. So I have also put that into this example. Let’s have a brief run through of this and I think it will cover all the main points then I’ll leave you to go showcase everything. First off we have created an Activity with a Spinner, some text and a button. The Spinner doesn't have any items but just highlights navigation. The button navigates to a second activity that also has a ShowcaseView. MainActivity In the MainActivity , onCreate will tell it to show its own showcase. This is the ShowcaseManager, a nice way to encapsulate what changes about showcases in one place. ShowcaseManager So in the ShowcaseManager here for the SecondActivity it shows an easy way to Showcase a View, note that this is much better than the static methods offered by the library as you now have access to the XML layout file where you can change the colours of the text and background. If you just used the static methods you have no control over these. public void showcaseSecondActivity() {\nShowcaseView showcaseView = (ShowcaseView) activity.getLayoutInflater().inflate(R.layout.view_showcase, null); showcaseView.setShowcaseItem(ShowcaseView.ITEM_ACTION_HOME, android.R.id.home, activity);\n        showcaseView.setText(\"Up Button\", \"Press this to send you 'up' one level\");\n        ((ViewGroup) activity.getWindow().getDecorView()).addView(showcaseView);\n    } I also found that the .show() method would not work (at least for me) so I pulled out the getDecorView().addView to do myself (which is a bit of a hack). And in case you’re wondering view_showcase looks like this: <com.github.espiandev.showcaseview.ShowcaseView xmlns:android=\"http://schemas.android.com/apk/res/android\"\n      xmlns:showcaseview=\"http://schemas.android.com/apk/res/com.blundell.tut\"\n      android:orientation=\"vertical\"\n      android:layout_width=\"match_parent\"\n      android:layout_height=\"match_parent\"\n      showcaseview:sv_backgroundColor=\"@color/showcase_background\"\n      showcaseview:sv_buttonText=\"@string/showcase_button_ok\" /> So the above is how you fully customise a ShowcaseView. The below handles the scenario where you want to show a few showcases in the row, i.e. show them all the funcitonality in a wizard / step by step kind of fashion. public void showcaseMainActivity() {\n        ShowcaseViews views = new ShowcaseViews(activity, R.layout.view_showcase);\n        views.addView(new ShowcaseViews.ItemViewProperties(ID_SPINNER, R.string.showcase_main_spinner_title, R.string.showcase_main_spinner_message, ShowcaseView.ITEM_TITLE_OR_SPINNER));\n        views.addView(new ShowcaseViews.ViewProperties(R.id.activity_main_button_continue, R.string.showcase_main_continue_title, R.string.showcase_main_continue_message));\n        views.show();\n    } ShowcaseViews is a class I have written to wrap ShowcaseView allowing for easy adding of Showcases then showing each one as the user clicks the ok button. ShowcaseViews It comprises of a list of ShowcaseViews ’s and will show the next one after the first has been acknowledged. Again this is not immediately possible with the static helper methods or original library. I think this class wraps up that functionality nicely and I should do a pull request on the original library. That’s it, I hope the info helps if you ever want to Showcase your app UX to users. Thanks to: ShowcaseView Library NineOldAndroid Library Bacon Ipsum", "date": "2013-06-12"},
{"website": "Novoda", "title": "From Holo to Material", "author": ["David González (Technical Product Owner)"], "link": "https://blog.novoda.com/from-holo-to-material/", "abstract": "Material Design is the new design language from Google. This is a very exciting moment for Android—the new Lollipop release has put the platform in a mature position. Material Design is not just another update. There are no more excuses for bad user experience: developers and designers now have all the tools available in order to create beautiful experiences. For the last months we’ve been working on The Sun Mobile for Android and we couldn’t be happier about the results. If you are one of the readers of The Sun , your days just got better: we’ve just published a new update to Google Play with a lot of love for the Material Design principles. We thought it’d be useful to share what we’ve done and the reasons behind the decisions we made. What did we focus on? Updating your current application from Holo to Material is not a trivial job. There are many things to take into account, but there is one which is by far the most important, that is your users. The main reason why you should update your app is not for being cool, the main reason why you should update is for your users. The second most important reason, you ask? Yes, I’m sure you got that right too, that’s your users again. Material Design offers a lot of different patterns that will make your users’ life easier, help them finding content faster and reward them with a great experience. It’s not about adding all the new features at once. The vast majority of our users are running KitKat (and our minSdk is Jelly Bean), that makes things easier for us in terms of updating the app. The previous version was fully compliant with the Holo design language, so our users were already familiar with the patterns introduced. In our case, the main aspect we wanted to improve was the access to the content. The navigation was done via the “Navigation Drawer”, and that made things a bit complicated for the users when there were a lot of categories to navigate. Makes sense then to focus on that first, and find ways of improving it. The Navigation Drawer Much clearer, don’t you think? Following the Material Design guidelines, sizes, paddings and fonts have now been updated. We’ve basically removed one level of information from the Navigation Drawer, and only left what amounts to the top level navigation between Sun+ contents. All those categories still need to go somewhere … but where? The Ribbon There’s not that many places we could move the categories index to, and since Spinners are not cool anymore (if they ever were), the most obvious thing here is using a SlidingTabStrip . Big difference! So what’s happened here? One can notice that the Spinner has disappeared, leaving a clean and branded Toolbar . Having all the categories makes it much easier to navigate through the content and accessing all the news. Small changes can make a big impact It’s all about the details—that’s what really makes a difference and this is where we tried to make the biggest impact. This is the icing on the cake. The status bar is transparent with 20% black colour, that makes it to appear darker in comparison to the Toolbar . By animating the change from one colour to another, we also travel with the user from one category to another. It’s not just about moving from one section to another, it’s also about the journey. Instead of the old Pull to Refresh we’ve now moved to the new SwipeRefreshLayout . Yes, you’ve seen that right, the colour also changes whilst navigation from one category to the next one. The sort of quick return pattern is also a pleasant surprise, allowing more space to the content. Inspired in Google Books, we try to hide and show the Toolbar just when it’s important to the user. Keep it simple and prioritise There is so much room for improvement here: animations, shared elements, etc. Thanks to the new set of APIs and new libraries like the amazing Palette , we can’t wait to start improving and get the most of it. For now, this is not an all or nothing, only the most important and valuable features will be added. The guidelines are there as a reference. It’s not a design checklist, just implement what really matters to your application and to your users.", "date": "2014-12-16"},
{"website": "Novoda", "title": "Working Practices Discussed and Displayed", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/working-practices-discussed-and-displayed/", "abstract": "Thankfully, the days when software developers just guessed the customer’s requirements, slung a product together and threw it out the door are long over. Instead, we now advocate the principles of software craftsmanship—taking pride in our software processes and in the code that we write. In collaboration with Qi Qu, one of our amazing designers, we have come up with an inspirational and understandable representation of some of the working practices we employ at Novoda. It’s important to keep track of what you do—to remind yourself and your colleagues of best practises and the parts each one played in the course of a project. So we created this wonderful poster. The outer circle in the poster is all about working with our clients to deliver what they want, when they want it, with a sprinkling of Novoda awesomisity. Working alongside our customers, we use Agile methodologies to come up with the application that is exactly what they need. Ensuring we have shippable releases at the end of every sprint (sprints are either one week or two weeks long, depending on the client and the product) means the customer always has something they can get their hands on and look at. This allows for a quick cycle of feedback and responsive iteration on designs and requirements, all in the customer’s best interest. The middle circle represents team collaboration. A “team” can mean the whole company, the group working on a particular customer product, a pair working on a poster, or anything in between—even ad-hoc teams hacking something together over lunch! Having your whole team believe in craftsmanship gives you benefits well beyond clean code. A collective ownership of what we do, continuously integrating with each other and working to meet the high standards we’ve set at Novoda means somebody is always trying to better or improve upon our ideas and work. This pragmatic attitude always feeds back into the product, iterating to enchant, surprise and delight within our apps but also to simplify the customer’s understanding of what they want. The innermost circle is the mantra for individuals, a reminder to keep it simple, always support your code using tests and refactor or reflect on your work to ensure it is something you are proud to let others see. Speaking of letting others see your code leads us to pairing. Pairing is an integral part of working at Novoda: it encourages knowledge sharing, provides sanity checks and also promotes social engagement and communication between team members. These key points help craftsmen build robust, well-tested and maintainable applications. When asked to design this poster, the first question that came to mind was “what is this poster for?” The purpose behind it was to share Novoda’s working practices with the world, and even though it’s explaining code and software, it needs to be understandable and visually enticing. After discussing with Paul and his ideas described above, we came up with a first rough sketch. I wanted the poster to represent Novoda effectively, so I chose to use the Novoda brand colours: blue as the primary colour and the orange as the accent colour. To communicate Novoda’s expertise regarding the Android platform, I injected a little fun with the Android robot in a hat, holding a pointer stick to make it look like he’s explaining the content. Shadows gave each element more depth, affording each item a bit more visual impact. I also spent time refining the font styles to ensure optimum legibility. We made some test prints at the end of the first iteration and decided that the process descriptions were too wordy. I worked together with Paul and Kevin to re-organise and simplify the copy, making it more digestible for the reader. We also felt that the style of each circle could be improved to emphasise each area of development. It was important to stick to the Novoda colour palette so I needed to think of another way to set each circle apart. I decided to change the outer circle to a dotted line and highlight the words by placing them insides boxes. This style change achieved a distinctive look for the outer circle and the addition of the key also helped to summarise the different areas. I like to show my work to everybody. Part of the Novoda design process includes stages of critical feedback (within the design team and also with anyone walking past our desks!)—whether it’s positive or negative comments, any constructive criticism is welcome, as it’s the perfect opportunity to improve my work. I hope you like the poster. We will also be releasing the SVG, so feel free to print out your own copy. SVG is now uploaded to dropbox and available here .", "date": "2014-10-07"},
{"website": "Novoda", "title": "Holo—Android after Ugly", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/holo-android-after-ugly/", "abstract": "Android was once the domain of amazing utility at the expense of beauty. This has long been false and it is very disappointing how many designers fail to realise the amazing opportunities presented by today’s modern interfaces. The Android design guidelines outline “Holo” which is a modern design language which celebrates the upmost utility in a minimal aesthetic. It is now essential for designers who wish their applications to be modern, relevant have any chance of appearing within the celebrated best or within Google Play's featured apps to recognise and  adhere to the Android design guidelines . All the slides are from the guidelines but you can also find them here: Holo. Android after ugly slides on Scribd . Holo resources: Android design guidelines Blog: Holo everywhere Gallery: Android Nicities Handy tools to help “Holo-ing” out your apps: Gallery: Android Asset Studio Thanks to the Product Design Guild London for hosting the talk at the Google Campus on the 3rs Februar 2013. Thanks to Kevin for giving the talk as well!", "date": "2013-03-08"},
{"website": "Novoda", "title": "Paper Prototyping", "author": ["Dave Clements"], "link": "https://blog.novoda.com/paper-prototyping/", "abstract": "Dave Clements and David Gonzales show how they use paper wireframing to build native prototypes of soon-to-be Android applications, crafting user journeys to send to clients for fundamental feedback.", "date": "2014-12-18"},
{"website": "Novoda", "title": "Certified “Wise” with WideVine Certification", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/certified-wise-with-widevine-certification-certainly/", "abstract": "Novoda are now a certified Google Implementation Partner for Widevine Modular DRM. In case you weren’t aware, Widevine Modular DRM provides support for the latest industry standards for content protection on multiple platforms—especially Android. After a lot of hard graft, study, and being shipped half-way around the world to talk to the Widevine guys directly in sunny rainy Seattle USA, my colleague Volker Leck and I finally received our shiny Widevine certificates. It may have taken two days without sleep, 4 flights, 2 taxis, missing the Super Bowl, lost luggage, and a lot of coffee, but it was worth it. Recently, Novoda has been working with a number of video content streaming partners and helping them in their endeavor to provide the best solutions to their customers. And a best solution also means a smooth experience when watching video that is protected by DRM. For the Android platform, Widevine DRM is the only available choice of DRM solution. Following Google’s announcement that it will no longer support Microsoft Silverlight (and therefore PlayReady DRM) in its Chrome browser as of early 2015, Novoda are taking a bold leap for Android-kind to help implement MPEG-DASH/EME solutions with Widevine. Google WideVine office We’ll be using our knowledge to craft enchanting, buttery-smooth, seamless media content applications. Having extensively researched the best approach on the platform for legacy Widevine Classic DRM and MediaPlayer , as well as pushing the boundaries with MediaCodec , MediaExtractor , Widevine Modular DRM and Google’s open-source ExoPlayer . Using this approach, we’re able to cover the wide variety of devices to which the platform lends itself. The fact that YouTube is also rocking the ExoPlayer inspires us to learn and try even harder to show just what can be done in the world of video applications.", "date": "2015-03-30"},
{"website": "Novoda", "title": "MUBI for Android TV", "author": ["Ferran Garriga (Senior Software Craftsman)"], "link": "https://blog.novoda.com/mubi-for-android-tv/", "abstract": "For the last couple of months, my colleague Daniele Bonaldo and I have been working on a new project for MUBI . While Novoda had previously developed the Android phone, tablet, and Amazon Fire applications for the service, this project promised to be an especially exciting new challenge. MUBI is a video streaming service with a focus only offering high-quality, hand-picked movies which film enthusiasts can enjoy. While other streaming services offering difficult-to-explore catalogues of varying quality, MUBI focuses on providing the user just 30 films and with a new, hand-picked film added daily. The service is already available on multiple platforms, including via the web, on Android tablets and phones , Amazon Fire TV , Playstation, and more. During the course of this project, we’re proud to have developed for the newest platform to be added to that list: Google’s Android TV. MUBI for Android TV ( Android TV , for anyone who may have missed its launch, is Google’s new smart-TV platform, offering applications designed to be used on a TV and from the comfort of a sofa, all without the need for a phone or tablet.) After a lot of hard work, we are proud to announce that not only is MUBI is now available on Android TV, but has also has been nominated by Google to be featured on the Android TV platform as well as being pre-installed on all Nexus Player devices sold in the UK! But how did we get there? Let’s get into the detail. Exciting new features Before we began we were already excited about the opportunities the platform presented, and not only because of the varied technical challenges it would offer. We were also looking forward to the opportunity to integrate the service with all the new Android TV features that the platform provides. Features like OS-level recommendations integrated voice search and beautiful ‘Daydream’ screensavers are just a few examples of integrations that applications on the platform can leverage. That level of integration doesn’t happen without some work, of course, so let’s talk about how we approached each of these features. Global search A great way to increase the visibility of an app and surface its content to the user is global search . With global integration, an app can be queried by the system and have the results displayed on a global result screen (alongside the results from other apps). For MUBI, this is a perfect way to make sure the user is able to discover all the currently available films without necessarily having to open the app. This not only provides quick access for those who know what they want to watch, but also allows for happy discovery by users who’re just browsing around. On Android TV the user has two ways to perform a global search: Through the search widget on the home screen By initiating a voice search using the remote In both cases, if the query leads to one or more matching films a dedicated row is presented in the search results screen. The user can then directly access the film details and start playing without having to navigate further inside the app. Search results The Android TV search framework can also create a deep-link to a ‘watch’ action for a specific film from the search results screen. This will happen when a search result matches a specific piece of data for a given film, including title and production year. Here, the system will sometimes propose different apps providing the same content. This means the user can browse content (in this case, movies) without having to dive into specific applications. In the case of MUBI, when the user selects “available on MUBI”, the application will start and land the user on the the relevant film details screen, allowing them to immediately play the film if they wish. Deep-link in the film details result In order to deliver these unified search results to the user, Android TV retrieves content data from all installed apps using content providers. An app that wants to provide global search must implement: A content provider that will serve suggestions while the user enters characters in the search dialogue, along with the permission android.permission.GLOBAL_SEARCH in order to allow Android TV to query it. A configuration file which describes the content provider and contains informations required by Android TV. An Activity that is able to handle the Intent fired when the user selects one of the query results must be specified too. So it’s actually pretty simple. Recommendations It is always difficult to decide what to watch among a myriad of content and services. It is really not user friendly to have to go to every single application, check the content, check if it’s new, navigate through all the sections, and finally then decide that we are going to move to another application because there is nothing interesting to watch. For that reason, Google offers a recommendations stream directly on the main Android TV home screen. This stream is populated by the operating system after learning what content the user is interested in. This allows the home screen to constantly surface the most relevant content from all of the applications that the user has installed. Of course, this only happens if those applications are set up to contribute to the recommendations stream. This is something we were keen to implement with MUBI. The most popular film recommendation card Working with MUBI we iterated through various approaches to the feature before finally settling on a fairly simple approach: we recommend the most popular film among the 30 films available at any time. If the user has already watched the film, we simply update the recommendation to display the next most popular film. This all happens under the hood and without the user having to do anything—just enjoy watching movies and MUBI will always be hand to recommend some other great film. Google Cast integration Google Cast is a technology that allows Android, iOS apps and Chrome web apps to ‘cast’ video, audio, and screen sharing to Cast-ready devices. The most popular of these devices is the Chromecast , but Android TV is another Google Cast ready device that we were eager to cater for. An obvious use-case for such a technology is to allow a user to cast films from a streaming service (such as MUBI) and watch them directly on the big screen of a TV. For this reason we wanted to ensure that we implemented the best possible Cast support for MUBI. The end-result is that a user can use MUBI from any mobile device (not necessarily Android) to cast their chosen film directly to a Chromecast or an Android TV without necessarily having the app installed in the device itself. Once the film is being cast, the mobile device becomes a controller for the film’s playback, while the Cast device handles streaming the selected film itself. As the mobile device is not streaming video to the TV or Cast device directly itself, casting a film has no great effect on the mobile device’s battery—far less than streaming the film directly on the phone or tablet would do. Alongside integration of the Chromecast into the Android applications, we also helped implement the server communication and synchronization in the Chromecast-specific code to allow for seamless pausing and resuming across devices and casting—this provides a unified experience for the MUBI users. The end result is that a user can start casting a MUBI film to watch on their TV, stop watching it, and continue right from where they left off on their mobile device. And this device doesn’t even have to be an Android phone or tablet—the server-side synchronization gives the user the ability to continue watching a movie at the correct position on all devices and platforms supported by MUBI, including from the web or from a Fire TV device. If you’re interested in more details, this article includes the whole set of cast-related features we introduced in MUBI. Daydream Screensavers have existed for a long time, with the express purpose of protecting monitors from a burn-in effect. From Android 4.2, an equivalent feature, with the name of Daydream , became available on phones and tablets. Given their availability on Android TV, their purpose is now even more relevant. Daydreams are particularly interesting and fun for an app like MUBI. With its great selection of high-quality film posters, there was obvious choice for what we could showcase as a MUBI daydream. The final Daydream MUBI implementation shows the poster of a current film, displaying the title and director of the film with subtle but beautiful animations. This is changed every few minute to prevent screen-burn, but also serves to highlight MUBI content, as well as just being pleasantly distracting. Daydream screen saver A single APK Android applications are packaged in a file called APK (Android application Package). When you install an application from the Google Play store, this APK file is downloaded and installed into your device. Many developers create different APK files for different devices or platforms. For example, many create one APK file that runs on phones and another to be used by tablet. This would typically happen if the tablet version differs heavily in format or layout to the phone version or the developer is simply feeling lazy. When a user searches for an application, the listing in the Play Store may look the same, but the APK the user ends up downloading will be different. This is actually a bad practice. Google recommends creating one APK for all platforms and screen factors. This single application should then adapt itself to be displayed in the right format for phones, tablets, or TV. There are advantages and disadvantages to creating just one APK. Here the key points: If one APK serves all platforms, that APK has to have the resources for every platform. This means that if the device has a low-resolution screen, the APK will still come bundled with high-resolution icons and resources that will never be used. As a result, the size of the APK file created for all devices will be bigger than a APK created for a single device type. Sometimes it’s simply is not possible to offer just one APK file due to limitations on third-party stores or the targeting of specific hardware architecture. For example, the Amazon App Store doesn’t allow APKs that contain specific libraries built for the Google Play Store. In this case, the only option is to remove all code that is not allowed from the final APK and create a variant specifically for submission to the Amazon App store. But there is one major selling point for offering a single APK (at least in the Google ecosystem): when you first log in into a new Android device (such as a new Android TV), Google automatically downloads any applications you may have installed on other Android devices you already own (such as an Android phone). Working right out of the box This means that if you have the MUBI installed on your phone and you buy a new tablet, shortly after logging in you’ll find you have it installed. And, as it’s built using a single, adaptive APK, the application will be tablet-ready right out of the box. This becomes even more important with Android TV, where searching for applications can be a little trickier than with a touch-screen. With the single APK approach, users who’ve installed MUBI on their phones will automatically have the MUBI Android TV version ready to use shortly after turning it on, all without having to visit the Play Store. The future of the application There are even more features to come, such as integration with Google Now, seamless background playing on Android TV and, well, you’ll have to see. It is a really exciting moment to be in the Android system right now and we can’t say how proud we are to worked with MUBI on bringing their fantastic service to another platform. Get downloading and let us know what you think. (Special thanks to Daniele and everyone who has collaborated in this blogpost.)", "date": "2015-04-10"},
{"website": "Novoda", "title": "Growing Android Applications, Guided by Tests. Part I: The Setup", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/growing-android-applications-guided-by-tests-part-i-the-setup/", "abstract": "This is the first of a series of posts aimed at showing and discussing best practices on TDD and Android. The title is obviously an homage to the GOOSGBT book, as it shares the same purpose of showing how a real application grows guided by tests. The project I'm working on is a Github Android client , and I’m trying to make every commit as self-descriptive as possible, so that you can follow a logical series of action jumping from commit to commit. These blog posts are a way to expand those commit messages, giving more insight on why certain choices are made, discussing alternative approaches and gathering feedback from other developers. The next parts will cover how to build a walking skeleton and mocking the API service. While GOOSGBT is still a great reference for TDD and Java, Android developers face particular challenges while testing their code, mainly because the lifecycle of key objects like Activities and Fragments is managed by the framework. Thankfully, Android testing has gone a long way since its inception, and now we have an interesting array of tools tat our disposal. We can use Robotium to drive our integration tests and Robolectric to unit-test objects that depend on Android OS. We can use Injection to mock singletons and inject dependencies on the Activities and REST clients like Retrofit to abstract over a third-party service. Of course, this application is mainly a pet project (see Breakable Toy ), where I can try out new ideas and verify alternative strategies that are hard to setup in production code. Sometimes you'll find things that will make a TDD purist cry, sometimes I'll skip a few steps just to include an interesting library. That’s ok, if you see something that you don't like you're welcome to make a pull request. But enough for the introduction, let's jump right into the code. Every paragraph will show the link to the commit I’m discussing, so that you can read the post and the code side by side. Initial Commit (Click here for code) Define the project structure as follows: github-android contains the actual Android Application and the unit tests for its declared objects. It depends on github-android-core. github-android-core contains plain java objects used to exchange information with external services (e.g. an api server) plus the associated unit tests. github-android-it contains the integration tests used to verify the overall behaviour of the application. It depends on github-android. github-android-smoke contains test fixtures used to verify the expected behaviour of external services. It depends on github-android-core only, so that the fixtures can run on a plain JVM. We start with a project configuration that is an extension of the plain maven release archetype for android . What is crucial in this step, and I can't stress it enough, is that you want, you really want to mock the external services that your app uses. Just as in GOOSBT, you may build your application against a service that you can't access, and you have to rely on its documentation. But more often, you find yourself in a situation where the external service is half-completed, with some features already implemented, some missing, and some that will change in the future. Having a dummy service at hand is a lifesaver in these situations, because you can quickly swap the original service for your mock, test and implement the required features, and merge them back to production when the apis are stable. The worst-case scenario is when the service is being implemented at the same time as your application: in this case not having a dummy service abstraction is just like digging your own grave. But even if you're working against a reliable service (as we are, with our Github application), I think you should go the extra mile and use this configuration anyway. This is because it has several advantages: It makes writing tests easier Knowing the response in advance is a huge advantage because you can predict what you should look for in your tests. It makes running tests easier No network activity means faster and reliable tests. If a build fails you know that it's your fault. If your build starts failing because of random connection timeouts, server outage, or something else you can't control, you tend to ignore your “Build Failed” messages, and your CI become useless. It pushes you to write more flexible code Your core package is the bridge between the service and your application. Here is where you validate your models, where you abstract over the service interface and where you try not to let external concepts leak in your application. This is your first line of defence against external changes, and in a perfect world, if something changes in the external services implementation you would only need to touch you core package, making it transparent for the rest of the application. As a minor plus, being a pure Java project means that this module is potentially reusable in other applications (e.g. Desktop or J2EE) that interact with the same service. It pushes you to write tests for the service you're mocking If you mock an existing service, you want to be sure that your predictions are correct. In order to ensure that, for every expectation you have you write a test. This is a good way to understand the service and it's code that you want to write anyway because it is necessary in order to place the correct calls. Moreover, you can catch unexpected or wrong behaviour early on, before it crashes your application and you have to do an intense debug session just to discover that it's not your fault. You will find those test useful even if you just want to double check a particular call: it is really handy to have a named test method for that, instead of copying and pasting urls in your browser, or using external scripts. You should have some cached responses for testing anyway When you're writing your POJO deserializers you need a proof that the deserialization is always successful, right? It is always nice to run your app in mock mode: The service is down for the rest of the day. Or you have to show the app to the client and want to avoid random crashes. Or you need to verify the behaviour in a particular edge case (a network timeout, a particular response, a user with too many objects in its basket etc.). Or the service is rolling api v2 today breaking retro compatibility with your application. Or you're stuck in some weird place with no network connectivity. Bottom line: once you have a mock mode ready at your disposal, you’ll find more and more uses for it.", "date": "2013-05-21"},
{"website": "Novoda", "title": "Novoda in Italy: ESA AppCamp 2014", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/novoda-in-italy-esa-appcamp-2014/", "abstract": "One month on, we look back to September when Novoda were selected to join four other teams at ESRIN in Italy to take part in the third annual ESA App Camp. The European Space Agency (ESA) created the first AppCamp in 2012, inviting researchers and developers from all over Europe to participate in an eight-day competition. This year, we were lucky enough to be chosen from over a hundred applications, to visit ESRIN , ESA’s Centre for Earth Observation based in Frascati, Italy, where we would spend a little over a week building a prototype for an app backed by Earth Observation data. What is Earth Observation? In a nutshell, Earth Observation is the collecting of information about our planet, as well as the analysis and presentation of this data. ESA is huge player in this field, leading the European initiative to learn more about Earth via the use of remote sensors. Copernicus Copernicus is the European Commission’s Earth Observation Programme, aggregating data from a variety of sources: Sentinels: a set of purpose-built satellites for the Copernicus Programme Contributing Missions: the collection of third-party satellites that also feed data into Copernicus services In-situ data: ground based weather stations, ocean buoys, air quality monitors, etc. ESA is responsible for the design and launch phases. The Brief Each group needed to develop an app to solve some existing problem (that is, has some market value), that was feasible to get started with currently available data, and one that used, in some way, the satellite data from the Copernicus Programme. Devs hard at work Copernicus offers six main services (datasets). We had to develop an app that fit into one of these services: land monitoring marine monitoring atmosphere monitoring emergency management security climate change We entered under the land monitoring category; we released Cattle Manager early in 2013 under the Apps for Good programme, and we thought it’d be nice to expand on that and provide some utility for agricultural farmers too. Antonio and Zvonko pairing When we arrived and learned a bit more about what we had to do, we decided to switch to the marine monitoring category; no one else was tackling that and we had a good idea what we could do. Qinu Here’s the app: Qinu (the name is based on one of many Inuit words for ice), was built on the idea that the sea is dangerous. It provides easy to understand forecasts for ice thickness, ice density, speed and more, and is meant to facilitate the planning of shipping and fishing routes. There’s also the ability to collaborate with other vessels to enrich the data for everyone. Layers were obtained from the Ramani API, providing overlays for Google Maps containing data (either based on samples or from the Contributing Missions) regarding all of the services mentioned before. We chose to include a subset of the layers pertaining to the marine service. We added mock data to help visualise the social/crowd-sourced data aspect to Qinu: the idea is that users of the app would be able to augment the data obtained from the APIs, either verifying, correcting or adding information, inspired by spotter services that exist today . The hardest part of the camp was understanding what the end goal was. Working at Novoda, we had become used to a process where we work with a client that has a business need, typically approaching us with an existing customer base. The clients come with the knowledge about the product requirements, and we provide them with guidance and expertise in the Android space. In this case, we had a bunch of data sources, but no client. We decided to come up with a user persona (a small, unnamed fisherman) and tried to represent his needs in the app. As it turns out, this was precisely the way to go. The judges at the end were looking for a story in the presentation—something to convince them that the app we proposed was going to use the data from the Copernicus programme to solve a real need, or at least a viable business opportunity. It wasn’t only the app they were interested in - they wanted to see ideas for the promotion (marketing) of the app, how we expected to grow and scale; the camp was just as much business as it was technical, which is something we didn’t expect but was really good to see from other teams. Now that we have a decent idea of what the AppCamp is about, I can see us applying again next year with a pre-baked idea and a designer to add some sparkle and magic. If you’re a mobile developer (or development team), I’d highly recommend applying to the 2015 camp; there’s a great mix of people and very sharp minds, and it feels wonderful to work on something a little “bigger-picture” than usual. A+++ would go again", "date": "2014-10-07"},
{"website": "Novoda", "title": "Espresso UI testing with RxJava", "author": ["Benjamin Augustin (Software Craftsman)"], "link": "https://blog.novoda.com/espresso-ui-testing-made-easy-for-android-applications-using-rxjava/", "abstract": "I’m happy to announce the release of RxPresso, a new library to make Espresso UI testing easy for Android applications using RxJava. It will help you set up and test the UI of your Android application, with a predictable data source. What if we could do UI testing with a predictable data source that would be easy to setup? Here’s a quick tl;dr version of the process. If you use RxJava in your application your UI tests are as simple as this: rxPresso.given(mockedRepo.getUser(\"id\"))\n       .withEventsFrom(Observable.just(new User(\"some name\")))\n       .expect(any(User.class))\n       .thenOnView(withText(\"some name\"))\n       .perform(click()); Go and check it out at github.com/novoda/rxpresso . So why a new library? Isn’t Espresso enough? At Novoda we try to test all aspects of our applications. This means presentation layer testing as well. In the past we would rely on end-to-end Espresso tests for our presentation layer. It’s good to have a few end-to-end tests to check user journeys. Ideally, though, we would be able to fully test the presentation layer, including all error states, empty states, variations of data models, edge cases, and so on. Relying on a real data source to test these presentation layer scenarios can be difficult, if not impossible. This reliance on a real data source can also lead to tests that are unreliable, flaky, or difficult to maintain. This leads to less UI tests being written as no self-respecting developer wants to write tests that can’t be relied on. On the other hand, Unit tests are usually quite easy to implement and maintain as, if written well, they act as stand-alone tests, with no reliance on external resources. So what if we could do UI testing with a predictable data source that would be easy to setup? There are a few existing attempts at this. We initially experimented with MockWebServer , a mock datasource implementation, but we found that the difficulty of setting up each test was far from the ease of mocking data in a Unit test. So we decided to see whether we could build a framework that would let us achieve the simplicity of unit test syntax in presentation layer tests. RxPresso is the result of this effort. Test & Synchronise RxPresso is a simple way to mock data while testing and for synchronizing Espresso view actions with Rx pipelines. RxPresso doesn’t replace Espresso it’s simply a layer on top of it to deal with mocking data and synchronization. Here’s what that looks like in practise: rxPresso.given(mockedRepo.getUser(\"id\"))\n       .withEventsFrom(Observable.just(new User(\"some name\")))\n       .expect(any(User.class))\n       .thenOnView(withText(\"some name\"))\n       .perform(click()); If we look at this example in more detail: rxPresso.given(mockedRepo.getUser(\"id\"))\n       .withEventsFrom(Observable.just(new User(\"some name\"))) You can compare those two lines to the equivalent in a normal Unit test for synchronous repository. when(mockedRepo.getUser(\"id\")).thenReturn(new User(\"some name\")) And the remaining lines: .expect(any(User.class))\n       .thenOnView(withText(\"some name\"))\n       .perform(click()); This tells what event we are waiting for before performing the test (here any User since we inject it above). The thenOnView() call is what links us back to Espresso world. It acts as the onView() of Espresso and allows you to chain whatever Espresso test you want. A look under the hood RxPresso relies on two concepts. The first one is a tool we developed called RxMocks , that allows us to control the Observables. The second one is a custom IdlingResource, that synchronizes Espresso with the Observables. RxMocks mocks the Rx pipelines and allows us to inject data into them. This provides the stable data source needed to make tests reliable. In order to achieve this, we use Java reflection with Proxy.newProxyInstance() to generate a mock of the Repository. Here you can see an example of a Repository that could be mocked: public interface DataRepository {\n        Observable<User> getUser(String id);\n        Observable<Articles> getArticles();\n} This mock of the repository will hold a distinct Observable for each method/parameter combination. In the Repository above, this would result in one Observable for the call getUser(\"Bob\") , another one for getUser(\"Alice\") , and so on. These Observables can be injected with any data you need to fully test all cases your presentation layer might need to handle. RxMocks also provides a way to observe the events passing through those pipelines without impacting the subscription chain. This ability to monitor the events going through those pipelines is what enables us to synchronize the state of the IdlingResource with the injection of the mocked data. IdlingResource is an interface provided by Espresso that allows you to declare to Espresso a resource to wait for before running tests. If you ever played with it yourself you know that implementing it is usually a lot of boilerplate code. Our IdlingResource implementation tells Espresso to wait for the data to come through the pipelines. This lock state ensures that your Espresso ViewActions are executed only once the data has been delivered to the presentation layer. To see more examples of usage or set it up in your project, look at the read me or check out the demo . We are really excited about how this enables us to test our presentation layer in an easy and extensive way and we already have plans to improve on RxPresso’s functionality. We are eager to get community feedback and make RxPresso a tool that meets the needs of a wider audience. So feel free to contact us, open an issue on the Github repo, or even submit a pull request.", "date": "2015-05-14"},
{"website": "Novoda", "title": "The Hudl 2", "author": ["Ryan Bateman (Head of Product)"], "link": "https://blog.novoda.com/the-hud/", "abstract": "We’re incredibly proud to announce that we’ve again worked with TESCO on a native suite of Android applications, customised OS and Launcher, and an entirely new set of features for Tesco’s new hudl2 tablet . More than 11 months in the making, the hudl2 project involved development, design, and implementation work by of many of our software craftsmen, working from our offices in Berlin and London. The TESCO Launcher we created is a completely customised Android launcher, developed expressly to bring Tesco’s many services closer to the customer. Swiping left from the main screen brings you straight to ‘My Tesco’—an updated, personalised view of Tesco services relevant to the user. This view constantly adapts to your behaviour, habits, and location to surface the best of Tesco content, including Blinkbox books and movies, updates on your shopping orders, your Clubcard points, your Tesco banking, recipes suggestions, and much, much more. We’re really happy with the way it turned out, and the fact it is reactive means it’ll stay relevant for users throughout the tablets’ lifespan. Paul Blundell, Team Lead on the MyTesco project Feedback from the first hudl revealed that users really appreciated its focus on family-friendliness. Tesco was eager to work with Novoda to ensure the hudl2 made parents feel even safer and happier. With this in mind, Novoda worked to ensure that the hudl2 allows parents to set daily time-limits for the children, gives them fine-grained control over their children’s web access, and also allows them to define exactly which applications their children can use and for what lengths of time. Novoda also worked with Tesco to create ‘Top apps’—an application that highlights the best apps and games for the hudl2 as chosen by your fellow hudl2 users, giving a curated view into which apps and games work best for the new tablet. From a development perspective, working on the hudl2 differed from the original product due to some of the new features Tesco had in mind. This time around we had to really delve into the ROM and the platform to utilise all that Android has to offer to achieve our goals. It was a big technical challenge, and a great experience getting to work on building the new device after such great success on the first. Shivam Gadhia, Team Lead on the Child Safety Project We’ll be diving into some of the technical challenges of the project in coming blog posts, but for moment go out there and and get ready to order a hudl2 for yourself on the 8th of October.", "date": "2014-10-03"},
{"website": "Novoda", "title": "What’s Going on over HTTP Logging?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/whats-going-on-over-http-logging/", "abstract": "Often, when working to debug over HTTP, you want to see everything that’s going over the wire. You can sniff the connection with wireshark, or on Android you can output it directly to your local log.", "date": "2012-12-12"},
{"website": "Novoda", "title": "Collapsing AppBarLayout on Android TV", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/fixing-hiding-appbarlayout-android-tv/", "abstract": "CoordinatorLayout has really made it easy to perform animations for elements that are dependent on one another, but not all of the behaviours work out of the box for Android TV. There are some compelling use cases for CoordinatorLayout - Behaviors are a neat (as in \"neat-o\") way to encapsulate view logic that involves multiple elements - like the simple app bar collapsing behaviour : Excellent, and took less than ten minutes to implement. Let's test it works for keyboard users, :shipit: and lunchtime! But... not so fast. Here's what it looks like on a Nexus Player (or any device with non-touch input): the app bar doesn't actually collapse The last item in the list isn't fully visible We could live with the first one, but the second actually degrades the user's experience (compared to not using this Behavior). The app bar doesn't react to scrolling of the RecyclerView as you navigate down with a directional pad so the scrolling view is still the same size of the viewport, but its offset isn't adjusted. We can fix this. We have the technology. Beginning with version 23 of the design support library, AppBarLayout includes a new show/hide API: [ setExpanded(boolean expand, boolean animate) ]( https://developer.android.com/reference/android/support/design/widget/AppBarLayout.html#setExpanded (boolean, boolean)). When a d-pad is used to scroll the view, we can trigger that behaviour ourselves: public interface AppBarExpander {\n    \n    void expandAppBar();\n    \n    void collapseAppBar();\n    \n} public class SeasonsActivity extends AppCompatActivity implements AppBarExpander {\n    ...\n\n    @Override\n    public void expandAppBar() {\n        boolean animate = true;\n        appBarLayout.setExpanded(true, animate);\n    }\n\n    @Override\n    public void collapseAppBar() {\n        boolean animate = true;\n        appBarLayout.setExpanded(false, animate);\n    }\n} The [ scrollVerticallyBy(int dy, ...) ]( https://developer.android.com/reference/android/support/v7/widget/RecyclerView.LayoutManager.html#scrollVerticallyBy (int, android.support.v7.widget.RecyclerView.Recycler, android.support.v7.widget.RecyclerView.State)) method in RecyclerView.LayoutManager is called whenever the RecyclerView is scrolled - in both touch and non-touch mode. We can override this method, and if we're in non-touch mode (i.e. using a d-pad), then we can notify our AppBarExpander: AppBarExpander appBarExpander;\n...\n\nfinal RecyclerView view = (RecyclerView) layoutInflater.inflate(R.layout.view_season_page, container, false);\nview.setDescendantFocusability(ViewGroup.FOCUS_AFTER_DESCENDANTS);\nview.setLayoutManager(new LinearLayoutManager(container.getContext()) {\n\n    @Override\n    public int scrollVerticallyBy(int dy, RecyclerView.Recycler recycler, RecyclerView.State state) {\n        if (!view.isInTouchMode()) {\n            onScrollWhenInNonTouchMode(dy);\n        }\n        return super.scrollVerticallyBy(dy, recycler, state);\n    }\n\n    private void onScrollWhenInNonTouchMode(int dy) {\n        if (dy > 0) {\n            appBarExpander.collapseAppBar();\n        } else {\n            appBarExpander.expandAppBar();\n        }\n    }\n\n}); And here's the end result (or as I like to call it, lunchtime): 2 September 2015: you can also use this technique by adding a RecyclerView.OnScrollListener to the RecyclerView directly, which should be more robust (at least in terms of switching LayoutManagers).", "date": "2015-08-22"},
{"website": "Novoda", "title": "Growing Android Applications, Guided by Tests. Part II: A Walking Skeleton", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/growing-android-applications-guided-by-tests-part-ii-a-walking-skeleton/", "abstract": "This is the second part of our series Growing Android Applications Guided by Tests. We’ve already covered The Setup in part one. Now we’re here to build a walking skeleton. We start by writing a unit test for your scenario. This Java object will represent a single flow of our application and will encapsulate the business rules behind this user story. Going test first allow us to discover the various dependencies that our scenario needs and to define the project structure. This package structure clearly reflects the intent of the application: all supported user stories sit on a package, and the boundaries should be as decoupled as possible from the rest of the application. By defining the first scenario, “Search repositories by keyword” ( find code here ) we set up the basic structure. But how can you start development? Should you start within the Database, or in implementing the Rest client or tweaking the Xml layouts? As many of you know, deciding where to start is an important step in Agile development , so important that an entire discipline has been created for this purpose . BDD is at the core of the development here at Novoda. With the help of tools like Pivotal Tracker our clients can add and review their user stories, developers can monitor the state of the development and the QA can accept or reject the stories that have been implemented. This is a great way to promote transparency and keep the team focused for the current sprint , but there's more than that. Just as TDD is more than having your code under test, BDD is more than involving the clients in the development and to keep track of the current state of the application: it is a discipline that makes you write code in a particular way, focusing on what you want to implement, not how . Let's make an example in our domain. Suppose that the first feature we want to include in our app is the ability to search for public repositories. A user story for this scenario may sound like: As a user, I should be able to search repositories by keyword and can be formally represented as follows: @Given The github api, A user interface, A search request containing a keyword @When The user triggers a search request through the ui @Then The ui should display the github repositories retrieved for that request The first thing that you'll notice if you're doing BDD is that every user story defined in a formal way can be written as a Java object: @Given defines the constructor parameters, @When is a method and @Then is the behaviour executed in that method. Being a Java object means that we know where to start: we should write a test for it. While writing this test we'll stumble upon the first objects and dependencies contained in our domain: the Github Api Service , the Presentation Layer , the Request and Response messages passed throughout the system and so on. And as soon as we make the test pass, we effectively implemented the business logic required to run that scenario. Now look at the package structure. Usually, an Android project has a top level structure of activities, adapters, loaders, etc. To find out what that app does you have to dig deeper in the activities and fragments code and even then you don't have a clear picture of the intent of the developers because the business logic is spread throughout their architecture. Compare this with our project: if you want to see what this application does, you simply open the scenario package and you should see a clear list of all the features that the current implementation supports. You may open the request and response packages and you'll know immediately what kind of messages are passed from the presentation layer to the boundaries and vice versa. This idea of making your architecture explicit has been know for a while in the developer community, but only gained momentum after a recent Uncle Bob talk . It can be summarised like that: databases, services and ui are not the important part of your application, they are just implementation details . It may seem strange for an Android developer to think of Android as only a detail in its architectural design but the truth is, that this will lead to better a better de-coupled architecture. We should push Android to it's boundaries and retain as much as plain Java code as possible to keep objects easy to test. In principle, we should be able to run the very same application on a J2EE server, or on a Swing application. Of course you'll never do that, but that's the key to a clean, flexible and maintainable architecture, which is why you started doing TDD in the first place, right? Add an end-to-end test for the first scenario Click here for code In order to actually explore the boundaries of our scenario, such as the UI interaction, we write a test that runs on a real device. This test will exercise the scenario code and the android frameworks, and will remain red until we have implemented the required UI elements. Thus, in our presentation package we are forced to deal with android specific concepts like activities, adapters and managed lifecycle. We decide to use Robotium as our IT library as it make easier to write expressive and more robust tests, as independent as possible from the actual implementation. Now that the business logic for our scenario is implemented, we can start exploring the boundaries and in particular the presentation layer that we will write for Android. We want to write a failing test that can accompany us until we can effectively show the result in our UI. We'll write a test that runs on a real device, so that we can simulate the UI events that our user will trigger in order to execute the search repositories scenario. Integration tests are slow and brittle by nature, so we are not aiming for an high code coverage here. We just want our test to exercise the code in the scenario, so that we can be sure that all our objects interacts the way they should. You can use a code coverage plugin like Emma to verify that the code you want to test is effectively called . You'll notice that in the test we are searching only for an handful of strings instead of checking a full response of 30-40 repositories. The reason is that Robotium waitForText method is really slow, and you don't want those slow device tests to be even slower. Plus, the test resources that we are using should be kept in sync while we are expanding our model objects. All summed up, 3-4 objects are enough for this kind of test: remember that you'll probably want to run all tests each time that you want to commit to your central repository.", "date": "2013-05-28"},
{"website": "Novoda", "title": "Styling the ActionBar SearchView", "author": ["David González (Technical Product Owner)"], "link": "https://blog.novoda.com/styling-the-actionbar-searchview/", "abstract": "By now you should be into Holo . If you are not, there’s still hope , but don’t say we didn’t warn you. Now that we are all on the same page, delivering the most holofied style application is our perogative and we want to give the user the best Android experience possible. But sometimes creating a Holo theme to the letter is not altogether simple simple. Most of the views are very easy to customize but we can occasionally find that some UI elements don’t play well with the theme. See the below example of the search view. So, you want to style the SearchView? As of my writing this, it is not yet possible to style the SearchView from the xml. Assuming you’ve followed all the steps from the guidelines your options menu xml wil look like this: We can set the icon which appears in the ActionBar via xml and it would be super nice to continue workign in xml? No, that would be just too easy. Using reflections ... yes, reflections We can figure out the id of the resources and the declared fields we want to have access to from the Android source code of the SearchView . This is how the magic works, if we override onCreateOptionsMenu we have full access to the SearchView and by that, we can modify the attributes we want. What just happened? By doing this we have just changed the background resource of the EditText on the SearchView. We also changed the EditText text colour and, to give a context to everything, we made the action button of the keyboard to show a nice search icon. Note that this won't work if you are using ActionBarSherlock, since the searchView doesn't have the same id: We could continue customizing all the items following this pattern, this is what we'll do for now: Same as before, if you are using ActionBarSherlock there are some different id's. Pulling it all together Using reflection we can now access all the declared fields inside the SearchView. If you fancy, fork the repo and use it at your own risk!", "date": "2013-06-04"},
{"website": "Novoda", "title": "Transitioning to Material Design", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/transition/", "abstract": "Android L (5.0) is the first time since the original release where Google have shared lots of info and tools before launching into the commercial market. This is an ideal opportunity for products, developers and designers to prepare transitioning to the latest updates. Material design is an evolution. It sees Android’s design patterns extending outside of 2D static rules and into designs which consider animations and feedback within their interactions. HOLO design brought us a long way, but what now? How do we transition? How soon? On August 26 at Skillsmatter , we invited Visual Designers to re-imagine some well-known applications. A big Thank You goes out to clients Songkick , CCleaner , and The Sun who allowed us an opportunity to preview some concepts of Material design at the meetup. Here is what we presented. The Sun Material Design is largely based on the study of paper and ink, so redesigning a newspaper felt natural. Use of bold imagery, colour, cards, iconography and the new typography standards really encourage the content to stand out. Using colour to indicate categories without labelling each article helps clean up the layouts and the use of a transparent App bar allows the user to take in the beautiful, large edge to edge images. Dave Clements, @DigitalPencils Songkick Songkick really benefits from Material Design. Its strong emphasis on photography works beautifully within photo grids, showing bold hero imagery within a consistent layout. This new design also take’s advantage of cards to group specific areas of information across the gig listing and detail screens. I’d love to see all this really moving, the material transitions would be a delightful addition! I think smooth transitions and animations across states would add a fluidity and enhance the user experience. Leonie Brewin, @leoniebrewin CCleaner CCleaner for Android is the most efficient way to keep your device clean. This design pushes the branding by taking two of the colours from CCleaner’s logo as primary and accent colours. We use cards as an entry point to ever more complex and detailed screens. Putting the ‘Clean’ action into the floating action button makes sense here, as it’s the primary action. Also we’ve made the bar charts take up the whole header space to create a bold and graphic interface that immerses the user. Qi Qu, @cooky77 Discussions from the meetup After discussing the designs, as a group, @Cennydd posed a few key questions to the crowd and we discussed. Here is a summary: How exactly can we convey “Material” in our designs? Material Design is about layering information and presenting and interacting with that information in a predictable way. Android now ships with in-built support for touch feedback, transitions, and viewstate changes. You can also customise the colour palette by simply changing to the new Material theme. Animations: https://developer.android.com/preview/material/animations.html Color Palette: https://developer.android.com/preview/material/theme.html#colorpalette Shadows: https://developer.android.com/preview/material/views-shadows.html Branding: How can we retain a unique identity? Every medium poses its own unique challenges for a designer conveying a brand’s values. Holo was initially criticised as being too minimal and not allowing enough room for interpretation and individuality. As designers innovated around their constraints, more importance was poured upon the prevalent icons in the action bar and drawer. Material will pose the same challenges, but also new opportunities for expression. Many anticipated a trend in following Google’s initial lead but later an emphasis on unique animations, transitions and general interaction patterns. Experimentation will take part soon after apps have built upon the initial rules. Form Factors: How can we stay “responsive”? Material is an evolution within all our previous design learnings still apply. Clumping together information to contextualise it, keeping actions together and close to the content upon which they affect. Creating interfaces within which the bound content can scale up and down across screens gracefully is now more important than ever. What are “Quick Wins” for a Material Design App? Decide on the primary and accent colours of a brand. Data should not just disappear, instead take the chance to communicate the results of a user’s interactions by animating data emerging from interactions. If in doubt remember some of the wins from Holo, they needn’t necessarily conflict with Material instead we can build upon them as safe foundations. Generally it was agreed that if we anticipate the adoption of patterns to be similar to previous releases, we should start considering updates as early as possible. Considering changes early will give Apps a chance to out perform competitors and benefit from the good will earned by happy customers who expect their software to perform to the best available utility. Further info on Material Design: Material Design Guidelines Layout templates Transitioning to Material with Banesy Join future discussions, held monthly in London at www.meetup.com/android .", "date": "2014-09-15"},
{"website": "Novoda", "title": "Google Hangout: Time Outs Fix", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/google-hangout-time-outs-fix/", "abstract": "Are you still there?!? YES … well, no, but YES! At Novoda we use Google Hangouts for communication between our London and Berlin office and also for connecting people when working remotely. We aim to always have our hangout in the London office live so anyone can connect and see what’s going on. The problem comes because if you leave a Google Hangout running for ~2 hours it brings up a dialog asking “Are you still there?”. If you don’t click “ok” it will disconnect you from the hangout. Solution (Chrome variant) There is a nice Chrome plugin called “Ninja Script”. This allows you to run javascript when a specific URL is loaded. i.e. when you connect to the Hangout, you can run some javascript that will click the ‘ok’ button when it appears, yay! After you have installed the plugin, click the blue cross on your Chrome bar. Press the ‘new’ button at the bottom. Give it a name and description, paste the javascript from below. Finally press the “Autorun” button, here you can paste https://plus.google.com/hangouts and then this script will run automatically when you start or join a new hangout. Done! One caveat, this plugin is a bit flaky in its UI (but once setup runs great). If you find you cannot click some of the buttons go into your Chrome settings and disable and re-enable the plugin. Here is a script to solve the problem! Thanks to spencerelliott on userscripts.org userscripts.org For convenience, the script is below (v1.0.8) (function()\n\t{\n\n\t\tfunction addJQuery(callback)\n\t\t{\n    \t\tvar script = document.createElement(\"script\");\n    \t\tscript.setAttribute(\"src\",\"https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js\");\n    \t\tscript.addEventListener('load', function()\n    \t\t{\n      \t\t\tvar script = document.createElement(\"script\");\n      \t\t\tscript.textContent = \"(\" + callback.toString() +\")();\";\n      \t\t\tdocument.body.appendChild(script);\n    \t\t}, false);\n\t\t\tdocument.body.appendChild(script);\n \t\t}\n\n  \t\tfunction checkForPrompt()\n  \t\t{\n    \t\tfunction simulate(target, evtName)\n    \t\t{\n\t\t\t\tevt = document.createEvent(\"MouseEvents\");\n      \t\t\tevt.initMouseEvent(evtName, true, true,document.defaultView, 0, 0, 0, 0, 0, false, false,false, false, 0, target);\n      \t\t\ttarget.dispatchEvent(evt);\n\t\t\t}\n\n    \t\tfunction simulateClick(target)\n    \t\t{\n      \t\t\tsimulate(target, \"mouseover\");\n      \t\t\tsimulate(target, \"mousedown\");\n      \t\t\tsimulate(target, \"mouseup\");\n      \t\t\tsimulate(target, \"mouseout\");\n    \t\t}\n\n    \t\t$('div[role=\"button\"]').each(function(idx, item) // For each div with attribute role = \"button\"\n    \t\t{\n      \t\t\tif ($(item).html().indexOf(\"Yes\") >= 0) // Correct button found\n      \t\t\t{\n        \t\t\tsimulateClick(item);\n      \t\t\t}\n    \t\t});\n\n    \t\tsetTimeout(checkForPrompt, 6000); // Repeat every 6 seconds\n\t\t}\n\n\t\tfunction init()\n\t\t{\n\t\t\taddJQuery(checkForPrompt);\n\t\t}\n\n\t\tsetTimeout(init, 6000); // Start after 6 seconds\n\t\talert(\"Keep Alive Script Running\");\n\n\t})(); Here is the original script .", "date": "2013-04-02"},
{"website": "Novoda", "title": "Coursera—Android Learning Reviewed", "author": ["Ryan Feline"], "link": "https://blog.novoda.com/coursera-android-learning-rreviewed/", "abstract": "Launched in 2012 and founded by professors Andrew Ng and Daphne Koller, Coursera is an education platform offering over 1,000 courses from different fields. The courses are created by Coursera with the help of partnering universities and organizations worldwide and the best bit is: they are free and available for all. Coursera Android course The Coursera Android Course The course was 8 weeks long and pitched as an entry level course into the world of Android Development. Android Development—not programming—so you needed to have a little bit of prior programming experience to get the most out of this course. The course content was taught by a professor of computer science at the University of Maryland, Dr. Adam Porter, who was supported by a group of Community Teaching Assistants. Which was probably a good thing given there were around 250,000 students enrolled on the course worldwide. Now, how about we go through an average week of the course? No objections? Fantastic! Each ‘week’ of the course was released on a Friday morning (evening if you are in the UK like me), and you had time until the following Sunday to watch the presentations, do the quizzes, the labs and a survey. Downloading the presentations onto my phone with the Coursera app made consuming the presentations on my way to work extremely easy, assuming I managed to get a seat that is, not so much when your personal space is being invaded by your fellow commuters. The presentations as a whole I found easy to absorb. I really liked the use of code samples and demoing apps on an emulator to really hammer home important Android Principles. All of the code samples used were made available to students for them to look into at their leisure. After finishing the presentations, usually on a Wednesday night, I would sit down and go through that weeks quiz. The quizzes themselves did not take very long to complete, assuming you were paying attention and weren’t overridden with guilt when you got a question wrong and just had to take the quiz again to redeem yourself … (deep breath) … all in all they were pretty standard and it was good to have a small exercise to complete to test your knowledge on what was covered in the presentations. Labs A Lab is a coding exercise to give students an opportunity to put into practice what has been learnt from the presentations. For quite sometime now I have been debating on how to write this section—I could rant, I could be polite and say everything was perfect. But to avoid both and stay as neutral as possible, let’s do this as a series of questions. Just like Dexters lab What did you get for each lab session? Each week consisted of a Skeleton project and an associated worksheet. Was the worksheet useful? Yes, mostly. I found the summary section the most useful part. It gave a nice overview of the problem that needed to be tackled for that week. Mostly? What was wrong with it? Well, the implementation details section was annoying. Most of the time it contained the same spiel about downloading the skeleton project, importing it into the IDE and looking for the TODOs. I know it had some actual important implementation details buried in there but I gave up, I don’t want to read duplicate information week after week—‘ain’t nobody got time for that’. What would you do? Put it on the website, have a “Important information to remember for the rest of time” section. Then I can have just the important implementation details in the document. This applies for the testing section too, I don’t need to know each week that there are tests, tell me once I’m sure I can remember. How was the skeleton project? In a word: messy. I think Novoda has made me a snob when it comes to reading code. Code at Novoda reads like a story, methods do exactly what they say they are going to do. The Methods are ordered based on when they are called. All in all it is a pleasant experience. Now the skeleton project did read like a story, if that story was a horror story. Comments all over the place, Hungarian notation used throughout, it was painful. I had to resist the urge to clean up the code base every week. I could have done it but that would have taken a significant amount of time each week to accomplish. I did feel a little like I was cheating on Novoda when I didn’t clean the code base before working on each of the labs—I’m sorry! Would you do the lab section any differently? Yes, I really did not like the whole “here’s a skeleton project, fix it” approach to coding. I felt really disconnected from the code I was writing. I appreciate that this is very close to a real world scenario where you could be jumping from projects week to week, but I don’t think that this works in a learning environment. I sincerely believe that a better approach would have been to guide students through the setup of a base project. This base project could then slowly be built upon each week to encompass the key concepts that go into making an Android app. For me, a lot of learning in code comes from understanding the code base you are working with, and whilst it is true that as a developer you will need to look at someone else’s code, when you are in a learning situation you are better off starting from a clean base to understand how everything fits together, without someone else obfuscating the code. How was the submission system? The submission was the bane of my life whilst doing this course. Each week the lab worksheet would supply a folder structure that the lab assignment had to follow for the submission system to process it. So did it work? No. I curse you submission system! If you tell me to put it in that order, make sure that the submission system actually recognises it, please. I nearly gave up on this course every week because of that stupid system. ‘But it can’t be that bad, you uploaded eventually right?’ Yes I did, that’s because the upload system, when you eventually get the submission result back, has a rather useful error message system. Here’s the thing though, I don’t want to rely on an error log to tell me how to upload a project. I have one extra thing to say about the submission system: why you no use GitHub? Why you no use Git? Seriously, this is a useful skill to have if you want to do development. I used it all the time whilst doing this course. I think that every programming course should have one or two lessons on the basics of Git and how to set it up for development. Do it Coursera! I haz certificate A final note It is worth saying that the team at Coursera have made a commendable effort to make an introductory course into Android and whilst it was not for me, I am sure that there are many students that found this course extremely useful. I am sure that they will continue to iterate over the course material to make it better year on year. Give the course a try, you might get a certificate.", "date": "2015-04-01"},
{"website": "Novoda", "title": "The Making of an Android Auto Demo Unit", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/the-making-of-an-android-auto-demo-unit/", "abstract": "Android Auto is a protocol developed by Google to allow mobile devices to be controlled by a car head unit. This means you can use Google Maps navigation, play music, get notifications of important happenings, all from your car dashboard. The official page has more android.com/auto . Here we are going to show you how we put together an Android Auto demonstration box, this is so Novoda can use it to help our clients write apps for awesome automobile experiences. Inspiration was from the original demo units that where demonstrated at Google I/O this year. You cannot purchase them as an item and we figured, we want one! This post was originally going to be cookbook style. However after creating our demonstration box we have learnt the same recipe doesn’t work for everybody. Whilst creating our box Google came out with their own DIY how-to . This was too late for us and reading it now looks like it makes a lot of assumptions. So here we will outline the steps you need to take, but really detailed specifics are not necessary as these will vary depending on the shop / country / unit system you adhere to. The idea is to have a car head unit powered off the mains running Android Auto and connected to our mobile phone via a USB cable. The whole system will be mounted inside of a builders toolbox, this lets us transport it safely and acts as a nice harness to encompass everything … sounds simple enough, right? Lets do it. Gather the parts There are a few ways to set out your stand, we opted for a light up switch for the power, this actually made the build slightly more complicated as we had to work out how to connect three wires instead of the regular two on a simple switch. We also opted for a big (10 amp) power converter, this was on the recommendation that the Pioneer headunit ran at about ±5.2 amps. While a smaller one should work fine, we opted to stay on the safe side. 1 x Toolbox 1 x Acrylic sheet 1 x Android Auto headunit 1 x Headunit-to-wires connecter 2 x Speakers 2 x Switches (power & handbrake) 1 x Cigarette power converter 1 x Power converter 2 x Hinges 2 x Brackets 1 x Sticky-back velcro strap Then you need a load of wire to connect the power up (our speakers came with audio wire). Here we also opted for thicker wire than was probably necessary but it allowed greater flexibility to pull and bend. The downside was when we wanted to put two wires onto one switch, the twisted pair was very thick and so it was hard to thread through other clamps. Gather the tools We used so many tools! This was not predicted before the build was started. I would recommend to make sure you have a full compliment or know someone who has such things as, wire strippers, pliers, tweezer pliers, bremmle drill, file, hand drill, wire tape and wire connectors. We also used a laser cutter at DoES Liverpool hacker space to cut out the acrylic sheet, engraving the Novoda logo and the bugdroids you can see on the finished product. Watch out when creating CAD (computer aided design) designs they may look wonderful on your own computer but when transferred to the actual laser cutter it’s a whole different ballgame. This is because CAD files uses weigh-points to distinguish where to cut and each version of the software (including across mac / windows) have a different perception of how weigh-points work. Many hours spent back and forth here. Understand the wiring Side note: The handbrake (parking brake for you Americans) on a car determines whether you can type text and do other functions on a car headunit. When the handbrake is on, the head unit assumes the car is stationary and so allows keyboard input. Since we are not a car we will be attaching the handbrake to a switch. Electronics are not our forte I must say. However let me try and explain. Car batteries have ground ( - ) and live ( + ), connecting a device with these gives you power. Switches with built in LED’s need this circuit also. The cigarette power converter has two wires ground and live. The power of the Pioneer head unit has power and ground. The handbrake only needs ground. Therefore to turn our demo unit on and off, one of our switches has ground from the power supply and ground from the head unit into the ground terminal. It then has live from the power supply into its power terminal and live from the headunit into the accessory (acc) terminal. This allows the light to come on on the switch as we turn the head unit on. For the handbrake to complete that circuit you have to connect the power supply ground to the green handbrake cable. Unfortunately we couldn’t work out how to get the light to come on when the handbrake was on without shorting out the whole box, so there is no light on the handbrake. Audio wiring is simpler: it’s plus to plus, minus to minus. Make sure you keep as much audio cable bundled up in a cable tie so as to later avoid cable entanglement hell. Measure it all “Measure twice cut once” is the mantra. I recommend measuring four or five times! Here we first measure the size of the box so we could order the correct size acrylic perspex for the top. Pro tip: if you’re doing this on a CAD machine, allow extra space on all sides for a margin of error because it’s actually really hard to know exactly where the laser cutter is pointing. Our speakers were four inches in diameter so we traced two circles for them either side of the head unit. This was ordered as a custom piece of perspex from Sign Materials Direct. Measure your headunit—note that these come in a caddy designed for you to easily fit it into your car, this can be removed before you measure. Another thing to measure is the amount of cable you will need, we added extra slack as we were not sure whereabouts in the box the switches and battery would sit compared to each other. Something we didn’t do but should have: measure where the hinges, headunit braces and speakers are going to go and add these to your CAD designs. This stops you going back later on and having to drill the plastic (each time hoping you don’t crack it). Connect the bits Connecting it all together allowed us to test the full system before we put it into the box. Connecting the switches allowed us to test the handbrake was working. If you go onto Google maps when the car is in motion you will not be able to search via the keyboard. Design fascia We added some flair to the fascia by engraving Android logos and also positioning the switches in between the “O”s of our company logo. Install into fascia Using nuts and bolts we attached the speakers to the perspex, then the head unit with brackets, nuts and bolts to the perspex. The switches already had a screw mechanism that fit nicely through our holes. Thing to note here: the Pioneer head unit comes with its own plastic surround. You can use this to hide any ‘mishaps’ from your cutting. One thing to keep in mind is that it has little nodules on it that you have to account for when cutting your rectangle (or you’ll need to file them out afterwards like we did—oops). Install into box Adding the hinges is what you need to do here. Pro tip: remove the speakers and pioneer head unit from the perspex before you line up the hinges and attach them. This allows you make sure you have an flat surface to get the contact correct. We also had some sticky back velcro so that we could attach the power supply to the bottom of the box. Don’t forget to drill a hole out of the side so you can hide the power lead when displaying your amazing Android Auto demo unit.", "date": "2015-06-29"},
{"website": "Novoda", "title": "Growing Android Applications Guided by Tests. Part III: Mocking the API Service", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/growing-android-applications-guided-by-tests-part-iii-mocking-the-api-service/", "abstract": "This is the third part of our series Growing Android Applications Guided by Tests. We’ve already covered The Setup in part I, and we’ve built A Walking Skeleton in part II. Now we’re here to mock the API service. To parse a search repository response into Java models ( see code here ) we decided to use Jackson as a Json parsing library for its performance, flexibility and annotation-based configuration. To ensure maximum flexibility, we hide the ObjectMapper object with our own implementation, JsonConverter. We use custom converters for every entity, so that the object creation must pass through the constructor. This way our models remain immutable and we can enforce class invariants such as \"startDate < endDate\". This also keeps Jackson as an implementation detail and we're free to swap for another library (like Gson) without touching our entities. We make sure that entities that are Collections are treated as first class objects, because they represent a concept in our domain and may contain specific business logic. Right now our failing test is asking us to present repositories on our UI. Funny thing is: we don’t even have a Repository object, so we should start writing some entity models. What should a Repository object look like? Well, since we are only displaying its description in our UI, it is fine to only contain a description field for now. But what about the concept “ A list of repositories ”? Should we represent it with a List<Repository> type? We strongly disagree. In fact, our idea is that Collections should be first class citizens. This is one of the Object Calisthenics Rules that we try to follow when we’re designing code, and we follow it because of several advantages. First, an object called Repositories may contain business logic relative to its element that a List<Repository> cannot. For example, it can have a method areFromSameAuthor or even a groupByAuthor that splits this list into various sublists. If you're passing an encapsulated Repositories object around your application you're hiding its status inside the class, preventing details from leaking all over the architecture. Finally, Repositories is much easier to read and write than List<Repository> . Until we associate behaviour with these classes it is pointless to write entities test-first as they are just a bag of objects but we can make a start by writing tests for our Json conversion tools. We can define our own JsonConverter class which abstracts from  the real library that we will use for the conversion (either Gson or Jackson since these are the two competing standards). We exercise this converter with some sample responses to check that the mapping works smoothly. If you haven't already, take a second to look at the source code in the entity package. Notice that this commit contains a fully functional conversion framework, but do you see any trace of this Json parsing mechanism in those classes? That's exactly what we were aiming for: all the conversion details are hidden in a separate package, and none of these details leaks into our entity package. Many developers when using Jackson or Gson are tempted either to overload their classes with annotations or to use specific names for fields and accessor methods. We're working hard to resist this temptation because it will create a tight coupling between the conversion tool and our model objects. If you are a seasoned developer you will know that the more you loosely couple your modules, the more flexiblility you will retain within your application and the easier it will be to embrace change. This is particularly critical when you're dealing with third party services and APIs. It is likely that the Json returned from the service will not be well suited for your application and you need a place to add customisations and convert the response into a meaningful object for your domain. Even if you completely trust the external service, it is always a good idea to add a separate layer whose responsibility is to take care of the conversion. The received Json may contain badly named variables, inconsistencies and other funny oddities. If you have found yourself casting Strings to booleans or Dates inside your business logic, you know what I mean. Dealing with these problems inside your models will only add noise and distract you each time you open these classes within your IDE. Since  these models are going to be integral to your development and tesing, it is better to keep them in lean and be very strict in applying the Single Responsibility Principle . Remember: as a developer you will spend 5 more times reading code than actually writing it . If you apply the conversion via annotations or accessors you will be giving up one of the most powerful tools at your disposal: enforcing constraints. Since you're not in control of what you are receiving, you should mandate a centralized point from where you can sanitise the input and apply some logic in order to fail fast if necessary. A Constructor is the ideal place where to apply any logic, and you can't use it if you create the object via reflection. If your Invoice object has an invalid date, you may not going to find this problem until it's too late to recover, or to throw a meaningful error. As you can see from the source code, if you are using Jackson 's custom converters and builders, you can externalise the parsing framework with little overhead, keeping your application clean, your model objects small and enforcing their creation through the constructor. If that isn't cool enough, you can bask in the satisfying glow of knowing that at any moment you could swap to another parsing library such as Gson . All you would need to touch was one small package, leaving the rest of your application unaltered. That's ultimate flexibility! Write a stub for the Github API See code here We’re writing an implementation for the GithubApi interface that will provide predictable responses for every query. That way we can run the entire application without having to rely on the Github servers, speeding up both testing and development. Also, those mocked responses allow us to eliminate some duplication in the test classes. When I started this series of posts, I stressed the importance of having stubbed implementations for each third party service you are using within your application. In this commit you can see how easy it is to now create stub implementations with a conversion framework in place. Your stub service will always return the same Json response for a certain subset of requests and predictable responses while we're writing test fixtures. As a rule of thumb, it is sufficient to provide at least two different responses for a certain API request: one coming from a local file, and the other coming from in-memory Java objects. The reason is that you want some in-memory objects so that you can reference them in your tests (look at the MockSearchRepositories usages, for example), while the file-cached response simulatates the normal behaviour of the service. Two responses are really the minimum viable implementation but you can have as many local Json files as you like.", "date": "2013-06-21"},
{"website": "Novoda", "title": "Improve Your Cast", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/-improve-your-cast/", "abstract": "Google Cast is a technology that allows Android, iOS apps and Chrome web apps to cast video, audio, and screen sharing to Cast-ready devices, like the Chromecast or the ones running Android TV. An obvious use case for such a technology is to allow the user to cast films from a streaming service, watching them directly on the big screen of a TV instead of the small one of a phone. Here at Novoda we’ve been working on the MUBI Android app for quite some time, and the last weeks have been focused especially about improving the Google Cast experience. This culminated in the new update we published last week to Google Play and we are really satisfied about the end result. We greatly improved the whole Cast experience, a really important feature for an app like MUBI, being it so focused on providing great films to the user. In this post I want to describe some of the enhancements we applied and the reasons behind the decisions we made. Cast button availability The user will have the first impact with Cast functions in your app through the cast button. This button usually sits in the Action Bar of your app and has the double function to notify the use about the presence of Cast receivers and to invoke a menu to connect, control and disconnect from Cast receivers. The cast button displayed on the app Action Bar First of all, the user must be aware of the Cast receivers availability. In order to do so, the Cast button status is updated accordingly: Unavailable: while Cast receivers are not available, the cast button is not shown Disconnected: while Cast receivers are available, the cast button is shown Connecting: while the app is connecting to the Cast receiver the cast button animates the waves in the icon progressively Connected: while the Cast receiver is connected, the cast button appears with a filled frame shape Different cast button statuses: unavailable, disconnected, connecting, connected Connection As soon as the user selects a Cast device and the app connects to it, the MUBI logo is displayed on the receiver, as a confirmation that the the app is correctly connected. After that a slideshow of beautiful film posters is presented, while the receiver is in standby waiting for the user to select the film to cast. Every poster contains the MUBI logo, so that the user is always aware of which app is casting at the moment and the posters change every 15 seconds, in order to prevent any damage of the TV panel due to burn-in. During a connection, nice posters are more interesting than a black screen Start playing When the user selects the film to watch, the slideshow stops and the loading screen is displayed. This contains an indeterminate loading bar, along with some information about the current film, such as the poster, title and country. Loading feedback plus a poster tell the user a film is loading Pause As soon as the user pauses playback the app provides the expected feedback, showing the current (paused) status and time of play. The same information shown during buffering is displayed for a limited amount of time, before disappearing while still on pause. This allow the user to see as much content as possible, while keeping the essential UI elements visible. Pausing a film should also show feedback Seek From the mobile app, the user is able to seek to an arbitrary position of the film. In that case the change of position is displayed through Google Cast showing the updated seek progress bar with the new playing position. Seeking shows the seek state on screen Ad-hoc controls As already mentioned, the user can control casting from the mobile app by pausing and resuming the playback and seeking to different position in the film while playing. Furthermore we introduced a couple of quick actions directly available from the Cast notification or the lock screen, in order to cover another use case likely to happen while watching a film: external distractions. Thanks to these new quick actions, it is possible to pause the playback, rewind the last 30 seconds of the film or seek forward by 30 seconds without even unlock the phone. Quick access to controls directly from the lock screen As we saw, the main important thing is to provide the right feedbacks to the user, during the whole cast experience. For more information about what to consider in order to provide the best Google Cast integration in your app, have a look at the official Google Cast Design Checklist .", "date": "2015-03-20"},
{"website": "Novoda", "title": "#myhomescreen, Pt. I", "author": ["Ryan Bateman (Head of Product)"], "link": "https://blog.novoda.com/myhomescreen-pt-i/", "abstract": "myhomescreen is a series of blog posts that shows off some of Novoda’s homescreens, highlighting apps we use and love, games we like playing, and sometimes our own small projects. Ryan Bateman (Technical Product Owner): I like to keep my homescreen simple—no widgets and a simple, abstract wallpaper generated using a Muzei plugin I wrote called MuzeiColours. At the moment my gaming obsessions are Desert Golfing and Out There —a Sisyphean golfing experience and a sci-fi, roguelike, story-focused game respectively. They’re both more fun than that makes them sound. I've been testing out dice.fm recently too as a way to find some live music gigs in London without booking fees. It's great, though the app itself is occasionally a little sluggish. Dave Clements (Head of Design): I’m quite obsessive about space. I like my wallpapers to show as fully as possible (this one is of my wife on our wedding day). My usage of apps boils down to two different things: networking and research. So I actually very rarely have much more than the stock apps from Google Apps. I actually have an HTC One but have Paranoid Android running and have a couple of alterations to the theme, one being the Lollipop system-bar icons and one being the latest Roboto Light for my font. Volker Leck (Senior Software Craftsman): I have a folder for all the social apps I use, and one for all the Google services. Also, I keep shortcuts for Tasks and Gesture Search . I’m currently trying to pick up some French, so the Duolingo app is there too. As a wallpaper I use my own wallpaper, SliderLiveWallpaper .", "date": "2014-10-23"},
{"website": "Novoda", "title": "Wormholes and Chromebox for Meetings", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/wormholes-and-chromebox-for-meetings/", "abstract": "Frequent and open communication is core to Novoda’s culture. Hipchat is great for the majority of day to day chatter but you can’t beat good old face-to-face speaking. This doesn’t mean endless meetings—in fact, our process recognises only three important meetings: daily standups, fortnightly sprint planning, and retrospectives. Otherwise meetings are kept to a minimum. Until recently, Novodans were connected through a mish-mash of iMacs, 1st-gen Chromeboxes, random old machines and Android boards. Cheap old iMacs are a good way to start dedicated meetings points but the hardware was getting old. Novodans at both ends of Hangouts occasionally looked like 8-bit characters and the conversation was regularly unintelligible. Having grown accustomed to free telepresence solutions such as Skype and Google Hangouts, we grudgingly admitted a requirement for a paid solutions. Professional networking solutions such as HDFaces , Vu , Vidyo start retail at around $2k per installation. With these services, what you are paying for is dedicated audio/video de/compression hardware/software, along with the infrastructure to support some sort of SLA. Everyone in Novoda speaks to one another and clients all day, every day, and so the cracks were beginning to show with free, non-SLA services. You want as little friction as possible in day-to-day operations and so it became a business priority to start investing in our telepresence. Chromebox for meetings boxes were a welcome upgrade. Wormholes “Out of sight, out of mind” is the biggest problem of any distributed team. You might not work everyday with the guy a few desks over, but every now and then you’ll bump into him in the kitchen or in the lounge. Developers and designers within Novoda are all working in-house but the challenge is that they are spread through offices in London, Berlin, NYC, and Liverpool and across multiple teams. Permanent “wormholes” connect common places in each Novoda office. Wormholes are windows across a world of similar work environments. To begin with, each Novoda office has at least one screen permanently dedicated as a wormhole within the common desk space. This is not intended to create a Big Brotheresque working environment any more than your desk being physically next to someone else’s desk. Seeing your colleagues and being able to call over to them allows for a special serendipity that is hard to otherwise emulate without both high-fidelity audio and visuals. In future we’d like to have wormholes for water coolers, kitchens, and and lounges across the world. Chromebox for meetings Each Chromebox shuttle is bundled with a camera, microphone/speaker combo, and a remote control. The remote control is the nicest I have seen and is easily the most comfortable way to interact with big television screens with no work surface. The remote offers a qwerty keyboard and quick-mute button access without the tiresome straggling wires of a mouse and keyboard. Daily use of Google Hangouts in any Google Business domain is summarised by these three URLs: Start / attend a meeting: g.co/hangouts Share screen at a meeting: g.co/present Schedule meetings: https://www.google.com/calendar/ All meetings are scheduled on company calendars and, as each Chromebox For Business is in a different room, you can add one of the boxes to your meeting to reserve the space of that given room. Each box has an ID and each hangout/meeting has a human readable name. Anyone within the business can type a Hangout name into the portal at ‘g.co/hangouts’ to attend. In our experience, the biggest hurdle to using Chromebox for business has been the education of the three URLs above and the idea that you have to add the Hangout/Chromebox to your meeting if you want to book the room. I recommend sticking notes with the URLs to all the TVs and devices. We went with the naming convention of LDN/BLN/NYC and then the name of the room (for example, ‘LDN-Large-Playroom’). Overall, if your team is of 20+ people and has three rooms or more to juggle, I’d recommend sticking a few Google Chromebox For Meetings in them. Onwards reading might be a little boring if you aren’t currently using the service. Rather than just singing its praises below, I’ll share some of the services current problems which we hope will soon be addressed. Chromebox for meetings issues (28/09/14) Licence per box? Hangout Invitations by nobody? Timezone issues / US Address needed Adhoc meetings Audio / feedback problems Refused invitations Initial setup and Login confusion, who should login to a chrome box for meetings? Wormhole calendar invites Hangouts cutting out Can’t join a meeting in progress outside allotted time? Meeting name privacy Attendees outside a domain Hangout box auto-completion Google Wallpapers & Screensaver Patchy images while presenting Licence per box? Each user upon a Google for business domain is charged for their access to the network and services of Google, which seems fair. It is however unclear why after a business buys a Chromebox for meeting then has to pay an additional annual fee for each box. It seems that business users are already paying for each employee, I’m not sure why Google would think it fair to charge for those employees to speak with one another through previously paid for hardware. Hangout Invitations by nobody? Should someone invite others to a chrome box call in progress the email says you are “invited by nobody”. This is confusing and impersonal. Timezone issues / US Address needed We needed a US address to register our Chrome boxes, but this isn’t the issue. Our first three chrome boxes were successfully set up in London, but when we tried to set up the boxes in Berlin we received the error: “Invalid timestamp”. Choosing the timezone as “British summer time (BST) during setup allowed for us also to run 2 chrome boxes successfully in Berlin. Ad-hoc meetings and general confusion All meetings are expected as calendar entries. Everything should be scheduled within a calendar. If anyone within the google apps domain of novoda.com is invited to a meeting they can add the chrome box to their meeting and benefit from the convenience of a mic/speaker, big screen combo. However, sometimes clients or colleagues just call via Google Hangouts. If no meeting invite exists, there is no way to give an in progress meeting a ‘name’ or any way by which during an ad-hoc meeting they can add the chrome box in the room within which they are sitting. As an attendee you can send invites from the meeting but otherwise others cannot attend. This needs to be addressed and in general is our biggest criticism. Audio / feedback problems The Mic and Speaker bundled within one hardware component is a great innovation and handles the majority of feedback problems. Feedback problems happen when there is a loop of audio from a speaker being fed into the mic producing the audio. Having both the Mic and speaker in one will be able to handle these issues in one place. The Mic is meant for 4 people around a conference table not 10 people in a large room. Occasionally we have switched to a blue snowball for clearer audio in a bigger space with more people (stand ups) but sometimes everyone complains that this sounds quieter and more echo’s. The bundled speaker is really very good in all 1-4 meetings where generally one person is speaking but if there are multiple people in a room the blue snowball is generally better but the jury is still out on a large group solution. We have it on good advice we have to put the speaker as far away as possible from this microphone to minimise feedback. Refused Meeting invitations We are unsure as to why a hangout box could refuse someone an invitation but occasionally people complained that their meeting invites had been declined when adding a room to the calendar. Initial setup login confusion, who should login to a chrome box for meetings? Each hangout box has been allocated a room. Upon setting up a chrome box we were asked for a login. We were unsure as to who should login to the box and so we have used one shared account through out all the meeting boxes across the world. We are unsure if this is the intended setup. Wormhole calendar invites Hangouts are meant to start and end over a specific amount of time. They do not cater for perpetual hangouts like our “wormholes”, these sit as annoyingly refused invites, taking up space in my calendar as refused meetings otherwise I wouldn’t be able to schedule anything which conflicted with them as they are all day event. Hangouts cutting out Wormholes are supposed to remain open all day long as a constant window into another workplace and allow for workmates to dynamically chat with one another. For some reason the meetings keep cutting out after a time. Perhaps they think that no one is around. Can’t join a meeting in progress outside allotted time? Meetings run over, all the time. Perhaps it could be seen as a feature that once a Google Hangout in progress runs over it’s allotted calendar time you can no longer join it’s meeting. Unfortunately such a heavy handed approach will likely cost some companies reputation as another paying client can’t join your meeting and you have to insinuate they aren’t important enough to interfere with your timekeeping. Meeting name privacy Chromeboxes display all of their daily meetings upon the screen with a nice backdrop behind them, this is not appropriate for sensitive meeting titles. Some meeting titles reveal clients and projects and so should not be revealed openly within the list of meetings. Attendees outside a domain Only those within the Novoda domain can invite people to hangouts this took a while to be understood. If someone is attending from outside the company this should be signposted more strongly upon the screen. Hovering over the contact reveals their textual representation. We suggest something stronger is required, a different border perhaps red around all attendees who are outside the App domain. Hangout invite auto-completion When adding additional members to a call in progress it should be expected that additional rooms which have chrome boxes and domain users will be regular invites. A simple list of all the chrome boxes available within a domain would be helpful, clicking a chrome box would add it to a call. Also inviting many team members to a call is a tiresome process via the invites during an in progress call, some sort of auto completion should exist which at least pre-empts @novoda.com addresses within the domain of Novoda. Google Wallpapers & Screensaver When no meeting is in progress, a Google Chromebox for meeting displays the same portal as attending g.co/hangouts where the backgrounds cycle through an attractive selection of wallpapers sourced by an editor at Google. All the wallpapers upon the screen are from Google plus images are very attractive but the company which has paid for both the hardware, the licence fee and all the users should surely be able to customise these images. Additionally the screensaver keeps kicking in and there are no timeout options available. The screen saver should be configurable as to whether it is ever required. Patchy images while presenting Visiting g.co/present from any machine allows a participant to stream their screen, which is neat. But streaming in our experience has always been very ropey. You couldn’t for instance share a video or benefit from animations in your slides. If you were relying upon the screen for cues you would quickly fall out of sync. Overall, again I think Chromebox for meetings are pretty good and I do hope Google will soon get around the fixing these issues. Here is a pic of Ryan presenting over a hangout one lunchtime. Note: I'm aware also of Highfive and would love to hear of others experiences: https://www.highfive.com", "date": "2014-09-28"},
{"website": "Novoda", "title": "CHI2017: A conference of Humans, Bots and Interactions", "author": ["Alex Styl"], "link": "https://blog.novoda.com/chi2017/", "abstract": "The Human Factors in Human Computer Interaction 2017 conference took place in Denver, Colorado. I was lucky enough to be participating in person this year, with my first ever publication in the field of cross-device interaction with SenseBelt . It was a conference full of inspiring talks, innovative pieces of work, brilliant people and interactive demos and surprises. CHI2017 took place in the Colorado Conference Center, a venue befitting such a grand event. The conference was four days long. Each day consisted of an inspiring keynote by recognised researchers from the field of human-computer interaction (HCI) and a wide range of talks on the field on a wide range of topics. Their were four keynote speakers, firstly Neri Oxman with her talk on material ecology and the combination of separate disciplines in order to create new forms of technology and solutions. Next was Ben Schneiderman with the story of how the CHI community came to be. Wael Ghonim questioned whether the social media platforms we use are in fact changing the way democracy works. Lastly, Nicholas Carr argued how automation and the ease we get out of technology is great, but might actually make us forget some important skills we currently have such as navigation. There was a plethora of incredible talks as well. I was primarily focused on talks on ubiquity and cross-device interactions, virtual and augmented reality and video conferencing. I always find it useful to attend talks outside of your core interests too. In his talk on bots and personas Bert Vandenberghe talked about creating chatbots from user research data , so that every team member can interact with and better understand user research data. It was interesting to see how much time is wasted every day and how this time can actually be used to learn new things , like languages. In terms of cross-device interactions, Improv stood out to me . It is a framework that allows users to create cross-device interactions on the spot from any device they want. SenseBelt in late-breaking work CHI also includes late-breaking work sessions, in which researchers could present the early stages of their work as posters. I was pleased that my work on SenseBelt attracted plenty of attention. It was great to talk to so many people and discuss SenseBelt and related ideas. Sketching user experiences The conference hosted a variety of great courses every single day as well. I participated in Professor  Nicolai Marquardt 's course on sketching user experiences. The course did a great job of teaching you ways of communicating your ideas and scenarios through sketching and it was great fun too. Highly recommended if you ever get the chance to do it. Telepresence bots One thing that couldn't go unnoticed was the amount of telepresence bots around the conference. Not only were there several talks about them, but remote participants could also join the conference remotely by using these bots. Remote speaking was a thing too with Evan Golub 's talk on Life as a Robot (at CHI) , covering the problems that telepresense bot users faced in CHI2016: One telepresence robot presenting to another, the humans mere observers resigned to their fate #chi2017 pic.twitter.com/GKTmN5mIwE — Dan Lockton (@danlockton) May 9, 2017 Last but not least, one of the newest additions to the conference was CHI Stories. In this session, members of the CHI community talked through their life stories and experiences behind their work and struggles. It's easy to overlook the hard work, stress and sleepless nights that go into a piece of work, so it's always good to hear stories that remind you that great achievements do not happen overnight. You can find all the inspiring stories on the CHI Stories playlist on YouTube . CHI Stories features the personal stories of people in the field Conclusion All in all, CHI2017 was a great conference. I left Denver full of new research questions and realisations, feeling excited about what is yet to come.", "date": "2017-07-06"},
{"website": "Novoda", "title": "Mobiconf 2017 in Cracow - Sum up", "author": ["Bart Ziemba"], "link": "https://blog.novoda.com/mobiconf-2017-cracow/", "abstract": "It's the first time I went to a mobile conference and after attending a great Mobiconf for sure it will not be the last! Keep reading for a summary of the most interesting talks and conclusions on the event. About Mobiconf 2017 Mobiconf is a 2 day conference for iOS and Android developers, UX/UI designers and Product Owners. In general, I would say it's for everyone (at any level) interested in mobile related topics. The 4th edition of Mobiconf took place on the 5-6th of October at Multikino in northern Cracow. There were 412 registered attendees from 20 countries attending 34 talks. Everyone had a chance to follow and participate in the presentations of three different paths of expertise - Everything Mobile, Android and iOS full agenda . I didn’t stick only to one of them as each had some really interesting presentations to see. Talks from Day 1 From all talks I have seen, here are ones that have been most interesting for me. I really liked the opening talk \"The Future in Mobile\" by Johan Adda ex-Apple UX Architect who has been presenting some tips for designing videos for social media in portrait mode. He also mentioned the topic of the security of mobile devices based on the recent face recognition feature for iPhone X. He claimed that in a near future we should start designing safeguards based on biometrics such as walking style or typing e.g. UnifyID . Another amazing talk \"Remote, lonely and productive\" by David González, former Novodian, was about traps that can meet remote developers that work alone on the project. He explained tools, practices and approach to build the applications as effective as he would work with the team. Some of the tips were: Run static analysis on your code. Follow Android or iOS project guidelines. Automate release process. Have a separate room that you enter only when you work. Or as simple as: do sports and meet with other people to not to feel isolated. The third talk I liked during the 1st day was by Dimitris Kontaris (HSBC) titled \"I’m sorry, I didn’t understand your question; The ambitious past, awkward present, and promising future of voice interfaces\". Dimitris was talking about voice driven user interfaces and their usage in our devices, homes, wearables and IoTs. Especially, I liked the example he presented how voice interfaces help people with disabilities presenting case of Todd Stabelfeldt . Apart from really interesting talks, after the first day of the event there was Mobiconf 2017 Integration Party at B4 Club. Talks from Day 2 First talk I liked the most during the second day of the event was about \"Building UI Consistent Android Apps\" by Nicola Corti (Yelp). He presented the problem of how easy the consistency of the UI across the applications and platforms can be broken and provided with some tips to maintain it. The crucial things to consider are: To define style guide. He have an example of the style guide they created at Yelp. To follow design guidelines provided by Android and Apple . To create custom lint checks To create tests that generate screenshots for the further verification. Second talk by Mateusz Biliński (Niebezpiecznik.pl) was about the recent vulnerabilities found on the iOS platform and showing their impact. He also gave some ready solutions that everyone could take home and apply in real project such as upgrading AFNetworking library, that provides convenience functionality on top of Apple's built-in frameworks, to upgrade to the newer version than 2.4.1 as lower ones include serious security gap. The venue and the whole organisation was really cool 👏. Fresh coffee, squeezed juices and canapés welcomed attendees throughout the whole conference. For retro games and computers fans there were also some attractions prepared. I think some tips for next year would be that the conference could take place in a bit more central location. I would like to see a more diverse set of speakers next year, to get a more rounded view of the industry. Other than that I was really enjoying it! Mobiconf 2017 highlights See you next year at Mobiconf!", "date": "2017-10-31"},
{"website": "Novoda", "title": "Google Playtime 2017", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/google-playtime-2017/", "abstract": "Google Playtime is an event organised for Google Play apps and games partners. With inspiring talks, group discussions and pre-booked one-on-ones with members of the Google team, it's an opportunity to learn about the latest Google product updates, experience new technologies and of course, play in the ball pit during the breaks. The day, hosted at Kraftwerk (a quirky, warehouse event space) started with a waited breakfast in the registration queue and continued to a smooth registration process where we were treated to the space and activities of the Google Playground. The hospitality continued inside with much-needed coffee and snacks, a great space for some early-morning product chats before getting seated for the opening film and keynote. The opening keynote The opening film ‘Playtime 2017: Changing lives with Android and Google Play‘ featured the stories of inspirational Google developers creating meaningful, life-changing services and applications across health, communication & education with the Google Play Store. The keynote covered the latest advances in the Play Store across Games, Play Console, Android Vitals, The Google Play Security Award, beta testing and machine learning, with one clear message for product management: Understand and act at every step Kobi Glick, Product Manager for Google Play Google Play Store A ‘Don't forget to try' section promotes recently-installed apps The Android Excellence Programme showcases apps and games that deliver quality user experiences on Android Google Play Security offers reviews, feedback & rewards Games The second edition of the Indie Games Contest has been launched You can now offer video examples for recommended games New sections have been created for premium (paid for) and sale games Android Instant Apps available for game sampling Play Console & Android Vitals Android Vitals available for tracking app quality & performance Play Console available for release management, analytics & pre-launch reporting New: Country-targeted rollouts now available Following a great kick-off, we stopped off at the ball pit in the Playground for some silliness. The Talks The day was packed with a great selection of talks across three tracks: Apps, Games & Innovation. It was tough to choose which sessions to attend and even more tricky to pick favourites but I've given it a go! Talk #1: Quality over quantity: Why quality matters Alexey Kokin (Partner Development Manager, Google Play) and Riccardo Govoni (Engineering Manager, Google Play) spoke through ways to improve your user experience, engagement and retention using Android Vitals, Material Design and other app quality best practices. Android Vitals is an initiative by Google to improve the stability and performance of Android devices. The Play Console helps developers to log metrics around stability, render time, and battery usage. The Android Vitals dashboard helps developers to better-understand the performance of their app and alerts them when their app is exhibiting bad behaviour. The initiative also rewards developers for product excellence. Performance Get featured in New, Updated & Editor’s Choice collections for high-performant apps Technical excellence Get rewarded for well-designed, apps of exemplary quality Stability Get recognised for low ANR & crash-rates Talk #2: Of flying lamborghinis and gummy bear robots Franz Blach (Design Director, IDEO) and Fabian Herrmann (Senior Design Lead, IDEO) explained their research approach when exploring opportunities in emerging technologies, specifically Virtual Reality, Augmented Reality, Digital Assistants, and Ephemeral Apps. We want to shift design from what technologies are capable of to what we are enabled to do as humans Alexey Kokin and Riccardo Govoni, IDEO They explained how ‘tapping into user’s imaginations’ through specially designed exercises could help to inspire new product ideas that deliver real value. The IDEO card pack 'Human-Centred Design Prompts for Emerging Technologies' suggest possible design approaches to problem-solving in new technologies with a clear reminder: The promise of all new technology has always been to expand our abilities as humans Alexey Kokin and Riccardo Govoni, IDEO Talk #3: Impactful research methods to supercharge your app or game Ross McLachlan - UX Researcher, Google Play talked through the latest research methods being used in the Google Play team to increase engagement and customer satisfaction through 3 key product questions: Strategy What shall we build? Design How shall we build it? Evaluate Did we succeed? Top tips: Listen to your users, ask open questions and discover things you don’t already know Collaborate in multidisciplinary teams to increase empathy and understanding across the team Deep-dive into key feedback areas for behavioural analysis and technical app health The event finished up with a networking happy hour with cocktails and wine to accompany the product discussions. All in all, an incredibly well-organised event with quality content and a talented group of app creators! Thanks to all at Google for a great day!", "date": "2017-10-27"},
{"website": "Novoda", "title": "Make it better, for Everyone", "author": ["Alex Styl"], "link": "https://blog.novoda.com/muxl2017-inclusive-design/", "abstract": "Not so long ago, computing was a luxury for the selected few. Just a small number of people had access to a computer and less so, to the internet. This changed dramatically after the release of the smartphone. Computing became more personal and people started using technology more than ever. Today technology is all around us. Digital boards and public displays on the streets, self-checkouts in stores, computers in our workspaces, smart TVs and devices in our homes. This trend towards ubiquitous technology is quickly becoming the norm. Technology is becoming an established part of our everyday lives. With Internet of Things platforms such as Android Things, people are able to build their very own internet-powered device. As we are designing in this world of devices and ubiquitous systems, we must take care to ensure everyone feels welcome in it. With different form factors and systems available, we need to make sure that we design our products in a way that everyone can use them, no matter of their background or technological abilities. It is not just technological or environmental changes that we need to consider. People go through all sorts of changes all the time. Some through natural processes, such as pregnancy, growing older, loss of hearing or eyesight, but others may be due to an accident, such as a broken hand or temporarily losing your voice due to sickness. Products should be flexible to accommodate these situations, to such situations and provide as many different ways to using them as possible, so that everyone can enjoy them equally. Inclusive design is not a specific process or a checklist that you need to complete in order to make your product more widely accessible. Inclusive design is the design philosophy that allows you to design products that fit as many user needs as humanly possible. Understanding the problems that people might be facing allows you to think outside of the box and deliver better solutions for everyone. Take for example a TV show. As TV contents tends to focus on audio and visual, this means that not everyone can enjoy using the medium as it is. Closed captions were build so that people with hearing problems were able to enjoy video content despite their difficulty hearing. Now think about who else gets a better experience through closed captions. It is not just people with hearing disabilities, but anyone in a very crowded or noisy place. TVs in places such as airports, pubs or gyms often have closed caption enabled so that people do not have to rely on everyone in the room to be quiet or the TV to be super loud for them to enjoy the show. Some shows can benefit greatly from transcribing which is used in closed captions. A transcribed version of a show makes the content more flexible in terms of how it can be used. Having it as text allows you to read it on your own pace and time. You can upload it to your eReader so that you can read it anywhere you are. The same content can now be enjoyed by more people in their own time using the medium they prefer. Inclusive design is opportunity to reach more people across situations, broadening possibilities and outreach. If this is something that interests you, join our Inclusive Design in Action workshop in MUXL London 2017 to learn more about how to design products for as many people as possible and make them better for everyone. Preview Photo by rawpixel on Unsplash. This post was also published on the MUXL2017 blog post .", "date": "2017-10-25"},
{"website": "Novoda", "title": "Software Craftsmanship Conference - London 2017", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/software-craftsmanship-conference-london-2017/", "abstract": "SCLondon is for people with the mindset of a Software Craftsperson and for whom delivering software on time simply isn’t enough. Andrei and Luis had the opportunity to attend this year’s SCLondon conference and below are their thoughts on it and details on some of their favourite talks. Software Craftsmanship Conference London was a great event which gathered software craftspeople from all around the world. We were looking forward to it since it has been announced as here at Novoda we identify ourselves with the software craftsperson mindset. Here are the talks that caught our eye: Software Craftsmanship - 8 years by Sandro Mancuso The conference kicked off with Sandro delving into the history of Software Craft and the evolution of the community around the world both with its ups and downs. A few takeaways: The software craftsmanship movement is a reaction to what happened to agile Up until 2016 the focus was mainly technical (extreme programming, cross-pollination, deliberate practice, micro design / clean code) Software craftsmanship is becoming mainstream, but we are still inside our own bubble, a tiny bubble If you want to be treated as a professional, start behaving like one: be in control of your career Practices are means to an end; we should be attached to the value they provide, not to the practices themselves Software craftsmanship is not XP The future of Software craftsmanship will be driven by investing into innovation, architecture, learning how to drive technical changes, collaboration and sharing Software Craftsmanship is about sharing our knowledge, preparing the next generation, and moving our industry forward The Novoda Craft University by Paul Blundell Our Head of Engineering - Paul - walked us through the challenges and rewards of the journey of a craftsperson. At Novoda, we are encouraging and supporting this journey of learning, so we created the NCU (Novoda Craft University). This was the first time it has been presented to a wider audience. One thing that isn’t in the definition of craft is pragmatism. We value crafting, but we also value pragmatism The NCU is our way to introduce our new hires to Software Crafting practices. It walks the student through 5 modules which involve platform specific learnings, language best practices, clean code, testing and refactoring, agility,processes, and professionalism. You can read more on the NCU and how it developed here Get Kata by Kevlin Henney In his usual manner, Kevlin has delighted us with his talk on katas, their origin, their meaning and why they are important to practice. The distinction from daily work is you do deliberate practice to master the task not to complete the task. Often times we only concentrate on getting things done and that’s it, but, similarly to other fields like music or sports, coding takes practice on your journey to mastery. This is where katas come into picture. Next, Kevlin took us on a journey on exploring different ways of coding the popular FizzBuzz problem, by thinking outside of the box. Craft Thinking by Mashooq Badar In this talk, Mashooq took a philosophical look of what craftsmanship is and what does it means to us. He explained that software craftsmanship is not a metaphor, is a direct description of the work we do. Craftsmanship is directly applicable to our profession, because software is a craft Craftsmanship is a Human impulse. We all want to do a good job and we all take pleasure and satisfaction out of it. But motivation matters more than talent. With talent but without the right kind of motivation you can get obsessed about the practice itself but not the value you are bringing out of that practice. Main takeaways: Experimentation is key. We learn from observation and experimentation, but often the first time we are learning something is when we write the production code. We need to understand there’s a balance between perfection and pragmatism. Only the organised obsession is a good thing Pragmatism is not just compromise. Pragmatism is a force that helps you create the best possible thing, leveraging the strengths and weakness of the material you need to work with When there’s an unnatural gap between the doing and seeing the result your mind is no longer into the problem. This feedback loop of doing and feeling is important and we need to make it as short as possible. Are we using a sledgehammer to crack a nut? Even before thinking about what problem we want to solve we are already thinking about what framework, libraries or languages we are going to use to solve it. Generic tools let you do many things, but they require lot of skill. Specific tools do just one job very well and they require very little skill to use them. But it is not about one vs the other. It’s about understanding when to use which and mastering both The importance of syntax in Clean Code by Hadi Hariri Hadi Hariri is a developer advocate at Jetbrains and one of the most well known faces behind Kotlin. In his talk, Hadi started presenting different definitions of what Clean Code means to come to the conclusion that what Clean Code is varies from person to person. It’s a subjective term. What everyone seems to agree on is in what is Messy Code. Noisy and distracting code is generally accepted as bad code. Kotlin focus on the developer’s intent. It’s a language that helps programmers express themselves better through code and, hence, removes noise. Syntax contributes to clutter and noise and, if Clean Code is about comprehension, then it matters Closing Keynote by Rachel Davies In the closing Keynote Rachel talked about introducing new learnings into our organisations. Learning makes us happy and software development is a continuous stream of problems to solve, but introducing changes to an organisation is not easy. Change happens one person at a time and very slowly The number #1 tip for introducing any change in any organisation: don’t push it too hard Go slowly but keep going. You’ll need to invest time in your own learning but also in helping other people learn. Make your learning process open and welcome people into it by creating a space for learning together in your work week. Try to find opportunities where people can step back from work and try things out. Finally, Rachel gave some practical tips to introduce new learnings from her experience at Unruly: Do pair/mob programming to learn new things Introduce Dojos where navigator-driver rotate every 5-7 minutes. At Novoda, we use our own dojos repository where people can add or practice existing katas Do lightning talks on the new things you learnt about Try to find learning time during work time Sharing learnings at the end of the week (chat about it or present something if you like) Swapping teams: every 2-3 months ask if there’s someone who want to swap Swapping organisations: developer exchanges for a week. Needs some official paperwork. Must be with company with similar practises but not in the same business. Take turns to organise stuff Track learning (a chart or matrix where you record how you are learning: pairing, taking 20% off time for learning, etc..) Celebrate learning Be the change you wish to see in the world. Wrapping up All in all, Software Craftsmanship London 2017 was a great conference, packed with a lot of insightful content and top of the industry speakers. No matter the experience and skill set, being either a novice or a master, there is always something to learn from the Software Craftsmanship community. If you would like to find out more about the software crafter mindset, we highly recommend The Software Craftsman: Professionalism, Pragmatism, Pride by Sandro Mancuso. It is a great and easy read, touching on both technical and non-technical points. Check out our new starter reading list here . We are looking forward to next year’s edition!", "date": "2017-10-23"},
{"website": "Novoda", "title": "The Novoda Craft University - Part 1: The Journey", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/the-novoda-craft-university-part-1-the-journey/", "abstract": "This blog post explains how a high level of craft is kept up and shared amongst our 40 plus engineers. Part one follows your journey of learning and how crafting should lead the way. We’re a digital product agency, and we try and deliver the best value for our partners through mobile apps and digital transformations. We’ve found through excellent communication, constant feedback loops and the art of crafting, we can deliver value over an over for partners. First, let’s take a step back. What is crafting? Crafter isn’t just a fancy title we throw around, it has a purpose and meaning. It allows us to explain in shorthand the care and dedication we put into our jobs. It’s an encompassing word, that we feel includes, crafting, adding value, encouraging community, and having productive partnerships The software craftsmanship manifesto starts off: As aspiring Software Craftspeople, we are raising the bar of professional software development by practising it and helping others learn the craft. The manifesto is a stake in the ground - something to believe in and something to work towards. Its aim is to raise the bar of professional development. Now let’s look at the word itself. Software The programs & other operating information used by a computer. craft (see below) man A human being of any gender; a person. ship Indicating the qualities belonging to a class of people. Craft Craft is where all the juicy stuff is. You have to remember crafting was around well before the term software craft was coined. An activity involving skill in making things by hand. Interesting that it says, by hand, technically we use our hands but is copy and paste still by hand? I don’t know. Skills involved in carrying out one's work. We need a bag of skills, always learning, collecting more skills. Made in a traditional or non-mechanized way. There is thought process, a bespoke-ness about each deliverable. The members of a skilled profession. This is about sitting together, learning together, sharing, being part of something (like reading this). The organisation of Freemasons. Sure everyone wants to be in a cult I guess.. :shrug: The biggest takeaway I want you to have here is crafting is about sitting together, setting standards together, going on a journey together . Challenges Craft, togetherness, learning skills, going on a journey is what encompasses crafting, and what we see as value. The journey of a craftsperson is difficult and never-ending, If you aren’t conscious of your learning it can feel like its desert in all directions. You aren’t sure where to turn. Your days can be full of hard work, sweat and tears. We’re constantly learning as individuals. We have to communicate with others developing alongside us. We have to inherit knowledge, from others, living and codified knowledge. And we have to communicate with non-technical people in a way they understand. Rewards On the other hand, it is also a journey of joy. Everyone loves seeing a suite of green tests. It’s a great feeling when all your tests are green. All that confidence. It’s quite exciting, learning, playing with something new, sharing and understanding. Your journey is about knowledge acquisition. Did you see how exhilarated Neo was after his learning of kung fu? Whilst we don’t learn quite that fast, you should still find it enjoyable and rewarding to learn. Your journey should be about professional growth. Me writing and sharing this blog post (and conference talk), this is growth for me, a step forward in my career and my crafting path. You should always be looking to grow. Asking what your next step is. These are the rewards & successes that come with our journey. We’re constantly getting more personal skills & capabilities. Every time we grow, things we did previously become easier and situations less stressful. You can have a better work-life balance - look at where you’ve come from and the ease at which you can do certain tasks now. You’re growing your bag of skills, and this gives you more opportunities and prospects. Learning This is your continuous journey and the learning that is involved is yours. Nobody comes to a job knowing everything, we each have different strengths and weaknesses, knowledge in some areas and not in other areas. Throughout your journey, you learn new things and adapt and change to what everyone else is learning around you. What you have to remember is everyone is the same. We have this psychological condition that makes us feel as though everyone else knows more than ourselves, but this isn’t the case. Everyone has different knowledge, in your career you learn new things and hear new ideas.  We want this journey for everyone at Novoda, we want to encourage the learning and have a path inside of the company. Pragmatism The one thing that isn’t in the definition of craft is pragmatism. We value crafting but we also value pragmatism. Pragmatism is about understanding the goal and getting there without delay. Whilst you believe in crafting you should also believe in pragmatism and shipping value. Pragmatism is a skill that you have to learn. Taking a step back and saying; what are we talking about, what is the aim, is what I am doing aligned with what I am aiming for? Compromise Discussion Decisive decision making Understanding the bigger picture Can all be seen as pragmatic behaviours. what is the value in that? Value is the most underrated word in a team environment. Walk around saying this to everyone and you will go far. Pragmatism can be seen as taking the shortest path to the value. Conclusion Everyone is learning and should want to be continuously learning throughout their career. Crafting is a path you can take to learn more about engineering and deliver better products. So how do you start learning to craft? The next post will look at how we have created a tailored solution to explore learning and give a path and direction to the topic of Crafting on Mobile. Read part two here: https://blog.novoda.com/ncu-part-2-the-teaching", "date": "2017-10-19"},
{"website": "Novoda", "title": "In plain words, Machine Learning", "author": ["Alex Styl"], "link": "https://blog.novoda.com/machine-learning/", "abstract": "Voice recognition, face recognition, text recognition are only some possibilities that are enabled through Machine Learning. It is a technology that can allow our devices to get a better understanding of our world, our actions and intentions of using them, allowing us that way to design and build better context-aware applications. Computers can do all sorts of incredible things. They are really fast at performing billions of calculations or go through huge amounts of data in mere seconds. One thing they are not really good at yet though is understanding the world we are living in. They are not that good at figuring out what is the occasion we are trying to take a picture of with our smartphones, why we are scrolling this infinite list of posts on some social media website, or what the text is we are typing on some text editor. Every picture we take, every video we watch, or song we listen to is just pure data and means nothing. They do not really help us achieve the goals we are trying to achieve. Instead they wait for us to explicitly tell them exactly what to do for a task to be complete. Machine Learning is the kind of technology that allows machines to make sense of all this data and extract useful information out of it. Machine Learning has been used in various different industries for various purposes. We see Machine Learning enabled applications in the fields of medicine , robotics , and even in various art projects . Smart cars use Machine Learning in order to maintain the car on the road, make it understand car signs, pedestrians and get an understanding of its surroundings . On the other hand, Machine Learning is more relevant than ever for the mobile industry, as you no longer need a PhD in order to get into it. Companies such as IBM, Microsoft and Google, have built their own Machine Learning solutions such as Watson , Machine Learning Studio and TensorFlow that make applying it to your own problems much more approachable. \"This is a picture of a dog\" The way Machine Learning works is similar to how toddlers learn their first language. After a lot of repetition and examples the toddler is able to understand and learn what words like \"Mom\" or \"Dad\" sound like and tell different objects such as a chair or a bus apart. Similarly, if you were to teach a computer how to understand any kind of image of a dog, it won't be able to understand it just by looking at a single image of a dog. Instead we would have to show it thousands and thousands different images of dogs for it to finally get it. Eventually, the computer will start picking up some patterns on all these pictures and have an idea of what a dog looks like. It might have a fluffy tail, short or long legs, or a black nose. The result of this would be that for any image of a dog that we show to the computer, it will be able to understand that the image contains a dog. The most important thing to note here is that we would not need to create any set of rules, or write any code, but only provide the data. The more rich and diverse the selection of data we provide, the more accurate the computer becomes at identifying dogs in pictures. 🐶 Learning from various sources Images are not the only source that a computer can learn from. Images, audio or videos might be the obvious ones to go for, but think about all the different types of information a computer can hold. Any data captured by any sensor such as the microphone, light sensors, accelerometer, gyroscope can be used as well. This can unlock a lot of interesting interactions without having to depend on touching a screen, but create new gestures and interactions instead (such as waving or hand gestures). Machine Learning in the wild A lot of applications and services already use Machine Learning to enhance their user experience. Google Photos allows you to search all your 'ice-cream' pictures, without you having to mark any picture as ice-cream. Gmail is 'smart' enough to differentiate spam emails from regular ones. Recommendation services such as Spotify's weekly or YouTube's recommendations are powered by machine learning in order to understand each user’s unique entertainment preferences and provide richer recommendations. The recent introduction of Apple's Animoji uses the technology too. The phone is able to understand facial expressions such as raising your eyebrow or smile and uses that information to animate the emoji character of your choice. Machine Learning could also be used to categorise some members of a population according to their preferences/interests. This is something that Snapchat is using in order to understand which of its users are most likely to interact with a specific type of ad Apple's  Animoji understands the user's facial expressions in order to animate the emoji character Tools you can use Even if your team does not specialise in dealing with data you can still use Machine Learning in your applications. Some standard use cases of Machine Learning are already being offered by various companies, one of them being Google's  Google Cloud Platform: What's in this picture? The Vision API can be used to understand what is visible in a picture. You can get labels of items within it, along with any text there might be in it, or recognise logos, celebrity faces or items from movies. This is ideal for categorising pictures together according to their content, getting information from receipts and credit cards or memos. \"OK, Google...\" The Speech API helps you convert speech (audio). Such an API can enable hands-free interactions much easier than before. Think about applications or use cases where the user might have their hands busy, such as cooking or driving. What's in this video? Similar to the Vision API, the Cloud Video Intelligence API can identify objects on each separate shot of a video. Understanding text You can find text anywhere you look around you. Articles on the web have text (proof: you are reading this), boards, magazines, signs and labels on the streets, receipts, credit cards, notes-to-self and so on. The Cloud Natural Language API helps you analyse all this text, and understand what this text talks about and get a sentiment of it. This is particular useful when you want to go through user reviews and quickly understand if they are positive or negative or moderate online conversations. What language is that? The Cloud Translation API can be used to translate dynamic text on the spot. Useful for cases where people from different backgrounds might want to interact, such as a social media platform, emails or chat applications. Enabling new interactions and applications Machine Learning is a unique enabler that can lead to better experiences and context-aware applications. Think about how you would assist the user if you knew what they were trying to accomplish via your app's features. Does the information the user is browsing give away any hint of their intention? Could you somehow provide alternative ways of managing content if you knew what it was about? Photo by Marion Michele on Unsplash Follow Alex on Twitter", "date": "2017-10-05"},
{"website": "Novoda", "title": "The Novoda Craft University - Part 3: The Learning", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/ncu-part-3-the-learning/", "abstract": "This blog post explains how a high level of craft is kept up and shared amongst our 40 plus engineers. Part three explains what we have learnt from the NCU and how you can improve inside your company. We’re a digital product agency, and we try and deliver the best value for our partners through mobile apps and digital transformations. We’ve found through excellent communication, constant feedback loops and the art of crafting, we can deliver value over an over for partners. If you haven't read part two yet, you can find it here . Historically we kept the NCU fluid, it was a spoken effort, loosely defined and verbally given. Some people really enjoyed it, the learning, the introduction to our processes. However, some other people fed back that they wanted more structure, more definition. They wanted a checklist to work from. They wanted to know the reasons behind each module. And they wanted to make sure they didn’t miss any of awesome potential learning to be had. So what did we do? We then created the booklet, gave it more structure, a path to follow. Inside the booklet, we have introductions to each module. We have the reading list. And we have checkboxes of each stage of the work, so you can keep track. This went on for a while and then people fed back. People fed back that it was too rigid, to tightly coupled, to stringent. Read this exact book. Do this exact point. But it was never meant to be this way, it’s a guide to learning, but the fact we made it real, a physical thing, people felt they couldn’t do anything other than what it explicitly said. NCU Process Conclusion At the end of the day throughout this journey. The NCU is always what you make it. There is a rigid booklet for those that want instructions, but there is no downside to going off the path. We let people change books, investigate anything, run it the way they want to. For example some ppl like watching video not reading. The NCU is about learning and encouraging people to develop. Not about box ticking or forced learning. Learning is fun and we want to keep it that way. Here is what some graduates say about the NCU: I really enjoyed the process. It exposed me to books and problems otherwise I would skip or overlook. Flexibility could be improved in my opinion Learning is the best thing that happens at novoda, I wish some of the NCU modules where shorter I really appreciated the NCU as guide for my career path, but why does it stop after one year? Further Improvement This discussion about the fluidity of learning vs rigidity of a defined task it really needs some thought. Guilds are our next iteration, separating up into smaller groups who want to learn around a specific topic. We have Android and iOS guilds, IoT, Machine Learning and Data science guilds. This approach could impact the NCU in the creation of new modules or a complete reorganisation of what is deemed a \"module\". Right now, people feedback to me to change and expand the NCU, but I want everyone to have the power to change. Need to think how to increase flexibility and involvement in the materials creation. After speaking to a few people we are hoping to open source the NCU in the near future . Digital tracking will help people record where they are up to, set goals and targets easier, breaking down the problem into smaller chunks. Crafting Conclusion Craft isn’t just hype - businesses want people who have quality, want people who give value. Involve your company on your journey, let them know your aims and goals, but also measure your impact, show how your crafting process and practices add value and help you succeed. When you succeed your company succeeds. Novoda believes in quality, this includes quality of its engineers as well as everything else. Therefore your craft is important to Novoda if you do well the company does well.", "date": "2017-10-19"},
{"website": "Novoda", "title": "Metrics, Management & Revolution: Insights from Agile Cambridge 2017", "author": ["Kathleen Bright (Scrum Master)"], "link": "https://blog.novoda.com/metrics-management-revolution-insights-from-agile-cambridge-2017/", "abstract": "Before I set foot in Cambridge last week, I made a note of which sessions I was most looking forward to at Agile Cambridge 2017. Here’s what I thought of them… The Metrics You Should Use But Don’t by Cat Swetel, a case study We’ve been getting increasingly data-oriented at Novoda to make better decisions. This quarter, each department has been experimenting with paying attention to different metrics. So, I eagerly anticipated what I'd learn from this session. Cat Swetel demonstrated how to use the same data to present different things to different people, depending on what they needed. Data can provide answers, tell stories, enable decisions and inform trade-offs, but it needs to be visualised in different ways. Cat showed us which visualisations she used for different needs. Bar charts made giving a direct answer straightforward and clear, whilst a scatter graph illustrated a story about the impact of one work type on delivery of another. Cat gave us an invaluable reminder of the importance of context when looking at metrics, and of the danger of looking at numbers in isolation. Next steps I’m looking forward to watching the recording of this talk and going through the slides when they’re published. I’m having a play with some of the resources Cat shared, in order to increase my understanding and improve my analytical skills. I want to generate better insights from our historical data and communicate these more clearly. Recommendations 💻 Resources for forecasting and modelling software projects 📕 Dynamics in Action by Alicia Juarrero 📕 Actionable Agile Metrics for Predictability: An Introduction by Daniel S. Vacanti 📕 The Big Book of Dashboards by Andy Cotgreave, Jeffrey Shaffer and Steve Wexler Unblocking Middle Management by Dr Clara Juanes-Vallejo and Carl Whittaker Sometimes people get stuck when it comes to driving change higher up the chain of command. It's common when it comes to trying out different ways of working. I attended this workshop to learn about how to better support managers in this position. Carl and Clara introduced us to the idea of using user personas… for managers. Personas help us understand the needs and capabilities of people, so why not apply that internally? Working in groups we extrapolated the influencing power and relationships of fictional characters. I loved the simplicity of taking a tool many of us are familiar with and applying it in a different context in this way. Next steps I'm excited to create a persona for myself and to share it, encouraging workmates to do the same. Agile isn’t enough: Revolution over Transformation by Todd Charron, a tutorial I chose this session for a different approach to supporting organisations embracing change. I'm keen to learn as much I can from people's diverse experiences of agile transformations. Todd gave an overview of different styles of agile transformation: top down, bottom up, pilot, gradual evolution... and the perils of each. He made a persuasive case for pursuing a different course... revolution, which made for a compelling story. The forces or challenges resisting the change are your dictator. Those championing the change alongside you are the rebels. With this in mind, Todd described features of dictatorships and revolutions. He emphasised the importance of a team identity, rituals and other initiatives. Todd shared practical guidance for introducing change whilst demonstrating the power of storytelling. Next steps: Books to read 📕 From Dictatorship to Democracy by Gene Sharp 📕 Turn the Ship Around! by L. David Marquet 📕 The Righteous Mind by Jonathan Haidt 📕 The Method Method by Eric Ryan, Adam Lowry and Lucas Conley ...and another thing All the sessions were engaging, interesting and useful, with skilled and entertaining speakers. Dr Brian Little delighted us with his keynote “How Personality Matters”. Watch his TED talk: “ Who are you, really? ” to understand other people better and learn more about yourself too. Check out #AgileCam on Twitter for a wealth of references and resources from an event rich with ideas and insights. Looking forward to watching the videos from Agile Cambridge 2017 when they’re released. Can't wait for next year’s event!", "date": "2017-10-03"},
{"website": "Novoda", "title": "Everything Assistant at Google Developer Days 2017", "author": ["Francesco Pontillo"], "link": "https://blog.novoda.com/everything-assistant-at-google-developer-days-2017/", "abstract": "The latest Google Developer Days conference was held in Krakow on 5th and 6th of September 2017, where Google announced the latest news for Google Assistant developers. This post aggregates all that information for you and helps explain how to build your perfect action. Contextual understanding If you currently trigger the Assistant, be it on Home, on your phone, and ask it a question, the answer will be built based on the global knowledge that Google has of the world. This means that Google will favour the path that is more likely to give any user the correct answer. Starting from - likely - next year, Google will leverage the previous search results to provide the following answers, therefore increasing the likelihood of previous contexts of appearing again in following answers. Google Assistant on Android already does this, albeit partially, when you fire it up from any screen - let's say, a music player - and ask it questions like: when did it come out? This is possible given the context is provided by the currently playing song, artist and album. Teaching something new to Google If you ever wrote a Prolog program, you may have used the assert meta-predicate to teach your program new rules in a dynamic way. Google will roll out a very similar feature that will accept instructions such as: when it's raining I can not go to work walking These kinds of instructions will effectively teach your unique Assistant new information about your lifestyle that it may not be able to understand on its own. Technically, it will generate new internal rules to be able to answer queries such as: can I walk to work tomorrow morning? As small as it is, I think this is the biggest takeaway of all the Google Assistant news, as users will now be able to increase the power of their Assistant without any hard-coded path from Google (as it is for the weather) or from developers (as it is now with custom Actions for Assistant). More about Conversational Interfaces Since the Google Assistant is planned to be available on Google Home, Android, Android Wear, Android TV, and Android Auto, any action you, as a developer, create for it should support different means of interaction : taps on screen, speech, text and visuals. The Assistant ecosystem supports that with a multitude of features: cards , lists and carousels for screen-based devices different voice outputs for conversation-only devices such as Google Home hints that suggest possible continuations for the conversation text and speech combinations for devices that can display the text and say something at the same time (e.g. the Google Assistant for Android) Detailed information with dos and don’ts is available at The Conversational UI and Why it Matters . Requesting permissions The Assistant SDK allows Actions to request permissions in the middle of a conversation , if the permission wasn't provided before. This is much cleaner than asking for it up front, as it provides the user the context of their question: \"Assistant is asking me for location because I just asked it to deliver me a pizza\". The best part about this type of permission request is that developers can provide the reason for it as part of the request (which cannot happen on Android, for instance): const permission = app.SupportedPermissions.DEVICE_PRECISE_LOCATION;\napp.askForPermission('To deliver your porcini mushroom pizza', permission); This will result in Assistant asking for something like: To deliver your porcini mushroom pizza, I'll need your exact location. Is that ok? You can read more about this in the Helpers for Actions guide . Identity and Payments Last but not least, Google Assistant also supports user identity . By simply building your own OAuth 2 identity server (or configuring one of the many open source alternatives available ), you can allow users to link their accounts to your service. The obvious reasons for this is to authorise payments through the Google platform, but also with external providers such as Stripe or Braintree. You can read this detailed step-by-step guide on how to add money transactions to your Actions. Actions Discovery All actions available for Google Assistant can be found in the Google Assistant Discovery, where they're listed according to user's guessed preferences and from where they can be linked to anywhere, since the Assistant platform is available for all kinds of devices. How to get started? There's no better way of understanding the Google Assistant platform than getting hands on and developing a very simple Action . You can watch the talk \"Developing Conversational Assistant Apps Using Actions on Google\" to get an in-deep understanding of the developer features we discussed in this blog post. If you are more interested in the upcoming user-based features for Google Assistant, then you should watch the GDD Day 2 Keynote . This Youtube playlist holds all the videos of the Google Developer Days 2017, if you're interested in more Google-related products. Finally, if you're interested in learning more about designing Conversational Interfaces, you can check out Alex 's blog post, \"Building natural dialogues for your voice assistant\" .", "date": "2017-09-21"},
{"website": "Novoda", "title": "Using TalkBack (II)", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/using-talkback-part-two/", "abstract": "Did you think I forgot? 😅 Part two will introduce some more gestures as well as some advanced TalkBack usage. Continuing from our last post from the Using TalkBack series, we'll cover system navigation gestures, TalkBack's global context menu, local context menu as well as customised actions. (This post assumes TalkBack version 5.2.1 with default gestures - it's possible to modify the gesture-to-action pairings in TalkBack's settings!) System navigation The Back action is super useful. Swipe down and to the left (like a reverse L-shape). Multi-part gestures must be completed without taking your finger off the screen: make sure your corner is reasonably sharp since it won't recognise sloppy right-angles. The Home action is similar but it's up and to the left. Instead of using gestures, partially sighted users might just focus on the soft navigation buttons and activate these explicitly. Global context menu The global context menu is a menu that contains context-agnostic actions (hence \"global\"). On recent versions of TalkBack, it's surfaced as a list dialog, Swipe down and then right (an L-shape) to activate it. \"Pause feedback\" might be most useful here for users who are just testing TalkBack and need a way to quickly switch back into a single-touch-to-activate mode. The behaviour of the service is suspended, and can be resumed from the notification shade, but the service itself is still running, and so any apps that check whether TalkBack is enabled will still behave as if it's turned on. The \"dim screen\" option can be used by testers who don't want to be tempted into cheating , but its real purpose is to save battery life on devices with AMOLED screens. The easiest way to \"undim\" the screen is to pull up the global context menu again. This menu also includes some cool navigation and testing tools alike: \"read from top\" and \"copy last utterance to clipboard\" being the two which stand out. Local context menu Local context menus are more interesting since the options change based on what you currently have selected. Swipe up and to the right (an r-shape) to activate this menu. At a basic level, there'll either be no menu or you'll be able to change the navigation granularity of TalkBack - instead of each swipe navigating between elements, it can instead read the next sentence, word or letter (\"default\" reads the whole element). If the selected element is a direct or indirect child of a ViewPager , then you'll see \"Page navigation\", allowing you to navigate to the next and previous pages. In this case, the navigation granularity is hidden behind \"Navigation settings\". The local context menu is also where you'll find customised actions, under the \"Actions\" item, if any are available. Another exercise for the reader Try using your Android device with the \"dim screen\" option enabled for 15 minutes, let me know how you get on and get in touch if you'd like to learn more!", "date": "2017-09-19"},
{"website": "Novoda", "title": "Take control of your backend with Firebase Cloud Functions (II)", "author": ["Luis G. Valle"], "link": "https://blog.novoda.com/take-control-of-your-backend-with-firebase-cloud-functions-ii/", "abstract": "Use Firebase Realtime Database to implement an easy yet powerful API cache for your mobile apps. In the previous post , we learned how to use Firebase Cloud Functions to clean up a backend response and make it more mobile friendly. In this post, we are going to show how to use Firebase Realtime Database to save that cleaned response, using it as a cache. This will prevent calling the original backend API too frequently as well as unnecessary transformations. Why use a cache at all? The cloud function we developed in part one was used to fetch raw data from our backend and transform it into something easier to process by a mobile app. But, in the majority of cases, there’s no need to fetch new fresh data every time a client requests it: cheaper, cached data is good enough. What to cache and for how long will depend on several factors unique to your case: Business rules: perhaps your backend only produces new data at certain known times. Some newspapers, for example, only have morning, midday and evening editions. Or maybe you know your backend only generates updated data every hour. Costs: putting together the data a mobile client is requesting could be an expensive operation for your backend. Time: this is related to the previous point. If generating requested data is expensive, then it is most certainly going to be slow. And you don't want to waste users’ time waiting. In all these cases it seems wise to establish a data validity policy and rely on already cleaned cache data instead of fetching the raw data for every request. As an added bonus, having our cached data in Firebase allows us to share cache logic across multiple clients . Persist cleaned up model to Firebase Continuing with the example introduced in part one, what we are going to save is the cleaned up feed from The Guardian. The feed , once cleaned looks like this: There are three things to consider for this example: Saving cleaned up data into our project’s Firebase database. Checking if there is valid cached data before fetching new. Cache invalidation policy: deciding when this cache is not valid anymore. 1. Saving transformed data into Firebase database In this gist , you can find the full code for fetching and transforming data we did in part one. Let’s start by refactoring that code into something more Promising exports.fetchGuardian = functions.https.onRequest((req, res) => {\n   return request(URL_THE_GUARDIAN)\n       .then(data => cleanUp(data))\n       .then(items => response(res, items, 201))\n});\n\nfunction request(url) {\n   return new Promise(function (fulfill, reject) {\n       client.get(url, function (data, response) {\n           fulfill(data)\n       })\n   })\n}\n\nfunction response(res, items, code) {\n   return Promise.resolve(res.status(code)\n       .type('application/json')\n       .send(items))\n} This code is equivalent to the one in the previous post, but by using Promises , the flow is easier to understand and modify. I’m omitting here the cleanUp function as it’s not relevant, but check the previous post if you are interested in it. The first thing we are going to do is saving items in Firebase database before returning them to the client. That can be done by modifying the previous code and adding a call to save(items) in the Promises chain: exports.fetchGuardian = functions.https.onRequest((req, res) => {\n   return request(URL_THE_GUARDIAN)\n       .then(data => cleanUp(data))\n       .then(items => save(items))\n       .then(items => response(res, items, 201))\n});\n\nfunction save(items) {\n   return admin.database().ref('/feed/guardian')\n       .set({ items: items })\n       .then(() => {\n           return Promise.resolve(items);\n       })\n} With admin.database().ref(‘/feed/guardian’) we obtain a reference to a path in our database. .set({items: items}) pushes the items array to that path in the database with the key \"items\" and returns an empty Promise. Finally, when the promise is fulfilled (meaning the writing process is done) we return a new Promise with the original items array to continue the chain. In Firebase this will generate: 2. Checking cached data before fetching new At this point, we are persisting the cleaned up data in our database but we are not doing anything with it. The next step is to check if there’s saved data in Firebase database before making an HTTP request to our backend. exports.fetchGuardian = functions.https.onRequest((req, res) => {\n   return admin.database().ref('/feed/guardian')\n       .once('value')\n       .then(snapshot => {\n           if (isCacheValid(snapshot)) {\n               return response(res, snapshot.val(), 200)\n           } else {\n               return request(URL_THE_GUARDIAN)\n                   .then(data => cleanUp(data))\n                   .then(items => save(items))\n                   .then(items => response(res, items, 201))\n           }\n       })\n});\n\nfunction isCacheValid(snapshot) {\n   return (snapshot.exists())\n} What changes from step 1 is that we read from the database before fetching the feed. As we already know, admin.database().ref('/feed/guardian') is a path in the database. With .once('value') we read the values at that path once and return a Promise with them. What we do afterward is simple: If the data read from /feed/guardian is valid, we return it (at this point valid means just existing) If the data is not valid (it doesn't exist) we do exactly the same thing we were doing in step 1: read from the original feed, persist in our Firebase database and return. 3. Cache invalidation policy Finally, we need to set rules for cache validity. To keep this example simple we are going to consider the data is valid for 1h since it’s saved. After that time, the next request should fetch new items and replace the cached ones with them. To do that we need to save the fetching time along with the items when we persist them. We need to modify our save function like this: function save(items) {\n   return admin.database().ref('/feed/guardian')\n       .set({\n           date: new Date(Date.now()).toISOString(),\n           items: items\n       })\n       .then(() => {\n           return Promise.resolve(items);\n       })\n} This will produce a new field in our database: The last bit is to use this date to check how old our cached data is . If the cached data was saved less than one hour ago we’ll consider it valid. Otherwise, we invalidate the cache by fetching fresh data and overriding it. function isCacheValid(snapshot) {\n   return (\n       snapshot.exists() &&\n       elapsed(snapshot.val().date) < ONE_HOUR\n   )\n}\nfunction elapsed(date) {\n   const then = new Date(date)\n   const now = new Date(Date.now())\n   return now.getTime() - then.getTime()\n} All together The code is also available here as a gist snippet const ONE_HOUR = 3600000\n\nvar functions = require('firebase-functions');\nconst URL_THE_GUARDIAN = \"https://www.theguardian.com/uk/london/rss\"\n\nvar Client = require('node-rest-client').Client;\nvar client = new Client();\n\nconst admin = require('firebase-admin');\nadmin.initializeApp(functions.config().firebase);\n\nexports.fetchGuardian = functions.https.onRequest((req, res) => {\n    var lastEdition = admin.database().ref('/feed/guardian');\n    return lastEdition\n        .once('value')\n        .then(snapshot => {\n            if (isCacheValid(snapshot)) {\n                return response(res, snapshot.val(), 200)\n            } else {\n                return request(URL_THE_GUARDIAN)\n                    .then(data => cleanUp(data))\n                    .then(items => save(lastEdition, items))\n                    .then(items => response(res, items, 201))\n            }\n        })\n});\n\nfunction save(databaseRef, items) {\n    return databaseRef\n        .set({\n            date: new Date(Date.now()).toISOString(),\n            items: items\n        })\n        .then(() => {\n            return Promise.resolve(items);\n        })\n}\n\nfunction request(url) {\n    return new Promise(function (fulfill, reject) {\n        client.get(url, function (data, response) {\n            fulfill(data)\n        })\n    })\n}\n\nfunction response(res, items, code) {\n    return Promise.resolve(res.status(code)\n        .type('application/json')\n        .send(items))\n}\n\nfunction isCacheValid(snapshot) {\n    return (\n        snapshot.exists() &&\n        elapsed(snapshot.val().date) < ONE_HOUR\n    )\n}\n\nfunction elapsed(date) {\n    const then = new Date(date)\n    const now = new Date(Date.now())\n    return now.getTime() - then.getTime()\n}\n\nfunction cleanUp(data) {\n    const items = []\n    const channel = data.rss.channel\n\n    channel.item.forEach(element => {\n        item = {\n            title: element.title,\n            description: element.description,\n            date: element.pubDate,\n            creator: element['dc:creator'],\n            media: []\n        }\n        // Iterates through all the elements named '<media:content>' extracting the info we care about\n        element['media:content'].forEach(mediaContent => {\n            item.media.push({\n                url: mediaContent.$.url,                // Parses media:content url attribute\n                credit: mediaContent['media:credit']._ // Parses media:cretit tag content\n            })\n        });\n        items.push(item);\n    });\n    return Promise.resolve(items);\n} Stay tuned! This is part two of a three-part series of articles about Firebase. In the final installment we'll learn how to use Google Cloud Natural Language API from our Firebase Cloud Function to enrich the backend response, for example, adding sentiment analysis. Find me on Twitter @lgvalle , I'd love to chat about Android, Firebase & Cloud Functions.", "date": "2017-09-12"},
{"website": "Novoda", "title": "A trip to the seaside: iOS Dev UK", "author": ["Niamh Power (iOS & Android Software Crafter)"], "link": "https://blog.novoda.com/ios-dev-uk/", "abstract": "This week I had the pleasure of attending iOS Dev UK, in the lovely Aberystwyth on the Welsh coast. Here’s my thoughts on the conference, including a look at some of the topics that were on offer and what could be improved for next year. Having only been to cross-platform conferences in the past, I was curious to attend a platform-specific event , and the selection of talks seemed to cover a range of interesting topics. Furthermore, since I’ve mainly interacted with the local Android community (through GDG Liverpool ), it was great to meet more Liverpool/North West iOS developers. On arrival, the weather was as expected for Wales (extremely wet) and with the university up a seemingly-never-ending hill, it wasn’t off to the best start! However after visiting the town, getting the chance to meet a range of new faces, and stumbling across a sizeable Northern contingent, any worries were soon mitigated. A stand-out talk for me was on 'Resource Management', from Rebecca Eakins and Carson Ramsden from ThoughtWorks. This included some great insights into the tools available in Xcode to a developer, in particular the Instruments tool , and how to use them effectively to create an efficient app with a smooth UI. By analysing the results from the Activity Monitor, a developer can dig into possible issues in their code and UI that could be slowing down the application, which is especially important as an app grows in size. The memory graph in Xcode was also showcased, which allows a developer to easily spot memory leaks by showing the objects currently on the heap, and their respective references. Another great talk was on 'Taming Animations' from Sash Zats . This covered the different tools available for your animation needs in iOS. These included UIViewPropertyAnimator , CoreGraphics and UIDynamics . Sash also discussed 'Lottie', a tool from Airbnb , although this limits you to needing a knowledge of Adobe After Effects, the consensus was that it’s a great tool for adding fun and meaningful animations to your app. You can find out more on Lottie from one of our product designers Chris here . Core ML also had a decent representation, with Machine Learning being this year's hot topic in development . I was pleasantly surprised at how much the framework does for you, and it will definitely be something to experiment with over the next few months. On the final two days, there were hour long sets of Lightning Talks. I always enjoy these at conferences as it allows a broad range of developers to get up and talk about interesting topics, and these didn't disappoint. Some of the stand out talks from the two sessions: Snapshot testing with FBSnapshot Smoke testing iOS Apps Architecting iOS Apps beyond VIPER The Lock Screen is the new Home Screen. Overall I felt the conference was a great event, and a brilliant way to meet others in the iOS community. The diversity of the attendees and speakers could be improved, perhaps by doing more proactive outreach for next year’s event, and I think this would help diversify the content in the talks further. Also, an Android app for the conference would be great for the cross-platform developers in attendance who don’t have a particular allegiance. 😜", "date": "2017-09-14"},
{"website": "Novoda", "title": "IoT @ Google Developer Days - Krakow 2017", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/iot-google-developer-days-krakow-2017/", "abstract": "After an amazing event, this blog gives you the lowdown on all things IoT at GDD Krakow 2017. Google Developer Days (GDD) Krakow was a global event showcasing the latest developer products and platforms from Google to help you quickly develop high-quality apps, grow and retain an active user base. Let’s see what it offered for Android Things enthusiasts. Google Developer Days was a huge event with over 2000 attendees. The conference itself was made up of multiple sections. There were the demonstration stands and sandboxes, the conference rooms for the talks, hands on training code labs as well as office hours for one to one guidance and Q&A. Let’s see where all the IoT and Android Things fits in. Android Things demonstration stand The demonstration stand/sand box displayed projects created by Google, Google Developer Experts and Novoda. These examples showed the multiple ways Android Things can be leveraged to create smart IoT devices and systems. All the projects can be found at hackster.io/google . These included: A textual smart clock, comprising of an Android Things board pulling the time over WiFi and controlling an Arduino Nano via UART to control a strip of LEDs. An MQTT climate control system, a temperature sensor and infra-red controller connected via a local WiFI network talking MQTT on an Android Things board. This could read the temperature data and feed it back to a Firebase real-time database which then showed the changes on a companion Android app. If the temperature reached a certain threshold the Android Things board would signal the infra-red LED to turn a theoretical air conditioning system on/off. Piano hero game, which leverages the Android MIDI APIs as well as Android Things’ optional display capabilities to show a piano game where you press the keys corresponding to what is on the screen. It also uses PWM to play the notes via a Piezo buzzer. The TensorFlow identity camera. Using a trained TensorFlow model, you press the button on top of the green Android and point it at a cat or dog (or picture of) and it will tell you the type and breed of the animal. The drawing bot. Take a picture of your face and watch the Android Things bot draw you like one of its French robots. This uses PWM motor control to physically draw your image on a piece of paper. It is running on batteries and takes about 6 minutes per image with the batteries lasting about 2-4 hours. At the IoT stand you could also pick up a TechNexion Android Things starter kit that comprised of some of the components shown in the demos, such as an NXP Pico board, LCD display, and a camera module. There was also an area on the second floor where you could grab some old hardware components and string or glue them together into art/jewellery. Android Things talks Dave Smith talked about the IoT ecosystem at Google and all the possibilities that are out there. One very interesting graph in his session showed that while about 5 billion smartphones will be in use in 2020, by then there will already be 30 billion IoT devices. What's up with Internet of Things and the Google Assistant? Google has a wide range of new platforms and tools to support computing anywhere and everywhere. Learn how Android Things can be used to simplify the development and production of IoT devices. Hear how the Google Assistant enables users to have conversations with your Actions. See how Android phones, Android Wear, and Android Auto allow users to interact with your service anytime and anywhere. Study how TensorFlow can be used to make machine learning really easy in all kinds of IoT applications, beyond mobile. Rebecca Franks talked about Android Things development and what is needed to get started. The Novoda Piano Hero game got a shout out on one of the slides. :-) Android Things: The IoT Platform for Everyone Android Things is Google’s Internet of Things (IoT) platform that is based on the Android operating system. In this video, Rebecca Franks covers the basics of getting started with the platform and how any developer without electronics experience can build IoT apps with Android Things. She presents a few use cases and examples, along with an overview of how you can use the existing Android libraries with your next IoT project. Gus Class talked the audience through Google's IoT Core platform. This is meant for enterprise scale deployments where you want to manage input and control provisioning from 10,000+ simultaneous devices. Google Cloud IoT Core Technical Deep Dive Details of high-level features of the Google Cloud IoT Core product. Additionally, he explains how the underlying Google Cloud products that make up the Cloud Solution let you add IoT capabilities to your products at scale for both data ingress and analytics. An end-to-end demonstration of the product concludes the presentation. Android Things Codelabs Codelabs were available for learning and training. All training areas were up on the top floor, in a quieter area. There were also office hours with patient Googlers sitting and waiting to answer your questions. It was great having knowledgeable people giving great advice, help and insight. All the Codelabs can be found here . For Android Things, these are the highlights; Understanding Peripheral I/O , Creating a Weather Station , Classifying Images and deploying the Android Things assistant . GDD Europe was a great event. As well as all of the awesome technology and organised learning, there were a lot of amazing people and friendly insightful conversations going on from just walking around and bumping into people. And a final thank you for the awesome facilities!", "date": "2017-09-07"},
{"website": "Novoda", "title": "Supporting Droidcon Berlin", "author": ["Stefan Hoth (Head of Operations, Germany)"], "link": "https://blog.novoda.com/supporting-droidcon-berlin-2017/", "abstract": "At Novoda we value the exchange of ideas and solutions. That is why we make a point of contributing to the communities we’re part of. We’ve created and supported meetups for development & design, co-organized conferences - most prominently Droidcon London - and open sourced many useful libraries and tools . In this vein I’m honored to be once more part of the program committee for Droidcon Berlin along with well known and respected members of the German Android community. Examining all the submissions is a great way to see what the community cares about most. A big challenge is always to create a balanced program for the event, both in terms of topics as well as in experience level. We want to allow both novices and outspoken experts to enjoy the conference and learn something. 📊 Talk categories 39% Best Practise / Code Quality 23% Architecture 😯 14% APIs 8% Design ☹️ 8% Langs (81% @Kotlin ) 6% IoT 2% GameDev&VR — Stefan 💭🇪🇺🕹️📷🍦 (@stefanhoth) May 20, 2017 One thing I really like is the anonymous submissions . If the speaker fills out all fields correctly there’s no way for the program committee to know their identity, company or gender, which is an effective way to counter any bias, conscious or unconscious, and puts the content of the submission first. A great article on why this is good for everyone can be found on mindtools.com . For years Droidcon Berlin has been the heart of the Android community of the region and it is going on strong. I’m looking forward to learning a lot and spending time with the community once again. If you don't have your ticket yet get them here and I hope to see you there, too! Post image: cc-by-sa 2.0 Stefan Hoth", "date": "2017-08-29"},
{"website": "Novoda", "title": "Porting a python library to Android .. Things - The InkypHat", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/porting-a-python-library-to-android-things-the-inkyphat/", "abstract": "Android Things allows you to create driver libraries around your IoT peripherals. This is a powerful platform feature allowing you to use peripherals without needing to know the low level details. In this blog post, I’ll show you how such a driver was created for the Inky pHAT e-paper display. The Inky pHAT e-ink display is a 212 x 104 pixel three-colour display. You can draw in black, white or red. As it uses e-ink, the screen is perfectly readable in bright sunlight. You can also disconnect the power and it will continue to display the last image drawn. Ideal for badges or battery-powered IoT devices. Whenever starting with a new peripheral, first you should check an Android Things driver library has not already been created. After that, reading the datasheet or the documentation that is sometimes supplied on the vendor’s website will suffice. I bought the Inky pHAT from Pimoroni and their website links directly to the Python library . Reading the Python source code in order to create an Android Things driver library, we need to break down our problem and understand a few things to proceed. Which Peripheral IO APIs does it use (GPIO, SPI, I2C etc)? What is the public API exposed to in order to use the peripheral? Which features does the device support? Once we know these three definitions, we can go about starting to write some Android code. The Python repository is made up of a couple of folders including a sample use folder and the library itself. The code for driving the Inky pHAT is mostly within Inky214x104.py ,  so we’ll concentrate on this file. Which Peripheral IO APIs does it use (GPIO, SPI, I2C etc)? First, look at the imports and the __init__ method (equivalent to a Java constructor). We can see the class is importing SPI and GPIO . It seems there are 3 GPIO pins used and 1 SPI bus . That’s good for us, we have GPIO & SPI in the AndroidThings Peripheral IO APIs . Converting the peripheral IO use from Python to Java will look something like this: Python GPIO.setup(self.dc_pin, GPIO.OUT, initial=GPIO.LOW)\n       GPIO.setup(self.reset_pin, GPIO.OUT, initial=GPIO.HIGH)\n       GPIO.setup(self.busy_pin, GPIO.IN)\n\n\n       self._spi = spidev.SpiDev()\n       self._spi.open(0, self.cs_pin) Java spiBus.setMode(SpiDevice.MODE0);\n   chipCommandPin.setDirection(Gpio.DIRECTION_OUT_INITIALLY_LOW);\n   chipCommandPin.setActiveType(Gpio.ACTIVE_HIGH);\n   chipResetPin.setDirection(Gpio.DIRECTION_OUT_INITIALLY_HIGH);\n   chipResetPin.setActiveType(Gpio.ACTIVE_HIGH);\n   chipBusyPin.setDirection(Gpio.DIRECTION_IN);\n   chipBusyPin.setActiveType(Gpio.ACTIVE_HIGH); What is the public API exposed in order to use the peripheral? The public API can tell us how the original author expects the peripheral to be communicated with. It can give hints to features available but also explain timings or execution order necessary for correct use. Looking through the examples . It sets a border, sets an image on the Inky pHAT, then calls show. Diving into the show() method we have set_pixel and update . This makes sense, we can set any pixel we want on the display. After we’ve configured what we want to show, we call update to refresh the display itself. Whilst this isn’t the full API, it's enough to get us started and to use the display. [1] Python def set_pixel(self, x, y, v):\ndef update(self): Java void setPixel(int x, int y, int color);\nvoid refresh(); Which features does the device support? The examples are a great place to understand feature support. We already know we can set any pixel to black, white or red. Looking at a further example, it seems partial updates of the screen are possible. [2] Looking into the update(self) mechanism for the Inky pHAT. It seems with every call to update: the display turns itself on sends over the black, white and red pixel data (it sends black and red, any remaining pixels will be white) turns the display back off again , which is its power saving feature. Diving into the black pixel data transmission it looks like this: Python # start black data transmission\n        self._send_command(_DATA_START_TRANSMISSION_1)\n        self._send_data(buf_black) Which is easily translated to Android: Java sendCommand(DATA_START_TRANSMISSION_1, pixelBuffer.getDisplayPixelsForColor(Palette.BLACK)); Understanding the send_command method and the send_data method is getting into the knowledge of the Peripheral IO APIs. Once we have converted these we will be able to communicate with the Inky pHAT from our Android driver library. Python def _send_command(self, command, data = []):\n        #print(\"Command {0:02x}\".format(command))\n        self._spi_write(_SPI_COMMAND, [command])\n        if len(data) > 0:\n            self._spi_write(_SPI_DATA, data)\n\n    def _spi_write(self, dc, values):\n        GPIO.output(self.dc_pin, dc)\n        self._spi.xfer(values) Here _send_command first writes to the command GPIO pin ( dc_pin ). It sets this pin to true [3] which tells the Inky pHAT that the next chunk of bytes it receives over SPI is a command. Then it sets the same command pin to false, meaning the next chunk of bytes is a data array. I think this is conveyed a little better when moving to Android. Java private void sendCommand(byte command, byte[] data) throws IOException {\n   sendCommand(command);\n   sendData(data);\n}\n\nprivate void sendCommand(byte command) throws IOException {\n   chipCommandPin.setValue(SPI_COMMAND);\n   byte[] buffer = new byte[]{command};\n   spiBus.write(buffer, buffer.length);\n}\n\nprivate void sendData(byte[] data) throws IOException {\n   chipCommandPin.setValue(SPI_DATA);\n   spiBus.write(data, data.length);\n} The beauty of porting the Python library in this way, means after understanding these points, the rest of the code can be moved from Python to Android without detailed knowledge of how it actually works. Here’s an example: Python def _display_fini(self):\n        self._busy_wait()\n        self._send_command(_VCOM_DATA_INTERVAL_SETTING, [0x00])\n        self._send_command(_POWER_SETTING, [0x02, 0x00, 0x00, 0x00])\n        self._send_command(_POWER_OFF) Java private void turnDisplayOff() throws IOException {\n   busyWait();\n\n   sendCommand(VCOM_DATA_INTERVAL_SETTING, new byte[]{0x00});\n   sendCommand(POWER_SETTING, new byte[]{0x02, 0x00, 0x00, 0x00});\n   sendCommand(POWER_OFF);\n} We can then turn the display off by adjusting some on-device settings. The four bytes 0x02, 0x00, 0x00, 0x00 must command it to reduce the power. However we don’t need to know more than that to port a working AndroidThings driver library from Python. The AndroidThings Pimoroni Inky pHAT driver library is now awaiting merge on the contrib-drivers GitHub repository, and will soon be available for use by anyone. All the end user will need to do is something like this: Java InkyPhat inkyPhat = InkyPhat.Factory.create(INKY_PHAT_DISPLAY, BUSY_PIN, RESET_PIN, COMMAND_PIN, Orientation.LANDSCAPE);\nBitmap bitmap = BitmapFactory.decodeResource(resources, R.drawable.ic_logo);\ninkyPhat.setBorder(InkyPhat.Palette.RED);\ninkyPhat.setImage(0, 0, bitmap, InkyPhat.Scale.FIT_X_OR_Y);\ninkyPhat.refresh(); This knowledge is transferable to porting other IoT peripheral libraries from multiple languages to Android with AndroidThings. I’m looking forward to the future of IoT with AndroidThings and I hope after having read this you will be inspired yourself to contribute and perhaps make a driver of your own for others to use and benefit from. If you do, or if there is a peripheral you want that is missing a driver library :-) then let me know on Twitter @Blundell_apps . I recommend just getting a high level understanding before diving into the coding, rather than attempting to grasp everything upfront. I think you’ll get to the fun stuff faster and more easily that way. ↩︎ While it’s an interesting feature, I don’t have a use case for this right now, so I’ll save some time and not address partial updates here. ↩︎ _SPI_COMMAND is a constant for true ↩︎", "date": "2017-08-15"},
{"website": "Novoda", "title": "Use cases for wearables", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/use-cases-for-wearables/", "abstract": "Android Wear 2.0 is focused on bringing standalone apps on smartwatches, but that does not invalidate the key principles that should be followed when designing wearable applications: glanceability and micro user interactions. Glanceability is still the key when presenting data to the user in the small screen of a smartwatch. The user needs to be able to consume the key information at a glance. At the same time, a standalone Wear version of a phone app shouldn't necessarily include all the functions and controls of the main handheld app. Wearable applications should focus on a reduced set of actions with minimal user interaction and frustration-proof controls. Remember that the user's phone is the place for complex and long interactions. With this in mind, we at Novoda did a small exercise: we thought about how different categories of applications could benefit from a wearable version, allowing users to access some of the main features directly from the wrist. Here's the list, divided into different app categories: Event venue Event ticket reminder Quick access to the venue with event ticket validation showing a barcode/QR code on the watch screen or directly with NFC Membership validation Real estate agency Location-based notifications for nearby listing matching pre-set criteria Quick details of a viewing Quick list of questions to be asked when viewing a place Hotel / accommodation Quick check-in showing a barcode/QR code or with NFC Room unlock with NFC Room service (order towels, breakfast, etc) from the wrist Restaurant / coffee Loyalty program: automatically recognised and favourite order preselected, more efficient and more personal In case of a queue: scan a code displayed on the display when placing the order and receive a notification on the watch later, when the order is ready Vending machine company Supplies refill alert: a notification to the technician when some of the items need to be refilled. Integration with current machines using an ad-hoc IoT solution Warehouses management / cleaning After supervisors create the workflow plan, the workers will have a todo list directly available on the wrist Time tracking app: the worker can manually check-in or check-out or with a location-based app it will be easy to automatically keep track of the time Online newspaper / magazine Using text-to-speech while on the move to listen to articles The smartwatch will show the selected articles playlist and playback controls Use of voice commands to search for an article by topic or keywords Shopping centre Quick shopping list Indoor user location with special offers displayed on watch Smart home Quickly control appliances Track indoor user location More generic use cases Share data using motion-based actions, for example handshake trigger at conferences Fitness tracking with activity detection Extension of other devices UI Remote control for a bigger screen This is just a small example of how existing apps could benefit from having a wearable version. Do you have any idea for other interesting wearable use cases? Let me know at @danybony_ on Twitter. A special thanks to Qi Qu for the amazing mockups!", "date": "2017-08-08"},
{"website": "Novoda", "title": "Q&A - Software Craftsmanship Conference London 2017", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/q_a_scl_london/", "abstract": "SCLConf is for software craftspeople for whom delivering software on time simply isn’t enough. Read on for some insights into crafting software at Novoda. This is a cross-post from the Software Craftsmanship Conference London 2017 website, a quick Q&A session with our Head of Engineering: Paul Blundell. In this series of ‘3-minute Q&As’ we give you an insight into some of the world’s leading thinkers and doers in the world of agile and software development. So sit back, relax and take a sneak peek. Oh, and if you like this and want more then check out sc-london.com , where you can immerse yourself in two days of talks, discussions and networking with the blog authors. Q. When did you first come across Software Craftsmanship? I first came across software craftsmanship when I was working as a graduate software engineer in the web space, this was before I moved to mobile. A colleague had a copy of the recently published Clean Code by Robert C Martin. I was struck by the prescriptive nature of an answer to every scenario, and it intrigued me to seek out these scenarios and apply the practices to learn and improve. Later on when I read The Clean Coder it really hit home that being a good developer is not just about the code you write, but how you react to interactions and having a bar for your professional behaviour. This is the side of software craftsmanship that I really enjoy, the psychology and professional best practices. Developers have a responsibility to share knowledge and enable others on their team to understand the why and how of their reasoning and this is what I enjoy improving for myself and enabling in others. Q. What does software craftsmanship mean to you? I might have answered a little already. If we relate back to the manifesto, I see craftsmanship as, not only individuals and interactions, but also a community of professionals and not only customer collaboration, but also productive partnerships. For me, this means showing others in your team, or showing the client that developers/engineers are not just there to code with their heads down. We shouldn't just leave the business of why the product potentially wasn't what the company wanted to others. Software craftsmanship means you have the capabilities to talk to non-technical people, to translate what they want into working software and to translate back improvements or reasons why things may need to change from the coding or framework level. Software craftsmanship means good communication at all levels, all developers being able to understand the code and all team members being able to understand the developers. Q. What do you foresee is the future for software craftsmanship? That is a hard question. As I've matured and gained experience my feelings towards craftsmanship have also changed. It's like the four stages of competence, you start off unconscious and incompetent, not crafting very well and not recognising you need craftsmanship. Conscious incompetent, you learn about craftsmanship but you fail to apply it consistently. Conscious competence, you know what you are doing but you have to concentrate when applying it. Unconscious competence, craftsmanship is second nature, you apply it without thought whilst taking other things into account. I feel many developers stay at stage 3 'conscious competence' and rigorously apply what they deem as craftsmanship without thought for what else is going on in the team. When you get to stage four 'unconscious competence' you have the freedom to consider other things and understand that you sometime need to apply some pragmatism in your practices. This allows for deadlines, demos, unforeseen outside influences and shipping deadlines. Perhaps the future of craftsmanship will be some backlash to this stage three perfectionism, every now and then I see people mob programming, which is all about a whole team agreeing on ideas and setting out a rough framework of the code. It's not \"crafting\" but it should be a tool in a craftsman's toolbox. Developers should be getting more involved in how businesses operate and applying craftsmanship to the bigger picture. Craftsmanship will have its part in digital transformation, helping the organisation to evolve around software delivery, as software is the main medium for communication with the customer. That, in turn, sets a focus on software and craftsmanship. As we see more and more transformation (like Spotify) we will see more emphasis on craftsmanship within organisations. The future is hard to predict and maybe I'm rambling a bit, but I see the future of software craftsmanship in embracing more pragmatism and people starting to talk about outside influences and quantifying value gained from each practice. The other side of the future is - how inclusive is the word \"craftsmanship\" and should the whole idea be reinvented/reworded to allow for diversity and difference. Q. What do you hope people will take away from your presentation? That learning is a journey, you meet people on the way, share and grow together. Continuous learning is what keeps life exciting and your brain active. Q. Google and Apple recently added support for testing in IDEs. How do you see these ecosystems developing for XP practices and what more could they do? I see the ecosystems developing slowly. There is always a big emphasis on acceptance/UI testing when it is enabled inside an IDE, but the real benefits of testing come from the unit tests and the lower levels which don't get so much of a fanfare. I think the next steps for IDEs especially in mobile is to make the loop of testing faster, faster build and execution cycles as well as more stable emulators gives people less excuses to avoid testing. I'd love to see unit tests executed on every save for mobile IDEs (like Android Studio or XCode). Q. What was the biggest obstacle at Novoda when starting your journey towards software craftsmanship? Client acceptance for software craftsmanship. We found that some clients took a little convincing about our methods initially, but they soon saw the value in the speed of delivery and lack of bugs and regressions. It's hard to quantify software craftsmanship, clean code and best practices to a business and most of the literature out there is aimed at convincing developers. So turning software craftsmanship values into explainable value for the business was the biggest challenge. Q. Are there any projects you'd like people to be aware of? How can people help out? I help run the Liverpool Google Developer Group. I have a dream of creating another big tech city in the UK outside of London and you can contribute a small step towards this :-) We are always looking for monthly meet-up speakers, so get in touch at mailto:gdg-liverpool@novoda.com and see more here www.meetup.com/GDG-Liverpool/ Novoda is a big supporter of open source software. We have many OSS repositories including helper libraries and demos. You can check them out here https://github.com/novoda/ and pull requests are always welcome. Q. Where can people follow you online? Technical & demo blog: http://blog.blundellapps.co.uk/ Sharing & talking: @blundell_apps Thoughts & insights blog: https://blog.novoda.com/author/blundell/", "date": "2017-07-18"},
{"website": "Novoda", "title": "Take control of your backend with Firebase Cloud Functions", "author": ["Luis G. Valle"], "link": "https://blog.novoda.com/take-control-of-your-backend-with-firebase-cloud-functions/", "abstract": "As a software engineer building Android apps, I inevitably run into problems with server API’s not designed with mobile clients in mind. In this post I’ll explain how we can use Firebase Cloud Functions to clean up and transform those API’s responses before they reach our app. Backend API responses that are hard to work on mobile apps usually happen in two types of companies: Startups: not enough people to deal with everything, so they need to compromise. If the same person is doing backend, frontend, devops, sysadmin and this month's hipster beer order, then they are probably too busy to care about your concerns on the server API responses. Large corporations: in this case there's enough people. In fact, there's too many people. Nobody knows who's responsible for making the changes you need, and by the time details are ironed out, budgets are approved, tickets are created, hands shaken, permissions granted and goats sacrificed you are already shipping version 3.0 of the app. This is why I love Firebase: it's the bicycle of mobile apps developers. It gives you freedom and autonomy with almost no investment. And you can get it up and running even if you know nothing about servers, lambdas, node.js or hosting. Take control of your backend API What is the worst thing you've ever found on a server API? I can think of a bunch: Same field type could contain numbers, strings, or arrays API will ignore basic HTTP response codes and return 200 OK for errors, putting the error code in the body of the response ...which of course has a totally different schema from the content of normal responses Multiple fields with the same name instead of arrays Responses with no fixed schema, which require manual parsing Different response schemas for the same endpoint, depending on parameters Mixed return formats from different endpoints (some XML, some JSON, etc) Wouldn't be great if you could fix all those problems before they even reach your app? It's not only convenient but more efficient; mainly for two reasons: Not wasting time and power on massaging awkward responses into something you can work with No need to duplicate the response manipulation logic on several platforms The example: a news app Let's define a practical case for this example. We want to build a news app that reads an edition feed from a company's backend. Now imagine there are a series of problems with that feed: The structure is heavily nested, with lots of unnecessary data The feed is messy and contains different elements identified with the same key, instead of using arrays The response is not JSON Parsing this is not a pleasant job on Android. If you use Retrofit you'll need a separate converter, lot of Java classes to act as Server API models for the unnecessary received data and helper classes to transform that Server models into your tidied up Domain models. HTTP Triggers to the rescue Firebase HTTP triggers let you trigger a function through an HTTP request. We'll use them to proxy our backend. Instead of calling the problematic API directly, we'll call our HTTP trigger endpoint, that will execute a function to fetch today's news edition from the backend, clean it up and return it as beautiful small JSON. Have in mind Cloud Functions have a limited time of 540 seconds to run before they are forcibly terminated, so we need to keep them small. 1. Define a function to be triggered through HTTP exports.fetch = functions.https.onRequest((req, res) => {\n    (...) // TODO Fetch from API\n    (...) // TODO Clean up\n    (...) // TODO Return result\n}) That's it. When deployed to your Firebase project this will generate a URL you can call using an HTTP request. Our mobile app network requests will go through this URL, the code in the function's body will get executed, fetching, cleaning and returning the data in the format we need. 2. Fetch raw backend data We need a little bit more code for this. We also need to make sure your Firebase project has billing enabled . It is necessary to do external HTTP requests and since our backend lives outside Google's servers we need to switch to the Blaze plan, which is surprisingly cheap . The first 2,000.000 function executions a month are free and, to put it on perspective, I only did 68 while developing this project. After that you pay $0.40/million requests. At that price point, it’s a cost I am happy to underwrite to save us the pain of dealing with problematic and inconsistent APIs.. The other thing we need is a REST client to call our backend . I like node rest client because it’s super simple to use and comes with beautiful features out of the box, like converting XML to JSON automatically. 2.1 Adding a REST client const Client = require('node-rest-client').Client\nconst client = new Client()\n\nexports.fetch = functions.https.onRequest((req, res) => {\n    client.get(BACKEND_URL, function (data, response) {\n        \n        (...) // TODO Clean up\n        \n        return res.status(200)\n                .type('application/json')\n                .send(data)\n        })\n    })  \n}) This snippet is already doing a lot: we haven't written much, but this code fetches our backend XML edition feed and returns it as JSON with minimum effort from us. You could leave this example as it is and you would already be saving yourself from converting XML to JSON in the app. 2.2 Cleaning up raw data const Client = require('node-rest-client').Client\nconst client = new Client()\n\nexports.fetch = functions.https.onRequest((req, res) => {\n    client.get(URL_THE_GUARDIAN, function (data, response) {\n        const items = cleanUp(data)\n        res.status(201)\n            .type('application/json')\n            .send(items)\n    })\n}) At this point all the heavy work of cleaning up is extracted into a cleanUp function, that takes the raw data we get from the backend and extracts only the bits we are interested in. This is the function you'll need to change for your particular case . In this sample we are going to use the format returned by The Guardian public RSS feed as an example of a convoluted raw backend response. So let's imagine your backend returns something like this: <rss xmlns:media=\"http://search.yahoo.com/mrss/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" version=\"2.0\">\n  <channel>\n    <title>London | The Guardian</title>\n    <link>https://www.theguardian.com/uk/london</link>\n    <description>Latest news and features from theguardian.com, the world's leading liberal voice</description>\n    <language>en-gb</language>\n    <copyright>Guardian News and Media Limited or its affiliated companies. All rights reserved. 2017</copyright>\n    <pubDate>Mon, 31 Jul 2017 15:32:49 GMT</pubDate>\n    <dc:date>2017-07-31T15:32:49Z</dc:date>\n    <dc:language>en-gb</dc:language>\n    <dc:rights>Guardian News and Media Limited or its affiliated companies. All rights reserved. 2017</dc:rights>\n    <image>\n      <title>The Guardian</title>\n      <url>https://assets.guim.co.uk/images/guardian-logo-rss.c45beb1bafa34b347ac333af2e6fe23f.png</url>\n      <link>https://www.theguardian.com</link>\n    </image>\n    <item>\n      <title>'Leaving London means I can afford kids': readers on why the capital lost its sparkle</title>\n      <link>https://www.theguardian.com/uk-news/2017/jul/31/leaving-london-means-i-can-afford-kids-readers-on-why-the-capital-lost-its-sparkle</link>\n      <description>&lt;p&gt;Almost 100,000 Londoners moved out last year. Here they, and others who are avoiding the city altogether, explain why it is no longer the place to be&lt;br&gt;&lt;/p&gt;&lt;p&gt;The rate of Londoners leaving the capital is more than &lt;a href=\"https://www.theguardian.com/uk-news/2017/jul/24/bloated-london-property-prices-fuelling-exodus-from-capital\"&gt;80% higher than five years ago&lt;/a&gt;, according to Savills, with people in their thirties the age group most likely to leave. &lt;/p&gt;&lt;p&gt;We asked readers why they’re leaving London, or avoiding moving to the capital altogether. Here’s what you said:&lt;/p&gt;&lt;p&gt;All my salary was being spent on living costs in London&lt;/p&gt; &lt;a href=\"https://www.theguardian.com/uk-news/2017/jul/31/leaving-london-means-i-can-afford-kids-readers-on-why-the-capital-lost-its-sparkle\"&gt;Continue reading...&lt;/a&gt;</description>\n      <category domain=\"https://www.theguardian.com/uk/london\">London</category>\n      <category domain=\"https://www.theguardian.com/money/family-finances\">Family finances</category>\n      <category domain=\"https://www.theguardian.com/society/housing\">Housing</category>\n      <category domain=\"https://www.theguardian.com/society/communities\">Communities</category>\n      <category domain=\"https://www.theguardian.com/money/work-and-careers\">Work &amp; careers</category>\n      <category domain=\"https://www.theguardian.com/cities/commuting\">Commuting</category>\n      <category domain=\"https://www.theguardian.com/lifeandstyle/parents-and-parenting\">Parents and parenting</category>\n      <category domain=\"https://www.theguardian.com/cities/cities\">Cities</category>\n      <category domain=\"https://www.theguardian.com/money/pay\">Pay</category>\n      <category domain=\"https://www.theguardian.com/lifeandstyle/family\">Family</category>\n      <category domain=\"https://www.theguardian.com/money/money\">Money</category>\n      <category domain=\"https://www.theguardian.com/lifeandstyle/lifeandstyle\">Life and style</category>\n      <category domain=\"https://www.theguardian.com/society/society\">Society</category>\n      <category domain=\"https://www.theguardian.com/uk/uk\">UK news</category>\n      <pubDate>Mon, 31 Jul 2017 12:43:16 GMT</pubDate>\n      <guid isPermaLink=\"false\">http://www.theguardian.com/uk-news/2017/jul/31/leaving-london-means-i-can-afford-kids-readers-on-why-the-capital-lost-its-sparkle</guid>\n      <media:content width=\"140\" url=\"https://i.guim.co.uk/img/media/33387252cc566ca74f8c0ccd4eef1f85ca81233c/0_141_3500_2102/master/3500.jpg?w=140&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=max&amp;s=b3f65cf4bdb2597a770f32e3c49cf223\">\n        <media:credit scheme=\"urn:ebu\">Photograph: Dominic Lipinski/PA</media:credit>\n      </media:content>\n      <media:content width=\"460\" url=\"https://i.guim.co.uk/img/media/33387252cc566ca74f8c0ccd4eef1f85ca81233c/0_141_3500_2102/master/3500.jpg?w=460&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=max&amp;s=0b078928a97ab43f0add6a9bf46c0b50\">\n        <media:credit scheme=\"urn:ebu\">Photograph: Dominic Lipinski/PA</media:credit>\n      </media:content>\n      <dc:creator>Rachel Banning-Lover and Guardian readers</dc:creator>\n      <dc:date>2017-07-31T12:43:16Z</dc:date>\n    </item>\n    <item> ... </item>\n    <item> ... </item> Let’s say we are only interested in building a list of items with a few of those properties, like title , description , pubDate , creator and what seems to be a list of media . Our target will be to produce a JSON response like this : [{\n\t\"title\": \"'Leaving London means I can afford kids': readers on why the capital lost its sparkle\",\n\t\"description\": \"Almost 100,000 Londoners moved out last year. Here they, and others who are avoiding the city altogether, explain why it is no longer the place to be&lt;br&gt;&lt;/p&gt;&lt;p&gt;The rate of Londoners leaving the capital is more than &lt;a href=\"https://www.theguardian.com/uk-news/2017/jul/24/bloated-london-property-prices-fuelling-exodus-from-capital\"&gt;80% higher than five years ago&lt;/a&gt;, according to Savills, with people in their thirties the age group most likely to leave. &lt;/p&gt;&lt;p&gt;We asked readers why they’re leaving London, or avoiding moving to the capital altogether. Here’s what you said:&lt;/p&gt;&lt;p&gt;All my salary was being spent on living costs in London&lt;/p&gt; &lt;a href=\"https://www.theguardian.com/uk-news/2017/jul/31/leaving-london-means-i-can-afford-kids-readers-on-why-the-capital-lost-its-sparkle\"&gt;Continue reading...&lt;/a&gt;\",\n\t\"date\": \"Mon, 31 Jul 2017 15:32:49 GMT\",\n    \"creator\": \"Rachel Banning-Lover and Guardian readers\",\n\t\"media\": [{\n\t\t\"url\": \"https://i.guim.co.uk/img/media/33387252cc566ca74f8c0ccd4eef1f85ca81233c/0_141_3500_2102/master/3500.jpg?w=140&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=max&amp;s=b3f65cf4bdb2597a770f32e3c49cf223\",\n\t\t\"credit\": \"Photograph: Dominic Lipinski/PA\"\n\t}, {\n\t\t\"url\": \"https://i.guim.co.uk/img/media/33387252cc566ca74f8c0ccd4eef1f85ca81233c/0_141_3500_2102/master/3500.jpg?w=460&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=max&amp;s=0b078928a97ab43f0add6a9bf46c0b50\",\n\t\t\"credit\": \"Photograph: Dominic Lipinski/PA\"\n\t}]\n}, \n{<item2>}, \n{<item3>},\n...\n] This is something really easy to achieve in JS: function cleanUp(data) {\n    // Empty array to add cleaned up elements to\n    const items = []\n    // We are only interested in children of the 'channel' element\n    const channel = data.rss.channel\n\n    channel.item.forEach(element => {\n        item = {\n            title: element.title,\n            description: element.description,\n            date: element.pubDate,\n            creator: element['dc:creator'],\n            media: []\n        }\n        // Iterates through all the elements named '<media:content>' extracting the info we care about\n        element['media:content'].forEach(mediaContent => {\n            item.media.push({\n                url: mediaContent.$.url,                // Parses media:content url attribute\n                credit: mediaContent['media:credit']._  // Parses media:credit tag content\n            })\n        })\n        items.push(item)\n    })\n    return items\n} Putting it all together: all the code you need for the clean up Firebase Cloud Function This is all you need to use a Firebase HTTP trigger as clean up proxy for a mobile unfriendly API. Here's the full code for this example: const functions = require('firebase-functions')\nconst URL_THE_GUARDIAN = \"https://www.theguardian.com/uk/london/rss\"\n\nconst Client = require('node-rest-client').Client\nconst client = new Client()\n\nexports.fetch = functions.https.onRequest((req, res) => {\n    client.get(URL_THE_GUARDIAN, function (data, response) {\n        const items = cleanUp(data)\n        res.status(201)\n            .type('application/json')\n            .send(items)\n    })\n})\n\nfunction cleanUp(data) {\n    // Empty array to add cleaned up elements to\n    const items = []\n    // We are only interested in children of the 'channel' element\n    const channel = data.rss.channel\n\n    channel.item.forEach(element => {\n        item = {\n            title: element.title,\n            description: element.description,\n            date: element.pubDate,\n            creator: element['dc:creator'],\n            media: []\n        }\n        // Iterates through all the elements named '<media:content>' extracting the info we care about\n        element['media:content'].forEach(mediaContent => {\n            item.media.push({\n                url: mediaContent.$.url,                // Parses media:content url attribute\n                credit: mediaContent['media:credit']._  // Parses media:credit tag content\n            })\n        })\n        items.push(item)\n    })\n    return items\n} Stay tuned! This is the first post in a new series. In the next article we’ll learn about using Firebase Database as an intermediate cache for all this data, so you can save the result and only request fresh data from the backend after a set period of time. I’d love to hear your thoughts on this, please do reach me at @lgvalle Special thanks and ❤️ to @takecare for making my JS makes sense, to @seebrock3r for making my English makes sense and to nnnneeeiil , @blundell , @niamh__power and @electryc for their reviews & suggestions.", "date": "2017-08-03"},
{"website": "Novoda", "title": "Using Sonar in Swift", "author": ["Eduardo Urso"], "link": "https://blog.novoda.com/using-sonar-in-swift/", "abstract": "At Novoda we find it extremely important to measure the efficiency of a team and the quality of our work. To do so, we need tools that help us demonstrate our capability and competence in developing trustable and powerful software, as well as analyzing the code we deliver. Code Analysis Metrics have always been a guidance tool to help those inquisitive enough to decide the future of their choices and organization. However as technology has evolved, metrics and code analysis must evolve as well. Code analysis is nothing more than inspecting and investigating the code produced, generally used to find bugs or ensure conformance to coding guidelines. When integrated into the build process it helps maintain code quality. What is Sonar? It is an open-source quality management platform, dedicated to continuously analyze and measure technical quality, from project portfolio to method. In other words, Sonar is a web-based code quality analysis tool for Java projects that can be extended with open-source plugins . In particular, it has been extended to iOS projects, thanks to the open-source community, and it supports Objective-C and Swift languages. Sonar for iOS covers a generous area of code quality check points which include: Architecture & Design Complexity Duplications Coding Rules Potential Bugs Unit Test And that is where sonar-swift comes in, an open-source initiative for Apple's programming language Swift, based on the sonar-objective-c plugin. Sonar in Swift The Swift plugin developed by Backelite has helped people to integrate Sonar in Swift projects, and I will be covering the following points which I find the most useful ones: Code coverage Code coverage is one of the measurements available in Sonarqube, that describes how many lines of your code are executed while the automated tests are running. Sonar-Swift will give you the chunk of the code that is executed, as well as allow you to drill into the data and see exactly which lines of code were and were not executed during a particular test. Having code coverage setup in your project helps to keep your code maintainable and easy to change and having a test suite that covers most or all of your application means that you will have more security that something is not going to collapse in production. Maintainability Sonar maintainability is the number of code smells plus the technical debt you have in your code according to a set of predefined rules (based on the Lint you're using). Sonar provides a simple rating for each section which allow you to quickly see how well the app being analyzed is performing. Reliability Reliability is equated to correctness, the number of bugs found and fixed, how consistent the application is in delivery and confidence in known outcomes of code routines. The ability of a system or component to perform its required functions under stated conditions for a specified period of time. Security Regarding security, Sonar covers the number of vulnerabilities that can be found in the project. The effort to fix all vulnerability issues is shown in minutes. Setting up Sonar for Swift Installing and configuring Sonar for Swift takes a few steps, running a local (or remote) Sonar server, installing the Swift plugin and then setup your Swift project. Lets run through each step now. Adding Sonar to your Swift project is easy, and I highly recommend you do so. But first you have to make sure you have a Sonar server running locally, this can be done by following the steps under the Prerequisites section in the Sonar-Swift open-source page . After installing the Sonar server locally, you need to add support for Swift in Sonar, and it can be done by downloading the latests Swift plugin from the Releases page on the Sonar Swift website. Then, move the .jar to the plugins folder where the Sonarqube server has been installed ( $SONARQUBE_HOME/extensions/plugins ). The next step is to add the Swift Sonar script run-sonar-swift.sh to somewhere in your path. If you are just running it for one Swift project I would recommend adding it to the root folder of your project. It can also be downloaded from the Releases page or you can access it directly here . Once you have done the previous steps, you should restart the Sonar server using sonar restart in the command line in order to apply the plugin and start supporting Swift. We still need to configure our project to gather data to feed Sonar but we are done with the server part. N.B. if you are having trouble installing sonar-runner , note that it has been renamed to sonar-scanner so instead use sonar-scanner . Configuring the Swift project: It’s time to configure your project to be able to generate useful reports for Sonar, and to start gathering information from the project. This is done by downloading and adding sonar-project.properties file beside the .xcodeproj file, in your Xcode project root folder. To make it work we need to update this file to match the iOS project by setting a few keys such as sonar.projectKey , sonar.projectName , sonar.swift.project , sonar.swift.workspace and sonar.swift.appScheme . Once you have completed all the steps above, you can run the script run-sonar-swift.sh and see the magic happening. It will create a folder called sonar-reports in the project directory where the reports will be stored. N.B. if you are having trouble running the run-sonar-swift.sh script make sure you give write permission to the folder called sonar-reports . Open locahost:9000 in your browser to see the Sonar results. Conclusion Software quality is key when developing a trustable and reliable product, and Sonar-Swift allows you to measure the impact of technical debt and the decisions you take during development. It is a highly recommended tool that demonstrates and generates useful metrics that will help maintain code quality throughout the entire development process. Understanding the capability and competence of the development team, while developing a complex software system, is important to be able to better plan timelines and deliverables. What is the next step? Try it out and start analyzing your code as well as generating metrics out of it. What to share knowledge on the usage of Sonar Swift? Leave me a tweet !", "date": "2017-07-11"},
{"website": "Novoda", "title": "Whole Lottie Love", "author": ["Chris Basha"], "link": "https://blog.novoda.com/whole-lottie-love/", "abstract": "I have to admit, when Lottie was initially announced by Airbnb I didn’t pay too much attention. I just skimmed the Github page without realising its true potential. Fast forward a few months to a project where we had to implement a hero animation in an app and Lottie popped into my head. So I had another look and boy, was I in for a surprise. To quickly summarise Lottie , it’s essentially a view that can play almost any animation created in After Effects. The steps are easy: you make your animations in After Effects, export it into a JSON file with a plugin called bodymovin , add that file to your app and tell Lottie to play it. Super easy and super fast. Okay, that’s cool, but why would you use Lottie? Apart from being able to reproduce intricate animations, the best thing about Lottie is that it recreates these animations on your app as vector layers, so they can scale across multiple screen sizes. And don’t forget that it’s all coming from a minuscule JSON file. So you can get rid of bulky drawables and complicated code and see even better results. Let’s take a look at some major use cases of Lottie: Big sparkly hero animations Micro interactions eg icons animating on touch or other triggers User-controlled UI elements - my personal favourite, more on this later. Let’s go through each one of those use-cases, and then follow up with some guidelines on how to design Lottie animations for specific scenarios. You can expect demos and eye-candy along the way (all the gifs in this post are screen recordings from an Android emulator). Let’s go! Hero Animations This is the most basic form of a Lottie animation. Let’s go through its setup starting with After Effects. First thing you have to do is… wait for it… create the animation on After Effects. Or ask your designer to do it. You can then export it through the bodymovin plugin into a JSON file. Now let’s jump into Android Studio and drop it into the assets folder of your project. If you don’t have an assets folder yet, make sure you add it to the root of the relevant sourceset (e.g., \"src/main\"). If Lottie isn’t set up yet, just add it normally as a dependency. In order to add the animation, use a LottieAnimationView in your layout and link it to the JSON file you just added to your asset folder, like so: <com.airbnb.lottie.LottieAnimationView\n    android:id=\"@+id/animation_view\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    app:lottie_fileName=\"hero.json\"\n    app:lottie_loop=\"false\"\n    app:lottie_autoPlay=\"true\"/> There are a few attributes to note here: lottie_fileName is obviously the name of the JSON file lottie_loop will make your animation loop (wow) lottie_autoPlay will make your animation start playing automatically when your layout is created (double wow) Try to use wrap_content as your LottieAnimationView’s width and height. It seems like the view gets clipped in unpredictable ways when setting specific dimensions but it will always scale proportionally. The designer has to make sure the animation is designed with the correct dimensions before exporting the animation. Micro interactions These are the animations that make your app feel hand-crafted and premium. You definitely don’t want to miss these. Think of icons that react to your touch, morphing shapes, task success indicators, loading loops, you name it. There have been some tools emerging that use default Android APIs to recreate these animations such as ShapeShifter , although personally I would prefer to use Lottie as it’s much simpler to use and allows more complex animations. These are implemented the same way as hero animations, except maybe you’d want them to start animating on a specific trigger. That’s easily done by calling lottieView.playAnimation(); Lottie also provides a good set of animation listeners, so you can swap JSON files, start them over, stop at a specific point or whatever you like. lottieView.addAnimatorListener(new Animator.AnimatorListener() {\n\n    @Override\n    public void onAnimationStart(Animator animation) {}\n\n    @Override\n    public void onAnimationEnd(Animator animation) {}\n\n    @Override\n    public void onAnimationCancel(Animator animation) {}\n\n    @Override\n    public void onAnimationRepeat(Animator animation) {}\n }\n}); User-controlled UI elements My favourite part of Lottie is the method called .setProgress(float) . Let me tell you why this is awesome. Views such as ViewPagers, BottomSheets, ScrollViews often offer an offset value that tells you how much the user has scrolled. How awesome would it be if you could run a certain animation based on the user’s amount of scrolling? Let me show you an example. What we’re doing there is simply setting the progress of the animation based on the amount of scroll. If the user swipes forward, the animation goes forward. If the user swipes backwards, the animation will follow. It's all synchronised with the user interactions. Let’s dive in a little deeper and separate the animation in five parts, one for each view of the ViewPager (yes, there are 5 pages in the animation above). One option is to create five different JSON files and swap them when the user reaches the limit of each page. Or, if you want to be super cool, progress the animation by 1/5 on each page. The following code will allow you to do just that: viewPager.addOnPageChangeListener(new ViewPager.OnPageChangeListener(){\n    @Override\n    public void onPageScrolled(int position, \n                               float positionOffset, \n                               int positionOffsetPixels) {\n\n        lottieView.setProgress((position * (1f / PAGE_COUNT)) + (positionOffset * (1f / PAGE_COUNT)));\n    }\n   ...\n}); And this applies for BottomSheets, ScrollViews, PullToRefresh views, basically anything that gives you an offset value. You don’t have to worry about frame rate for these animations, because even though the animation you’ve created on After Effects has a specific frame rate, Lottie creates each part of the animation on the fly. So your animation won’t appear choppy. Think of it as an SVG format for animations. There are a few things to keep in mind when creating the animation in After Effects. More on that below. As the number of pages increases, your animation will likely begin to get choppy as the progress is a float and has limited resolution. Miscellaneous Let’s talk about other useful things that Lottie allows you to do. Dynamic colouring If your branding includes multiple colouring which you have to apply to your animations, or if it’s something user generated, you don’t have to create an animation for each one of those colours. Instead, Lottie offers a colour filter API, which you can use to set a colour to your animation in real-time. Note that this is not the same as ImageView#setColorFilter() which is also available because LottieAnimationView inherits from ImageView. lottieView.addColorFilter(new PorterDuffColorFilter(Color.RED, PorterDuff.Mode.SRC_TOP)); If you're planning to dynamically tint your animation or some of its layers, make sure you draw them in a solid colour such as black or white, for optimal results. Dynamic duration & interpolation You might come across a scenario where you have to set the duration (and/or the interpolation) of your animation in code. You can do that by setting up a ValueAnimator which will run through 0 to your desired time using an interpolator, and in continuation, setting its value to the progress of your Lottie animation. ValueAnimator valueAnimator =  ValueAnimator.ofFloat(0f, 1f);\nvalueAnimator.setDuration(300);\nvalueAnimator.setInterpolator(new AccelerateDecelerateInterpolator());\nvalueAnimator.addUpdateListener(new ValueAnimator.AnimatorUpdateListener() {\n    @Override\n    public void onAnimationUpdate(ValueAnimator animation) {\n        lottieView.setProgress(animation.getAnimatedFraction());\n    }\n});\nvalueAnimator.start(); Images from asset folder & JSON files on-the-go In the \"Loading\" animation shown above, the lettering used is not a vector image but a png. You are able to images in your Lottie animation file, you can do so by adding them in your assets file and telling Lottie where to find them. lottieView.setImageAssetsFolder(\"images\"); And you don’t even have to keep your JSON files locally. If you need them to be dynamic, be it for A/B testing or if you’re frequently updating them, you can choose to download them. Cancellable compositionCancellable = LottieComposition.Factory.fromJson(getResources(), jsonObject, (composition) -> {\n    lottieView.setComposition(composition);\n    lottieView.playAnimation();\n} Design considerations We mentioned that everything Lottie draws is a vector, right? It’s a good idea as a designer to start drawing your animations in Illustrator. It integrates seamlessly with After Effects, which allows quick iteration. The size of your animation has to be in 1x (as you should be designing), but since Android screens vary it’s best to talk to your developer to find the best solution. Your timings and interpolators will be exported and reproduced just fine, but if you’re doing user-controlled animations, you might not want to include interpolations in your animations. Since they will run based on the user scrolling, the interpolations will be transferred from the offset value to your animation. This doesn’t mean that you shouldn’t make use of interpolators in such scenarios. If that’s what you’re going after, by all means go ahead. What you should expect is the animation to run normally, it just won’t be synced to the user’s interaction. Conclusion By now you might have realised that the best thing Lottie has to offer is easy reproduction of your After Effects animations with full control over their appearance. What does this mean for us? It means that we have much more freedom to create sparkles for our app with minimal work from a development perspective. It’s super fast and easy to iterate these animations, as you only have to provide a new JSON file for the developers. This also means that apps can be greatly reduced in size, as there’s no need for drawables in all the different DPI buckets. Check out the source .aep file of the animations showcased above here . I'd love to see your cool Lottie designs and implementations. Feel free to tag me or drop me a line at @BashaChris on Twitter! 👍", "date": "2017-07-13"},
{"website": "Novoda", "title": "Android Things & IoT @ Liverpool Makefest", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/android-things-iot-liverpool-makefest/", "abstract": "MakeFest Liverpool is a family-friendly event celebrating low level technologies, hardware, hacking and making. We participated again with a stand at the 2017 event and built two fun Internet of Things games to try and engage those attending and learn more about Android Things for ourselves. So here's our review on what we did and what we learnt. Three of us attended from Novoda. It was a crazy busy day with lots of people visiting us on our stand. Everyone had a wide range of knowledge, which made for some great conversations. People played the games and were really interested in what Android Things is all about. Most attendees of MakeFest were families with children aged 7–17, which is why we created IoT games to spark their interest. Attending MakeFest had two aims for us: We love helping the community, and seeing young people engaging with technology is a real inspiration. We enjoy helping to spark creative ideas and showing how easy it can be to get into technology. Secondly, we are always learning and trying to push the boundaries of our own knowledge. Working with hardware is not a daily occurrence yet for most of our projects, so MakeFest really pushed us to learn new things. Our first project was a game based on the idea of Guitar Hero but using a piano. We used Android Things as our IoT platform, which meant we had to research how to connect an electronic piano (midi controller) with a Raspberry Pi 3. Our other idea for MakeFest was based around interactivity. We wanted people to come and engage with the stand but we also wanted to iterate on our ideas from Makefest 2016. So we re-made the Simon Says game we created in 2016 . This time using Android Things and hardware peripherals rather than a gamepad. Simon Says displays a sequence of colours on four LEDs, which then has to be repeated on four hardware buttons. This was all soldered together onto a prototyping board, including a buzzer to make game sounds to help players audibly distinguish between the patterns. Our game winners this year each won an Android Things Rainbow Hat & case , which, when added to a Raspberry Pi 3, has everything needed to get started programming and hacking Android Things and similar IoT platforms. We hope our winners enjoy using them to make something awesome. I thought the variety of people and activities was fantastic, and I was particularly impressed by the number of young girls getting involved in the technical side - made even better by the prominence of the ‘Women in Tech’ area. I’m really looking forward to attending again next year to see how it grows and to help inspire more kids to come into the creative space. Niamh It was the first maker event I’ve attended, and it was really fun. I had the opportunity to see all the creativity other makers put into their work, and see how the children were mesmerised by our games, even the retro ones like Simon Says. I hope one day they will take the maker path and build something themselves. Eduard I always enjoy MakeFest, seeing all the kids in awe of the games we make reminds me what a fun job I have, and how it all seems like magic when you don't live through the sweat and tears. It also inspires me to share more and bring more young people into the creative industries. Paul We can't wait for next year. Each year we learn more and want to make it more awesome next time. We'll have to think of ways to get everyone interacting even more and learning whilst playing. Stay tuned!", "date": "2017-07-04"},
{"website": "Novoda", "title": "The importance of measuring large-scale impact", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-importance-of-measuring-large-scale-impact/", "abstract": "The best Product Owners are those able to see the bigger picture. They analyse growing markets, identify opportunities and set the trajectory for business growth by harnessing domain knowledge to guide a successful product strategy. A good Product Owner keeps ahead of the market to deliver the right solutions at the right time for the maximum impact. Arguably a Product Owner's most important role is to create a strategy based on business intelligence thus producing more realistic goals. The ability to understand the practical opportunity and brief a product team with well-founded goals, realistic success criteria and clearly defined milestones is an invaluable skill. Set clear goals Definitive goals and success metrics allow teams to deliver solutions through a shared focus. Taking a data-driven approach to product releases allows well-considered analytics to measure and conclude large-scale business impact and changes in user behaviour. Stay in control A gradual roll-out to market enables low-risk releases that can be distributed further as data significance is reached and confidence in positive impact is achieved. Success metrics can be monitored to ensure continuity of impact post-release. This approach, providing that the control group (original version) is maintained, allows the impact of multiple instances to be measured and compared for better-informed product decisions. Iterate and improve In some instances, large-scale insights can challenge the results of usability reviews and consequent behaviour predictions. This is because usability testing sessions often result in greater levels of consideration and patience in comparison to ‘real world’ usage. In the cases where data suggests surprising results or negative impact on success metrics then a release can be rolled-back or iterated upon for additional review. What out for inferred effects When focussing on a specific success metric, it’s important to measure how product changes might impact user behaviour across the experience. For instance, if your goal is to increase the number of signed in users then you may decide to include a login prompt at the start of the experience. This may increase your metric but could have significant effect on total number of new users converting. An effort to consider and measure inferred effects will increase awareness of potential impact on user behaviour and other business metrics. Celebrate a data-driven team A data-driven approach to product development allows team members to see the direct effect of their product changes on the business. Reviewing success metrics regularly increases transparency, encourages a more data-driven culture and provides the opportunity to celebrate successes. Controlled releases allow the team to learn and react to smaller-scale insights, providing a feeling of control and shared ownership. A strong Product Owner leads business growth on well-founded goals, communicating clear success metrics to the team, encouraging a shared focus with freedom to achieve. They lead a transparent, controlled approach to releases, reducing risk and increasing awareness of business impact among stakeholders. With the opportunity to celebrate success, this approach makes for happy teams producing high-impact products.", "date": "2017-06-22"},
{"website": "Novoda", "title": "Building natural dialogues for your voice assistant", "author": ["Alex Styl"], "link": "https://blog.novoda.com/designing-vuis/", "abstract": "My esteemed colleague Flo and I recently worked on a television guide bot using api.ai + Amazon's AWS backend service to experiment with the capabilities of Voice User Interfaces (VUI). In this blog post I’ve covered the design process we followed to enable voice interfaces in an existing product and how we used api.ai to build natural dialogues. Defining the features Our aim was to explore what it takes to enable voice capabilities in an existing product and find out what we can do with it. There are some great design resources on VUIs, such as Google's Crafting a Conversation , but there is no mention of integrating voice with an existing product. Having clear information architecture for your project will help you create relevant user journeys that you want your assistant to support, without having to worry about platform-specific navigation patterns. Visual vs audio The main difference between a visual interface and an audio one is that dialogues are a single, linear communication channel. Here is an example on the current Netflix app compared to a potential audio assistant: How long does it take to get to all possible features of the GUI vs the VUI? It might take a split second to scan a screen and get a good idea of what to do next. But explaining every action verbally can take much longer. Dialogues need to be short and clear so the user will not lose interest and walk away. On the other hand, voice input can be simpler and more direct than touch. When you need something you just speak your mind. You don't have to worry about finding the feature you need on the screen, navigating around the app, waiting for animations to end and so on. It’s easier and more natural to say \"Send a message to Mum: I’m writing a blog post\", instead of finding the 'New Message' icon, typing Mum's number, tapping the text input, typing the message and hitting the send button. Vocabulary and branded words In the field of computer science there are many metaphors and synonyms already being used (a computer can have a mouse , keep information on its memory , Material design talks about the metaphor of paper , the list goes on). It’s likely that you’re already using several metaphors in your own apps and systems to represent multiple features with a single word. All 4's 'Catchup' feature , for example, allows the users to see which episodes aired in the past so they can find any updates from the shows they’re following. But how can the user be expected to know this feature exists, let alone know the correct name? How do you teach people detailed vocabulary or should this only be available for ‘power’ users? It’s worth creating alternative ways of prompting such features rather than expecting the user to know the branded word (in this case Catchup ). Think about how someone would describe that feature conversationally. This can help when thinking about how a user would ask the chatbot for information such as \" What happened in the latest episode of The Big Bang Theory? \". Designing dialogues for old and new users Many users may not have used a voice assistant before, so you need to guide them through the process. One way to do that is through audio onboarding. A good way to approach this is with an introductory conversation with first time users. Let the user know what the assistant can do and how the user can access the features. Teach your user what to do if they’re not sure how to move forward. Let them know if they’re stuck at any given point they can ask for help. Remember, there are no visual hints at this point. You can’t expect a user to know what they can do without giving them hints. In terms of experienced or power users, things need to move a bit faster. They already know the ins and outs of your assistant so they know how to act and what to say. For these users, you might want to include voice commands instead of dialogues, removing hints or guidance on features. For example a new user would be more likely to speak naturally eg \"Could you please tell me more about X\", but an experienced user may just ask for \"Details\". Flexible scenarios via api.ai When implementing your scripts into code, you need to consider which part of the discussion your inputs will be in. A phrase such as \"Tell me more about X\" is something a user could say at the beginning or in the middle of a conversation. There may be several variations of that question, such as \"I would like to know more information about X\" or the user may not refer to the show by its full name or use “it”. Luckily, all those scenarios can be easily handled with api.ai. The way input dialogues are defined in api.ai is done through intents . These are the questions the user can ask the chatbot, the chatbot’s responses back to the user and how those two statements are connected. What is really cool about api.ai is that you can define contexts around those intents. This means that if you start a conversation about a show, the bot will be able to remember which show you are talking about if you use the word \"it\" ('Tell me more about it ', or 'Remind me to watch it ?'). Lastly, api.ai makes it really easy to create flexible conversations without having to tie specific parts of the discussion together. By creating two versions of the same intent, one that requires a context of discussion and one that doesn't, you can place the intent at any point in the conversation. Voice is not just for assistants Even though voice interfaces are often associated with assistants such as Google Home or Amazon Alexa, this is just the tip of the iceberg in terms of possible applications for this medium. How can your existing application be enhanced by giving it voice capabilities? Maybe your app could allow users to listen to news on-the-go, or allow the user to navigate through content by verbally telling the app which sections they are interested in? How about replacing annoying guide screens with a chatbot that informs the user of potential ways of interacting with the app instead? You could even combine different technological  mediums together? A great example of this is Starship Commander , a virtual reality game, which allows the player to control the narrative of the story by talking to the in-game characters. If you’re interested in tinkering with Arduino or Android Things then why not build a robot and give it a personality through conversation? ##### In Starship Commander, the player is able to control the narrative by talking to in-game characters Conclusion When designing dialogues, take into account the time it takes to say the words out loud and consider how direct the voice input might be. It’s advisable to cater for new and experienced users with input phrases and commands that will be meaningful to both groups. Working on a VUI project can be fun and interesting from a design and a development perspective. I particularly enjoyed working with api.ai on this as it provides the freedom to create flexible scenarios. I’m looking forward to exploring how to improve the discoverability of features through dialogue structure amongst other techniques. If you are interested in building your own voice assistant, Fransesco 's blog on Everything Assistant at Google Developer Days 2017 would be a great place to get started. Listing image by Jason Rosewell", "date": "2017-06-29"},
{"website": "Novoda", "title": "CodeMobile 2017 - the lowdown", "author": ["Niamh Power (iOS & Android Software Crafter)"], "link": "https://blog.novoda.com/codemobile-2017-the-lowdown/", "abstract": "Here’s what I learnt from CodeMobile, Chester’s very own mobile developer conference. Last month I had the pleasure of attending the inaugural CodeMobile conference for mobile developers in Chester, UK. Having lived in the north west of England for most of my life, it was brilliant to see a conference located outside London that was attracting speakers from such high profile companies as Google, Facebook and Instagram. Here’s a summary of the topics covered and a few of my thoughts on the three-day conference. Laurence Moroney - Google One of the more high profile talks, ‘Firebase 2017: Build, Grow and Learn’ , came from Laurence Moroney , a developer advocate at Google, specialising in Firebase and Google Maps. The talk focused on Firebase, a tool I am relatively new to. I’ve recently completed the Udacity course in Firebase for both Android and iOS , so it was inspiring to hear from the course leader in person. One of the more startling statistics that was presented was the number of developers earning below the ‘poverty line’ from their apps - 39% are earning less than US$100 a month. It was interesting to see how they tackled this with Firebase, how this influenced their strategy and how that evolved after Google's acquisition of Firebase in 2015. By providing more detailed insights into user behaviours, as well as advertising revenue and being able to target this to certain audiences, developers can now have a better understanding of how their app is performing, and better target areas for improvement. Find out more about the latest Firebase features and how to get started here . Heather Burns - Web Matters A stand-out talk for me was from Heather Burns, ‘Mobile Development and the Law’ . Understanding data protection and relevant international laws is vital when developing large scale applications. I found it interesting to learn that in the US 'free speech' is considered a right, however 'data protection' is a commodity. Privacy is ‘seen as an impediment to free speech’ which seems like an alien concept on this side of the Atlantic. The main topic of the talk covered the new EU Data Protection Act, GDPR . This will replace the Data Protection Act and will come into force on 25 May 2018. 'What about Brexit?' you may be wondering, but Heather advised us that the UK government have committed to go into GDPR regardless of Brexit. However, the uncertainty of the next few years does pose some questions. What if the UK government decide to ditch GDPR? This would invalidate any business with European users, as the law requires any third party country to have an equal standard of data protection law in place. Heather's recommendation was to stick with the requirements of GDPR, no matter what, as this would guarantee your ability to have EU users use your app with all its intended functionality. Lightning Talks One of the great features of the conference was the lightning talks. These were five-minute talks given by a range of speakers over an hour period. It was a great way to keep the audience interested and involved - and also posed a challenge to the speakers as the time-keeping was very strict. A highlight for me was Michael May's talk 'Cats and Dogs' , which focused on the similarities and differences between Swift and Kotlin. This is even more relevant now that Google has adopted Kotlin as an official language for Android. Overall, I felt the conference was a great success, as the first of its kind in the area. I’m hopeful that more events like this start appearing and I’m already looking forward to CodeMobile 2018 . Images sourced from Code Mobile", "date": "2017-06-20"},
{"website": "Novoda", "title": "Custom authentication with Amazon Cognito and Firebase", "author": ["Florian Mierzejewski"], "link": "https://blog.novoda.com/custom-authentication-with-amazon-cognito-and-firebase/", "abstract": "As developers, we are always trying to move our technology knowledge forward and at Novoda, we are especially interested in BFF ( backend for frontend ) solutions to have some abilities on the \"backend in the cloud\" front. In this blog post we investigate one of the main points of concern: authentication services, and more generally, how to adjust our authentication stack to fit a mobile first approach. When it comes to hosting your backend, the two main alternatives are Amazon Web Service (AWS) and Google Cloud Platform (GCP). Both provide out-of-the-box support for authentication, respectively via Cognito and via Firebase Authentication . They both can handle username / password based authentication, social logins (Facebook, Twitter, Google...) as well as custom authentication. Here we focus on the latter as this is the scenario you would encounter when transitioning to BFF. Diving into the samples We started our journey by forking two samples from Amazon, client and authentication server . They have an extensive post on how this works over here . The server sample The Amazon sample needs to be deployed into a Tomcat environment. We converted it into a Spring Boot application deployable into any Java8 environment. In that process, we also introduced Gradle to build the project. The original sample provides three endpoints, registration of new users, user login and cognito token provision. We added a fourth one to also provide a firebase token. There is also a JSP web interface to register new users. The result can be found here . The flow can be schematized like this: We use the web interface of the authentication server to register users in the database (left, red). We can then log in from the Android client to the authentication server that will in turn check our credentials against the database (center, blue). Once logged in we can ask the authentication server to fetch tokens from Cognito (left, purple) and Firebase (right, orange). The client can use them to access resources restricted to authenticated users from those services (respectively an Amazon Lambda and the Firebase realtime database). The client sample We removed all code not related to developer authentication from the original sample. Moreover, we revisited the package structure, streamlined the code, reworked the UI, and added access to a restricted AWS lambda. Finally, we added some logic to fetch a firebase token from our server and use it to access a restricted firebase database. This is our revamped sample Android client: The Login button allows you to log in to your authentication server by asking for the username / password you gave when registering a user through the web interface. The two following buttons are going to ask the authentication server to fetch a token from the corresponding service (if not already cached) and try to access a restricted resource (a lambda or the firebase database). Wipe data will remove everything that might have been cached and log you out of both services. Let’s now dive a bit more into each service and see how the authentication flow work. Amazon Cognito Cognito is an Amazon service that lets you add user sign-up and sign-in to your mobile applications. Using Amazon Cognito Federated Identities you can also authenticate users through social identity providers or use your own back-end authentication system. With a federated identity, you can obtain temporary, limited-privilege AWS credentials to synchronize data with Amazon Cognito Sync or to securely access other AWS services such as DynamoDB, S3, API Gateway and Lambda. We log into our authentication server We can now ask for Cognito credentials that allow us to access AWS restricted resource (in our case a lambda). The device initiates a request for the credentials (login) to our authentication server Our authentication server call GetOpenIdTokenForDeveloperIdentity which will register (or retrieve if already existing) a Cognito IdentityId and an OpenID Connect token based on an UID (a string that uniquely identify the user or device). Whenever the device want to access an AWS resource, the Amazon SDK will retrieve the credentials we need directly from Cognito. Now that we are authenticated we can access our lambda named CognitoAuthTest as the authenticated role allows access to this resource as seen below: {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n         [...]\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n               \"arn:aws:lambda:REGION:ACCOUNT_ID:function:CognitoAuthTest\"\n            ]\n        }\n    ]\n} Using the Amazon SDK we can then call this lambda on the client: LambdaInvokerFactory factory = new LambdaInvokerFactory(context, region, credentialsProvider);\nLambdaInterface lambdaAccess = factory.build(LambdaInterface.class);\nString result = lambdaAccess.readHiddenText(); With LambdaInterface being interface LambdaInterface {\n     @LambdaFunction(functionName = \"CognitoAuthTest\")\n     String readHiddenText();\n} And check the response to make sure we are actually logged into Cognito. Firebase Firebase Authentication supports authentication using passwords, popular federated identity providers like Google, Facebook and can also be easily integrated with a custom backend. As for the authentication flow it is a bit simpler than Cognito, we request a Firebase token at our authentication server which, with just one API call, retrieves it from Firebase using a UID that uniquely identifies the user or device you are authenticating: FirebaseAuth.getInstance().createCustomToken(uid)\n    .addOnSuccessListener(customToken -> {\n        // Send token back to client\n    }); And return it to the client. Client-side we then need to sign in using this custom token: FirebaseAuth.getInstance().signInWithCustomToken(token)\n    .addOnCompleteListener(new OnCompleteListener<AuthResult>() {\n        @Override\n        public void onComplete(@NonNull Task<AuthResult> task) {\n            if (task.isSuccessful()) {\n                // success\n            } else {\n                // error\n            }\n        }\n    }); One call on the authentication server, one call on the client and everything else is handled internally. To verify we are indeed logged in we try to access a database reference which by default is accessible only by authenticated users. final FirebaseDatabase database = FirebaseDatabase.getInstance();\nfinal DatabaseReference myRef = database.getReference(\"test_reading\");\nmyRef.addValueEventListener(new ValueEventListener() {\n    @Override\n    public void onDataChange(DataSnapshot dataSnapshot) {\n        myRef.removeEventListener(this);\n        String value = dataSnapshot.getValue().toString();\n    }\n\n    @Override\n    public void onCancelled(DatabaseError databaseError) {\n        DatabaseException databaseException = databaseError.toException();\n        Log.e(\"FirebaseResourceAccess\", \"accessing resource failed!\", databaseException);\n    }\n}); So what should I choose? When considering pro and cons we can talk about the services offered and ease of implementation. With Cognito you get access to all the Amazon stack and especially Lambda which are only beta on Google side. Pretty much every other Amazon service has a Google equivalent. With Firebase you get access to all their stack which target particularly mobile platforms and give you a realtime database, cloud messaging, analytics… You need to keep in mind though that most of these services would only work on devices that have Google Play Services installed which exclude Amazon devices for instance (more information here ). Cognito is pretty easy to integrate server side but is harder on the client side. You need to inherit from AWSAbstractCognitoDeveloperIdentityProvider and provide a way to retrieve the Cognito token from your server. On the other hand, Firebase Authentication is really trivial to implement, one call on the server, one on the client. Everything else is handled by the sdk. We didn’t test Firebase UI but it looks like a quite nice solution if you don’t want to have to deal directly with the really complex authentication flows of most social services. Something to also take in consideration is that we didn't take pricing into account in our comparison and Amazon mobile SDKs are open while Firebase is a gigantic black box. Wait, do I even need to choose? All in all and as always this depends on the stack you use or the one you want to use. Amazon being more popular in this area you’ll probably choose Cognito to integrate with Lambda and API Gateway for the API side of things and with Firebase for analytics, remote configs, cloud messaging and the real time database.", "date": "2017-06-14"},
{"website": "Novoda", "title": "Testing Talkback in isolation with Espresso", "author": ["Jacek Szmelter"], "link": "https://blog.novoda.com/testing-talkback-in-isolation-with-espresso/", "abstract": "You can easily detect if TalkBack is enabled when binding views. You can then change their behaviour to make interaction with elements more natural. This blogpost explains how to test this functionality. The biggest problem with testing custom behaviour with TalkBack is time. You turn it on, navigate to the app, ensure navigation between elements works as expected, check elements have the correct content descriptions and usage hints (with different data sets) and finally turn it off. It can be difficult to unit test this behaviour as it relies heavily on Android framework classes. You could also use Espresso to run tests on real devices, but the problem still remains with the setup and running of the tests. We noticed this problem in some of our projects, so we wrote a simple library that is able to turn TalkBack on, run Espresso tests and turn it off after each test. System under test So let's imagine you have a TweetView with multiple actions represented by different buttons. The TweetView hides these buttons when TalkBack is enabled, but the user can choose any of the actions from a dialog, which appears after clicking on the view. public void bind(final Tweet tweet, final TweetActions tweetActions) {\n    authorTextView.setText(tweet.author);\n    summaryTextView.setText(tweet.summary);\n    setContentDescription(tweet.author + \", \" + tweet.summary);\n \n    if (accessibilityServices.isSpokenFeedbackEnabled()) {\n        buttonsView.setVisibility(GONE);\n \n         setOnClickListener(new OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                createActionsDialog(tweetActions);\n            }\n        });\n \n        setOnLongClickListener(\n            detailsAction.asLongClickListener()\n        );\n    ...\n} On top of that, you can add custom click and long click labels. usageHintsAccessibilityDelegate.setClickLabel(\n    R.string.tweet_click_usage_hint\n);\nusageHintsAccessibilityDelegate.setLongClickLabel(\n    R.string.tweet_long_click_usage_hint\n);\n \nViewCompat.setAccessibilityDelegate(\n    this, \n    usageHintsAccessibilityDelegate\n); It’s important that you test that everything behaves as expected when TalkBack is disabled. It’s worth reading our previous blogpost on testing views in isolation with Espresso to help you setup the library and add your base tests. When TalkBack is enabled, you can test: the buttons are hidden action dialog appears when clicking the view each action triggers its corresponding callback the content description and usage hints are set correctly Addition setup for TalkBack Toggling TalkBack state requires the WRITE_SECURE_SETTINGS permission, so you need to grant it to your app. First, you need to install the app, then you can run this command: adb shell pm grant $PACKAGE_NAME android.permission.WRITE_SECURE_SETTINGS Just remember to change $PACKAGE_NAME to your own. Once the app is installed and the permission is granted you can easily toggle TalkBack in two ways: via adb by running these actions: $ adb shell am start -a \"com.novoda.espresso.ENABLE_TALKBACK\"\n$ adb shell am start -a \"com.novoda.espresso.DISABLE_TALKBACK\" programmatically by starting an Intent : Intent intent = new Intent(\"com.novoda.espresso.DISABLE_TALKBACK\"); // or ENABLE_TALKBACK \ncontext.startActivity(intent); Tests So let's create a TweetViewTalkBackTest in androidTest with TalkBackViewTestRule . @RunWith(AndroidJUnit4.class)\n@LargeTest\n@RequiresApi(api = Build.VERSION_CODES.LOLLIPOP)\npublic class TweetViewTalkBackTest {\n \n    @Rule\n    public ViewTestRule<TweetView> viewTestRule = \n        new TalkBackViewTestRule<>(R.layout.view_tweet_view); This first test will check if buttons are actually hidden. @Test\npublic void whenBoundTweetView_thenButtonsAreHidden() {\n    bindTweetView();\n \n    onView(withId(R.id.tweet_buttons_view))\n        .check(matches(withEffectiveVisibility(Visibility.GONE)));\n}\n \nprivate void bindTweetView( {\n    viewTestRule.bindViewUsing(new ViewTestRule.Binder<TweetView>() {\n        @Override\n        public void bind(TweetView view) {\n            view.bind(TEST_TWEET, tweetActions);\n        }\n    });\n} Then you can check if clicking on a View displays the ActionDialog and has all required actions. @Test\npublic void whenClicking_thenDisplaysDialogWithAllActions() {\n    onView(withClassName(is(TweetView.class.getName())))\n        .perform(click());\n \n    checkViewsWithTextDisplayed(\n        R.string.tweet_action_details,\n        R.string.tweet_action_reply,\n        R.string.tweet_action_retweet,\n        R.string.tweet_action_like\n    );\n}\n \nprivate void checkViewsWithTextDisplayed(int... ids) {\n    for (int id : ids) {\n        onView(withText(id))\n            .check(matches(isDisplayed()));\n    } After that, you can make sure that clicking particular actions triggers the correct callback. @Test\npublic void whenClickingDetails_ThenOpensDetails() {\n    displayActionDialog();\n \n    onView(withText(R.string.tweet_action_details))\n        .perform(click());\n \n    verify(tweetActions).onDetails(TEST_TWEET);\n} Finally, you can test if the content description and usage hints have been set up the way you want them: @Test\npublic void whenBoundTweetView_thenHasCorrectContentDescription() {\n    bindTweetView();\n \n    onView(withClassName(is(TweetView.class.getName())))\n            .check(matches(withContentDescription(\n                TWEET_AUTHOR + \", \" + TWEET_SUMMARY)));\n}\n \n@Test\npublic void whenBoundTweetView_thenHasCustomUsageHint() {\n    bindTweetView();\n \n    onView(withClassName(is(TweetView.class.getName())))\n        .check(matches(withUsageHintOnClick(\"See actions\")));\n}\n \n@Test\npublic void whenBoundTweetView_thenHasLongClickCustomUsageHint() {\n    bindTweetView();\n \n    onView(withClassName(is(TweetView.class.getName())))\n        .check(matches(withUsageHintOnLongClick(\"Open details\")));\n} And that's it 🎉 . Our QA team loves this tool, so let us know how it works for you!", "date": "2017-06-13"},
{"website": "Novoda", "title": "Scribbling your way through app design", "author": ["Dave Clements"], "link": "https://blog.novoda.com/scribbling-your-way-through-app-design/", "abstract": "It is the most useful tool a designer can possess, and yet designers are still jumping into digital tools before they consider a classic combo… The pencil and sketchbook. As Novoda's creative lead, it’s not my job to dictate which tools the design team uses to create their masterpieces. It’s up to the team to democratically decide how they wish to communicate their ideas. But everyone on the team agrees that designing at a paper level is absolutely essential. Scribbles , as we refer to them at Novoda – to ensure we don't confuse sketches with the Sketch app we use for visual design – are the fastest, simplest way to put together prototypes for reviewing and testing. In fact, we often begin development of native apps using scribbles, sliced up into .png files, as the basis for the developers. The beauty of working at this level of speed is that we can discover new ideas, fail fast (a buzz phrase for sure, but an essential part of the process) and validate UX concepts with customers at an early stage. Skipping this phase (as some designers are still doing) can drastically increase your time spent figuring out the right design, costing your client or your startup a lot of time and money. Delaying shipping because you’re spending time fussing over the visual aspects of a design before you know if the customer can even use the product, is shortsighted and will often result in a less usable product. That is a disastrous situation, that will ultimately lose you customers. The pencil is a designer’s trusty sidekick. One that travels with you wherever you go, and always has your back. My favourite scribbling tools My preferred tools are listed below. I would never be without them and I have a set of these tools at home, in the office and carried in my bag every day. Staedtler 3B Pencil http://amzn.to/2a3Wwqz My favourite pencil to draw with is the 3B from Staedtler. Cheap, solid, and the 3B softness is easily erased but dark enough to photograph well for sharing with others on Basecamp. Derwent A4 Sketchbook http://amzn.to/2ahYuGX There’s really very little that is unique about this sketchbook and there are many other similarly-priced brands available. The thing I love about it is the 110gsm cartridge paper. Thick enough to have a satisfying feel, but not so thick you feel precious about scribbling quickly - a good balance. It works as well with pen as it does with pencil. Uni Pin Fine line 0.3 Pens http://amzn.to/2a4CsnR These pens feel great when drawing and I often use them to draw the final line over my pencil work so it stands out in photos. Some people enjoy working with different thicknesses, and I do sometimes switch to 0.2 or even 0.1 for a finer line. UI Stencils Pixel Ruler http://bit.ly/pixelsruler This is a great tool. No more using millimetres on a ruler to do measurements. Ok, I know this is utterly non-essential, but it really is a handy tool and I find myself using it daily. Mainly to ensure all my drawings have a fundamentally similar size so when they’re easy to work with when I slice them for development prototypes. Plus it’s super cool and looks great on your desk. Dot Grid Sketchbook http://amzn.to/2ahYTJN This is a bit of a luxury to be fair. A £22.99 (With free UK delivery!) sketchbook is quite over the top and many other grid sketchbooks exist for far less. But I love this book for working on the detailing of small icon designs or lettering ideas that I want to see on a grid. “But I can’t draw!” So what? An ability to draw beautifully is not a requirement when scribbling down wireframes. None of our scribbles are works of art. In fact in some cases, they can be an absolute monstrosity of pencil or Sharpie lines criss-crossing over themselves. The only thing we ensure with our scribbles, is that the message of the journey is clear. A button looks somewhat like a button ie it’s a box or circle with perhaps a scribble of an icon or text, or maybe we’ll surround it with a coloured pen. They don’t always end up being the concepts we go with, which is why we scribble. To ensure that if we put a button in the wrong place we have only wasted moments of our time, rather than hours or days . Just have fun The great thing about scribbling your ideas is that it's fun. This is personally my favourite part of the design process. It's the place we have the most freedom to quickly try new ideas and collaborating over a whiteboard or sketchbook is really satisfying. Make the most of this step, for it is here that you will be at your most creative and form the basis of what will become your successful product.", "date": "2017-06-08"},
{"website": "Novoda", "title": "Mapping the agile manifesto", "author": ["Fran Avila"], "link": "https://blog.novoda.com/mapping-the-agile-manifesto/", "abstract": "A handy guide to help you navigate the wonderful world of agile practices, principles and values. I was wondering how I could enable the teams that I work with to make the best decisions on which agile methodologies they utilise. I thought it could be helpful to create a map detailing some of the principal agile practices for teams to check their approach against. I decided to create a map that would link agile practices and principles with Scrum values within the context of the agile manifesto. Linking all these elements to the values of my team, department and company could help to improve day to day communications, which is vital for effective teams. This map can be used in retrospectives and meetings to provide a focal point and stimulate useful debate. The map represents the relationship between a practice, a principle, a value and an element of the agile manifesto. Each practice connects with more than one principle, value and element, which can be quite complex, so I simplified it as much as possible. Note: The principles are colour-coded to connect them to corresponding agile principles and manifesto elements. The practices are positioned alongside related principles. I have made the principles of ‘technical excellence’ and ‘business and developers daily collaboration’ larger than the others to show their weight and importance. I’d love to hear any suggestions you have to improve the map. It could also be used in team exercises, eg the team could decide how to connect the elements on a black and white version of the map. This could promote discussions on values and principles, and how to put them into practice. You can also pick a practice, describe it to the team and then have an open discussion about which principles, values and topics within the agile manifesto relate to this practice. This works best with a black and white version of the map so people can draw their own conclusions about the connections. Last but not least, this document contains a list of agile practices, principles, values and manifesto . It shows the relationship between these and includes interesting links describing each point. If you use it, we’d love to hear your team's experiences.", "date": "2017-06-06"},
{"website": "Novoda", "title": "Accessibility Sessions at Google I/O 2017", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/accessibility-sessions-at-google-io-2017/", "abstract": "There were three recorded accessibility sessions at Google I/O this year, with three additional \"office hours\" with Google's accessibility experts for those lucky enough to attend. Read on for a summary of this year's events. What's New in Android Accessibility First up was the staple \"What's New in Android Accessibility\" , introduced by the product manager for accessibility, Patrick Clary. He was joined on stage by Maya Ben Ari (project manager on Android accessibility) and Victor Tsaran (technical program manager) as well as Astrid Weber (UX research lead) and Melissa Barnhart (UX researcher on Android). Patrick did the typical introduction to accessibility in general and specifically on Android. Victor showcased the new user-facing features from the accessibility team, including: accessibility volume - made possible by new audio stream on Android, separate from the media stream fingerprint sensor gestures (used by accessibility services on supported devices as an alternative means of user input) multilingual support for text-to-speech engine accessibility shortcut - toggle accessibility service (configurable, defaults to TalkBack) with long-press of both volume keys Maya talked about new APIs: continuous gesture API - it wasn't clear to me how this would be used. It looked like APIs supporting gestures apart from clicks, like drag/swipe/pinch-to-zoom accessibility button - this is different from the accessibility shortcut, allowing users access to a feature within an enabled accessibility service, rather than just toggling the service on and off Select to Speak service - like TalkBack but the user must choose explicitly which part of the screen to have read aloud. Suitable for low-vision users. Testing accessibility - includes Accessibility Scanner and Espresso/Robolectric automatic tests Finally Astrid and Melissa spoke about user testing (UX research methods) for accessibility: definition of UX research - change your perspective to that of your users research methods - usability studies and intercepts (I think this is a euphemism for snaring unsuspecting participants for a quick Q & A) outcomes - some conclusions based on the research Even though this was a high-level session, I was disappointed, frustrated but not altogether unsurprised at the lack of technical examples. When I downloaded the developer preview of O to try the new accessibility APIs, I opened a few tickets (unsure if there were bugs or if I was using the APIs incorrectly) but they were all closed. I can't see the point in adding new APIs if there's no easy-to-follow documentation; there are people who are interested in developing for accessibility, and there are people who can use the limited documentation available, but the overlap of these two groups is very small. Takeaways: the fingerprint sensor gestures look cool - it would be great if someone developed a service that could emulate the old trackballs the biggest user benefit will be the accessibility volume (separate from media volume) and multilingual support which can switch between languages when reading aloud with regards to testing accessibility, focus on manual testing and (when you get confident) trying to automate these tests. Don't rely on the \"automatic\" tests from Accessibility Scanner or the additions to Espresso/Robolectric, but use them as a starting point to highlight issues. Accessibility UX Insights: Designing for the Next Billion Users Astrid from the initial session and Nithya Sambasivan (UX researcher) present the NBU (Next Billion Users) Accessibility Framework . This was pretty interesting, it felt like a TED talk. If you're just after the actions though, you can skip forward to the summary . Pragmatic Accessibility: A How-To Guide for Teams This was my favourite session, with Rob Dodson presenting Accessibility for Teams . It's one of the best talks I've seen on accessibility (and in general). One of the points Rob mentioned is something I cannot emphasise enough - just because something is accessible , it's not necessarily usable . He suggests different responsibilities based on your role in the team: Product managers UX designers Developers Takeaways: accessibility is a team effort - everyone has a role create checklists - useful for new projects and legacy projects find ways to automate your tests (blogpost coming soon for Android from Novoda) watch the whole video ! And that's all the accessibility related news from Google I/O 2017, let me know what you think on Twitter .", "date": "2017-05-26"},
{"website": "Novoda", "title": "If I learnt one thing from Google I/O '17 it'd be ...", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/if-i-learnt-one-thing-from-google-i-o-17-itd-be-2/", "abstract": "Novoda recently attended Google I/O in force with five of our engineers making the trip to Mountain View in sunny California. With over 150 sessions, and a lot of announcements of new tech, what were our key takeaways? There was a lot happening at Google I/O: three jam-packed days of sessions, five sessions in each time slot and nine hours in the day - there was a lot to pack in. If you missed it and would like to get an insight into some of the key learnings and our recommendations, then read on. [1] Android Things is here to stay Android Things is still in developer preview , however, there was a great session on creating your own hardware and shipping Android Things to production. This shows Google is really behind Android Things and wants to help it succeed. The session is awesome in linking to relevant technologies about creating your own electronic circuit boards and getting a handle on what is involved in creating Internet of Things hardware. Check out the Google I/O session here: Bringing Device Production to Everyone With Android Things . Paul Blundell Big focus on testing Every year at Google I/O, the focus on testing is increasing. This year it was no different. Newly announced Architecture Components libraries enable decoupling components from the activity. This is achieved by making components lifecycle-aware . That means components can be unit tested in isolation much more easily. There were also updates on Espresso. With Android Test Orchestrator , every Android UI test is now run in isolation. This has 2 main benefits: No shared memory and state between tests A crash in one test does not crash the whole test suite Check out the Google I/O sessions here: Architecture Components - Introduction Test-Driven Development on Android with the Android Testing Support Library Said Tahsin Dane Data available at a glance with Android Wear Complications support was introduced with Android Wear 2.0 . It's a way to conveniently display data directly on the user's watchface, and now Google has released new components and tools to test this. Thanks to the Complications API , every app can now present its own data in a glanceable way on the user's favourite watch face, matching the face design. Google has announced some new components that make it even easier to draw any kind of complication data, and to test that the supported data is displayed correctly on the watch face. A lot of awesome effort has been put into creating UI components that developers can use to provide the best user experience for the peculiar form factor of a smartwatch. Check out the session here Android Wear UI Development Best Practice Daniele Bonaldo Firebase Cloud Functions opens the gate to Machine Learning in your app Firebase Cloud Functions let you easily run your own code on Google's server in response to events in your app. And because the code is running on Google's Cloud you are automatically granted access to all the cool Machine Learning APIs , like translation, sentiment analysis, speech recognition, and computer vision just to name a few. Running your own code on Google's cloud makes it really easy to start using machine learning, all the necessary dependencies and heavy calculations run on the server. This also offers you a centralised secure place to store all your logic and secrets, without the burden of running and maintaining your own server. For more info, check the session Supercharging Firebase Apps with Machine Learning and Cloud Functions where they demonstrate the use of machine learning APIs through Firebase Cloud Functions to build a multiplayer image-recognition game. Luis G. Valle You get fonts, you get fonts... everybody gets fonts! 🎁 I am super happy that we got fonts support in the support library , and that it's even better than the native implementation because it comes with downloadable fonts from Google Fonts. Yes, this means you don't have to download and embed fonts manually in your APKs anymore. You don't need to use additional libraries such as Calligraphy . Super easy to use, mobile-optimised, secure fonts, for free? We think that's awesome. Oh, and have I mentioned, the support library is now on a Maven repo . No more fumbling around with the SDK manager on your CI 🎉 Get the full picture watching the What's New in Android Support Library on YouTube. Sebastiano Poggi All images thanks to our own Daniele Bonaldo and Sebastiano Poggi ↩︎", "date": "2017-05-24"},
{"website": "Novoda", "title": "I/O '17 - What’s new in Firebase", "author": ["Luis G. Valle"], "link": "https://blog.novoda.com/io-17-whats-new-in-firebase/", "abstract": "Great times are coming for Firebase. It was the subject of 25 talks at the recent Google I/O '17 event. So, what's new? This is a quick bullet point summary of all the new stuff already available or coming in the next few weeks: Fabric Crashlytics is going to be integrated into Firebase as the main crash reporting solution. Digits technology will allow free phone number authentication in Firebase for up to 10,000 verifications, which should cover most regular apps. Phone authentication is available now on iOS and Web and coming to Android in the next few weeks. Digits SDK will be deprecated in the coming weeks, so keep an eye on Digits Blog for more info on migration tools. Opensource All Firebase SDKs are now open source https://opensource.google.com/projects/firebase-sdk Firebase Cloud Functions Cloud functions now integrate with Hosting to provide full dynamic webapps. Firebase Storage You can now map existing storage buckets into your Firebase project. The region in which your data is going to be stored now can be chosen. This is great for legal & performance reasons. Firebase Database Expanding to 100,000 simultaneous connections. Introducing the database profiler tool to allow you to introspect bandwidth and latency at path level. Performance Monitor (beta) New dashboard to get insights about your app network response latency, success rate, payload size and more. Requires you to add a new SDK dependency into your app https://firebase.google.com/docs/perf-mon/ It uses something called traces to measure time based events. Some are already defined but you can add your own. Test Lab Adding first-class support for games: game tests can now run on test lab. Test lab now allows you to simulate different network conditions, like 4G, 3G, or slower ones. Expanding device selection to Google Pixel and Galaxy S7. Access to the latest version of Android O. Firebase Alpha You can now sign up to get the new stuff before it's released and provide feedback about it at https://services.google.com/fb/forms/firebasealphaprogram/ Want to see more? The Firebase Blog Google IO 2017 What's new in Firebase session", "date": "2017-05-23"},
{"website": "Novoda", "title": "Adapting design sprints for continuous delivery", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/implementing-design-sprints-into-a-continuous-release-cycle/", "abstract": "As an advocate of research-driven design, having the opportunity to facilitate design sprints and explore product solutions in interdisciplinary teams is a dream come true. However, making this work in a continuous development cycle, delivering client products to market on a bi-weekly basis, can be a challenge. The advantages of running a design sprint are undeniable. Understanding customer needs allows you to make strategic design decisions and prioritise product opportunities based on the value you’ll deliver. Assessing the validity of product concepts with users at an early stage helps to eliminate concepts that aren’t working, reducing risk and increasing your confidence in the product. Working in interdisciplinary teams with a collaborative approach to design helps to improve team dynamics - happy teams deliver products faster and more effectively. In just five days, you can deep-dive into customer needs, generate an expanse of product ideas and have concepts prototyped and tested with users. But with the team working towards bi-weekly releases to market, it’s difficult to avoid the feeling of separation between the two working modes. It’s important to find the right balance between running regular design sprints and getting product updates to market sooner. How often should you run design sprints? A high frequency of design sprints can result in an overwhelming backlog of work for the product team. Being flexible with your sprint planning allows dedicated time for design, development, testing and preparation for release. This allows you to stay focussed on getting the latest update to market, avoiding the pressure of a looming backlog. As you approach feature releases, you can start planning the next incentive. When is a prototype ready for implementation? It can be a good idea to produce two comparable prototypes during a design sprint, which you can then test with users to see which one performs better.  Once you’re confident you’ve validated your design decisions then the prototype is ready to be implemented. How should you avoid development delays? To avoid delays to development, a staggered implementation with two task types can be effective: low-fidelity and styling . A low-fidelity ticket could be started immediately, based on the prototype(s) from the design sprint and a styling ticket could come later, giving designers time to focus on the intricacies of interaction. How do you ensure design consistency across feature teams? If designers are split across interdisciplinary feature teams, it’s easy to generate design inconsistencies. An agreed style guide can be a huge help, as well as regular design reviews. Including design review as a stage in your workflow can help to ensure you’re staying aligned. How do you prevent risky releases? Even when you’re confident in the product, you never really know how it will perform until it’s in the hands of real users . One useful way to avoid minimise risk is to start with a pilot release to a small percentage of users. This allows you to validate your hypotheses on a larger scale and make refinements before pushing the rollout more widely. Can design sprints and implementation be combined more effectively? Taking all this into account, it can still be difficult to shake that uncomfortable feeling that design sprints fall dangerously close to the waterfall method - completing the design thinking phase before the implementation begins. I can’t help but wonder if it was possible to bring the design and implementation cycles closer in some way, whether that might be more effective overall? Design sprints are a collection of highly-effective research and design thinking techniques that can be used independently for each stage: understand and define, diverge (or ideate) and decide, prototype and validate, design, implement & release. Separating each of the steps into tasks allows you to work more organically, from initial research through to release, while keeping dedicated team time for ideation and prototyping. This can help you to retain focus on a bi-weekly release but still benefit from research and design thinking methods. It’s tough to find the right balance between dedicated collaborative design time and a sharp focus on the release, but in my experience, the closer and more organically we work across disciplines, the more effective we can be.", "date": "2017-05-11"},
{"website": "Novoda", "title": "All 4 on your wrist", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/all-4-on-your-wrist/", "abstract": "In this age of ubiquitous computing, phones and tablets aren’t the only smart devices that users interact with during the day. Smartwatches give instant access to snippets of news and information to a fast-growing group of tech-savvy users. Read on to see how we brought All 4 to viewers' wrists. All 4 is the video-on-demand service from Channel 4 . Novoda worked with Channel 4 to create the All 4 application for Android, available for tablets and phones, which includes the functionality for users to watch films and TV shows in the app. One of the most popular features of the Android app is the ability to watch live shows from Channel 4’s stable of channels: Channel 4, E4, More4, Film4 and 4seven. The screen of a smartwatch is too small to provide a great TV-watching experience, but users can still benefit from quickly accessing information directly from their wrist. To suit this format, we created an Android Wear application, which displays the TV guide for currently on-air shows. The design follows the recently updated material design guidelines for Wear 2.0 . Our design approach is based on the individual channels’ branding, while incorporating darker colours that support better battery performance. The navigation is based on a vertical layout, and the user can easily switch between different channels using the new Wear navigation drawer . For every live programme, the user can quickly read the title, the summary and see the associated image. The new All 4 Wear app is available for smartwatches running both Android Wear 1.x and 2.0.", "date": "2017-05-16"},
{"website": "Novoda", "title": "Plan ahead with user research", "author": ["Alex Styl"], "link": "https://blog.novoda.com/plan-with-user-research/", "abstract": "You can’t expect a product to be successful unless it solves a problem, big or small. How do you define that problem when you’re working on internal projects? In this blog post, we'll talk about how basic user research helped us to understand the pain points of our potential users and how this made our roadmap planning much easier. At Novoda, we have lots of internal presentations. The subject of these can vary greatly, from coding to design or agile processes, and sometimes even random topics like coffee or dragons. We record these talks so that anyone can watch them later. Some problems arose, as we didn’t have any specific guidelines for recording or keeping track of the talks, so people didn’t know how to watch previous ones or record and upload their own. This inspired us to create NovodaTV . What we did First of all, you need to make sure that the problem actually needs solving. It wouldn't make sense to build something that, by definition, brings zero value to its users, right? We also wanted to make sure we were building something that people across the entire company would enjoy using. Customer survey The first thing we did was send out a quick three-question anonymous survey to the whole company to gauge opinions: Are you watching the talks? If not, are the talks something you are interested in? Which device would you prefer to watch them on? How could we improve the experience? The results of this survey would not only give us insights into users’ pain points but also ensure that we prioritise features based on what will provide the greatest value. Over 70% of Novoda employees responded, highlighting several pain points and providing valuable suggestions that we would not otherwise have considered adding to our roadmap. Stakeholder interview In addition to the survey, we held a semi-structured interview with our content manager of the Novoda videos to help us understand what the pain points are around distributing the videos. The interview was really helpful as it gave us insights into how painful the current process was for them. It’s important to hear users’ firsthand experiences in order to fully understand the barriers that are preventing them from achieving their goals. Personas After collecting and analysing these findings, we were able to create our very first user personas. 💯 We also had a good sense of the user stories, making planning easier and more streamlined. This helped us identify the priority features that would deliver the greatest value with the least amount of development time. The more knowledge you gather on your users, the more you can refine your personas over time to reflect that. Your users evolve with your product, so it’s important that your personas do the same. If you are interested in user experience (UX) or user research make sure to check out Leonie 's 5 steps of better product design series. It contains a lots of useful information and handy tips to help you create great experiences for your users.", "date": "2017-04-27"},
{"website": "Novoda", "title": "Writing your first chat bot - where do you start?", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/writing-your-first-chat-bot-where-do-you-start/", "abstract": "What do you need to start writing your first chatbot? You have an awesome idea and you know it’ll work well on a Google Home or Amazon Alexa, but where do you start? Let’s run through a simple user scenario to understand the product. Then we’ll trace that back and see what technologies you need to understand to implement your first chatbot. User X asks their Google Home: “Hey Google, Let me talk to Novoda” Google Home Responds: “What do you want to know?” “How to make a chatbot in 3 steps” “Well done, you have already completed the first step, do you understand?” “Yes” “Next, get your phone out, have you got it?” “Yes” “Finally, read this blog online.” In the above the final piece of consumer technology the user is talking to is a Google Home . However the bot created is a more generic chat bot and could be built to work for other capable platforms. Such as Amazon Alexa , your Android or iOS smartphone, Slack , Skype , Facebook Messenger etc etc) Writing a chatbot. A chatbot needs to have two aspects to it. Firstly, it needs to be conversational - nobody wants a silent chatbot. A bit of small talk, being able to comprehend accents and understanding synonyms for words would really help as well. Luckily there is a platform that will give you all of this out of the box. API.AI is a conversational bot creation website. Its GUI interface allows you to create a bot without needing to understand the complexity of conversation, synonyms and accents. Perfect for any chatbot. API.AI: Natural Language Understanding Tools to design unique conversation scenarios, design corresponding actions and analyse interactions with users. Secondly, your bot needs to be uniquely smart, ie it has to be able to serve unique content. You can enable this by hooking your conversational API.AI chatbot into a cloud-connected back end through a system known as webhooks . You can host this on Amazon Lambda , Google Cloud , Microsoft Azure or your own company stack. Your backend service needs to respond to commands. The API.AI bot tells you the question that was asked, breaking it down into its component parts. Your backend then answers the question, returning what is to be said to API.AI. Uploading the chatbot When you know the final technology your bot will inhabit, you’ll need to upload the bot to this platform. For Google Home this is the cloud console the other devices we discussed will have something similar. Recap That’s it. Put these parts together and you have yourself a perfectly functioning chatbot. Use API.AI to create a platform compatible conversational bot and smarten it up with a webhook to your backend unique content. Package it and ship to your preferred chat platform. Sit back and watch the bots take over.", "date": "2017-05-02"},
{"website": "Novoda", "title": "Android Things Best Practices, 3 ways to Thread", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/threading-best-practices/", "abstract": "Working with Android Things means connecting to peripheral sensors & actuators to send or receive commands and data. As with all Android development, you should be evaluating the performance impact of this and consider offloading the effort onto another thread wherever possible. So what is best practice for threading on Android Things? Like with all demo code, the threading solutions you see in some examples have to be taken with a pinch of salt. If threading is not the focus of the example then it’s often done in the quickest or simplest way to keep the demo clear. While this might help people understand a specific point, it doesn’t help advocate the best approach to threading. Beware of blindly following the threading solutions in Android Things demo code. Let's explore some different patterns to find out why. If you prefer a code-driven blog with less theory and more code then take a look at this blog post on best practices when threading with Android Things . #1 Directly on the main thread Forget threading, let’s use the one thread we have. Doing work on the Main Thread is usually a no-no in Android mobile apps, but for Android Things it is possible. Working directly on the main thread means writing the code to be executed alongside the Android lifecycle and always working in a serial fashion. Why? The 5 second timeout ANR (Activity Not Responding) check has been removed from Android Things. This means you can get away with doing long running operations on the main thread. There are no other apps so the system is yours. There’s no need to worry about being a good citizen as every device is not a village . the system (and probably the entire hardware device) is under solely your control and you can make the most of all the resources available. No need to worry about callback hell . Doing things synchronously means no thread-hopping and potentially no need to use callbacks. Everything can just be a return value and each line of code can wait for the last to finish, in theory . Why not? If you use Android Thing’s optional UI components, it can still cause jank . This is because, like Android, UI is drawn on the Main Thread [1] . So if you add a UI component to your app, and you’re doing work on the Main Thread, you could come back to this problem of having a janky, slow, unresponsive UI because your Main Thread is busy doing other things. You may want to do more in the future. Right now it might make sense to do the work on the main thread because it’s ”just this one thing” and you could say YAGNI . But if you know in the future this will get more complicated (and most of the time it does), then using the Main Thread now may cause you a lot of pain refactoring away from that decision in the future. As an Android developer, it goes against everything you've learnt so far. You will already know practices for threading and doing things as separate tasks. Why not use that knowledge if it wouldn’t cause you to slow down? Just because something is easy, eg coding without Threads, it doesn’t mean it's right. If you have a de facto pattern you use for threading, use it on Android Things as well. #2  Asynchronously but still on the main thread The Android Main Thread is a Looper Thread , this means we can post messages to it. Posting messages in this way is asynchronous. Therefore we can post a message to do some work and it will join the back of the queue of other messages. The Main Thread executes the messages one at a time in a serial fashion. Why? This allows UI components to draw smoothly. Joining the queue for execution means you don’t block the Main Thread and, as mentioned earlier, this means the UI component can be drawn with less potential for jank. You don't have to worry about how many threads there are. Sometimes hopping between threads can cause real evasive bugs, especially when debugging a lot of callbacks. This also means the problem of only being allowed to draw UI components on the Main Thread never appears. It’s easier to understand order of execution. Always working on one thread means things cannot happen in parallel, so you are never left wondering if something could have quickly happened before you expected it to. Why not? As with #1, you may want to do more in the future. Right now it might make sense to post the work on the main thread because it’s ”just this one thing” and you could say YAGNI . But if you know in the future this will get more complicated (and most of the time it does), then doing it asynchronously to the Main Thread now may cause you some pain refactoring away from that decision in the future. If the messages you post are too big you can be caught out with UI jank doing long running work. You will also have less control of execution times. With both you and the system posting messages to the same queue, while you can safely assume they will be executed as soon as possible, you won’t have control over the priority or timing. As an Android/Java developer, it goes against threading you've learnt so far. You will already know practices for threading. Why not use that knowledge if it wouldn’t cause you to slow down? Just because something is easier, eg posting single threaded messages, doesn’t mean it's right. If you have a de facto pattern you use for threading, use it on Android Things as well. #3 On a background thread This is the neatest option - create your own threads to do long running or complex operations. This means allowing your work to be completed on a background thread using an AsyncTask, ThreadPoolExecutor, or a Loader. In all cases, a callback mechanism would inform the Main Thread of the result. Why? This follows conventions trialled and tested in the Android development community over time. The community has built up a set of practices and techniques that it is beneficial to take advantage of. It separates processing and UI work. A good separation of concerns is always important, and keeping the communication and data sharing tasks separate from the UI drawing work is a part of this. It’s also the basis of many better architectures, such as Model-View-Presenter. It’s future-proofing. No matter how complex the future communication becomes, you are now working on a background thread and don’t have to worry about overlapping other tasks or UI jank by the amount of communication happening on your thread. Why not? It can be overly complex for simple tasks. There is no need to add extra threads (and callbacks) if the requirements stay simple. It can be callback hell. Understanding the call flow of an application can be difficult and callbacks tend to make code less readable. Threading should always be avoided if possible. Deadlock, Livelock and starvation are problems no developer ever wants to face. Any threading starts discussions about performance overhead and the costs of thread hopping. If possible, it’s always best to avoid this complexity. Android Things threading best practices Given the above, it can seem like there’s no right answer. Depending on your situation, you should take into account what’s going to change, when change is likely and what your development needs are, to help you make an informed choice. Developers who are familiar with Android should find using background threads straightforward, especially when de-facto libraries are in use. However, it is necessary to make a conscious effort to write the code this way from the start. It is important to understand that example code on the internet has often been created to explain another concept, so it uses a quick and dirty approach to threading and shouldn’t be considered best practice. Don’t let the sheer number of these examples influence what you perceive as platform best practice when it comes to threading. and some on the Render Thread but let’s ignore that complication for now. ↩︎", "date": "2017-04-25"},
{"website": "Novoda", "title": "Using TalkBack (I)", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/using-talkback-part-one/", "abstract": "At Novoda, we maintain a competency or skills matrix for each of our projects. We rate ourselves in areas of the project or technology, from one to four, and this helps us to identify potential information silos so we can avoid them. I was surprised that no one wrote \"4\" for \"using TalkBack\" because I had seen everyone use it at some point. I realised that my understanding of a \"4\" would be different to someone else's. These posts will explain what I've learned about using TalkBack. Turn it on Turn TalkBack on from the Accessibility Settings screen. Focusing on elements using single tap Tap an element to focus on it. Notice the green box around the element that indicates it is focused (but not \"clicked\"). You can also drag your finger around the screen to focus the element that is under your finger. Focusing on elements using gestures \"Touch to explore\" is fine for users with partial sight. If, however, your vision is more severely impaired, you might need a different navigation method. It's possible to navigate through elements one-by-one. You can swipe right to navigate to the next item and swipe left to go to the previous item. Clicking focused elements using double-tap Once an element is focused, you can double-tap anywhere on the screen to send a click event to that element. Notice that you do not have to double-tap on the element. I've seen people try to double-tap directly on an element when a different element has focus, but it doesn't work as expected because the location of the double-tap is not relevant; Android will forward the event to the focused element. Scrolling through a list Since dragging one finger is used for \"touch to explore\", it's necessary to use two fingers to scroll through content on the screen. You can navigate through list items by swiping right - it should also scroll the list as you reach the end. An exercise for the reader You now have the necessary skills to turn TalkBack off (navigate to the screen at the start). A future post will demonstrate more gestures and look at some options to configure TalkBack.", "date": "2017-04-11"},
{"website": "Novoda", "title": "A window on Novoda's Liverpool office", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/a-wonderful-window-design/", "abstract": "Our inspiring designers have been at it again. Our Liverpool office has a striking new window decal thanks to Dave & Qi and the fine folks at AB ScreenPrint. Here's a little insight into the minds of our designers to see how this came about. First, a little background. Novoda moved into a new office space in Liverpool with a large public-facing front window. We knew we were moving, but the space hadn't been built yet, so to start with we only had an artist's impression to guide our window design. In the Baltic creative area in Liverpool there are a number of offices with similar front-facing windows. Many are designed differently to the Novoda office but we wanted to take them all into account with our own design approach, identifying any themes or inspirational ideas that could suit our office. At this point we got a sneak peek into what our office was going to look like. We were able to take pictures of the space we had to work with and create the vision of our front window design. Although it hadn't yet been glazed, we could see how the four main areas and the contours would fit together. Drawing on our ideas from the local area, we also did some online research on inspiring or cutting edge office design. We wanted colour in the design and we wanted the window to stand out but also represent who we are and how we work. We liked the idea of using the negative space to enable an image to stand out. Finally our office (and front window) was complete and we could see it in all its glory. The front window decal was not only important from a branding perspective and creating hte impression of a fun and inviting office, but for health and safety reasons. We need to make sure people don't walk straight into the window without realising it's there. Our initial ideas involved adapting design elements of our website, using line-based illustrations to signify the technologies we use. We kept the design minimal to ensure as much light as possible would stream into the office, making it bright and vibrant. After much discussion we realised this design didn't enable us to use much colour. There was also a lack of continuity of the window space, it made each section look like its own and so gave the perception of a smaller office. We wanted bigger, brighter and more colourful. This second concept came from the idea of using the O from the name Novoda as a giant window into our workplace. Using a cutout circle allowed us to merge all four windows into one space, increasing the sense of size and space. It also meant we could use more colour and utilise the negative space. We wanted to work with the circle to incorporate some of what makes Novoda unique. As you can see in the three pictures above, we started to think about how we work on different platforms like mobile, wearables, TV and other devices. How could we incorporate these and to what extent? Although our design says a lot about who we are, it can help to add some text. We wanted to celebrate our use of mobile platforms and be proud of our certifications. We're particularly proud of being one of very few Google Certified Agencies in the world, so we wanted to include a mention of this. We didn't want to overcrowd or add noise to the design so we decided to limit the text on the door, where it would have maximum impact. We played with a few variations of this text, considering the angle that a person would be reading it from, the distance and height. Our final choice made sure it was legible from both sides of the opposite street at the natural eye line of passersby. From this final design we created a template and AB ScreenPrint did a great job of putting it together and affixing it the window in one afternoon. Our final design is bold, beautiful, makes great use of negative space and has a big impact on the street with its colourful blue background and subtle salute to the Novoda logo. It allows a lot of light to come into the office, keeping it bright and airy, whilst conveying who we are and what we do. Just in case you wanted to see it in action, let's end with a gif. Thanks Joe for being our model :-)", "date": "2017-04-06"},
{"website": "Novoda", "title": "Getting started with XCUITest framework for testing iOS apps.", "author": ["Bart Ziemba"], "link": "https://blog.novoda.com/getting-started-with-xcuitest-framework-for-testing-ios-apps/", "abstract": "Xcode comes with the XCUITest framework which allows you to easily develop UI tests that reflect users' interaction with the application. Find out how to kick off with the XCUITest framework to grow your automated UI test suite. Background In November 2016, I joined my first iOS project as a tester. It was a time when my project team had upgraded to Xcode 8, and started with the refactoring from Swift 2 to Swift 3. With Xcode 8, Apple has deprecated UIAutomation in favour of the XCUITest framework. What's more, at that time Appium's support with Xcode 8 was far from ideal and required some workarounds to inspect elements of the iOS application. Looking at the above, I decided to give the XCUITest framework a try and below you can find my findings. What is UI testing? In simple terms. UI testing is interacting with an app's UI elements by tapping, swiping, scrolling and verifying behaviour. Automating UI tests can save you plenty of time, especially during regression testing, but at the same time automation testing needs to simulate the human's behaviour which is not always straightforward. Thankfully, Xcode provides you with the XCUIElement class which allows gestural interactions such as: func tap()\nfunc double​Tap()\nfunc two​Finger​Tap()\nfunc tap(with​Number​Of​Taps:​ UInt, number​Of​Touches:​ UInt)\nfunc press(for​Duration:​ Time​Interval)\nfunc press(for​Duration:​ Time​Interval, then​Drag​To:​ XCUIElement)\nfunc swipe​Left()\nfunc swipe​Right()\nfunc swipe​Up()\nfunc swipe​Down()\nfunc pinch(with​Scale:​ CGFloat, velocity:​ CGFloat)\nfunc rotate(CGFloat, with​Velocity:​ CGFloat) Xcode setup for UI testing Create UI testing target. If you have an existing project and would like to add automated UI tests to it, first you need to create iOS UI testing target. This is how you do it. Open your Xcode project. Go to: File -> New -> Target From the window Choose a template for your new target: select iOS UI Testing Bundle and hit Next : From the window Choose options for your new target: select your Team and Target to be tested Select Finish button and new test target has been created. Create UI test file. Pick the location in your Project navigator where would you like your test file to be created. Right-click and select New File... From the window Choose a template for your new file select UI Test Case Class and hit Next button. 4.In the Choose options for your new file: window provide class name and hit Next button. Select the location where you want the file to be created and hit Create button. You have just created your UI test class with setUp , tearDown and testExample methods. Remember: Your test methods always have to start with the \"test\" word, otherwise you won't be able to run them. UI Recorder Once you are set up with the target and have created your test method you can start using the handy UI recorder which will generate the test script and identify the UI elements for you. Recorder works with both simulators and physical devices, although you might sometimes find it easier to identify the UI element to use one or the other. To kick off recording the tests go to your test method and hit the red dot button placed next to the Debug area. Once you press the button, your application should be launched on simulator or device depending which one you have selected in Xcode. From now on, every interaction with the application will be recorded and the test script will be generated in your test method including your UI elements' identifiers. Here’s an example of a test where user taps on the email field and types their username to login to the app: let emailAddressTextField = application.collectionViews.scrollViews.otherElements.textFields[\"Email address\"]\n        emailAddressTextField.tap()\n        emailAddressTextField.typeText(\"bart@novoda.com\") Xcode alert. While recording your tests and interacting with the app I experienced the Xcode bug saying \"Timestamped Event Manager Error: Failed to find matching element\". That error means that the Xcode has not found the UI element that we interacted with. At that point the recording stopped automatically. For me the solution was to simply start recording again and interact with the element. Run your tests There are couple of ways to run your tests. 1.In Xcode go to Product -> Test (cmd+u) . This will run all the tests in the project including unit tests. 2.If you want to run only the UI tests go to Test navigator. If your UI tests are located in one folder you can run all of them by pressing the play icon next to the folder where your tests are located. 3.There might be a situation in which you won't want to run all your tests. If you would like to run them individually go to the test class. Next to your test method there should be a diamond icon visible. When you click on it, your test will start. Make your tests more reliable and stable Every test has its expected output which should be verified by the test itself. As the UI tests are extend the already existing XCTest class we have the access to all the normal assertions e.g. XCTAssertEqual , XCTAssertTrue , XCTAssertNotEqual and many more. Assert if an element exists. This only takes these two lines of code. Let's say you want to make sure that the Login button exists on the login screen. let loginButton = app.staticTexts[\"Login\"]\nXCTAssertEqual(loginButton.exists, true) Assert if an error message displays the correct value. Let's say you want to make sure that when the user enters the wrong password, the correct message will display. let tooShortPasswordMessage = app.staticTexts[\"Password must exceed 3 characters\"]\nXCTAssertEqual(tooShortPasswordMessage.label, \"Password must exceed 3 characters\") Wait for an element to appear on the screen. Some UI elements take time to appear on the screen. That may depend for example on the API response or WiFi/3G speed. That is why it is necessary to have a solution that waits for those elements, performs the interaction on them and continue with executing the next steps of the tests. Let's say that after tapping on 'Create account' button, you expect the application to show 'Account successfully created!', but this message isn't displayed immediately. That's the perfect situation to apply this waitForExpectations solution: let accountSuccessfullyCreatedMessage = self.app.staticTexts[\"Account successfully created!\"]\nlet exists = NSPredicate(format: \"exists == true\")\nexpectation(for: exists, evaluatedWithObject: accountSuccessfullyCreatedMessage, handler: nil)\n\napp.buttons[\"Create Button\"].tap()\n\nwaitForExpectations(timeout: 10, handler: nil)\nXCTAssert(accountSuccessfullyCreatedMessage.exists)\nXCTAssert(accountSuccessfullyCreatedMessage.label, \"Account successfully created!\") The above code waits for the text 'Account successfully created!' to appear on the screen after pressing on 'Create account' button. The predicate matches when element exists NSPredicate(format: \"exists == true\") . Additionally we are setting out the expectation for the object to exist. After tapping on the button, the waitForExpectations is going to be executed and wait for the element to appear on the screen with the timeout given. Test failures When your tests fail you want to know why. There are a couple of ways of debugging the tests. Printing. 1.First one is printing the accessibility hierarchy, which allows us to find out how our tests see the app's interface. To print out the accessibility hierarchy use: print(app.debugDescription) Do not forget to set the breakpoint after printing so that you will be able to verify the hierarchy in the debug window. 2.If you want to reduce the information displayed in the debug window you can also specify the UI element of which you want to print the accessibility hierarchy. To do so, use: let accountSuccessfullyCreatedMessage = self.app.staticTexts[\"Account successfully created!\"]\nprint(accountSuccessfullyCreatedMessage.debugDescription) Jump to report. When your tests fail, go to the Test navigator: In that view you can see which of your tests failed, as they are marked with the red exclamation mark. Right click on the test and select Jump to report . Expand the view with the arrows given which will lead you to the point where your test failed. Example view: The report shows that test did not find the Home button on the screen. Sum up After reading this article you will be able to: Explain what UI tests are. Set up the Xcode project to start working with XCUITest framweork. Use UI recorder and identify elements. Perform basic interactions on the UI of the app. Run and debug tests. What is the next step? Launch Xcode, configure the project and start writing your tests. Happy testing!", "date": "2017-03-30"},
{"website": "Novoda", "title": "O-h yeah! What we look forward to in Android O", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/o-h-yeah-what-we-look-forward-to-in-android-o/", "abstract": "This week Google has announced the new Android O Preview programme. Like most Android developers out there, we poured over the documentation to find out what new feature tickles our fancies the most. Here’s what people at Novoda are looking forward to. Sebastiano Poggi There are a lot of very interesting new APIs and features in this release; to me, being a UI kind of person, there are a few that have been making me giggle with delight. Wide gamut and diverse colour spaces support 🌈 First and foremost, I'm extremely happy that Android will be able to handle colour spaces properly. No more being limited to sRGB, now apps will be able to properly display images that are stored in the Adobe RGB, ProPhoto RGB and many others. The new APIs will also help you with converting between colour spaces . The documentation seems excellent as well, even just to learn about colour spaces! Fun fact: all the named colour space graphs in the docs have been generated with the new ColorSpace.Renderer on Android. First-party and first-class fonts support ❤️ For years now Android developers have been forced to use clever hacks to implement fonts support in apps, such as Calligraphy or custom span-based solutions. Unfortunately there is still a lot of limitations with that approach, such as the layout preview in Android Studio not showing the custom fonts, or the inability to apply the fonts in some unusual cases (such as TabLayout ‘s tabs that get their textAppearance separately from their inflation ಠ_ಠ). So I’m very, very happy that all these things will be sorted out in the next version of the OS, with the introduction of font resources . It took some time, but we’re getting there. Hopefully it’ll get to the support library , too… Adaptive Icons ⚪ ⬛ 🔴 ⬜ I think this might have been introduced to put a leash on OEMs that like to mess a bit too much with things. Embracing the current (mal-)practice of adding a background to icons is not exactly new— round launcher icons in Android N were a first step in that direction. Adaptive icons will take it the next level though, allowing OEMs and launcher developers to specify a mask to apply to an application-provided background image. This way, icons will fit into whatever style the context they show up into dictates, without having to ship all possible variations. In addition, the new assets are supposed to be substantially larger than the previous images, with a lot of leeway for animating the icons: Ataul Munim A few things caught my eye which will be interesting for those of you interested in inclusive design. Accessibility Button Accessibility services (like Google TalkBack) will be able to request an additional button in the navigation bar for devices with soft navigation keys. This button will provide a service-specific shortcut. So far, only TalkBack has implemented it—though I think they had the inside track on this one! It’ll be interesting to see how services use (and mis-use) this feature, and how the system will deal with multiple services that are vying for the same space. Fingerprint gestures Many of you will already be using fingerprint gestures as convenient shortcuts for frequently performed tasks on your phones, from scrolling through content to pulling down the notification shade. Fingerprint gestures being made available to accessibility services can only be considered a good thing, providing this addition is transparent to app developers and is handled wholly by Android or the service in question. I wonder if it could be used to bring back something similar to the optical trackball! Autosizing TextView 👓 This is the one I’m a little worried about. All too often we see apps that don’t cater for users that make use of the system font size selector (available in Settings > Display), resulting in clipped text and thoroughly confused readers. I suspect we’ll see this issue exacerbated by apps using the autosizing TextView, though if I put my Hopeful Hat on, maybe it'll prompt designers and developers to consider what their apps will look like with various text sizes. Paul Blundell Everyone is thinking, there are no desserts beginning with the letter O... While I'm excited for the new APIs, I'm more excited that perhaps M will get more device adoption now that people is looking at O! AutoFill APIs Didn't realise I needed it until it was pointed out it was missing. So many times do I give up logging into an app because I know if I go on the mobile website my login will be autofilled. No more. Two points from the announcement: Users can select an autofill app, similar to the way they select a keyboard app. The autofill app stores and secures user data, such as addresses, user names, and even passwords. Interestingly, this pushes security concerns onto 3rd parties. I wonder who will come out on top. With Google having their own Smart Lock concept, I imagine Google may be releasing an autofill app alongside O. For apps that want to handle autofill, we're adding new APIs to implement an Autofill service. Oh boy, I want to play with this. I've read about the security researchers who created invisible input fields on a webpage to contain credit card details, that would get autofilled while the user only saw the name field. Users were tricked into allowing autofill and, when they submitted the form, unintentionally disclosed their card details. Looking forward to seeing how Android tackles this and other security questions around Autofill.", "date": "2017-03-23"},
{"website": "Novoda", "title": "It’s playtime: can you learn about agile concepts and have fun in an hour or two?", "author": ["Kathleen Bright (Scrum Master)"], "link": "https://blog.novoda.com/its-playtime-can-you-learn-about-agile-concepts-and-have-fun-in-an-hour-or-two-2/", "abstract": "We played a few games in our Barcelona office to reinforce and explain some agile concepts. It really helped our learning, and was a lot of fun. Why not try them out and see if they help you too? For us it was an effective way of demonstrating the importance of agile values, practices and principles. How to play Most of the games take 15–30 minutes, although there are a couple of five-minute ones too. You can also vary the length if you want to go into more discussion. Playing all the games, including introductions and discussion, took us around two hours. You can reduce that to fit a smaller time slot or increase it for more discussion. Each game had a starting point for the first round, then we modified one parameter, and played again. So it was good practice for running experiments too. We played our games with four participants but they could easily be played by up to eight participants plus a facilitator. If you play these games with your team, we’d love to know what you think. Can you stand alone? We learned about communication, dependencies and teamwork. Objective: Stand up Set up: Everyone gets into pairs and sits together back to back. No equipment required. Time to play: 5 mins Round 1 Observation: Everyone puts their hands on the floor and pushes on each other’s backs to stand up, without talking about it. Round 2 Setup: As round 1, but this time each participant links arms with their partner. Observation: Participants started to talk to each other in their pairs and organise themselves, e.g. saying “Push back. Move your leg. No, push less.” There was a lot of laughter, but don’t just take it from me. Juan said, The collaboration game was not only funny, but also a great way to understand how communication and synergy between team members helps achieve goals and success. Ideas we explored When we had to rely on someone else we started talking to each other, because it’s not possible to navigate and negotiate dependencies without engaging with other people. If we are connected and interdependent, there’s no way we can stand up alone. Party planners We learned about the impact of context Objective: Seat the celebrities at the party Set up: Each participant has a celebrity name (we used a random celebrity name generator). No equipment required. Time to play: 20–25 mins, because the conversation was so good. Round 1 You are having a party and need to plan the seating arrangements for the celebrity guests. Observation: Participants started to seat the celebrities by known things they have in common. For instance, they sat women together, actors together and so on. Round 2 Setup: As round 1, but now Carl and Kevin (our CTO and CEO) say we need to bring in some business for Novoda. Observation: Participants started to change the seating, considering who can make introductions. Round 3 Setup: As round 1, but now your brother or sister is coming. Observation: Participants started to change the seating again and explain why, eg I’ll sit this person here because my sister loves Adele. Ideas we explored It’s really good to zoom out sometimes to see all the components Changing the context means we make different decisions and plans Survive the desert island We learned about stereotypes and archetypes and the value of being multidisciplinary (T-shaped people) and not having fixed roles. Objective: Survival Set up: Sticky notes and pens required Time to play: 20 mins, probably 30 mins if more players Round 1 You’re in a plane which crash lands. Everyone suffers amnesia and so no one knows who they are. Everyone else knows who you are, but they don’t remember how to tell you. Instead, each person can only tell you, \"Yes, I think you’d be good at doing that task\" or \"No, I don’t think you’d be right for that.\" This is a role-playing game. Each team member is given a sticky note with a description of who they are (from the below list), which they stick to their head without reading it. So you can't see who you are, but you can see who everyone else is. The roles are: 60-something comedian 70-something actor Guitar player Mr Bean You have to survive together and there are some items on the island that might help. What will you do? How will you organise? You have: Broken radio Lantern Guitar Observation: The plan that emerged was to make a fire and wait for rescue. Tasks were assigned to each person based on stereotypes: Guitar player - take some wood to build a refuge and hunt some animals, because you’re strong. Actor and comedian — organising, cleaning, preparation. Mr Bean - sit on the beach so as not to mess-up or destroy anything Round 2 Setup: As round 1, but giving the following additional information: The comedian is Le Grand Wyoming who is also a medical doctor. The guitarist is Brian May who also has a PhD in astrophysics. Mr Bean is played by Rowan Atkinson who also has an IQ of 140 and a degree in electronics. The actor is Arnold Schwarzenegger who is also a former professional bodybuilder and therefore very strong. Observation: Plan that emerged: Fix radio and use it with positions of stars to save ourselves. Ideas we explored How labels like “QA” and “developer” can limit us; when we think someone can only do a few things because of stereotypes/roles, we reduce the capacity of team. When we don’t limit ourselves in this way, we start to be more creative. We learned about flow, focus and context switching Objective: Your name written out first Setup: One participant takes the role of developer and will be writing on a whiteboard. The others are stakeholders and each has a name given to them. All names are the same length and each has an unexpected spelling, such as those listed below. Whiteboard or flipchart paper and marker required. BORWN RABITRA MILHAR DIBIDE DEVAS JESEHPA WHOTA THIMASE MERTAN TREANA MERJAN SHIRON KUOPER LOERAN GRISE DIBAHRO HAGIHS JESIKA MARFSI ENGALE For each round, the facilitator keeps a record of: Length of time from starting to finishing writing each name. Length of time from starting the first name to finishing the last name. Time to play: 15 mins Round 1 Observation: Automatically, participants each start to shout their names. (The squeaky wheel gets the grease.) Round 2 Setup: As round 1, but participants are instructed to take turns spelling their name but only giving one letter at a time. Round 3 Setup: As round 1, but participants are instructed to take turns spelling the name, this time the whole name. Observation: 3rd name in last scenario was completed before 1st name completed in the other scenarios. Ideas we explored Impact of limiting work in progress. Stop starting, start finishing. Better return on investment for everyone when we finish something before starting something else, including for items at the bottom of the priority list. Coins We learned about flow, batches and sizing Objective: Process coins Batch of 10 Set up: 10 pennies required per participant. Each participant has to process coins (eg flip each one, throw each one etc). Each person gets to decide how to process their coins and must continue processing in the same way each iteration. Facilitator to measure time. Time to play: 20–30 mins Round 1 Batch size 10: The first person processes all the coins, and then the second person processes all the coins and so on until all the coins are processed. Round 2 Batch size 1: As round 1, but now the second person can start processing their first coin as soon as the first person has processed their first coin. However, each person is located in a different corner of the room and must pass their coin to the next person. Round 3 Batch size: 5: Setup: As round 1, but increase size of batch up to 5 (the second person can start once the first person has finished processing 5 coins). 3 Cs Pictionary: Card, Conversation, Confirmation We observed how difficult it is to write a specification and the impact of discussion and feedback on quality. Objective: Try to describe a figure made up of simple geometric figures by giving a specification to the team and seeing how they draw it Setup: Paper and pens required. Time to play: 3 rounds, 6m each, 30 mins including switching between rounds Round 1 One participant watches someone drawing and then describes to the others (the team) how it should be done, without being able to see what the others are producing. Round 2 Setup: As round 1, but with one person sitting with the team as they implement it and giving feedback If you'd like to learn more, or if you have any comments or questions, get in touch with an agile companion. We'd love to hear from you. Tweet Kathleen @gobrightly and Fran @Ripham_78 .", "date": "2017-03-21"},
{"website": "Novoda", "title": "Avoiding primitive obsession in Swift", "author": ["Alex Curran"], "link": "https://blog.novoda.com/avoiding-primitive-obsession-in-swift/", "abstract": "It’s all too easy to pass information around in your code as strings or ints, but this can soon catch up with you. Swift has a powerful set of protocols to avoid this situation, which can ensure your code is still well-modelled, whilst being easy to write. Ensuring your code represents the problem being solved is called domain modelling and is an important part of software craft. This means you should create classes ( or structs! ) that represent the problem, as opposed to using structures like dictionaries or tuples to store your information. The benefit of creating these domain concepts is that you create a richer API and reduce the amount of effort a developer has to use to understand a piece of code. The opposite of domain modelling — using primitives to represent complex ideas — is called primitive obsession , and is a code smell . An example of this is representing a website's URL by storing it as a String . A URL has more information and specific properties compared to a String (e.g. the scheme, query parameters, protocol), and by storing it as a String you can no longer access these URL -specific items (the domain concepts) without additional code. As part of your domain modelling, there are two things you want to aim for: Making your code as explanatory as possible, without requiring documentation Code which acts as its own documentation is one of the holy grails of software development. If the code is self-documenting in this way, then the documentation will never go out-of-date because it is part of the code itself. You don't need to remember to update the documentation each time you change the code, which is easy to overlook. Prevent misuse of your code in ways that don’t make sense for the business One thing often overlooked about good code is that it makes it difficult to do the wrong thing. A function that requires a URL should take a URL as a parameter, not a String. Whilst it might be easier for a user of the function if they can pass in a String, the function now has to ensure that String is actually a URL too. By forcing the user to do this you put some of the burden on them, but also prevent them from not only misusing your API, but being able to misuse it. Let's focus on this little snippet of code and look at some ways of modelling this better: func showDetailsForEmail(withId: String) {\n    // push new view controller\n} Using typealiases typealias is a keyword - available both in Objective-C and Swift - useful to show where something in your code could be easily represented by something else: typealias EmailId = String\n\nfunc showDetailsForEmail(withId: EmailId) {\n    // push new view controller\n} Typealiases are good for explaining what something can be used for in this context, and they satisfy the first goal (making your code more explanatory). Here, by looking at the function signature, you can see what you’re required to send to the function to get it to work - the ID of an email. Previously when the function just required a String , it is less obvious what that string should represent and how you can create one. It would have been all too easy to pass in a completely unrelated String to the method, which isn’t what you want. Unfortunately, typealiases fail the second requirement - preventing misuse of the API. Typealiases are a “nickname” for another object; they’re not a separate type so they won’t prevent their “nicknamed” type being used in their place. Even though the above function requires an EmailId , you could still pass a String (or even another typealias) in its place: typealias EmailId = String\ntypealias SMSId = String\n\nfunc showDetailsForEmail(withId: EmailId) {\n    // push new view controller\n}\n\nlet smsOne: SMSId = \"from:alice:to:bob\"\nshowDetailsForEmail(withId: smsOne) // compiles, even though this isn't correct This code doesn’t make sense from the perspective of a developer, but the compiler can’t know that. A mistake like this would be caught at runtime, either by a crash later down the line, or just by strange behaviour (if you put an SMS ID in accidentally, it would try to show you emails from this SMS conversation!). If you can tell the compiler — and the developer — what you explicitly mean when you ask for an EmailId, you can catch the error at compile time. Removing primitive obsession The simplest way to ensure that you don’t end up accidentally passing the wrong “meaning” of one of these strings to a method is to make two different objects for the two different types, promoting them from being lowly typealiases: struct EmailId {\n    let rawValue: String\n}\n\nstruct UserId {\n    let rawValue: String\n}\n\nshowDetailsForEmail(withId emailId: EmailId) {\n    // push new view controller\n}\n\nlet joe = UserId(rawValue: \"joeBloggs\")\nshowDetailsForEmail(withId: joe) // this now fails to compile This will fail to compile with the error “ Cannot convert value of type UserId to EmailId ”. Which is perfect! Whilst it might seem like overhead to create two separate objects for what seems like a string, it’s important to realise that isn’t the case. In this instance, it makes no sense to be able to pass a UserId to the showDetailsForEmail() method, so you should prevent developers from doing it. In the eyes of the business that is setting these requirements, a UserId and an EmailId are clearly two different things, so making this evident in your code gets you closer to matching the business rules . Furthermore, it is far quicker to catch an error at compile time, than at runtime. Testing your domain structs A common annoyance with creating objects to wrap domain items is that it can add a lot of boilerplate to the data you create in your tests: let testEmail = Email(id: EmailId(rawValue: \"from:alice:to:bob\"), message: \"...\") Ensuring tests are expressive and quick and easy to understand is important. Luckily, Swift can make your life easier here. Swift has a suite of protocols which can be used to say “this object can be constructed just from a primitive”. These are are the ExpressibleBy*Literal protocols (in Swift 2 and below, called *LiteralConvertible) where * can be some built-in types, e.g. a String, Int, Double, or Array. Implementing these protocols just requires implementing some initialisers: struct EmailId: ExpressibleByStringLiteral {\n    \n    let rawValue: String\n    \n    public init(stringLiteral value: String) {\n        self.rawValue = value\n    }\n    \n    public init(extendedGraphemeClusterLiteral value: String) {\n        self.rawValue = value\n    }\n    \n    public init(unicodeScalarLiteral value: String) {\n        self.rawValue = value\n    }\n    \n} You can then pass a string literal instead of having to create this struct yourself, which means you don’t have to worry about the boilerplate of creating your wrapping struct all the time: let firstEmailId = EmailId(rawValue: \"from:jane:to:kathy\")\nlet secondEmailId: EmailId = \"from:daniel:to:erica\" This can make your test setup easier as now instead of: let testEmail = Email(id: EmailId(rawValue: \"from:sue:to:terry\"), message: \"...\") You can simply write: let testEmail = Email(id: \"from:sue:to:terry\", message: \"...\") You would still create an EmailId , but you’ve reduced some of the boilerplate code. One really important point is that this will only work with string literals (ie text contained in “quotation marks”). So the following code won’t compile because stringId is a String (as soon as a string literal is assigned, it is assigned as a String ): let stringId = \"any\"\nlet testEmail = Email(id: stringId, message: \"...\") // fails to compile, as stringId is a String Which is what you wanted, as this prevents you passing random strings. These literal convertible protocols are really useful for tests because they allow you to write less test boilerplates. Their usefulness is limited in production because, in this example, it is unlikely you’d create EmailId s from string literals, it is more likely to be delivered from JSON (and thus be a String ). Furthermore, if you used this in production, you would lose all the safety you’ve gained from having structs — you could just type any String much like a typealias. Conclusion Domain modelling is essential to making code that is easy for new and experienced developers alike to be able to understand. By making your code more explanatory, you make it easier for developers to understand, and easier to refactor. By ensuring you prevent misuse of code, you make it safer to refactor and reuse code. Type systems are powerful tools to help improve your code, and can also have benefits for understanding the business rules you need to write. But in Swift, the types don’t have to be a burden while testing, by implementing the ExpressibleLiteral protocols. Know any other good uses for these protocols? Tweet me !", "date": "2017-03-14"},
{"website": "Novoda", "title": "Here’s how testing can provide you with the best documentation", "author": ["Tobias Heine (Android Developer)"], "link": "https://blog.novoda.com/tests-are-the-best-documentation/", "abstract": "Mobile applications are complex software systems. Automated tests can help to ensure reliability, which leads to satisfied customers - an essential ingredient for any successful product. What if you could write tests that document requirements and features in a way that everyone involved in a project could understand? If tests are done the right way they should communicate how the system works. Even when this is achieved, reading and understanding these tests assumes knowledge of programming languages and tooling, so in the end the only group that can understand the tests is developers. Let’s look at some ways to remove this barrier to the wider team’s involvement. A little background In this blog post, I’ll be talking about using Espresso and Cucumber to follow behaviour-driven development (BDD) on Android. If you’re not yet familiar with BDD, I recommend reading this blog post to get an overview of what it can do: ‘Growing Android applications, guided by tests’ . In short, BDD means that your software development process is driven by user stories, created with the involvement of all stakeholders. Cucumber, in combination with Gherkin (the language that is used to write Cucumber Scenarios), enables anyone to write plain-text descriptions of the desired behaviour in any spoken language and run automated tests. Following the pattern ‘given/when/then’, the expected behaviour can be described clearly, even for non-programmers. Features like scenarios and arguments can help to reduce repetition. Originally developed for Ruby, there are now ports for other programming languages available. Some of them use Ruby Cucumber with a bridge into the target language. Others, like the Android port, use the Gherkin parser, but implement everything else in the target language. An example of Cucumber in action Imagine you’re developing your own Android-based movie database app. You might want the app to display information about a movie the user has selected from an overview. You could write the following feature scenario: Feature: Movie information\nScenario: Show information for a selected movie\n   Given the following remote movies exist\n     | movieId \t| posterPath \t| title     \t| description  \t |\n     | 1       \t| deadpool.jpg \t| Deadpool \t    | kick ass movie |\n   When I select the details for movie with title Deadpool\n   Then I expect to see the following movie information\n     | title \t  | description \t  |\n     | Deadpool\t| kick ass movie \t| The result is an automated test, which also serves as feature documentation and can be read by non-programmers. User stories could be considered as documentation, but they often don’t give enough context for people unfamiliar with the feature, like new team members or colleagues from another group. Over time these user stories can also become outdated. Using automated tests that describe the behaviour allows you to document the current state. It’s always good to keep your test suite intact, so it’s worth updating the test and documentation if the behaviour changes. You could write such tests together with the product owners when defining the acceptance criteria for your user stories. This way you ensure a certain level of test coverage. It can also help to gain the confidence of stakeholders as everyone is aware of the tests that you’re writing. Sound cool? Let’s have a look at how this works in more detail. Set up Cucumber for Android First of all, we need to set up the correct dependencies. Besides the dependencies for Espresso, you need to add these dependencies for Cucumber to your build.gradle : androidTestCompile 'info.cukes:cucumber-android:1.2.2'\nandroidTestCompile 'info.cukes:cucumber-picocontainer:1.2.0' Cucumber needs its own instrumentation. Create a class in your test folder that extends from AndroidJunitRunner and initialise the CucumberInstrumentationCore : public class CucumberInstrumentation extends AndroidJUnitRunner {\n\n   private final CucumberInstrumentationCore instrumentationCore = new CucumberInstrumentationCore(this);\n\n   @Override\n   public void onCreate(final Bundle bundle) {\n       instrumentationCore.create(bundle);\n       super.onCreate(bundle);\n   }\n\n   @Override\n   public void onStart() {\n       waitForIdleSync();\n       instrumentationCore.start();\n   }\n} Next you need to tell Android to use this instrumentation. You can do this by setting the testInstrumentationRunner to your.project.package.CucumberInstrumentation . But this would mean the CucumberInstrumentation would be used for every test located in your androidTest folder, which is probably not what you want (The CucumberInstrumentation skips regular Espresso tests, so you would just run Cucumber tests with this set up). You should consider carefully when it’s more appropriate to use Cucumber or the regular instrumentation to run your tests. You can configure this by adding a method that returns the correct instrumentation based on a project property: defaultConfig {\n   testInstrumentationRunner getInstrumentation()\n...\n}\n\ndef getInstrumentation() {\n   project.hasProperty(‘cucumber’) ? 'com.tobi.movies.utils.CucumberInstrumentation' : 'android.support.test.runner.AndroidJUnitRunner'\n} Run your Cucumber tests using the command line by typing: ./gradlew connectedCheck -Pcucumber. Cucumber executes feature definitions which are stored in *.feature files located in a folder of your choice under androidTest/assets . To tell Cucumber where the files are located, you have to create a class in your androidTest/java folder, e.g. CucumberRunner , and specify the folder where the feature files are located using the CucumberOptions annotation. Cucumber will scan your test folder until it finds a class that uses this annotation: @CucumberOptions(features = \"features\")\npublic class CucumberRunner {\n\t// class body can be empty\n} Create Cucumber features Now you can start writing your first feature definition, for example androidTest/assets/features/your_first_feature.feature including step definitions in Java code The step definitions contain view assertions and view interactions. It’s worth trying to encapsulate this code so you can reuse it among different step definitions or share it with the regular Espresso tests. This avoids code duplication, helps you separate concerns and keeps your codebase clean. One good design pattern to follow is PageObjects . A good example of a concrete implementation for Android are these testing robots introduced by Jake Wharton. To make Cucumber aware of your steps you need to annotate your methods using @Given / @When / @Then : @When(\"^I select the movie poster at position (\\\\d+)$\")\npublic void I_select_a_movie_poster_at(final int position) {\n   // put here your test code\n} As you can see in the example, the step definition takes a position as argument. You can pass almost any type of argument, including domain objects. More options can be seen here . You can even transform your arguments into other types using the transform annotation. In this example I convert a String into a Date : @Given(\"^I want to transform this (\\\\S+) to a date\")\npublic void transform_string_to_date(@Transform(DateFormatter.class) Date date) {\n\t// you can use the date directly\n} In the step implementation: public static class DateFormatter extends Transformer<Date> {\n\n   @Override\n   public Date transform(String s) {\n       SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"MM/dd/yyyy\", Locale.ENGLISH);\n       try {\n           return simpleDateFormat.parse(s);\n       } catch (ParseException e) {\n           // handle error\n       }\n   }\n} As an entry point for your test, you could create a step to launch the application: @Given(\"^I start the application$\")\npublic void I_start_app() {\n  //launch your activity test rule here\n} If you need to execute some code before or after your tests run, Cucumber comes with @Before and @After annotations. Tips Scenario Outlines Cucumber offers a similar feature to parametrised JUnit tests called scenario outlines which allow you to execute the same ocenario with different parameters. Instead of defining the parameters as part of the scenario, you can add and an examples section. Scenario Outline: Show movie descriptions for all movies\n Given the following movies exist\n   | movieId \t| title     \t| description           \t|\n   | 100     \t| Deadpool  \t| awesome movie         \t|\n   | 200     \t| X-Men    \t    | wolverine rocks \t        |\n   | 300     \t| Star Wars \t| may the force with you \t|\n When I launch the movie overview screen\n And I select the movie at position <pos>\n Then I expect to see the following movie description\n   | title \t  | description   |\n   | <title>     | <description> |\n\n Examples:\n   | pos \t| title     | description            \t\t|\n   | 0   \t| Deadpool\t| awesome movie         \t\t|\n   | 1   \t| X-Men    \t| wolverine rocks        \t\t|\n   | 2   \t| Star Wars\t| may the force be with you \t| Run Cucumber Tests by Tags Cucumber provides the option to run tests marked by annotations or by scenario name, which gives you plenty of flexibility. You can run tests related to the feature you’re currently working on, or you could annotate tests with a @smoke annotation and run these tests as part of your build pipeline on the CI. To do this you’d need to make some changes. First, extend your build.gradle - to pass the information to the CucumberInstrumentation : debug {\n   buildConfigField ‘String’, ‘TEST_TAGS’, ‘“‘+getTestTags()+’”’\n   buildConfigField ‘String’, ‘TEST_SCENARIO’, ‘“‘+getTestScenario()+’”’\n}\n\ndef getTestTags() {\n   project.getProperties().get(‘tags’) ?: ‘’\n}\n\ndef getTestScenario() {\n   project.getProperties().get(‘scenario’) ?: ‘’\n} Next, access the information in your CucumberInstrumentation and pass it to Cucumber in the expected format: public class CucumberInstrumentation extends AndroidJUnitRunner {\n\n   private static final String CUCUMBER_TAGS_KEY = \"tags\";\n   private static final String CUCUMBER_SCENARIO_KEY = \"name\";\n\n   @Override\n   public void onCreate(final Bundle bundle) {\n       String tags = BuildConfig.TEST_TAGS;\n       if (!tags.isEmpty()) {\n           bundle.putString(CUCUMBER_TAGS_KEY, tags.replaceAll(\"\\\\s\", \"\"));\n       }\n\n       String scenario = BuildConfig.TEST_SCENARIO;\n       if (!scenario.isEmpty()) {\n           scenario = scenario.replaceAll(\" \", \"\\\\\\\\s\");\n           bundle.putString(CUCUMBER_SCENARIO_KEY, scenario);\n       }\n       ...\n   }\n\n ...\n} Then run the test you want via the command line: ./gradlew connectedCheck -Pcucumber -Ptags=\"@smoke\"\n./gradlew connectedCheck -Pcucumber -Pscenario=\"Your scenario name comes here\" Android Studio Last but not least, there is support for Cucumber and Gherkin in Android Studio. You can install the Cucumber for Java and Gherkin plugins which give you syntax highlighting, autocompletion, navigation between steps and step definitions and much more: Conclusion Cucumber on Android works pretty well and results in maintainable and readable tests and up-to-date feature documentation. The setup is straightforward and you can smoothly migrate existing tests or start with new ones in addition to your test suite. From here it’s up to you how you use it. Try to integrate Cucumber into your software creation process by writing scenarios together with your team when grooming a new feature. Or just use it to document complex business rules for your colleagues. There are many ways to use this tool to fit your needs. That’s it! Have fun with Cucumber on Android. If you’re looking for a running example, check out my repo .", "date": "2017-03-02"},
{"website": "Novoda", "title": "Getting started with Cloud Functions for Firebase", "author": ["Jozef Celuch"], "link": "https://blog.novoda.com/getting-started-with-cloud-functions-for-firebase/", "abstract": "Cloud Functions for Firebase , the latest tool added to the Firebase toolbox lets you do all the wild things with Firebase that you always wanted to do. In this blog post we will take a look at how you can get started with the Cloud Functions even if you have no experience with JavaScript. Novoda have wished for cloud functions ever since we first started playing around with Firebase (see Bonfire ). At the time we were just wishing for the ability to execute a piece of code when there are changes in the Real Time Database. However, with this latest Firebase addition you can execute code not only on database changes but also on analytics events, authentication events and much more. Setting up Cloud Functions with JavaScript Currently, Cloud Functions only support Node.js, as opposed to AWS Lambdas, which also support Python 2.7 and Java 8. If you’re a native mobile developer like us, this might sound scary to you, but don’t worry, it’s fun! First thing you’ll need to install is NodeJS. The easiest way on MacOS is to use Homebrew and run: brew install npm Next step is to install Firebase CLI tools and initialize the Cloud Functions in your project. All the steps are described in the official documentation but here are the commands you’ll need to run: npm install -g firebase-tools firebase login And then finally go inside your project directory and run this command to initialize the functions. firebase init functions Now you’re all set for the Cloud Functions. Let’s have a look at what you need to configure in order to allow you to run tests. Testing Firebase cloud functions In our project we’re using the testing framework called Jasmine and a code coverage tool called Istanbul . Since this is the JavaScript world you could choose any of the plethora of the libraries and tools available. To install these libraries to your project, just go inside the functions folder and run the following command: npm install istanbul jasmine --save-dev This will download the dependencies into your project and also add these libraries as devDependencies into your project.json file. To initialise Jasmine, all you need to do is run this command: jasmine init This will generate all the necessary Jasmine configuration files and allow you to run the tests. However, we were not really happy with the default configuration so we decided to move things around a bit, mainly because we wanted to have both source files and test files together. As the first step we moved our jasmine.json file to the functions folder and changed the spec_dir to src which is the directory that contains all our code. The final configuration file looks like this: {\n  \"spec_dir\": \"src\",\n  \"spec_files\": [\n    \"**/*[sS]pec.js\"\n  ],\n  \"helpers\": [\n    \"mocks/**/*.js\"\n  ],\n  \"stopSpecOnExpectationFailure\": false,\n  \"random\": false\n} Next we had to tell Jasmine where to find the configuration file. We do this in the package.json file. Adding the test script into the package.json also allows us to run the tests by simply executing npm test command which will be also handy for running the tests on the CI. \"scripts\": {\n    \"test\": node_modules/jasmine/bin/jasmine.js JASMINE_CONFIG_PATH=jasmine.json\n  } The last step is to configure the code coverage tool, you can do this by updating the test script we defined earlier so that it contains the following command: \"scripts\": {\n    \"test\": \"./node_modules/.bin/istanbul cover -x \\\"**/*[sS]pec.js\\\" node_modules/jasmine/bin/jasmine.js JASMINE_CONFIG_PATH=jasmine.json && ./node_modules/.bin/istanbul report cobertura\"\n  } Here we’re telling Istanbul to run the code coverage of the Jasmine tests excluding spec files. The last part of the command is optional, since it only generates the cobertura test report so that it can be visualized on the CI and make your build fail in case the code coverage falls below a certain threshold. But that also requires a little bit of configuration on your CI server. Creating your first cloud function You may have noticed that we have been talking about a src folder but there is index.js already generated for us in the root of our project. This is the file where we need to export our functions so that they can be deployed. However, this file is not included in our tests or code coverage setup, so we don’t want to have any logic here; we only want to use it to let Firebase know about our functions. We ended up creating another index.js file inside the src folder and moving all the contents of the root-level index.js there. Then the only thing we added into the root-level index is a single line of code that lets Firebase know about the functions we want to export: module.exports = require('./src/index'); In this simple case the exports are the same in both index files so this seems like a bit of an overkill. But with this setup we can structure our source code in a completely different way without affecting how we export functions to Firebase. We could even write all our logic in TypeScript and all we would need to change here is the location of the index.js we require from the current JavaScript file to the location of the compiled index.ts . Now we can write a simple test database function in the src/index.js . Our hello world function writes the value of anything that just got written in the database to the console log. Here’s the code of the function: var functions = require('firebase-functions');\n\nexports.helloWorld = functions.database.ref('/')\n\t.onWrite(event => {\n\t\tconsole.log('New data: ', event.data.val());\n\t}); Push it to the Firebase cloud ☁️ Our awesome function is ready to be deployed now. We can do that by simply calling a single command in the root of our functions folder. firebase deploy --only functions When the deploy finishes you can see your functions in the Functions section of the Firebase Console , where you will also find the logs and usage statistics. The console contains only the very basic information, which is perfectly sufficient most of the time. However, if you need to find some more information, then you can always head over to the Google Cloud Platform Console and look for it there. Conclusions Thanks to the fact that Novoda is a part of the Google Agency Program we’ve had a chance to work with the Cloud Functions for Firebase for a while now. We are definitely excited about the possibilities that they offer together with other Firebase tools and we will be sharing more about our experiences with it in future blog posts. In addition to that, we will be talking about the Functions in this month’s episode of our monthly Firebase Hangout . Thanks to Andrei for his help with the blog post and to Francesco and Daniele for their help with JavaScript. Don’t forget to checkout our Bonfire app that uses Firebase. Android, Google Play and the Google Play logo are trademarks of Google Inc. The iOS app hasn’t made it through the app review process, but you can sign up to our beta", "date": "2017-03-10"},
{"website": "Novoda", "title": "Helping the Liverpool Community Grid", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/helping-the-liverpool-community-grid/", "abstract": "World Community Grid enables anyone with a computer, smartphone or tablet to donate their unused computing power to advance cutting-edge scientific research on topics related to health, poverty and sustainability. Through the contributions of over 650,000 individuals and 460 organisations, World Community Grid has supported 28 research projects to date, including searches for more effective treatments for cancer, HIV/AIDS and several tropical diseases. Other projects are looking for low-cost water filtration systems and new materials for capturing solar energy efficiently. Based in Liverpool in the UK, Liverpool Community Grid (LCG) is a local grid computing project that donates spare computing power to help scientists tackle some of the world’s biggest problems. Novoda's Liverpool office is proud to be the first node in this grid. Giving back to the community is very important to us, so we are happy to use our office space for this fantastic project. We also run the Liverpool Google Developer Group bi-monthly meet-up where developers can get together to talk about the software behind projects like this. Checkout the live statistics If you or your company want to get involved by donating computers, or if you have technical skills and could donate your time, please phone or text +44 (0)7971 403502.", "date": "2017-02-28"},
{"website": "Novoda", "title": "How we use Firebase to optimise CCleaner", "author": ["Denis Akan"], "link": "https://blog.novoda.com/how-we-use-firebase-to-optimise-ccleaner/", "abstract": "As a Google-certified agency, Novoda is fortunate to have a close working relationship with Google and get early access to their new technologies. When they approached us in 2016 to see if we would like to collaborate on a case study for their developer blog about our use of Firebase, we were excited about this opportunity. The video focuses on how we used Firebase to identify new audiences, find out about user behaviour and monetise existing users for our client on the popular CCleaner app. Firebase helped us to conduct A/B tests to optimise and validate our designs and fine tune the overall strategy for the CCleaner Android app. This enabled us to make the CCleaner app a better, smarter product, which has now had over 50 million downloads. Watch the video to find out more about this project, Firebase and the Google Developer Agency program. As ongoing partners, CCleaner asked Novoda to help them optimise their Play Store listing. We added emojis to the listing's short description and the results showed an increase in downloads of up to 20% in some countries. Read more about the Play Store emoji testing 🚀 here 🚀 .", "date": "2017-02-27"},
{"website": "Novoda", "title": "Become a master of automation with Fastlane", "author": ["Giuseppe Basile (iOS Software Craftsman)"], "link": "https://blog.novoda.com/become-a-master-of-automation-with-fastlane/", "abstract": "There are several reasons to start using automation in your application development process: running your tests, distributing your app internally or into the App Store, or setting up well-defined processes in your team. Whatever the reason, if you are a mobile developer you might want to consider using Fastlane. iOS project automation Automation is your best friend when working on a project. With a small investment of time you can get rid of all the operations you normally have to do manually. It saves you time, helps you maintain high quality standards and avoids human error when doing releases. Here at Novoda we use automation to: run tests every time changes are pushed to the repository (ensuring we don't create regression bugs), build and distribute an artefact every night to our internal QA testers, and minimise human errors with signing certificates as part of our release process. Fastlane is a command line tool, initially created by Felix Krause , and recently acquired by Google within the Fabric suite. It is currently the most popular solution for automation on iOS and they've recently rolled out support for Android too. You can trigger Fastlane on your computer if you wish, or  integrate it with popular CI solutions like Jenkins to trigger your jobs there. Fastlane install requirements To get started you need an up-to-date version of Ruby (at time of writing 2.4.0). You might have a problem installing Fastlane on Ruby 2.0 and below. To update, use your preferred Ruby version manager - RVM or RBENV - to get the last stable version of Ruby. At this point you can install Fastlane using gem install fastlane -NV or Homebrew if you prefer brew cask install fastlane . Now you have everything you need to start to automate all the things ! 🍭 Fastlane project setup You need to remember one command to do almost everything: fastlane . The first time you run the command from your project directory it will ask you basic information about your project and your Apple Developer account. Everything is interactive and all the required configuration will be created. Don't worry if your Apple Developer account has multiple teams or it's protected with 2-step authentication, Fastlane can handle that. A new directory is created for you with two different configuration files. ─── fastlane\n    ├── Appfile\n    └── Fastfile The Fastlane folder contains all the configuration used by Fastlane. It's strongly recommended to include this folder in your VCS so that you can share it with everyone in your team. There are just two files at the moment, as you add more actions , other configuration files will be added. The Appfile stores general information about your app that can be shared across all the different actions: the bundle identifier, your Apple Developer account and your Team ID. The Fastfile is where you defined your lanes. It's the most important file of your configuration and where you are going to spend most of your time. Fastfile is where you defined your lanes and it's the most important file of your configuration Configuring Fastlane By default Fastlane is already configured to drive some lanes (which means 'run some jobs' in their jargon). Now that you have the basic configuration, you can type fastlane again and you will see the list of possible lanes: $ fastlane\n\nWelcome to fastlane! Here's what your app is setup to do:\n+--------+-------------+----------------------------------------------------+\n|                          Available lanes to run                           |\n+--------+-------------+----------------------------------------------------+\n| Number | Lane Name   | Description                                        |\n+--------+-------------+----------------------------------------------------+\n| 1      | ios test    | Runs all the tests                                 |\n| 2      | ios beta    | Submit a new Beta Build to Apple TestFlight        |\n|        |             | This will also make sure the profile is up to date |\n| 3      | ios release | Deploy a new version to the App Store              |\n| 0      | cancel      | No selection, exit fastlane!                       |\n+--------+-------------+----------------------------------------------------+\nWhich number would you like run? If you already know the name of the lane you want to drive, you can append it to the Fastlane command. Let's say you want to run the tests in your app, type fastlane ios test . If you don't have any Android projects in the same folder you can type fastlane test . If you already have the relevant certificates installed on your machine you can run all the existing lanes, otherwise you need to set up match in order to run beta and release . We will look at this in more detail in a future blog post. You can do almost everything using the command fastlane Configure your first lane The Fastfile contains the configuration of your lanes. 90% of the work is normally done in this file, so let's open it and take a closer look. The Fastlane team help you here. The file is commented and you have all the information you need to understand what's going on. For this blog post, I have reduced the Fastfile to the bare minimum with a single lane that will only execute the tests of your app. fastlane_version \"2.14.2\"\ndefault_platform :ios\n\nplatform :ios do\n\n  desc \"Runs all the tests\"\n  lane :test do\n    scan\n  end\n\nend Starting from now, test will be the only lane available. A lane is composed of a list of sequential actions. In this case, we are only using the scan action, which is the recommended way to run tests. Actions can be configured and you can see the list of possible arguments directly from Fastlane: fastlane action scan Scan can be configured with several options. By default, you don't need to specify anything, as Fastlane infers most of the information for you. When you're ready to specify your own options, the easiest way is to define them directly inside the lane where you use the action. For example: desc \"Runs all the tests\"\n  lane :test do\n    scan(\n      scheme: 'RandMTests',\n      clean: true\n    )\n  end We're now using scan with a well-defined scheme and we're forcing the tool to clean the project before building it. You can find all the keys for your configuration dictionary in the first column of the documentation of the action, as seen above. Check the documentation of an action using fastlane action #{actionName} Fastlane configuration tips If you want to set up a default value for your action, you can define an environment variable. There are different ways to setup an environment variable, and some people do it directly inside the Fastfile . I prefer to create an .env file to define them, as the separation makes your configuration more readable. SCAN_SCHEME='RandMTests'\nSCAN_CLEAN=true After you have declared the default values in your .env , you can now remove the values from the configuration for scan in the Fastfile desc \"Runs all the tests\"\n  lane :test do\n    scan\n  end As a rule of thumb, I always configure my actions in the .env file. I pass options directly on the action only when necessary, for example when I want to use the same action in different lanes with different options. Chaining actions In a real world scenario, you’ll want to add more actions into the same lane, which is trivial using Fastlane. All the output parameters of an action are available in the next actions gym is the action to build and create an IPA of your app. This is maintained by the Fastlane team and it’s the recommended solution to build your project. testflight is the action that will upload the IPA file for testing. If you run gym you don't need to manually tell testflight the path of your IPA as this is automatically done for you by Fastlane. Cool, isn't it? When you create your lanes you don't have to set up every required option/parameter as some previous action might have set up those values for you. You can see all the available actions in Fastlane using fastlane actions . There are several and you can do a lot of cool advanced things by chaining them, for example: Generate localised screenshot of your app with snapshot Add device frames to these screenshots with frameit Create an ipa of your app with gym Send your ipa and the screenshot to iTunes with deliver You can even go more advanced and have an entire release workflow that executes your tests, updates the build number of your app, commits the changes and creates a Git tag for you. Fastlane is very flexible, you just have to configure the actions already available. Configuration Example You already have all the information that you need, but here’s an example from a recent Novoda project for inspiration: fastlane_version \"2.14.2\"\ndefault_platform :ios\n\nplatform :ios do\n  desc \"Runs all the tests\"\n  lane :test do\n    scan\n  end\n\n  desc \"Runs all the UI tests\"\n  lane :ui_tests do\n    scan(scheme: 'UITests')\n  end\n\n  desc \"Build and send the release to Crashlytics for internal testing\"\n  lane :release_dev do\n    ensure_git_status_clean\n    match(type: 'development')\n    increment_build_number\n    increment_version_number\n    commit_version_bump\n    custom_overlay\n    gym(configuration: 'Debug')\n    crashlytics(notes: release_notes)\n    reset_git_repo\n  end\n\n  desc \"Test, Build and send a release to Testflight\"\n  lane :release_testflight do\n    gym(configuration: 'Release')\n    testflight\n  end\n\n  desc \"Generate Unit Test Code Coverage\" \n  lane :code_coverage do \n    scan\n    xcov(scheme: MyScheme, only_project_targets: true)\n  end\n\n  desc \"Update the version number for a new sprint\"\n  lane :new_sprint do\n    ensure_git_status_clean\n    get_version_number\n    current_version = lane_context[SharedValues::VERSION_NUMBER]\n    version_array = current_version.split(\".\").map(&:to_i)\n\n    new_sprint_number = version_array[1] + 1\n\n    version_array[1] = new_sprint_number\n    version_array[2] = 0\n\n    new_version = version_array.join(\".\")\n    increment_version_number(\n      version_number: new_version\n    )\n    commit_version_bump\n  end\n\n  desc \"Install developer certificates and provisioning profiles\"\n  lane :install_certificates do\n    match(type: 'development')\n  end\n\n  desc \"Generate a custom icon overlay\"\n  private_lane :custom_overlay do\n    git_commit = last_git_commit[:abbreviated_commit_hash]\n    get_version_number\n    version_number = lane_context[SharedValues::VERSION_NUMBER]\n    badge(\n     shield: \"#{version_number}-#{git_commit}-blue\",\n     alpha: true\n    )\n  end\n\n  desc \"Generate release notes\"\n  private_lane :release_notes do\n    get_version_number\n    changelog = changelog_from_git_commits\n    version_number = lane_context[SharedValues::VERSION_NUMBER]\n    \"Release notes #{version_number} Automatic build (Last 5 PR):\\n#{changelog}\"\n  end\nend One of my favourite actions in this project is release_dev , because it: Ensures the local Git repository is clean Installs or updates certificates Increments build and version numbers Commits the changes Creates a custom overlay of the app icon with the version number and the last git commit hash Builds and archives Upload the app to Crashlytics and shares it with the beta users Cleans the repo (we don't want to keep the changed icons) Can you imagine having to do all these steps each day to send the build to your QA team? You can always have a look at the collection of examples maintained in the Fastlane repo . Master of Fastlane You are now ready to start automating your tasks using Fastlane. There are more topics to discuss and no doubt you'll discover even more advanced uses of the tool as you become more familiar with it. If you want to discuss Fastlane or have a feature you want me to explain in another blog post then get in touch @pepito . There are recommended best practices, which I may go into further in a future blog post. For now, my advice is to open your terminal and start automating!", "date": "2017-02-23"},
{"website": "Novoda", "title": "Android Tablet Keyboard Freebie.Sketch", "author": ["Qi"], "link": "https://blog.novoda.com/android-tablet-keyboard-freebie-sketch/", "abstract": "The other day I was working on the search screens for my current project, but I couldn’t find any Android tablet keyboard resources online. So I decided to create some and share them with you all. Sketch file contains: Dark theme keyboard with border (landscape & portrait) Dark theme keyboard without border (landscape & portrait) Light theme keyboard with border  (landscape & portrait) Light theme keyboard without border  (landscape & portrait) Feel free to download the kit from sketchappsources , use it in your projects and share it with your co-workers.", "date": "2017-02-16"},
{"website": "Novoda", "title": "The 5 steps of better product design:  5. Measure impact", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-5-steps-of-better-product-design-5-track-impact/", "abstract": "Use mobile analytics to define, collect and format live data from your application to validate your hypotheses, monitor user behaviour and measure your product’s success. This data can enable you to gather large-scale insights into how your products are being used, which can be viewed alongside qualitative research to guide your product decisions. This is step 5 in a 5-piece series exploring the stages of product design. Understand Ideate Prototype & test Visual & motion design Measure impact As discussed in the ‘understand’ phase of this series, there are two types of research that can provide useful user insight; qualitative (customer interviews, focus groups, app store reviews, customer support feedback) and quantitative (survey results, app store ratings, data analytics, crash reports). This post will focus on analysing quantitative data post-release. Tips for product designers Here are some tips to help you get the most from your analytics, work out what to track and plan for the future. Stick to your goals It’s important to stay focussed on the main goals of your release. What are your overall KPIs? What is your hypothesis? How do you define product success? What do you need to track to confidently measure success? Think big When working on a specific KPI, it’s easy to lose sight of the bigger picture. Make sure you communicate product changes across the company prior to each release and be aware of how product changes might affect user behaviour in other areas. Define early Defining your analytics requirements early on will help you ensure that tracking points are implemented in time for release with the appropriate tracking tools. Consider questions that might arise post-release and ensure you have sufficient data to provide validated answers. Be specific It’s important to specify exactly what you’re measuring. Start with a question, define a measurement and hypothesise possible outcomes. This will help you to define the best tracking metrics for your needs and generate valuable insights. Start small When testing a feature for the first time, limit the release to a small percentage of users. This will allow you to measure the impact and react quickly. Be transparent Taking a learning-based approach to product development can help your team to understand the impact of product changes more clearly. Holding regular progress reviews can also encourage a unified focus and an increased feeling of responsibility across the team. What can we track? If you’re a product designer, you’ll no doubt spend a lot of your time focussing on business goals and customer needs. On release, you need a way to track whether you have succeeded or not. Some general measurements can be useful, such as customer satisfaction or number of downloads, but you may also require more detailed metrics. The degree of granularity will depend on what you’re trying to understand. The most important thing is to ask the right questions. Why are you implementing this change? What are you trying to achieve? What is success and what is failure? Here are some tracking options to consider: User satisfaction A simple in-app questionnaire can help you to benchmark an overall user satisfaction rating, measuring customer happiness. Click interaction Check your analytics for the percentage of users that are clicking on important elements (eg buttons, links, images, icons). This might help you understand the value of the action or the visual prominence of the element. Screen views Find out the percentage of users that are landing on a particular screen. Tracking this across multiple screens will help you understand how far users proceed through an expected user journey and identify any major drop-off points. Load time Check how long your application, or a specific screen, is taking to load. This can help you to identify issues and optimise your loading times. Sessions Monitor the number of overall sessions and sessions per user. This metric can help you measure product growth over time and the level of customer retention. Session duration Find out how long users typically spend using your application. This can give you an indication of levels of user engagement. Account status Check how many of your users sign in when they use your application and take a look at any demographic information you hold on those users. This can help you assess the value of any signed-in-only features and which user groups they appeal to. Entry points Take a look at where your users are coming from. How many users are opening the app directly from their device versus a web smart banner, link or native notification? This can help to analyse and optimise retention and measure the effect of notifications. Crash reports It’s always a good idea to keep track of the number of crash reports and keep an eye on problem areas. This can help you improve your overall crash rate and increase user satisfaction. Conclusion Analytics tracking allows you to measure the impact your products are having on the business and your end users. By asking the right questions, you can validate hypotheses on a large scale and gather insights to help you optimise your future product development. It can also create a unified focus across the team and generate clear success indicators for stakeholders. I hope you’ve found this series interesting and taken away some useful ideas for your own projects. If you have any other tips or feedback you’d like to share on this subject then please get in touch on leonie@novoda.com .", "date": "2017-02-14"},
{"website": "Novoda", "title": "Android Things Architecture - Sensors & Actuators", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/android-things-sensors-and-actuators/", "abstract": "Android Things works by creating an Android app and tying it to peripherals using User Drivers. These peripherals come in a variety of forms, this post will explore their differences and commonalities to help you understand where User Driver code fits in your architecture & codebase. When using peripherals, you will need to create or reuse a Peripheral I/O driver. I highly recommend you take a look at this blog post on creating your own drivers with Android Things and check out this list of ready to use drivers on Github . Android Things peripherals Each peripheral you use will have a different Peripheral I/O protocol (or multiple protocols in some cases). The differences in protocols mean your peripheral communication can be one-way (sending actions or receiving data), or two-way with actions and data feedback. Your peripheral might only need one-pin monitoring, or it might monitor multiple pins, which means you can use serial protocols and send and receive larger payloads of data. Once your driver is working, you can use this knowledge to start refactoring and ensuring your code is clean and correctly composed. Peripherals come in different forms: active peripherals do their own thing once they have power, while passive peripherals wait for something to activate them. There are many hardware examples, like buttons, switches, motion, light or sound detectors, metal detectors, temperature gauges, solar panels, GPS chips, accelerometers, LEDs, screens, speakers, vibrators, lasers, segment displays, Bluetooth WiFi or RFID transmitters, electric motors and many more. These fall into two categories: Input peripherals that consume information from the outside world Output peripherals that produce information for the outside world. Android Things code structure for peripherals In your codebase, a simple but naive way to structure your architecture is to group your peripherals by protocol. This would mean having all the GPIO peripherals in one place and all the I2C peripherals in another. It would allow you to quickly find the code for each peripheral once you know the driver protocol. This is handy for fixing all GPIO peripherals at once, but do you ever need to do that? It’s pretty rare. This is reminiscent to architecting by class type, when you have all your Activities in one place, all your Services in another, Views somewhere else and so on. This type of grouping is “ tidy ” but it comes with other disadvantages. For example, if you added a speaker peripheral and then later wanted to update it, you may not recall that it uses I2C. This protocol driven structure would make it hard to look that up. However you will  know the speaker makes beeping noises and is turned on in reaction to some user input. Let’s look at another way to group peripherals. Structuring your architecture to group domain peripherals by input and output is an efficient way to understand when and where they are used. [1] The speaker we described above or an LED would both be output devices, meaning they will feedback to the user. A button would be an input device, as the user presses it to give information to the system. This means you only have to search for input or output when looking for the relevant block of code for this peripheral. This is a useful step towards a cleaner architecture. Naming Android Things peripherals An input peripheral can also be called a sensor . A sensor detects or measures a physical property and responds to it. This means a sensor measures a particular element in the outside world and feeds its knowledge back into the system. This sounds quite like a description of a temperature gauge, a motion detector or a button. A device which detects or measures a physical property and records, indicates, or otherwise responds to it. sensor [2] Another name for an output peripheral is an actuator . An actuator is responsible for moving or controlling a mechanism or system, or presenting data to the outside world. This means an actuator moves or acts to change the outside world with knowledge we’ve shared from the system. This sounds quite like a description of a speaker, LED screen or electric motor. [3] A component of a machine that is responsible for moving or controlling a mechanism or system. actuator [4] Model View Presenter with Android Things peripherals What’s the best way to use this clean code and architecture theory in practice? Let's take our Android Things peripherals with sensor and actuator drivers and attempt to use them in an MVP UI architecture. Remember this is my MVP, there are many ways to implement MVP but this one is mine. [5] Let’s start with an example. You have an application using a PIR motion detector and an LED. When the motion detector senses enough movement the LED will turn on. The LED would be an actuator and the motion detector a sensor. From an MVP point of view your LED is part of the View as it is the visible output of the app and the motion detector is part of our Model as it is involved in the logic for movement , especially the rule “enough movement”. The presenter mediates between the two and isn’t important in this context. Following this train of thought the naming of your peripherals can follow the same pattern. You would have PirMovementSensor and MotionLedActuator . This improves the consistency of your code and makes future peripheral lookups easier [6] . The domain is clearly expressed in the above application. You know where to look for each class and the pattern that you would use for adding any new features. The peripherals used are clearly identifiable and separated based on behaviour so you could easily find them. As an example the MotionLedActuator would sit in the View and be controlled by the Presenter. The Presenter knows when the app is starting (onCreate) or when the LED needs to be turned on (from the Model). The actuator is very dumb and its interface exposes a lot of methods for the Presenter to control it: public class MotionLedActuator implements LedActuator {\n\n   private final Gpio bus;\n\n   public MotionLedActuator(Gpio bus) {\n       this.bus = bus;\n   }\n\n   @Override\n   public void startup() {\n       try {\n           bus.setDirection(Gpio.DIRECTION_OUT_INITIALLY_LOW);\n       } catch (IOException e) {\n           Log.e(\"blog\", \"Failed to startup.\", e);\n       }\n       turnOff();\n   }\n\n   @Override\n   public void turnOn() {\n       try {\n           bus.setValue(true);\n       } catch (IOException e) {\n           Log.e(\"blog\", \"Failed to turn on.\", e);\n       }\n   }\n\n   @Override\n   public void turnOff() {\n       try {\n           bus.setValue(false);\n       } catch (IOException e) {\n           Log.e(\"blog\", \"Failed to turn off.\", e);\n       }\n   }\n\n   @Override\n   public void shutdown() {\n       try {\n           bus.close();\n       } catch (IOException e) {\n           Log.e(\"blog\", \"Failed to shut down.\", e);\n       }\n   }\n\n} The sensor and actuator interfaces allow for testability and replacement of peripherals with either alternative hardware or mock implementations. Read more in this blog post on testing Android Things . Conclusion Understanding the problem space, including Peripheral I/O and protocols, is the first step to a more indepth insight into your architecture and creating cleaner code. This can help you identify which hardware peripherals are available and how you can split these up into two groups: sensors for input, actuators for output. Structured naming conventions can increase clarity when you’re reading code and searching for classes. Also helping communication within your team because everyone understands what an actuator/sensor is, everyone is on the same page during discussions. Having an architectural separation between components allows you to fit Android Things peripherals into the MVP UI design pattern, leading to a cleaner codebase. The key naming takeaway: Name your input peripherals *Sensor.java Name your output peripherals *Actuator.java This assumes that you already structure your codebase by domain and describes the substructure that comes below that in your architecture. DDD ↩︎ https://en.oxforddictionaries.com/definition/sensor ↩︎ If you’re wondering about the Raspberry Pi Rainbow Hat, this is a combination of multiple sensors and actuators. In your architecture your individual objects should not know of the hat itself, just of the bits it is made up of - the sensors and actuators. ↩︎ https://en.wikipedia.org/wiki/Actuator ↩︎ https://en.wikipedia.org/wiki/Rifleman's_Creed ↩︎ For example, in Android Studio on Mac, press CMD + O, then type *Sensor to see all sensor peripherals. ↩︎", "date": "2017-02-02"},
{"website": "Novoda", "title": "Testing your first Android Things driver", "author": ["Xavi Rigau"], "link": "https://blog.novoda.com/writing-your-first-android-things-driver-p2/", "abstract": "Controlling any peripheral device from an Android Things board requires a driver. If no driver is available it's down to you to write your own. Once that's written, how do you know it will keep working? You need to test it. Let’s take a look at unit testing for Android Things device drivers. In my previous blog post we learnt about input/output protocols to write an Android Things driver for a new peripheral device. Taking the WS2801 chip protocol as an example and writing a driver to control its LEDs. In this blog post we’ll discuss how the driver can be unit tested to give us the confidence to use for a real project. Testing to complete our hacking This is probably the most important part of writing a driver, especially if you're going to share it with others: we have to make sure it works and we have to prove it with tests. If you haven't read this blog post on testing Android Things by my colleague Paul Blundell then I'd recommend you take a look now. It includes useful insights on how to tackle testing an Android Things app when peripheral devices are involved (spoiler: it doesn't involve peripherals). As a TLDR: write your code as if the hardware was a replaceable component, just an implementation detail, so you can swap it with a fake implementation (or a mock) and test your code easily. Android Things driver testing Looking specifically at testing a driver, you can apply the same principle. This means the SpiDevice , Gpio or whatever component you use, should be an external collaborator that gets injected into the constructor (this may sound similar to the dependency inversion principle ). This will allow you to use a mock object during your tests to abstract you from the platform. Here’s how the first iteration of Ws2801Test.java might look: @RunWith(MockitoJUnitRunner.class)\npublic class Ws2801Test {\n\n  @Mock\n  private SpiDevice device;\n\n  private Ws2801 driver;\n\n  @Before\n  public void setUp() throws IOException {\n    driver = new Ws2801(device);\n  }\n\n  @Test\n  public void configures1MHzClockFrequencyWhenCreated() throws Exception {\n    verify(device).setFrequency(1_000_000);\n  }\n\n  @Test\n  public void configuresClockToTransmitOnLeadingEdgeModeWhenCreated() throws Exception {\n    verify(device).setMode(SpiDevice.MODE0);\n  }\n\n  @Test\n  public void configuresBusToSend8BitsPerColorComponentWhenCreated() throws Exception {\n    verify(device).setBitsPerWord(8);\n  }\n\n  @Test\n  public void writesToSpiDeviceWhenWriting() throws Exception {\n    int[] anyColors = {Color.RED, Color.DKGRAY, Color.GREEN, Color.WHITE, Color.YELLOW};\n    driver.write(anyColors);\n\n    verify(device).write(any(byte[].class), anyInt());\n  }\n} One problem you may face when unit testing a driver is that you may be relying on some of Android’s built in utilities (mainly static methods) that aren’t available in a test environment as these tests will run on your computer using a mockable android.jar [1] . In my case I had a method in Ws2801.java that returned a byte[] with the red, green and blue values of a given colour. It looked like this: byte[] getOrderedRgbBytes(int color) {\n    int r = Color.red(color);\n    int g = Color.green(color);\n    int b = Color.blue(color);\n    switch (ledMode) { // The LED mode is an enum that specifies the order the red, green and blue values will be sent to the LED strip\n      case RBG:\n        return new byte[]{(byte) r, (byte) b, (byte) g};\n      case BGR:\n        return new byte[]{(byte) b, (byte) g, (byte) r};\n      case BRG:\n        return new byte[]{(byte) b, (byte) r, (byte) g};\n      case GRB:\n        return new byte[]{(byte) g, (byte) r, (byte) b};\n      case GBR:\n        return new byte[]{(byte) g, (byte) b, (byte) r};\n      default:\n        throw new IllegalArgumentException(ledMode.name() + \" is an unknown Mode.”);\n    }\n  } When executing the above method during a JUnit test, a Stub! error would be thrown because the methods in Color couldn’t be mocked. Color is an Android framework class with static methods. To avoid this Stub! problem there’s a couple of things you can do: Use mocking framework that makes use of bytecode manipulation to mock static methods, such as Powermock . Extract the behaviour that relies on the non-mockable components and test them separately (without testing the framework). I went for option 2, since mocking static methods felt a bit like cheating. I extracted a ColorUnpacker class to hold that method, which can be unit tested separately and mocked in the original tests. This is the result: class ColorUnpacker {\n\n  private final Mode ledMode;\n\n  ColorUnpacker(Mode ledMode) {\n    this.ledMode = ledMode;\n  }\n\n  byte[] unpack(int color) {\n    int r = Color.red(color);\n    int g = Color.green(color);\n    int b = Color.blue(color);\n    return getOrderedRgbBytes(ledMode, (byte) r, (byte) g, (byte) b);\n  }\n\n  static byte[] getOrderedRgbBytes(Mode ledMode, byte r, byte g, byte b) {\n    switch (ledMode) {\n      case RBG:\n        return new byte[]{r, b, g};\n      case BGR:\n        return new byte[]{b, g, r};\n      // … etc.\n    }\n  }\n\n} Now the setUp method of the Ws2801Test class should look something like this: @Mock\n  private ColorUnpacker unpacker;\n\n  @Before\n  public void setUp() throws IOException {\n    driver = new Ws2801(device, unpacker);\n    when(unpacker.unpack(anyInt())).thenReturn(new byte[]{0, 1, 42}); // The values in the array don’t matter as long as it’s the right size\n  } With this, changing the direction of the LED strip is very easy to test because we only need to verify that the unpack method in ColorUnpacker gets called as many times as we expect with the expected values. Since the new class is small and simple, we can easily unit test it too: public class ColorUnpackerTest {\n\n  private static final byte R = (byte) 111;\n  private static final byte G = (byte) 222;\n  private static final byte B = (byte) 333;\n  // The values of the constants above don’t matter\n\n  @Test\n  public void orderedBytesWhenModeIsRBG() {\n    Ws2801.Mode mode = Ws2801.Mode.RBG;\n\n    byte[] result = ColorUnpacker.getOrderedRgbBytes(mode, R, G, B);\n\n    assertBytesOrder(result, R, B, G);\n  }\n\n  @Test\n  public void orderedBytesWhenModeIsBGR() {\n    Ws2801.Mode mode = Ws2801.Mode.BGR;\n\n    byte[] result = ColorUnpacker.getOrderedRgbBytes(mode, R, G, B);\n\n    assertBytesOrder(result, B, G, R);\n  }\n\n  // … and so on\n\n  private void assertBytesOrder(byte[] bytes, byte... order) {\n    assertEquals(order[0], bytes[0]);\n    assertEquals(order[1], bytes[1]);\n    assertEquals(order[2], bytes[2]);\n  }\n\n} Notice that in Ws2801Test.java we’re verifying the internal behaviour of the Ws2801 driver and ensuring it interacts correctly with the SpiDevice . In ColorUnpackerTest.java we’re asserting if a function returns the expected result given predefined inputs. Here's the full source code for the unit tests . Conclusion We made it! At this point we can use this driver in multiple projects really easily and have the confidence from our test that it behaves correctly. We could even publish the driver in a maven repository so that it can be used by other developers. I hope you enjoyed the process and found this informative. I think once you’ve made one driver, the next time should be a lot easier as you’ll be familiar with the terminology in datasheets and you’ll have some knowledge of which pins can be used for what in your Android Things device. To learn more, download the Android Things SDK documentation . It will go up on the Android Developer website once it comes out of preview. You can find the full source code of this blog post on Github and please feel free to chat with me about this on Twitter or Google+ . Happy coding and happy hacking! Last but not least I’d like to give a big thanks to Paul Blundell for reviewing this post. By default, the Android Plug-in for Gradle executes your local unit tests against a modified version of the android.jar library, which does not contain any actual code. Instead, method calls to Android classes from your unit test throw an exception. This is to make sure you test only your code and do not depend on any particular behaviour of the Android platform (that you have not explicitly mocked). ↩︎", "date": "2017-02-07"},
{"website": "Novoda", "title": "Downloading apps is emojinal 🚀 - a simple hack to get more downloads", "author": ["Denis Akan"], "link": "https://blog.novoda.com/downloading-apps-is-emojinal-a-simple-hack-to-get-more-downloads/", "abstract": "People ❤️️ emojis. They are a quick and easy way to express a mood, feeling or reaction via chat apps, social media and email. Now Hollywood is even making an Emoji Movie where Patrick Stewart plays 💩. In the last couple of years marketers have started using these smiley faces in email subject lines and push notifications, hoping to increase open rates by making their messages stand out. In the spirit of continuous app store optimisation (ASO) , we decided to find out ourselves if we could increase our downloads by adding emojis to an app’s short description in the Play Store. We decided to specifically test the short description as it’s seen by the most visitors. Adding the 🚀 emoji to the short description increased downloads by up to 20% in Germany. Localise your A/B Tests Thanks to the integrated A/B testing tool in Google’s Developer Console , hypotheses such as “ emojis in the short description generate more downloads ” are easily testable. After we identified the emojis we wanted to compare ( 🚀 🚮 💯 ), we set up a multivariate test by adding these variations to the existing short description. We didn’t change the copy or any other aspects so the emojis would be tested in isolation to give the clearest results. Then we ran localised tests in several languages to see if our findings were consistent. Each variant was rolled out to 16.6%-25% of visitors. Based on the impact of each variant, the tests would then run for between two and seven days. The exact length of the tests is determined by the Google testing tool, which decides when it has gathered enough information to form a conclusive result. Our Findings We ran this test in eight different languages and saw positive results in 50% of the languages/countries. Using emojis in the short description had a significant positive impact on downloads in Germany [German], Italy [Italian] and Poland [Polish] of up to 20%. Winner in Germany: 🚀 Winner in Italy: 💯 Winner in Poland: 🚮 In English speaking countries on the other hand, the original short description generated more downloads than the versions with emojis. There is one caveat to these results: we haven’t (yet) localised the English language listing so all English-speaking countries see the same version. Once we are able to break out the different countries we can explore the differences further. Winner in English: No Emoji 😢 Some of our tests returned inconclusive results. This means the differences were too small to get a clear indication on whether emojis had a positive or negative impact on the download rate. These countries were Russia, Spain and France. Results in Russia: 😕 Results in Spain: 😕 Results in France: 😕 Conclusion 🏁 The key take-aways from these tests are: App store optimisation (ASO) has real and measurable effects on your revenue KPIs 💸 Emojis can boost your downloads in some countries 📈 But not in all countries 😕 Different countries like different emojis 🙀 Localise your tests to get the best results 🌎 By constantly optimising and testing the Play Store listing, we have increased our global conversion rate (view / download) by over 4.5%. This translates into hundreds of thousands of new downloads and paying customers. Time to start optimising and testing your listing! 😉 [UPDATE 30.01.17] We've split our US listing into several regions and tested the effects on Emojis in the UK's listing with positive results! With the 🚀 emoji, the download conversion went up by up to 7.1%! Winner in UK: 🚀", "date": "2017-01-26"},
{"website": "Novoda", "title": "Testing persistence in the Android ecosystem", "author": ["Luis G. Valle"], "link": "https://blog.novoda.com/testing-persistence-in-the-android-ecosystem/", "abstract": "Growing Object-Oriented Software: Guided by Tests is the best book I’ve ever read about testing. Because it’s not about testing. Yes, the book talks a lot about clean and readable tests. But tests are just a tool to guide you to create good object oriented software . The authors walk you through the principles of well-designed object-oriented software and, incidentally, how tests can help you achieve that. In this post, I want to share what I learnt in Growing Object-Oriented Software: Guided by Tests about testing persistence and how to apply it to the Android ecosystem . Persistence refers to the ability of an object to survive the process that created it. One way to achieve that goal is by saving or persisting its state in a database. When dealing with persistence, you will usually have to rely on third-party code to do the heavy lifting for you. A critical point about third-party code is that you don't own it. You need to take it as you find it and focus on the integration between your system and the external code. When integrating with a third party database API (eg Android SQLiteDatabase ) you need to test that your implementation does all these things correctly: Sends correct queries Maps correctly between objects and the database schema Performs updates and deletes that are compatible with database integrity constraints Interacts correctly with the database transaction manager Releases external resources correctly etc... When testing persistence you need to pay extra attention to test quality. There are several components involved that your tests need to set up correctly. And there is always a persistent state that can make your tests interfere with each other. This example uses a simple TODO app , specifically exploring its persistence layer. A user can create tasks with a name and an expiration date. The task list can be filtered so only non-expired tasks are displayed. The persistence layer of the system is represented in the figure below. Isolate tests that affect persistent state Persistent data stays around from one test to the next, so you need to take extra care to ensure persistence tests are isolated from one another. In database tests this means deleting rows before a test starts . This cleaning process will depend on the database's integrity constraints (foreign keys, cascade deletes, etc.) Cleaning persistent data on test start and not on test finish has two main advantages: Test data remains after a test finish so it’s easier to diagnose failures. Returning to initial state after testing can be risky. It can lead to tests not doing anything at all and still passing. Database cleaning constraints, like tables delete order, should be captured in one single place, since the database scheme tends to evolve. You can use DatabaseCleaner to group those constraints. In this example it only contains one table, but you could add more in a real implementation. public class DatabaseCleaner {\n\n    private static final String[] TABLES = {\n            // Add tables to delete here\n            TaskEntry.TABLE_NAME\n    };\n\n    private final TaskReaderDbHelper dbHelper;\n\n    public DatabaseCleaner(TaskReaderDbHelper dbHelper) {\n        this.dbHelper = dbHelper;\n    }\n\n    public void clean() throws SQLException {\n        SQLiteDatabase sqLiteDatabase = dbHelper.getWritableDatabase();\n\n        for (String table : TABLES) {\n            sqLiteDatabase.delete(table, null, null);\n        }\n\n        dbHelper.close();\n    }\n} It uses a list of tables to ensure correct cleaning order. Now you can add this to your test suite setup method to clean up the database before each test: @RunWith(AndroidJUnit4.class)\npublic class ExamplePersistenceTest {\n\n    @Before\n    public void setUp() throws Exception {\n        [...]\n\n        DatabaseCleaner cleaner = new DatabaseCleaner(dbHelper);\n        cleaner.clean();\n    }\n    \n    [...]\n    \n} Do commit! A common testing technique to isolate tests is to run each test in a transaction and roll it back at the end of the test. The problem with this technique is that it doesn't check what happens on commit. A database checks integrity constraints on commit . A test that never commits won't be fully checking how the class under test interacts with the database. This committed data could also be useful for diagnosing failures. So tests should make it obvious when transactions happen. In this case, every call to TaskRepository.persistTask() will commit to the database directly. That is our transaction boundary. Testing an object that performs persistence operations You can use the above previous setup to start testing objects that persist data in a database. In our domain model, a TaskRepository represents all the operations you can perform around saving or reading: you can add new tasks to the database, find tasks by name and find tasks that are already expired. public class TaskRepository {\n    [...]\n    public void persist(Task task) {...}\n    public List<Task> tasksExpiredBy(Date date) {...} \n    public Task taskWithName(String taskName) {...}\n} TaskRepository has two collaborators: TaskStorage which does the low level communication with the Android SQLite database. TaskMapper which converters from domain model objects to database objects. When unit-testing code that uses a TaskRepository as collaborator you can mock the interface directly. There is no need for real database access. Classes using TaskRepository to persist and query tasks need to trust their collaborators to work correctly. It is the collaborator responsibility to deal with a real database but, from an external point of view, TaskRepository just returns tasks stored somewhere. However, when testing TaskRepository , you need to be sure it queries and maps objects into the database correctly . In the test below we exercise the tasksExpiredBy method: @RunWith(AndroidJUnit4.class)\npublic class TaskRepositoryTest {\n    private static SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd\");\n\n    private TaskRepository taskRepository;\n\n    @Before\n    public void setUp() throws Exception {\n        Context appContext = InstrumentationRegistry.getTargetContext();\n\n        TaskReaderDbHelper dbHelper = new TaskReaderDbHelper(appContext);\n        TaskDBStorage storage = new TaskDBStorage(dbHelper);\n        TaskMapper mapper = new TaskMapper();\n\n        taskRepository = new TaskRepository(mapper, storage);\n\n        DatabaseCleaner cleaner = new DatabaseCleaner(dbHelper);\n        cleaner.clean();\n    }\n\n    @Test\n    public void findsExpiredTasks() throws Exception {\n        String deadline = \"2017-01-14\";\n\n        addTasks(\n                aTask().withName(\"Task 1 (-Valid-)\").withExpirationDate(\"2017-01-31\"),\n                aTask().withName(\"Task 2 (Expired)\").withExpirationDate(\"2017-01-01\"),\n                aTask().withName(\"Task 3 (-Valid-)\").withExpirationDate(\"2017-02-11\"),\n                aTask().withName(\"Task 4 (-Valid-)\").withExpirationDate(\"2017-02-14\"),\n                aTask().withName(\"Task 5 (Expired)\").withExpirationDate(\"2017-01-13\")\n        );\n\n        assertTasksExpiringOn(deadline,\n                containsInAnyOrder(\n                        aTaskNamed(\"Task 2 (Expired)\"),\n                        aTaskNamed(\"Task 5 (Expired)\"))\n        );\n    }\n\n    private void addTasks(final TaskBuilder... tasks) {\n        for (TaskBuilder task : tasks) {\n            taskRepository.persist(task.build());\n        }\n    }\n\n    private void assertTasksExpiringOn(String deadline, Matcher<Iterable<? extends Task>> taskMatcher) throws ParseException {\n        Date date = dateFormat.parse(deadline);\n        assertThat(taskRepository.tasksExpiredBy(date), taskMatcher);\n    }\n} Interesting things happening in this test: addTasks method receives a builder to set up name and task expiration date. The expiration date is the most significant field for this test, so you can create tasks with different dates around the deadline to test boundary conditions. Each task name is self-describing to easily identify instances in case of failure. assertTasksExpiringOn runs the query you are testing and checks the results. containsInAnyOrder returns a matcher that checks for elements in a collection. aTaskNamed is a custom matcher that checks whether an object is a Task with a given name. This test implicitly exercises TaskRepository.persistTask when setting up the database for the query. The relationship between adding a task and querying that task is something that matters to the app domain logic, so you shouldn’t need to test persistTask independently. ( If there is an effect on the system by persistTask that is not checkable using tasksExpiredBy then you have bigger problems you will need to address before considering adding a separate test for persistTask ) This test shows a very nice example of using custom matchers for better test structure and readability. So, why not learn how to create your own? Here is an implementation of TaskRepository that passes the test: public class TaskRepository {\n    private final TaskMapper taskMapper;\n    private final TaskStorage taskDBStorage;\n\n    public TaskRepository(TaskMapper taskMapper, TaskDBStorage taskDBStorage) {\n        this.taskMapper = taskMapper;\n        this.taskDBStorage = taskDBStorage;\n    }\n\n    public void persistTask(Task task) {\n        TaskDBModel taskDBModel = taskMapper.fromDomain(task);\n        taskDBStorage.insert(taskDBModel);\n    }\n\n    public List<Task> tasksExpiredBy(Date date) {\n        long expirationDate = date.getTime();\n\n        return dbTasksToDomain(taskDBStorage.findAllExpiredBy(expirationDate));\n    }\n\n    public Task taskWithName(String taskName) {\n        TaskDBModel taskDBModel = taskDBStorage.findByName(taskName);\n        return taskMapper.toDomain(taskDBModel);\n    }\n\n    @NonNull\n    private List<Task> dbTasksToDomain(List<TaskDBModel> allExpiredBy) {\n        List<Task> tasks = new ArrayList<>();\n        for (TaskDBModel taskDBModel : allExpiredBy) {\n            tasks.add(taskMapper.toDomain(taskDBModel));\n        }\n        return tasks;\n    }\n} Testing mappings Round-trip tests check that the mappings to and from the database are configured correctly. Mappings can be defined by code or configuration, and errors on those are very difficult to diagnose. You need to round-trip all the possible object types that could be persisted in the database . For that, you can use a list of test data builders for those types. You can use these builders more than once, with different setups, to create round-tripping entities in different states. This test goes through the list of builders, creates and persists an entity in one transaction and retrieves and compares the result in another one. @RunWith(AndroidJUnit4.class)\npublic class PersistabilityTest {    \n    List<TestBuilder<Task>> persistentObjectBuilders = Arrays.<TestBuilder<Task>>asList(\n    \t\t// Add different Task configurations here\n            aTask().withName(\"A task\").withExpirationDate(\"2017-03-16\"),\n            aTask().withName(\"A task (with date in the past)\").withExpirationDate(\"2017-01-01\")\n    );\n\n    @Before\n    public void setUp() throws Exception {\n        [...] // Same as before\n    }\n\n    @Test\n    public void roundTripsPersistentObjects() {\n        for (TestBuilder builder : persistentObjectBuilders) {\n            assertCanBePersisted(builder);\n        }\n    }\n\n    private void assertCanBePersisted(TestBuilder<Task> builder) {\n        assertReloadsWithSameStateAs(persistedObjectFrom(builder));\n    }\n\n    private void assertReloadsWithSameStateAs(Task original) {\n        Task savedTask = taskRepository.taskWithName(original.getName());\n        assertThat(savedTask, equalTo(original));\n    }\n\n    private Task persistedObjectFrom(TestBuilder<Task> builder) {\n        Task original = builder.build();\n        taskRepository.persistTask(original);\n        return original;\n    }\n    \n} persistedObjectFrom asks its given builder to create an entity and persists it. Then it returns that entity for later comparison. assertReloadsWithSameStateAs retrieves the given entity from the database using its name and calls a matcher to check if the two copies are the same. Round-tripping related entities Things get complicated when there are relationships between entities. Database constraints are violated if you try to save an entity without related existing data. To fix this you need to make sure that related data exists in the database before saving an entity for a round-trip test . For example, let’s say you were to add Lists to your model so each Task belongs to a List . Your Task round-trip tests will need to change to insert a dummy List before inserting tasks in the database. A good way to do that is to delegate the creation of related data to another builder. The builder of the entity under test ( Task ) will use the builder of the other entity it depends on ( List ) to persist the related data (a dummy list) before the entity under test is persisted. You could change your tests to include a ListBuilder decorated like this: private TestBuilder<List> persisted(final TestBuilder<List> listBuilder) {\n    return new TestBuilder<List>() {\n        @Override\n        public List build() {\n            List list = listBuilder.build();\n            listRepository.persistList(list);\n            return list;\n        }\n    };\n} See more about round-tripping related entities in this test . Conclusion Persistence tests are more complicated than regular ones. They require more setup and scaffolding making them difficult to read, maintain and evolve. But don’t let that put you off. There are two main things to remember : Test whether the find queries return the data they are supposed to return (this will test insertions as well). Test whether the data mappings convert the persistence model to the domain model correctly. Covering those will give you a high level of confidence that your persistence logic is working correctly. You will have unit tests for the individual components, like mappers, and integration tests to make sure the whole abstraction built for persistence is working correctly. And by following the ideas and principles described in this post, like custom matchers, your tests will stay nice and clean. A working example can be found here .", "date": "2017-01-31"},
{"website": "Novoda", "title": "Improving completion blocks in Swift", "author": ["Alex Curran"], "link": "https://blog.novoda.com/improving-completion-blocks-in-swift/", "abstract": "Swift as a programming language focuses on making APIs descriptive and determinate. Completion blocks are less than perfect — but what is wrong with them, and how can we improve their usage? The completion block is a very familiar pattern in both Objective-C and Swift. It is a useful feature that allows us to handle asynchronous actions whilst keeping the method call and the resultant code close together. Completion blocks are found all over iOS code, such as this example taken from URLSession : let task = URLSession.shared.dataTask(with: aUrl, completionHandler: { (data, response, error) in\n    // handle the result here\n})\ntask.resume() So long as you avoid callback hell , they’re a short and easy-to-write way of handling a single result from a method; if you have multiple outcomes or calls you’re better off using the delegate pattern . Completion blocks have problems but they’re not always obvious to see, which we can see with an implementation of one: let task = URLSession.shared.dataTask(with: aUrl, completionHandler: { (data, response, error) in\n    if let data = data {\n      parse(data.asJSON)\n    } else if let error = error {\n      display(error)\n    } else {\n      // no data and no error... what happened???\n    }\n})\ntask.resume() Hopefully now the problem is more obvious! The Single Responsibility Principle tells us “a class should only have one reason to change” , but it should also apply to functions. In the case of this function, it does multiple things — it first determines whether the response was successful, and then does actions according to that result. Not only that, but we should try to avoid conditionals or abstract them as best we can. So we know we should split these completion block apart. The other problem is made far more obvious by Swift’s type system. Take a look at the signature of the completion block: (Data?, URLResponse?, Error?) -> Void So, when the request completes, we may get some Data , we may get a URLResponse , and we may get an Error .As iOS developers, we know by convention that we'll either get data or an error, not both. But this is not enforced in the API; it is completely feasible by the API's design that it could return data and an error at the same time. As for the URLResponse , you have to dig into the documentation to see when you will receive one of these. This is a problem because a convention is just another way of saying something requires implicit knowledge. If you didn't know about iOS conventions, you wouldn't be able tell what this method returns under what conditions without Googling it or hunting API documentation. Looking at it another way, the conventions is that returning some data or an error are two mutually exclusive outcomes — if one happens, the other will not. However, the API does not represent that these are mutually exclusive — rather the API, by having all three parameters as optional, declares that we could get any mix of them at any time! We could remove a lot of the ambiguity in this code and make it easier to use for clients of this API. Refactoring a better solution using functions Using the interesting concept of higher-order functions , we can improve the API of a lot of these functions (except for URLSession , we’ll address that one at the bottom of this post). Most completion blocks take the signature, where Result can be any piece of useful data: (Result?, Error?) -> Void What we should be aiming for instead (remembering the term mutually exclusive ) is two blocks, one taking the form: resultHandler: (Result) -> Void And another with the form: errorHandler: (Error) -> Void If any of you are familiar with RxSwift , then these will look pretty familiar . To implement something wrapping this functionality, we can use generics: func completion<Result>(onResult: @escaping (Result) -> Void, onError: @escaping (Error) -> Void) -> ((Result?, Error?) -> Void) {\n    return { (maybeResult, maybeError) in\n        if let result = maybeResult {\n            onResult(result)\n        } else if let error = maybeError {\n            onError(error)\n        } else {\n            onError(SplitError.NoResultFound)\n        }\n    }\n}\n\nenum SplitError: Error {\n    case NoResultFound\n} This function creates a closure which will use two separate closures to handle the results. Here's a before and after using CLGeocoder : CLGeocoder().geocodeAddressString(location, completionHandler: { [weak self] (maybePlaces, maybeError) in\n    if let places = maybePlaces {\n        self?.handleGeocoding(places: places)\n    } else if let error = maybeError {\n        self?.handleError(error: error)\n    } else {\n        // what now??\n    }\n}) And here’s the implementation with our closures: CLGeocoder().geocodeAddressString(location, completionHandler: completion(\n    onResult: { [weak self] places in\n        self?.handleGeocoding(places: places)\n    },\n    onError: { [weak self] error in\n        self?.handleError(error: error)\n    })) The benefit here is that no longer do we have to deal with Optionals everywhere in the result, which makes our code more direct. The other benefit is now that the two data flows in the result are separated into two distinct cases — one where the request succeeds, and one where it fails. This reduces boilerplate, ambiguity and also allows us to use function pointers to write succinct, readable code: CLGeocoder().geocodeAddressString(location, completionHandler: completion(\n    onResult: zoomToFirstPlace,\n    onError: showToast)) Refactoring URLSession We can’t use our completion function defined above in the case of URLSession as the result is two separate objects — Data and a URLResponse . But if we think of the result to be both of these objects combined , it becomes clearer what we can do. If an error happens, and we don’t care about the URLResponse in that case, we can define a struct to encapsulate the Data and URLResponse : struct Response {\n  let data: Data\n  let metadata: URLResponse?\n}\n\nextension URLSession {\n\n  func dataTask(with url: URL, completion: @escaping ((Response?, Error?) -> Void)) -> URLSessionDataTask {\n    return dataTask(with: url, completionHandler: { (maybeData, maybeResponse, maybeError) in\n      if let data = maybeData {\n        completion(Response(data: data, metadata: maybeResponse), nil)\n      } else if let error = maybeError {\n        completion(nil, error)\n      }\n    })\n  }\n\n} Which would then allow us to use our function as we see fit: URLSession.shared.dataTask(with: aUrl, completion: completion(\n    onResult: parseResponseAsJSON,\n    onError: tryCachedVersion\n)) Summing up Completion blocks, whilst useful, aren’t quite perfect. They make code less direct and make it easier for bugs to creep in. Splitting out our completion blocks into separate data flows allows us to more easily keep our happy path and error handling separate. Removing the handling of optionals allows us to write our code more in terms of what we expect to do in these conditions rather than what the language tells us we have to do. Swift’s first class functions allow us to do this is in a maintainable yet approachable way. You can find the function we used above to make our completion blocks in this gist . If you have any more tips about working with completion blocks, why not tweet me !", "date": "2017-01-24"},
{"website": "Novoda", "title": "Writing your first Android Things driver", "author": ["Xavi Rigau"], "link": "https://blog.novoda.com/writing-your-first-android-things-driver-p1/", "abstract": "Android Things is a lightweight version of the Android operating system that can be installed on IoT-focused devices. Android Things includes 'user drivers' functionality, enabling developers to easily control a wide range of peripherals (LEDs, switches, buttons, servo motors etc). Let’s take a look at what it takes to write a new peripherals driver for Android Things. At the time of writing, Google hasn't announced what the requirements will be for a piece of hardware to run Android Things but the hardware documentation states that they're focusing on system on module (SoM) boards. While the only supported development boards for Android Things are the Raspberry Pi 3, the Intel Edison and the NXP Pico i.MX6UL, it would seem that almost any board will be able to run Android Things. The supported boards don't offer many sensors, LEDs and buttons to play with, but once an Android Things device is up and running you can easily plug in peripherals. User drivers One of the things that got me excited about working on an Android Things project was the idea of user drivers . As shown in the above image, developers can use the Android Framework APIs directly and build their apps using user drivers. These are a thin layer that accesses and controls a peripheral piece of hardware from the Android Things app. This allows us to focus on what matters: writing an awesome app. You can find a list of the available user drivers on Github . The cool thing about these drivers is that they're like any other Android library, meaning you can add one line to the project's build.gradle dependencies and the driver will be imported. What happens if you want to use a peripheral that doesn't have an Android Things driver yet? You have 2 options: Pick a different peripheral and hope there's a driver for that. Go the DIY-way and write the driver yourself. If you take the second option, you can contribute back to the open source community by sharing the driver to let anyone (including you) reuse that code in future projects. To do this you’ll need the peripheral documentation that the vendor provides [1] . Hardware Hacking First of all you have to understand which communication protocol the peripheral uses. Android Things has support for the following types of communication: General-purpose input/output ( GPIO ): This is the simplest way of communicating, you can use it to read from the peripheral (input) and write to the peripheral (output) [2] . Each physical pin represents either an input or an output (you can configure each pin mode from your app) and it can take only two values: up and down (or 1 and 0). Use GPIO with buttons, motion sensors, etc. Pulse Width Modulation ( PWM ): Similarly to GPIO, PWM uses one physical pin but it is output-only (meaning the Android Things board sends data, never reads). Using PWM lets you control more complex devices such as servo motors, dimmable lights, and other devices that can take a wider range of values, rather than just 1 or 0. See the documentation of PWM for further reading on this interface. Serial Peripheral Interface ( SPI ): If your peripheral is a bit more complex it will probably need SPI, which is used in a wide range of peripherals, from RGB LED strips to graphical displays. SPI allows a master (in this case the Android Things device) to communicate with one or more slaves. [3] It uses either one uni-directional data exchange pin or two bi-directional exchange pins for data transfer, one pin for clock and, in the case of multiple slaves, it needs extra Chip Select lines . Inter-Integrated Circuit ( I2C ): I2C is based on two pins since it needs the data connection and a clock. Similarly to SPI, I2C can be used with one or more slaves, but each slave has its own address that the master can specify. In this case, the master sends a byte of data at a time and the slave has to acknowledge if it has received it correctly. This acknowledgement is built-into I2C. This protocol is used in some sensors, LCD displays etc. Universal Asynchronous Receiver Transmitter ( UART ): While SPI and I2C are both synchronous interfaces (because they both need a clock line to synchronise), UART is asynchronous. A simple implementation of UART needs two pins: one data in and one data out. Unlike SPI and I2C, UART doesn't support multiple slaves and the data is wrapped in data frames. A data frame includes a start bit, 5-9 data bits, possibly a parity bit and one or two end bits. UART is used with GPS devices, XBee radios, some printers etc. Read the documentation from the peripheral vendor to find out what to use. If it uses one of the above methods then implementing a driver should be quite straight forward. Otherwise you'll have to implement whichever protocol it uses with GPIO ports. Investigating the driver protocol I bought this RGB LED strip with the code WS2801 printed on both chips. This is the datasheet for the WS2801 , which I found by searching for ‘WS2801 datasheet’. In the PDF, the features list mentions that the chip uses PWM, but don’t let that fool you. On page five there’s a list of input and output pins and you can see the chip has a clock input (CKI), a data input (SDI), a power supply (VCC) and ground (GND), which are the pins that you can also find in the LED strip. Investigating which protocol to use, the main indicator here is the number of pins needed. While power and ground are generic for most peripherals, the need for a clock and data input as separate pins indicates that you have to use either I2C or SPI (not PWM as that only uses one line). Another indicator is the clock speed: the I2C standard is 400kHz (but can go up to ~3MHz), while the datasheet says the WS2801 accepts a maximum clock frequency of 25MHz. The third and last indicator is the data format: on page 12 it shows that the only data the chip needs to operate is 3 bytes (red, green and blue values) and it will leave the rest of the data for the next chip (ie next LED of the strip). There is no mention of acknowledgement or 2-way communication. This leaves out I2C and UART from the list so you can use SPI . A peripheral datasheet will also specify important values such as minimum and maximum voltages, maximum clock speed and whether the data is read on the leading or trailing clock edge. Once you know which form of communication you have to use (SPI in the above example) you can start thinking about connecting and coding. Connecting the peripheral to the developer board It's time to find out which pins on the board need to be used in order to talk to the peripheral from the Android side. The easiest way is to find the pinout for your development board on the Android Things developer site. A pinout shows which pin can be used for each form of communication. Have a look at the Raspberry Pi 3 pinout for example: In this case I used pin 2 (or 4) for power (VCC), pin 6 (or 9, 14, etc) for ground (GND), pin 23 for clock (CKI) and pin 19 for data (SDI). Note that there are two similar pins: 19 – MOSI, which stands for master out slave in and pin 21 – MISO or master in slave out. In this case I had to use MOSI because the master (Android Things device) will send out data and the slave (LED strip) will read it in . One important thing to note is different boards have different output voltages and different peripherals need different voltages to operate , so be careful. The peripheral may not work and could burn out, so always check the specs before plugging a peripheral into a board and use a level converter if needed. For example, this article on Raspberry Pi 3 GPIO explains that pins can output either 0V(low) or 3.3V (high) and it also has two 5V pins. If the GPIO pins are configured as input then using any value higher than 3.3V will damage the board. Software Hacking At this point you should have a peripheral device plugged in to your Android Things board and you know the protocol you're going to use. So, it's time to start writing some code. Now let's look at how to implement the peripheral driver using SPI. A few days ago I submitted a PR to the drivers repository on Github , adding the driver for WS2801. These are the main things you need to include when writing a driver. Android Things driver creation Start by creating a standard gradle module and in the build.gradle add: apply plugin: 'com.android.library'\n\nandroid {\n    compileSdkVersion 24\n    buildToolsVersion '24.0.3'\n\n    defaultConfig {\n        minSdkVersion 24\n        targetSdkVersion 24\n        // Other default stuff...\n    }\n}\n\ndependencies {\n    provided 'com.google.android.things:androidthings:0.1-devpreview'\n} Remember, this is like any other Android library. You can use the com.android.library plugin and for Android Things you'll need to use at least API 24 as minimum SDK version. You should also specify com.google.android.things:android things as a provided dependency. This means you need it to be able to compile the app, but you don't want to distribute this dependency with the driver once it’s released. Next you need to add one line to your AndroidManifest.xml : <manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.xrigau.driver.ws2801\">\n    <application>\n        <uses-library android:name=\"com.google.android.things\" /> // THIS\n    </application>\n</manifest> Adding this line is the way to instruct the app to only run on Android Things devices rather than phones, tablets etc. Now for the driver itself. Create a new class (I called it Ws2801.java ) and write the driver code. The first thing you’ll need to do is open the SPI port so you can start sending data to it, and then close the port. Here are the main parts: package com.xrigau.driver.ws2801;\n\nimport com.google.android.things.pio.PeripheralManagerService;\nimport com.google.android.things.pio.SpiDevice;\nimport java.io.IOException;\n\npublic class Ws2801 implements AutoCloseable {\n  // ...\n  private final SpiDevice device;\n  \n  public static Ws2801 create(String spiBusPort) throws IOException {\n    PeripheralManagerService pioService = new PeripheralManagerService();\n    try {\n      return new Ws2801(pioService.openSpiDevice(spiBusPort));\n    } catch (IOException e) {\n      throw new IOException(\"Unable to open SPI device in bus port \" + spiBusPort, e);\n    }\n  }\n\n  Ws2801(SpiDevice device) throws IOException {\n    this.device = device;\n    device.setFrequency(1000000); // 1MHz clock frequency\n    device.setMode(SpiDevice.MODE0); // Mode 0 seems to work best for WS2801\n    device.setBitsPerWord(8);\n  }\n\n  public void write(int[] colors) throws IOException {\n    // … some computation\n    Int[] colorsToSend = new int[]{Color.RED, Color.WHITE, Color.parseColor(“#0FACE0”)}; // As many as LEDs in the strip\n    device.write(colorsToSend, colorsToSend.length);\n  }\n\n  @Override\n  public void close() throws IOException {\n    device.close(); // IMPORTANT: If you don’t close it then the resource can’t be opened again.\n  }\n} Notice that first you need to create an instance of the PeripheralManagerService . This is used to get information of the different input/output (I/O) interfaces the development board has, as well as open buses or GPIO pins. In the example code above, I called the openSpiDevice(String spiBusName) method to open the SPI bus interface. Once you have a reference to the SpiDevice you can configure the bus and start sending it data. Note: the code has been simplified, see the full source code on Github . And that’s all you need. [4] Now you can start using the driver. So far we have looked at the different types of inputs and outputs Android Things boards support and how to identify which interface is needed to communicate with a peripheral. We’ve covered which pins to use for each different interface and how to implement a driver to control the peripheral using the Android Things SDK. In my next blog post we’ll look at how to test the peripheral driver to improve your confidence when using or sharing your peripheral driver. If you’d like to discuss this post or give me any feedback on your own experiences for my next blog post please get in touch with me on Twitter or Google+ . Last but not least, I’d like to give thanks to Daniele Bonaldo , Luis Valle and especially to Paul Blundell for reviewing this post. Read the second part of this blog post here: Testing your first Android Things driver When you want to use any peripheral for the first time always check the manufacturer’s website for the datasheet, usually a PDF file. It may also be in the product description on the seller’s website. If you can’t find it on either of those websites then search for the keywords you see written on the chips or the board. ↩︎ GPIO pins cannot be used as an output if they were previously enabled as an input with an edge trigger enabled since the last reboot. ↩︎ A slave can be seen as an individual component on your hardware peripheral, eg each sensor in a rack of sensors. ↩︎ In the existing drivers, there's a .driver-metadata file but it’s only used by the repo maintainers to automatically generate some documentation as explained in this StackOverflow question . ↩︎", "date": "2017-01-17"},
{"website": "Novoda", "title": "The 5 steps of better product design: 4. Visual & motion design", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-5-steps-of-better-product-design-step-4-visual-motion-design-2/", "abstract": "So, you've tested your prototype with users and it's working well. What next? Now it's time to develop the visual design and take time to focus on the intricacies of user interaction. This is step 4 in a 5-piece series exploring product design best practices. This week, we'll be considering ways to prepare and manage designs for a smooth development process. It will also offer a set of guidelines for making  more widely accessible products. Understand Ideate Prototype & test Visual & motion design Measure impact After concluding usability testing outcomes, it’s important to record and share any design decisions with the team to create a unified understanding of the proposed solution. A rough sketch of the user journey can help to clarify screens, states and interactions. Once the journey is defined, it’s important to plan how designs for different platforms, devices, languages and states will be managed. Defining a clear folder structure with clean, well-organised files will make your designs easier to navigate and maintain. At this stage, you can also consider how you might track user behaviour. Early definition of analytics questions will help to ensure tracking is implemented in time for release with the appropriate tracking tools. Data Analytics will be covered further in Step 5: Track & validate. Visual, interaction & motion design A thorough understanding of your company’s brand is essential to create a consistent experience across touch-points. A well-defined style guide with re-usable elements and styles will help to increase efficiency and consistency. A persistent visual language will help to ensure hierarchy, meaning and focus, guiding the user through the experience with ease and efficiency. Here are some things to consider. Grid-based layouts Consistent spacing Clear, legible typography Consistent use of colour Element hierarchy View states & interaction feedback Guiding transitions between states Loading and progress indication Supportive error guidance Inclusive design By considering some basic accessibility guidelines, we are able to open our products to users of all abilities, including those with low vision, blindness, cognitive impairments, or motor impairments. Colour contrast Text should meet a contrast ratio of 4.5:1 for normal size text and 3:1 for large text (14 point, typically 18.66px) against your chosen background colour. This allows users with low screen definition or visual impairments to read your content more easily. You can test your colour scheme using this free online contrast checker . Text resizing Enable text resizing in your app within responsive layouts. This will allow users with visual impairments to set a size appropriate to them. Colour blindness Make sure you’re not using colour as the only indicator for differences between elements. This will allow users with a colour vision limitation, which is roughly 4.5% of the global population, to clearly understand the differences between indicators or actions. View states Define view states for actionable items. This will ensure users interacting with your product via touch or external devices (keyboards, remotes, d-pads, joysticks, switch devices etc) will receive appropriate feedback as they navigate through the interface. Actionable items: Default, focussed, pressed Selectable items: Default, focussed, pressed, selected, selected & focussed, selected & pressed Screen readers Voiceover (iOS) and TalkBack (Android) are accessibility services that help vision-impaired users interact with their devices by providing spoken, audible, and vibration feedback on content and available actions. You can help to enhance these services by: Considering the order of focus and content grouping for sequential navigation Including content descriptions and usage hints on all elements Considering alternative UIs for temporary views Connection speed Don’t forget about users with a poor internet connection. To avoid frustrating these users, give some thought to: Content loading optimisation Progress indication Connection error guidance International languages If you’re designing an international app, consider how your design will respond to localisation, as languages often have much longer words than English so your designs should adapt to accommodate this. Clear layout and content A clear layout will help the interface to appear easy to use and avoid users feeling intimidated or frustrated. Clear error guidance Guide users to avoid mistakes and offer positive support to enable a smooth, guided recovery should an error occur. Time Ensure sufficient time is allowed for communication and interactions. You can find further advice on accessibility at W3C or in the Google Material Design Guidelines . Preparing for development Visual & interaction design specifications Zeplin is the ultimate collaboration tool for designers and developers providing sizes, colours, font information and spacing with a simple cmd+e export from Sketch. It also generates visual assets and allows team members to tag and comment on screens should any questions arise. Motion specification Principle is a great tool for high-fidelity interactive prototypes and a natural choice for motion design. The record function in the preview is useful for sharing motion examples with the rest of the team. Adobe After Effects can be used for more advanced motion and visual effects. Development acceptance criteria When finalising designs, we’ve found it useful to detail the interaction in the development ticket. This highlights any specifics that haven’t been sufficiently detailed and ensures the desired behaviour is clearly outlined for development. During native development, designers and developers can work together to create smooth, natural transitions between states. This can help to save time and avoid endless review cycles. It's also important at this stage to outline analytics tracking points so we are able to measure the success of our product changes at a larger scale. The next post in this series will explore the process of tracking behaviour and KPIs post-release.", "date": "2017-01-05"},
{"website": "Novoda", "title": "The only way to implement Model View Presenter", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/the-only-way-to-implement-model-view-presenter/", "abstract": "The only way to do Model View Presenter (MVP) and you probably didn't even know it existed! Everyone on the internet is talking about MVP and everyone likes to share ' the best way ', little do they know the best way is written in this blog. I will talk about how you can architect MVP to work the best for your team. Also when using this specific design how your team can work together to make awesomeness happen quickly and easily. Doing MVP this way also allows for easily testable code and legacy developers to pick up and add features at the drop of a hat. How do you decide which MVP architecture to use? Here it is. The best way to do MVP is for your team to agree on the approach to and the structure of your architecture. [1] Then to implement it in a way that the aims and intended behaviour are explicitly documented. The actual way your Model, View and Presenter inter-communicate or the roles that they take is not as important as all the team agreeing and understanding together how it will work . Why does the team need to agree on MVP architecture Each developer will be re-working each others code and in the end you have a big ball of mud. Context switching , having multiple ways to implement an architecture means you constantly have to be aware of where you are in the code base. If two objects with the same naming convention (Model, Presenter) have to communicate you would first need to take a step back and see where in the codebase you are. This involves a context switch, and knowing if this is MVP/A or MVP/B before you even start to tackle your original problem, puts more load on your cognitive process and slows you down. Too many cooks can spoil your software broth. With multiple architectures and potentially each team member thinking something differently you will never come to a refined, clean , simple, SOLID architecture. Each developer would be re-working each others code and in the end you have a big ball of mud . Duplication of ideas. When there are different architectures for expressing the same layer of a codebase it makes it less obvious that you may have behavioural similarities. In this case developers would not be able to see potential refactorings where they could simplify, or for example; remove two View objects in favor of one. The best code is no code at all . I could give more reasons ( spaghetti code , DRY , KISS , ...) but I think you understand. Not being consistent and disagreeing about your architecture is bad. Now how do you get your development team to come to a single understanding of what MVP means for your project? How to agree on MVP architecture as a team Meetings aren’t your enemy and sometimes it’s the best way to get people singing from the same song sheet. Agreement starts with talking to each other. It sounds simple but it takes a lot of effort. Explicit communication really helps a team function. Never assume what your other developer team mates are thinking always explicitly state the direction of your thoughts. You’ll be surprised how often someone says “oh I wasn’t thinking of it like that”. Organise a meeting for everyone to talk about MVP. Meetings aren’t your enemy and sometimes it’s the best way to get people singing from the same song sheet. In meetings always make sure an agenda is set and that there is some explicit outcome from the meeting. Talking things through does not come naturally to developers but it’s a skill all of us can improve. Pair programming allows developers to share thoughts and ideas as well as learn from each other. Pairing on an architecture problem like MVP gives a great way for thoughts to be shared by example. You can see what your pair is trying to say in the code in front of you both. You can quickly code up two ideas for an architecture and compare them whilst sitting and talking together. Mob programming or mobbing has the same benefits as pair programming except it relates to the whole team. Get everyone in front of the same screen to show the big ideas, agree on the structure and direction of the architecture. It’s a great way for everyone to input and for everyone to get to the same level of understanding of a problem. If you haven’t tried this yet, it’s highly recommended and you will see the benefits by example if you give it a go. Grab your team, a Kata from here and give it a go. Why do you need to document your MVP architecture Think of documentation as asynchronous communication, allowing developers to learn from you without you being there. Documentation is underrated. There has been a massive backlash on documentation in the developer community, everyone has gone for no documentation when actually literature has been trying to explain minimal documentation . You can think of documentation as asynchronous communication , allowing developers to learn from you without you being there. Documenting any architectural decisions is really important for the codebase and for the team. You can’t always be there to explain the structural decisions, but the documentation will be there. Having documentation makes the implicit, explicit [2] . This means less unknowns, leading to less guess work and less mistakes. You should always consider the legacy developer as well, don’t be selfish now and think ‘oh this is some person in X years time who cares’ , a legacy developer is also you next week [3] , or your new awesome team mate next month. Having the code and architecture documented frees you up to carry on with your tasks and makes their onboarding a lot smoother and faster. How to document your MVP code Explain why the code is here and explain things that are not understandable from the code on its own. Start at the smallest details to encourage a mentality of documentation and explicit communication everywhere. Writing tests allows you to have living documentation. That is your tests can explain expected behaviour of how your architectural elements are supposed to work. You can write tests that show how the view receives data or how the presenter controls aspects of your MVP setup. Always remember your commit messages can help explain why you are doing something. A great tip for explanatory commit messages is that they should be able to complete this sentence “If applied, this commit will...” [4] . Using commit messages to ensure explicit documentation helps everyone on your team, helps them now when you’re working together and helps them in the future when reviewing already written code. Code reviews are a great example of documentation, when you write up the details of the code you have changed you are explaining to others why you have done this, which is quite hard to capture in the code itself. Agreeing through documentation can make the most implicit thoughts of a single developer, explicit for all to read. Being explicit in your class naming can be the first step to documenting your MVP architecture. Here is a very explicit way of showing other developers exactly how your MVP framework is structured through class naming . You end up with self documenting code like this: public interface TweetsMvp {\n\n    interface Model {\n      ...    \n    }\n\n    interface View {\n      ...    \n    }\n\n    interface Presenter {\n      ...    \n    }\n} You want a naming solution that works for your problem and is agreed by your team, “ Naming is hard ”, but there is also another saying ”you have to be in it to win it” . If you don’t think about naming as a documenting tool and try to improve your attempts, then naming will always be hard. Splitting your code into modules allows you to make some implicit boundaries explicit. In Java, modules allow you set in place some rules for what code can go where. For example imagine, a module A with a dependency on library X . This means that module B cannot easily write any code that relies on library X . Unless you add that dependency to B or more likely you should move whatever you are attempted to module A . You can also add a README file to each module to further explain why this separation has occurred. Javadoc allows you to write code comments in a standardised format explaining why some code or a class exists. If you have an architecture with boundaries then each part of the code will have a public API, potentially with multiple team members working on each. It’s important to add comments to these public APIs. Just remember you are trying to explain why the code is here and explain things that are not understandable from the code on its own. Another clue for when to write a comment is when you are using a class from one module in another, this is a boundary that should be explained. Architecture diagrams can be a great team tool, they help understanding of the bigger picture and get consensus on change. The team should agree on a common diagramming syntax and be involved in the creation of architecture diagrams. When you use diagrams it is important that they always resemble the latest thinking in your code base and are kept up to date alongside your project. This is where a tool like the c4 architecture model is beneficial to use, it allows diagrams to be coded and compiled, therefore they form part of your app that you need to keep up to date. Onboarding new team members to a project and even refreshing yourself before a new feature needs to be implemented are great reasons to have and maintain your projects architecture as diagrams. Conclusion There is no single right way to implement Model View Presenter and the one you choose does not matter. What matters is that your team should all agree on your team’s way of implementing this UI architecture. Doing so through explicit code, great team communication, always learning and sharing with each other and documenting for the future. This ensures you will have an easy to understand, agreed and hopefully straight forward code base that your team and any future teammates would be happy to start contributing to. Thanks to @riggaroo , @FMuntenescu , @lgvalle for reviewing the content and my sanity. By architecture here, and throughout the article I mean MVP as an UI architecture . You still need to architect the rest of your app! ↩︎ Don’t worry about the management speak, this skill applies to everyone and being explicit is important for all communication between everyone. ↩︎ Ever come back to your computer Monday morning, looked at the code you wrote Friday afternoon and thought … what the hell was i thinking? That. ↩︎ So many awesome tips for writing great commit messages here , I can’t explain how many “WTF is this code” moments would be resolved if all commit messages were written this way. (fyi usually the code in question is my own written the previous week..) ↩︎", "date": "2017-01-10"},
{"website": "Novoda", "title": "Android Wear packaging", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/android-wear-packaging/", "abstract": "Android Wear 2.0 will be released to the public in 2017, but at the moment there are a lot of smartwatches still running Wear 1.x and for a big part of them there is no guarantee at all that they will receive the update. For this reason it's very important to keep supporting the publishing mechanism used by Wear 1.x devices. With Android Wear 2.0, apps can be distributed as standalone and installed directly from the watch. This requires the creation of a dedicated APK for the watch to be uploaded on the Play Store developer console using the multiple APK approach. Doing so, both the mobile and wearable APKs are independent versions of the application, each one targeting a different category of devices but sharing the same application listing on the Play Store. With the previous Wear version though, the wearable APK needs to be embedded into the phone APK. When creating a release build for our mobile app, we can configure Gradle to automatically add the Wear APK into the mobile one raw resources, as described in the official documentation . What the documentation is really lacking is how to tune the build config in order to take into consideration some real-world scenarios. Let’s have a look at some of these. Multi-dimensions flavors In many apps we need to specify different environments, for example for development and production, each one with different configurations for several components like analytics, endpoints, etc. Usually this is done specifying a productFlavors closure in the mobile build.gradle containing the definition and different configuration for each environment flavor. Another use of flavors is to provide different versions of the apps with or without Play Services. I know it seems difficult to believe, but there are devices out there not including them and even markets that reject every submitted app using any API provided by Play Services. In order to maintain compatibility with these markets and devices, it’s a good practice to create two different flavors of the app, with or without Play Services dependency. In order to make these two requirements coexist, we can define the so called flavorDimensions inside the productFlavors closure and associate every flavor with one specific dimension. For every combination of flavors a specific task will be automatically created. Here’s an example of the described product flavors: productFlavors {\n    flavorDimensions 'environment', 'playServices'\n\n    noPlayServices {\n        dimension 'playServices'\n    }\n    \n    playServices {\n        dimension 'playServices'\n    }\n    \n    development {\n        dimension 'environment'\n    }\n    \n    production {\n        dimension 'environment'\n    }\n} Signing The automatic packaging task will add the Wear APK to the mobile one only if they are both signed with a release (non-debug) key. Also, the Package Manager will check that the two APKs (the mobile and the Wear ones) keys are the same when installing on the smartwatch. To implement this, we extracted the signing config into a shared Gradle file which is referenced by both the mobile and wear build.gradle with apply from: rootProject.file('shared-config/android-signing.gradle') Build types In a real project, different build types are used when developing, for release and for testing. Each build type can have a different configurations, for example regarding signing, API keys, or even different package ids. An important thing to remember is that Package Manager expects the two mobile and wear APKs to have the same package id. So if your app has different package ids for different build types on mobile, you need to have the same structure in wear module. As mentioned before, we want the Wear APK to be embedded only when building for Play Services-enabled devices, and only for release builds. To limit the combinations of build types and flavors which will contain the Wear APK, we specify a list of configurations in the mobile build.gradle and and specify the relative configuration from the Wear module fo be used as dependency. configurations {\n    developmentPlayServicesReleaseWearApp\n    productionPlayServicesReleaseWearApp\n    developmentPlayServicesQaWearApp\n    productionPlayServicesQaWearApp\n}\n\ndependencies {\n    developmentPlayServicesReleaseWearApp project(path: ':wear', configuration: 'developmentRelease')\n    productionPlayServicesReleaseWearApp project(path: ':wear', configuration: 'productionRelease')\n    developmentPlayServicesQaWearApp project(path: ':wear', configuration: 'developmentQa')\n    productionPlayServicesQaWearApp project(path: ':wear', configuration: 'productionQa')\n} The Wear APK is not embedded in the noPlayServices variant of the app. Android Studio Analyze APK tool is a great way to check if the APK has been included or not. I created a simple demo app which shows how to integrate a Wear module into a project with the requirements we just saw. You can find the full demo source code at https://github.com/novoda/android-demos/tree/master/WearBuildConfig", "date": "2017-01-03"},
{"website": "Novoda", "title": "Learning about Firebase", "author": ["Daniel Blatchford"], "link": "https://blog.novoda.com/learning-about-firebase/", "abstract": "At Novoda, we love to share. So, in an effort to help everyone build better products on the Google Firebase platform, we hosted an open office hours hangout on 9 December. Anyone could join and ask questions on how we’ve found it so far and share their experiences with their own products. Our background with Firebase Over the last year or so, we’ve been lucky enough to experiment with Google Firebase on various projects. To find out what was possible  with the Firebase platform we decided to build Smart TV apps using Firebase as the backend - not something that Firebase generally supports. We’ve been impressed with Firebase’s ability to quickly fulfill product needs out of the box. Gone are the days of having to wait for backend teams to deliver functionality to allow us to run simple A/B tests. To create a unified experience for users, we wanted to power mobile apps and Smart TV apps using Firebase, allowing us to engage with users wherever they are, on any of their devices. Where to watch You can watch a video of the hangout on YouTube: https://www.youtube.com/watch?v=MQDP3j2aApo When’s the next one? We’re going to host our next office hours hangout on Friday 20 January, which you are welcome to join to ask questions or share your experiences. We’ll be posting more details about the event in the coming weeks, so follow us on Twitter to stay updated and send us any questions you have. Hope you can join us.", "date": "2016-12-23"},
{"website": "Novoda", "title": "Testing Android Things - IoT meets Java", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/testing-android-things/", "abstract": "Google has just announced the amazing Android Things . This gives the Internet of Things (IoT) a real jump start with the availability of the Android ecosystem and all the open source code and ready to use libraries that go with it. You'll want to start creating an IoT Android Things app as soon as possible but don't forget IoT devices need testing too. This blog explores ports and adapters and vendor tests to keep your codebase clean and your IoT devices solid . Testing still needs to be done in the internet of things. However testing integration with hardware is awkward, and how can you unit test something that isn't technically a unit? You would need the peripherals plugged in to be able to take readings so you can test the output. But why are you testing the peripherals works as designed? They're built by the manufacturer, you shouldn't have to do their work for them. Android Things provides Peripheral I/O APIs to communicate with sensors and actuators using industry standard protocols and interfaces. Whilst Peripheral I/O APIs allow for standardised communication. They still allow all general behaviours that the manufacturer allows. What you want is a peripheral that does the specific things you expect. For example some manufacturers LED strip could show every colour in the rainbow, it can strobe, flash, fade and dim - but what you need is a consistent red or green. Let's try and test only what we need and what we can control. When first using a new peripheral you should create the contract that you want the peripheral to adhere to. Using this contract (an interface in Java) you can create a mock of the peripheral that emits fake data, so that you don't have to have the physical peripheral plugged in. This contract can allow you to have unit tests for any classes that interact with the hardware peripherals. If you have tests for this contract, you can plug in any alternative peripheral and test that it adheres to how you expect it to work. These are known as vendor tests, when changing from a Panasonic LED to a Huawei LED strip, your tests will stay the same and your confidence that it \"just works\" will be sky high. There are two types of testing going on here. Unit testing that involves ignoring the hardware peripherals when testing your own code and Integration testing that involves determining the hardware works as expected in your system and can be changed out easily. Both of these different testing abstractions involve the use of a contract and the pattern known as ports and adapters . Create your application to work without either a UI or a database so you can run automated regression-tests against the application, work when the database becomes unavailable, and link applications together without any user involvement. - Ports and Adapters / Hexagonal Architecture Ports & Adapters can allow your application to create a plug and play architecture. For each Port you declare the contract ( in java an Interface ) which you would like some other part of the system to adhere to. In the quoted example above this is a Port for reading a database. You then fulfil the contract by creating an Adapter ( in java a class implementing the Interface ) and in the example this would be a SQLite database or a PostgreSQL implementation. Ports & Adapters allow your application to be driven by users, other apps or automated tests; meaning you can develop and test in isolation away from the applications eventual run-time configuration. Creating a Port for our LED peripheral we now have a contract we control. This contract can say: \"we expect to be able to turn this LED off instantly\" \"we expect to be able to turn this LED on in Red\" \"we expect to be able to turn this LED on in Green\" \"we expect no more\". At a glance it might seem that we are restricting the omnipotence & features of our LED and that doing so is bad but restriction is a good thing, let me explain. We've declared a contract that says \"The LED in our system needs to turn off and turn on in red and green\". Restricting the behaviour like this is what gives us a contract and is what allows us to control the LED as our own in our own system and be able to swap it out for any other manufacturers LED and still get the same functionality. Now that we have a contract, we could implement it using a hardware manufacturers Peripherals SDK for their LED. Moreover we can also implement the contract with our own \"mock\" implementation. For example: Port / Contract: interface CoolLed {\n\t\n\t// ...\n\n\n\tvoid turnOnRed();\n\n\n\tvoid turnOnGreen();\n\t\n} Adapter / Panasonic implementation: class PanasonicCoolLed implements CoolLed {\n\t\n \t// ...\n\n\n\t@Override\n\tpublic void turnOnRed() {\n\t\tledInputs.enableLight();\n\t\tledInputs.setColor(0xFF0000);\n\t}\n\n\n\t@Override\n\tpublic void turnOnGreen() {\n\t\tledInputs.enableLight();\n\t\tledInputs.setColor(0x00FF00);\n\t}\n} Adapter / Mock implementation: class LoggingCoolLed implements CoolLed {\n\t\n\t// ...\n\n\n\t@Override\n\tpublic void turnOnRed() {\n\t\tLog.i(\"led\", \"ON - RED\");\n\t}\n\n\n\t@Override\n\tpublic void turnOnGreen() {\n\t\tLog.i(\"led\", \"ON - GREEN\");\n\t}\n} Having a mock implementation means we can carry on working on our system and use our Logging LED anywhere we want to integrate the real LED. The benefits of this are, you don't have to worry about having the peripherals to hand, plugged in or even working. When coding, instead of watching an LED turn on and off you can watch the console log our messages saying on and off. I can't highlight enough how powerful this design pattern is. It allows you to concentrate on getting your system up and running without worrying about peripheral pieces. The separation of concerns means you can even have different people / teams working on the separate parts of the application as long as you agree on the contract. You can give demos even if the hardware cables were damaged in transit and you can fully unit test your system. So far having this contract in place has allowed us to unit test our system because we can replace the hardware at testing time with our mock implementation, but what about the other side of the contract: should we test the implementation and how? This blog post explains the theory of unit testing with Ports and Adapters. If you want to see a hands on example see my other blog post here . Vendor Tests Everyone says you shouldn't test other peoples code (as in external dependencies ) but why not? It should have been tested by them so you are duplicating work? It is not under your control so how can you guarantee test consistency? You only use a subset of their library so how do you know when to stop testing? All of these are valid concerns for normal use of a library, however when integrating with a library you want to make sure the integration works, don't you? Vendor Tests help you guarantee the sanity of the code you are integrating with. They are an integration test, so don't run as fast or as often as unit tests. You could write them once when you first meet a library then throw them away once you are more confident with how the library works. Or you can keep them around in your test suite and use them to guarantee your use of a third party library when you update it to the latest version. The contract/ port in our architecture allows for some powerful Vendor tests. We have the contract stating how our red or green LED peripheral should work, we now want to test that the Panasonic LED library can give us this functionality. Testing using the PanasonicCoolLed Adapter means we can show that the library is functioning as we expect and adhering to our contract. Note that we only test this library through the contract methods of the port, this approach ensures we are testing only the functionality we want and nothing more. If our vendor tests assert the outcome of the methods on our contract (remember these are integration tests not unit tests) and we wanted to swap hardware from Panasonic to Huawei we would still be using our same contract/ port . Therefore even if we change the Adapter, the same tests should still be able to run and still give us the same confidence that the new 3rd party library is working. [1] Thinking of these tests in terms of testing the output of sensors might make more sense. For instance a vendor test suite around a temperature gauge to make sure you always receive the same degree of accuracy. Conclusion Android Things brings the Internet of Things to the Android Framework. Applying Java testing design principles to IoT takes just a little bit of new thinking. However once you do start architecting and testing for the Internet of Things you can see how powerful a good separation of concerns can be. Ports & Adapters lends itself to this separation and allows your system to become modularised around the core business logic that you want your device to apply and the separate peripherals that you want to use. This separation allows for testing at every layer, giving you more confidence in your own system and even better; a solid codebase that you can change the peripherals of whenever it is required or requested. For a hands on walk through example of testing this way see my other blog post here . These being hardware peripherals the Integration test assertions for knowing if hardware is working might have to be a bit more clever. For instance, how do you assert that an LED has turned RED? You might start to think about creating custom test rigs with assertion sensors or leaving this for manual testing. ↩︎", "date": "2016-12-21"},
{"website": "Novoda", "title": "Working with the Layout Preview", "author": ["Alex Styl"], "link": "https://blog.novoda.com/layout-preview-101/", "abstract": "Android Studio comes with a powerful tool that helps you understand how your layout files are going to be rendered on the user's device. As powerful as this tool can be, it can lead to some misleading errors driving the developer crazy. This post talks about how to get the hang of the Preview tool and design layouts that can be grasped at the first glance. The Layout Preview can be accessed by pressing the Preview tab on the right XML layouts are probably the most frequently used resource in Android development. Chances are you have at least one layout file for every Activity you have in your project. Android Studio's Preview tool helps you implement those great designs and iterate through them quickly without even the need to run your app. The Layout Preview displays a representation of how your XML code will be displayed on the device. It also allows you to see the different configurations of your layout, such as how would it look like while in portrait or landscape, or how does that TextView look on multiple locales such as English, German or Greek. When everything works, that is... Even though the preview tool is powerful and can make your development days a breeze, it can also make your life miserable and frustrating. Unless you know its limitations and how to overcome them, that is! Here is a list with the most frequently Preview issues I have faced and their solutions for easier development: Issue #1: The Preview looks empty Assume you have a layout whose content is to be filled in with data obtained from the backend... You quickly realise that, since the content is dynamic, the Preview tool cannot populate the screen and you see nothing. The naive solution to this issue is to test the layout on-device, where you actually have that data. The problem in this case is that the TextView and ImageView do not have any content to display. This is a common issue when dealing with dynamic content. Even when the code compiles without issues, no one can make sense of the layout without looking at the XML code. When creating a layout that uses any content related view, a good practice would be to populate it only while in preview. By using the tools namespace instead of android , while declaring xml attributes, allows you to specify attributes that are going to be used only while in preview. In this case we use tools:text=\"Title\" and tools:src=\"@drawable/cool_pic\" and voilà! Attributes declared with the tools prefix work exactly as the android ones but only for preview. This means that your Preview renders the layout with some content which does not get shipped with your app. What happens though if you don't have images that match the aspect ratio of all ImageView s? You could include some debug resources, which might require some additional effort to make and maintain. Or, you could read Tip #2: Tip #2: Making dynamic content visible on Preview When your layout is meant to display some content that comes from an external source, it sometimes helps to have some maximum width or/and height for the parent View. This will ensure that your layout looks good even when the external source sends images that are larger than expected or in some aspect ratio that was not agreed. You can specify the size of your views while in Preview only with tools:layout_height and tools:layout_width . You can use that in combination with tools:background to see how much space those view could take while in preview. If you are interested in design time configurations, make sure to checkout Sebastiano’s Tools of the trade series. Tip #3: Fixing broken Previews When creating a custom View it is important to ensure that your View can be instantiated without using any external dependencies that might not be present while in Preview. Keep in mind that the Preview doesn’t run in your application, but rather on the JVM in the IDE. This will emulate how things work on an Android device, but there’s a lot of shortcuts taken and you should assume you cannot access any number of dependencies that aren’t inside of the View framework. Using an image loader such as Glide, for example, will not be possible. For the same reason, any Dependency Injection framework will not work as it won’t be initialised in the preview context, causing the View to throw an exception while being inflated. In this case View.isInEditMode() saves the day. Use it to check whether you are running on the Preview tool and skip any initialisation that requires dependencies that aren’t available at design time: public ImageWithCaptionView(Context context, AttributeSet attrs) {\n        super(context, attrs);\n\n\n        if (!isInEditMode()) {\n            ArticlesApplication.getInjector().inject(this);\n        }\n    } Protip: You can use the tools: namespace to show some default values while in Preview, or have some special handling for the Preview mode within your custom view. Tip #4: <merge> layout doesn't get rendered The <merge> tag is a great in helping you reduce duplication of layout code. If you are not using it, make sure to check it out and make your layouts more performant. The problem with merge, though, is that all the components inside it are going to be collapsed together while displayed in the Preview, creating a visual chaos. The caption is drawn on top of the image You can use tools:showIn=\"layout\" to display the contents of the <merge> layout inside some other existing layout that uses it. Keep in mind that if you use the same merge layout in multiple places, you can only choose one layout to preview it into. As of Android Studio 2.2, you can now use the tools:parentTag in order to define the behavior of the <merge> tag for preview purposes. Using tools:parentTag=\"LinearLayout\" for example is going to render the layout as a LinearLayout . The caption is drawn below the image as it should 💃 Tip #5: Show hidden Views while in Preview Your activity might contain some logic that hides some views on creation, but get displayed them after some event. By setting the visibility of those views to gone in your layout, you are ensuring that they are never going to be visible on inflation. The problem is that these views will disappear from the preview too, and if some other developer opens the layout and looks for them in the preview, they won’t find it. This is a problem because it requires more effort and time to understand what is going on in the screen. You can yet again use the power of design-time overrides and put a tools:visibility=\"visible\" attribute on the view to show it in the preview panel... Use this in moderation though. If your layout preview ends up being too different from how the layout will actually look on devices, it can be very confusing. If, for example, you have a bunch of invisible views of which only one at a time can be visible, showing them all in the preview might be chaotic. This might also be an indication you might want to use other lazy-loading mechanisms too, such as inflating only the views you need at runtime... Those were the most frequent issues I come across when working with the Layout Preview tool in Android Studio. I hope these tips will make your development days easier. Do you use any other nifty tricks to improve the visualization of your layouts? I'd be more than happy to know! Many thanks to Sebastiano Poggi , Daniele Conti and Daniele Bonaldo for all their help in writing this article.", "date": "2016-12-15"},
{"website": "Novoda", "title": "6 tips to improve your app store listing and get more downloads", "author": ["Denis Akan"], "link": "https://blog.novoda.com/6-tips-for-a-high-performing-app-store-listing-google-play-and-appstore/", "abstract": "With over 4.5 million apps across Apple’s App Store and the Google Play Store, app store optimisation (ASO) is increasingly important to help your apps get discovered. In such a competitive environment it’s essential that you present your app in the most effective way to optimise your download and view rate. This post gives some best practice tips for a great app store listing. 1. Give your app a descriptive title The app title is probably the first thing your customers will see next to your app icon, so it should be as relevant, readable and descriptive as possible. Try adding keywords to your title if you have space, but beware of over-stuffing and make sure it’s easily readable. Space available varies between app stores - you get 30 characters on Google Play and 50 characters on the App Store. Left: Good use of Title and Keywords Right: Avoid Keyword stuffing - you might get penalised by Apple 2. Add impactful images Appealing images are arguably the most important element of your app store listing. They give you the opportunity to showcase the design and value of your app and  are seen by all visitors to your page. Chances are that a potential customer who sees low quality or poorly-designed app store images might not be inclined to read on or download your app. Don’t just upload screenshots of your app, create mock-ups to give your customer a clearer view of how it looks on a device screen. Describe a key feature on each screenshot, or better still, tell a story with your images as a visual way of conveying your app’s USPs in a logical sequence. Localise your screenshots - translate your copy for each language you provide. Apple App Store only: Create mock-ups for each device type your app is compatible with (eg iPhone 4s, iPhone 7 Plus, iPad Air). Design your app preview images, don't simply upload a screenshot. 3. Create a compelling short description (Google Play only) The short description is limited to 80 characters to describe the essence of your app. It’s important as it’s the only text shown to users unless they click on ‘Read more’ to view the long description. Talk about user benefits rather than your app’s features Explain clearly what your app does, what problems it solves and how it fulfills users’ needs 4. Front-load your app description with the most important message Your potential user will see the full description when they click on ‘Read more’. Both app stores give you 4,000 characters to describe your app’s key features and benefits in detail here. Use relevant keywords that describe the key functions of your app and clearly explain the value it provides. Front-load your description with the most important messages to ensure they are conveyed to as many users as possible. Your users may only scan this section so make sure the key elements you want to communicate stand out clearly. 5. Use all 100 characters for keywords (App Store only) Apple's App Store lets you add a total of 100 characters for keywords that best describe your app. Don’t waste characters on any unnecessary spaces and avoid duplicating the same words. Put yourself in the mind of your customer and think about what they might search for to discover your app. Good free tools for keyword research are Sensor Tower or Google’s Keyword Planner . 6. Create an app video A video is a great way to show your app in action and bring its value to life for your customers. When creating a video, make sure you include the key messages in the first 5 seconds as it’s key to grab your customer’s attention from the start. Although Google Play allows videos of up to two minutes we recommend you limit the length to 30 seconds for maximum impact, which is also the limit on the App Store. Don’t just explain how to use the app, focus on the value it creates for your users. Creating a high quality video is a serious undertaking and requires a significant investment of time and money. It’s worth making sure all other parts of your store listing are as effective as possible before you focus your resources on this. Hopefully that has given you some fresh ideas on how to improve the conversion rate of your app store listing.", "date": "2016-12-19"},
{"website": "Novoda", "title": "5 tips for productive design meetings", "author": ["Qi"], "link": "https://blog.novoda.com/5-tips-for-productive-design-meetings/", "abstract": "As a designer here at Novoda I’m often out meeting clients and presenting design concepts and iterations. I’ve picked up a few tips along the way that might help my fellow designers to have more productive client meetings: 1. Be prepared Set an agenda in the meeting invite to make it clear exactly what will be covered. Open all the necessary documents and web pages before the meeting so you don’t have to take time to find things in the meeting, which can look a little unprofessional. 2. Speak the client’s language In any industry there is a lot of jargon and not everyone will use the same terminology as you do. Clients may not appreciate you correcting them, so it helps if you adopt their way of describing things or find easy ways to explain what you mean without making anyone feel stupid. For example, not all clients are familiar with the Overflow button in Android so I sometimes refer to it as the ‘3 dots’ button to make it as clear as possible what I mean. 3. Keep the bigger picture in mind When you’re presenting a design to client, don’t get drowned in the details like shadow depth or corner rounding. Put the design in context of the bigger picture eg how your design serves the business goals or meets the user requirements effectively. 4. Don’t get defensive It can be hard if a client is critical of your design but it’s always best to avoid getting defensive. Clients may have valid questions about how it suits their branding or meets their needs and they may know their audience better than you so their points could be valid. Ask open questions to get as much information from them as you can about what they feel isn’t working. Revisit the brief to see if this needs to be adjusted and get any further relevant information you can from the client, like brand guidelines or examples of other materials. If you feel you have a strong argument then don’t be afraid to discuss this further with the client and try to help them see why you’ve approached the design in that way. 5. Be smart when challenging clients When you have a strong opinion about why something you’ve been asked to do won’t work, you need to explain your thought process clearly to the client to get them on board. Avoid making negative statements like “This won’t work!”. Instead try walking through step by step explaining what might happen if their design suggestion is implemented, and pointing out the potential risks along the way before offering them another solution. It will be even more convincing if you can back this up with evidence or examples to support your argument.", "date": "2016-12-13"},
{"website": "Novoda", "title": "Making your Android Apps More Inclusive with accessibilitools", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/accessibilitools/", "abstract": "accessibilitools 1.4.0 was released a few weeks ago. In this post, I'll share what this small set of classes consists of and how you can use it to create more inclusive Android apps. accessibilitools is a tiny collection of utilities to help the production of accessible Android apps. Since a lot of the work we've been doing at Novoda over the past year has been focused on support visually impaired users, so too is the library focused (for now!). I'm excited, let's get started. Determine state of accessibility services One of the things I try to explain to folks is that it's not necessary to provide the same experience for every user of your product; user journeys can be different but still achieve the same goal. To this end, it's useful to be able to detect whether a user has an accessibility service enabled, like TalkBack or the captioning preference. AccessibilityServices a11yServices = AccessibilityServices.newInstance(context);\nif (a11yServices.isSpokenFeedbackEnabled()) {  \n    // show UI\n} else {\n    // show different UI\n} You can use it to display (or hide) affordances from one set of users. In the example, we show a \"dismiss controls\" button in addition to the regular player controls. Setting usage hints TalkBack is an accessibility service that helps render the UI in spoken word. One of the things it does is to help the user understand when a UI element is actionable. For example, when reading aloud the description of a button, it will append \"Double tap to activate\", a usage hint that tells the user what gesture is necessary and what action will be performed. For custom views or other view groups, it'll rely on the content description you set. In general, you'll use it to describe the element itself, not what action it will perform. You can customise the usage hint (separate from the content description) so it fits your app. The way to do it is to override the label for the click event in the View's accessibility node. @Override\npublic void onInitializeAccessibilityNodeInfo(View host, AccessibilityNodeInfoCompat info) {\n    super.onInitializeAccessibilityNodeInfo(host, info);\n\n    info.addAction(new AccessibilityActionCompat(ACTION_CLICK, \"start playback\"));\n} Now it'll say \"Double tap to start playback\"! With accessibilitools, you can do it in a few lines using a class called UsageHintsAccessibilityDelegate . This helps you override the default labels for click and long click events: UsageHintsAccessibilityDelegate delegate = new UsageHintsAccessibilityDelegate(resources);\ndelegate.setClickLabel(\"start playback\");\n\nViewCompat.setAccesibilityDelegate(buttonView, delegate); Actions dialog A lot of the time, we include inline actions on our list items. This makes it more difficult for TalkBack or keyboard users to navigate between items since the extra icons are focusable. Here we move from a visually and cognitively busy element to one which is truly bare bones; for a TalkBack user, it could be simplified to just a card with some text describing the video. In this case, there's a single active area and we're able to pop a dialog containing all the actions for that particular item. private final ActionsAlertDialogCreator dialogCreator;\n\n@Override\npublic void bindViewHolder(ViewHolder vh, int position) {\n    Actions actions = createActionsForItemAt(position);\n    final Dialog actionsDialog = dialogCreator.create(actions);\n    \n    vh.itemView.setOnClickListener(new OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            actionsDialog.show();\n        }\n    });\n    \n    // bind rest of view\n}\n\nprivate Actions createActionsForItemAt(int position) {\n    final Video video = videos.get(position);\n    return new Actions(Arrays.asList(\n        new Action(R.id.play, R.string.play, new Runnable() {\n            @Override\n            public void run() {\n                listener.onClickPlay(video);\n            }\n        }),\n        new Action(R.id.mark_watched, R.string.mark_watched, new Runnable() {\n            @Override\n            public void run() {\n                listener.onClickMarkWatched(video);\n            }\n        }),\n        new Action(R.id.download, R.string.download, new Runnable() {\n            @Override\n            public void run() {\n                listener.onClickDownload(video);\n            }\n        }),\n        new Action(R.id.favorite, R.string.favorite, new Runnable() {\n            @Override\n            public void run() {\n                listener.onClickFavorite(video);\n            }\n        })\n    ));\n} Custom accessibility actions Did you know that TalkBack allows you to specify the actions on a View, and these are read aloud along with usage hints? It also makes these available from the custom actions menu, available to TalkBack users via a local context menu. Here's how we would have to do it previously, again, overriding the onInitializeAccessibilityNodeInfo(View host, AccessibilityNodeInfoCompat info) method in View: @Override\npublic void onInitializeAccessibilityNodeInfo(View host, AccessibilityNodeInfoCompat info) {\n    super.onInitializeAccessibilityNodeInfo(host, info);\n    info.addAction(new AccessibilityActionCompat(R.id.play, R.string.play));\n    // ... same for all other actions\n}\n\n@Override\npublic boolean performAccessibilityAction(View host, int actionId, Bundle args) {\n    // map up the actionId, and fire a callback for the correct position\n} accessibilitools includes the ActionsAccessibilityDelegate class so you can do it in two lines: @Override\npublic void bindViewHolder(ViewHolder vh, int position) {\n    ...\n    \n    ActionsAccessibilityDelegate delegate = new ActionsAccessibilityDelegate(getResources(), actions);\n    ViewCompat.setAccessibilityDelegate(this, delegate);\n\n    // bind rest of view\n} ActionsAccessibilityDelegate also lets you set the usage hints for click and long click events, so you can make the behaviour clear to users. Give it a go! Please try it out and let us know if it's been helpful in making your Android app more accessible. If you encounter any bugs or have any questions, you can open an issue on the GitHub page , otherwise you can reach out to me directly on Twitter .", "date": "2016-12-05"},
{"website": "Novoda", "title": "The 5 steps of better product design: 3. Prototype & test", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-5-steps-of-better-product-design-step-3-prototype-test/", "abstract": "This week, we’re looking at how we can use prototyping and usability testing to bring our ideas to life and assess their validity with users. Prototyping allows us to quickly build on ideas generated in the ideation stage and observe how customers interact with them. Using a range of tools and techniques, we can test for usability issues early and avoid spending time developing concepts that don’t meet the expectations of our users. This is step 3 in a 5-piece product design series. Understand Ideate Prototype & test Visual & motion design Measure impact Step 3: Prototype & test At the end of the ideation stage , you should have an idea or two that you’ve decided to develop further. To ensure you stay focussed, it’s important to define the goals of your usability test: What are you testing? What questions do you want answered? How do you define success and failure? What are the distinct differences between different prototypes? All of these questions will help you to define the goals of your test and work out the best approach. Defining these goals will also help you to choose an appropriate tool to create your prototype. For high-level user journeys, you may find a paper prototype is sufficient, but for a more complex interaction, a higher-fidelity prototyping tool may be more suitable. Prototyping tools Paper & pens Paper prototyping is a quick, creative, cost-effective technique, perfect for assessing high-level user journeys. For this, you’ll need a pen, paper (or device templates) and an assistant to manipulate the prototype as the user interacts with it. POP The (free) POP app allows you to link the screens of your paper prototype with hotspots and test them on a mobile device. Simply add photos of each screen and link the screens together to create a more native, clickable experience. Invision The Invision tool, along with many other great features, allows you to create high-fidelity interactive prototypes that can be tested from the web app, mobile device or via a remote testing service powered by Lookback . You can even define gestures and transitions for a more realistic experience. Dropbox Sync also allows you to sync your design files directly to a prototype for quick, easy updates from your Mac. Principle Principle provides the ability to define more complex gestures, interactions and transitions, allowing you to create a near-native experience with minimum effort. There are a number of online tutorials that can help get you started. Usability testing Observing the way your customers interact with your product provides you with invaluable insight into user behaviour. It allows you to prove or disprove hypothesis, validate your ideas and unearth usability issues early on. Ways to test Depending on the tool used to create your prototype, there are a variety of approaches you can take to conduct usability testing. In-house sessions allow face-to-face contact with users, provide invaluable insight and encourage empathy within the team. There are also some great remote usability testing services such as WhatUsersDo and UserTesting.com that allow your screened participants to follow a test script while recording their experience for you to watch back later. Recruitment Recruiting the right users is key to achieving realistic results from your tests. The ideal participants are customers who are currently engaged with your product on the platform you are testing on. If you are unable to do this, recruiting users of a similar demographic would be the next-best thing. You can advertise for participants on local classifieds or use a research agency to arrange your sessions. If you use a remote usability testing service you will also be able to screen participants based on demographic, location and device/operating system usage. Test script Preparing a test script not only forms the basis for your session but also encourages focus on the goals. It’s important to state goals, hypothesis, session structure and approach methods as well as specific questions and guidelines for the moderator who is leading the session. It’s always worth running a pilot test so any script or technology issues will be highlighted prior to your participant's arrival. For remote testing sessions, don’t forget to break out the participant questions into a separate document. Moderating & recording During the testing session, you will need two people: a moderator and a recorder. The moderator is responsible for keeping the session on-track and guiding the participant through the tasks and the recorder is responsible for recording notes, quotes and any usability observations. If you have observers outside the testing room they can write key points onto sticky notes and add them to boards assigned to each test goal, ready for the conclusion session. It’s worth getting as many of the team along to observe as possible as seeing a user interact with your prototype firsthand can be a great learning experience for everyone involved. Concluding results and defining next steps By the end of the session, you will have gathered a number of useful insights on the usability and perceptions of your product. It’s important at this point to organise insights into each of the test goals and separate any additional insights for consideration. You'll notice some recurring issues, which can be grouped and summarised. Grading usability issues with a severity rating will help you to prioritise changes. You can also document additional insights to prioritise for future development. Once your testing has finished it's important to document your findings for future use and share them with the rest of the team. A unified understanding of the insights will help to generate empathy for the customer and create a team focus. Thanks to Caroline Smith , Lena Sarp , Rimas Albert and Robbin Staack for making prototyping and testing so fun and insightful.", "date": "2016-12-09"},
{"website": "Novoda", "title": "Why Native Apps Really aren't doomed: let's talk facts", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/native-apps-are-not-dead/", "abstract": "Why Native Apps Really aren't doomed: let's talk facts I've recently read a blog post by Eric Elliott . The title of that post was Why Native Apps Really are Doomed . As you can imagine from its clickbaity title, the author postulates that mobile native apps are essentially doomed to disappear in favour of the new kid on the block, PWAs. I beg to differ. A PWA, or Progressive Web Application , is a website that uses the more or less recent Web technologies that allow it to approximate the behaviour of a native application, but inside the browser. Examples of such technologies are Service Workers , that allow background processing and \"always on\" apps; Web Push , that allow web apps to react to push notifications; local storage; and so on. It is an interesting thesis, for several reasons. But before diving into the merit of that claim, I'd like to give some context to the post. Its author is a Javascript advocate, with what seems a mostly web-oriented professional profile. The article is published in a collection for Javascript developers, called JavaScript Scene . I am not sure if and how this context actually influenced the author's perspective, but I think it's important to understand where ideas originate, especially when they are claiming \"X is dead\", or \" Y considered harmful \". Since I like to be upfront, full disclosure here: I'm a mobile \"native\" developer (in Eric Elliotts' terms, even though the apps I develop run in a VM) by trade and a Google Developer Expert on Android. I work in a mobile agency, Novoda , but what I write here is in no way expression of the company’s stance, or written on behalf of the company. It’s my personal opinion on the matter. I'll try to be as unbiased as I can in my analysis here, but please do forgive me if something slips through the cracks. I’ll be happy to amend things that might prove inexact or subjective. What is the thesis? This post is actually a follow-up to another post , Native Apps Are Doomed by the same author, after he found many people contested his original stance. It's particularly interesting that this is a follow-up to his first post; its goal is to \"bring the facts\" on why native apps are effectively on the verge of extinction. So, why are native apps about to disappear , according to Eric? He makes a few points: PWAs are write-once, run-everywhere By extension, PWAs are dirt cheap and native apps are prohibitively expensive to build Native apps have massive friction, because: People don't install apps When they do install apps, they only install the well established brands' apps (Facebook, ...) You can only monetise with IAPs, and you have to give a cut to Apple or Google Yes, iOS is missing some crucial tech for PWAs such as Service Workers, but there are alternatives Discovery on the web is massively better than in app stores The post is a great read, but I disagree with some of those points. Write-once, run-everywhere The reason why I don't agree that this is a \"real thing\" could be summarised by this: Sure, you can technically write a PWA and it should theoretically \"just work\" across any OS and browser, just like that. The same goes for Java, as Oracle is so keen to remind everyone that installs the JDK. Or, for that matter, Xamarin/Mono's .Net. Or Cordova, Titanium, PhoneGap, and any other HTML5/JS/CSS-based framework out there. Or, heck, the Web itself. That is the theory. Things in the real world pan out a bit differently . Have you ever heard of any of those frameworks being really at a \"it just works\" level, universally across compatible environments? If your answer is yes, I would love to hear about your experience. It would be a first for me. Let me elaborate a bit further. Cross-platform frameworks do work in some specific circumstances. In some cases, they're the best option you have, so you just go and run with it. That's perfectly fine. Say you are Amazon and you want a mobile app for your immense catalog of items; your best bet is to use a thin wrapper around your mobile website that adds \"just enough\" behaviour on top of it. The complexity and variety of the items in their catalogue would make for an incredibly complex native application . The closest example I can think of that has a similar level of complexity and is fully native is Facebook's app, which isn't the most shiny example of mobile app when it comes to performances, memory and power usage (and design, arguably). Their website is a vastly superior alternative, because it is a very well done web app . But unless you have a great engineering team, and you invest enough to bring your web app over the \"uncanny valley\" of mediocre websites the way Facebook and Google do, your web app risks being worse than an equivalently mediocre native app in terms of performances and user experience. Running in a browser will have an impact on performances, if you don’t optimise, whereas native apps can get 60fps almost for free (almost). Since you can’t have a PWA without a web app, this extends to PWAs. And that's without considering the profound differences in design and navigation patterns users are used to on Android and iOS. If you build a PWA without keeping that in mind, it will never blend in with a device. Best case scenario, you can make one platform's users comfortable. Worst case scenario, and most common case, you'll end up with a UX that doesn’t fit on any platform and alienates users. Writing a well-done PWA is neither cheap, nor fast, nor easy . It takes the same amount of effort and care you need to build a good native experience. Sure, you might need specialised devs for each supported platform, but unless you’re doing throwaway apps, your product survival depends on it being better than the competitors, so you will need a high degree of expertise. You can’t put a web dev or designer that has never done mobile before to doing mobile and expect great results from day one. You’ll still need time, effort, specialisation, skills, and thus money. A lot of it. Browser incompatibilities and quirks won't go away any time soon. The tech powering PWAs has limitations (e.g., local storage space caps, implementation-specific JS bugs). And so on and so forth. It's a whole bag of Your Mileage May Vary , these days. When you start saying \"oh, X is not available on Y, but hey for that case you can do Z\" you're running into the same fragmentation and complexity you get with supporting native apps, but without the great tools native apps provide you for dealing with that fragmentation. It's a jungle out there, and a naive \"workaround all the things\" approach ain't gonna cut it. Oh, and I won't even get into making complex web apps as performant as native code. The mere fact there's more software layers a PWAs is sitting on top of, compared to a native app, means it won't be able to achieve the same performances. When comparing apples to apples, you can make web apps performant and incredibly nice experiences, but it takes a lot of effort and thus money, as mentioned. Then you’re back at square one. For the sake of being exhaustive, I have to mention that PWAs don't have access to equivalents for a lot of a platform's native APIs . Sensors, fingerprint readers, power management, inter-app integrations (e.g., sharing to or from other apps), close-to-metal and performance critical code, are the first that come to mind. If you don't need them, great. If you do... a native app is your best shot. That is unlikely to change, at least in terms of getting a common and standardised way, in my opinion. Friction on native apps, discovery woes Native apps do suffer from friction issues [1] , that much is true. But there are very interesting initiatives going on, at least on the Android side. Instant Apps [2] , while yet to be released, will be a great way for some apps to avoid the problem altogether, since users won't even need to install the apps to be able to use them. Before that, Google tested a more rudimentary \"try before you install\" technology, App Streaming , that allowed users to stream a remote device's screen with the app running in it. I'm not sure where that technology has gone nowadays, but I imagine it's being replaced by Instant Apps anyway. On the other hand, mobile apps still manage to emerge from the mass, sometimes even without the need of crazy marketing campaigns. As a first-hand experience, I had an open source app on the Play Store that, with absolutely zero advertising effort, got to 150k+ users in a year. It was a homescreen widget, that most people don't even realise are a thing that exists on Android. I can’t really think of an example for PWAs here; this might be a symptom that discovery isn’t that easy on the web either. The author also states that discovery is very difficult on mobile, but he seems to assert on the web it's different and you're almost guaranteed people will use your website and services. In my experience, the bias towards well known and established properties is not limited to discovery on the app stores. People will barely ever go over to the second page of Google results, when looking for something [3] . The first search result gets ~33% of the whole traffic, in real life [4] . Meanwhile, native apps can take advantage of technologies such as App Invites and Dynamic Links , that allow organic recommendations and referrals from friends. Considering that half of app installs already come from a direct, personalised referral, this has a huge impact on discovery. Monetisation It's true that the majority of top grossing mobile apps rely on In App Purchases , especially on Android. But those are mostly games, in which the freemium model is pretty much the only way to go. iOS users are more affluent on average, and more inclined on paying for an app or game; Android users tend to prefer freemium. Say you don't want ads in your app—that would give you roughly the same monetisation opportunities on native and PWAs, assuming you don't want to go rogue and have annoying, intrusive ads. Let's also say you want to go with the freemium model. You will have to give Google or Apple a cut of your IAPs, sure. But you would also get a whole lot less friction on the actual monetisation flow. The payment flows for native apps' IAPs are extremely simplified to reduce friction to the minimum. In most cases, users just need to touch their devices' fingerprint scanners. I’d like to see some real data there; I haven’t found any comparison, unfortunately. You can get similar results on the web using things like Apple Pay and Android Pay to reduce friction, but then... you're back to giving money to Google and Apple. And this is without even considering that those mechanisms take care of the vast majority of the payments business, keep track of who's bought what, when, keep the users' payment details, and so on. You can get some of those things with other processors such as PayPal or Stripe, but you won't get as good of a payment flow with either. If your product is primarily targeting Western markets, your users will predominantly be registered and set up with those built-in processors too. In emerging markets, you face the same challenges as people is mostly used to third party processors that tend to be tied to the local market. It's not true you always have to use IAPs , even though there are reasons why you might want to. If your app or service sells goods or services that are available through other channels (say, a website) depending on the case the app stores might allow you to use your own payments processing, completely bypassing the fees. Lastly, there's some interesting new ideas for monetising apps coming up, again on Android. For example, Amazon has launched Underground , a programme in which developers can publish their apps without ads and IAPs, and fully unlocked. Users will be able to download them for free. Amazon will pay the developers based on the amount of time users spend on their apps, or games. I'm not aware of any similar possibility on iOS and PWAs. Conclusions PWAs are a great thing to be excited for ; it means one can have a good experience on a web app, instead of the crappy one you would've expected a couple of years ago. New, great tech is surfacing constantly in the web landscape (even a bit too quickly , according to some). The breakneck pace means it will maybe one day catch up with the experience users can get on native. But as of today, PWAs are not there yet . You can probably still get better results with some (semi-)native cross-platform frameworks. Web-based frameworks like Cordova in my opinion are nowadays mostly used to create cheap, “quick&dirty” apps by companies that don't care enough about their users to get them a proper experience. Their usage is thankfully in rapid decline [5] . React-Native is nice but has its own bugs and limitations ; some are turned off even by its licensing terms. Xamarin is the same (but it's nice it's OSS now). I see some potential in Flutter because it's a mobile cross-platform framework that really compiles to native code, but it's still very early days for it. Most JS/web developers I have spoken to have very little clue what the challenges and gotchas of developing for a mobile platform are. On the other hand, most of those who do know mobile platforms wouldn't touch JS with a stick, either because of the horrible fame it built for itself over the years, or simply because it's not as good as the languages you can use with 1st party mobile SDKs, in this context. JS might work in some contexts but that doesn’t mean everybody loves it. After all JS is getting better, but it’s still that JS to many. For as much as a nice read the article was, it felt like some sort of naïvety emerged from the author in expressing his point of view. In my opinion, the author’s statements regarding the state of PWAs are simply too optimistic. While the technology is progressing quickly, my personal experience with native development indicates that there are a lot of problems that it’s not yet close to addressing. For as much as one might really think and hope one day we'll manage to get over \"native\" apps, I can't get myself to think we're even close to it right now . Sure, the article says native apps are doomed \"eventually\", but realistically, if it's in five to ten years...who knows what better alternative technologies might arise in that time [6] ... I can't help but think that each tool has its own place in a developer's toolbelt . PWAs make sense in some cases; \"native\" (as in developed using the main language and APIs for a platform, such as Java for Android and ObjC/Swift for iOS) make sense in others; and \"real-native\" apps that use closer-to-metal languages such as C/C++ are sometimes the only way to go. In some cases you might get the best results with a combination of all those technologies, or maybe just some. If you’re developing an app with a short shelf life, such as a marketing app or an event app, then a PWA might actually work just fine. If you’re developing an app that is supposed to represent a long-lived product or brand [7] (especially mobile-focused ones), or you hit any of the limitations PWAs have today, then going native is and likely will be the only way, at least in the medium term. In conclusion: no, native apps ain't going anywhere right now . Maybe they will one day. Maybe we won't have walled gardens and proprietary APIs, maybe we won't even need to know a programming language to instruct a computing device. Who knows. In my opinion though, with more than 65k new native apps being uploaded to the Play Store every month, and that rate increasing, native apps aren’t nearly as dead as the article would have you believe. A huge, massive thanks to everyone that helped me out with the editing and their suggestions: Francesco, Daniele, Ryan, David, Anup, and Erik. A note on the app stores: I find the Apple Appstore is a dreadful piece of software , and I'm absolutely not surprised that 60% of the apps on it are never installed (the video mentioned in the article is not talking about Android and the Play Store, but about iOS). ↩︎ I strongly recommend you watch the intro video on the technology released after Google I/O 2016. ↩︎ https://www.quora.com/How-many-Google-searchers-go-to-page-two-of-their-search-results ↩︎ https://searchenginewatch.com/sew/study/2276184/no-1-position-in-google-gets-33-of-search-traffic-study ↩︎ According to AppBrain , on the Play Store only ~8% of apps use HTML5 cross-platform framework such as PhoneGap/Cordova ( 6.22% ), Appcelerator/Titanium ( 0.77% ), Marmalade ( 0.12% ) and Corona ( 0.92% ). In the top 500 apps, that shrinks to 2.89% apps using them. Even more interestingly, although 8% of the apps use them, only 2.15% of the actual installs ever includes any, meaning 73% of those apps are never installed—considerably more than native apps. ↩︎ I'm not even sure the world will not be a nuclear wasteland, by then, given the recent turn of events in some nuclear powers ↩︎ One case is the Financial Times, that left app stores in 2011 to migrate to a responsive web app to avoid paying a cut of their subscriptions to Google and Apple. They reportedly spent a lot of time and money building the website, and as of today, it's still only compatible with iOS devices . On Android , they have apparently given up and resorted to a native app instead. ↩︎", "date": "2016-12-01"},
{"website": "Novoda", "title": "The 5 steps of better product design: 2. Ideate", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-5-steps-of-better-product-design-step-2-ideate/", "abstract": "This is part two in our product design series exploring the phases of digital product design from initial understanding to final delivery. This week, we’re looking at the Ideation phase where cross-discipline teams work together to generate a wide range of ideas and solutions to form the basis of a prototype. Understand Ideate Prototype & test Visual & motion design Measure impact Step 2: Ideate Making the session informative, interactive and enjoyable is the key to keeping participants engaged. It's an opportunity for the group to explore their thoughts and ideas without limitation and enjoy the creative process of problem solving. Ideation workshops typically kick off with: Workshop aims Customer research insights Data insights Business goals and KPIs The most successful ideation workshops are those with well-informed, enthusiastic participants, free to express their ideas. Workshop energisers Starting a workshop with an energiser activity is a fun way to encourage participation and excitement for the session. Break the ice, especially if working with unfamiliar colleagues Introduce cross-discipline teamwork Stimulate mental activity Encourage freedom to express ideas Here are some of my favourites. 1. Paper creations Appropriate for any size group - a great ice breaker if you're low on time. Materials and requirements: 5 minutes A4 paper for each participant Running the activity: Introduce the activity including timings and expectations Ask the participants to take a piece of paper and hold it with two hands behind their backs Task them with creating an object (animals and landmarks often work well) within a 2 minute time box Ask the participants to show their creations to the rest of the group Finish with a group photo 2. Find your pair Works well with medium-sized groups (max 12). Be sure there are an even number of participants. Materials and requirements: 10 minutes An even number of participants One note for each participant with the name of an animal (eg lion, cow). Each animal must be written on two separate notes. Space for your participants to walk around the room Running the activity: Introduce the activity to the group including timings and expectations Hand one note to each participant, asking them to check their animal but keep it concealed Ask the participants to cover their eyes and make the noise of their animal, while walking the room searching for their animal pair Stop when each participant has successfully found their pair 3. Collaborative face drawing Great for smaller groups getting to know one another. This one has had participants rolling around the floor with laughter. Materials and requirements: 15 minutes A4 paper Pens Running the activity: Introduce the activity to the group including timings and expectations Give each participant a piece of A4 paper and a pen Ask participants to walk around the room until you say the word stop Each participant should pair up with someone near by Each participant should draw the other person's eyes (time box to 2 minutes) Repeat steps 3, 4 & 5 for all face parts (eyes, nose, ears, chin, hair, facial hair and accessories) Ask participants to pin their drawings on the wall and present to the group Ideation exercises Following an energiser and an effective introduction to the workshop, the group should be well-informed, enthusiastic and be ready to begin. During the session, no ideas should be off limits. The more diverse the ideas, the better. Ideation sessions tend to be more effective when attendees work individually in a creative, collective space. This encourages equal input and avoids influence. Any feedback should be saved for after the ideation, allowing participants to express their ideas with freedom. The session can be concluded with group discussion and a voting session to decide which ideas to take forward to the next stage. Mind maps and note-taking Mind maps are an effective way for participants to gather and record their thoughts in a logical format that can be easily discussed. (10 minutes) Idea sketching This is an opportunity for attendees to sketch as many thoughts and ideas as possible without restriction. Encouraging an open mind will help participants to generate new ideas and solutions. (10 minutes) Crazy eights Taken from Google Design Sprints , this method allows participants to define and condense their best ideas. Ask each member of the group to fold an A4 sheet of paper to create 8 frames. They can then spend one minute to add each idea into a frame. (8 minutes) Facilitation techniques Your job as a facilitator is to outline clear goals, define a session structure and ensure the session stays focussed, productive and enjoyable. Here are some tips to help make sure your workshops run smoothly and on time. Preparation Having a clear, detailed plan will help to keep the session focussed and on track. Select attendees carefully - ensure a range of specialisms and perspectives Be mindful of attendees' availability - plan accordingly Ensure your aims, research insights and business goals are clearly defined Ensure spaces are booked and ready Run through the session in your head considering alternative dynamics and consider how you might react in different scenarios Introduction A clear, concise introduction to the workshop will help attendees to feel comfortable, well-informed and confident. Provide sufficient background Be clear with goals and objectives Give clear instructions and be ready to offer support Keeping the session on-track & enjoyable Sticking to time boxes and making the session fun is the key to a successful workshop. Define appropriate time boundaries - time-boxing ensures efficiency and structure Assign a decider - someone to make the final call if the group is struggling to reach a decision Be visual - using colour, sketches and the physical space encourages attention and engagement Be reactive - be sensitive to the group dynamic and be prepared to adapt the session direction if necessary Be reflective - help the group to summarise and record discussions, reach a definitive outcome and define next steps We're confident that following these steps will help your workshops produce high quality output. Try them out and let us know how you get on. Big thanks to Caroline Smith , Lena Sarp , Rimas Albert , Robbin Staack and Paul Befort and the rest of the team at ImmobilienScout24 for the workshops we've enjoyed together over the past few months.", "date": "2016-11-24"},
{"website": "Novoda", "title": "Exploring Android Nougat 7.1 App Shortcuts", "author": ["Andrei Catinean (Software Craftsman)"], "link": "https://blog.novoda.com/exploring-android-nougat-7-1-app-shortcuts/", "abstract": "Google has brought Android Nougat to its second iteration with the 7.1 version (API 25). This one is not a minor release - as a matter of fact it bundles some interesting features under the hood. One of these extra features is App Shortcuts . This post explores what they are, how they work, and how you can implement them. The end result looks like this: If you want to go through a step by step guide, please read on. What are App Shortcuts and why would you need them? App Shortcuts are a means of exposing your application’s common actions or tasks on the user’s home screen. Your users can reveal the shortcuts by long-pressing the app's launcher icon. From a technical perspective, App Shortcuts are a simple and quick way of firing your application’s Intents[^n]. They are of two types: Static : These are shortcuts that are defined statically in a resource file; These cannot be changed unless you modify the file and re-deploy the app Dynamic : These are shortcuts that are published at runtime; These can be updated without the need to re-deploy the app By exposing your common tasks, you'll make it easier for your users to quickly get back into specific parts of your application  without the need of additional navigation. Adding App Shortcuts Adding Shortcuts to your app is pretty straightforward. Let's start with creating a simple static shortcut [^n]. Static shortcuts This example assumes you already have a project set up in Android Studio. Navigate to your AndroidManifest.xml and add the following meta-data tag to your main activity: <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  package=\"com.catinean.appshortcutsdemo\">\n\n\n  <application\n    android:allowBackup=\"true\"\n    android:icon=\"@mipmap/ic_launcher\"\n    android:label=\"@string/app_name\"\n    android:supportsRtl=\"true\"\n    android:theme=\"@style/AppTheme\">\n    <activity android:name=\".MainActivity\">\n      <intent-filter>\n        <action android:name=\"android.intent.action.MAIN\" />\n\n\n        <category android:name=\"android.intent.category.LAUNCHER\" />\n        <category android:name=\"android.intent.category.DEFAULT\" />\n      </intent-filter>\n\n\n      <meta-data\n        android:name=\"android.app.shortcuts\"\n        android:resource=\"@xml/shortcuts\" />\n    </activity>\n  </application>\n\n\n</manifest> In the meta-data tag, the android:resource key corresponds to a resource defined in your res/xml/shortcuts.xml . Here you need to define all of your static shortcuts. Let's add one that will open a certain activity from your app. In the example  below I've created a dummy StaticShortcutActivity : <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<shortcuts xmlns:android=\"http://schemas.android.com/apk/res/android\">\n  <shortcut\n    android:enabled=\"true\"\n    android:icon=\"@drawable/ic_static_shortcut\"\n    android:shortcutDisabledMessage=\"@string/static_shortcut_disabled_message\"\n    android:shortcutId=\"static\"\n    android:shortcutLongLabel=\"@string/static_shortcut_long_label\"\n    android:shortcutShortLabel=\"@string/static_shortcut_short_label\">\n    <intent\n      android:action=\"android.intent.action.VIEW\"\n      android:targetClass=\"com.catinean.appshortcutsdemo.StaticShortcutActivity\"\n      android:targetPackage=\"com.catinean.appshortcutsdemo\" />\n  </shortcut>\n</shortcuts> You can see that the root tag of this file is <shortcuts> , which can hold multiple <shortcut> blocks. Each of them, as you may have guessed, represents a static shortcut. Here, the following properties can be set on one shortcut: enabled : As the name states, whether the shortcut is enabled or not. If you decide to disable your static shortcut you could either set this to false or simply remove it from the <shortcuts> set. One use-case where you might want to use this feature is to control which shortcut is disabled by build flavour. icon : The icon shown on the left hand side of the shortcut. shortcutDisabledMessage :  this is the string shown to a user if they try to launch a disabled shortcut pinned to their home screen. shortcutLongLabel : This is a longer variant of the shortcut text, shown when the launcher has enough space. shortcutShortLabel : This is a concise description of the shortcut.This field is mandatory. This is probably the shortcut text that  most users will see on their home screen. intent : here you define your intent (or more intents) that your shortcut will open upon being tapped Here’s how this shortcut would appear to a user of your app: Nice and easy, but if you implement this you may notice that upon pressing back the user is taken back to the home screen. What about instead navigating ‘up’  within the app? To do so, we can add multiple intent tags under the shortcut ones we previously created: <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<shortcuts xmlns:android=\"http://schemas.android.com/apk/res/android\">\n  <shortcut\n  ...>\n    <intent\n      android:action=\"android.intent.action.MAIN\"\n      android:targetClass=\"com.catinean.appshortcutsdemo.MainActivity\"\n      android:targetPackage=\"com.catinean.appshortcutsdemo\" />\n    <intent\n      android:action=\"android.intent.action.VIEW\"\n      android:targetClass=\"com.catinean.appshortcutsdemo.StaticShortcutActivity\"\n      android:targetPackage=\"com.catinean.appshortcutsdemo\" />\n  </shortcut>\n</shortcuts> Notice how we added an extra <intent> before the one that we had, pointing to the MainActivity . This will create a back stack of Intents, the last one being the one opened by the shortcut. In our case the back stack looks like MainActivity -> Static ShortcutActivity , so when pressing back the user is taken into the MainActivity : Adding static shortcuts is pretty easy. Let's move on defining some dynamic ones. Dynamic shortcuts As their name suggests, dynamic shortcuts can be modified at runtime without the need of re-deploying your app. As you may have guessed, these are not defined through a static resource ( shortcuts.xml ) like the static ones, but are created in code. Let's add our first dynamic shortcut! In order to do so, you will have to make use of the ShortcutManager and the ShortcutInfo.Builder . I'll be constructing the first dynamic shortcut in my MainActivity.#onCreate() : @Override\nprotected void onCreate(Bundle savedInstanceState) {\n\n\n    ...\n\n\n    ShortcutManager shortcutManager = getSystemService(ShortcutManager.class);\n\n\n    ShortcutInfo webShortcut = new ShortcutInfo.Builder(this, \"shortcut_web\")\n            .setShortLabel(\"novoda.com\")\n            .setLongLabel(\"Open novoda.com web site\")\n            .setIcon(Icon.createWithResource(this, R.drawable.ic_dynamic_shortcut))\n            .setIntent(new Intent(Intent.ACTION_VIEW, Uri.parse(\"https://novoda.com\")))\n            .build();\n\n\n    shortcutManager.setDynamicShortcuts(Collections.singletonList(webShortcut));\n} In the example above  we acquire the shortcutManager and construct a ShortcutInfo . By using the ShortcutInfo.Builder we can set various properties for the shortcut we want to create. All the builder methods we use above correspond to the same properties used for a static shortcut, so I’ll skip e explaining them again. However, one property that is a bit obscure is the id of the shortcut which is defined in the StaticInfo.Builder constructor as second parameter - shortcut_web . In the example above I've defined the Intent being one that will open my website. Finally, I set the dynamic shortcut on the ShortcutManager . Let's see now how our shortcuts look now: Great! Now we have 2 app shortcuts in our app - one static and one dynamic. Let's add another one that will point to an activity inside the app and see how we can create a back stack for it: @Override\nprotected void onCreate(Bundle savedInstanceState) {\n\n\n    ...\n\n\n    ShortcutInfo dynamicShortcut = new ShortcutInfo.Builder(this, \"shortcut_dynamic\")\n            .setShortLabel(\"Dynamic\")\n            .setLongLabel(\"Open dynamic shortcut\")\n            .setIcon(Icon.createWithResource(this, R.drawable.ic_dynamic_shortcut_2))\n            .setIntents(\n                    new Intent[]{\n                            new Intent(Intent.ACTION_MAIN, Uri.EMPTY, this, MainActivity.class).setFlags(Intent.FLAG_ACTIVITY_CLEAR_TASK),\n                            new Intent(DynamicShortcutActivity.ACTION)\n                    })\n            .build();\n\n\n    shortcutManager.setDynamicShortcuts(Arrays.asList(webShortcut, dynamicShortcut));\n} You can see now that we now setIntents() on the builder in order to build a back stack: The first intent corresponds to the MainActivity . We specify a FLAG_ACTIVITY_CLEAR_TASK flag in order to clear any existing tasks related to the app and make the MainActivity the current root activity The second one corresponds to the DynamicShortcutActivity (which is just an empty activity that I created). In order to do so, we need to provide an Intent with a specific action , which is defined as a static String in DynamicShortcutActivity and corresponds to the intent-filter action name defined in AndroidManifest.xml for the same activity: <activity\n      android:name=\".DynamicShortcutActivity\"\n      android:label=\"Dynamic shortcut activity\">\n      <intent-filter>\n        <action android:name=\"com.catinean.appshortcutsdemo.OPEN_DYNAMIC_SHORTCUT\" />\n        <category android:name=\"android.intent.category.DEFAULT\" />\n      </intent-filter>\n</activity> By declaring this array of intents in this order, we ensure that when the user presses back after opening DynamicShortcutActivity through the shortcut we created, the MainActivity will be opened. Let's see how they look like: Shortcut ordering Now that we have 1 static shortcut and 2 dynamic ones, how can we specify a custom order for them? If we take a closer look at the ShortcutInfo.Builder methods, one in particular gives us a clue: setRank(int) . By setting a custom rank to a dynamic shortcut we can control the order they appear when revealed: the higher the rank, the most top the shortcut goes. As an example, say we want shortcut number 2 ( novoda.com ) to sit at the top. We can dynamically change the ranks of the already added dynamic shortcuts. Let's do this when pressing a button from MainActivity : findViewById(R.id.main_rank_button).setOnClickListener(new View.OnClickListener() {\n\n\n      @Override\n      public void onClick(View view) {\n          ShortcutInfo webShortcut = new ShortcutInfo.Builder(MainActivity.this, \"shortcut_web\")\n                  .setRank(1)\n                  .build();\n\n\n          ShortcutInfo dynamicShortcut = new ShortcutInfo.Builder(MainActivity.this, \"shortcut_dynamic\")\n                  .setRank(0)\n                  .build();\n\n\n          shortcutManager.updateShortcuts(Arrays.asList(webShortcut, dynamicShortcut));\n      }\n}); In the click listener of the button we create a new ShortcutInfo for each shortcut we have previously added with the same IDs, but now we set a higher rank to the shortcut_web one and a lower one for shortcut_dynamic . Finally, we use the updateShortcuts(List<ShortcutInfo>) method of the ShortcutManager to update the shortcuts with the newly set ranks: You can see from the above gif that the static shortcut sits at the bottom of the list. One thing to note: you cannot change the rank of a static shortcut. They will be shown in the order they're defined in the shortcuts.xml file. Since we have only one static shortcut, it has the default rank of 0 which cannot be changed. Extra bits If we take a closer look at the setShortLabel(CharSequence) method of ShortcutInfo.Builder , we can see that it accepts a CharSequence as a parameter. What does this mean? Well, it means that we can play around a little with it as we can attach custom spans to it. Let's say we want to change its colour to red when pressing the above created button. We can create a SpannableStringBuilder and set to it a ForegroundColorSpan with the desired colour and then pass the spannableStringBuilder as a shortLabel (as SpannableStringBuilder implements CharSequence ): findViewById(R.id.main_rank_button).setOnClickListener(new View.OnClickListener() {\n\n\n    @Override\n    public void onClick(View view) {\n\n\n        ForegroundColorSpan colorSpan = new ForegroundColorSpan(getResources().getColor(android.R.color.holo_red_dark, getTheme()));\n        String label = \"novoda.com\";\n        SpannableStringBuilder colouredLabel = new SpannableStringBuilder(label);\n        colouredLabel.setSpan(colorSpan, 0, label.length(), Spanned.SPAN_INCLUSIVE_INCLUSIVE);\n\n\n        ShortcutInfo webShortcut = new ShortcutInfo.Builder(MainActivity.this, \"shortcut_web\")\n                .setShortLabel(colouredLabel)\n                .setRank(1)\n                .build();\n\n\n        ...\n    }\n}); Wrapping up App Shortcuts are great for exposing actions of your app and bringing back users into your flow They can be static or dynamic Static shortcuts are set in stone once you define them (you can only update them with an app redeploy) Dynamic shortcuts can be changed on the fly You can create a back stack of activities for each shortcut Shortcuts can be reordered, but only in their respective type Static shortcuts will come always at the bottom as they're added first (there's no rank property to be defined on them) The labels of the shortcuts contain a CharSequence so you can manipulate them through spans You can checkout this blog post's sample app on Github This is a cross-post from https://catinean.com/2016/10/20/exploring-android-nougat-7-1-app-shortcuts/", "date": "2016-11-22"},
{"website": "Novoda", "title": "CleanCoders - Mobile App Case Study Review", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/mobile-app-case-study-review/", "abstract": "Learning is hard and comes in many forms. I have recently finished watching \"Mobile Development with Swift\" an online ~10 hour video series by Robert C Martin of the Clean Coders website. It aims to teach you about mobile development, test driven development and clean code. What did I learn? Was it worth it? And should you give it a go? Let's find out. In this blog post I’m giving a personal review with my thoughts and opinions. Uncle Bob himself is an opinionated guy and I get the impression he can appreciate this style of post. My aim is to help you understand what I saw as the key takeaways of this video series so you can make an informed decision about watching them yourself. I’m also hoping that this feedback can inform your approach for any video series you might want to make in the future. Get ready for quite a trip as we spend 7 episodes (21 tomatoes) TDD our way to a GoMoku application using XCode and Swift. As you watch us you'll see the problems we had with the IDE, with the language, and with the framework. You'll also see how we solve them, and drive the application from it's initial meager beginnings to a final scalable architecture. The video series comes in seven parts. Uncle Bob and Justin Martin sit down together and create a Swift application of the board game Gomoku using Xcode. They use a test driven development (TDD) approach and work in pomodoros - meaning they code for 25 minutes, take a 5 minute break and then repeat for three sessions per video. In the break they try to reflect on their decisions so far and discuss the direction they are taking. [1] My initial feeling before I watched the videos was one of excitement. This is going to be great, getting to watch a legend writing code, seeing all his tricks of the trade. Watching a TDD master flex his testing muscles. I also don't know Swift that well and iOS application development at all so this series gives me insight into the platform to start writing an app myself. Let's see how those thoughts pan out. Test Driven Development XCode and the standard tooling was used for writing and running tests. Bob was spot on with his criticisms here, the speed of iteration when running the tests is super slow. When doing TDD you want your tests to run instantly; if you have a single test it should run in something like 0.001s. Instead, the test suite as they had it set up would have like a 3-5 seconds cycle. This becomes painful when you want to run the tests over and over. it seems like the IDE developers have never done TDD I noticed Bob would start doing changes without updating the test name, so I wasn't sure sometimes what they are attempting to do or change in the code. I feel this is a pairing faux pas. The first thing you declare and agree on as a coding entity together is the test name, this sets the direction and the scope of what you want to do together. The great thing about these videos is when a discussion happens around the code. The Swift language is very type safe and Bob was feeling this was leading to verbose code at times. This lead to the declaration that if you are doing TDD it doesn't matter how much power the type safety of a language gives vs a more dynamic language, the tests you have written will keep you more safe than anything. one gets the feeling that the people that wrote this testing framework did not write a lot of tests A lot of people talk about TDD and how they \"do it all the time\" . My opinion has always been that it is a tool, and each tool has a time and place. I noticed this happening in the videos. Bob was coding the board to be shown on the screen and using maths to display 20 vertical and horizontal lines. This is not something I have much experience of and so I would have definitely TDD'd this area, however they did not! They did look back in the pomodoro break and mention this, but I like to see it as an acknowledgment that just because you know TDD doesn't mean you have to TDD every single thing ever. If you understand the problem space and feel you don't need that support, then don't use it. For what concerns that age-old question of when to deduplicate code, there’s different schools of thought. Some do it at the first duplication, when the same snippet appears two times; some when it appears three times. Apparently Bob likes to remove it when it's twice in the code; I disagree and think it should be 3 times. The first duplication is to me some kind of “coincidence”, that can be tolerated, but when that duplication occurs again, it’s time to refactor.. This saves a bit of premature optimisation and the risk of creating the wrong abstraction. It was also painful to watch them refactor as XCode does not make it simple. Possibly another reason would be to wait until three times. doing UI changes can be an alternative to TDD at the GUI level A great point was that TDD gives a quick feedback loop and so doing small GUI changes quickly can be an alternative to TDD at the unit level. This is a great way to think about the GUI, running the UI code and testing it with your eyes. I always had a problem with the lack of TDD at the user interface level and I was glad to hear this little factoid. Learning Swift & iOS One takeaway is definitely that XCode sucks as an IDE. Bob attempted to be political in the video and not blame the IDE but the more they wanted to use the tools they realised the tools were lacking. Swift is a typesafe language but there is no support for refactoring in the IDE. The closest you get to automated refactoring (for example: extracting a method) is search and replace, which is stone-age stuff for those used to JetBrains’ and Microsoft IDEs. I felt the videos didn't allow you to learn much Swift or gain much iOS knowledge, which was a shame. I watched them coding and so I wasn't hands on, which is one downside in addition to that they weren't Swift experts themselves. That’s fine to an extent, although when they had problems it seemed they would go away off camera and find out the answer then come back with the solution. It would have been nicer to go on this journey with the developers watching them \"google it\". swift has type safe exceptions, every other language has abandoned them A thought provoking storyline of the videos was the use of exceptions. It seems that you cannot have runtime exceptions in Swift, so any exception that is stated has to be declared by all calling methods up the chain. After a while this got too painful for Bob and they switched to returning a tuple from these methods with an optional result and optional error. It was a really interesting discussion and good to watch the code evolve. I'm still not sure that the tuple was the answer but the app works, that's the main thing. I'm not sure why but they kept fighting against the language to avoid using the named arguments. I disagreed with them but I quite enjoyed (and learned from) watching them lament and argue with the codebase. It reflected a lot on conversations I've had when coding. Named arguments allow you to have more readable methods invocations, such as fireMissile(at:England, lat:35, long:32) . Here, the at , lat , and long are the named arguments. If you want to, you can declare that you don’t need to name them explicitly, so the invocations would look like this: fireMissile(England, 35, 32) . Unfortunately, this got them into a corner. They decided to change the parameter order of two methods but without named arguments they were not sure what method invocations they had done and which were still left. Keeping the named arguments would have helped so much and I really like that feature in Swift. On a similar note, they got rid of an object they had called Intersection (this was an X,Y position) in favor of an int X and int Y . The videos seemed to gloss over why they removed it but I felt this was a good abstraction that would have minimised the named args problem to a single object construction. I did learn a few parallels with what I’m used to in Android development: gesture recogniser & custom view constructors are the same as Android setNeedsDisplay() is the same idea as invalidate() paint.stroke() is similar in idea to a custom views onDraw() I felt the videos were lacking a real discussion about the iOS framework. Making a game you get away with not knowing as much about the framework (vs writing an app) but I was disappointed to not learn how to navigate around or understand the iOS lifecycle. A lot of time was used talking about how to find the win conditions in Gomuku, not something I can easily transfer. A lot was promised in the first video and it felt like they'd already planned to write 8 hours of video so when they realised they wouldn't get it all done in that timeframe they sped up at the end and skipped pieces out. Organisational techniques I know I'm an organised person so maybe my next point is an over specification, a TODO list is something I like to practice when I pair and I think it really benefits both of us, before, during and after each pairing session. Contrary to what I had expected, they did not keep any notes or a step by step TODO list. It was not until episode three that they got to writing a high level list on the whiteboard—and then they didn’t follow it explicitly anyway. I find it really important when working in a pair that you both understand what you are working on (and if I'm watching a video of pairing, then me knowing what you are working on). TDD is great for understanding the current task, you always have the current red test that you are trying to make pass, and both of you know this is the current aim. However in the refactor step of TDD there is no test name telling you what to refactor. Time and time again Bob & Justin would be talking and say \"oh we should fix this later\", \"ah that'd be nice if we renamed this\", \"we could extract a class here, maybe once the tests pass\". However this knowledge was just kept as a verbal acknowledgement, and throughout the videos they would forget they had said these things. As a result, they missed several opportunities to refactor and clean up the code. In my experience, a notepad shared between the pair to write down these notes works great. I have also watched JB Rainsberger's Intro to TDD which is a great example of what I am talking about. [1:1] Pomodoro technique I'd heard of the pomodoro technique but never actually done it myself. The explanation in the video of how it works was a bit confusing as a starting point, but then I learnt by example. Each time a pomodoro ended a clip of a tomato being chopped in half would play. I hated that tomato cut scene though, it got so repetitive and I was actually analysing the chop downwards and thinking how unrealistic it looks (instead of thinking of code). Maybe a bit of variety would have stopped me from obsessing over it. Sometimes the pomodoro allows them to brainstorm for a solution \"away from the camera\". In the breaks they’d resolve the problem then come back and say “eureka this is the answer”. As a spectator I felt I was missing the real world action of maybe a more heated or confusing discussion or some Googling. In one instance, Justin’s hair changed from one pairing session to the discussion that was supposedly happening straight after , so that gave it away a bit. However it has inspired me to try Pomodoros when pairing, to have that discussion of where are we up to and what do you think of our decisions so far. I have some minor gripes over the way the pomodoros were treated in this series. Sometimes Bob felt he had to keep talking up until the pomodoro ended but I think you should be more flexible. If there is one minute until the end of the cycle and you have reached some type of milestone, I believe it's ok to finish early and ignore the tomato. That's exactly what Bob did at another time, ignored the tomato to ensure a refactoring was completed and running 2 minutes over. It’s nice to see some pragmatism. There was no discussion after the final tomato of the final episode, which I found very disappointing. It would have been really nice if they had a little retrospective about what went well, what didn't go so well. Architecture, or the lack thereof... then, magic ✨ At the very beginning they discussed a global architecture, from the games point of view. This helped direct the responsibilities of some of the code but not everything. I was looking forward to the discussion about what had been created and how this could be refactored into something more fitting with the original episode one discussion. Sadly, this didn't happen. An architecture diagram appeared out of thin air, showing what we had now and what we wanted to aim at. This felt slightly like the architecture was coming from an ivory tower , and an ex post facto rationalisation of what had been already done. I wanted to see some insight in the videos of how this came about and how it was built up from its pieces rather than handed to us on a plate. Bob reviewed the architecture diagram they had created, they laughed at the mess they'd made \"because that's what developers do\" . However I feel they arrived at this point because they missed the TDD opportunities in refactoring after green tests. This would avoid having to do this \"tech debt\" and reimplementing the architecture. I missed understanding the reason for splitting the board model into state and data, with duplicate methods due to the fact the Architecture diagram was just handed to us and only then implemented. That was quite annoying. I was hoping to learn a lot more about how Bob examined a current architecture then how he would find responsibilities, define interfaces and separate things, but that process was done off camera. Conclusion After a while it feels like you are pairing with them, not concentrating on the video of them coding. Rather, you just focus on the code that is being written and the conversation being had. However every time I noticed one of the small issues outlined above it would take me out of this mindset and remind me I was just watching a video (as I couldn't feed back into the pair), which negatively impacted on my learning. All in all, I enjoyed the video and felt I learnt more insight into TDD and pairing techniques. It gives me confidence that everyone is the same once they sit down in front of that keyboard; 2 years experience or 22 years of experience doesn’t make much difference. I enjoyed Bob's reaction to the tooling for Swift in XCode, and would love to see Apple take this feedback on board. I am disappointed there wasn't more about the iOS framework itself, but making a game is an easy way to avoid this. There was so much more I could have written but I tried to pick out the major talking points rather than nitpicking or repeating the video content. I would totally recommend any developer watching this . You don't need to want to learn Swift or iOS, but you do have to find and dedicate ~8 hours of your life to this series. My top tip for watching is to have a text editor open and write down anything you find interesting or want to look up more information on later. This way you can watch the videos without pausing. Here is a link to the video series, https://cleancoders.com/videos/mobile-app-case-study and an acknowledgement that all video image rights are belong to them. I had to watch these videos with headphones. The recordings didn't appear to use one microphone for each presenter, just a single shared mic. This means they could not be separately equalised and Bob's voice and Justin's voice are at two different levels. It was really hard to pick a volume level where you could hear them both clearly and not pierce your eardrum when the louder of the two spoke. ↩︎ ↩︎", "date": "2016-11-29"},
{"website": "Novoda", "title": "The 5 steps of better product design: 1. Understand", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/the-5-steps-of-better-product-design-step-1/", "abstract": "This series will cover all phases of product design from initial business understanding and customer research, ideation, prototyping and testing through to product delivery and analysis. If you want to create valuable, profitable products that have significant business impact, then you may find this series useful. This week, we’re looking at step one: Understand Ideate Prototype & validate Visual & motion design Measure impact Step 1: Understand Whether you’re starting out with an entirely new product, or improving an existing one, it’s essential to begin with a solid understanding of the market, the business and your customer's experience. The Market In a promising market, there’s always competition. Whether it’s a non-digital solution or a product that appears to have the customer experience nailed, it’s important to understand what’s out there. Look at what other services are offering and most importantly, what opportunities there are to improve the customer experience through your product. The Business Especially with larger companies, it’s important to understand all areas of the business: strategy, research, technology and anything else that could support your product decisions. Lightning talks, as described in Google’s design sprints are a great way to share knowledge concisely and quickly with the whole team. The Customer When it comes to understanding your customers, it’s essential to build a realistic, informed picture of the overall customer experience beyond the confines of your product. Focus groups Focus groups can provide a relaxed environment for open discussions and large-scale sketch-noting. Starting with a group energiser session helps participants to break the ice and encourages conversation. Customer interviews and journey mapping Spending one-on-one time with your users is one of the most effective ways to gain insight on their experience. An ideal interview starts at the beginning of their journey and continues through the process, describing each step along the way. As the interview proceeds, you can use sketch-noting to create an experience map highlighting key moments, pain-points and emotions. These unique insights can then be analysed alongside your other findings and together form an overall customer experience map, highlighting obstacles and opportunities. App store reviews & customer care It goes without saying, app store reviews can be harsh but they can also be a great ego boost and provide vital feedback to the product design process. Spend some time reading the good and the bad to identify customer pain-points and favourite features to help you improve the product design. The customer care team (if you’re lucky enough to have one) will be a fountain of knowledge on any issues your customers may be having with the service. Asking them to give a lightning talk about their insights could be of great benefit to the team and the project. Analytics Analytics refers the process of collecting, analysing and formatting large-scale data. When exploring a new design challenge, analytics can help to understand current usage and behaviours such as element interaction, screen views, session information and crashes. You can read more about analytics in Step 5: Track & validate . What’s next? When you’ve concluded your research, it’s time to share! It’s important the whole team starts the project well-informed, with a shared focus on the end goal. A high-level presentation outlining the results of your research, clear business goals and key opportunities will form the perfect platform to kick-start your project and prioritise solutions. Read 'The 5 steps of better product design: Step 2: Ideate' now.", "date": "2016-11-17"},
{"website": "Novoda", "title": "How we enjoyed Droidcon London 2016", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/droidcon-london-2016/", "abstract": "Droidcon London is the biggest and best Android conference of the year. You have knowledgeable speakers, enthusiastic people, engaged sponsors and worthy competitions. Novoda goes to Droidcon London every year and this year we wanted to give back a little of our experience, sharing some of our best moments with you. ‘Be together, not the same’ \" presented by Anastasia Lopez and Romain Piel is the talk that surprised me. This talk repositioned the human in the center of the software industry and insist about the importance of the diversity on software companies. They redefine in a brilliant and enthusiastic way the sense of community. Hawazine One of my favourite talks was Romain Piel’s ‘Hypothesis-driven development on Android’ , where he emphasised how important it is everyone from development to product teams to experiment through A/B testing and getting to know their user in order to iterate faster and be more productive and efficient Andrei An amazing conference even for an iOS developer’s point of view, I enjoyed great talks such as “ Did You Test It? ” presented by  Derek Rozycki and Kirk Chambers. As an iOS developer, I highly recommend you attend a conference that diverges from what you do everyday, in order to observe differences and similarities, which is definitely an valuable experience. Urso The Novoda stand this year tried to have a relaxed and inviting vibe to it. We had our CI wall up showing a range of devices running a set of acceptance tests that were open source and anyone could contribute to. Two examples being: a test that showed random images of bacon and a test that displayed the green moving text of the matrix. In the middle of our stand was a MarioKart tournament (we’re big fans) that was open for anyone to come play. To the left of the stand was a retro, two player arcade gaming unit. The Novoda stand also had a competition for writing postcards to someone who couldn’t make it to droidcon. The prize was one of the amazing SOLID posters created by Qi Qu . 'The best thing about Droidcon is the community coming together. Every year it’s like a family reunion, catching up with old friends, making new ones and sharing our knowledge.' Leonie A big thank you to all the speakers this year! The calibre of the talks was really high and everyone we have been getting feedback from gained a lot and found something interesting they could learn from or relate to. Thanks for taking the time to submit and the time out to deliver your talks. A big thank you to all the sponsors. Without sponsors a bunch of talks is only half an event. It’s great to see the diversity and interests that the sponsors have, and as always what makes a great sponsor stand is a great competition. (Big shout out to Amazon who gave away 40 Fire TV sticks!) Jose Alcérreca (Google) and David González (GDE) presented Android Architecture Blueprints , a project featuring a collection of app architecture samples. The project contains an pretty common to-do app implemented using different architecture approaches, which could help comparing them and making the decision on which one to use on your next project. Luis Valle A great event, with many opportunities. My personal favourite & most beneficial part of the conference was meeting developers & industry related professionals that had travelled from across the globe to be at the conference. Stretching from Denver USA to Norway, we got to explore the Android world under one roof. Natalie Harman I especially enjoyed Huyen Tue Dao's 'A New View' , a reassuring introduction to ConstraintLayout, and Kelly Shuster’s 'Android is for Everyone' ,  in which she created a compelling narrative for inclusive design Ataul I thought the lightning talks where a really good idea, they were 3 x 15 minute slots. You can’t go too indepth in 15 minutes but it’s good way to have an intro to something. For me the lightning talk “iOS for Android devs” given by Daryl Bayliss was an eye opener for how much the two platforms share in terms of ideas for development and gave me a strange feeling that they are converging... Paul Droidcon London is a great event for learning but also a great event for meeting other developers and other people in the industry. If we have one final thought it’s to always remember to attend at least one after hours event. Some of the best conversations happen in the social events that happen on the Wednesday before, the Thursday night of, and the Friday night after Droidcon. See you next year!", "date": "2016-11-15"},
{"website": "Novoda", "title": "Github Data Mining: on Amazon Web Services", "author": ["Rui Teixeira"], "link": "https://blog.novoda.com/github-data-mining-on-amazon-web-services/", "abstract": "This blog post describes how we implemented our Github Reports ecosystem on Amazon Web Services (AWS) and the technical challenges we faced. We will dive into some of the main components of AWS, so that you will hopefully get set up quicker than we did and avoid the pitfalls we fell into. In our previous blog post we discussed about the general idea behind mining our Github data, with the purpose of extracting useful organisation information. We showed an ideal algorithm to retrieve historical data, handle network errors, retry operations and decompose the problem into multiple pieces. An algorithm and all of its friends The process we thought of consisted in the following steps: Starting the process Requesting a piece of Github information Converting the information Persisting the information Handling API rate limits Handling network errors Understanding “what to request next”, then doing it Keeping everything abstract helped us think more clearly of the components we needed: some kind of worker instance to actually run the code a database to store and read data from some scheduler to restart requests that have failed for any error a queue to store the messages (all the information we want to extract) In fact, we wrote an abstract class that wraps all of these meta-components together to provide global functionality without detailing internal implementation (following code is a slight simplification of the actual one ): if (configuration.hasAlarm()) {\n    String alarmName + configuration.alarmName();\n    alarmService.removeAlarm(alarmName);\n}\nQ queue = getQueue(configuration);\nM queueMessage = getItem(queue);\nList<M> newMessages = handleQueueMessage(configuration, queueMessage);\nupdateQueue(queue, queueMessage, newMessages);\nrescheduleImmediately(configuration); A “good enough” approach would be to implement this system on a local computer, which provides all the pieces we need: the worker can be implemented as a simple command line program the database can be easily stored locally the scheduler is simply cron Theoretically, there’s nothing wrong with this approach, except that it would keep a machine busy all the time, is not scalable but, mostly, not kewl. Cartman is not happy Introducing Amazon Web Services Amazon Web Services (AWS) is a suite of cloud services that range from the infrastructure to the platform level. It enables you to define APIs, manage databases, create your own Virtual Private Network, handle traffic scaling manually or automatically, and so on. We chose AWS over similar service providers (Google Cloud, Microsoft Azure) because it seemed as the most complete suite. In fact, after a quick documentation dive, we found out it satisfies our requirements, providing all of the components we need. Simple Queue System SQS is a queue system (no surprise so far) that is “fast, reliable, scalable, fully managed”. Through the AWS SDK, developers can perform all basic operations one would expect to be possible with queues: create, purge, delete the queue add an object read a message delete a message Apart from the basic stuff, SQS provides a model that guarantees that any message, as soon as it’s read by someone, won’t be visible by other queue clients until a certain timeout expires. This means that it’s guaranteed that multiple clients reading the queue at very close instants in time will not read the same message. After the timeout expires, if the message was not deleted, it becomes visible again by any other client. Therefore, SQS was the perfect candidate for deploying and managing our message queue, where every message consists in one atomic operation we need to perform (e.g., get repositories list at page 2, get comments for issue 1337 at page 10, etc.). In our specific implementation, we set the visibility timeout to 0 for a couple of reasons. First, we don’t allow for parallel executions of worker instances, since we rely on the partial order of messages to avoid inconsistencies in the database (e.g., we must hold a reference to a repository before being able to store those repository’s issues). Don’t be like Cartman, keep your queue ordered and tidy Second, if the handling of a message errors, we reschedule the execution on the same message at a variable rate, be it immediate or delayed in time; it’s therefore important for us that the message is immediately visible at any moment after the first read, so that it can be picked up by any “retry” operation. Lambda You’ve probably heard of microservices or even serverless architecture by now, which means you’re very likely to have heard about Amazon’s Lambda too. In a few words, Lambda is a realisation of serverless configuration, allowing you to run units - functions - in response to events , in what is known as event-driven computing: something happens → your unit runs. This implies a little paradigm shift from the usual, or more traditional, way of doing things. Instead of having your application running on a server (or servers) somewhere, you break it down to smaller pieces and you’re billed on the resources it takes to run each piece when the corresponding event occurs. So, you code each unit, upload it and somehow configure it to respond (i.e. run) to events of your choosing. It may also be simpler for you to envision, and develop, your application broken down this way and having it running quicker too (there’s no need to buy/configure servers). This is essentially what AWS offers when it comes to serverless computing, through Lambda. Now, if you read the previous post on Github data mining endeavours, you might remember the tree structure and the need to go through each of its nodes, or “messages” to actually get the information we want, parse it appropriately and persist it according to our needs. This would be our worker !  Backtracking a bit to the beginning of this blog post we start to see what our unit might be: fetch a message; parse it, retrieving its content; and then do whatever it needs to be done, according to the content. We coded our application in such a way that we could pack these three steps in one unit, our worker. So, as an example, and looking at the tree, imagine the worker starts, getting a message (step 1), it goes on parsing it and figuring out it needs to fetch the repositories (step 2) and then does so, adding to the queue new messages relative to each repository (step 3). We have our unit - our worker - we now need to make it work, that is, respond to events. CloudWatch Events CloudWatch is a AWS module that contains lots of resources useful for performance/cost analysis of your cloud platform. It enables you to monitor resource consumption, errors and success rates and effectively plan your spending with billing estimates. Within all of these features, a gem hides: Events. Events is a sub-component of CloudWatch that lets you associate a source event to a target one; for example, you might want to react to AWS API calls for your account to perform some custom metric logging logic, by either calling a Lambda function, do some queue operation, consume a stream, etc. Our simple algorithm requires us to react to some “alarm” event, in a way similar to how cron jobs work. As it turns out, CloudWatch Events has a “Schedule” source event type that allows us to specify a cron expression, so that when that one is triggered the target gets called. In our case, the target is the Lambda function that implements our worker . You can create events manually or via the provided SDK When a lambda is triggered from a schedule, it looks for the associated rule on CloudWatch and removes it, so that a new target won’t be triggered; then, it proceeds its regular behaviour (gets the first message from the queue, processes it, etc.). REST APIs - API Gateway As you already know by now we had to consider two different scenarios where we needed to get data from Github, parse it and persist it. One of these scenarios was pretty much defined from the start: live data; where Github POSTs data as events on repositories occur, via a webhook . This case requires us to be able to respond to these events appropriately. You’ve probably started seeing the unit ↔ event mapping by now… We can have a worker that’s responsible for receiving the payload from these webhook events: every time the webhook is triggered a corresponding worker starts, parses the data and then persists it. Given that the webhook’s payload already contains data about the event itself (e.g. a user commented on a pull request), we don’t need to go through the first step described above (“fetch a message”). So we’re left with the need to bridge the webhook and our worker unit somehow and, given that we have to provide an endpoint for the webhook to hit, having the event that triggers our worker be a request to a certain endpoint only seemed reasonable. Fortunately, AWS offers a service that makes it easy to create, manage and expose an API, with direct tie-in to their Lambda service: API Gateway. You can easily configure API Gateway, creating a resource - webhook - and a method - POST - and then point it to one of your Lambdas. Deploy it when you’re done and that’s it, you’ve got the event that triggers your unit setup. Feelings, wohoo feelings We’ve talked about how we used the tools at hand to achieve what we wanted, but what was the overall experience of actually using these tools? Is is it easy to get up and ready with the AWS solutions? Development Experience Starting with AWS Lambda we can’t say developing for it was a breeze. If you go and have a quick look at their Hello World tutorial [go on, do it!] you’ll see that it’s fairly limited. Lambda supports (only) three environments: Java (Java 8); Node.js (v0.10.36, v4.3.2); Python 2.7. If you took the time to browse through some of their documentation you were probably quick to see that the focus is on the Node.js and Python environments. This was a bit of a set back when we started working with Lambda. Even though we were aware about this from the start, when we actually started coding our Lambdas we didn’t expect the support for the Java 8 environment to be so poor. If you want to target AWS Lambda with Java 8 as your main development environment prepare yourself for quite a bit of digging around the AWS Forums, blog posts, etc. How you parse your input data and how you should prepare your output (erroneous or not) is not clearly defined, or even practical at times. Easiness of Use We felt really confident working with queues on Amazon SQS , as everything worked out just fine right from the first hacks. CloudWatch Events were pretty easy to use as well, even fi we had issues with authorising events to execute custom lambdas. In fact, the authorisation/security model a bit hard to understand , maybe because a bit undocumented. You have roles, users and user groups, policies, ARNs: grasping the basics of how all of this rules mix up is not easy at all. API Gateway is an easy way of connecting lambdas to usable endpoints, even though we found some difficulties working with Velocity templates , a Java-based template engine. The official page claims that: It permits anyone to use a simple yet powerful template language to reference objects defined in Java code. The reality is that Velocity doesn’t look easy to use and is mostly annoying when dealing with query string parameters, as it doesn’t offer helper methods to unwrap common objects. For instance, the following template is used to simply convert the query string parameters into a simple JSON object that can be read from the lambda: #set($params = $input.params().querystring)\n\n#if($params.from != \"\")\n#set($from = \"\"\"${params.from}\"\"\")\n#else\n#set($from = \"null\")\n#end\n\n#if($params.to != \"\")\n#set($to = \"\"\"${params.to}\"\"\")\n#else\n#set($to = \"null\")\n#end\n\n#if($params.users != \"\")\n#set($users = $params.users)\n#else\n#set($users = \"[]\")\n#end\n\n#if($params.timezone != \"\")\n#set($timezone = \"\"\"${params.timezone}\"\"\")\n#else\n#set($timezone = \"null\")\n#end\n\n{\n    \"from\": $from,\n    \"to\": $to,\n    \"users\": $users,\n    \"timezone\": $timezone\n} Now imagine doing this for every endpoint, and/or with regularly changing specs. We really feel like Amazon could provide an (optional) automatic wrapping/unwrapping mechanism for connecting API Gateway to lambdas: converting to custom model objects is much easier from your lambda than in a non-programmable Velocity template. Another pain point was the fact that, if the lambda throws an exception, the API Gateway does not treat the non-0 return code as an error , but simply returns the serialised exception in a 200 Success HTTP response. To return custom HTTP error codes you have to set up custom Integration Responses for every endpoint (again), using regular expressions to parse the content of the returned error: In this case too, Amazon should give us easier ways to achieve this, possibly avoiding the need to configure regular expression matchers on the Web UI. Tooling Using AWS provided tools, such as the CLI or the Web UI, didn’t prove as easy as we wished. We had a very specific issue that we were able to understand only after talking to the AWS support, since nobody replied us in the AWS forums . The Web UI is OK for initially playing with configurations and trying to understand how the numerous options work and belong together. After a while, though, we felt like we needed a tool for automating most of our work: deploying lambdas, changing API Gateway configurations, etc. The AWS CLI , which in theory allows to tweak every single piece of configuration on AWS, isn’t really useful, since commands are very atomic and most likely the base for some other tool. Just look at our instructions on how to setup one single endpoint and make up your own mind on the issue: to us, the AWS CLI was unusable. In order to deploy lambdas without manually zipping and uploading via the Web interface, we used a very nice Gradle plugin that allowed us to work with most Amazon services. Conclusion After our 4 months long experience we feel like Amazon AWS offers a complete service set for our use case, so rich that it cannot be matched by any other cloud provider , at the moment. The learning curve is also pretty low at the beginning, while the Web UI is decent, given the amount of options you need to work with. The lack of good documentation , the bugs we encountered while trying to configure all services and the poor quality of tooling , though, made us roll our eyes multiple times, and will likely cause us to look for and compare alternative providers next time.", "date": "2016-11-08"},
{"website": "Novoda", "title": "Composing functions in Kotlin with extensions and operators", "author": ["Daniele Conti"], "link": "https://blog.novoda.com/composing-functions-in-kotlin-with-extensions-and-operators/", "abstract": "Kotlin gives us powerful ways to expand the language, but is there something we can exploit to improve the way functions are invoked? Today I was wondering if I could find a more elegant way to compose functions with Kotlin. Let’s imagine we have this: fun same(val: Int) = val\nfun twice(val: Int) = val * 2\nfun trice(val: Int) = val * 3\nfun composed(val: Int) = same(twice(trice(int))) That gets the job done, but wouldn’t it be better if there was a way to remove all those brackets and get something more like a pipeline? Keep in mind: the order of execution will be inverse (trice-twice-same), which is not intuitive. Turns out there is, if we use two awesome Kotlin features: extensions and operator overloading .  What are those? Extensions Extensions let you add extra methods to any type, calling them as if you were declaring a method inside that type. An example could be: fun Int.double() {\n  return this * 2\n}\n2.double() == 4 This basically means: add a method to the type Int with name double , which you can then invoke on any Int . The keyword this is used to get the instance on which you’re invoking the method (the receiver object). How this works in more detail is compiling to a static method of which this is the first parameter. An interesting thing is that, in Kotlin, a function can be used as a type too. So we might be able to write something like this, as an extension function: fun (() -> Unit).andHello() {\n  this()\n  println(\"Hello, world!\")\n}\nfun greet() {\n  println(\"Hey there!\")\n}\nfun greetAndHello() {\n  ::greet.andHello()\n} () -> Unit is the way you can describe a function type (you’d do the same if it was a parameter). Inside the () you would put the parameters, Unit means it’s a void function (otherwise it would be the return type, for example (Int, Int) -> Int would be a function taking 2 Int parameters and returning an Int ). Here we are referencing a function with the :: operator, and applying the andHello() method to it. Pretty neat! Operator overloading Imagine you have an interesting type, and you’d like to use some standard operators with it. Wouldn’t it be great, for example, if you could access the elements in a map simply using square brackets? You can indeed! val map = mapOf(\"a\" to 1, \"b\" to 2, \"c\" to 3)\nprintln(map[\"a\"]) // 1 How does this work? Via operator overloading : you declare a method with the keyword operator in front of it, plus a certain name and signature, and then implement the method as you would do with any other method. class Map<Key, Value> {\n  operator get(key: Key): Value {\n    // get and return the value from the map here\n  }\n} You can then use the operator on that type normally. That can come in handy to redeclare operators in a way that makes more sense for your classes (maybe you want to sum two Time instances together?). You can find the full list of available operators and respective signatures in the operator overloading documentation page . So, let’s take a small leap and imagine this is the final result we wanted: fun composed(val: Int) = (::trice..::twice..::same) (val) What are we trying to achieve? It’s pretty clear: we want to call, in sequence, in this order, the functions. Notice how the call order is now reversed compared to the first example (and makes it easier to read). I’m using the .. operator (range) because it reminds me of chaining when we talk about functions. For example, it’s used to call multiple methods in sequence on an object in Dart. How could we achieve that? All those functions are receiving one parameter and returning something, and we know that we can create an extension for it. operator fun <T, R, V> ((T) -> R).rangeTo(other: (R) -> V): ((T) -> V) {\n    return {\n        other(this(it))\n    }\n} What is this? Let’s go one step at a time. First of all, we’re declaring an operator overload. The rangeTo method represents the .. operator. We’re declaring this extension on any function of type (T) -> R so any function that takes a parameter of type T and returns a type R . Notice that T and R can be the same (for example, Int s). This extension accepts another function, of type (R) -> V , so it will take the previous function return type and return another type. Finally, this will generate another function, of type (T) -> V which is what we expect after calling both functions. With the acquired knowledge, let’s look at this again, does it make more sense? fun composed(val: Int) = (::trice..::twice..::same) (val) We’re applying the .. operator to trice , twice , and same , in order. Since the return type is a function, the result of ::trice..::twice can be then chained with ::same . The result of this last operation is again a function, which then we invoke with (val) as an argument. Kotlin metaprogramming features are extremely interesting, and give you the chance to make your code more expressive, especially when you combine features like extensions and operator overloading. This is only a small example of what you can achieve, so start playing with it if you aren’t already. This post has been cross-posted from my Medium story .", "date": "2016-11-03"},
{"website": "Novoda", "title": "Android/iOS cross-platform project setup", "author": ["Francesco Pontillo"], "link": "https://blog.novoda.com/android-ios-cross-platform-project-setup/", "abstract": "Is there a good way of setting up a cross-platform Android and iOS project? How can we help developers from different backgrounds contribute as much as possible on other technologies they’re not familiar with? What is the “least effort, maximum gain” set of tools we can use to achieve all of this? One of the most painful parts of starting the development of a new project is going through the setup of build tools, plugins, helpers, shrinkers, continuous integration, etc. When we have to develop for multiple platforms, the effort is exponentially bigger: achieving feature parity means that developers from those platforms must be able to start together from a common ground, then evolve gradually. Here at Novoda, we are experimenting with different approaches for cross-platform projects. Read through to make your own idea about the optimal way of doing it. Photo by clement127 Apples are fruit, androids are robots For Oddschecker , which we developed for both Android and iOS , we created two separate Github repositories. This setup allowed us to use the most appropriate, known and widespread tools for each platform: Gradle for Android xcodebuild driven with Gradle tasks This reflected on a number of different things. First, we had to create and maintain many Jenkins jobs for each repository: one for the Pull Request Builder, one for the nightly builds and one on the master branch. For Android, we also use the Monkey runner to stress test the applications we develop. Secondly, having separate projects also tends to discourage cross-platform contributions , which is something we pride ourselves in. Having Android developers check on iOS pull requests (and vice versa), make comments and ask questions helps all of us grow professionally and learn. One repository to rule them all The desire to improve cross-platform collaboration and decrease setup times led us to shift our approach towards a single repository that contains both Android and iOS projects. The root directory would contain a simple README to guide first-time developers through the specific project folders, which then include their own README and structure, just as if they were separate repositories. This structure has helped us reach the level of cross-platform collaboration we wanted; we currently have a rule that any pull request cannot be approved if at least a “developer from the other side” hasn’t approved it. Such collaboration is fundamental especially in projects where we want to have a common architecture across platforms. Thanks to the shared knowledge and vocabulary we can discuss concepts and develop components together. The single repository is also a strong point towards the simplification of the CI setup: less configuration means less mistakes, which leads to improved time management for everyone! But how can we use a single repository and compact CI configurations for technologies that share very little between them? Gradle as the Master of Ceremonies When we started our multi-platform project we immediately asked ourselves what was going to drive the repository build and test commands. We immediately thought of Gradle as the ideal tool for the job. There are two reasons for the choice. Firstly, we have an extensive and proven knowledge of Gradle. And secondly, iOS doesn’t really have an official configurable command line development tool. An Android developer hears about iOS dev tools In case you haven’t heard about it just yet, Gradle is an extensible set of libraries written in Groovy and running on the JVM, which acts as a configuration-based task runner reading configurations from .gradle scripts. Android and iOS as Gradle modules We started our configuration process by creating two projects, android and ios , in directories with the same names under the main root folder. Then, we declared these two directories as Gradle modules by adding a settings.gradle file to include those projects: include 'android', 'ios' After that, we just had to create a build.gradle for each platform-specific project. Plain Gradle for Android, duh Android projects are Gradle-based by default since 2013, but including them as sub-projects didn’t turn out to be as easy as we imagined. In fact, we required that: Android developers can run the Gradle wrapper inside the android sub-project directory iOS developers can run the Gradle wrapper inside the ios sub-project directory the CI can run the Gradle wrapper in the root project directory the build behaviour must be consistent regardless of the wrapper location Achieving all of this didn’t prove painless, since we got stuck with relative path errors and inconsistent behaviours with custom plugins. After a few hours spent tweaking and fiddling with the configurations, our project built perfectly. xcodebuild vs fastlane for Apple Apple’s tool chain, on the other hand, is heavily tied to the Xcode IDE. The associated command line tool, xcodebuild , can be used to test and archive your app from the shell. The main weaknesses of xcodebuild is that is not easy to configure a task, and that there is no task manager, so you have to create your own collection of bash scripts to reuse the original xcodebuild command. According to our experience, xcodebuild is not the best solution to setup and configure our tasks, for this reason we moved to fastlane , a build tool that in the last few years has become the de-facto-standard in iOS development. fastlane , in fact, has a proper task manager and comes with a great collection of actions that will simplify your life as an iOS developer. Building, testing and deploying are just some of the basic actions, but there are more advanced capabilities. For example you can generate screenshots for the AppStore, setup certificates on the local machine and notify your team with a Slack message. Actions can be collected in “lanes” that you can “drive” using the fastlane command line. The output of an action is available for the next actions, so you don’t have to worry about passing parameters around; normally you just have to setup your action with the bare minimum amount of parameters in order to make it work. Also, information will be inferred whenever possible, so you don’t have to be super explicit. For instance, you don’t have to specify the workspace if you only have one in your working directory. Most of the actions available are meant to be used in a CI machine. scan , for example, runs tests for your project and generates a report that can be seamlessly consumed by Jenkins. Bridging fastlane to Gradle was ironically much easier than making the Android project work consistently, since we simply execute commands on the shell, calling fastlane with the appropriate arguments. Good enough, ship it? Overall, we were pretty happy with this setup. However, we weren’t sure about the maintainability of the Android configuration, since it is pretty complex and many code quality plugins rely on relative paths. Our main problem, though, was related to the Gradle bootstrapping time , which took from the optimistic 5 to the more realistic 10 seconds depending on the machine just to evaluate all projects configurations. This meant that an iOS developer who wants to run the same iOS tests that run on the CI will have to wait for the Android project to be evaluated by Gradle, even if it’s not needed at all! An iOS developer shocked at Gradle spin-up times Gradle with no sub-projects The Gradle evaluation issue led us to re-think the whole setup: do we really need to evaluate the Android and iOS projects right away? As a matter of fact, we don’t. We removed the root settings.gradle file, which completely annihilated the bootstrap times. To run the platform-specific tasks, we now simply make the root project call another Gradle or fastlane instance in the platform directory: // Android tasks\n\ntask('testAndroid', type: Exec, group: 'verification') {\n    this.workingDir = './android'\n    commandLine './gradlew', 'clean', 'test'\n}\n\n// iOS tasks\n\ndef fastlane(lane) {\n    return ['bundle', 'exec', 'fastlane', lane]\n}\n\ntask('buildDependentsApple', type: Exec, group: 'build', description: 'Setup the machine with all the iOS dependencies') {\n    this.workingDir = './ios'\n    commandLine 'bundle', 'install', '--path', 'vendor/bundle'\n}\n\ntask('testApple', type: Exec, group: 'verification', dependsOn: 'buildDependentsApple') {\n    this.workingDir = './ios'\n    commandLine fastlane('test')\n} This setup enables us to save lots of configuration time, since the Android project is evaluated by the Android-specific Gradle wrapper, and only when it is actually called. Of course, we can still run both Android and iOS tests with ./gradlew testAndroid testApple . Although this project setup might seem a bit silly (all of this can be implemented with Bash scripts, after all!), Gradle gives us a better understanding of task definitions and dependencies, and can be easily extended with future team-level tasks. In fact, we have developed tasks for commit message validation and selective tests: stay up to date on Twitter and Google Plus to know when we open source them! TL;DR In our experience with developing the same application for both Android and iOS, we found out that: using a single repository facilitates cross-platform collaboration, improving code quality and shared understanding Gradle is a good build tool to coordinate builds for different platforms there’s no need to include the actual Android and iOS projects as Gradle modules, since there are no inter-dependencies between them Gradle can be used to define and share team-level tasks , beyond build and test", "date": "2016-11-10"},
{"website": "Novoda", "title": "Refactoring: Introduce Parameter Object", "author": ["Luis G. Valle"], "link": "https://blog.novoda.com/refactoring-introduce-parameter-object/", "abstract": "˝Refactoring is a controlled technique for improving the design of an existing code base” — Martin Fowler. Discipline and methodology are critical when refactoring. We should refactor applying a series of small controlled steps. After each of these steps our tests should still be green . This way, if we break something in the process we can easily roll back. Which prevents having a broken code base while we do the refactor. How to do it? Imagine we have a class which has some public methods and they have similar signatures. This is the perfect case to apply the Parameter Object Refactoring. Here are the steps on how to do it: Write unit tests if the class to refactor isn't tested yet :D Create a new immutable class to group the parameters of the method you want to refactor. Add a new parameter of this class to the method under refactor. In all the callers use null for this parameter value. Run your tests. One by one, remove the old parameters from the signature of the method under refactor. Then in the method’s body  you can use the new parameter object for the replaced value. Run your tests after each step. If we do it right, then at the end of the refactor our tests should pass and the system should work as before . It is important to have in mind that we are only improving the design of the code, not changing its behaviour. Bonus points! Once you are done removing all the parameters you should think as to whether the behaviour you are executing can be moved to the new class! Remember the Tell, don't ask principle: \"Object-orientation is about bundling together data and behaviour.\" Why are you doing it? Passing around the same group of parameters, like database query parts, is a code smell we call data clumps which can be seen in the book Refactoring by Martin Fowler . These clumps of data should be grouped in a class because they lead to code duplication . Maybe not in the sense of duplicated methods but in the treatment we do to those parameters like validations, parsing, etc. You can check out this nice list for more code smells like the one mentioned. Need for a common language Learning about refactoring techniques is very important for two reasons: It helps you improve the quality of existing code without breaking it. The key part is without breaking it . It sets a common language on how to improve code quality . This has the same benefit as Design Patterns as having a common language makes it much easier to discuss with colleagues in PRs or pairing sessions. Show me how! Automatic refactor If you like this technique and you can't wait to jump in your code and refactor all the things I have one more  piece of good news for you. Android Studio (and of course IntelliJ) does this for you. For free. Gratis. Automatically. https://www.jetbrains.com/help/idea/2016.2/extract-parameter-object.html So you don't need to remember all those steps. But it is always good to know what is going on under the hood, right? ;)", "date": "2016-10-24"},
{"website": "Novoda", "title": "Swanseacon 2016 : An agile software craftsman's view", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/swanseacon-2016/", "abstract": "SwanseaCon is billed as \"a technical event for Software Developers, Software Architects, Project Managers, Analysts and Consultants.\" I fit in there somewhere so I decided to go! Here I want to share some thoughts on the event and the talks, to inspire you to learn more or even to turn up next year. The keynote was a great way to kick off the event and to inspire thinking on a wider range of subjects. Simon Brown talked about the birth of agile and how it replaced \"big upfront design (BUD).\" He argued that it  replaced BUD with no upfront design and deferring of decisions until too late. He talked about how agile was created as a backlash in response to previous ways of working, meaning that there was a tendency to forget, or even sneer at, the past. The talk was a reminder to \"not throw the baby out with the bathwater\" . Frameworks like the Rational Unified Process is a great tool for risk analysis and class responsibility collaboration cards are a great process for responsibility analysis. It was a thought provoking keynote to remind us that just because some tools are old it doesn't mean they aren't useful in the right time or place. The next session was a hands on explanation of Kano Analysis by Dave Grant . I hadn't heard of this before hand but it is a quite interesting way to understand what features to build next for your product. You ask yourself questions about your product and get a group of users to answer if they like the idea, expect it, neutral to it, tolerate it, dislike it. Grouping users and features you can see what will give you the biggest return over time. It was interesting to learn about for the first time and I would like to try it again soon. ‘It doesn't always have to be scrum’ by James Harvey was a great reminder that there are other methodologies out there besides Scrum. If you think of agile as an umbrella you can do scrum , kanban , less , dad , dsdm , SAFe , lean and probably more. It was a real eye opener to other possibilities and shows that being agile is all about adapting your process to work with the people around you, not forcing your team into Scrum by default. ‘Understanding abstractions’ by Matt Ellis was an informative talk attempting to explain how abstractions can save us time but they do not save us from learning. Unfortunately it doesn't look like it was recorded and I don't think I can do it justice here. Matt explained how regular expressions work and how they are an abstraction that we don't need to know what they do... until we do. Recommended further reading is, The law of leaky abstractions and catastrophic regular expressions . ‘How deep are your tests’ by Thomas Sundberg was a play on words to describe the testing pyramid . The aim was to promote thinking at each level of the pyramid about what you were  testing and what the depth of those tests was. It referenced the book Growing Object Orientated Software Guided by Tests and attempted to build on top of its ideas, to not just assume once the tests are written they should not be left alone; ask questions of your testing suite, around diagnosis of bugs, speed of the tests and confidence in the results. ‘Introducing eager design’ by Marcello Duarte continued nicely on where the keynote had left off. Marcello explained how a lot of things you develop are easy to implement and you should concentrate your effort on starting with that which is hard. He boiled it down to four prerogatives: Jump to the problem worth solving Eagerly replace primitive types Compose (the domain algebra) inside out Avoid mutable state He gave many intriguing references and I couldn't keep up to write them all down but am excited to read/watch what I did get: When cucumbers go bad , Modelling by example , Meaning of OO and The value of values . ‘Teal organisations’ by Sandro Mancuso was a real insight into the choices and responsibilities in building a company. Sandro is taking the craftsmanship approach to organisational structure in his company. Having everyone responsible for making company wide decisions (if they have a proposal and a backing of X other employees). It also means having company public financials and being explicit about pay rises and who earns what. There are no ‘support’ roles (HR, hiring, accounts) so that responsibility is on the developers and this talk explained some ideas on how to make that work. My main takeaway was around team organisation, explaining how you rely on each other as team members, and that when a team changes it is a different team. This really impacted me in my thinking of a team makeup and how this affects some of our own team rotation. Teams are immutable if you add or remove people it is a different team The venue was smart and compact inside of the Swansea football stadium, with rooms either side of the main area and the food / sponsors in the middle. There were ongoing sponsor competitions for winning books or consultancy time, and free cakes for talking to the facilitators of some stands (always a winner). One really cool thing about Swanseacon is they had a dedicated sketchnoter. This was an artist who attended  every talk and summarisedtried to summarise each talk in the form of a sketch note ( which was a great reminder for me to look back at to write this blog post ). There where two tracks and I don't have a time turner necklace so I missed half, but you can catch those that where recorded on the YouTube channel here . Software craftsmanship and agile processes are at the heart of the SwanseaCon talks. I hope my brief insight into some of the topics and technologies have inspired you to go learn more on some of the interesting subjects. Software craftsmanship is about continuous learning and development and I personally will be looking for more conferences to go to next year, if you have any recommendations please get in touch .", "date": "2016-11-01"},
{"website": "Novoda", "title": "Customising usage hints for Android TalkBack", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/customising-usage-hints-for-android-talkback/", "abstract": "In this post, you'll discover how to provide usage hints for visually impaired users and customise built-in ones to suit your app. TalkBack offers a set of usage hints which are read aloud on actionable Views. An action could be something as simple as a click or a long-click. Consider the View below ( previously seen in an earlier post ), which has three inline actions, \"reply\", \"retweet\" and \"like\". For users with TalkBack enabled, the inline actions are disabled and hidden. Instead, we provide a dialog (containing these actions) which will open when they click the View. When the user long-clicks the View, it will trigger the \"like\" action. On TalkBack, clicking is achieved by double-tapping the screen while the View is selected. Long-clicking is achieved with a double-tap and hold. TalkBack will announce these as available actions after reading the View's content description. You can customise this usage hint so it's a bit more explicit than \"activate\" or \"long press\" by overriding the label for the click and long click actions. class BetterLabelsDelegate extends AccessibilityDelegateCompat {\n\n    @Override\n    public void onInitializeAccessibilityNodeInfo(View host, AccessibilityNodeInfoCompat info) {\n        super.onInitializeAccessibilityNodeInfo(host, info);\n\n        AccessibilityActionCompat click = new AccessibilityActionCompat(AccessibilityNodeInfo.ACTION_CLICK, \"Select an action\");\n        AccessibilityActionCompat longClick = new AccessibilityActionCompat(AccessibilityNodeInfo.ACTION_LONG_CLICK, \"Like\");\n\n        info.addAction(click);\n        info.addAction(longClick);\n    }\n\n}\n\n...\n\n// where you create your View / where you bind it if your custom labels are dependent on the data being bound\nViewCompat.setAccessibilityDelegate(yourTweetView, new BetterLabelsDelegate()); And here's the result: There were a few bugs around this prior to TalkBack v5.0.2: overriding the label for click and long-click would also remove the gesture portion of the text the usage hint would only be read aloud if the View was selected using a \"next\" or \"previous\" gesture but these have since been fixed. Give it a go and, as always, if you have any feedback or questions, feel free to reach out !", "date": "2016-10-13"},
{"website": "Novoda", "title": "Designing something S.O.L.I.D", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/designing-something-solid/", "abstract": "Novoda is an advocate for clean code and  actively using pair-programming, collaboration and code reviews. The SOLID principles are fundamental to explaining a part of what clean code is, but sometimes explaining what SOLID means is a task in itself. This blog posts shows how we tried to re-examine SOLID visually to help all our developers embrace and learn from SOLID design. Before we begin to visualise the SOLID principles, we should clear what they stand for. Let's start off with the basics. Here it is in a Wikipedia nutshell: Initial Acronym Stands for Concept S SRP Single responsibility principle a class should have only a single responsibility (i.e. only one potential change in the software's specification should be able to affect the specification of the class) O OCP Open/closed principle “software entities … should be open for extension, but closed for modification.” L LSP Liskov substitution principle “objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program.” I ISP Interface segregation principle “many client-specific interfaces are better than one general-purpose interface.” D DIP Dependency inversion principle one should “Depend upon Abstractions. Do not depend upon concretions.” The Wikipedia excerpt gives you an idea of what the principles stand for but if you didn't know them you would have to go do some further reading to understand them in depth. Furthermore, as a refresher the Wikipedia page is not particularly interesting or stimulating and could be considered verbose. One of our amazing designers - Qi Qu - undertook the challenge of re-imagining these principles visually to help drive understanding. The idea behind our imagery is to show visually what the principles say you should not be doing. There is psychology behind this idea. People tend to remember lucid, stupid or silly imagery much more readily than a bland static image. 1 Single Responsibility Principle Don't have your class, method, object, or concept doing more than one thing. Imagine a single chef running the entire restaurant, being responsible for taking orders, cooking all the meals, and washing up. Eventually the chef would make a mistake. It is too complex a system for one actor to do everything correctly every time. Your code should be written with this in mind, aiming to do one simple task well. Open Closed Principle Don't program in such a way that, in order to add new behaviour, you need to write additional code in lower layers of your code-base. Imagine each time you wanted to try on new shoes you had to cut off your own foot to get the shoe off, put the shoe on the bloodied foot then surgically re-attach your foot with the new shoe in place. Layers of code - like layers of clothing - should be independent and not rely on one another. Liskov Substitution Principle Don't implement interfaces in a way that breaks expected semantic behaviour. For example, an interface for sunglasses would have fairly simple rules like: shields from the sun; attaches to a face. Implementing the sunglasses interface with suntan lotion would seem to make sense: it shields from the sun and attaches to the face. But semantically the expected behaviour is different enough to cause behavioural problems - in this case, by stinging your eyes! Interface Segregation Principle Don't create one all encompassing interface with all the methods you might ever need on it. As an example, consider an interface for charging your phone - a simple power socket. The problem is that a power socket can come in many different variations: by country, device, voltage, size etc. Designing an interface which tried to accommodate all of these would not only be very confusing but could even end up with you breaking your device. The simplest possibly interface to get the job done - arguably USB - is likely the best. Dependency Inversion Principle Don't let your components depend on specific implementations. For example, having your lamp wired directly into your house mains the lamp 'knows of' the specific electric system and how its individual wires connect to it. Instead it's better for the lamp to expose a plug and depend on a socket - an abstraction of an electrical provider - knowing nothing of the house. This means it's easier to maintain, move, or re-use elsewhere, just as your code should be. Creating these images was difficult but fun and required a lot of discussion, collaboration and determination. The thought process involved and the iterations of the ideas was a long time in the making. Even after this, we still find ourselves discussing them internally and arguing about how best to represent the principles. Nevertheless, we hope at least some of these pictures help you remember SOLID. Interested in getting these posters for your office? We'll have some available in our competition at our stand at Droidcon London 2016 this year. If you want to purchase your own head but can't make it to Droidcon, you can head over to our redbubble page to grab them from October 31st. Posters / Cushions & More", "date": "2016-10-18"},
{"website": "Novoda", "title": "Approaching Outside-in TDD on Android (Pt. 3)", "author": ["Christian Panadero"], "link": "https://blog.novoda.com/approaching-outside-in-tdd-on-android-pt-3/", "abstract": "In the previous post , we wrote the acceptance test as a first step and started creating the most external classes of our implementation. In this post, we will finish implementing the system, and will summarize what we have learnt during the process. To finish the BankAccount class, we need to implement its last public method, showStatement . Let's dive into the next iteration of the inner loop cycle. ● Red - We created a StatementFormatter to format the statement lines. We considered the statement to be a domain concept important enough to have its own class. It acts as a first class collection around the statement lines that attracts behaviour related to the statement lines.\nThe failing test ensures that when we show an account statement, the view is called with the appropriate statement lines. ● Green - The implementation is quite simple in this case. The BankAccount just needs to create a new statement with the transactions from the repository, pass it to the formatter to format and show the formatted statement using the view. ● Refactor - Nothing to refactor here. As a rule of thumb, once a class is done, the next step is to run the acceptance test to check the progress and to know what is the next collaborator to implement. If we executed the acceptance test at this point, we would see that the TransactionRepository throws an UnsupportedOperationException. As we stated before, throwing an exception in the methods that we have not implemented yet will guide us through the feature implementation and will point us to the next collaborator to implement. It is quite important, as doing so, we have greater control over the current feature progress. We just need to follow the exceptions until the feature is fully implemented. ● Red - TransactionRepository is a leaf node in the tree of collaborators. Meaning that, it does not collaborate with other classes. Leaf nodes can not be tested using mocks as there is no collaboration, so we need to flip to the classicist style of TDD and test through state rather than through behaviour. TransactionRepository is going to be implemented as an in-memory storage, so we decided to test it by storing a transaction and checking that the list returned in the transactions method contains the previously stored transaction. ● Green - To make the test pass we just need to return the current list of transactions, as best practice we recommend to return an immutable list to protect it for clients that could try to modify it. ● Refactor - Some readability improvements in TransactionRepository test. Remember that readability is important both in production and test code. Now that TransactionRepository is done, if we execute the acceptance test, StatementFormatter throws an UnsupportedOperationException. That means that it should be the next one to be implemented. ● Red - StatementFormatter is responsible for creating ViewStatementLines from a given Statement, and sorting them in reverse chronological order. In order to do so, the StatementFormatter has to take the StatementLines from the statement, map them to ViewStatementLines and sort them. This is tested by stubing the StatementLines that the Statement returns and asserting that the output of the format method contains the expected ViewStatementLine information and order. Note that we are not going to fulfil the statementLines method yet. Instead we mock it and throw an UnsupportedOperationException accordingly. We will jump there when needed. ● Green - As described before, the production code for the StatementFormatter takes the lines from the statement, sort them in inverse chronological order and map them them to ViewStatementLines. ● Refactor - We extracted a method that maps a line in the production code and do some clean up in the test code. Once again, we run the acceptance test to check the progress and now it guide us to the lines method in the Statement class (The one that we just mocked in the previous inner loop to implement the StatementFormatter). Let's get rid of the exception and jump into the next inner loop. ● Red - Statement has to map every transaction to a StatementLine that contains amount, date and running balance.\nAs in the case of the TransactionRepository, Statement is a leaf node. Therefore, it is tested using the classicist approach by asserting the state of the StatementLines returned in the lines method. ● Green - The implementation just map all Statement's transactions to StatementLines, calculating their current balance. ● Refactor - We extracted some methods in the production code and improved test readability. We are now done with the Statement class. Let's run the acceptance test again to find out that the next class that needs to be implemented is the show method of the ShowStatementActivity . ● Red - During most part of the implementation we were not dealing with any Android code, but at this level we moved back to the to UI layer and, as the code is running inside an Activity, we have to write the unit test using Espresso. Here we need to test that once the show method is called, the RecyclerView contains a list of rows representing the information contained in the given ViewStatementLines. ● Green - In order to show the required rows in the RecyclerView we need to make some steps. First we need to create a RecyclerView.Adapter that holds the dataset and creates the required ViewHolder associated with every row contained in the RecyclerView. ● Refactor - Improves test readability. If we execute the acceptance test at this point, we can see that it passes. We can consider the feature DONE as it passes the given acceptance criteria in an automated fashion. Conclusions To wrap-up the series, we are going to summarize what we have learnt and make some comments about our implementation. We would like to mention that we have used Espresso to assert the state of the views in Android, but you could use the testing framework that works best for you, i.e. Cucumber. That being said, we have found that outside-in is an approach that requires good design skills and that we need to have a good idea of how the design of the system will look like beforehand. It usually means that we have built a similar feature or system in the past and therefore we have an overall idea of what we need and where we are led to. PROS The public interface of every class is always designed to serve an existing client. As we work from the outside in, clients are implemented first, and they define the public API of the classes they collaborate with. The resulting design should read better, adhere well to the “tell don't ask” principle, and tend to avoid feature envy and anaemic domain models. Benefits of acceptance tests included in the outside-in flow: Automated requirements validation. Offer a way to check the progress of a feature at any given time. Offer a way to validate our system in an end-to-end fashion. Are written in a non-technical language, therefore can be understood by the business. The addition of acceptance + unit test offers a complete validation that our system works and fulfils the requirements as expected. Outside-in offers a common workflow for developers CONS Because outside-in focus on how the different parts collaborate, the implementation details of the design are part of the tests. As a result, the tests are coupled to the implementation. This situation introduce a bigger risk to refactoring as the tests have to change when the design of the system changes. Features are guided by acceptance tests. That is fine, but remember that acceptance tests tend to be slower than a unit test and will make the test suite slower as we add more features. It is a trade-off that we need to consider. A possible approach is to include acceptance tests only for the features that are critical to the business. It is usually harder to write behaviour test than state tests These series have now come to an end with this third post. We hope you enjoyed and learnt something.", "date": "2016-10-11"},
{"website": "Novoda", "title": "Show us what you got in our Design competition", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/show-us-what-you-got/", "abstract": "A great application is not just an application that has subtle but immediately recognisable branding. It’s not just an app that has a great visual design. A great app makes it easy and immediately understandable to its users, showcasing how to get the most out of it. It captures their attention, delights them. Using motion wisely and creatively is key to that. And we want to see what you and your team have built . We are looking for the work you are most proud of , that has the sort of extra attention to detail that creates the highest quality experience for your users using motion. Whether you are a designer or a developer , we’d love to see what you’ve got. Our team of all-star jurors is going to pick the two best entries. We have the best prize possible for design geeks: two one on one sessions with super skilled designers. One session is going to be hosted by Nick Bearman , Motion Designer at Google; the other will be hosted by Novoda’s Head of Design, Dave Clements . Runner-ups will receive an Amazon Echo , so they can explore the exciting possibilities offered by its new voice-based interactions. To enter the contest and get more informations, simply sign up on http://bit.ly/droidcon-design by October 17th, 23:59:59.999. We'll announce the winners at Droidcon London.", "date": "2016-10-11"},
{"website": "Novoda", "title": "Hacktoberfest", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/hacktoberfest/", "abstract": "Open source is a big part of what we do at Novoda. A lot of our projects lean on the great work of others, but we also enjoy giving back. Whenever possible, we like to take time and spin out some cool and useful stuff, so others can take advantage of what we've gone through already and build on top of it. No dev is an island, entire of itself, every dev should be a part of the community, a part of the whole. Over the years, we've released different types of content, from design freebies to libraries and whole applications, with some build plugins in-between. Whilst we strongly encourage our collaborators to contribute to our, and others', open source endeavours, we've always been open to and welcoming of external contributions too. We feel this is a great way to manage open source and the community has much more to gain by having as many interested parties involved as possible. If a project is washed away, we're all the less. These are only some of our dear contributors. Your avatar would look great here, wouldn't it? If you head over to novoda.github.io for a quick glance, you’ll find a nice overview of most of our public repositories. Go on, take a look. See anything you fancy? Tried something and found an issue? Add it! If not, you’re more than free to pick any issue and get started with it. Plus, from the October 1st to the 31st, Github is running Hacktoberfest ! ![](/content/images/2016/09/vectorpaint.png) In case you haven’t heard, or read, about it already, Github and DigitalOcean are promoting contributions to open source projects and you get to win a cool t-shirt! By contributing with as few as four pull-requests you’re all set, but why stop there, right? All contributions strengthen us, because we’re all involved in the community. We’ve taken the liberty to tag some of the existing issues on our repositories so you can start right ahead. Check them out ! Here you can find a review of last year's event.", "date": "2016-09-30"},
{"website": "Novoda", "title": "5 easy ways to run successful community events", "author": ["Oana Botez"], "link": "https://blog.novoda.com/5-easy-ways-to-run-successful-community-events/", "abstract": "For the past five years I’ve had the opportunity of running and being part of hundreds of tech events. Everything from workshops and meetups to training courses and large scale conferences. This is what I learned from it and my advice to anyone trying to find their feet in the industry. In the beginning I was thrown in at the deep end and, I have to be honest, at times it has been tough but I wouldn't change a thing. It has been a great journey full of steep learning curves and enjoyable experiences. I was never fully alone in my learning so always remember: 1. Teamwork - your team is your most valuable asset In events, you succeed or fail as a team. Running events is never a one person job as it’s all about teamwork. Take the time to get to know everyone around you, understand their strengths and weaknesses, ask for feedback and always look for ways to improve the way you work together. Learn how and when to delegate and make the best use of all the resources available. 2. Communication, communication, communication... Good communication is the key to running a successful event. From dealing with your internal team to dealing with external speakers, sponsors and suppliers. Always make sure everyone is on the same page. Be clear and explain thoroughly keeping in mind that people have different backgrounds. Just because it makes sense to you it doesn’t necessarily mean it makes sense to everyone. 3. Listen to your community and become part of it. Running tech community events and being part of creating and growing a tech community is a very exciting experience. Developer or not, once you feel part of the community you will be able to better understand the synergy and the needs of the group. Don’t be afraid to ask questions because your community is the best source of information. 4. Things always go wrong, don’t panic there is always a solution! It doesn’t matter how much you plan inevitably something will always go wrong! You need to be prepared to think outside the box and come up with solutions on your feet. Remember that problem solving can be fun. However in order to do so, be organised, plan ahead and make sure you give yourself enough time prior to the event to deal with all the last minute bugs. 5. Agile is not only a buzzword We are living in a fast paced world where things are constantly changing so you must be prepared to adapt. The fact that things have always been done in a certain way is not a reason to carry on the same path. Try to understand the reasoning behind it and always look for ways of  improving and optimising the process. Change is good. In addition, after an event always take the time to discuss what went well and what could be done better. Feedback and constructive criticism are essential factors in helping to ensure improved future events! Running events can be great fun. For me, it’s like building with lego where you’ve have all these distinct pieces and you need to make them fit together. On top of that, the tech community is a very friendly and welcoming environment. You always get the chance to learn something new so don’t forget to make the most out of it and enjoy yourself.", "date": "2016-09-29"},
{"website": "Novoda", "title": "Top 3 features you should implement on Android, Apple & Fire TV", "author": ["Hawazine"], "link": "https://blog.novoda.com/3-top-features-you-should-implement-on-android-apple-fire-tv/", "abstract": "Every platform aims to create a great user experience for all its different form factors. A great app designed for smartphones doesn’t always give a great experience on other types of device. Each of the major Smart TV platform attempts to offer a unique user experience, and here I will outline the features you should be interested in. Smart TV platforms tend to change quickly among the big TV brands. By 2019, more than 50% of TV households in Japan, the US, the UK, France and Germany will have Smart TVs, according to IHS Markit . When distributors and TV industries combine their efforts to improve the viewer experience, it deserves to be looked at closer. Let’s have a look at the most interesting features of Amazon Fire TV , Apple TV and Android TV that will help you to build smart TV app. Discovery and Launch (DIAL) Dial (Discovery and Launch) is an open protocol that enables Fire TV apps to be discovered and launched from a second screen device. Typically, the first screen is your Fire TV and the second screen can be a phone or a tablet for example. Both Fire TV and the second screen device must be on the same network. Netflix , YouTube , Samsung , Sony have developed DIAL in order to compete with Apple’s AirPlay. Most of smart televisions and different game consoles like the PS4 , the Xbox One , Roku support DIAL. But keep in mind that every platform that uses DIAL can have their own version of DIAL, and they are not all compatible. Using DIAL  technology or Google Cast is not the same. Google Cast used the DIAL protocol in its first versions and then decided to use its own protocol called mDNS (multicast Domain Name System). With this functionality, a user can start watching a video on your app on a smartphone and then watch the rest of the video on the Amazon Fire TV. There is no Java implementation involved for using DIAL on your app but we do need to modify the app’s manifest and resources to indicate support for DIAL and to accept launch intents. You can decide to use as well Amazon Fling which is the equivalent of Google Cast. Amazon provide an SDK that allows videos, audios and images to be sent from a device to a Fire TV. More on this: DIAL Integration documentation and Amazon Fling sdk Fire TV Catalog integration process The Fire TV catalog integration allows your content to be included in the results of a search performed from the Fire TV home screen. First you will need to create a catalog file. A catalog file is an XML file that provide details about the content (films, series, etc..) that you have in your app. Catalog  examples After creating your catalog file you will have to upload it. The uploading process is independent from your app, it will be part of your backend. The validation of your catalog file is needed, for this you can use XML validation tools in order to avoid broken or missing tags. Then you have to setting up your AWS account with the help of Amazon. Amazon will configure the AWS S3 bucket that you will use for uploading your catalog file 1 . More on this: uploading process . In addition to uploading your catalog you will have to modify your app to be used with Amazon Fire TV’s home screen launcher. You will have to create new Android Intents, which your app will need to broadcast and to modify your Manifest . More on this: here . Imagine you have a movie in your app that you would like to be discovered and launched from the Fire TV home screen. An example of a catalog file with one item could be: <?xml version=\"1.0\" encoding=\"utf-8\" ?>\n    <Catalog xmlns=\"http://www.amazon.com/FireTv/2016-09-19/ingestion\" version=\"FireTv-v1.3\" >\n        <Partner>Everything Ever Made Filmworks</Partner>\n        <Works>\n            <Movie>\n                <ID>MV-12345</ID>\n                <Title locale=\"en-US\">Your movie title</Title>\n                <Offers>\n                    <SubscriptionOffer>\n                        <Regions>\n                            <Country>US</Country>\n                        </Regions>\n                    </SubscriptionOffer>\n                </Offers>\n            </Movie>\n        </Works>\n    </Catalog> It’s recommended to provide as much details as possible about each item inside <works> for improving the chance to retrieve successfully your content when a user performs a search. More on this: Integrating your catalog with Amazon Fire TV . Top Shelf The Top Shelf is in the Apple TV Home screen. As a user you can choose five TV apps that will appear in the top row of your home screen. When a user selects one of the five app icons in the top row, the top shelf area will then showcase content related to the chosen app. As a developer you can decide how the showcase of your app will be visualized. It can be through a static or dynamic content. For a static content you will use a static image. If you want to display your content in a more sophisticated way you can use Dynamic Content Layouts provided by the TVServices framework . The Dynamic Content can be used for giving a preview of the series in your app. Dynamic Content Layouts includes the Sectioned Content Row and Scrolling Inset Banner layout. The Section Content Row displays a list of section, each section has its own title and can have one or more child content items. The Scrolling Inset Banner layout is a list of large picture displayed automatically by your apple TV: It’s an interesting way, for enhancing your app’s content and let people know more about it, when your app is in focus. TV Markup Language (TVML) TVML is a language provided by Apple to develop Apple TV apps. TVML is a form of XML that describes how elements are displayed in an Apple TV app. Apple provides a number of TVML templates for building an App. The TVMLKit JavaScript (TVMLKit JS) and the TVMLKit framework are the tools used for developing TVML apps. TVML provides an easier, faster way to build standard applications with common features/user interface. For example, you can easily embed videos directly into TVML elements or set videos to play immediately upon page presentation or when an element comes into focus. TVML let you create interactive video overlays that contain focusable elements. TVML is a good option for beginners, prototype applications or even basic applications, but native applications provide a lot more control over how the application looks, works and feels. New features for tvOS 10 sdk Just a reminder, tvOS is the operating system that run in Apple TV, it inherits of a lot of frameworks from iOS but not all. Here is two key developer-related features added in tvOS 10 sdk that are already existing in iOS frameworks. HTTP Live Streaming (HLS) is used to send live and on‐demand audio and video to devices like the iPad and iPhone to Apple TV. HLS is an HTTP-based media streaming communications protocol implemented by Apple, and is similar in functionality to the widely-supported MPEG-DASH protocol. The ReplayKit framework provides the ability to record video and audio within an app. TV Channels Apps can create their own channels and specify data according to their TV channels live  programs. This looks similar to what modern TVs (prior to SmartTVs) call guide. A list of channels is displayed and the user can have a look at what’s next, how long the current program lasts information regarding the program they are currently watching. More info: Building TV Channels Picture-in-picture Users have the ability to watch video playing on a pinned picture in the top corner of the screen, whilst freely navigating around their TV apps and menus. Here is a Picture-in-picture video visible in a corner of the screen while the user browses content on the main screen: This feature is only available in Android 7.0 Nougat and up. More on this: Picture-in-Picture TV Recording Users are able to record live TV sessions. Imagine the scenario that you want to watch your favorite show but it is always airing while you are at work. You can schedule a timer at when the show is aired and your Android app will wake up at that moment and record the show using the API linked. You can view the recorded sessions within your app. The recordings are stored into the device. More info: TV Recording Conclusion Platforms like Amazon Fire TV, Apple TV and Android TV offer interesting features that help developers build amazing smart TV apps. In this blog post we have cited some of the key developer-related features available. You can find more, on each platform’s documentation. Keep in mind that Smart TVs are in constant evolution. More features for Smart TV apps are expected to always improve user experience! 1 : Note that Amazon expects your catalog to be uploaded at least once per week, regardless of whether the catalog has changed.", "date": "2016-09-20"},
{"website": "Novoda", "title": "Asking for app feedback — the effective way", "author": ["Denis Akan"], "link": "https://blog.novoda.com/asking-for-app-feedback-the-effective-way/", "abstract": "We’ve all been there: In the midst of using an app a dialogue pops up asking us to stop everything we are doing and to leave a review on the Play Store. This interruption is not only annoying but also not very effective as most people will simply dismiss the dialogue and continue what they are doing. In this post we’ll explain how we implemented a prompt for feedback process for The Times & The Sunday Times Android app that helped boosting the average user rating from 2.8 stars to 4.3 stars, increased the average number of ratings per day by tenfold, and helped create a dialogue with unhappy users. The importance of app ratings Before we dive into how it was done, let’s talk a bit about the why. Simply put, ratings are important because potential users view them as a way to indicate if an app is good or bad. Users are more likely to consider downloading a higher rated app than one with bad ratings. [1] That aside, app ratings also influence the discoverability of an app. The most common way to discover apps - after general browsing through the store or referrals by friends - is by browsing the “top rated” and “most popular” sections of app stores. [2] Thus an app with 1 or 2 star-ratings is potentially losing out on new users. The problem We worked with News UK to re-launch the new The Times & The Sunday Times app as an update instead of as a new, standalone app. This meant upon release we had an existing user base but also inherited a play store rating of 2.79 stars based on ~1,500 ratings. So our first challenge was to overcome this historic rating and improve it. With over 30,000 monthly active users and an average of six read articles per session,  we knew that customers used the app regularly. Surveys and interviews also highlighted that the customers enjoyed using the app.  Despite this positive feedback, we hadn’t received the ratings we hoped for. The average rating remained stable with only a slow upwards trend. Our goals We wanted to achieve these three overarching goals: Improve The Times & The Sunday Times overall rating Boost the average number of daily/monthly ratings Gather customer feedback and start a dialogue with dissatisfied customers A two-step prompt The minimal changes of the average rating of the app was consistent with the general notion that ratings are usually left by a few vocal critics and a few vocal fans. This state leaves out the ‘silent majority’ of customers who use the app but don’t rate or give feedback. We set ourselves the goal of targeting active customers to rate the app or give us feedback for improvement. At the same time we wanted to learn how our frequently active users feel about using the app. After some consideration, we opted to introduce a two-step prompt: In the first step we ask the customer if they enjoy using the app. If they do, in the next step we ask if they're happy to leave a review in the Play Store. If the customer is not enjoying using the app, we instead ask they can give us some feedback through our in-app customer service tool. A dark pattern? It can be argued that directing users who are having negative experience to a funnel other than the Play Store or AppStore constitutes a 'dark pattern' . The reasoning behind this is that it deflects only potential negative ratings, leading to an uptick in ratings by default and at the cost of an 'honest' reflection of user feedback. It's a solid argument, but we believe that this is not the case if the app is directing you to a better customer support avenue than the Play or App Store can provide. To give an example: if a user is having a negative app experience because they believe the app is missing a feature they've simply not noticed, they may leave a 1 star review simply for lack of other options. While it's possible to respond to reviews on the Play Store, these often aren't seen by users nor is there a guarantee they'll update their rating once they realise their mistake. With our pattern, this user could be directed to a customer support representative able to quickly point out the feature or to an FAQ covering it. This, in our opinion, is a faster, better support experience for the user and an easier, more manageable support experience for the maintainers of the app. With the rise of direct, in-app customer support tools such as Intercom , it can also be very easy to implement. And, of course, users are still able of leaving negative reviews in the Play/App Store at any time. What we did Instead of integrating a dialogue which interrupts the customer while using the app, we thought about how customers are using the app, what constitutes an engaged user, and how we can communicate with them without being interrupting their experience. The Times & The Sunday Times app is used by subscribers to consume news, magazine articles, and videos. Knowing what our customers use the app for, we set out to define what “engaged” means for us in this context. This would help us know from whom to solicit feedback. We decided to use two triggers that would classify a user as engaged. Trigger 1 is activated after the customer opens the app for a certain number of days in a row. Trigger 2 requires the customer to save 5 articles in the app. After we'd defined these triggers, we had to pick a good moment to ask users who'd triggered them for their feedback. Our assumption was that customers are more likely to leave feedback when they’ve completed the primary action for which they use the app. In this moment they should be in the best possible mental state to supply a rating: happy that they’ve been able to complete their task, uninterrupted, and open to new engagement with the app. It was also vital that the prompt for feedback should not interrupt the user. No-one has ever enjoyed an unsolicited pop-up - most people will just dismiss them. Our hypothesis was that a styled inline prompt asking whether the user was enjoying the app  would lead happy users toward responding and ultimately toward leaving a review. We also decided that asking the user whether they were enjoying the app, rather than directly asking them to give a rating, would soften the request. This would also give them an opportunity to reach out to us with our in-app customer service tool if they weren’t enjoying their experience. With the ‘silent majority’ in mindset we mentioned above, we believed they would feel relaxed and willing to leave us their otherwise unheard feedback. The results The impact of this prompt was fantastic. Five days after we'd release (when our first trigger started to be activated) we saw a huge spike in ratings. Even after this initial spike, the average number of ratings we received remained at around 7x-10x higher than previously. The average rating we received also increased from 2.9 stars to 4.4 stars, while the number of 1 & 2 stars ratings dropped to an all-time low. If you'd like to read a little more about our work on the Times & Sunday Times app, check out our case-study ! Quick facts: App feedback prompt 10x more ratings per day Boosted average rating by 1.5 stars Halved 1 and 2 ratings** The Mobile Marketer's Guide to App Store Ratings & Reviews: http://cdn2.hubspot.net/hubfs/232559/The_Mobile_Marketers_Guide_To_App_Store_Ratings_and_Reviews.pdf?t=1470268162234 ↩︎ Ibid. ↩︎", "date": "2016-09-27"},
{"website": "Novoda", "title": "My week with a tomato", "author": ["Ryan Feline"], "link": "https://blog.novoda.com/my-week-with-a-tomato/", "abstract": "This post documents my experience with the Pomodoro time management technique over a 5 day period. The Pomodoro technique was invented by Francesco Cirillo , who named the technique after a tomato-shaped timer he used to track his work as a university student. A deceptively simple Time Management Technique, Pomodoro, teaches you to work with time instead of against it. Pick a task, small or large, and work solely on that for a single Pomodoro, normally 25-minutes. Simple. After 25-minutes? You stop. Put a little tick against that task. Congratulations you've spent an entire Pomodoro on a single task, without interruptions too! What do you do now? You take a break. Essential to the Pomodoro technique is the notion that taking short regular breaks eliminates burnout and reduces stress. After a certain number of Pomodoros, take a slightly longer break and give your brain a moment to assimilate all that you have done. Conclusion: What I found in the tomato Distractions are key As the old adage states “if you fail to plan, you plan to fail”, with that in mind, list your top distractions and come up with a mitigation strategy for them. I cannot stress how important this single step is. It forms the basis of having a great experience with the Pomodoro technique. Seriously, take a break Breaks are good. You need to eat. You need to drink. You need to have conversations. A happy employee is a productive employee. If you get headaches or a stiff back from sitting in one spot all day trying to complete a task, who do you think you are helping? Your client? No. By neglecting your health you are actually doing more harm than good. Breaks allow you to assimilate information and being less stressed in your job will make you more efficient. But, small breaks every 25 minutes?! That’s a lot of breaks! Yes….but your downtime is now scheduled for set periods in the day rather than occurring at random throughout the day like it was before. Hey, I do stuff! What have you done today? It turns out, I do a lot more than I ever thought I did. Using “Pomodoro Time” to track tasks gives visibility to the tasks completed over a given timespan. I only wrote this blog because I regularly assigned Pomodoros for it! App or not to app In essence, the Pomodoro technique can be achieved simply with a timer and paper for recording tasks. I choose to augment my experience with the “Pomodoro Time” app for the timing accuracy and reporting capabilities offered. In future, I would actually like to improve on this by using an Android wearable device with a Pomodoro app to move the technique away from the office environment. I even know a guy that has a wearable app, Daniele Bonaldo ! Tomato Log Below follows a slightly more verbose log of my usage of Pomodoro over a five day period. For timing - Pomodoro Time I found “ Pomodoro Time ”, a free app on the AppStore for tracking time. There is also a paid version that removes ads, provides access to syncing across the Apple ecosystem and allows exporting to CSV. For my trial of the Pomodoro technique, I decided that the free version would be more than sufficient. To kick off my time with the tomato I created a list of todos that I hoped to complete by the end of the day, nicely facilitated by the app. I decided to go with a 25-minute Pomodoro followed by a 5-minute break, I reasoned this would give me enough time to take a quick peak at Slack and emails to determine if I needed to change priorities in the next session or, you know, have an actual break. (Time for a break!) Day 1 At Novoda we have very active Slack rooms, so every few minutes I found my attention wondering to the jumping Slack icon and the hidden Slack rooms within. And then there is the email and the constant, niggling feeling, that PRs need to be checked and emails replied to. All of these distractions culminated in the first day feeling something like this: Tasks according to Pomodoro Time: Tomato blog x2 Project task x5 Review & address comments on PRs x5 Release library x3 A more accurate representation: Tomato blog x2 Project task x12 Review & address comments on PRs x18 Release library x8 Recheck Slack for the thousandth time x20 Day 2 Pomodoro Time can be used track the number of cycles spent on a particular task. Day two was a veritable success when compared with day one, which I am chalking down to the “distraction mitigation strategy”. Tackle the distractions List your distractions and try to come up with an action plan on how to reduce the risk of this distraction affecting you. Here are my distractions: Colleagues / Slack / Email Impromptu discussions with colleagues is a given in any work environment. To mitigate this, the use of the Pomodoro technique needs to be clearly communicated to the people with whom you work. For myself, this was clearly communicated in the company and project standups for the most visibility. Scheduling several communication Pomodoros throughout the day can be used to pigeonhole communication through Slack and email, allowing other Pomodoros to be distraction free. Text Messaging / Phone calls Everyone receives text messages or phone calls at work, how to deal with these without breaking concentration is hard. I personally use Pushbullet to receive live notifications to my MacBook so I can easily decide if a phone call is worth answering, without breaking concentration by switching devices. Mania, the dog She'll sleep, sleep some more, and then sleep again. She's pretty content until she knows it is my lunchtime and of course, her lunchtime. Day 3 Following the success of the “distraction mitigation strategy” from day 2, I thought I would expand on the process by “putting distractions at arms length”. Tackle the distractions part II Here is my “arms length” approach to tackling distractions. I created three virtual desktops running the following: Android Studio Fullscreen. Try distraction free mode. Browser for looking up documentation / tackling development issues, also in fullscreen. Communication / social desktop not fullscreen. Fullscreen applications become a dedicated workspace, free from distractions. To reach any communication channels I need to actively switch workspaces. Too much effort, I’ll just finish the task. A bonus would be to add a nice warning background on a workspace between your communication channels and your workspaces to hold you back from the distractions. Day 4 Client office. No distractions. Mitigation strategy is working. It is OK to communicate. In the confines of the ongoing task. When looking at PRs I’ll frequent the Slack channel for the given project to ask questions. Day 5 Headphones, plain and simple. Something about wearing headphones at the Novoda office that screams \"LEAVE ME ALONE!\" that, or \"pairing, please come back later\" either way it worked. No distractions!", "date": "2016-09-14"},
{"website": "Novoda", "title": "Approaching Outside-in TDD on Android (Pt. 2)", "author": ["Christian Panadero"], "link": "https://blog.novoda.com/approaching-outside-in-tdd-on-android-ii/", "abstract": "In the previous post , we introduced the Bank kata. We explained how we are going to implement it in Android, reviewed the different kinds of tests that we use in Outside-in and transformed a user story into a bunch of acceptance criteria. In this second post, we will focus more on the practical side. We will show how to build the acceptance test from the acceptance criteria, how acceptance tests and unit tests fit in the double loop of TDD and how we rely on them to guide us through the implementation. We will be explaining everything following the flow of Outside-In TDD in a step by step fashion. Outer loop - Acceptance test As we discussed early on, we have to show the statement lines that compose an account statement. Concepts such as Statement , Statement line and Transactions are part of our domain. We will follow them throughout the post and code. To show the statement lines we decided to use a Recyclerview . Following the acceptance criteria each statement line has to be formatted as follows: 'Date' - 'Amount' - 'Running balance' It is worth mentioning that many people leave the UI out of the scope of the acceptance tests. In our case, we consider that in most mobile applications the UI is critical and therefore should generally be included. Whether you do it or not is your decision, however, we will show how it can be done. To assert the state of the view we are going to use Espresso, the official Android UI testing framework. Espresso does not offer a way to assert the state of each row in a RecyclerView out of the box, so we need to use the following snippet to be able to do so, RecyclerViewInteraction . We consider that this piece of code is part of our testing framework. That is why it is already included as part of the initial steps. As we stated in the previous post, Android is the responsible for instantiating Activities . We can’t instantiate them manually. Bearing in mind that dependency injection through the constructor is not an option here, we had to come up with an alternative mechanism to instantiate custom dependencies for tests. For simplicity’s sake, we are not going to use any dependency injection framework. Instead, we are going to use the Service locator pattern. Using this pattern and a static method setInstance we will be able to provide custom dependencies to the activity without using constructor dependency injection. Before asserting that the UI is showing the expected information, we need to define the layout that it is going to use to show it. In Android every view has an ID. We will need those IDs to reference the views in the acceptance test to assert that it is showing the correct information. Having said that, let’s focus on the important stuff. Following the acceptance criteria, defined in the previous post , we have to make two deposits and one withdrawal. Notice that withdrawals are transactions with negative amounts. As in Outside-In design happen in the red phase , we have to design the skeleton of the classes that we know are needed at this point. We do not need to know and define the whole tree of collaborators, only the ones that we know that are needed. In contrast with classicist TDD, where everything emerges from the tests, in outside-in we need to do some design up front. At this point we know we need the following collaborators: TransactionRepository - We know that we need some storage to save transactions. We decided to follow the Repository pattern to hold an in-memory implementation. ViewStatementLine - We are going to use a RecyclerView that comes with an adapter. The adapter needs to hold a collection of a type that contains the information required for each row. ViewStatementLine is that type. It does not have any business logic and it is mapped 1:1 to the information shown in the UI. StatementFormatter - This collaborator will convert the domain objects to the format needed in the UI. Date, amounts and text formating is the kind of logic that has to be included here. ServiceLocator - As mentioned before , this collaborator provides custom BankAccount instances to the Activity. Clock - Time… time has a random factor and is outside of our control. To gain control over it, we have to encapsulate time randomness to have control over it during testing. We will see the collaborators in more detail later on, while we implement them. Once we have finished writing the acceptance test and we see it failing for the right reason (The view is not showing the statement), we will dive into the inner loop and start writing the individual pieces required for the feature. Inner loop - Unit test The first collaborator to implement is the one in the outer layer of the system, in this case, the ShowStatementActivity . ● c22ab01 - Let's start writing a failing test that will lead us to create the UI to show the statement. Here we are going to use a vertical RecyclerView. ● 8d6122f - Once we have the failing test, we have to make it pass by loading the activity layout, holding a reference to the RecyclerView and setting a LinearLayoutManager with a vertical orientation. ● 08a593 - We started by including all the code inside the onCreate method. In the refactor phase, we decided to extract a initView method to wrap view initialisation code. At this point, the RecyclerView is initialised and ready to hold the statement rows, but we still need to show the account statement . Let's do it. ● 8520fb - The test has to verify that when the activity is launched, the view gets attached to the account and the BankAccount showStatement method is called. As mentioned before, we do not have control over the activity instantiation so we need to use the ServiceLocator to provide a test BankAccount for the activity. ● fcb403 - To make the test pass we need to obtain the account reference from the ServiceLocator, attach the ShowStatementView (activity) to the account instance, and call its showStatement method. ● bd961c - We added all the code inside the onCreate method. Following the same reasoning as in the previous refactor, we extract a initAccount method that wraps bank account initialisation code. The activity is now complete. The next step would be to implement the next collaborator, the ServiceLocator . The ServiceLocator job is to provide BankAccount instances to its clients. Let's implement it doing a second iteration of the inner loop cycle. ● 7fb17b - The test for the ServiceLocator has to assert that the provided BankAccount is not null. ● 8ce146 - In this case is just as simple as returning a new BankAccunt instance. ● There is no refactor to do in this step, so let's move to the next iteration. Now that the outer layer is done, it is time to move to the next level of abstraction and implement the BankAccount . Among the BankAccount public methods we need to decide which to implement first - deposit() , withdraw() and showStatement() . We recommend following the order in which the methods are used in the acceptance test. We will show how we can ensure the order later on. In this case, the method that is used first is deposit() . Let's implement it by going through another iteration of the inner loop cycle. ● a1f870 - The behaviour of this method is to create a transaction, using the amount parameter and the current time, storing it in some kind of persistence. Remember that the acceptance criteria explicitly states that the statement should be sorted in reverse chronological order and that each line has to contain the transaction date. To be able to test that the transaction was created with the current date we need to stub the Clock the same way that was done for the acceptance test. Later on, we need to verify that the repository stores the correct transaction. One last point to make is that the TransactionRepository store method is now throwing an UnsupportedOperationException . As we are mocking it in the current test, we do not care what is inside the production code of the TransactionRepository. We will explain later on how throwing exceptions is going to guide us showing what we need to implement next. ● 1f5cda - As we have already made the decision about the design, making the test pass is pretty simple. We just need to store a new Transaction with the amount parameter and the current time using the TransactionRepository. We also needed to implement equals() and hashcode() in the Transaction, so that its equality is solved using its fields and not comparing references (Default behaviour in Java). ● There is nothing to refactor here. Let's move on and implement the next BankAccount operation. Now It's time to create the BankAccount withdraw operation. ● 13c1b41 - Having a look at the test we can observe that it is precisely the same as the one for the deposit operation, but storing a transaction with a negative amount. ● 83a326 - As defined in the test, the implementation is simply storing a transaction with a negative amount. ● 27da82 - After implementing both deposit and withdraw methods we wondered if the knowledge of those terms belongs to the BankAccount. In our opinion, the bank account should know about the concepts of deposit and withdrawal , but not what defines them internally (a positive or negative amount). Therefore, we decided to push the logic related to the amount to the Deposit and Withdrawal abstractions respectively. Deposit and Withdrawal are subclasses of the now abstract Transaction class. The repository will continue handling transactions. The BankAccount instantiates Deposit or Withdrawal respectively for the deposit and withdraw operations. It is important to note that in this case, inheritance is used to specialise the transaction behaviour related to the amount and not for code reuse. Inheritance is a mechanism aimed for specialisation, not for code reuse. The code is now much more expressive, and the responsibilities are where we think they belong. Conclusion We have reviewed how to write an acceptance test based on the acceptance criteria. As design occurs upfront, we have decided to create the collaborators that we know that were needed at that time. Lastly, we started to explore our system with the subsequent inner loops. In the next post of the series, we will finish implementing the system. We will conclude summarising some of the most valuable insights that we have learnt during the process.", "date": "2016-09-22"},
{"website": "Novoda", "title": "Deceiving dates", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/deceiving-dates/", "abstract": "JodaTime is a very powerful tool in a developer's toolbelt. At Novoda, we use it in almost all of our projects as it provides a set of date and time manipulation APIs that are vastly superior to the ones provided by Java itself. While it is more intuitive and it provides a lot of convenience over the standard Date APIs, it is not without caveats. In this post we’ll see how the wrong assumption can cause hard to catch bugs when it comes to comparing dates. CC BY-SA 2.0 by Archives New Zealand on flickr Just a matter of time While tweaking a small bit of code and the associated tests, a few of those tests turned red with a rather confusing message: java.lang.AssertionError: expected:\n<'1970-01-02T11:17:36.789+01:00 (DateTime@26a52cca)'>\n but was:\n<'1970-01-02T11:17:36.789+01:00 (DateTime@621b503a)'> How is it possible that two seemingly identical DateTime s are considered different? The issue here is masked by the representation. What JUnit uses to give you that String is DateTime.toString() , which happens to print out the date contents as an ISO-8601 formatted string. That representation is using the standard format: yyyy-MM-dd'T'HH:mm:ss.SSSXXX This representation contains all the basic information about the date ( yyyy-MM-dd ), the time ( HH:mm:ss.SSS ), and the timezone ( XXX ). So where is this difference, and what is masking it? Let's take a step back and look at how the DateTime s are constructed. Every name tells a story The two DateTime s we are comparing are constructed in a slightly different way: DateTime firstDate = new DateTime(123456789L);\nDateTime secondDate = DateTime.parse(\"1970-01-02T11:17:36.789+01:00\"); The local timezone is, in case you were wondering, the London one. It's summer, so that means we are in BST (UTC+1). The DateTime constructor we use will get the system timezone, which will be Europe/London . If you format that to an ISO-8601 string, you indeed get \"1970-01-02T11:17:36.789+01:00\" , the same as the string we parse to obtain the other date. And yet, they are subtly different, even though their string representation is the same. Why are they different? If you do firstDate.getZone() you get \"Europe/London\" , whereas secondDate.getZone() will give you \"+01:00\" . They indeed resolve to the same offset from UTC, one hour, but they are not the same! If you look at the types of the timezones, you'll notice the first difference: firstDate.getZone() is a CachedDateTimeZone , and secondDate.getZone() is a FixedDateTimeZone ! What does that mean? In simple terms, a FixedDateTimeZone is a timezone with an absolute, fixed offset from UTC. That means it is representing an offset that doesn't depend on things such as DST (and thus, the date in which you \"resolve\" its value). On the other hand, CachedDateTimeZone accounts for timezone offset changes throughout a year, such as those caused by DST. When you have a timezone that is tied to a geographic location, you use a CachedDateTimeZone because that location will have a different UTC offset depending on the instant the offset is resolved for. This is made explicit by the DateTimeZone.isFixed() method, which will return true for a FixedDateTimeZone , and false for a CachedDateTimeZone . Never mix and match! The issue here is then caused by the fact that we mix and match a constructor, which uses the computer's timezone (we're running JUnit tests on the JVM), and a parse() call, which reads an absolute offset, such as \"+01:00\" . It's now clear what the issue is: a computer's timezone is usually bound in its settings to a location — in this case, Europe/London . An ISO-8601 string can only represent a fixed offset, in this case \"+01:00\" . As we've seen before, that means the timezones are different between firstDate and secondDate . Even though they both resolve to +01:00 today, because we're in BST, they will eventually diverge when the UK goes to the GMT (which is +00:00 ) timezone. Consequently, firstDate.getZone() and secondDate.getZone() are not equal. The catch here is that Joda-Time, when comparing two DateTime s, compares their time zones too (contained in the Chronology , amongst other things). It is not simply comparing the Unix time millis as some might expect. Finally, we understand why firstDate and secondDate are not equal! You can verify this behaviour yourself with a simple test case: @Test\npublic void givenADateWithLocalTimeZone_andADateWithAFixedTimezone_whenCheckingEquality_thenTheyAreNotEqual() {\n    DateTime firstDate = new DateTime(123456789L);\n    DateTime secondDate = DateTime.parse(firstDate.toString());\n\n    assertThat(secondDate).isNotEqualTo(firstDate);\n} This is a problem , since a lot of code relies on equals , and it might behave differently than expected. If you are going to compare the values of two DateTime s, you have a few ways to make sure those comparisons don't fail: Compare the resolved UTC timestamps in millis, instead of comparing the DateTime s directly If you don't care about timezones for your usecase, use LocalDateTime wherever possible, instead of DateTime Write tests, and make sure you create expected dates in different ways you create precondition dates (i.e., don't create them all via a constructor or via parsing, if the System Under Test is using parse() ). This will help you catch these subtle mistakes. As always, remember that dates are hard . Treat them with all the care, and get to know how they work, because if you don’t you’ll end up staring at a screen for hours trying to figure out some obscure bug. Murphy’s law is real, and dates are a great way to test it.", "date": "2016-09-06"},
{"website": "Novoda", "title": "Can you use Firebase on Amazon Android devices?", "author": ["Jozef Celuch"], "link": "https://blog.novoda.com/firebase-on-amazon-devices/", "abstract": "Firebase depends heavily on Google Play Services which isn't available on Amazon’s FireOS. Here I discuss the available Firebase tools & features with Amazon-specific alternatives that you could possibly use on FireOS. All Firebase features require Google Play Services and play services availability on the device is checked when a feature is attempted to be used. In case the device does not have Play Services installed most of the features seem to fail silently or log a warning message. Let's look into each feature and what actually works on Amazon's flavour of Android . ✅ Firebase Realtime Database The realtime database works. The presence of the Play Services is checked, a warning message is logged but the database has a fallback implementation of the required classes. All database operations seem to work normally, without any problems. Check out the log for more info: W/GooglePlayServicesUtil: Cannot find Google Play services package name.\nandroid.content.pm.PackageManager$NameNotFoundException: com.google.android.gms\nat android.app.ApplicationPackageManager.getPackageInfo(ApplicationPackageManager.java:115)\nat amazon.content.pm.AmazonPackageManagerImpl.getPackageInfo(AmazonPackageManagerImpl.java:273)\nat com.google.android.gms.internal.zzro.getPackageInfo(Unknown Source)\n     at com.google.android.gms.common.zze.zzby(Unknown Source)\n     at com.google.android.gms.common.zze.zzbx(Unknown Source)\n     at com.google.android.gms.common.zze.zzbs(Unknown Source)\n     at com.google.android.gms.common.zze.isGooglePlayServicesAvailable(Unknown Source)\n     at com.google.android.gms.common.zzc.isGooglePlayServicesAvailable(Unknown Source)\n     at com.google.android.gms.common.GoogleApiAvailability.isGooglePlayServicesAvailable(Unknown Source)\n     at com.google.android.gms.internal.zzqc$zzc.connect(Unknown Source)\n     at com.google.android.gms.internal.zzqc$zzc.zza(Unknown Source)\n     at com.google.android.gms.internal.zzqc.zza(Unknown Source)\n     at com.google.android.gms.internal.zzqc.handleMessage(Unknown Source)\n     at android.os.Handler.dispatchMessage(Handler.java:98)\n     at android.os.Looper.loop(Looper.java:135)\n     at android.os.HandlerThread.run(HandlerThread.java:61)\nW/InstanceID/Rpc: Failed to resolve REGISTER intent, falling back\nW/InstanceID/Rpc: Both Google Play Services and legacy GSF package are missing\nD/FirebaseApp: com.google.firebase.crash.FirebaseCrash is not linked. Skipping initialization.\nW/GooglePlayServicesUtil: Google Play Store is missing.\nI/FA: App measurement is starting up, version: 9452\nI/FA: To enable debug logging run: adb shell setprop log.tag.FA VERBOSE\nI/FirebaseInitProvider: FirebaseApp initialization successful\n... Noticeably here: ...\nW/GooglePlayServicesUtil: Google Play Store is missing.\nI/DynamiteModule: Considering local module com.google.android.gms.firebase_database:3 and remote module com.google.android.gms.firebase_database:0\nI/DynamiteModule: Selected local version of com.google.android.gms.firebase_database ❌ Authentication Authentication does not work. The auth module fails silently with the same logged warning as the database but there is no local version of the Firebase Auth module so nothing happens. W/DynamiteModule: Local module descriptor class for com.google.firebase.auth not found.\nW/GooglePlayServicesUtil: Google Play Store is missing. You can still auth with Google api’s, potentially using the Java client . You could also look into auth using Amazon cognito . Both of these options will get you an auth token, however I cannot find a way to give the realtime database Android SDK that auth token for it to use (the only way is the Authentication SDK which we just stated doesn’t work). An alternative if you have to have auth is to not use the Firebase Realtime database Android SDK and to use the Firebase Realtime REST api’s which do you let you pass a custom auth token. ❌ Remote Config Attempting to change behaviour with remote config does not work. When accessing the FirebaseRemoteConfig.getInstance() there was only a warning log and the functionality fails silently. W/GooglePlayServicesUtil: Google Play Store is missing.\nW/GooglePlayServicesUtil: Google Play Store is missing. ❌ Cloud Messaging and Notifications Following the tutorial on how to add the cloud messaging , I was not able to get any message through to my implementation of FirebaseMessagingService . Amazon Device Messaging could be used instead to achieve similar functionality. ✅ Analytics The Firebase Analytics seem to work (I saw a number of first_open and session_start events in the logs). Even the device model and Android versions are recognised correctly. On Android devices with Google Play Services there is also quite a lot of information about the users (e.g. gender, age, interests) which seems to be missing in the analytics events from the FireTV. However, that might be caused by the fact that I only had a small number of logged events in the console. I also attempted to log custom events. Analytics events can take up to 24 hours to show up in the console so they did not appear straight away but after waiting for what felt like forever, it is confirmed custom events also work. ❌ Crash Reporting When I tried to manually log a non-fatal exception using the Crash Reporting static method FirebaseCrash.report(Throwable , there was only the following log message: V/FirebaseCrash: Firebase Crash Reporting is disabled. When I tried crashing the app, nothing showed up in the console so again it’s not working. If you cannot have Firebase analytics, an Amazon alternative is not necessary and I would recommend to use Crashyltics . Conclusion In conclusion you have 2 out of 6 features working. The main positive being the realtime database is the dominant feature of Firebase and this is working. However the combination of the authentication not working and the fact Google themselves neither confirm or deny guaranteed Amazon support should have you asking questions. Will this continue to work? Will any future updates or features work? More generally is Firebase viable for use on Amazon Android devices? Where do you go from here? There are a few choices, if you are used to using Firebase on your Google Android devices none of them are ideal. Each has its tradeoffs, you may have to sacrifice functionality, learn more technologies, add complexity, delay your release or fight a holy war to unite the platforms. Some ideas are: Create a fallback mechanism; for example, use Firebase Android on Google Devices and the REST APIs when on Amazon Android Use the lowest common denominator of features; for example REST APIs on all platforms Have a subset of features working on Amazon devices, preferring to support your main features on Google Android devices Keep prodding Google for confirmed Amazon support Don’t use Firebase as your backend Drop support for Amazon devices", "date": "2016-09-08"},
{"website": "Novoda", "title": "Clean Code: Java Class Fields", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/clean-code-java-class-fields/", "abstract": "Keeping your code clean is no easy task. Clean code allows your project to pass the test of time. It can be broken down into making your code simple to read, write, and understand. This post will explain how you can clean up your class fields to move towards a cleaner project. Code maintenance is a full time job. It's important to clean up your code as you go, so that you don't leave yourself a mountain of technical debt to purge later on. Think of yourself as a lazy developer, better to clean code now so you don't need to do more work later! The other reason to keep on top of your codebase is the 'broken window theory' . maintaining and monitoring urban environments to prevent small crimes such as vandalism, public drinking, and toll-jumping helps to create an atmosphere of order and lawfulness, thereby preventing more serious crimes from happening. [1] Using broken window theory gives us a real boost to our reasoning for cleaning up the class fields. Class fields (also referred as member variables ) are the first thing ( potentially ) a developer will see when reading the code of a class. Let's start off well and make sure they are: simple to read, simple to add to and simple to understand . Getting insight into 'modifiers' The examples discussed and given below are based on Java, but the theory and takeaway can be applied to any language. Class fields can have 3 different types of modifiers. You use different combinations of these modifiers for different things. I've noticed we use some combinations a lot more than others and this usage rate can affect the definition of clean class fields. 1 The first type of modifier are the access modifiers. Public , Protected , '' ( Default/Package ) and Private these affect the scope of your field. Going from most accessible or most visible to the least respectively. Knowing the access modifier of a variable is important because it gives you some more understanding about the coupling of a class. Does this class share its data? Is it expected to work in conjunction with other classes? Is it attempting to be open for extension ? Does it have a declared public interface? The static modifier is for declaring a field working at the class scope (1 per class definition) instead of at the instance scope (1 per instance). The final modifier effects the immutability of a class field. Final class fields can be set once at construction and then their reference can not change, i.e. immutable . Knowing if a field is static or final gives you more understanding about the behaviour of the class as a whole. Is the class expected to act alone? Does it want to use caching? Does it hold state or change over time? Clean by order Now that we have a shared understanding of the different modifiers let's show a potential clean ordering. What do you think of this 3 : public static final\n\nprivate static final\n\nprotected final\n\nprivate final\n\nprivate From our shared understanding, this ordering for our class fields declares the most accessible at the top ( the most shared ) to the least accessible ( and only for us ) at the bottom. The public variables come first, then the protected then the private. The immutable final variables come first then the mutable non final variables which can be changed at any point in the class. It's important to also notice what is not written i.e. space (ie: blank lines) is important here to show clear separation and data clumping of different types as discussed. To keep the code clean here it's important to not add extra blank lines when more fields of any one type are added. 2 Examples over theory As worked examples, your class could look like one of these: A: public static final String EXTRA_ID = \"com.foo.EXTRA_ID\";\npublic static final String EXTRA_NAME = \"com.foo.EXTRA_NAME\";\n\nprivate static final int MAX_RETRIES = 3;\n\nprivate final FooCatcher fooCatcher;\nprivate final BarService barService;\n\nprivate int attempts = 0; B: private static final int RESULT_CODE = 123;\n\nprivate final Basket basket;\nprivate final WishList wishlist;\nprivate final AdvertDisplayer advertDisplayer;\nprivate final AnalyticsService analyticsService; C: public static final String TAG = \"some tag for droidz\";\npublic static final String COL_ID = \"id\";\npublic static final String COL_NAME = \"name\";\n\nprivate static final TAG_CACHE = TagCache.newInstance();\n\nprotected final TagManager tagManager;\nprotected final DatabaseAccess dbAccess;\n\nprivate int lastAccessId; Conclusion Keeping your code always in this ordering and having this shared understanding allows you to easily: visualise potential behaviour see the public api understand scope infer coupling accept dependencies acknowledge state I hope that gives you some inspiration to start looking at the smaller parts of your code base to help you move towards cleaner code. Explaining broken window theory through code, if you get your code base into the right state ( clean ) at the small components then the rest will fall in line. Good luck! 1 : We won't talk about transient , synchronised or volatile as these are only used on rare occasions, but you can factor them into the below quite easily when necessary. 2 : Some combinations are missing for example public final or protected , this is because for reasons we're not going to go into in this blog they're bad practice. 3 : Some assumed knowlege here is the Java convention for class ordering http://www.oracle.com/technetwork/java/javase/documentation/codeconventions-141855.html", "date": "2016-09-02"},
{"website": "Novoda", "title": "Cross platform architecture", "author": ["Dominic Freeston"], "link": "https://blog.novoda.com/cross-platform-architecture/", "abstract": "In this post I'll discuss how we approached building the Oddschecker app on both iOS and Android in parallel, and the benefits we've gained doing so. The similarities are more than skin-deep Introduction Novoda has always built amazing products for Android, and more recently decided to expand our expertise to other platforms. Last summer, Novoda were approached by Oddschecker to build their new mobile product. This went on to become our first project on both iOS and Android. I joined the project (and the company) with the goal of bringing the same engineering focus and quality that Novoda is known for on Android to iOS. With a small, brand new team of iOS developers, we wanted to be able to work alongside the Android developers and benefit from the existing expertise, processes and culture that have contributed to Novoda's success. In this post, I’ll discuss high level architecture and the project structure decisions we made as well as the concerted efforts we took to align them on both platforms. I'm going to discuss why we took the approach we took, what that architecture looks like, and some of the key benefits it provides for both Novoda and our clients. Motivation An app is an app. Underneath the cosmetic and slight navigation pattern differences, iOS and Android applications offer very similar experiences to their users. Creating great applications is time-consuming and expensive; anything that speeds up the development of amazing, robust and maintainable applications is a victory for our developers and clients alike. Many have tried various solutions, from the write-once-run-anywhere dream of old to the learn-once-write-anywhere esperanto-like pseudo-pragmatic utopia of today. JavaScript may be eating the web, but it's hard to believe it's the answer to our prayers. I personally love the idea of React Native , but as discussed in a previous post , it is not (at least, not yet) the solution for us and the kind of first-in-class experiences we want to create. Until we achieve cross-platform nirvana , how can we facilitate the development process on multiple platforms simultaneously? Language and Tools Whenever we build apps, we have to establish a domain-language for talking with the business and make sure we're working towards the same goal. The first thing we did was to make sure we did the same across platforms. Rather than look to share code, we looked to establish a shared mental model and a common set of terms. This meant creating similar layers in the application (more on that later), but it also went right down to how we structured the project and what frameworks we used. Domain-First Betting is a particularly arcane and complex domain , so it's proven particularly important that we all agreed on the meaning of various common domain terms, and also on how we refer to various parts of the app. We made the decision to group things by these domains (e.g. Account , Bet , BetReceipt ), rather than by layer (e.g. Model , Service , View ). Beyond the fact that we generally consider this a sensible way to organise a project, it also makes it much easier to deal with slight differences across platforms. Android has Activities and iOS has ViewControllers, but knowledge of these differences isn't needed to successfully navigate your way through the codebase. If you're interested in any aspect of account management or bet placement, you can find the relevant folder and the appropriate layers and components will soon become apparent. Rx(Swift/Java) Early on in the project we agreed to make use of ReactiveX (Rx) on both platforms. RxJava was already familiar to Novoda and we were keen to pick it up on iOS as well. We adopted RxSwift , right about the time it was being picked as the official Swift implementation of Rx. Although a potential risk due to the lack of maturity of the framework at the time, it's one that we considered worth taking and one that has paid off considerably. One of the key components to cross-platform architecture is having standard interfaces/protocols. The implementation can be totally different, but as long as the various components present similar APIs then we can have sensible discussions about how to extend, modify, fix or improve the application. Beyond the numerous advantages that Rx itself offers, having a standard way to transport data throughout the app on both platforms is a great advantage when designing cross-platform APIs. Layers of Abstraction Let's get a little more concrete about our layers of abstraction. The exact terms and architecture are not crucial here, the fact that they are platform-agnostic and that we agreed on them on both platforms throughout the project are the key. Service, Repositories and Data Sources Whatever you call the business logic layer of your application, somewhere along the line you'll need to fetch data from a network, make some requests, store data to or fetch data from disk, etc. This is where it happens, and anything above the networking or database layer is entirely platform agnostic and can be discussed as such. For us, this meant all data is exposed as Observables , containing not only the data but also the state captured in the pipeline. This DataState contains both optional, immutable data and whether the Service responsible for it is Idle , Busy , or has an Error . Other parts of the application can subscribe to updates and the UI layer can make sensible decisions as to how to display this to the user. Any actions on the domain are carried out via void methods, that is, no result is provided directly. Any changes to the state, and thus to the UI, are reflected in changes to the various DataStates that can be observed. This approach is inspired by various uni-directional data flow architectures used in web frameworks , including Flux . I'm personally a big fan of this approach, but once again, the key part is that this was exactly the same on both iOS and Android. Façades and View Models Facades are what we called the objects that convert the generic-domain APIs into the requirements for a particular context, generally corresponding to a “screen” in the application. This is where we combine various data streams into a ViewModel that contains all the presentation data for the current context. Note that in our case, a ViewModel is a simple value type, the Facade is what receives user commands from the UI layer and interacts with the Service layer. On iOS this meant ViewControllers that would subscribe to updates and bind themselves to this ViewModel , on Android the Activity would use a Presenter to bind the ViewModel and the View . There is scope for even further alignment between the platforms nearer the UI layer, but nonetheless the differences are minor and localised enough as not to get in the way of fruitful collaboration. Core and Mobile A slice through the app’s components and the data flow Novoda has long had a clear separation between the \"Core\" and the \"Mobile\" layer of any application. This started out in part from a desire to unit test back in a time where this was almost impossible on Android, but this pure Java Core used by an Android layer leads to a clean separation between the domain concerns and the UI concerns. This is something that all applications should strive for. On iOS, in Swift, this has meant a Core that relies only on Foundation and the Swift Standard Library (and in our case, RxSwift). Since iOS 8 and dynamic frameworks, Core can be a separate target that needs to clearly define its public API, thus enforcing a clean separation between the layers. For the architecture described above, this means Services , Repositories and DataSources living in Core and Facades , ViewModels and Views living in Mobile. This simple separation is a good step towards a proper hexagonal architecture , decoupling your business logic from the platform detail. In the ideal case, this Core would be truly cross-platform. Java2Objc is an attempt in this direction, but it still requires transpiling between languages. As Swift compiles to more and more platforms, and with Foundation being reimplemented in Swift , this could become genuinely viable. I dream of a Swift Core that can be shared across iOS, Android, the Desktop and the server. Flexible Teams One worry of working so closely across platforms that has cropped up is the idea that the resulting product could end up only reaching the lowest-common-denominator when it comes to feature-set. This is particularly true as our entire process is shared, down to having a single user story to be implemented on both platforms. If we're always working on the \"the same app\", how can we make the best of what each platform has to offer? Not only are differences in capabilities between the platforms get smaller each year, but the issue is easily avoided if the team is encouraged and willing to be flexible when necessary. If anything, a feature available natively on one platform might trigger some creative thinking in how to provide similar functionality on another, improving the experience for all users. More than the sum of its parts Whilst we’re still not sharing code across platforms, a shared architecture and a shared domain language has provided us with numerous advantages. For one, it enabled developers proficient on both platforms to effortlessly switch between them, maintaining the same mental model of the domain and the application structure. But as mentioned throughout this post, even without cross-platform developers, such an approach enables the entire team to discuss implementation decisions, possible issues, and estimate the complexity of upcoming features. This ceasefire in the platform wars enabled traditionally silo-ed teams to communicate and collaborate. Two pairs of developers can operate as a single team of four , sharing the impact of effort, insights and discoveries to both platforms whilst maintaining the platform-specific expertise required for top quality applications. This general collaborative approach extends to how we always aim to include backend developers, designers, testers and stakeholders in the process of making applications. If you're looking to offer high quality experiences to your users on both platforms and you want to deliver these in a timely and cost-effective manner, you need a single, cross-functional team. In Summary Agree on terminology and keep the same project structure Use similar tools and frameworks where possible Have consistent interfaces/protocols Separate your business logic from your application layer in a consistent manner Be flexible and creative Have a single happy team :)", "date": "2016-08-30"},
{"website": "Novoda", "title": "Github Data Mining: 101", "author": ["Francesco Pontillo"], "link": "https://blog.novoda.com/github-data-mining-101/", "abstract": "This blog post will outline how Novoda mines organisation data from our Github projects, talk about the data mining architecture we thought of, helping you understand the Github API and improve the visibility of your organisation data. Here at Novoda, we are proud to say that all decisions we take are data-driven. Why choose one technology over another? Why prefer a generic architecture over a specific one? Why develop a custom made solution over a pre-existing library? In combination with this line of questioning, we’ve been asking ourselves how can we extract and analyse relevant information from Github around: team dynamics, developer time spent on pull requests, details of cross-project collaboration and how much open source projects participation there is. Github Reports Objectives We can break down the above description into a list of goals for a report system that is able to give us metrics regarding: people’s work on assigned projects inter-project collaborations how much each developer comments on other developers’ pull requests opened, merged and closed pull requests average measures for the whole organisation both historical and future data Understanding this data may reveal fundamental clues in driving actions to improve team organisation, identify if and where bottlenecks are, then proactively solve them. Credits: Picture by Othree on Flickr Exploring Github’s Available Data Github already has a statistics page for each repo, but that is a bit limited and cannot respond to all of our requirements. So we began exploring all possibilities for retrieving these statistics using the Github APIs . At first we thought of requesting data on demand. For instance, getting statistics about a user in a certain period of the year would be solved by querying that user’s data from the API. But then we asked ourselves how we could compare multiple users’ data , which led to the realisation that we should actually query data for all of organisation's (Novoda) developers at once. For each report generation, then, we would need to do an unbelievable amount of API calls, since Novoda has ~130 repositories and we could assume that: Every repo has at least 5 issues or pull request opened per day Every new issue or pull request gets about 10 comments per day, so that we don’t have to handle paging (in fact, we do quite a lot of paging, but let’s stay on the simple estimate side here) This means that mining 30 days worth of data means executing: \\begin{align} & (130~repos) * (1~issue~list + 5~issues) * (30~days) \\simeq 23,400~requests \\Rightarrow \\ \\Rightarrow~ & \\frac{23,400~requests}{5000~requests~per~hour} \\simeq 5~hours \\end{align} Should any of these requests fail, or should we want to change the date range even by a day, we would have to run the whole querying process all over again! Data Mining Github As good Computer Science scholars, we know that the next step would be to get all of Novoda Github data in our own data mart , then mine it to extract all the pieces of information we need. In regards to the type of data we want to extract out of the database, it would only contain the minimum subset of information we need, making it as small and efficient as possible. Github provides us with two ways to access its information: Web hooks that continuously push data to a client’s endpoint, perfect for newest, live data. REST API that exposes historical organisation data, repositories, issues, etc., which is ideal for past data. Foreseeable issues For requests using Basic Authentication or OAuth, you can make up to 5,000 requests per hour. Currently set at 5000 request per hour , the generous Github rate limit allows for excellent integrations in minimum and medium-load clients, but it is definitely too low for us wanting to mine all of our historical data. This issue is connected to another, more practical issue: where can we run the whole system? Downloading the history for the entire organisation would definitely take a few days at the very least (we’ve been active on Github since 2011), while the live one (using the WebHooks) would need to be always on and performant to save all live data. Solution Architecture After a lot of coffee and many Super Mario Kart 8 races, we came up with a multi-tiered solution that we believe is optimal for our use case. Github Data Analysis Studying the Github API and data , we figured out that the data we needed to retrieve is tree-shaped: On top of the tree we have the organisation From the organisation we can extract repositories From each repository we can extract Issues Pull requests From each issue and pull request we can then extract Comments Reviews Reactions Actions (open, close, merge, etc.) By building and going through this tree we know for sure that all the information will be received and persisted in an orderly fashion, otherwise all our database constraints would be invalid (e.g., repositories must be retrieved and stored before moving on to issues). Brainstorming the Github data System view The “live” system would run on the cloud, exposing an endpoint that listens to, converts and persists relevant information sent by Github. The “historical/batch” system was a bit harder to figure out, since it is composed of simple operations that need to be repeated over time: Starting the process Requesting a piece of Github information Converting the information Persisting the information Handling API rate limits Handling network errors Understanding “what to request next”, then doing it And what do you do when you have a complex, repeating sequence of steps? Easy, you make it recursive! Recursion helps you figure out the smaller parts of the problem and reduce them to a final state, that is: your solution. Reducing from infinite to finite We started calling our tree nodes “messages”, something that our system receives and knows how to handle, eventually reducing it to: One or more new messages Zero new messages, which we called a “local termination” When we have a local termination, it means that we have reached a leaf node in the tree. Our system will then analyse and process one message: Start Accept a message in the “current position” within the tree a. The base step is “Get organisation” Perform the operation ( recursive step ) Executes the API request Saves data onto the database Creates 0 to n new messages in the tree Moves the “current position” within the tree to the “next position” The final step is reached when there are no more nodes Exit To simplify this, we linearised the tree to a queue employing a breadth-first search, so that the concept of “next position” is more explicit. A depth-first search would have worked just fine, since both ensure the same relative order between tree nodes. Linearising the tree in a queue Handle API rate limit What if the request fails because we have reached the API limit for the hour ? Our solution is to schedule the same message on a queue for future analysis as soon as the API limit is reset by Github. Let’s not worry about the actual implementation of the rescheduling system, keeping it abstract helps break all of the issues down and leave the implementation details for later. There will be some kind of active mechanism that wakes up the queue and restarts the process and since our system will always pick up the first message in the queue ( FIFO ), we know for a fact that it will re-run the operation that failed before. The same behaviour can be used for network/database errors, rescheduling the process to re-run immediately or after a short pause. Using paging with live data Something we have not considered yet is paging. To avoid exhausting the API limit in a short time, we should retrieve information in batches as large as possible. But the data we ask for isn’t stale: from the moment we ask for a page to the moment we ask for the following page new data might be added! This may introduce two problems: data duplication and incomplete data. Duplicated data As pages are ordered by descending date, if new data is added, old data might be retrieved again in following pages, then saved twice onto the database. The naive solution would be to UPDATE OR INSERT all data received from the API… which is just what we did! Simpler is better Incomplete data Since new data shifts old data towards later pages, we might not be able to retrieve the old information we need, as the number of messages generated from the previous node corresponds to the number of pages the Github API said we had at that particular moment in time. This means that we might be told we have 3 pages at the start, but the new data has shifted to a total of 5 pages, while our original count is still at 3! Our solution consists in retrieving the page from the API, then analysing if Github says it is the last one or not: if it is not, it means that new data has been generated and we need to request these new pages, i.e. add new messages to the queue. This leads to a change of spec: a tree node does not always generate new children, but also siblings! Having linearised our tree in a queue, this is a merely academic observation. Data results All of the information we mined from Github ended up in a database full of repositories, pull requests, merges, closes, comments and much more. Our next blog posts will show you how we use all this data to find interesting correlations, patterns and outliers. What’s next? Running this system on a local machine means you have all the data only on your own local machine AND you can’t do much else whilst it is running! Our next blog posts will also detail the implementation of such architecture on Amazon Web Services, go through the challenges we faced and help you get set up if you want to achieve similar results.", "date": "2016-08-31"},
{"website": "Novoda", "title": "Android TV UI Kit Freebie.Sketch", "author": ["Qi"], "link": "https://blog.novoda.com/android-tv-ui-kit-freebie-sketch/", "abstract": "Recently I was given the opportunity to work on a side project with Alex Styl and another craftsman because Novoda wanted to showcase an Android TV demo to our client. Firstly, I collected everything I could find about not only Android TV but anything to do with design for big screens including blog posts, videos and guidelines. Unfortunately, I have to say, compared to design resources for tablets and phones, there is very little available for Android TV. For that reason I decided to create some resources myself! I came up with the idea of an Android TV design template that anyone interested in designing for the big screen could use as a starting point. It’s based on the screenshots from the Android TV Design Guidelines . Sketch file contains: Browse Lane (Collapse/Expand view) Browse Row Browse Row Scroll Down Grid View Video Player Music Player In App Search Feel free to download the kit from GitHub or sketchappsources website , use it in your projects, and share it with your coworkers! Resources: An in-depth look at the Leanback Library - Google I/O 2016 UI Patterns for TV Designing for Android TV Android TV Guideline tvOS Human Interface Guidelines", "date": "2016-08-18"},
{"website": "Novoda", "title": "UX for Missing Content Descriptions", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/ux-for-missing-content-descriptions/", "abstract": "We know it's important to add content descriptions , but what happens when you don't? In this post, we look at a TalkBack feature designed to help users provide their own labels, how it can start to get a little messy and what you can do to help them. Let's pretend I wrote an app, Super Cool, for my social network. It currently only shows a single post with some gibberish, but it has like and dislike buttons for every post. Unfortunately, I forgot to add content descriptions to the buttons. This means that if a TalkBack user is interacting with Super Cool, and they focus on either of these buttons, they'll hear \"Button, Unlabelled\". The user will be frustrated, but is probably used to it - many apps are missing content descriptions. After a while however, they learn that the first button after the text post is Like so they decide to add a custom label. This will mean that they don't have to remember it anymore! Alas, I am a lazy developer: I used the android:onClick attribute in XML to bind to public methods in my Activity rather than creating a View ID and associating an OnClickListener with it. <ImageView\n    android:onClick=\"onClickUpvote\"\n    android:src=\"@drawable/up\" ... />\n\n<ImageView\n    android:onClick=\"onClickDownvote\"\n    android:src=\"@drawable/down\" ... /> What I didn't know was that using custom labels requires elements to have IDs, preventing users from even making the interface more helpful for themselves. Months down the line, I refactor the app and decide not to use the android:onClick attribute and to add an ID to each of my buttons: <ImageView\n    android:id=\"@+id/like_image\"\n    android:src=\"@drawable/up\" ... />\n\n<ImageView\n    android:id=\"@+id/dislike_image\"\n    android:src=\"@drawable/down\" ... /> \"Awesome,\" says the new user (the old user has long since uninstalled my app). \"I can finally add some custom labels!\" Now, at last, the buttons will have user-added content descriptions, \"like\" and \"dislike\", and these will be read aloud when TalkBack focuses on these elements. The user is now somewhat content. As a developer though, I continue to refactor the app. I notice that I added IDs named for \"like\" and \"dislike\" even though my domain language is \"upvote\" and \"downvote\". I decide to change it. Suddenly, that content user is once again missing TalkBack descriptions for these two buttons. The reason is that custom labels rely on the IDs remaining stable . If they change, then the labels are orphaned. Even worse, if the IDs are exchanged between Views, then the labels will be applied to the incorrect Views. At long last, I decide to be a good, considerate developer and add a content description to Super Cool’s buttons explicitly: <ImageView\n    android:id=\"@+id/upvote_image\"\n    android:contentDescription=\"upvote\"\n    android:src=\"@drawable/up\" ... /> The user’s custom labels will be ignored, and our content description will be used instead. In conclusion, it really is important to add content descriptions for actionable elements in your UI. Not only does it provide a better experience for your users from the start, but it also saves them a potentially long and frustrating journey.", "date": "2016-08-09"},
{"website": "Novoda", "title": "5 tools for a remote Android developers toolchest", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/5-tools-for-remote-working/", "abstract": "A poor developer always blames their tools. Well, let’s talk about the best tools for remote work so you can't blame them!  This blog post outlines 5 tools that come in handy when you don't work in the same physical space as your co-workers. There are three types of remote work—in an office with colleagues in another office, you in your own home office, or as an on the go digital nomad working from wherever you find WiFi. No matter your ilk, the information below should help you stay communicative, responsive and on top of your craft as a remote Android developer. The tools Remember: the tool gives you the means to the end but it's also important to understand why you are using the tool and what that end goal is. Take lessons to improve your remote capability from this blog post, not just a list of tools. ScreenHero Not being able to just grab someone in the office to point and show them the problem on your screen is a real pain for remote workers. Similarly, explaining to someone how to fix something via a chat message is just not the same as fixing by example in person. Screen sharing for collaboration in teams; Code with others remotely; You each get your own mouse cursor, and you're both always in control. Screenhero works with your favorite IDE and makes remote pair programming or debugging effortless. Screenhero really makes you feel like you're sitting next to the other person. You have an open voice channel and you can both see the same desktop. It's like having two keyboards and mice as you can both type and click at the same time (which sometimes does require a bit of etiquette to avoid battling over the cursor position). Screenhero was bought out by Slack , and although they have said integration is coming soon it is not there yet, right now Screenhero has stopped allowing new sign ups. However if you know someone who has already signed up e.g. us , you can get an invite off them and it'll work like normal. Ask anyone at Novoda we will happily share. A physical device Working remote can mean you are not plugged into an external power source so every lithium % counts. Using a real test device, rather than an emulator, keeps your laptop going that little bit longer. Android Device of your choice, be together not the same Plugging in a real device really does save you laptop battery consumption. If your computer is plugged in to the mains Genymotion works just as well, but coding on a plane, train or a beach you need all your juice for as long as possible. Always have a dedicated development device with you. Make it a low end device as well - a Samsung Galaxy S3 mini is my current preferred device. Having a development device means you aren't worried about using your personal phone battery and vice versa - you can still develop if you have been catching Pokémon on your phone all morning. Using a low end phone has two benefits: it means if you make your app work and run well on this device it'll run well on other more powerful devices the battery will last longer if it doesn't have to run a powerful CPU or power an amazing screen. Vysor After you have shared your screen and are pairing with another Android developer, you need to ship your code to a device. Unfortunately the other person, being remote, won’t be able to  see the device, tragedy. Vysor puts your Android on your desktop. Use apps, play games, control your Android. An easy install Chrome app, compatible with all operating systems. Vysor allows you to share your device with the person you are pairing with by displaying it on your (shared) desktop. They can even control the device, swiping and clicking buttons. Another reason for using Vysor can be the processing power of your machine. Using Vysor and a real device is not as CPU intensive as running up an emulator whilst also Screenhero'ing and compiling. Git Not being guaranteed a stable internet connection is a fact for any digital nomad. Even if you just plan to code on a flight between offices or a commuter train, you need to plan for no internet connection. Most operations in Git only need local files and resources to operate – generally no information is needed from another computer on your network. If you’re used to a CVCS [centralised version control system] where most operations have that network latency overhead, this aspect of Git will make you think that the gods of speed have blessed Git with unworldly powers. Because you have the entire history of the project right there on your local disk. Git is taken for granted in this day and age but being able to commit your code when you have no internet connection saves you from a world of pain when bug fixing or working on multiple code areas at once; you’re able to move between/revert changes at any point, all while offline. Always remember to get all the code pulled that you are going to work on before you go offline. This online prep can also include ensuring Android Studio IDE is up to date and Gradle has run at least one clean build which makes sure you have all dependencies needed. Communication skills Working remotely can be a lonely and isolated experience. Your closest human contact could be only through commit messages and you won't bump into people in the corridor for that ad hoc chat that is sometimes needed to resolve issues. You need to know how to communicate. Your own brain is the tool for this problem and all it needs is a small upgrade. Great code check in Great code check in... Great code check in ... :D The benefits of good communication is far reaching, and for you personally it will mean more enjoyment of work, better opportunities for more work, and a sense of a team and belonging. You really do need to know how to communicate as a remote developer and the explanation of this one point could be a whole blog post. For now here I will keep to the short version: Make sure you correctly organise your calendar events, invite the correct people and add an agenda (ask for an agenda if there isn’t one) so everyone who is attending knows why to attend and what is going to happen. This also helps stop meetings going off track and helps to finish them on time. Keep git commit messages perfectly formed to allow you to read them back when things go wrong, also for your colleagues who can't ask you questions face to face. Keep your inbox under control ( perhaps try inbox 0 ) and ensure you always reply to all emails . You need to keep up communication and ensure you are organised with giving people prompt feedback via email, you won't bump into them in the corridor to just say \"yes I got your email great thanks\" . Always consider the correct use of emojis in conversation (the emotion is lost in textual discussion) and remember that, being remote, people may not know you personally therefore not know your personality too well. Always add more communicative emotional context. Conclusion The world is becoming more connected and the need to be physically face to face is lessening. The future lies in remote work. The important point to take from this blog post is remote work is not an isolated process, you need to train up and evolve your handling of your working relationships. There are many people facing the same problems out there and there are a wide range of tools to investigate and adopt and don't forget to share your learnings with others ( online ), because hey, I won't see you in the office for you to tell me. :-)", "date": "2016-07-26"},
{"website": "Novoda", "title": "New pipelines in our CI", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/new-jenkins-pipelines/", "abstract": "Working with Agile, one of the most important tools we work with is a Continuous Integration (CI) system. This allows us to automate most of the tasks which follow a change in a project codebase, from testing to static analysis, including automatic deployment. At Novoda we use Jenkins for all our projects, in order to automate our daily working routine, and we recently started investigating a new feature of this tool: build pipelines. Pipeline what??? Wikipedia gives us the following definition for a pipeline: _In computing, a pipeline is a set of data processing elements connected in series, \nwhere the output of one element is the input of the next one._ The core point in this definition is the fact that a pipeline is composed by several elements, one executed after the other, and the execution of every element is dependent from the result of the previous one. It is easy to see a relation with a CI job, where the execution is composed by several steps, each one with its own configuration and dependent on the result of the previous ones. Compared to traditional (freestyle) jobs, pipelines provide a better visualisation of the status of the several parts composing a job and they are more versatile, given their ability to fork, join, iterate, and work in parallel with each other. Pipeline DSL With the Pipeline plugin, Jenkins introduces a domain-specific language (DSL) based on groovy which can be used to define a new pipeline as a script. The DSL is extensible and supports custom extensions and multiple options for integration with other plugins. Let’s have a look at the basic syntax which can be used to start defining a pipeline. Nodes In order to execute most of the steps in a pipeline, the job needs at least one node . The name might be misleading, given that usually in Jenkins UI a node is one of the elements of the CI network: the main one is the master coordinating a set of slaves (or agents). Every one of these nodes can have more executors : every executor allows one job instance to run, so having multiple executors associated to the same machine allows to run several jobs in parallel. The node we are talking about in the pipeline context is directly related to an executor. All the commands included in a node closure will be run by a dedicated executor in one of the CI agents. To request a node then we just need to use the syntax node {\n\t// commands to run\n} It is possible to filter which node can run the enclosed commands using one of the labels defined in Jenkins Nodes settings, specifying directly one of the agents name or based on some of the agent OS characteristics ( windows , unix , etc...). For example in our CI network we have several agents with the label “android” and we can select only them using node ('android') {\n\t// commands to run\n} Stages Stages allow to group job steps into different parts and they are the main components of a pipeline. The new job visualisation makes this aspect even more clear: It is easy to see the duration of a single stage in a pipeline, for multiple job runs. Also, in case of error in one of the stages, that one is immediately highlighted and the following stages are not executed. To define a set of stages we proceed as following: stage \"Checkout\"\n// get project source code from SCM\n    \nstage \"Build\"\n// run the project-specific build script\n\nStage \"Tests\"\n// if the build was successful, run all the tests Steps Inside a stage it is possible to add several steps, like it was possible to do for traditional freestyle jobs. Jenkins pipelines support any build step from the installed plugins in the Jenkins environment. For example, if we want our CI to change the status of the build to stable or unstable depending on the result of Findbugs static analysis, we can use the following step: step([$class             : 'FindBugsPublisher',\n      canComputeNew      : false,\n      pattern            : '**/findbugs/*.xml',\n      unstableTotalHigh  : '0'\n      unstableTotalNormal: '5',\n      unstableTotalLow   : '10']\n) Scripts It is also possible to specify shell (or batch) scripts to be executed in a pipeline. For example we can run gradle tasks for the current project using sh \"./gradlew clean build\" Parallel execution Pipeline plugin supports the parallel execution of commands, by creating a map of parallel branches and commands: def branches = [:]\nbranches[\"Branch 1\"] = {\n    node() {\n        unstash name: 'workspace'\n         // do something with the project files\n    }\n}\n\nbranches[\"Branch 2\"] = {\n    node() {\n        unstash name: 'workspace'\n        // do something else with the project files\n    }\n}\n\nparallel branches The first problem with parallel execution is that it is not possible to define stages inside a parallel branch, so it won’t be possible to have a graphical representation of the status of a single branch. The second and bigger problem is related to performance: we executed some benchmark, and we found that having serial execution takes less time than having the same steps executed in a parallel way. This is probably due to the extra time needed to copy the workspace files between two nodes. Pipeline job To create a new pipeline the first step is to create a new Pipeline Job. To do so, from the main Jenkins screen select “New Item”, specify a job name and then “Pipeline”. In the following configuration page, the first three sections are the same as for a traditional Jenkins job ( General , Build Triggers and Advanced Project Options ). What really changes is the new Pipeline section, where we can choose between two options in order to define the job pipeline: using the inline editor or loading the pipeline from a versioned file. Inline editor Using the inline editor allows to quickly test a new job configuration by editing the pipeline script in a text field, saving the configuration and running the job. Versioned pipeline As soon as pipelines grow, it would be difficult to maintain them only using the text area present in the Jenkins job configuration page. A better option is to store them in specific files versioned within your project repository. Doing so all the changes to the job configuration will be versioned and could be easily updated using a script (think about a sprint automatically lowering static analysis thresholds after every release, for example). In this case we won't have the option to type the pipeline content directly in the configuration page, but we will need to specify the repository parameters for the job and the path of the file containing the pipeline definition inside the repository itself. Snippet generator In order to better learn the pipelines DSL, an online snippet generator is provided along with the Pipeline Plugin. This generator allows to create snippets of code for practically all the steps available within a pipeline. More interestingly, it is aware of the Jenkins environment and then it will provide some error checking and additional steps depending on the installed plugins. As we can see in the following image, the snippet generator integrates with existing builds steps, allowing a configuration similar to the one we are used to for traditional Jenkins jobs. No pull request builder :( What described so far is great for a pipeline used to build the main branch of a repository projects, but what if we want to check our changes even before these get merged? At Novoda we use pull requests in order to have manual review of every change (if you don’t know what pull requests are, have a look at this page and start using them in your team NOW). Apart from a manual review by our peers, we use the Github Pull Request Builder plugin in most of our projects. This allow us to have our CI performing the same checks and tests that would be run on the main branch, even before the pull request is merged. More importantly, it will notify us posting the result of those checks on the pull request page. Now, this is beautiful, but there is a small problem: the Github Pull Request Builder plugin is not compatible with the Pipeline one, so it is not possible to use the two to automatically trigger a pipeline when a new pull request is opened or updated. Luckily there is an alternative to that which allows us to keep using pipelines with a pull requests-based flow. Multibranch pipelines A multibranch pipeline job will execute a given pipeline on multiple branches of a repository automatically creating a secondary job for every branch. In order for the job to correctly integrate with Github we need to use the Github plugin, when specifying the project source, using as credentials the ones of a Github user with admin access on the repo. Using Github webhooks, every time a change is pushed to the repository the plugin will scan the configured repository for branches containing a file named Jenkinsfile in the repository root directory. By default all the branches will be checked, but it is possible to apply filters in the job configuration. For every branch containing such file, a subproject will be automatically created. It will not be possible to edit these projects configuration, but it’s enough to say that when run they will execute the pipeline defined in the Jenkinsfile . Once the branch will be deleted from the repository, the related job will be automatically removed. For every change pushed to the repository, the parent project will trigger a build for the subproject related to the modified branch. In case of a pull request opened in Github, the status of the build for the modified branch is displayed on the pull request page and it is updated after subsequent commits. As it easy to see, this strategy will force the CI to work more, running a job every time a changed is pushed compared to doing it only when a pull request is open. This however has the big advantage of allowing us developers to be always aware of the status of the branch we are working against. It will also speed up the review of pull requests: once one is opened, the status will be immediately available, compared with the Pull Request Builder which asked us to wait for the build to complete after the pull request was opened to be able to know the status. Conclusions What we liked job configuration in version control stages visualisation automatic multi-branch jobs creation inline editor and snippet generator for fast test of new pipelines extensible DSL What we didn't like no clear parallel execution visualisation no Github Pull Request Builder compatibility missing visualisation for several static analysis tools results Pipelines provide new power and flexibility to our Jenkins CI configuration. and it is really easy to migrate existing jobs to new ones using the pipelines system. Multi-branch jobs provide great visibility over the status of a work-in-progress branch, and having pipelines defined in a versioned file allows us to easily find who changed what and why, making the history of changes to code readily apparent. Finally, the DSL used to defined pipelines is also extensible, allowing us to define new custom steps, in order to better specify all the steps our CI might need to execute. Unfortunately there still are several plugin that aren't entirely compatible with the pipelines plugin, for example the Pull Request Builder one and some static analysis visualisations. Luckily there seems to be a good effort in updating not-yet-compatible plugins, so these problems should be solved soon. For more information please have a look at the official documentation .", "date": "2016-08-02"},
{"website": "Novoda", "title": "Top 3 Android Studio Shortcuts - Rui", "author": ["Rui Teixeira"], "link": "https://blog.novoda.com/top-3-android-studio-shortcuts-rui/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Rui will talk about their top three Android Studio shortcuts and how they're better than the rest. ⌘0 … ⌘9 Open tool window When I’m not on my desk, I’m usually limited to my MacBook’s 13”. While I do love the portability, finding the balance between a resolution that doesn’t cause me eyestrain and one where I have the most room possible is hard. That’s why I have the constant need to show/hide Studio’s many panels: Project, Structure, Run, Debug, etc. When a panel is shown, it’s usually automatically focused, enabling you to navigate in it easily and go back to your editor by pressing Esc (if not, try pressing the shortcut again to gain focus). Sometimes I use ⌘7 (\"Structure\") this way as a replacement for the “File Structure” shortcut - ⌘F12 . ⌘⇧O Go to file I’m pretty sure this one is on many devs’ top 3. Given IntelliJ IDEA ’s powerful auto-completion features, you can easily find any file in your project with this shortcut. Don’t exactly remember that class name but you’re pretty sure it’s something like WutPrototypeDelegateFactoryAdapter ? Try typing in “PDFA” and you should see a list of PrototypeDelegateFactoryAdapters come up. If you’re looking specifically for a class, you can stick to ⌘O . ⌘⇧A Find action This one… This is the one. I know I could learn all those refactoring shortcuts and I’m pretty sure there are predefined ones for things such as “Split vertically”, “Close all”, “Close others”, etc. but let’s face it, I’m not a finger contortionist nor I want to be one. No need to bend and intertwine my fingers in awkward angles. “Find action” allows me to type want I want to do and it is context aware. Want to rename that method? Place your caret on its name, ⌘⇧A and type “ren”. That should be enough. This is a simple example, but it can do much more for you. If you’re looking for something more broad, “Search everywhere” - Double ⇧ - might be the one for you. Tip : when I can't open the tool panel I want with ⌘0 … ⌘9 , I either try \"Find action\" or \"Search everywhere\" and type in the panel's name.", "date": "2016-07-22"},
{"website": "Novoda", "title": "Top 3 Android Studio shortcuts - Eduard", "author": ["Eduard Bolos"], "link": "https://blog.novoda.com/top-3/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Eduard will talk about his top three Android Studio shortcuts and how they're better than the rest. Before actually jumping into the shortcuts, let me share with you a couple of plugins that will help you and your pairing peer learn more shortcuts, resulting in an increased productivity! Key promoter – Show the associated keyboard shortcut for actions done with the mouse. Presentation Assistant – Shows the shortcut used in a manner similar to a Toast. Show intention actions ⌥⏎ (Mac) | Alt+Enter (Win/Linux) This might be the most used shortcut in IntelliJ / Android Studio (at least by me). It's highly context-aware, giving you a wide range of helpful options, from quick fixes to compilation errors, to improvement suggestions of your code. I could give you a ton of examples of situations where this shortcut might be used, but I will name just a few of my favorites: adding a final field as a constructor parameter or binding a constructor parameter to a field, flip , , invert if condition. Don't be afraid to use it, and try it everywhere, you will be surprised what kind of suggestions it might give you. Add selection for next occurrence ⌃G (Mac) | Alt+J (Win/Linux) Given that you have selected something, it will add the next occurrence to the selection, also adding a cursor there, similar to how Sublime's multiple selection works. This is particularly useful when you want to apply collectively an operation to similar elements in a group. I mostly use it to alter field's modifiers and to split long lines in situations such as method declarations or calls with many parameters. If you go too far, you can use \"Unselect occurrence\" – ⌃⇧G (Mac) | Alt+Shift+J (Win/Linux) – to step backwards. Completion → SmartType ⌃⇧Space (Mac) | Ctrl+Shift+Space (Win/Linux) Some of my colleagues were mindblown by the power of this shortcut. Basically, it's similar to basic code completion ( ⌃Space on Mac, Ctrl+Space on Win/Linux), but it filters the list of results based on the type of the expected object. This will save you time showing only relevant suggestions, instead of going through an exhaustive list of possibilities. Even more, if you press the key combination a second time, it will go recursively one level down, giving chained expressions as suggestions. Also, if there is only one possible match, it will complete the statement with it, meaning that it will put the semicolon for you. Additionally, if you are in a method call, and in that context there are variables matching all the expected types and/or names of the method's parameters, it might give you a suggestion with all the required parameters at once. Pretty neat, isn't it? Happy and productive programming!", "date": "2016-07-22"},
{"website": "Novoda", "title": "Top 3 Android Studio Shortcuts - Ataul", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/top-3-android-studio-shortcuts-ataul/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Ataul will talk about his top three Android Studio shortcuts and how they're better than the rest. Cycle between open tabs Linux: alt + left AND alt + right  \nOSX: cmd + shift + [ AND cmd + shift + ] This shortcut switches to the next (or previous) open tab in the current editor. I know a lot of folks prefer hiding tabs and using ctrl + tab / ctrl + shift + tab , but I'm more partial to spamming the left/right shortcuts. As an added bonus, the OS X shortcut also works in Google Chrome to switch between tabs! Complete the current thing Linux: ctrl + shift + enter\nOSX: cmd + shift + enter This shortcut completes your current statement in the shortest way and moves the cursor to the line you need next. Most often, it's useful for just added braces and moving the cursor inside the braces, but when you couple it when the autocomplete for variables, the time saved adds up! Choose button in open dialog alt + [underlined letter] This shortcut lets you choose one of the options of an open dialog. When you press alt , an underline appears below the letter you need to press to perform that action. In the above example, you can see that the a in \"Select all\" and U in \"Unselect all\" are underlined, showing us that we need to press alt + a or alt + u to choose those options. There is one case it doesn't work. I always forget which case it is. But when it happens, it's very annoying.", "date": "2016-07-22"},
{"website": "Novoda", "title": "Top 3 Android Studio shortcuts - Xavi", "author": ["Xavi Rigau"], "link": "https://blog.novoda.com/top-3-android-studio-shortcuts-xavi/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Xavi Rigau will talk about his top three Android Studio shortcuts and how they're better than the rest. First of all let me start saying that I like having as much space as possible for the editor in my IDE, so I don't use toolbars at all. This is how my Android Studio looks like: All purpose shortcut ⌘ + ⇧ + A This is the best shortcut by far (at least for me) because it's basically a shortcut for shortcuts since when you trigger it, it shows a search bar that lets you perform any available action in Android Studio. Just start typing what you want to do and it'll suggest all the things that match what you type. And this is the main reason why I don't need any toolbar. It could be improved if you could use numbers [1-9] to select one of the suggestions, since the IDE does this for other dialogs, but for me it's not a big deal. Generate... ⌃ + ⏎ The 'Generate...' shortcut shows you a list of options for things that the IDE can generate for you (this is mainly code). This is great for generating Constructors, implementing toString methods, getters, test methods, etc. The only downside I find to this shortcut is it doesn't really work for Android XML files, but works well for source code and even gradle scripts. Run or Debug a configuration ⌃ + ⌥ + R or ⌃ + ⌥ + D This shortcut shows a list of run configurations for the current project and lets you select one of them to run it. This includes tests, Gradle tasks and any configuration you've got set up, and it even lets you use numbers to select one of the configurations! I find the 'Run configuration' shortcut very useful because it lets you switch between running the project and running the tests very rapidly and easily. I hope you found this list useful!", "date": "2016-07-22"},
{"website": "Novoda", "title": "Top 3 Android Studio shortcuts - Flo", "author": ["Florian Mierzejewski"], "link": "https://blog.novoda.com/top-3-android-shortcuts-flo/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Florian will talk about his top three Android Studio shortcuts and how they're better than the rest. Surround with Ctrl + Alt + T Surround a statement or a code block with if , else , a cast or even live templates! No more fiddling with parenthesis for ever. Make sure the area you want to operate with is selected (using the next tip for instance!) Incremental expression selection Linux: Ctrl + W / Ctrl + Shift + W OSX: Alt + ↑  / Alt + ↓ Extends / shrinks the selection. No more imprecise mouse selection! Very powerful used in combination with other refactoring shortcuts such as Ctrl + Alt + M to extract a new method for instance. Completing statements Ctrl + Shift + Enter Complete all the things! Insert smartly (most of the time) parenthesis and colons, no more painful going back and forth to the start and the end of the statement to make everything right. Sometime mess up the statement. I also wish there were a possibility to both complete a method name and import it statically.", "date": "2016-07-19"},
{"website": "Novoda", "title": "Top 3 Android Studio Shortcuts - Ryan Feline", "author": ["Ryan Feline"], "link": "https://blog.novoda.com/top-3-android-studio-shortcuts-ryan-feline/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Ryan will talk about their top three Android Studio shortcuts and how they're better than the rest. Change Signature cmd+F6 Pressing cmd+F6 on a method will open a dialog to change that methods signature. Safely refactor a method signature, previewing all changes of client calls before applying a change. Additional shortcuts in dialog: alt+up : move parameter up alt+down : move parameter down cmd+n : add a new parameter backspace : remove parameter alt+r : apply refactor alt+p : preview changes Run Configuration ctrl+alt+r This shortcut brings up a simple run dialog. Useful if you like to code in distraction free mode, no need to exit distraction free to select / edit a specific run configuration. To open the debug variant it is ctrl+alt+d. A downside is you can't attach the debugger this way as far as I can tell. For that, I need to search for the action. Navigate between files ctrl+tab This shortcut will show a small dialog to navigate  between recently opened files. Great for distraction free mode, no need to have more than one window on the code. Plus no need to open a side panel to navigate between files. I find this a bit cumbersome but ctrl+tab+shift will navigate to the previous file.", "date": "2016-07-19"},
{"website": "Novoda", "title": "Top 3 Android Studio Shortcuts - Paul", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/top-3-android-studio-shortcuts-paul/", "abstract": "Everyone likes to increase their productivity and everyone has their own favourite productivity hacks, but not everyone agrees! In this blog, Paul will talk about his top three Android Studio shortcuts and how they're better than the rest. Goto Next Error F2 Pressing F2 will move the cursor to the next compilation error in the class file. If there are no compilation errors it will move to the next warning. This is great when you know there's an error but you aren't sure where, or you can see there is a red error on the right side of Android Studio and to save you scrolling just hit F2. If you want to be extra productive you can also use SHIFT + F2 to go to the previous error, really great for navigating up and down. Start New Line SHIFT + ENTER This shortcut will put a blank line directly below where your cursor is positioned. The benefit being (for me at least) I no longer either have to grab the mouse or do the awkward dance of hitting END then ENTER to create a new line (without breaking the current). There is a really complicated alternative for creating a new line above where your cursor is positioned, I don't want to tell you it because it's much easier to press up then use this key combo. Highlight more key shortcuts ALT (when in a modal dialog) This shortcut is specifically when you are in dialog boxes (or many places with multiple options). Press the ALT key and it will put a little underline under other shortcuts that are available. Like in the GIF it shows R is available to Refactor, so if I press ALT + R it will refactor, or ALT + P to preview. Benefit is not having to touch that dahm mouse! A related shortcut and a bit of a hack (because I am never sure when to do it) but it is to hold CTRL when pressing Enter to confirm a dialog. Sometimes ENTER just doesn't work!", "date": "2016-07-19"},
{"website": "Novoda", "title": "The Novoda logo", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/whats-in-a-name-the-novoda-logo/", "abstract": "XP, Agile and ambition are the founding principles of Novoda. Eight years ago we were certain and remain convinced that all three are required to deliver the products of the future. We are a passionate team who practice Software Craftsmanship and experiment with extending the notions of XP into workplace best practices, communication and community outreach. In August 2008, Nokia E71 was the best selling phone, Android remained in Beta, and phones running Android were not yet publicly available. It was against this backdrop that Novoda focussed on satisfying a yet-to-emerge appetite for high-quality mobile software development. With only our fundamentals to guide us, we created a design brief which had the following excerpt: The brand seeks to become a mark of excellence within the technology and wireless sector. These words should be in consideration in ideas interpreted from the logo: Mobility Connected Geo location Social interaction Openness Transparency After an exhaustive search we found Roy Smith, a designer who was very patient in working with us to arrive at a trademark which described these principles. Roy’s portfolio demonstrated his ability to consistently achieve twofold meaning, conveying value in logo design aesthetically as well as through the subconscious. Here are some of the early rough sketches: Literal interpretation of our explanations served as a starting point. ‘Mobility’ was illustrated through radio waves and mobile antennas all incorporating ‘Novoda’. This was a good start but too limited in future scope, so we expanded the remit to more abstract avenues. One additional criteria was that whatever we arrived at had to work on small and large screens, with one or multiple colours, and for both digital or printed media. We then took another tack sidestepping these problems by first exploring typefaces. We agreed quickly on a typeface which seemed simple, but were still stuck on how to convey the  connected aspect. At the time our thinking revolved around geo-location (a good way to consider the empowerment of mobile) and so soon waypoints started appearing in our sketches. The beginnings of something unique were emerging but the waypoints weren't integrated and the N felt secondary. It wasn’t until we’d explored a full range of negative space possibilities that we arrived at the answer. Once you see it, you won’t be able to ‘unsee’ it. The journey of our logo is a story of simplification which like all evolutions seems obvious only in retrospect. But like writing a letter or creating the best software, it takes time to arrive at brevity. Our logo has held strong interpreted across mediums, small and large screens, online and even celebrated within printed collections ! Novoda are technologists with ambitions of creating our interconnected future. We take great pride in the challenge and opportunity of lending our technical knowledge to such a diverse range of industries. Our logo represents our journey.", "date": "2016-07-08"},
{"website": "Novoda", "title": "React Native: Is it the end of native development?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/is-it-the-end-of-native-development/", "abstract": "People see React Native as a write-once-deploy-anywhere solution, potentially speeding up development for applications that need to be written for both iOS and Android and making it possible for Web Developers to easily write native applications, but is React Native the best choice for your next project? What is React Native React Native is a native version of the popular web library of the same name and its main purpose is to bring the power of React to native development. React Native components are pure, side-effect-free functions that return what the views look like at any point in time. For this reason it is easier to write state-dependent views, as you don’t have to care about updating the view when the state changes since the framework does this for you. The UI is rendered using actual native views, so the final user experience is not as bad as other solutions that simply render a web component inside a WebView. How we investigated React Native The best way to evaluate a new technology is to use it. We gathered together a team of four developers native in programming iOS and Android, to plot, scheme and research React Native. Here was our action plan: Create a Twitter Client, running on both iOS & Android Measure code re-use Evaluate the look and feel of the final app Investigate 3rd party library maturity Interview current react native developers Read official docs Review source code of core features The source code of the Twitter Client we created, both for iOS & Android, is available in our Spikes repo: https://github.com/novoda/spikes/tree/master/ReactNative/ReactTwitter Costs of React Native Project Setup The documentation about setting up the testing environment is very poor, and mostly outdated. Consequently, we spent 4 days until we were able to run all of our tests successfully. While it’s true that we didn’t have any prior experience on running JavaScript tests, we found the amount of effort to set up the test environment outrageous. There were other setup quirks as well, mainly caused by incompatibility issues with different versions of the framework. IDE The recommended IDE is Atom , with a plugin on top of it, Nuclide , which adds support for React Native development and for its recommended static analysis tool Flow . While it might be a good editor in general for JavaScript development, we found it to be far behind compared to IDEs we are used to, like Xcode and Android Studio. For instance, there is no support for even the most basic refactors, this could potentially slow down the development speed. Speed As native developers we found the learning curve of React Native pretty high, as it may take up to a month until one could start feeling comfortable developing with this framework. For someone who is already familiar with JavaScript development, Redux , flow layouts and maybe even React, it is much faster to get up to speed with React Native. Native coding There are certain features that cannot be implemented solely in JavaScript, native code is needed for: push notifications deep linking native UI components that need to be exposed to React. Overall approximately 80% of the code is shared between iOS and Android and written in JavaScript. Hot Reloading Hot Reloading is similar in concept to Instant Run on Android. Every time a source file was saved, the changes were deployed immediately on the device where the app was running, thus greatly expediting the feedback loop. Although it works better than Instant Run, it still breaks from time to time, requiring a restart of the app. 3rd Party Libraries We cannot deny the fact that many times we have to resolve the same problems as others did before us, and that’s why we decide to make use of third party libraries, so we don’t have to reinvent the wheel. Whereas there are plenty of libraries to chose from on iOS and Android, we cannot say the same about React Native. There are definitely a considerable number of libraries, but as we experienced, some of them are not always working as expected. Release cycles One more thing worth mentioning is that React Native has a two week release cycle . While it is good that they bring more features and push the framework towards maturity, often they bring breaking changes as well, maybe in a “move fast and break things” fashion specific to Facebook. These changes sometimes where a burden to overcome, as we had people spending a lot of time fixing things when upgrading the framework. But we can’t deny that this isn’t expected, as the framework has yet to reach a stable status, still being under heavy development. Cost Saving - the numbers With these pros and cons, the state of the tools, the lite documentation and uncertainty of libraries as it is in the present, we feel that by using React Native you can actually save some time compared to traditional native development. In particular, according to the developers we have interviewed, on real projects one could save around 30% of development time. Of course this is just a high level estimate and depends on multiple factors, the most relevant is to have person with web-development knowledge in your team, as discussed previously on this article. Why Choose React Native At this point, you might think that React Native is another “write once, run anywhere” framework, like Titanium or PhoneGap , but you would be wrong. As Facebook very clearly states on their blog post, they acknowledge the differences between platforms, so instead, their goal is to bring the paradigms of React, which is very successful on the web, to native, while having the same set of engineers working on whatever platform they choose. They call this approach “learn once, write everywhere”. How React Native actually works is by embedding the JavaScript files in the app, and running them locally. But also, one could have these files remotely on a server fetching their latest version when the app has connectivity. This would allow updating an application very rapidly, without having to go through the app store’s review process. There are also third party services that offer such solutions, and using them potentially could mean that for smaller applications that don’t have a lot of data to display (e.g. festival apps), a backend isn’t needed anymore. Most of the coding is based on JavaScript and the paradigms in React. Thus, it makes more sense that a typical team working on a React Native application is composed mostly of JavaScript engineers, because ideally those will be already familiar with the tools and paradigms used by this framework: React, EcmaScript, Redux and Flex layout. Limitations Documentation Starting to work with a new platform like React Native, one of the first things we did was to explore the available documentation, both official and from the community. We found that, while Facebook is putting effort in keeping it up to date, a good amount of the components are not well documented for the current release of React Native. This is even more evident if we consider community-created articles, given that these are often obsolete after a few releases of React Native. Core React Native library When the documentation wasn’t satisfying enough we tried looking up things in the React Native source code, however we found ourselves disappointed by the poor quality of code base. Probably in a rush to push more functionality to the framework, they also sometimes overlook how clean the code is. Speaking about the core React Native library, we found many of the main components being still in a Work-In-Progress status. The official documentation regarding how to implement some key features (like navigation) changed the suggested component in different releases of React Native. The migration from one to another of these components is not a straightforward task and it requires some not-negligible work. Third-party components Of course it is possible to use third-party components, but the selection is not comparable with the number of community-generated libraries that we can find for iOS or Android. A big problem with these components is also the fact that the compatibility and support is not guaranteed with future React Native releases. Native SDKs update When the iOS or Android SDKs are updated, it takes some time for React Native to integrate these newly introduced APIs into their core library. The React Native team is pretty fast to update in order to allow developers to use new APIs, but the priority is given by the amount of requests that every API gets. It can happen then, that some new features will be included in the next React Native release, while some others are not going to be included at all. Conclusions We found that a good use case for React Native can be found in applications that need short time support, for example an app for a one-shot event like a concert or a conference. These kind of apps could also benefit from the fast release cycle available if dynamically loading part of the application logic from a remote server. However after our month-long investigation we can definitively say that React Native is not a mature or stable platform yet. For this reason, and what we highlighted during this post, we think that React Native is not ready to be used in a long-lasting supported application. This does not exclude the possibility for it to be used for other kind of apps, having the right team including the right level of JavaScript expertise. React Native seems to be suitable for apps with simple UI, simple animations, and not long life or performance-critical apps. The demo app we created is completely open-source, and it's available at https://github.com/novoda/spikes/tree/master/ReactNative/ReactTwitter For more Novoda open-source project, have a look at http://novoda.github.io/", "date": "2016-07-05"},
{"website": "Novoda", "title": "Five reasons why Android developers should start developing for the internet of things", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/five-reasons-why-android-developers-should-start-developing-for-the-internet-of-things/", "abstract": "The “internet of things” (IoT) is simple:  everyday objects that have internet connectivity and sensors that can gather, transmit and receive information. But why does this ubiquitous functionality affect you as an Android developer? Just as smartphones are being built with more and more sensors - for everything like movement, sound, temperature and touch - so the devices around your home will slowly begin to have the same. Soon, your fridge may have a barcode scanner so it knows when you run out of milk. It could then notify your car, which could in turn remind you if you are near a supermarket to pick up some semi-skimmed. Granted, we've been talking about commercial ubiquitous computing for over ten years now but with more and more big companies getting involved, the range and choice of developer-friendly kit out there is vast. The barriers to entry are now low enough that you can pick up a few bits on Amazon and be coding your own omnipotent fridge in no time. (Wondering what you need to get started? I will show you a few choices at the end of this article.) As an Android developer you are already coding for an IoT enabled device -your smartphone - but why should Android developers start developing the rest of the internet of things? 1- It will affect your job anyway Android applications don’t do much without external input, they need to talk to things. With more and more devices becoming internet connected, the apps you write will communicate with them. You might think you get away with this because the devices talk to the server and you just obey the server. However, security and performance being high value requirements for mobile, potentially having your Android apps talk to IoT devices on local networks or pairing with them one on one are highly crucial features. 2- Barriers to entry (skills needed) are lower than ever In the past to get started with the Internet of Things, you’d need knowledge of hardware protocols, networking, and board manipulation, amongst other things. While this knowledge is still valuable, kits now exist built specifically for beginners that aim to remove a lot of barriers. Super early adopters have created communities to support this and there is now a vast array of knowledge, blog posts and videos available on the internet to fall back upon. The IoT kits available (usually a starter board) also come with starter guides and walkthroughs allowing you to create a hello world morse code flashing LED in no time. 3- Early adoption allows you to mould and choose the direction Understanding a technology early can help you become the 'go to' person at your company, you can have a big impact on the way the company technology is shaped. Early adoption can be hard, but like I said in the last point it’s not super early adoption; this adoption is more in terms of your company (or you can think of it as mastery of your own career), so you still have the internet to reference. Learning the hard way gives you a solid foundation of knowledge to pull from and when the technology evolves and advances you can understand why; because you were there pulling your hair out at those crazy nonsensical (at the time) bugs. You can have a direct impact on the direction and strategy and choice of IoT technologies that are used. 4- Google is doing it! You know the old adage \"You wouldn't jump off a cliff just because everyones doing it\" but Google getting into IoT means they believe there is a product space. Google have announced Brillo and Weave - low level technologies for communicating between devices and sensors - in an attempt to offer an standard for IoT. While many manufacturers are building IoT devices with their own protocols and stacks, this can mean devices incapable of communicating with one another. Google is pushing Weave (and Brillo as a platform) to be a standard protocol to allow communication and information sharing across manufacturers. This would mean a unified platform, a better experience for consumer and, hopefully, greater adoption. As a developer, this means a fun, potentially lucrative new product space. 5- You should always be learning Knowing what's around the corner keeps you on top of your game. Hey! Even if the internet of things doesn't work out you will have learnt something. If the  next big thing is Project Ara 1 instead, you’ll still be able to apply what you have learnt about dispersed communication in IoT to Ara modules. As a developer, learning something new is never a bad thing and can only make you better at your job. Now that you are convinced , how do you get started? First off you need a computer to code, compile, and ship your software to an IoT device. For different IoT devices different computers and operating systems will be the recommended system to use (i.e. Windows, Mac, Linux).  In my experience a Linux machine has the most compatibility across IoT solutions. 2 Next you need to select your weapon of choice. This is the board you will be working on. It's the underlying hardware that will run your sensors and gather your information. This blog post isn't about comparing these, so below is a list of five possibilities: Intel Edison Board - Brillo C++ Qualcom Dragon Board - Brillo C++ Arduino Board - C++ Raspberry Pi - Scratch, Python (many other languages supported) BBC Micro - Python, Javascript, drag & drop Once you have a board you need to connect it your computer (so you can push code to it). Some boards come with a few built in sensors, but you also need a way of attaching more sensors to the board. The five things you usually need to get started are: Power cable USB cable Breadboard Breadboard wires sensors, LEDs, resistors, buttons For more reading about board peripherals, what they are, and how they work. I cannot recommend https://learn.sparkfun.com enough it is a great resource for all knowledge hardware related. Conclusion This blog post tried to show the potential value in working on IoT and that the entry barrier for Android developers wanting to learn about it is very low. I hope as an Android developer you can see the benefits in understanding and participating in creating hardware and software for the Internet of Things. Go forth and build! 1 : Google's modular phone, where you can strap together different modules, i.e. camera, extra storage 2 : You don't have to code on Linux, you could just code on your machine and send it to that box to run and build on the connected devices. Virtual Linux machines sometimes work, but you can get yourself into a confusing death spiral when one of your USB devices is only recognised intermittently, this is why I'd recommend a dedicated Linux machine.", "date": "2016-07-01"},
{"website": "Novoda", "title": "Eurovision!", "author": ["Qi"], "link": "https://blog.novoda.com/eurovision/", "abstract": "Lots of news regarding Europe at the moment! Novodans are part of communities in London, Liverpool, Berlin, Barcelona and represent a diverse range of people from across Europe. We look forward to keeping it that way. We wanted to share some love in wallpaper form. Desktop Wallpaper 2560x1440 Phone Wallpaper 1920X1080", "date": "2016-06-30"},
{"website": "Novoda", "title": "Liverpool MakeFest, See, Say & … Superwoman?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/liverpool-makefest-2016/", "abstract": "MakeFest Liverpool is a family-friendly event celebrating low level technologies, hardware, hacking and making! We had a stand at the 2016 event and built two interesting prototypes to try and engage those attending and learn more about the hardware space for ourselves. Here we review what we did and what we learnt. Five of us attended from Novoda, it’s a crazy busy day and having a group of us allowed 2–3 to man the stand whilst the others could explore or just take a breather from all the conversations. Most attendees of MakeFest are families with children 7–17. This year was even more unique, since the same venue was running ComicCon on another floor and so every now and then a Darth Vader, Naruto or Superwoman would attend our stand. MakeFest for Novoda has two aims: We love helping the community, and seeing young people so engaged with technology is a real inspiration. Helping spark creative ideas and showing that technology is easy to get into is a target for what we demonstrate. Secondly, we ourselves are always learning and trying to push the boundaries of our knowledge. Working with hardware is not a daily occurrence yet for the mainstay of Novoda and so MakeFest really pushes us to learn new things. Our first project was based on the idea of bringing technology into an every-day, not-so-smart object, so we created a smart mirror. Turned off, it just reflects the image of the person in front of it, but when turned on, more information can be seen as an overlay on the reflected image. In a typical domestic environment, a smart mirror would display useful information, like the weather forecast or the agenda for the day, but for MakeFest we considered the expected audience, especially the youngest part of it. For MakeFest we used face recognition to detect when someone was looking at the mirror and displayed different animations and smiling images depending on the happiness of the person in front of it. More details about how to build your own Smart Mirror will come in a future blog post, so stay tuned. Our other idea for MakeFest was based around interactivity. We wanted people to come and engage with the stand but also we wanted to give away prizes! Nothing is better for winning a prize than a game. We made a Simon Says game that would display a sequence of colors on the screen and then this had to repeated on the attached otg usb cable retro game controller. We had high bar ideas for making all the hardware ourselves and connecting the controller to the game via the cloud, but time caught up with us and we went with our minimal viable product approach. One big learning from MakeFest was color blindness. Every now and then a child couldn’t play the game because they couldn’t distinguish the colors. For the future, and if we were to do this again, adding a color blind mode that switched the colors to something with more contrast would really help. We do this for our client apps in our day job and I feel bad that we forgot to consider it. Here is a great reference website for selecting contrasting colors. Our game winners got a SparkFun Inventors Kit each, which has everything needed to get started programming and hacking hardware bits together. We hope you enjoy them and let us know when you make something awesome. Yet more proof that creativity and science go hand in hand! It was an interesting and exciting day and it was refreshing to see so many children and families taking part. Best lesson learned: do not challenge an 8 year old at ‘Simon Says’! Oana MakeFest was a fun experience. All the stands were displaying some very interesting and especially motivating projects, and ours created a lot of interest as well. It was very nice to see children playing hard with our game in order to win one of the prizes and our smart mirror aroused interest from both kids and adults, playing with funny faces in front of it. Daniele I loved it. Science, comics, and children all gathered in same place. Our magic mirror was so amazing; ‘Can I see a slim version of myself?’ asked one dad. :-) We met parents and children that competed against each other on our game and were extremely surprised to notice that their children were better than them! The vivacity and learning ability of children is inspiring. As Steve Jobs said: Stay hungry, stay foolish! Hawazine Events like MakeFest are great fun. They are full of passionate people being proud of their work and ideas, which is really inspiring. We had a chance to chat with other Makers about their inventions and play with their amazing gizmos. I am really glad that people enjoyed our inventions as well and hopefully inspired some of them to start playing with hardware too! Looking forward to next year’s MakeFest! Alex It was a great day and I was really impressed how all of the children enjoyed playing our Simon Says game. It was interesting to see how different age ranges approached the game in terms of speed of learning, rule comprehension and the average score per group. If I was doing it again I would add analytics to the game so that we could get further insights! Paul We cannot wait for 2017, so many ideas! The interactivity of our creations this year went down really well. Next year we want to go bigger and better, until then. In brightest day … in blackest night, no evil shall escape my sight! Let those who worship evil’s might beware my power—Green Lantern's light!", "date": "2016-06-28"},
{"website": "Novoda", "title": "Is the new Firebase production ready?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/bonfire/", "abstract": "One of the big announcements of Google IO 2016 was Firebase. No longer \"just\" a database, the Firebase umbrella now includes integrated Analytics, Crash Reporting, Push Messaging, Dynamic Links, Storage, Hosting, and more. Is this new platform ready for all your projects? Should you spend time learning everything about it? We like to keep on top of new technology at Novoda, so we decided to dedicate some time to explore the new Firebase. With a small team of four developers (two for iOS and two for Android) we took the idea of a chat application shown in most sample code and expanded it into a more feature-rich example. Today we want to share with you our findings and the demo app that was built during this exploration. Say hello to Bonfire! Android, Google Play and the Google Play logo are trademarks of Google Inc. The iOS app hasn’t made it through the app review process, but you can sign up to our beta A sample app We are releasing the full source code for Bonfire , together with this blog post. Play with the sources and experience what can be quickly built on top of the Firebase stack in a real-world app example. Because most sample apps out there are focused only on showcasing how to use Firebase, they often end up being too simplistic to be a real benchmark. We wanted to create a real example of what an application using Firebase might look like. Bonfire features: Real time chat organised into channels Authentication using Google Sign-In Channel names limited to one emoji on database level Public and private channels Any user can create a channel Channel members can add and remove members of a private channel Remote configuration of the order of channels in the channels screen Invite users to the app with a customised welcome screen Firebase as an architecture One of the most exciting things about Firebase is the ability to use it across multiple platforms: Android, iOS and web. At Novoda we’ve been working to create apps with a consistent architecture across iOS and Android. Firebase fits very well with that approach. Despite some minor differences between the APIs, having a common basis for discussing our code encourages and enables cross-platform collaboration. Database Firebase started out as just a realtime database. Now this term is collectively used for the whole suite of tools. Though its name has slightly changed, Firebase Realtime Database (as it’s now known) remains a very impressive piece of technology. Firebase Realtime Database is a cloud-hosted NoSQL database, and data is stored in a JSON structure. You can view the data in an online dashboard, which is really useful during development. Additionally, offline modifications and data synchronization are handled automatically across multiple platforms. This gives you a fully functioning key-value store with data sync that requires only minimal setup. #####Firebase Realtime Database Dashboard In order to make your data structures scale it’s a common practice to create indexes to query . The issue is that at the moment this means executing two queries separately. One to retrieve the index and a second one to read the data at the index. As such, it would be great to have the ability to declare a composition of those two queries in a single operation. An example found in Bonfire would be to query the list of channels a user has access to, and for each channel ID retrieve the channel info. We must handle this kind of query ourselves at the moment, which adds complexity in our code. This also makes it less efficient than if it were part of the Firebase query language. Another downside is the lack of control over the offline and synchronisation behaviour. As it stands now there is no control over conflict resolution when a sync happens. The ability to define a javascript function that is used to resolve conflicts would give back control to the developers and allow for more confidence in the service.. Access to key nodes is managed through a set of ”database rules” , which allow for powerful read-write access control and flexible data validation. This is not only crucial for security reasons; it is also very useful whilst developing your applications. Since in a Firebase backed app your database is essentially your API, you want to be able to ensure it is used consistently on different clients. With database rules preventing any not explicitly allowed interaction, you’ll be immediately notified if you try to write to the wrong place by mistake. This means you won't risk writing half your messages to messages and the other half to msgs because of miscommunication. #####Firebase Realtime Database Rules The database rules come with some limitations too. They can get quite complex even for our simple chat app. Luckily, the Firebase console contains a simulator where you can test access to specific nodes of your database. The main issue with the database rules at the moment is the lack of feedback on the client side when a query fails validation. The fact that the only reply when a read or write rule has failed is “Permission Denied” is a great security practice. However, if the write permission is granted, a validation rule failure still sends “Permission Denied”, and this is often too vague to be helpful. Being able to get some form of feedback from the data validation on the client side would be great. A possible solution could be adding tags with an error code to boolean expressions in the validation rule. Those codes could be sent alongside the error to the clients. Analytics Firebase Analytics automatically tracks a bunch of information about your users, such as location, device information, user engagement, and more. On top of that, there are multiple predefined events that you can use. #####Firebase Analytics Dashboard You can also define your own custom events. The difference we noticed between the two is that custom events are not displayed with all their details in the Firebase console. This means that you can't view parameters of your custom events or filter based on them. For instance, we wanted to track the average length of messages sent in the app. We could log that a message was sent, but could not see the 'message length' value in the Firebase console. However, there is a remedy for that: Firebase provides a handy integration with BigQuery, where you can get the full analytics data, including any custom parameters you have added. This particular feature is worth drawing attention to, since BigQuery allows in-depth analysis of both your analytics data and also external data in Federated data sources . This has tremendous potential for data analysis, custom dashboards, and visualisation of anything one can think of. Just look at what was acheived in this awesome Google I/O talk . Crash reporting Setting up Firebase Crash reporting is really simple and the basic setup automatically logs all fatal crashes without you having to do anything. Apart from that, it allows you to manually log non-fatal errors together with some custom additional information. If you are looking for more in depth analysis of the crash reporting go read this great post . #####Firebase Crash Reporting Dashboard Authentication Firebase integrates very easily with popular authentication methods such as Google Sign-In, Twitter and Facebook. The authentication status can easily be used as part of the database access rules. For example, the following code defines a user profile that only authenticated users can read and that only the owner can modify. \"$user_id\": {\n    \".read\": \"auth != null\",\n    \".write\": \"auth.uid === $user_id\"\n} #####Bonfire login screen on Android Dynamic links Dynamic links are pretty cool! If your app is installed, they will deep link into it. If it’s not, they will take you the relevant store (App Store or Play Store). After the app has been installed, they’ll pass you the link. This has multiple uses beyond analytics tracking, as it allows you to deep link immediately after installation or provide custom onboarding paths into the app. Crucially, they are cross-platform, which means that you can generate links from iOS or Android that will deep-link into any platform. #####Opening invite link when the app is not installed Firebase also contains Invites, which are built on top of dynamic links and aim to make it easy to create invitation messages. This feature relies entirely on Google Sign-In and feels totally out of place on iOS. Therefore, to keep the UX as consistent as possible across both platforms, we decided to only use dynamic links. Cloud testing Unfortunately Test Lab features are only available for Android. Regardless, the Test Lab features for Android are really impressive and still worth looking into. Firebase Test Lab for Android allows you to run your instrumentation tests in the cloud on a set of devices and API versions. On top of that, the lab contains a feature called Robo test . This could be described as a more intelligent monkeyrunnner which can traverse through your app automatically without you having to write the tests. It also generates a graph of your activities with the paths that it followed when moving between screens. #####Activity graph created by Robo For both instrumentation and Robo tests, Test Lab records a video of the screen, so you can watch the tests after they are finished. If all that is not cool enough, you can run the tests from Android Studio or from command line which makes it pretty easy to integrate with your favourite CI server. Server-side logic The ability to define some server-side logic would take Firebase from great to amazing. One of the biggest issues with a synced NoSQL database is that it requires each client to implement similar logic to handle things like indexes. Duplicating data is totally fine . Duplicating logic, on the other hand, not so much. What if you could write a single element to your database on the client, and update all the relevant indexes throughout the code in the cloud? There are of course caveats around offline capabilities when over-relying on server side logic. If used correctly, though, they could greatly reduce the amount of client-side code that needs to be written on each platform and the risks that results from those implementations getting out of sync. Drawbacks Firebase looks great to kickstart a new project with a small team, but it is not perfect just yet. Firebase depends on Google Play Services, which is a big issue if you have a project that you want to scale worldwide. A lot of users in China and other regions won’t be able to use your apps. In the Western world, Kindle Fire users will be excluded as well. In our opinion, this is a blocker to being able to use it as the base for most of our apps. It should be possible for Firebase to abstract away the dependency on the Play Services and allow support for a wider range of devices. As an example, a library called Firebase Job Dispatcher uses the abstraction of a Driver to enable replacement of the default GooglePlayDriver with a custom implementation. A similar approach for replacing Play Services (even if it meant writing a fair bit of logic ourselves) would enable developers to truly rely on Firebase for their apps without reducing their ability to reach developing markets. Google may never remove the dependency on Play Services as they have a clear vested interest in their adoption. However, we would like to believe that, like us, they can see the power and reach it would gain by doing so and that we will soon be able to use Firebase in any of our projects. Another blocker that prevents us from using Firebase on larger projects is the lack of configuration management across projects. You can easily create a project for production and one for development, but then you need to sync your database rules, manage multiple URLs, etc. ( cue headache ). The ability to have a production, development, or even short-lived testing environments where you could run tests would really take the platform to the next level. Firebase does have the ability to give users permission to only certain parts of the console, but being able to completely switch off some features (e.g. deleting all the data in the database with just a couple of clicks) would help a lot. Conclusion Even if Firebase might not be ready for your large scale projects, we think that it is worth diving into it right now. You can always start small with only a subset of the available features, like Dynamic links and Remote configuration. Just the combination of those two on a legacy project is already rather powerful, especially considering how easy to implement they are. In its current state, Firebase as a whole is great for small projects that are starting from scratch, and don’t mind the restriction to devices with Play Services. However, with a few adjustments from Google it truly has the potential to become the new backbone of most of the apps out there. Now go on and have some fun with it! Contributors: Ben Augustin and Jozef Celuch for developing the Android side, Yvette Cook and Dominic Freeston for the iOS part of this app, Joe Timmins for helping us keeping things testable and a nice fresh perspective and Qi Qu for the amazing design work and everyone else at Novoda always contributing comments, ideas and suggestions.", "date": "2016-06-23"},
{"website": "Novoda", "title": "Get Things Moving: Prototyping Animations in Playgrounds with Xcode 7", "author": ["Yvette Cook"], "link": "https://blog.novoda.com/prototyping-animations-in-playgrounds-with-xcode-7/", "abstract": "Working with animations in iOS is great, but when you're creating your own it can be time-consuming and tedious. This post shows how using Playgrounds lets you quickly prototype animations while skipping all the project setup. Before Playgrounds you'd have to re-build your project every time you want to check in on how the animation is coming together. The bigger the project the longer it takes. Luckily, there is a better way. Playgrounds are a lightweight interactive coding environment which allow us to experiment and explore Swift and the iOS SDK without the hassle of creating a new Xcode project. Using Playgrounds you can quickly create and polish animations and get quick visual feedback that you're on the right track. Check out the steps below to get set up and ready to go. Steps Create a new playground in Xcode by either selecting from the start menu (below) or hitting ⌥⇧⌘N. Import XCPlayground by adding import XCPlayground to the top of the file. This module lets you interact with Xcode from a playground. Add a container view - this will be your canvas for your animation. Set your containerView to be the 'live view' Open the assistant editor Start building your animation! This is a super-simple example, but you can build much more advanced animations using CoreAnimation and other APIs. You should see your animation run in the assistant editor: And that's it - a brilliant way to experiment with and build animations. You can find the full code here . Bonus Tip: To quickly re-run your animation you just need to change something in your playground. Adding a new line to the end of the file will do it. You can also access it in toolbar Editor > Execute Playground , or by pressing the play button on the bottom of the playground. However, my personal favourite is to add a new keyboard shortcut to Execute Playground . Do this in Xcode > Preferences > Key Bindings > Search for 'execute' . Happy animating!", "date": "2016-06-01"},
{"website": "Novoda", "title": "Google Agency Certified", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/novoda-are-google-agency-certified/", "abstract": "Novoda are proud to be one of four agencies in Europe to achieve Google certification for development agencies! In a world of design & development partners how do Novoda teams stand out? Since 2008 we have encouraged some of the world’s most active development communities such as Londroid , Droidcon and recently Swift London and NS London . Conference participation and Open Source contributions is part of everyones role and means we are always right in the thick of cutting edge mobile engineering. Novoda was born through attention to the  best quality of process and delivery and the frustration of working with companies who with no development experience or in-house capacity could still choose the working practices of experienced diligent professionals. Novoda teams are Agile in the real sense. Pro-active, critical, exciting and experienced people who enjoy working together as the very best mobile teams in London, Berlin, Barcelona and Liverpool. Our teams partnered with device companies like Sony, Motorola, Tesco & Silent Circle to ship innovative software which dares to ask how will this be different, better, more effective for it’s audience than what came before? What are the latest native platform features which make this a stand out experience? We helped Media channels like Channel 4, MUBI , Arte, The Sun and The Times upgrade their Apps from ’good enough’ into rock solid cornerstones of their customer’s habits. Or helping identify whole new audiences and opportunities for their business via mobile such as with CCleaner , Soundcloud . We are a proud bunch of Novodans who make these amazing products! Developers, designers, testers and product owners who have repeatedly had to figure out business value when dropped into crazy situations in random countries. Consistently against unfavourable odds, our great teams have assumed responsibility to conquer harsh deadlines of flashing golden master releases onto millions of devices in time for marketing campaigns to grace busses and TV screens. I speak for all the teams and to all our clients, now and in future to say it is an honour to be tasked with the responsibility of affecting millions lives through their most loved applications. Perhaps interested in working with us? Take part in our open source projects :) http://novoda.github.io/ Our official Google Agency Program video below:", "date": "2016-06-10"},
{"website": "Novoda", "title": "Leaner APKs with asset minification", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/leaner-apks-with-custom-asset-minification/", "abstract": "At Novoda, we always experiment new ways to improve the user experience of the apps we craft. Contrary to what one would normally believe, though, the user experience begins even before an app is installed. There are some important things to do to ensure your app is successful, starting at the very first moment a user sees your app on the store. Curate the listing for the app so that it makes it stand in the (over?)crowded market. Make the installation process as smooth as possible. First impressions do matter! We'll see how we changed our process for one of our apps to improve the user experience even before a user gets to its first screen. ##### A big APK has a bad install UX. The install phase alone can take minutes. Growing pains As apps become more complex and screens get more pixels, the APKs get bigger. Their size has been constantly growing over the years since Android was released, back in 2008. Nowadays, it's rather common to have apps weighing in at 15, 20 or even 50+ megabytes. Apps that do many things need more code, more libraries, more assets for all the features. Also, the ever growing screen density requires higher and higher resolution assets. You see where this is going. If you unpack an APK and look at what is taking up space, you can see that there are usually the same three items in the top spots: one or more classes.dex , the resources.asrc and the res folder. Sometimes the assets folder can contain a lot of stuff as well. Code in the dex file(s) has an impact on the final APK size, but reducing its size is a discussion for a whole other post. The compressed resources file ( resources.arsc ) size depends directly on how many resources and how many configurations are in the APK. Optimising it is a rather advanced topic; there are some tools that can help, if you're interested, but it's likely you won't need them. What's left then to optimise? Your resources. And what are the easiest ones to optimise? Drawables! Solutions? Errm, workarounds There are several ways to address the ever-growing size of assets in an APK. Some are very effective, but bring caveats with them. Others are quicker wins but might be less effective. Let's look into some of them. APK Splits One way to generate smaller APKs is to have the build tools create density-specific APK splits for you. It's easy to set this up and even create architecture-specific APKs if you have native libraries. The Play Store and the Amazon Appstore make it effortless to distribute the right APK to the right device, basically taking care of everything. You'll end up with a lot of APKs to test, maintain, version and archive. It can be very awkward to handle all of them, considering they grow in number almost exponentially with the number of split dimensions. Testing all the resulting APKs can be time consuming, especially if manual QA is part of your process. WEBP Another way to reduce the size of the APK resources is to use WEBP , an open-source image format created by Google drawing from the VP8 codec, that is best known for its use in the WEBM codec. Why use WEBP, you ask? Well, for starters it's much more efficient than the dated JPG for lossy images, but its lossless variant is also more efficient than PNG. $ ls -l image-test/\n39610 24 May 16:45 carota.png\n19031 24 May 17:09 carota-optim.png\n5868  24 May 17:03 carota.webp The same image in straight-out-of-Photoshop PNG, optimised PNG, and converted into WEBP using cwebp Google has done some rigorous research on the performance of lossy WEBP vs JPG and lossless WEBP vs PNG . So why doesn't everyone immediately jump on the WEBP bandwagon, since it's so clearly more efficient? Well, it's a two-fold issue. Firstly, it's a matter of compatibility. Lossy WEBP is only supported starting from Android 4.0 , and lossless WEBP support was introduced in Android 4.2.1. As an aside, WEBP images can't be used for launcher icons. The first requirement isn't too restrictive, as hopefully very few apps are still supporting devices on API 13 or lower. The API 18 requirement, on the other hand, is way harder to manage. That's unfortunate, given lossless WEBP is probably the most interesting format of the two for drawables, as PNG is the most used format for assets. There is not much support for the WEBP format in content creation software, either. Photoshop, Illustrator, Sketch, and most other applications don't support exporting to WEBP out of the box. You must add a step in your design pipeline to convert images using cwebp , imagemagick , a Photoshop plugin or some other tool. Designers aren't used to this format and its adoption might encounter some resistance. Using WEBP for images is still strongly recommended, if you can. Try using it at least for transmitting lossy images over the network, as they will allow either a much smaller size compared to JPG for the same quality, or a higher visual quality at the same size. This works down to API 14, which should meet most apps' minSdkVersion anyway. Vector Drawables Since a lot of the images used in apps tend to be simple, monochromatic icons, we can include them in vector format instead of raster. Lollipop introduced support for vector drawables, including animating them. Vector drawables are based off of a subset of SVG path commands, and as such are relatively straightforward for designers to produce (most tools support exporting as SVG, and there are conversion tools to get the final vector drawable). Since the release of Support Library 23.2.0 , we've even had an official way to use them in previous versions of Android (other libraries existed before for the same purpose, but none was as feature complete, nor officially supported). Unfortunately, a memory leak was quickly discovered that prompted Google to rush out a release to disable them completely. Google engineers have since restored access to the support vector drawables in 23.4.0 , but with some pretty strong limitations on how and when you can use them. Manually shrink drawables The last possibility is to use a tool to reduce the size of images before we add them to the res folder. One such tool is ImageOptim , which is rather efficient but only runs on Mac OS X. Several alternatives exist for Mac OS and for other platforms, both free/FOSS and paid. These tools run a series of algorithms in order to remove overhead from the files, optimise their compression levels, and even recompress the data streams for maximum efficiency. ImageOptim, for example, can turn a 24 bpp PNG into an 8 bpp one, strip all metadata, optimise the palette, and recompress the stream using Google's Zopfli algorithm. You'll remember that APK we looked into at the beginning of the post. That is the latest Google+ APK at the time of writing. It looks like no optimisation has been done on its drawables. How much would we save if we ran ImageOptim on it? And it wasn't even done crunching—it's a time-intensive task, as mentioned before. Pretty. Darn. Impressive. Why not just using the built-in AAPT PNG cruncher? Well, besides not supporting JPGs, the AAPT cruncher only does a few simple optimisations that in and by themselves won't likely save you much. Watch Colt McAnlis' talk on image compression at Google I/O 2016 for more details (link at the bottom). Build a process Running an image optimiser is yet another step required to deliver drawables. How does it fit into our process? Let's explore the alternatives. Whose job is it? If you decide to manually compress assets as they are about to be added to the app, it's very important that roles are clearly defined. Optimising images before dumping them in the APK is an important job, given we'll have to give up AAPT's built-in shrinker (more on that later). You must decided whether it's the designer's or the developer's job. You might wonder, why would anyone decide not to automate it? Well, if you only have a trickle of new assets coming into your app every now and then, automation might be overkill, or simply waste time (optimising a lot of images takes aLot^n time). In one of our projects, developers and designers sat down to decide how to integrate optimisation into the process, and designers volunteered to take care of this themselves. We have awesome designers! ✨ Continuous Integration An obvious idea is to automate optimising images. After all, you don't want to rely on humans to do repetitive tasks, right? Also, there's no risk someone forgets something, and nobody will have to think about it anymore. When could it be done? Setting up a cron on the CI server to compress all the assets, or do it on every CI build, might seem like a good idea. Unfortunately, it's often impractical to try to run image compression on all the assets you have in the app, as that process can take a significative amount of time. These algorithms often work on a \"try until you can't get any improvement, or I say you've spent enough time on it\" basis. Others, like Zopfli, are simply time consuming in nature. A full run might last anywhere from few minutes to several hours, depending on a variety of factors. That's why going the CI way is an option only if you change your assets very often, and you can spare the extra time it requires or have a very beefy CI. ⚠️ Optimisations aren't idempotent ⚠️ One very important caveat to both the aforementioned processes is that optimisation passes aren't necessarily idempotent . This means that if you run an image through an optimiser such as ImageOptim, and then run it through another one, the final image size might actually be bigger than the original one. This happens because re-processing images might end up undoing the optimisations done in the first pass, depending on the tools you use. Take for example AAPT, the Android APK Packaging Tool. It's the piece of the build toolchain responsible for packaging up APKs, handling resources, and so on. Unbeknownst to many, it also offers some basic crunching capabilities for PNG (and PNG only!) images. It's enabled by default, but you will want to disable it . Because of a regression (that should be fixed in the near future), it will try to recompress—badly—the images, effectively bloating them. In our tests, we noticed that leaving the aapt cruncher enabled can even make your final APK size bigger than the one you started with. Not good. Let's hope this issue is fixed soon, so we can even remove this step and make things even easier. To disable the AAPT cruncher, put this in your build.gradle file: aaptOptions {\n  cruncherEnabled = false\n} Note: this won't exclude 9-patch pre-processing Colt McAnlis goes a bit more into details on the matter on Medium . I will keep it short and simple, here. Conclusions Put your APK on a diet and you'll make people happy. Stop the seemingly unstoppable growth trend that APKs have seen for years, with very little effort. A great user experience begins with not having to wait for ages for your app to download and install! Acknowledgements I want to thank Wojtek Kaliciński from Google, for the great talk he gave a few months ago at Londroid, our monthly Android meetup in London . That talk contained a lot of super useful information that goes beyond the scope of this article, but also inspired me to look into shaving off the fat from our APKs. I recommend you watch his I/O talk, based on the posts on Medium and on the Londroid talk. Thanks to Mike Wolfson and Rui Teixeira for their help with editing and their suggestions. Further reading/watching Here's a few resources you can refer to if you want to go deeper down the rabbit hole: Wojtek Kaliciński's Lean and Fast: Putting Your App on a Diet talk at Google I/O 2016, and his articles on Medium Colt McAnlis' Image compression for Android developers talk at Google I/O 2016, and his articles on Medium", "date": "2016-05-27"},
{"website": "Novoda", "title": "This one simple change for better class naming will blow your mind", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/better-class-naming/", "abstract": "Single Responsibility [1] , Beware the Share [1:1] , Boyscout Rule [1:2] . These are some conventions that guide good practice in software development and I believe naming can benefit from these ideas and other practices. This blog post will discuss class naming using Model View Presenter as the example and show you one change you can do to make your code cleaner and more readable. MVP [1:3] is being used as one example of a design pattern that requires thoughtful naming, this article doesn't give you direction for what your Model, View or Presenter should do as behaviour, but we can all agree that you usually have an interface for each of these ideas. Infact the main benefits of this article can be taken to naming any grouping of classes and you can reinterpret these ideas for other class naming. Many people try to come up with a great name all at once. This is hard and rarely works well. The problem is that naming is design: it is picking the correct place for each thing and creating the right abstraction. This quote comes from an amazing eight part blog post [1:4] that talks about how naming can go through seven stages. As a TLDR; The stages include Missing name, Nonsense naming, Honest naming, Honest and Complete, Does the Right Thing, Intent, Domain Abstracted naming. I believe many people change variable names and go through the stages as discussed maybe not hitting every stage and maybe not getting to the end, but still, variables are renamed often. If these names move towards the right side of this scale, this is movement towards more expressive and readable code. I want to skip variable naming and move straight on to class naming. Variables get renamed an order of magnitude more often than Classes get renamed. There seems to be a bigger barrier to class renaming, in that it will likely touch more code, affect more people, ruffle more feathers, and so it is done less often. Let's drop this as an excuse not to strive for clean code and accept class naming as fair game. Consider a class that implements the interface TweetsView , one bad way of naming this implementer is like this TweetsViewImpl implements TweetsView . Impl ?? Impl !! Naming an implementation is an important opportunity to convey some meaning about what that specific implementation does. It's lazy to just go with Impl and not think about what your class does. My interface name describes the behaviour of my class. I can't think of anything more to add to that, I'll just use Impl. Next logical step people usually take is, they know Impl is bad naming, but still can't think of a name because the interface TweetsView seems exactly the right name; its a view of the tweets. They avoid the Impl code smell with something just as bad: DefaultTweetsView , SimpleTweetsView or flip it round TweetsView implements ITweetsView (this just moves the problem rather than resolves it, but a good hint of what is to come :-) ). You want your classes to be named after the behaviour they exhibit, following the Single Responsibility Principle you should know what this responsibility/behaviour is. There are two solutions here: Only 1 implementer of the interface? Get rid of the interface! Over abstraction is unnecessary and confusing. Understand the problem space and name explicitly according to behaviour TrendingTweetsView implements TweetsView Now thinking about class naming and interfaces from another perspective. If we are using MVP and have an interface for TweetsModel , TweetsView , TweetsPresenter . Then these three interfaces are actually intertwined and should be considering as a system. For example, someone new to the code base, seeing a class implementing TweetsView could think this is just some type of custom view (not MVP), they could then see the other code implementing TweetsModel and still consider this as some other pattern, it's not until they also notice implements TweetsPresenter that they'll put the three together. It would be nice if we could make this grouping explicit in the code, lets see an example. public interface TweetsMvp {\n    \n    interface Model {\n      ...    \n    }\n    \n    interface View {\n      ...    \n    }\n    \n    interface Presenter {\n      ...    \n    }\n} Awesome, we have declared our triad of interfaces under one common name. This fixes the problem of understanding the system. Any legacy developer reading this code or reading one of the implementations can see this interface comes from this triad and that it's clear intention is to be used in the MVP pattern. public class TweetsModel implements TweetsMvp.Model {\n  ...\n}\n\npublic class TweetsView implements TweetsMvp.View {\n  ...    \n}\n\npublic class TweetsPresenter implements TweetsMvp.Presenter {\n  ...    \n} When we implement the triad of interfaces it further backs up the knowledge that this class is being used in the MVP pattern. Further we have resolved our Impl naming issue. We can have our view of the tweets called TweetsView and this implements an MVP View of Tweets; implements TweetsMvp.View . This wouldn't be possible if you just called the individual interfaces Model , View and Presenter . Firstly you usually have multiple MVP interfaces in a codebase and so such generic naming would get very confusing very quickly. Secondly on Android View is already used for the UI system and so one of the two View classes/interfaces would have to be fully qualified which adds noise to the code. I hope you find the grouping concept useful even if my poor textual explanation doesn't get you, then the code example should make it obvious. Always remember to consider your systems as a whole, what parts of the system are interconnected and should these relationships be made explicit. Naming plays an important role in your codebase and getting it right is hard. If ever stuck, think where you are up to on the seven stages of naming; missing name, nonsense naming, honest naming, honest and complete, does the right thing, intent, domain abstracted and what other entities changing this name might effect. Sorry / Not Sorry for the viral headline. arlobelshee.com/naming-is-a-process ↩︎ ↩︎ ↩︎ ↩︎ ↩︎", "date": "2016-05-07"},
{"website": "Novoda", "title": "Approaching TDD Outside-in on Android (Pt. 1)", "author": ["Christian Panadero"], "link": "https://blog.novoda.com/approaching-tdd-outside-in-on-android-i/", "abstract": "An Outside-in Test-Driven Development (TDD) can be a challenge to implement. In this 3-part post series, we would like to share our experiences applying it to Android development and offer some practical tips for doing so yourself. In this first post of the series we will introduce the necessary concepts and present our broad approach to the problem. In most of cases, we tend to know the general components that compose a feature's design upfront. For this reason, outside-in TDD suits our process best as it allows you to develop faster than the 'baby steps' approach followed in the inside-out approach. In order to show how we adapted outside-in TDD to Android, and to make it easier to follow along, we'll illustrate the process using the \"Bank Kata\" that Sandro Mancuso uses in his \"Outside-in TDD\" screencast . (You don't have to watched this screencast to follow along but it's useful as a primer on the concepts.) In his screencast the problem isn't built specifically for Android, but we found it useful to have it as a focus so we decided to re-use it. The problem description is as follows: Create a simple bank application with the following features: Deposit into Account Withdraw from Account Show a bank statement The original kata provides a class with the following structure: public class Account {\n    public void deposit(int amount) {\n    }\n\n    public void withdraw(int amount) {\n    }\n\n    public void showStatement() {\n    }\n} And one constraint: You are not allowed to add any other public method to this class. But, we broke that rule It's worth mentioning that we broke this constraint somewhat to add another public method to the Account class. We did this to in order to attach the view to the account object. We needed to do so because we’ve chosen to use Android activities, and as we all know and suffer, they are instantiated by the system. Therefore, we can not pass the view through the BankAccount class constructor. [1] Extracting the acceptance criteria Let's start by describing the best-case workflow for tackling this problem. The starting point should ideally be a user story defining what needs to be done, who are we building it for, and why we are building it.[^n] The starting point should be a user story defining: What needs to be done, who are we building it for and why we are building it. The Bank Kata problem description above talks about three features so there should be three user stories. For the purpose of this post, we are going to focus on the show statement feature. The user story for the show statement could be written as follows: Story: Show account statement\nAs a user\nI want to be able to show a transactions details statement\nSo that I can easily check my account balance at any given time With the user story completed, we can now define the acceptance criteria - the series of results that required in order for the feature to be considered done. Once we have the user story, the next step would be to extract the conditions that the software must satisfy to be accepted, known as the acceptance criteria . Those acceptance criteria will define a series of results that must be validated to consider that the feature is done. The acceptance criteria that we have came up with for the “Show account statement” story are: Scenario 1: Account with transactions Given the account has the following transactions:\n- A deposit of 1000 on 01/04/2014\n- A withdraw of 100 on 02/04/2014\n- A deposit of 500 on 10/04/2014 When the user shows the account statement Then the statement should be a list with all the transactions in reverse chronological order And the statement lines should contain the transaction amount, date and running balance Setting up the project Before getting our hands dirty with the code, we have to configure the tools that we are going to use to write our tests. We have chosen JUnit , Mockito and Espresso for assertions in Android views. We have to add their dependencies to the project build.gradle as follows: testCompile 'junit:junit:4.12'\ntestCompile 'org.mockito:mockito-core:1.10.17'\n\nandroidTestCompile \"com.android.support.test:runner:0.4.1\"\nandroidTestCompile \"com.android.support.test:rules:0.4.1\"\nandroidTestCompile \"com.android.support.test.espresso:espresso-core:2.2.2\"\nandroidTestCompile(\"com.android.support.test.espresso:espresso-contrib:2.2.2\") {\n    exclude module: 'recyclerview-v7'\n    exclude module: 'support-v4'\n}\nandroidTestCompile 'org.mockito:mockito-core:1.10.17'\nandroidTestCompile 'com.google.dexmaker:dexmaker:1.2'\nandroidTestCompile 'com.google.dexmaker:dexmaker-mockito:1.2' And at the bottom of the file: resolutionStrategy.force \"com.android.support:support-annotations:$supportLibraryVersion\" This last line is needed due to the different versions of the support-annotations that are bundled with the espresso libraries and the one that you probably already had in your project. For the sake of clarity of this exercise, we are going to use the default source set src/androidTest for our acceptance tests and src/test for the unit tests. This configuration may not be the ideal in a production environment, due to the fact that you would need to add some unit tests for the Android components in your project. If you are in that case, you could end up with a mix of unit/acceptance tests in the same directory, losing the possibility to run all the tests quickly. Remember that acceptance and integration tests are slower than unit tests. In a real project we will need to run all the unit tests alone to ensure that they are passing and checking the current step in the inner TDD loop. That is why is a good idea to separate them. Writing an acceptance test for the acceptance criteria This image show the testing flow that we are going to follow: the double loop of TDD. The outside loop corresponds to the progress of our feature and the inner loop corresponds to the individual functionals required to implement the feature. It's worth defining our test types very clearly. Unit test : Test that our class does the correct thing Acceptance test : Test that our system passes the acceptance criteria and therefore behaves properly using real collaborators. Leaving aside external systems such as network, database, API, etc… Integration test : Tests that our system works together with external dependencies. So, now that we have our requisites, we have to start writing an acceptance test. It will provide the current step of the outside loop we are in. We can re-run this test anytime to figure out what the progress of our feature is. This test validates that our system complies with the acceptance criteria that the business owner has agreed to, being the bridge between developers and business. Once the feature is finished, the acceptance test will also serve as a regression test, quickly altering us if future code change breaks or changes the functionality. For now, it is going to offer us feedback about the progress. The acceptance test should be as end-to-end as possible, but still within the boundaries of your system and not relying on any external systems or dependencies. The naming conventions that we are going to use come from this Codurance article . The unit tests of our classes are going to use the suffix “Should” which allows us to read the class name and the test method name as a full sentence. For the acceptance test we are going to use the suffix “Feature” so we can easily differentiate between acceptance and unit tests. For the test implementation of this kata we are going to use the notation Given, when, then . This will allows to express the acceptance criteria in the code as closely as possible within the code itself. It will help us to use the ubiquitous language that the business owner has used to write the acceptance criteria. It is a win-win - we have the test written conforming to the 3-As pattern and we gain a ubiquitous language from the business. To assert our views in Android (i.e check the 'then' phase) we are going to use Espresso , which provides a simple way to test the state of the UI. But that is it for now! So far we have reviewed the basic concepts about TDD and, more specifically, the outside-in approach. In the next post we will start creating our first acceptance test and we will dive into the inner TDD loop until we get our feature working. Stay tuned! References and further reading Sandro's Outside-in screencast Growing Object-Oriented Software Guided by Tests Codurance test conventions Android testing support library For further information about how to write users stories, visit this link ↩︎", "date": "2016-04-25"},
{"website": "Novoda", "title": "Losing Focus", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/losing-focus/", "abstract": "Seb and Ataul were working on some accessibility improvements to the All-4 Android app and came across a RecyclerView focus bug. One of the areas we're... focusing on at Novoda is to increase support for vision impaired users in our apps. A common use case is to perform some action on a list item in place—for example marking a Tweet as favourite or starring a To-do. We found that a RecyclerView would lose its focus when you update the data backing its adapter. This happens for both view focus and accessibility focus (with a switch or screenreader). The former is the one you have when using a D-Pad or a trackball to navigate; the latter is the one that comes up when the user is using TalkBack, or a switch. Here's how the issue looks like for a user: For users that rely on interacting with focusable elements this is extremely annoying and time consuming. It might make navigation impossible. Luckily, this is not the intended behaviour, but rather \"just\" a RecyclerView item animator bug . A workaround exists to avoid the issue by disabling the item animations: RecyclerView recyclerView = (RecyclerView) findViewById(R.id.list);\nif (needToRetainFocusOnItemChange()) {\n    recyclerView.setItemAnimator(null);\n} For this to work, if you use adapter.notifyDataSetChanged() then your adapter also needs to support stable IDs ( setHasStableIds(true) ), and implement long getItemId(int position) . If you're using adapter.notifyItem*() then you're good with just removing the animator. As pointed out in the bug report, in the latter case you can also just disable the \"change\" item animation. Thanks to Yigit Boyar for the heads up!", "date": "2016-03-23"},
{"website": "Novoda", "title": "Boolean Algebra to the Rescue: Using Maths to Write Better Code", "author": ["Wagner Truppel (iOS Software Craftsman)"], "link": "https://blog.novoda.com/boolean-algebra-to-the-rescue-using-maths-to-write-better-code/", "abstract": "A few days ago I was fixing an auto-layout issue on a custom view in Novoda 's first iOS app and it gave me a chance to play with some boolean algebra. This post gives an overview of the practical implications of boolean algebra for problem solving and shows how you can apply it in order to write correct and simpler code. The custom view in question is a reusable component used to build custom alerts in the app, and has three buttons, like so: Sometimes, however, we only need to display one or two buttons so the remaining one(s) need to be constrained to have zero width. In itself that's not a particularly difficult auto-layout problem so that's not the focus of this post. Rather, I want to focus on the vertical dividers between the buttons. Here's an \"exploded\" view of the custom view so we can see more clearly the buttons and the dividers: The question, then, is: when should we show the left and/or the right vertical dividers? This is also not a particularly difficult problem to solve but it provides a great opportunity to show how one can use a little boolean algebra to make sure the code that computes the hidden state of those dividers is not only correct but also the simplest possible. What I'm going to explain below is a general method that can be applied to any problem involving boolean variables and boolean expressions. Truth tables We start by writing the so-called truth table that describes the problem at hand. A truth table lists all possible combinations of the boolean variables of interest and, for each such combination, shows the expected result. Here we have 3 buttons, each of which may or may not be present, and 2 vertical dividers whose present/not-present state we want to compute in terms of the present/not-present state of each button. Before I write that truth table, however, I want to talk about two other truth tables, those of the and and or boolean operators. Imagine that we have two boolean variables, A and B . The truth tables for A and B and for A or B are: Now imagine that false is represented by 0 and true is represented by 1 . Then we have and it becomes apparent that the logical and operator is like the multiplication of binary digits, in that the only way to get 1 out of the two variables is if both of them have the value 1 . Similarly, the logical or operator is like the addition of binary digits in that the only way to get 0 out of the two variables is if both of them have the value 0 . We can then use * for and and + for or , which will not just make the expressions I'm about to write simpler, but also more intuitive and easier to understand. Back to the buttons and dividers We have 3 buttons - each of which can be present or absent - so we have a total of 2^3 = 8 possible combinations. For each of those combinations, zero, one, or both of the dividers will be present. We then need to fill the truth table below: Incidentally, the boolean variables hasLeftButton , hasMiddleButton , and hasRightButton are called input variables and the boolean variables showLeftDivider and showRightDivider are called output variables. Obviously, if none of the buttons are present ( 0 s in the first three columns) then none of the dividers should be present, giving 0 s for the last two columns. Also, if all three buttons are present ( 1 s in the first three columns) then both dividers should be present, so we should have 1 s in the last two columns. Yet another simple case is that if only one button is present then no dividers should appear. Here's an interesting case, though: if we have the left and right buttons but not the middle button, then we need one divider to appear but... which one? If we make the convention that the left divider is associated with the left button and the right divider is associated with the middle button then not having the middle button means not having the right divider. Therefore, 101 (left and right but no middle) must result in 10 (left divider but no right divider). Working our way through the entire table, we get: This table describes every possible situation that could happen in this problem, in a compact way. The problem now is how to express the output variables showLeftDivider and showRightDivider as boolean expressions in terms of the input variables hasLeftButton , hasMiddleButton , and hasRightButton so that we match the results of the table when we write those expressions in code. Note that if the table is correct, the code will be correct. This shifts the problem from figuring out the logic as we code to figuring out the logic first and then having an almost automatic way to write the code, a way that is guaranteed to be correct. We're also interested in getting the simplest possible expressions that are still correct so that our code is as simple as it can be. From truth tables to boolean expressions: concrete example How do we do that, though? How do we write expressions for showLeftDivider and showRightDivider that are the simplest possible and also guaranteed to be correct? It's all in the truth table already! All we need to do is understand what the table is actually telling us. Take showRightDivider , for instance. What we really want to know is: when is that value true (ie, 1 )? Well, looking at the table, we see that it's true / 1 when either of the rows below happens. In other words, showRightDivider is true / 1 when either of these happen: hasLeftButton == 0 and hasMiddleButton == 1 and hasRightButton == 1 hasLeftButton == 1 and hasMiddleButton == 1 and hasRightButton == 1 But these can be written like so as well: !hasLeftButton == 1 and hasMiddleButton == 1 and hasRightButton == 1 hasLeftButton == 1 and hasMiddleButton == 1 and hasRightButton == 1 or, more simply, !hasLeftButton and hasMiddleButton and hasRightButton hasLeftButton and hasMiddleButton and hasRightButton Note the change from hasLeftButton == 0 to !hasLeftButton == 1 , which means exactly the same thing. Now, saying that either of these can happen is equivalent to saying that these possibilities are being or ed together: showRightDivider = (!hasLeftButton and hasMiddleButton and hasRightButton)\nor ( hasLeftButton and hasMiddleButton and hasRightButton) which translates directly into code as showRightDivider = (!hasLeftButton && hasMiddleButton && hasRightButton)\n|| (hasLeftButton && hasMiddleButton && hasRightButton) But this isn't the simplest possible expression we could write. In fact, the simplest expression is just showRightDivider = hasMiddleButton && hasRightButton We'll see below how to simplify an expression arising from the truth table. First, I need to explain how to get an expression out of the table, in the general case. From truth tables to boolean expressions: general rule The concrete example above shows that to get an expression for an output variable from a truth table we only need to focus on those rows for which the output variable in question has a value of true / 1 , and then we need to or those rows together, where each row is the and of the corresponding input variables, making sure to negate any input variable that has the value false / 0 . It's actually easier to do that if we think of and as * and or as + . Once we have the expressions for the output variables in terms of the input variables, we can use some of the standard properties of addition and multiplication to simplify those expressions. The most commonly used property is the distributive nature of multiplication: A * (B + C) = (A * B) + (A * C) except that we generally use it to go from the right-hand side to the left-hand side. The reason we can use this distributive law, of course, is that and itself is distributive with respect to or in the same way that multiplication is distributive with respect to addition. Other useful simplification rules come from the identities: A * 0 = 0 A * 1 = A A + 0 = A A + 1 = 1 A + !A = 1 where A is any boolean variable or expression. The last one simply says that or ing a boolean variable with its negation always results in 1 which is, of course, correct. Back to the buttons and dividers So now let's get back to the problem at hand and let's simplify the expressions for showLeftDivider and showRightDivider that arise from the truth table we built. First, we need to obtain the expression for showLeftDivider , which comes from: and results in showLeftDivider = (hasLeftButton * !hasMiddleButton * hasRightButton)\n+ (hasLeftButton * hasMiddleButton * !hasRightButton)\n+ (hasLeftButton * hasMiddleButton * hasRightButton) The expression for the other output variable is that which we had obtained before: showRightDivider = (!hasLeftButton * hasMiddleButton * hasRightButton)\n+ (hasLeftButton * hasMiddleButton * hasRightButton) Let's start with the second one. Note that hasMiddleButton * hasRightButton appears in both terms being \"added\" so we can use the distributive property to write showRightDivider = hasMiddleButton * hasRightButton *\n(!hasLeftButton + hasLeftButton) Now we use the property that anything added to its negation equals 1 , so (!hasLeftButton + hasLeftButton) = 1 and we have showRightDivider = hasMiddleButton * hasRightButton * 1 But \"anything\" times 1 equals the \"anything\" so showRightDivider = hasMiddleButton * hasRightButton and that's the simplest expression we can get for showRightDivider . Similarly, the last two terms in showLeftDivider have hasLeftButton * hasMiddleButton in common so showLeftDivider = (hasLeftButton * !hasMiddleButton * hasRightButton)\n+ hasLeftButton * hasMiddleButton * (!hasRightButton + hasRightButton)\n\nshowLeftDivider = (hasLeftButton * !hasMiddleButton * hasRightButton)\n+ hasLeftButton * hasMiddleButton * 1\n\nshowLeftDivider = (hasLeftButton * !hasMiddleButton * hasRightButton)\n+ hasLeftButton * hasMiddleButton But now hasLeftButton can be factored out as well and we get showLeftDivider = hasLeftButton *\n(!hasMiddleButton * hasRightButton + hasMiddleButton) As it turns out, the term in parenthesis can be simplified further because of another property of boolean algebra, A + (!A * B) = A + B (where, in the example of this post, A is hasMiddleButton and B is hasRightButton ). We can see that this identity is true by comparing the truth table for A + (!A * B) with the truth table for A + B (an exercise left to the reader) or by noticing that when A has the value 1 , A + (!A * B) has the value 1 regardless of the value of B and when A has the value 0 , A + (!A * B) has the value B . But that's exactly the same behaviour as that of A + B . So, the simplest expression for showLeftDivider is: showLeftDivider = hasLeftButton * (hasMiddleButton + hasRightButton) Does this make sense? Yes, since the left divider should indeed be present if we have the left and middle buttons, regardless of the state of the right button (the first term of the result) or if we have the left and right buttons, regardless of the state of the middle button (the second term of the result). We then translate those expressions directly into code and we're done: showRightDivider = hasMiddleButton && hasRightButton\n\nshowLeftDivider = hasLeftButton && (hasMiddleButton || hasRightButton) Summary Yes, of course using boolean algebra for such a simple problem is overkill. The point is, though, that it's there when we need it and can be very useful to make sure our code is correct and the simplest possible. All we need to do is write a truth table that correctly describes the problem we're trying to code and then apply what is mostly an automatic procedure. More specifically, the steps are: Write a truth table for the problem at hand; This is the only part of the problem that is domain-specific. For each output variable: Collect the rows in the table for which the output variable has the value true / 1 . For each such row, write an expression that is the product ( and ) of the input variables, making sure that variables which have a value of false / 0 are negated. Add ( or ) all those expressions together; That is the raw (i.e., not yet simplified) result for the output variable in question. Simplify the expression using the properties explained in this post. Alternatively, you can use Wolfram Alpha to do the simplification for you. Here is a particular example showing how to simplify A + (!A * B) . Convert the simplified expression into code. Big thanks to Ataul , Ferran , Ryan , and Yvette for providing great feedback on earlier drafts of this post.", "date": "2016-03-17"},
{"website": "Novoda", "title": "Design Freebie - Marshmallow Lockscreen Sketch resource", "author": ["Dave Clements"], "link": "https://blog.novoda.com/design-freebie-marshmallow-lockscreen-sketch-resource/", "abstract": "Ever found yourself knowing your client wants to see the notifications for your app design but you really don't want to use hacked screenshots again?  No? Oh, well I have! So I made this lock screen resource. I made this for myself initially, but I thought it would more than likely be useful to at least one other person in this known universe. So, I'd like to present v1.0 of the lock screen sketch freebie. The lock screen in Sketch Download and use it Hosted on Github, you can download the file here :) Also posted to Dribbble and MaterialUp Updates to come I'll be adding not only a tablet version, but in fact, various breakpoint versions that match with Google's own breakpoints.  This will happen over time. Don't expect it tomorrow. I have a job too you know! Enjoy.", "date": "2016-03-08"},
{"website": "Novoda", "title": "A story about XML parsing on Android", "author": ["Juanky Soriano"], "link": "https://blog.novoda.com/an-story-about-xml-parsing-on-android/", "abstract": "I remember one of the first tasks I paired on when I first joined Novoda. We were rewriting an Android legacy application but the rewrite conserved some of the old services. In this post I will describe the journey we took to deep dive, evaluate and craft this code to learn and develop as a team. When rewriting this legacy application, one of these services was a XML parser for Atom feeds written using Simple-Framework , an XML parser which provides a powerful and easy to use mechanism, based on annotations, which facilitates fast development. With that (legacy) parser already written we had working XML parsing for complex feeds, life was easy and beautiful... and we completely rewrote it. True we did. Yes I know, your reaction at this point is probably the same I had when I got assigned to this task. I remember my first thinking being like: These guys are crazy, this stuff works and they want to rewrite it completely. The following sentence by Donald Knuth [1] came to my mind: We should forget about small efficiencies, say about 97% of the time: premature optimisation is the root of all evil. And I couldn't stop thinking: Why? Why are they doing such a thing? Such overkill! There must be a reason for it. Getting to the heart I am curious by nature and I won't do something without knowing the reasoning behind it. I had to know, I had to ask why we decided to do it, get to the heart of the matter and I decided to ask one of the experienced craftsman in the team. By that time he had already worked on the application for a while, and his pragmatism and good practices were well known within our team. Well Juan, he said, Of course this is not a premature decision, obviously there is a reasoning for this change, otherwise we would be wasting precious time. Our application is not as fast as we want; we are trying to achieve the best performance possible and decided to run benchmarking to analyse what the reason was. We discovered that XML is a bottleneck in our app. That was true, I noticed that the app was taking some time to display the content but to be honest, maybe because of ignorance, maybe because of being new in the team, I thought that \"it was ok\". Numbers don't lie He offered to sit-down together with me and my pair for this task and try to replicate the original benchmarking. We did it and by doing so we had proof. The numbers don't lie. Do you see it? If the XML feed takes this long to be parsed there is no data, and during that time any of the other processes that rely on it cannot do their job. It is a bottleneck. The reasoning behind the long time taken to parse the feeds arose as soon as we took a deeper look into how the Simple-Framework 3rd party library works. Firstly Simple-Framework is a DOM [1:1] parser, with some of the performance penalties that this implies. For us it was doing a lot of unnecessary work, therefore slowing down the app. It was not only the abuse of reflection, but also the fact that it will do some processing for every single node in the XML; even if the element being represented was not relevant to us. And we had many of these. It was decided. Me and my pair had to find a replacement! And in order to do that we had to be craftsmen. Getting the job done In order to select a replacement for Simple-Framework we applied a bit of research on available options, out of our research we decided to pick two potential candidates; another DOM parser and a SAX [1:2] parser. As a DOM parser we picked Jackson Dataformat XML and as a SAX parser we picked Simple Easy Xml Parser . We had our candidates, Simple-Framework, SEXP and Jackson. The next natural step was to evaluate them. We wanted to be fair and give each an equal opportunity, so we established two necessary conditions for the tests to be performed. Each parser will be tested for the same data set. The parsing operations must be isolated from any other processes. Our test had to just parse, no other operations were allowed. This seemed to be a perfect case to apply micro-benchmarking [1:3] . Caliper is Google's micro-benchmarking tool that was perfect for the job. It is easy to use and has the capabilities we needed. For the comparison we made the three candidates parse a XML structure composed of ~100 entries that where neither simple in form nor excessively complex. The tests are completed! Let's analyse the results. The results [1:4] were revealing, SEXP was performing on average 2.5x faster than Jackson and 1.7x faster than Simple-Framework, and not only that, it also showed SEXP had an incredibly lower memory footprint , reducing ~80% the number of object allocations compared to Simple-Framework and ~30% the number of object allocations vs Jackson. SEXP was also reducing ~50% the amount of memory required vs both of its opponents. So we had a candidate. SEXP turned to be a very Android friendly XML parser which not only performs fast but is also respectful with the ecosystem of services it will be living with given its low memory footprint. Next steps The next steps were to materialise the replacement. It is beyond the scope of this tale to detail how it was done, but in short details I can say that it implied improving the code coverage by revisiting our integration tests, identify any refactoring previous to the replacement that could facilitate work, and then attacking the code to make it effective. Once this was done we were able to appreciate the real performance winning in our app. It was amazingly better. The content in all of our activities were displayed earlier, in average SEXP shown to be 2.08x faster than Simple-Framework for the target app. Moral A few lessons I learn from this pairing exercise. The most valuable is about not doing job that it is not required to achieve the goal. We could have tried to replace the parser with the first one we find, but that would have been a very silly thing as we didn't know before hand, which parser could have given us a better results. Instead of doing it we selected our candidates, executed benchmarking on static test data, and selected a winner. The second lesson is about pair programming. It was very valuable to find the time to sit down with more experienced members in the team. During the conversations all of us acquired a higher level vision of the problem we were handling, identifying the underlying problems and determining potential solutions to them. The last but not least is about knowing your app, and knowing your tools. In our case we where processing massive feeds from which we didn't need all the information they contained. In this case a parser which offered to us a finer granularity when deciding the parsing flow, the parsing events to be handled and how to make sense of the data they contained was essential to achieve the performance required. And remember, even if the 97% of the time is premature optimisation, Donald Knuth also pointed out that: Yet we should not pass up our opportunities in that critical 3%. More information about this particular use case for Caliper can be found here . ↩︎ ↩︎ ↩︎ ↩︎ ↩︎", "date": "2016-02-29"},
{"website": "Novoda", "title": "Taming your Github notification emails", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/taming-github-notification-emails/", "abstract": "If you use GitHub at your company and have more than one project on the go at once, you might start to notice you get a lot, A LOT of emails. You get email notifications of pull requests being created, people commenting, people mentioning your name, people merging or closing pull requests... you get the idea. In this post I lay out how I manage my Github emails using Gmail to turn all of this noise into something useful. Enjoy the freedom of having an inbox clear of these notifications (but still be within easy reach), and sneakily move you towards inbox zero wether you believe in it or not. Clearing up these emails will give you the freedom to see those notifications that are important and if you find the spare time, peruse those that aren't so urgent. First thing, you're thinking HUH what email notifications?? Well they're probably going to your personal email you originally signed up for GitHub with, you know the one you never check anymore because log(n^n) emails. You need to know which inbox your notifications are going to. If you are a member of an organisation you can choose separate email addresses for each organisations notifications. Lets fix that. GitHub > Settings > Notification center Now we're all on the same page lets get cleansing. First up we filter for all notifications from github and put them in their own folder. To do this we need to create a new filter. You want to filter From notifications@github.com , this will include of all the different pull requests & comments etc that we talk about above. You also want to filter Doesn't have for your Github handle and any global Github handles your company uses. (This will make more sense in a minute - keep reading!) Hit continue. You want to skip the inbox so that it goes straight to our new folder. Then you want to apply the label and this is what will move it to the folder of choice. Here we are putting it inside of /PullRequests/ We're close to perfection, the above is the basic outline of filtering and we will rinse and repeat with different paramaters to setup our full clean inbox setup. Create a second filter. This one also filters From notifications@github.com . The difference is we also filter to Includes the words using our GitHub handle and any global handles (the opposite of our previous filter). Hit continue Skip the inbox again and apply the label . This label (folder) is a subdirectory of our previous filter. So this time we use _/PullReqests/Ref Me/. Therefore this folder will be every notifcation from Github that directly affects me or talks about me. I know what you're thinking - \"yes but what about MY project ME ME ME ME. The company has many projects with a lot of notifications. I am interested in notifications from my project but not those other phhht inferior projects\". Don't worrry, we've got you covered. Create another filter. This time we use the Subject field. Every notification from Github has a subject line that begins with the project name in square brackets. Therefore I can filter here for my project that I work on by using [project name] . Don't forget to put it into a subdirectory of /PullRequests/ , a folder with the name of the project might be a good idea. Here comes the freedom I promised you. Cherish it. Now your inbox folders should look something like this. One tip is to work through these folders backwards. You always want to check Ref Me as this is direct conversation you are involved with on GitHub. Check Your Project when you find time in your project :-) and check PullRequests at your leisure to see what is going on in your organisation. Last thing. Every Monday morning, go into the PullRequests folder, select all and delete . Believe me you will NEVER get through all these emails and the older they are the more worthless they are. Do your self a favor and start a fresh every week. If you don't believe me try it for a month and tell me if you are losing out, or if you actually gain and feel fresher and better and more informed. This is not the only way to do filtering so feel free to berate me and show me how your filterings are n times better than this. Otherwise for those more pragmatic out there, give it a go, iterate and improve.", "date": "2016-02-26"},
{"website": "Novoda", "title": "Designing alternative interfaces: a Tweet view with custom actions", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/alternative-interfaces/", "abstract": "The Google+ Android team have been focusing a lot recently on improving TalkBack support. In this post, I'll explain one of my favourite features, custom actions, and how you can implement it. There's a Google+ Accessibility community which the team uses to collect feedback from the users and there's been a strong positive reaction to the improvements in the stream view. I've written before about how you can optimise lists (with multi-action list items) for TalkBack , specifically advocating for action dialogs (instead of inline actions) to facilitate faster navigation between list items. In the screenshots above, inline actions are disabled for TalkBack users - only the whole card is focusable (green outline). Tapping on the card as a whole displays the dialog containing all the actions, making it a lot quicker to navigate through a long list of cards for vision impaired users. Google+ has taken this one step further with the use of custom accessibility actions: The same actions shown in the dialog are available in this view, but for some users, a radial menu will be quicker to use than a linear list of actions. While the dialog is available on all API versions, custom accessibility actions are only available on Android Lollipop and above. Let's do it! Yet another Twitter client Don't worry, we're not going to create the whole app. I'm just going to throw my hat into the ring by creating a simple View to display a Tweet. We should present the author and the Tweet content; other information (like the date, time, location, etc.) can be displayed in a detail screen: public class Tweet {\n    public String getAuthor() {...}\n    public String getText() {...}\n} The View should also include affordances for user actions: public interface TweetActionListener {\n    void onClick(Tweet tweet);\n    void onClickReplyTo(Tweet tweet);\n    void onClickRetweet(Tweet tweet);\n    void onClickLike(Tweet tweet);\n} Extending a ViewGroup to add our own logic means we can hide the fact that the View will have essentially three different sets of affordances (inline actions, action dialog, and custom accessibility actions). You can alternatively put this logic in your presentation layer. To start with, this is our goal: It shows a Tweet View with the author, Tweet content and three Buttons to \"reply\", \"retweet\" and \"like\". Clicking anywhere (except the Buttons) should open the detail screen. The first thing I do is create the skeleton of the custom View: public class TweetSummaryView extends LinearLayout {\n\n    private TextView authorTextView;\n    private TextView contentTextView;\n    private View replyButton;\n    private View retweetButton;\n    private View likeButton;\n\n    public TweetSummaryView(Context context, AttributeSet attrs) {\n        super(context, attrs);\n        setOrientation(VERTICAL);\n        setBackgroundResource(android.R.color.holo_red_light);\n        View.inflate(getContext(), R.layout.merge_tweet_summary, this);\n    }\n\n    @Override\n    protected void onFinishInflate() {\n        super.onFinishInflate();\n        // TODO: use `findViewById` to find the widgets and assign to fields\n    }\n\n    public void display(Tweet tweet) {\n        // TODO: bind Tweet data to the TextViews\n    }\n\n} Next, I add this View’s internal layout, res/layout/merge_tweet_summary.xml : <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<merge xmlns:android=\"http://schemas.android.com/apk/res/android\">\n\n  <TextView\n    android:id=\"@+id/tweet_summary_text_author\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    android:layout_marginLeft=\"4dp\"\n    android:textColor=\"@android:color/white\" />\n\n    <!--not shown: another TextView for the tweet text-->\n    <!--not shown: a horizontal LinearLayout with three buttons-->\n\n</merge> We find and assign the Views: @Override\nprotected void onFinishInflate() {\n    super.onFinishInflate();\n    authorTextView = (TextView) findViewById(R.id.tweet_summary_text_author);\n    // ... finding and assigning the others\n} and bind data to them: public void display(Tweet tweet) {\n    authorTextView.setText(tweet.getAuthor());\n    contentTextView.setText(tweet.getText());\n} Finally, let’s add the callbacks: public void display(final Tweet tweet, final TweetActionListener tweetActions) {\n        ...\n\n        replyButton.setOnClickListener(new OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                tweetActions.onClickReplyTo(tweet);\n            }\n        });\n\n        // ... same for the other buttons\n\n        setOnClickListener(new OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                tweetActions.onClick(tweet);\n            }\n        });\n    }\n} Ta-da! We have everything: and our requirements are met: both the author and Tweet is readable the user can reply, retweet or like the Tweet the user can click on the Tweet to open a detail screen Considering TalkBack Testing it with TalkBack, a screenreader for Android, it still meets our requirements! TalkBack will stop on actionable elements to read them aloud - that covers all of our buttons - and because the whole view is clickable, the author and Tweet content is read aloud too. We can do better though - navigating through an infinite timeline of these Tweet Views would be awful because you'd have to perform four gestures to get to the next one. Our plan is to present the actions via a dialog, instead of inline, so let's disable the buttons first. We can do this by marking the buttons' ViewGroup as not important: <LinearLayout\n    android:importantForAccessibility=\"noHideDescendants\"\n    ... which tells TalkBack to ignore that ViewGroup and its children. Adding an actions dialog Now we need to repurpose the click listener on Tweet View (if TalkBack is enabled!) to show the dialog. A guide for detecting whether TalkBack is enabled is given in a previous post . public void display(final Tweet tweet, final TweetActionListener actions) {\n    ...\n\n    setOnClickListener(new OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            if (spokenFeedbackIsEnabled()) {\n                showActionsDialog(tweet, tweetActions);\n            } else {\n                actions.onClick(tweet);\n            }\n        }\n    });\n} I use a standard AlertDialog for the actions dialog: private void showActionsDialog(Tweet tweet, TweetActionListener tweetActions) {\n    CharSequence[] labels = getLabelsFrom(tweetActions);\n    DialogInterface.OnClickListener dialogItemClickListener = dialogItemClickListenerFrom(tweet, tweetActions);\n\n    new AlertDialog.Builder(getContext())\n                .setTitle(\"Tweet options\")\n                .setItems(labels, dialogItemClickListener)\n                .create()\n                .show();\n} Adding custom accessibility actions Adding custom actions is simple - you can either override two methods of the View, or you can set your own AccessibilityDelegate on the View and implement two methods in that. I went for the second option; with a bit of work, the AccessibilityDelegate class can be made reusable between Views. public void display(final Tweet tweet, final TweetActionListener actions) {\n    ...\n    ViewCompat.setAccessibilityDelegate(this, new TweetAccessibilityDelegateCompat(tweet, tweetActions));\n}\n\nprivate static class TweetAccessibilityDelegateCompat extends AccessibilityDelegateCompat {\n\n    private final Tweet tweet;\n    private final TweetActionListener tweetActions;\n\n    TweetAccessibilityDelegateCompat(Tweet tweet, TweetActionListener tweetActions) {\n        this.tweet = tweet;\n        this.tweetActions = tweetActions;\n    }\n\n    @Override\n    public void onInitializeAccessibilityNodeInfo(View host, AccessibilityNodeInfoCompat info) {\n        super.onInitializeAccessibilityNodeInfo(host, info);\n        info.addAction(new AccessibilityNodeInfoCompat.AccessibilityActionCompat(R.id.action_reply, \"Reply\"));\n        // ... for each action\n    }\n\n    @Override\n    public boolean performAccessibilityAction(View host, int action, Bundle args) {\n        switch (action) {\n            case R.id.action_reply:\n                tweetActions.onClickReplyTo(tweet);\n                return true;\n            // ... other actions\n            default:\n                return super.performAccessibilityAction(host, action, args);\n        }\n    }\n\n} And that does it. Any questions, shoot them over , otherwise get cracking!", "date": "2016-02-15"},
{"website": "Novoda", "title": "China: Apps & Online", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/apps-online-services-in-china/", "abstract": "Happy Chinese New Year everyone! I have started wading into the unfamiliar and disorientating digital territories of Chinese websites and apps. Previously I took for granted how much I've learned through casually clicking links but a lack of Chinese language understanding really hinders even casual browsing. Google Translations are cryptic and then there are the untranslatable, walled garden apps. I spent western new year in Beijing, China and it was there I realised how reliant I was on App habits native to London & Berlin but currently incompatible with China. Lots of goto apps and services are unavailable and even with access, some are unreliable sources of local information. Whether curious or planning a holiday (I recommend it) I’d have appreciated this guide before arriving. Plan and download all your apps BEFORE you leave because at the time of writing there was no access to Google Play in China. If you are just curious for a bunch of links to click check out below! App Stores QQ:应用宝 百度手机助手 hiAPK:安卓市场 Wandoujia:豌豆荚安装 Mi:小米商店 China Mobile:移动应用商场 360手机助手 Anzhi:安智 91手机助手 Messaging Wechat 微信 650m monthly active QQ 808m monthly active Weibos Sina Weibo 新浪微博 143.8m monthly active RenRen 人人 31m monthly active Taxi Uber Didi Kuadi Video Youku（优酷） 580 million monthly Maps Baidu Maps Google maps Payment Alipay 支付宝 300m users WeChat 微信 Shopping Alibaba 阿里巴巴集团 Taobao 淘宝网 Tmall 天猫 Jingdong mall 京东商城 Amazon 亚马逊中国 Restaurant TripAdvisor Eleme DianPing Baidu Waimai Foursquare Translation Pleco Taxibook Google Translate Now the details I'll go into each section with a few more details of personal experience. App Stores There are supposedly 200 app stores in China however I think really there are under 10 of note. These will mostly come with people’s mobile phones or be downloaded through banners while browsing mobile services. Starbucks advertises on the few below so I'd trust they want to be in front of the greatest audience: QQ:应用宝 , 百度手机助手 , hiAPK:安卓市场 , Wandoujia:豌豆荚安装 , Mi:小米商店 , China Mobile:移动应用商场 , 360手机助手 , Anzhi:安智 , 91手机助手 Shopping Alibaba got the jump on Chinese e-commerce and after starting with an import export specialist website spread out to direct consumer offerings Taobao & Tmall . Taobao is like ebay but with more emphasis on users who have stores. Tmall is like a verified seller shopping mall, you have to be quite a big glossy brand to appear on Tmall. Jingdong is really the only competition to Alibaba in China. Jingdong offers the mall like experience to consumers like tmall but it supposedly has much more reliable shipping options. Amazon isn't getting much of a look in totalling only 1.5% of china's online sales according to iResearch. But I did see Amazon delivery bikes zooming around Beijing. It's vital to remember China percentage figures are relative amounts of HUGE. Maps Paper Maps; take up room, don’t show my location, don’t intelligently direct and smaller ones don’t have enough info. I love Google Maps in Europe/America and so was really shocked to find them rendered wholly unreliable wandering around Beijing. I can only suppose that when Google pulled out of China they also stopped paying for updated map data because while I relied on Google Maps I was directed to building sites and hospitals which were labelled as hotels. Baidu Maps was not just a sufficient replacement but really impressive glimpse of map apps in the future! It did everything I expected around directing me to the location with a few neat tricks to boot. Luckily downloading the local area of Beijing was pretty painless although a large download. Once the maps were local there were super responsive and really useful for orientation. As i progressed along the mapped route on foot, along the way icons of special offers from nearby shops and services peeked into view. Once at destination high-rise towers I could without fail click through different floors and see conveniences and offices on every floor. Very impressively accurate and fine grained maps across the city. There was an icon to hail a cab integrated into the maps but I didn’t try it out as I had enough trouble chatting with Uber cab drivers who would inevitably call upon every booking. Translation The Taxibook app is paid but if you are going to get anywhere in Beijing on a short holiday without speaking any Chinese I recommend you get it. Otherwise you had better get a native speaker to write down your hotel and intended locations so that you can flash it at impatient taxi drivers or random locals. You'd best just write down the hotel anyways. Plecco is there for all your one word pointing to respond needs, It’s a fantastic dictionary resource for full-time Chinese language learners and the OCR translation that it offers is unparalleled and super impressive offline. Google Translate is excellent if you can get it through a VPN. Phone Numbers, SIMs & 3G/4G My mobile network caps the possible charges incurred via roaming without explicitly phoning to allow them to charge over the cap which is usually good but I was there during Christmas/New year and so alas I couldn’t get through. :/ So if you are spend thrift and are going to rely on crazy Europe > Asia roaming charges then remember to remove the cap before leaving. Getting an alternative data SIM didn’t turn out to be easy either. I’m told by local guides that up until some point in 2015 buying a SIM was easy. They were as shocked as I was when I visited in Jan 2016 it was quite a commitment. They could only suppose it was due to a recent crack down on 'cyber crime'. First I needed a local to vouch for me with an address and their phone number before I committed to a short term contract which would need to be terminated before my departure from China otherwise I’d continue paying as per any usual monthly contract. Not exactly convenient for foreign traveller just trying to get some smart phone data. All the digital services are charged and connected to phone numbers so you will certainly need a phone number if you are going to use any digital services. Messaging In Europe/US people ping over Facebook and Twitter and these two services are inaccessible through local internets. Plenty of Chinese people in cities continue to use these services through a VPN. On the whole though Chinese services are preferred. In China everyone’s faces are pressed into their phone... messaging. WeChat IDs are scribbled in felt tip next to dumpling sellers in the street, on the bottom of TV and magazine adverts. Street sellers also share their WeChat QR codes and offer discounts in exchange for giving you a WeChat ID. To be honest I’m still not completely clear on the appeal and use cases but I think people subscribe to receive push notification offers. The dumpling guy will let you know about fresh dumplings, the restaurant will take your order ahead of schedule. As it’s a 1-1 service and not 1-many it seems like a drop in replacement for SMS rather than twitter. Most everyone I met in tech was using WeChat although I’m told it’s fairly recent that it’s gained so much traction. Until recently QQ was the mainstay and it is what I have been using to speak with my Chinese teacher online. Skype / Google Hangouts voice and video quality were wholly unreliable and would constantly drop/echo but moving to QQ, conversation is crystal clear. Weibos Weibo means micro blog and as you can say a lot more in 140chars in Chinese than in English they feel like a difference beast in China. 140chars can be like 60/70 words so its less micro and more bloggy. Twitter is inaccessible without a VPN although plenty of people use it, not nearly as many as use Sina Weibo. Sina Weibo is the most popular social network for brand marketing but from what I can see all the most famous American celebrities are the most popularly followed accounts! It's worth noting also that instead of Google Sina is the most popular login method in China. RenRen used to be a popular Facebook alternative and still has 219m activated users. It likely has the same draw as Facebook in that if you only use one service online it will likely be this one through legacy but if you are a savvy user you'll use this network less. The network looks a bit dated and needs some love. I think Facebook sees what is on the cards and so is making major efforts to friendly up to China and take over from RenRen. Video YouTube is not accessible in China and so Youku rules the roost. It has lots more 'premium' content on there though focusses less on the average home video blogger. National TV stations upload lots of their content and there lots of films on there. Youtube videos often find themselves to Youku ripped and then re-uploaded. Taxi Pre-plotting my destination on a map before confusingly justifying it in my speckled Chinese was a god send. Uber had a bumpy start getting up and running in China however now I think they are operating as usual. Good news is that your normal Uber account will function as normal and is connected to your phone number so all normal billings. In Beijing there were plenty of Uber vehicles on the road and they were generally pretty friendly however I noticed I wasn’t given a chance to rate any drivers but they were rating me. There are two other local services to consider but who really don’t offer at the moment much outside of Uber as far as I can tell. DiDi is the main player with massive amounts of funding, its a bit cheaper than Uber. But considering how crazy cheap (£3 for a 15min ride?) I found it I decided to  stick with what I know while I can get away with it. Didi and Kuadi have now merged and so I expect them to quickly try and snuff out Uber's hard won influence in the region. VPN You’ll have to search close to your trip as it seems VPNs come and go. At the time of writing I used ExpressVPN and it served me well on wifi connections but didn’t work so well on mobile networks. Anyone staying in China for any significant length of time would definitely need a VPN. VPNs are not illegal in China although they certainly are not encouraged. Restaurant FourSquare seems alive and well although didn’t have a massive selection and is only available through VPN. TripAdvisor seems to have a completely different load of chinese character 汉语 reviews on their .cn version of their domain. Seems everyone is pretty wise to the effect of positive reviews on TripAdvisor and so it’s popular in the bigger restaurants that they’ll encourage you to comment. The top places on Foursquare and TripAdvisor both felt like a very small selection of possible options compared to the huge variety on offer and so have became tourist traps. Probably because of people’s lack of general connectivity to quickly contribute new reviews. TripAdvisor , Eleme , DianPing , Baidu Waimai , Foursquare Payment You will need a bank account to get a local phone number Phone numbers are used as identifying logins for many services. Alipay is accepted in a lot of places in Beijing! There are no transaction fees. Zero! Also there is a nice escrow service which allows you to put money somewhere before a deal has been agreed with someone so they know you have the money. I tried a few burner options like hush but on a short holiday it was more hassle than it was worth over a short trip. Hush is good in the west when you need a phone number to set up though.", "date": "2016-02-08"},
{"website": "Novoda", "title": "Demystifying the role of a product designer", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/demystifying-the-role-of-a-product-designer/", "abstract": "The role of a product designer has developed hugely over the years and the scope of what we do has broadened dramatically. We’re now responsible not just for aesthetics and interaction but the overall user experience and potential success of our products. Our roles have naturally developed to be far more business-oriented, using analysis and research to inform and validate our design and business decisions. The skills and techniques required in the design process are more advanced than ever, making it increasingly more difficult for designers to keep up with and excel at everything. There are many talented design specialists out there, leading new ways of thinking and working, but the reality is that many designers are still stretched across multiple disciplines. This post aims to outline the various roles and responsibilities within product design to help businesses understand what they need and designers to decide what they want. Research Analysing the customer experience using qualitative (user interviews and usability testing) and quantitative (data and analytics) research methods to form a realistic understanding of the current product offering is an essential part of the product development. This type of structured analysis allows companies to make informed decisions about how to provide user value, and in turn, grow their business. Skills Recruitment - Creating participant screeners, communication & logistics Customer interviews - Defining goals, planning and running the session Analysis & identification of user needs Persona creation Customer journey mapping Creation of data analysis specifications Stakeholder presentation of research findings & prioritised recommendations Product planning Working closely with stakeholders and product owners, providing insight into customer behaviour, guiding focus and planning product features. Skills A strong understanding of the business and customer behaviour Information architecture Feature definition & prioritisation User story creation Workshop coordination Design thinking Design thinking is the term used to describe the practical, creative processes of designers during problem solving, reignited by IBM’s recent introduction to IBM Design Thinking . At Novoda, we tend to use this term to describe the tasks that take place between feature planning and visual design, further defining user stories and sketching storyboards and screens that satisfy the user stories. Skills User story definition: Data variations & edge-cases Storyboard creation Interface sketching Prototyping Creating prototypes is a great way to quickly get a feel for how users will interact with your product and to highlight any usability problems early in the process. It presents a low-cost opportunity to test your prototype with users prior to development. With the introduction of great tools such as Invision , POP , Pixate and Principle , prototyping for mobile couldn’t get much easier. Skills User journey prototyping Interaction and transition prototyping Usability testing Usability testing allows us to challenge and validate our design decisions prior to public release. Testing regularly throughout the development process allows us to surface and react to any usability issues early and release our products with confidence. Skills Recruitment - Creating participant screeners, communication & logistics Forming hypotheses and testing goals Creation of test plans Session moderation Session analysis Stakeholder presentation of research findings & prioritised recommendations based on the severity of usability issues Interaction design Meaningful transitions and reactive interfaces guide the user's attention and communicate hierarchy, progress and achievement. Motion and interaction are now recognised as leading principles in product design. Google’s Material Design Guidelines and Apple's Human Interface Guidelines offer some very helpful guidance on how motion should be used to enhance the user experience of our products. Skills Knowledge of platform design patterns Motion design Creation of view states Visual design Visual design plays an intrinsic role in the creation of our products. We use colour, hierarchy, iconography and typography to communicate and provide feedback to the user, enhancing usability, brand perception and the overall emotional response of our users. Skills Knowledge of platform design patterns Layout: Colour, imagery, typography, elevation & hierarchy Iconography Illustration Effective design implementation handover and development review (graphic assets, design specs/exports, communication and design-developer pairing) Copywriting The art of crafting effective communication through type is one of the most challenging jobs in product design. Especially when designing for small devices, there is a necessity to communicate effectively with fewer words. Skills Effective communication Consideration of layout constraints Consideration of language variation Now, of course, we require more than these practical skills to be a successful product designer. We need the ‘soft skills’ too; empathy, curiosity, fearlessness and self-awareness will all help us to understand our users, put our ideas to the test and drive us to create exceptional products. We also need the ability to be methodical and pragmatic in our ways of working and analytical in our ways of thinking. We all have individual strengths and disciplines of design where our personalities are most suited, whether that’s research, creativity or process. Our roles are determined by individual skillsets and interests, the size and workload of our design teams and the skills required by the business. It’s important for us to recognise our individual skillsets and work within an environment that can nurture and benefit us and the business.", "date": "2016-02-04"},
{"website": "Novoda", "title": "Hexawhat Architecture?", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/hexawhat-architecture/", "abstract": "Have you ever heard of hexagonal architecture? In my experience, the question quite divides people. It's either yes! wow! I love it, I'm doing it / experimenting with it right now, OR no what? what even is that, is it Android based. hexa - six something? Here I'm going to try to explain hexagonal architecture in its simplest form and frankly in the way I understand it so far. Formally described by Alistair Cockburn in 2005 (otherwise known as Ports & Adapters ). I'm going to be framing the discussion from an Android perspective, and for those who answered no to the original question this should give you some grounding and a base for everyone to start to be involved in any hexagonal conversation. Let's set some pessimistic knowns: apps and or activities or fragments can become big and unwieldy. Android testing is near impossible or a pain in the ass. Libraries are needed and our favourite libs engrained in our conscious, if we had to switch from Glide to Picasso it would take a week of work. Behaviour is spread across the application, a change request involves touching n classes in n packages. Yes I'm being pessimistic but I'm sure you can relate to some part of that. I want to point out that the above does not have a golden bullet solution in hexagonal architecture and other software development strategies can help combat the code rot. Such things as front end architectures: MVP, MVC, MVVM, coding practices: SOLID, DRY, KISS, YAGNI, development practices: TDD, BDD, pairing, code reviews, clean code. Hexagonal architecture would never come to fruition without combining with some of the above mentioned practices and it is important to re-iterate you need a diverse set of tools in your toolbox to be able to create a maintainable, testable, bug free, feature rich application . With that said, hexagonal architecture is a selfish design pattern. It's about you sticking up for you and your business; letting others do as much work as they should. The trick here is being able to work out what is someone else's responsibility and what we should be doing ourselves. For example, our latest app (Channel 4 Video On Demand) plays streams of video content, organises this content by date/alphanumeric/genre and shows current programs whether you are offline or online. This means it needs to pull data from the web, rearrange data as it sees fit and also store data for when it can't access the web. Now pulling data from the web involves http call execution but Channel 4 don't care about that! Sorting data - hell I'm sure Channel 4 want to make sure we get this right, they want their latest series shown first, also showing data still when offline is important but knowing that to do this data is stored in an sqlite database or a flat file system or serialised into shared preferences is not at all important to Channel 4 (and therefore to us ) . This is the selfishness of hexaganol architecture, concentrate on what you are good at. Decouple your businesses domain from the Android framework , decouple what you care about from the implementation details of how it works. What this means, is to separate your concerns, make sure your business logic and business rules around sorting, filtering, calculating is separate from the implementation of fetching, persisting, communicating . We are experimenting with this on Android. At Novoda we have found a great start to this is having at least two modules per application: one called core and one called mobile . Core is a Java only module that contains the business logic paramount to your company as discussed above. It will expose one or more interfaces ( known as a port ) that a component inside of the mobile module will implement ( known as an adapter ). This is a one way relationship mobile knows about core but not the other way around. For example, Channel 4 want to request a video's details (programme name, title, duration) from the web. However your core app doesn't care how this is requested and so declares an interface like RequestExecutor . A class in mobile will implement this and perhaps use RxJava, HttpConnection or OkHttp or X, it doesn't matter what it is to core but it will get this job done. The advantages of this type of setup include testing, simplicity, re-use and maintainability. It is testable because our core module is java only. The ports we rely on are just interfaces the implementation details are abstracted. This means we can test through the JVM with pure JUnit tests but more importantly, because the business requirements are implemented in one place ( core ) we can surround these with tests that ensure when new functionality is added the business logic of already written features is not broken. Ports and Adapters help towards a separation of concerns which makes testing simpler and more modular. It is simple because we are starting to split our app apart, like we said before hexagonal architecture can feed into and work well with other best practices. Following the Single Responsibility Principle of SOLID allows each module to take on a single responsibility. The core is responsible for our business logic and how the application differs from everything else out there. Mobile is left to implement the intricacies of the platform and is very much specialised to the domain the user will be using the app upon - rather than the domain of what the user will be viewing. Re-use is improved with hexagonal architecture. The possibilities with creating such ports around your specialised domain logic means you could theoretically take the core module and only need to create a new mobile module written in another language or another platform like, the web, javascript, iOS on an iPhone, maybe even a CLI interface for integration testing. This way you can do cross-platform applications without having to re-invent the business logic wheel each time. Maintainability, this is a big big plus. Did you see what just happened with Parse having their kill switch flicked ? If you had a hexagonal architecture, changing out Parse is just small implementation detail. The benefit is that you have your full domain tested and changing Parse means changing it only in one place and running the tests afterwards for sanity. Like we discussed before with a port for a RequestExecutor and an adapter for a FacebookParseRequestExecutor you can imagine easily being able to switch this out for a FirebaseRequestExecutor or a RetrofitRequestExecutor or to take it one step further a MockRequestExecutor or a LocalFileSystemRequestExecutor . Hopefully you can start to see some of the power ports and adapters / hexagonal architecture can give you. Although a core module is not spoken about in the hexagonal architecture pattern, on Android it really helps you to understand what is part of your applications domain and what is part for the platform. In keeping with expanding the pattern, Robert Martin has took this further and declared his clean architecture which for me has many crossovers with hexagonal architecture. It is great at explicitly saying what layers should have what responsibilities (our original problem). What is misses out is the notion of ports and adapters , therefore knowing both architectures is a real benefit. I realise I've not given much implementation detail here, but that's on purpose - there are many ways to do this and hence why we are still experimenting. I want to start a discussion about pros, cons and possibilities, let's talk .", "date": "2016-02-01"},
{"website": "Novoda", "title": "Novoda’s Design Team Tools", "author": ["Dave Clements"], "link": "https://blog.novoda.com/novodas-design-team-tools/", "abstract": "At Novoda, our design team is full of enthusiastic individuals who tinker and toy with all manner of new tools. We’ve tried a lot of products just for fun, and sometimes those products end up being part of our process. To decide if a tool becomes ‘officially’ used is a simple process. Someone finds something cool, we all try it out, weigh up the pros and cons and vote on it. If we feel the product is the best and most efficient way of performing the tasks it’s aimed at, we’re won over. The list below all fall into that category. Our Favourite Design Tools In no particular order! Pencils, pens and paper Winsornewton.com The most useful of design tools. We can scribble up basic designs during meetings, workshops and at our desks, create simple user flows, and even work up fast prototypes. We all ensure we have these available at all times. Sketch Sketchapp.com We switched from Photoshop to Sketch for a couple of reasons. The biggest advantages were, for us, the sheer speed and the simplicity of Sketch. It just so happens that this little product has a huge heart in the form of a growing community of designers, plugin makers and tutorial writers. Zeplin Zeplin.io Zeplin is a product that plugs into Photoshop and Sketch. We use it primarily as a Sketch plugin. The idea behind Zeplin is simple: one shortcut key and your developers have immediate and up to the minute details in a visual format on the specs for your layouts. When the developer needs to know the size of a font, the colour of a background, or pretty much any other aspect of a static design, it’s a simple case of opening the app and viewing. Commenting is available too, so if a developer needs to ask a question pertaining to a specific item on the screen, it’s easy and quick to get a reply. Invision Invisionapp.com We post most of our work into projects on Invision. All members of the project team, be them from Novoda or from the client, have access to the project. Every time a screen is updated, these team members have an immediate visual on it. Invision has a full commenting system built in. The workflow to bring your designs from the work in progress phase through to approved state of each screen is fantastic and really simple. Full prototyping is also possible and you can preview, via a web link, to a mobile device, which is rather useful. With Invision, we’ve managed to decrease our iteration-to-approval time on design work and, so far, each client we’ve worked with has really enjoyed using it. Omnigraffle Omnigroup.com User flows are exceptionally important in the design process. Not only for the design team to work out the happy and sad flows, but also for everyone in the project to understand the various scenarios they’re aiming towards. While this could be done in a variety of other tools, we’ve found none to be as flexible and efficient as Omnigraffle. Sure, it’s not the prettiest app in the world. But for us, it’s the best way to achieve the results we need. Principle for Mac Principleformac.com We found out about this app in the last few months and simply can’t get over how beautifully simple it is to create motion and interaction prototypes. It’s so easy, you might say it’s the Sketch of motion design.  It is quite limited in its toolset, what you can achieve with it in just a few minutes is super impressive. Add to this a great community of designers and animators that is constantly putting up downloadable prototypes and really helping each other out. Photoshop Creativecloud.com Adobe’s powerhouse hasn’t so much fallen from grace for a lot of design teams as much as stagnated a little. Its sheer size and power is unmatched by any other app, and its ability to work within so many different scenarios is definitely impressive.  However, those two things are really what allowed Sketch to take the primary tool slot for us at Novoda. We still keep it around for compatibility, the vast feature set, and just a little nostalgic respect for helping us all learn our craft! Illustrator Creativecloud.com Illustrator isn’t actually left in the back as much as Photoshop is these days. While Sketch is fully vector (minus bitmap images of course), it can’t compare to the powerful editing tools in Illustrator, and so for making specific assets such as icons, Illustrator is still an incredibly powerful product.  It does suffer from the same overly complex nature that afflicts Photoshop too, and boy is it slow when you have a lot of files open - hence why we definitely don’t use it to create all our screen designs - but for now, it has a safe place in our process. Use what works for you We actually don’t tie everyone down to one set of tools.  Beyond these core products, we are always trying out new things, and occasionally other products are used for more specific tasks.  The tools we have above are found by the team to be the best for the core of our work. But everyone should work with the tools they feel most comfortable with, and give them the results they want.  Happy creating!", "date": "2016-01-28"},
{"website": "Novoda", "title": "Introduction to SimpleChromeCustomTabs", "author": ["Juanky Soriano"], "link": "https://blog.novoda.com/introduction-to-simplechromecustomtabs/", "abstract": "Before August 2015, when an application had to open third party content hosted on a website, developers options were limited to open the content in a external browser or using a WebView . This was apparently enough, but after a time developers started feeling that the options were insufficient. In one hand opening a link on an external browser represents a heavy transition as it doesn't allow UI customisations, making harder for the users to return to the app. In the other hand WebView allow UI customisation, however that customisation is not always trivial and WebView doesn't provide many of the features available in browsers. But as I said this was before August 2015 . So, what changed then on August 2015? Android received Chrome Custom Tabs as part of the Android Support Library revision 23 . Chrome Custom Tabs combines the best of the two options existing to the date; it will allow you to customise Chrome Browser UI, helping on making the transition between contents almost insignificant. This is possible because it allows customisations on the browser toolbar colour, up button, action button, menu items and the start / exit animations. So, problem solved, right? you might be thinking... But unfortunately it is not that simple, Chrome Custom Tabs is sometimes hard to integrate within your application and that could eventually lead some developers and products owners to postpone over and over the decision to devote precious time on that non-trivial integration; time that could be instead used on developing much cooler features. At Novoda we have been there, and for that reason we decided to create SimpleChromeCustomTabs as a way to reduce the effort required on this integration. Now we are sharing this approach with you. Show me how! In the following lines we are going to show you how to include SimpleChromeCustomTabs in your application. For this exercise we are going to take a DemoActivity consisting of a button which, on click, opens a website on the browser. Then we are going to include SimpleChromeCustomTabs in the project, and will use it to open the website into an non-styled Chrome Custom Tab. Finally we will style our tab to provide a different look & feel. Note : For convenience the Application class has been extended on DemoApplication . Adding SimpleChromeCustomTabs to your project The only thing you have to do is to include the following into the build.gradle of your project. repositories {\n    maven {\n        url  \"http://dl.bintray.com/novoda/maven\" \n    }\n}\n\ndependencies {\n    compile 'com.novoda:simple-chrome-custom-tabs:0.1.1 '\n} Integrating into your app Let's take a deeper look to our activity at this moment: public class DemoActivity extends Activity {\n    \n    private static final Uri WEBSITE_URI = Uri.parse(\"https://html5test.com/\");\n    \n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        \n        setContentView(R.layout.demo_layout);\n        Button button = (Button) findViewById(R.id.open_url_button);\n        button.setOnClickListener(openUrlButtonClickListener);\n    }\n\n    private final View.OnClickListener openUrlButtonClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            Intent intent = new Intent(Intent.ACTION_VIEW)\n                    .setData(WEBSITE_URI);\n                    .addFlags(Intent.FLAG_ACTIVITY_NEW_TASK);\n            startActivity(intent);\n        }\n    };\n\n} Nothing has changed so far, we have only included the dependency but we still have to make a good usage of it. Let's now use SimpleChromeCustomTabs to open the website; we are not going to style the Chrome tab yet, but don't worry, we will. Firstly we will initialise SimpleChromeCustomTabs . Initialisation only needs to happen once in the application lifecycle, but it is important to do it as otherwise we won't be able to connect SimpleChromeCustomTabs to our activity. We do it on DemoApplication.onCreate() . public void DemoApplication extends Application {\n \n    @Override\n    public void onCreate() {\n        super.onCreate();\n\n        SimpleChromeCustomTabs.initialize(this);\n    }\n\n} The following steps will be to open a connection between DemoActivity and SimpleChromeCustomTabs ; we do this on DemoActivity.onResume() . public class DemoActivity extends Activity {\n\n    ...\n    \n    public void onResume() {\n        super.onResume();\n        SimpleChromeCustomTabs.connectTo(this);\n    }\n\n} Remember to disconnect on DemoActivity.onPause() . public class DemoActivity extends Activity {\n\n    ...\n    \n    public void onPause() {\n        super.onPause();\n        SimpleChromeCustomTabs.disconnectFrom(this);\n    }\n\n} Once we have a connection we can navigate using a Chrome Custom Tab, so let's modify the way we handle click events in our button. public class DemoActivity extends Activity {\n\n    ...\n    \n    private final View.OnClickListener openUrlButtonClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            SimpleChromeCustomTabs.navigateTo(WEBSITE_URI);\n        }\n    }; \n\n    ...   \n} With this changes in place our activity will look pretty much like this: public class DemoActivity extends Activity {\n    \n    private static final Uri WEBSITE_URI = Uri.parse(\"https://html5test.com/\");\n    \n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        \n        setContentView(R.layout.demo_layout);\n        Button button = (Button) findViewById(R.id.open_url_button);\n        button.setOnClickListener(openUrlButtonClickListener);\n    }\n\n    private final View.OnClickListener openUrlButtonClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            SimpleChromeCustomTabs.navigateTo(WEBSITE_URI);\n        }\n    };\n\n    public void onResume() {\n        super.onResume();\n        SimpleChromeCustomTabs.connectTo(this);\n    }\n\n    public void onPause() {\n        super.onPause();\n        SimpleChromeCustomTabs.disconnectFrom(this);\n    }\n\n} Now we have a Chrome Custom Tab ready to be styled, Almost done! Styling the Chrome Tab Now we are going to give a different look and feel to our custom tab, there are many different customisations you can do, but as part of this exercise we will just tint the toolbar and change the back navigation icon to a custom one. To do so we will make use of the SimpleChromeCustomTabs navigation builder. public class DemoActivity extends Activity {\n\n    ...\n    \n    private final View.OnClickListener openUrlButtonClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            SimpleChromeCustomTabs.withIntentCustomizer(intentCustomizer)\n                    .navigateTo(WEBSITE_URI);\n        }\n    };   \n\n        private final IntentCustomizer intentCustomizer = new IntentCustomizer() {\n        @Override\n        public SimpleChromeCustomTabsIntentBuilder onCustomiseIntent(SimpleChromeCustomTabsIntentBuilder simpleChromeCustomTabsIntentBuilder) {\n            return simpleChromeCustomTabsIntentBuilder.withToolbarColor(getToolbarColor())\n                    .withCloseButtonIcon(decodeCloseBitmap());\n        }\n    }; \n\n    \n    @ColorInt\n    private int getToolbarColor() {\n        return ContextCompat.getColor(this, android.R.color.black);\n    }\n\n    private Bitmap decodeCloseBitmap() {\n        return BitmapFactory.decodeResource(getResources(),  R.drawable.ic_arrow_back);\n    }\n} What we have now will allow our users to quickly come back to our application once they are done with the website! Not only that, we have given a different look & feel to the Chrome tab. Wrapping up We have integrated SimpleChromeCustomTabs and replaced our old navigation to website procedure to a better one which will help us to maintain our users in context by mean of chrome tab customisations, helping them on coming back to the application with just one touch. But there are many more things you can do with SimpleChromeCustomTabs and we invite you to explore a more complete customisation at SimpleChromeCustomTabs - Extended demo activity . Time to navigate!", "date": "2016-01-08"},
{"website": "Novoda", "title": "Implement good auth flows with Smart Lock for Passwords", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/implement-good-auth-flows-with-smart-lock-for-passwords/", "abstract": "I hope you learned a bit about using Smart Lock for Passwords , by reading the first part of this series. Without further ado, let’s take a look at how easy it is to implement. “Ile de France” by Jean Helion , from Museu Berardo You’ll need to have Play Services as a dependency for your app: dependencies {\n  compile 'com.google.android.gms:play-services-auth:8.3.0'\n} Next, add a GoogleApiClient to your activity. This is the usual Play Services implementation, which you probably have already dealt with. I’ll leave the nitty gritty to you, but you can find a step-by-step guide on the Google Developers website . Retrieve credentials Once that’s done, it’s really easy to request any stored credentials , using this snippet: CredentialRequest credentialsRequest = \n  new CredentialRequest.Builder()\n    .setSupportsPasswordLogin(true)\n    .build();\n\nAuth.CredentialsApi\n  .request(apiClient, credentialsRequest)\n  .setResultCallback(myCallback); The callback is a ResultCallback<CredentialRequestResult> , where the (successful) result you receive in onResult() contains the credentials that you can then use to populate the UI. Store new credentials To store new credentials , simply pass an object containing the credentials to save: Auth.CredentialsApi\n  .save(credentialsClient, credentials)\n  .setResultCallback(myCallback); The Credential class is a simple key-value pair holder, for us, that contains the username and password we want to store. Also of note, you can keep more than one set of saved credentials per app at any given time in Smart Lock. If that was the case, when you try to load them, you’ll be presented with an error with a resolution. Launching that resolution will prompt the user to pick which of the stored credentials to use. The normal flow will then resume. Delete stored credentials To delete a set of saved credentials , use the delete method: Auth.CredentialsApi\n  .delete(credentialsClient, credentials)\n  .setResultCallback(myCallback); The credentials object doesn’t have to be the same instance you get back from Smart Lock. You can create a new one using the same data and it will still work. You don’t need to keep the credentials object around. Hint email from the device’s accounts Lastly, even if a user has no saved account info, you may want to use their email to help them sign up quickly . PendingIntent pendingIntent = Auth.CredentialsApi\n  .getHintPickerIntent(credentialsClient, hintRequest);\n\nstartIntentSenderForResult(\n  pendingIntent.getIntentSender(),\n  MY_REQUEST_CODE_SIGNIN_HINT,\n  null, \n  0,\n  0,\n  0\n); This will display a dialog that allows the user to use one of their device accounts email addresses, or enter a new one. Smart Lock will remember email addresses the user has used previously and propose them again. The hintRequest allows some customisation of the dialog. It is constructed simply using the HintRequest.Builder . If the user has picked one of their Google accounts, you will get the following information in the result callback: a Credential object that contains the user ID (the email) one or more authentication tokens (OpenID tokens — not used in our example) optionally, the display name and avatar image URL for the user. This should also contain a generated password, but in my tests it has always been null . Bringing it all together Please review the source code for this article on GitHub, if you would like a more complete sample implementation: https://github.com/rock3r/smartlock-demo The code in that repo is based on the official sample code for Smart Lock, but is more complete and shows how to handle more possible edge cases. Now go out there and make your users’ lives easier ! As always, a huge thanks to everyone involved in making this post as good as it is, both Googlers and proofreaders. You’re awesome, guys! This post has been cross-posted from my Medium blog .", "date": "2015-12-16"},
{"website": "Novoda", "title": "Happier Holidays", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/happier-holidays/", "abstract": "Last week our amazing designer Qi released a beautiful wallpaper as an happy holiday season wish from Novoda. While the picture looks great on both desktops and phones, the Android framework gives us developers the opportunity to create something even more captivating and I started playing a bit with live wallpapers . Enter Snowy Village Live Wallpaper . I divided Qi's design in 5 different layers used to draw the background, in order to provide a delightful parallax effect when scrolling through the launcher screens. On top of that a lovely sprinkle of snowflakes is falling, in order to increase the winter holiday atmosphere. The project is based on Nick Howes' Android Parallax Wallpaper . Did I mention that it's completely open source? If you are curious you can have a look at https://github.com/novoda/snowy-village-wallpaper . You can install Snowy Village Live Wallpaper on your Android phone or tablet from the following link: Android, Google Play and the Google Play logo are trademarks of Google Inc.", "date": "2015-12-15"},
{"website": "Novoda", "title": "Login experiences that don't suck", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/login-experiences-that-dont-suck/", "abstract": "We all loathe entering usernames and passwords into the fields, especially on mobile devices. And we all have very limited capacity of remembering a different password for each service, so we mostly end up ignoring all guidelines for secure passwords and end up using something memorable for everything. Logging into apps and websites is the worst. Some use password managers like LastPass or 1Password to make their life easier while being secure, but that’s not something most of the people even know about. That’s bad . “Ile de France” by Jean Helion , from Tate Gallery The return of the UX Luckily enough, quite some time ago, some smart people has come up with the idea of federated login . For years there’s been a better alternative to usernames and passwords. Starting with the early ventures such as OpenID, and often using technologies such as OAuth. Still don’t understood what we’re talking about? Well, it’s simple. We’re talking about the so-called social logins : The great UX advantage that these federated services offer for signup is that users don’t need to create a new account from scratch when they register, as the information is obtained through the identity provider, making the process a lot easier, decreasing friction and reducing the loss of users at this critical stage of the engagement funnel. But this is not all. After the initial signup process, subsequent logins will be a one-click experience , that don’t require any real user input if the user is already signed into the identity provider (which is usually the case). Now, this doesn’t mean all apps should be requiring users to login, and block them out of the content if they don’t. Remember, the best login experience is the one you don’t have to do. Always make your application and content immediately available to users so they can get engaged. Only once you’ve captured their interest and proved useful to them, then it’s time to show them the login prompt — obviously with federated login options. The credential strikes back For all that federated logins gives developers and users, there’s still people that feel uncomfortable using them, maybe over (legitimate?) privacy concerns. It’s even possible that some users can’t use any of your federated login providers , because they might not have an account with them or might be subject to nationwide Internet censorship. Think China, Russia or some Middle Eastern countries. The classic example would be my mom (hey mom!) logging into an application that only offers Facebook and Twitter as identity provider, but not Google. She has a Google account but neither Facebook nor Twitter. Yes, I am lucky. For all these people, though, you have to provide the good (?) old username & password signup/login . And, as we said, that sucks . One could argue that people that don’t want to use any federated login option does that out of their own choice. They will be likely to power through an old school signup/login flow, but the same assumption doesn’t stand for all other users. Oh, and one other dreadful aspect of credential-based authentication: unless you’re using a password manager that syncs stuff for you, there’s no way you can store and sync the credentials so they can simply be retrieved across devices and platforms. Well… that is true up to a certain point. Browsers such as Chrome and Firefox have offered basic password management and sync features for years , and more recently users of other browsers and operating system have gained such a useful capability. What if we could actually use this data to quickly sign into apps on mobile devices, given Chrome already knows the users’ credentials? Well, turns out, we can! A new hope At this year’s Google I/O, a new API was announced, kind of quietly, for Play Services. That API, called Smart Lock for Passwords . Now, Smart Lock for Passwords is a long name. From now on I’ll refer to it simply as Smart Lock — not to be confused with the other Smart Lock ! A short note is needed here. Smart Lock is a new API and probably still a bit immature. For example, when the API launched, and for the following couple of months, it was substantially broken on the backend side. Recently, with Play Services 8.3, it’s gotten a nice new functionality that we’ll see later, but invoking it in some ways crashes Play Services (not your app), as of version 8.3.01. That said, it’s a great way of simplifying users’ lives (and, thus, making them happy customers). It allows you to store and sync credentials or even federated login tokens for your users. That is, if you have their consent, of course. This means that you can ask Play Services to memorise their login data securely for you, and make it available in the context of your application (don’t worry, only yours) across devices and platforms. Currently Smart Lock APIs are available on Android and, partially, in Chrome — currently only in Beta, should get to the stable channel soon. Meanwhile, iOS support is in the works but there is no public roadmap for that. Smart Lock has already access to the passwords saved in Chrome, though. Smart Lock authentication flows When an user signs up or successfully logs into your app, you can prompt them if they want to save and sync their credentials . Play Services will show this dialog for you: Your job of setting it up is pretty much done by now, as a user and as a developer. The next time the user is shown the login screen, you can check if Smart Lock has any stored credentials for them and log them in automatically (or pre-populate the form), making the sign-in experience effectively a zero- or one-click flow . If there is just one set of stored credentials, you can go ahead and use them to automagically sign the user into your app. Zero clicks ftw! If your user has multiple stored sets of credentials for your app, they’ll be asked which ones they want to use to log into your app. On the code side it simply mean you’re handed back a resolution intent that you need to fire, but in the end your app will always only get a single set of credentials. In either case, the user will see an overlay drawn by Play Services informing them that Smart Lock has been used to retrieve their credentials, and then the flow is the normal signin one. If Smart Lock has no stored credentials for the user, you can use it to ask the user if they want to use one of their devices’ accounts to simplify the process. This doesn’t only include Google accounts, albeit those are currently the only ones that will provide you additional data (display name, profile picture, etc). For Google accounts, you can even get token IDs that you can use to authenticate users, and theoretically you should even get auto-generated passwords to use (but those are currently empty, in my tests). This is the new functionality introduced in version 8.3 that I mentioned earlier: And this is really all you need to do ! What you get almost for free is the ability to acquire users and re-authenticate them without the need for any real interactions (if they’re using their Google accounts to sign up), or at least a quick sign-in mechanism after an easy initial signup. In the second part of this series , we’ll see how to implement Smart Lock for Passwords in your app. Stay tuned! In the meantime, you can look at the post on the Android Developers blog by Steven Soneff, from the Identity team. A huge thanks to the Smart Lock for Passwords team at Google for their suggestions and support, and to all the proofreaders of this post. This post has been cross-posted from my Medium blog .", "date": "2015-12-10"},
{"website": "Novoda", "title": "Happy Holidays", "author": ["Qi"], "link": "https://blog.novoda.com/happy-holidays-from-novoda/", "abstract": "Hey everyone! Novoda wishes you a delightful holiday season and Happy New Year! Here are some festive wallpapers to get you in the holiday spirit - enjoy! Desktop Wallpaper 2560x1440 Phone Wallpaper 1920X1080", "date": "2015-12-09"},
{"website": "Novoda", "title": "Having Trouble Focusing? A Primer on Focus in Android", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/do-you-even-focus-bro/", "abstract": "Designing for non-touch users means ensuring two things: access and feedback . As always, it's a case of identifying user journeys and making sure they are achievable by your target users. A totally accessible app should allow non-touchscreen users to access all information and controls that a touchscreen user can access. The user should be given feedback (aural, visual and/or haptic) as they navigate through your app using non-touch input so they can locate themselves on a screen. This post will demonstrate how you can start to make your apps accessible to non-touchscreen users. These users include anyone with Google Android TV or Amazon Fire TV. It also includes any user that interacts with their Android device via keyboards, dpads, trackballs or joysticks. Android supports the use of bluetooth or USB mice - users can interact with controls by clicking when the mouse cursor is on top of the element they want to interact with. With dpad users, there is no cursor - the controls themselves gain focus and, ideally, indicate this state of focus to the user somehow. What is focus? Focus indicates a View's readiness to consume key events. For example, an EditText that has focus can consume character inputs from a keyboard. Only one View can be focused at a time - the key inputs must be directed to a single View (which can decide to consume the events or let them fall through to the next View, much like touch events). Although every View can be made focusable, not all are focusable by default. You can use the android:focusable property in XML or View#setFocusable(boolean) API in Java to override the default value. EditTexts, Buttons and ScrollViews are examples of Views that are focusable by default. TextView, ImageView and LinearLayout, among others, are not. Given what focusability implies (ability to consume key events), can you reason why these are the defaults? For Views that are focusable, you can use view.requestFocus() to ask the system to give it focus. Focused state The focused state refers to the state a View is in when it's focused - more typically, it refers to the visual treatment applied to the View when in this state. Press states are probably more familiar and are certainly more common. Here we want to change the background of our TextView when it's being pressed: <TextView\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"wrap_content\"\n  android:background=\"@drawable/background\" /> res/drawable/background.xml: <selector>\n    <item android:state_pressed=\"true\" android:drawable=\"@color/red\" />\n    <item android:drawable=\"@color/transparent\" />\n</selector> res/drawable/background.xml is a StateListDrawable. The Android framework will read from top-down, and choose the first drawable where all the conditions are true. In this case, it'll choose the color red when the View is in a pressed state, otherwise it'll choose transparent. For Lollipop and above, we can use a RippleDrawable. In the example below, the default color is transparent. When pressed, the Android framework will animate a ripple using the color red. The mask is used to create bounds for the ripple, so it clips at the edges of the mask - the color itself is not important: res/drawable-v21/background.xml: <ripple android:color=\"@color/red\">\n  <item android:drawable=\"@color/transparent\" />\n  <item\n    android:id=\"@android:id/mask\"\n    android:drawable=\"@android:color/white\" />\n</ripple> To add a focus state, we just need to add another entry in the StateListDrawable: <selector>\n    <item android:state_pressed=\"true\" android:drawable=\"@color/red\" />\n    <item android:state_focused=\"true\" android:drawable=\"@color/pink\" />\n    <item android:drawable=\"@color/transparent\" />\n</selector> Done! Well, almost. As TextView is not focusable by default, we need to make it so explicitly: <TextView\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"wrap_content\"\n  android:focusable=\"true\"\n  android:background=\"@drawable/background\" /> We don't need to do anything to res/drawable-v21/background.xml because the RippleDrawable already supports both focused and pressed states. Testing focused states Using Genymotion (or an emulator) is the most convenient way I've found to test non-touch input. You can also connect a bluetooth keyboard (or USB keyboard with an OTG adapter) to a physical device, or even test directly on an Android TV/Amazon Fire TV. You will find that Views seem to lose focus when you touch the screen (or click on the screen). This is because focus does not exist in touch mode. Touch mode Touch mode is a boolean state of your device - the device is either in touch mode or not. The device is in touch mode if the last interaction was via the user's touch (or emulated touch if using an emulator or Genymotion). Using the keyboard or a dpad will switch the device to non-touch mode. You can check if the device is in touch mode by using the View#isInTouchMode() API. Focusable in Touch mode I lied, focus can exist in touch mode. There's a great article on touch mode by Romain Guy . Some Views can be focused while in touch mode - EditText for example. Consider a login screen with input fields for username and password. In touch mode, you can touch on an EditText to give it focus, and then continue typing with the on-screen keyboard: key events will be directed to that EditText and not the other. New Android developers often think that focusable in touch mode is the solution they need to \"fix\" the problem of disappearing selection/focus The TL;DR of focusable in touch mode is that you rarely need it (beyond what the system gives you). Wrapping up We looked at what focus is, why it's used and how to implement it in simple cases. We covered how to test your app's focused states and touched on (oh ho) touch mode. Finally, introduced the concept of focusable in touch mode and presented a warning not to use it as a quick fix. In a later post, we'll cover some practical implementations - how to make common Android patterns compatible with non-touch input.", "date": "2015-11-06"},
{"website": "Novoda", "title": "Droidcon 2015", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/droidcon-2015/", "abstract": "#DroidconUK15 was a grand boiling pot for the liveliest conversations in Android design & development. We look forward to the annual gathering of the best creators from across the globe discussing and disagreeing in the aim of making better mobile products. Conference Talks Ever opinionated, Novoda were there in numbers this year. Did you attend our talks? If not you can catch up on the videos stored on Skillsmatter: Let's get Functional presented by Benjamin Augustin Let's talk usability testing presented by Leonie Brewin Hinting around: text demystified presented by Sebastiano Poggi & Eugenio Marletti From clockwork to smartwatch presented by Daniele Bonaldo Designing apps that everyone can use on an Android TV presented by Ataul Munim LEVEL UP! A better gaming experience with Google Play Games Services presented by Stefan Hoth Workshop: User Centred Design presented by Leonie Brewin ALL Droidcon London 2015 talks We're always looking to innovate in how we appear at conferences and this time we brought our Continuous Integration! We had a CI wall with a host of different devices which were building demos submitted to branches on a repository on our public github account. We offered a prize to whoever committed the best visualisation to be committed, compiled and plated live live on our booth CI devices during Droidcon. Well done to all those that submitted you can see them all here: github pull requests . Here are some of our key picks: Novoda dots by Ryan Bateman Bread cats by Arjen Boutsema 10 Secs of Bacon by Friedger Müffke Stars by Adam Brown Novoda's winner of the CI Booth demo competition is clearly Memeify for bringing us such meme joy! Memify by Frederik Schweiger Well done! We are sending you a Nexus 5 ! Hackathon A hackathon continued in Skillsmatter over the weekend. We had some great submissions including multi device Halloween pranks, in-car navigation games, photo rendering libraries, customer rewards schemes and games. But the one which really captured our hearts was the NFC Piano by Avelyn Goh . Avelyn wowed us with not only her innovative use of Moo's NFC Cards but also her tinkering to get the feedback just right for use. Novoda said well done by awarding her with a Nexus 5x ! Novoda's hackathon pick: NFC Piano by Avelyn Goh Props to Skillsmatter for all the photos in this article and you can see the full album of Droidcon photos over on their Flickr account here .", "date": "2015-11-13"},
{"website": "Novoda", "title": "Material ProgressBar", "author": ["Mark Allison"], "link": "https://blog.novoda.com/material-progressbar/", "abstract": "At the time of writing I am working with the good folks at Novoda on an on-demand video streaming app for UK broadcaster Channel 4 . One of the designs which I was asked to implement was a standard indeterminate ProgressBar with a material styling. While this was easy for Lollipop and later, the app needs to support earlier devices so the challenge was to come up with a lightweight approximation of the material ProgressBar which would work on older devices. In this post we'll take a look at the solution to that problem. Let's first take a look at a Lollipop indeterminate ProgressBar: While the bar styling itself is pretty easy to achieve, the problem lies in the indeterminate animation which looks rather complex. A short bar runs from left to right, but the length of this bar varies during its travel. My first approach was to try and back-port the material indeterminate implementation. However this proved difficult because it uses an AnimatedVectorDrawable which is not available pre-Lollipop. The solution that I came up with is quite sneaky, but gives a remarkably close approximation to what we're trying to achieve. The solution involves creating our own ProgressBar implementation (which subclasses the standard ProgressBar ) which completely bypasses the standard indeterminate logic and implements its own on top of the standard primary and secondary progress behaviours which are already built in to ProgressBar . The trick is because of how this is rendered - first the background, then the secondary progress, then the primary progress. If we have the background and the primary progress colour the same, and the secondary progress a different colour, we can give the illusion that a segment of the bar is being drawn. An example will show this. If we set the background colour to a light green, the secondary progress colour to a mid green and the progress colour to a dark green we get this: However, if we set the primary colour to match the background colour the section of the secondary progress which is visible gives the illusion that we have drawn a segment: We can specify the start and end points of this by simply setting the secondaryProgress and progress values of the ProgressBar respectively. So let's take a look at how we can implement this in code: public class MaterialProgressBar extends ProgressBar {\n    private static final int INDETERMINATE_MAX = 1000;\n    private static final String SECONDARY_PROGRESS = \"secondaryProgress\";\n    private static final String PROGRESS = \"progress\";\n\n    private Animator animator = null;\n\n    private final int duration;\n\n    public MaterialProgressBar(Context context, AttributeSet attrs, int defStyleAttr) {\n        super(context, attrs, defStyleAttr);\n\n        TypedArray ta = context.obtainStyledAttributes(attrs, R.styleable.MaterialProgressBar, defStyleAttr, 0);\n        int backgroundColour;\n        int progressColour;\n        try {\n            backgroundColour = ta.getColor(R.styleable.MaterialProgressBar_backgroundColour, 0);\n            progressColour = ta.getColor(R.styleable.MaterialProgressBar_progressColour, 0);\n            int defaultDuration = context.getResources().getInteger(android.R.integer.config_mediumAnimTime);\n            duration = ta.getInteger(R.styleable.MaterialProgressBar_duration, defaultDuration);\n        } finally {\n            ta.recycle();\n        }\n        Resources resources = context.getResources();\n        setProgressDrawable(resources.getDrawable(android.R.drawable.progress_horizontal));\n        createIndeterminateProgressDrawable(backgroundColour, progressColour);\n        setMax(INDETERMINATE_MAX);\n        super.setIndeterminate(false);\n        this.setIndeterminate(true);\n    }\n\n    private void createIndeterminateProgressDrawable(@ColorInt int backgroundColour, @ColorInt int progressColour) {\n        LayerDrawable layerDrawable = (LayerDrawable) getProgressDrawable();\n        if (layerDrawable != null) {\n            layerDrawable.mutate();\n            layerDrawable.setDrawableByLayerId(android.R.id.background, createShapeDrawable(backgroundColour));\n            layerDrawable.setDrawableByLayerId(android.R.id.progress, createClipDrawable(backgroundColour));\n            layerDrawable.setDrawableByLayerId(android.R.id.secondaryProgress, createClipDrawable(progressColour));\n        }\n    }\n\n    private Drawable createClipDrawable(@ColorInt int colour) {\n        ShapeDrawable shapeDrawable = createShapeDrawable(colour);\n        return new ClipDrawable(shapeDrawable, Gravity.START, ClipDrawable.HORIZONTAL);\n    }\n\n    private ShapeDrawable createShapeDrawable(@ColorInt int colour) {\n        ShapeDrawable shapeDrawable = new ShapeDrawable();\n        setColour(shapeDrawable, colour);\n        return shapeDrawable;\n    }\n\n    private void setColour(ShapeDrawable drawable, int colour) {\n        Paint paint = drawable.getPaint();\n        paint.setColor(colour);\n    }\n    .\n    .\n    .\n} The key method here is createIndeterminateProgressDrawable() which is replacing the layers in the LayerDrawable (which will be rendered as the ProgressBar) with those of the appropriate colours. The other thing worth noting is that we are hardcoding this as an indeterminate ProgressBar in the constructor - this is purely to keep the example code simple and easier to understand. In the production code this has some additional code to enable the control to operate as a standard ProgressBar as well as an indeterminate one. So now that we can draw a segment, how do we go about animating it? That bit is surprisingly easy - we animate the progress and secondary progress values of the ProgressBar , but use different interpolators for each end of the line segment which results in the segment length changing during the progress of the animation: public class MaterialProgressBar extends ProgressBar {\n    .\n    .\n    .\n    @Override\n    public synchronized void setIndeterminate(boolean indeterminate) {\n        if (isStarted()) {\n            return;\n        }\n        animator = createIndeterminateAnimator();\n        animator.setTarget(this);\n        animator.start();\n    }\n\n    private boolean isStarted() {\n        return animator != null &amp;&amp; animator.isStarted();\n    }\n\n    private Animator createIndeterminateAnimator() {\n        AnimatorSet set = new AnimatorSet();\n        Animator progressAnimator = getAnimator(SECONDARY_PROGRESS, new DecelerateInterpolator());\n        Animator secondaryProgressAnimator = getAnimator(PROGRESS, new AccelerateInterpolator());\n        set.playTogether(progressAnimator, secondaryProgressAnimator);\n        set.setDuration(duration);\n        return set;\n    }\n\n    @NonNull\n    private ObjectAnimator getAnimator(String propertyName, Interpolator interpolator) {\n        ObjectAnimator progressAnimator = ObjectAnimator.ofInt(this, propertyName, 0, INDETERMINATE_MAX);\n        progressAnimator.setInterpolator(interpolator);\n        progressAnimator.setDuration(duration);\n        progressAnimator.setRepeatMode(ValueAnimator.RESTART);\n        progressAnimator.setRepeatCount(ValueAnimator.INFINITE);\n        return progressAnimator;\n    }\n} By making the ProgressBar a little bigger than normal, and slowing down the animation we can see this: Let's return it to normal dimensions and speed and compare it to a standard Lollipop indeterminate ProgressBar implementation: They are by no means identical - the Lollipop implementation actually has a second, shorter animation phase. However, this legacy implementation is a close enough approximation to use on pre-Lollipop devices by having separate layouts the standard one containing our legacy implementation and the one in res/layout-v21 containing a standard ProgressBar . The source code for this article is available here . (This post has been kindly re-published from StylingAndroid", "date": "2015-10-26"},
{"website": "Novoda", "title": "Designing Android Apps For Vision Impaired Users", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/designing-android-apps-with-vision-impaired-users-in-mind/", "abstract": "Making your app suitable for vision impaired users is part of a larger topic on developing accessible apps. Although Android isn't as well regarded as iOS for its support of non-sighted users , over the last few years it has drastically improved . Developing a fully accessible app is hard , mostly because a fully accessible app means: Every goal that is designed to be achieved by a user, must be achievable by every user \"Every user\" is a large undertaking; this post will focus on describing approaches to make our apps accessible for vision impaired users, while not diminishing the experience for sighted users. Avoid using color as the only means to convey information The easiest way to get going on the right track is to avoid using color as the only means to convey information. Color should augment other affordances or used decoratively only. Using the color red, for example, to indicate error states is helpful as an additional cue to the user. But if the user suffers from protanopia or deuteranopia, they may miss the cue - use an error icon or error message as the primary indication of error. The images below show what some different forms of color blindness may look like: _(left to right) original, protanopia, deuteranopia, and tritanopia_ Color contrast Ensuring that the contrast between foreground (text) and background colors is sufficient will help users with low vision. The Web Content Accessibility Group, or WCAG , requires a color contrast ratio at least 4.5 between foreground and background colors to reach \"AA standard\". To determine the ratio, you can use a contrast checker like the one written by Lea Verou on GitHub or on the WebAIM site . Content resizing Allowing users to resize content (especially text) lets them use your app comfortably. There are two base considerations: use \"sp\" units to define android:textSize make sure your layouts are stretchy and don't clip text Supporting text resizing Using scale-independent-pixel (sp) units is similar to density-independent-pixels (dp) but it also takes into account whether the user has checked the \"Large Text\" option in the system \"Settings, Accessibility\". You can see the difference in the Google Play Books app (left is default, right is with \"Large Text\" enabled): For text-heavy apps like Play Books and apps like BBC News , offering an internal text-resize dialog that only affects your app is option worth considering. Of the two shown, I prefer the BBC News app's implementation because it's simpler and the dialog is focused solely on the concept of text resizing: _left: Google Play Books, right: BBC News_ One thing to be aware of when setting the text size programmatically is to make sure you use the correct units in code too. This will ensure your text size still respects the \"Large text\" option in Accessibility settings. Declare your text size options using sp units as per usual: <dimen name=\"text_normal\">17sp</dimen>\n<dimen name=\"text_large\">20sp</dimen>\n<dimen name=\"text_extra_large\">25sp</dimen> In your code, use TextView#setTextSize(int unit, float size) specifying COMPLEX_UNIT_PX as the unit: private void applyCustom(TextSize textSize) {\n    float textSizePx = getTextSizePx(textSize);\n    myTextView.setTextSize(TypedValue.COMPLEX_UNIT_PX, textSizePx);\n}\n\nprivate float getTextSizePx(TextSize textSize) {\n    switch (textSize) {\n        case NORMAL:\n            return getResources().getDimension(R.dimen.text_normal);\n        case LARGE:\n            return getResources().getDimension(R.dimen.text_large);\n        case EXTRA_LARGE:\n            return getResources().getDimension(R.dimen.text_extra_large);\n        default:\n            throw new IllegalArgumentException(\"Unexpected TextSize: \" + textSize);\n    }\n} Responsive layouts The second consideration is about responsive layouts. Designers should be familiar with the term \"responsive design\" - typified on the web with CSS media queries, and on Android with resource qualifiers. Designers and developers need to work together to ensure that specifications show which parts of a layout can grow, and where content should align. When a layout needs to have a fixed size (e.g. a uniform grid), it's important to test that content isn't going to be clipped unexpectedly when the user activates the large text option or if the user's locale is set to use a language that often has words of a longer length. Here, you must find some way to purposefully clip the content (e.g. ellipsizing), but only if that content is available unclipped elsewhere (e.g. a detail screen). Failing to do this makes the content inaccessible for sighted users . Content descriptions A content description is exactly what it sounds like: a textual description that describes the contents of your view. Accessibility services like TalkBack use content descriptions to relate to the user what's on screen. In the case of TalkBack, which functions both as a screenreader and also as an input mechanism, the text-to-speech engine is used to read aloud the content descriptions, and it's this functionality which is useful for vision impaired users. TalkBack You can enable TalkBack via \"Settings, Accessibility, TalkBack\". It's pre-installed on Google Play-certified devices, but if not, you can download it from the Play Store or ApkMirror.com . This video shows you how to enable the TalkBack service and suspend the TalkBack functionality using the L gesture: There are a few settings that you might find useful before switching it on: Explore by Touch should be checked. This enables gesture-based navigation. You can swipe right to navigate to the next item, left to navigate to the previous, and double tap to click the selected item. Use two fingers to drag. Draw an L shape to access the context menu, where you can suspend TalkBack. Automatically scroll lists should be checked. This will scroll a list when you get to the end, so you can continue to use the swipe right gesture, instead of the two finger gesture to manually scroll. Resume from suspend should be set to From notification only . This prevents TalkBack from re-enabling itself from your lock-screen after you've suspended it. Let's try it out! Use the L gesture to open the context menu and, pressing on the circle, drag up to activate \"Read from Top\". This will cycle through all the on-screen views and read them aloud which is a great starting point to test your app (note, this doesn't auto-scroll lists): To navigate manually, you can either run your finger over elements, and TalkBack will attempt to read them aloud. This doesn't scale very well because it's difficult for a non-sighted user to anchor themselves, especially on larger screens, but it might be used by a person with a less severe visual impairment. Instead, you can use swipe right and swipe left gestures to navigate to the next and previous element: To achieve TalkBack support, there's only two things you need to check: everything can be navigated to via next/previous swipe gestures every action can be performed using a \"click\" (double-tap) or \"long-press\" (double-tap-and-hold) After that, it's a case of improving the usability of your app with TalkBack. Example Let's see how we'd do that with a list of items that have sub-actions inside. Our example app will be very simple - it's a RecyclerView with items showing episode descriptions from Adventure Time season one (data sourced from adventuretime.wikia.com ). Here's the app being used with TalkBack suspended: and here's the default behaviour with TalkBack turned on (without any special amendments to support TalkBack): To help assess what needs to be worked on, we can make a list of all the important views and actions: the title of the episode the title card (an image) from that episode the description of the episode onClick star icon which toggles between empty/filled on click onClick item view which opens the details activity and a list of the issues we can see: star icon is unlabelled actioning the star icon loses position in list long time to read (because description) navigating between items takes two gestures per item Star icon is unlabelled The first fix is easy. Android Lint (a static code analysis tool) warns you when you have an ImageView with no android:contentDescription . If the ImageView is actionable (like in this case with the star), then TalkBack will expose it to the user, but with a generated name like \"Button 37\". The description is often dependent on state. In this case, I changed the action (remove vs. add) and also put the episode title in the action so the user doesn't lose context: if (isStarred) {\n    starButtonView.setImageResource(R.drawable.ic_star_filled);\n    String descWhileStarred = \"remove \" + episode.getTitle() + \" from favourites\";\n    starButtonView.setContentDescription(descWhileStarred);\n} else {\n    starButtonView.setImageResource(R.drawable.ic_star_empty);\n    String descWhileUnstarred = \"add \" + episode.getTitle() + \" to favourites\";\n    starButtonView.setContentDescription(descWhileUnstarred);\n} Lint will still warn you though - it only checks the XML for a content description. We used to set android:contentDescription=\"@null\" which tells Lint, \"don't worry, we have thought about it\", or more accurately \"I'm explicitly setting this to have no content description\". I am more likely to suppress this Lint warning globally as it's rare that the content description is not set programmatically. Here we should also have some feedback when the star is clicked - sighted users can see the state of the button being swapped, but TalkBack users should have spoken feedback: private void setStarClickListenerFor(final Episode episode, final boolean wasStarred) {\n    starButtonView.setOnClickListener(new View.OnClickListener() {\n\n        @Override\n        public void onClick(View v) {\n            episodeClickListener.onClickStar(episode);\n            String announcement = wasStarred\n                    ? \"Removed \" + episode.getTitle() + \" from favourites\"\n                    : \"Added \" + episode.getTitle() + \" to favourites\";\n            starButtonView.announceForAccessibility(announcement);\n        }\n\n    });\n} This doesn't do anything if TalkBack is suspended/disabled. Losing position on toggling star In the demo above, TalkBack loses its position when the star is toggled. This happens when using adapter.notifyItemChanged(int position) . It was fixed by adding adapter.setHasStableIds(true) (and ensuring so, by overriding getItemId(int position) in the adapter). This is something that's useful to do in the general case; stable IDs is how scroll position and View state is restored with RecyclerView and AdapterViews. However, the fix also included having to swap adapter.notifyItemChanged(int position) (re-query data for View at position) for the more brutal adapter.notifyDataSetChanged() (re-query data for all visible Views) - I would have expected it to work with the stable IDs change only but it didn't. Long time to read Here the content description is being inferred by TalkBack - the clickListener is applied to the entire item View, but because there is no explicit content description, it will concatenate the content descriptions from children of the item View that both these criteria: child View is not clickable/focusable child View has explicit content description or inherent text (TextView, Button) The simple fix is to set an explicit content description that's shorter: itemView.setContentDescription(episode.getTitle()); This makes the list more glance-able because we lose the long description. We are only allowed to do this (clip content) because the description will be available on the details page for that episode, otherwise this constitutes a loss of functionality for vision impaired users. Navigating through list takes multiple gestures per item Screen readers navigate all accessible content sequentially and linearly: Depending on how many inline actions you have, it can be lose context of the item the action should be performed on - it's for this reason we added the episode title in the content description of the action. There's several approaches you can take here. Mark the star as android:importantForAccessibility=\"no\" (API 16+). This makes TalkBack skip the item, but if TalkBack is switched off, users can still access the toggle as normal. You should only do this if the user is able to star the item from another place, e.g. the details screen. This is the easiest option. Add a long-press action to the item View to toggle the star. This only works if you have a single action (and if you don't have long-press already mapped to something else, e.g. initiating multi-select). TalkBack will announce when there's a long-press (or click) action so it's slightly more discoverable than long-press for non-TalkBack users, but still would have this action somewhere else, e.g. the details screen. Add a context menu/dialog on long-press of the item View to display actions (star, cancel). Again, this should be a convenience feature; there should be a more discoverable way to star this episode, e.g. on the details screen. I like this option the best because it allows for multiple actions and dialog title ( \"actions for \" + episode.getTitle() ) to scope context. You can add the \"cancel\" option to make it easier - otherwise the user will need to use the system Back button to dismiss the dialog. I didn't implement this last fix because it makes it difficult to show the other examples, but here's the app with TalkBack support: FAQ. How can I make my informational list compatible with TalkBack? If the item Views have no click actions, then you can mark each item View as android:focusable=\"true\" and TalkBack will read each separately. Ensure you add a content description to each of these item Views that conveys all the important information. This will not affect non-TalkBack, touch-screen users. It has the side effect of making your list partially compatible with d-pad users though! <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"wrap_content\"\n  android:focusable=\"true\">\n\n  <!-- ... content of list item -->\n\n</RelativeLayout> Are there any resource qualifiers to know when a user has enabled TalkBack? Not as of yet . Can I detect if TalkBack is running to customise the user experience? Yes, please do! The recommended way is to query the AccessibilityManager and check if Touch Exploration is enabled: AccessibilityManager am = (AccessibilityManager) getSystemService(ACCESSIBILITY_SERVICE);\nboolean isAccessibilityEnabled = am.isEnabled();\nboolean isExploreByTouchEnabled = am.isTouchExplorationEnabled(); Be wary of am.isEnabled() as many apps use Accessibility Services as workarounds and the Accessibility Manager will not make any distinction via this method. am.isTouchExplorationEnabled() is better - most TalkBack users will have this option enabled. It isn't a requirement though, so I prefer to be more explicit: private AccessibilityManager am;\n\npublic boolean isSpokenFeedbackEnabled() {\n    List<AccessibilityServiceInfo> enabledServices = getEnabledServicesFor(AccessibilityServiceInfo.FEEDBACK_SPOKEN);\n    return !enabledServices.isEmpty();\n}\n\nprivate List<AccessibilityServiceInfo> getEnabledServicesFor(int feedbackTypeFlags) {\n    return am.getEnabledAccessibilityServiceList(feedbackTypeFlags);\n} In all these cases, you'll learn if TalkBack is enabled , regardless of whether the user has suspended the service. Can you change the voice that TalkBack uses? You can't, but the user can. TalkBack uses the currently enabled Text-to-Speech (TTS) engine. The user can install an alternative TTS engine and select that. IVONA is a highly rated alternative. If you find one with funny voices, please let me know . Wrapping up At Novoda, product features are derived from user requirements - we create personas to write requirements from the point of view of a particular user. These requirements are expressed as user stories - the goal of that user and the steps that user will take to fulfil that goal. Different users will have different goals, and if the app allows, users may achieve the same goal using different steps. With regards to making an app accessible, instead of trying to cater for every user (which is the ideal but very hard), we can focus on the users for which we've added personas (more practical). When we test our app, we ensure that Alex, a 27-year old copywriter who uses Google TalkBack to read AskReddit posts on her way to work, is able to achieve the same goals in our app as any other sighted user. Fortunately, it's not so hard; we probably already do some of it, and the bit we don't do is easy, as demonstrated above. The important (and probably the hardest) part is understanding the effect of how we design and code from the point of view of the user, and in this case, from the point of view of the vision impaired user. Recommended videos and links: Accessibility for Android (Sommer Panage, Twitter University 2014) Accessibility in Action (Kelly Shuster, Droidcon DE 2015) Android Accessibility 101 (Haley Smith, Droidcon NYC 2015) Screen Readers on Touchscreen Devices (Katie Sherwin) Inclusive Design part 1 and part 2 (Bruce Tognazzini)", "date": "2015-10-16"},
{"website": "Novoda", "title": "MakeFest Liverpool", "author": ["Natasha Borsos"], "link": "https://blog.novoda.com/makefest-liverpool/", "abstract": "As events and community manager at Novoda I’ve had the opportunity to attend and travel to a variety of exhibitions and conferences. Most recent of these was MakeFest Liverpool . This was a particularly noteworthy event for me in 2015 for the following reasons: It’s not all Android & apps. I regularly attend Droidcon and other app focused events. MakeFest however had such a variety of exciting technologies - and non-tech but crafting companies alike. From autonomous drones, to augmented reality to masks of aliens at Doctor Who! I had a great time talking to people from a diverse range of industries and left feeling invigorated and refreshed. The new generation. Companies and exhibitors aside, one thing that caught my attention above all was the audience. MakeFest attracted local enthused and curious guests - including a large number of families. I found it incredibly impressive how many young children attended and how much they knew, understood and loved about technology. I met children who were coding their own apps at age 12 and parents who couldn’t keep up with them but supported them immensely. This left me feeling curious for what the future holds for our next generation (and motivated to up my game!) picture Get out of London. It was refreshing to be part of something outside of London - There is a tendency to get lost in a bubble when you live and work in the big smoke. For me this was a real eye opener into the amazing communities forming outside of London and around technology and enthusiasm in Liverpool. MakeFest showcased the variety of talent, innovation and creativity Liverpool has to offer. For Novoda MakeFest was a great opportunity to reach a wider audience and get them excited about what we do. It also inspired us to get creative and build something we may not have done without this opportunity. Our Android developers; Paul and Connor worked together on creating a portable Android auto box ( find a write up here )  which took a lot of research and hands on graft, things they aren’t used to day to day writing Android software. Leading to a hard earned and well received piece of hardware, which we will now use at future exhibitions. Overall a brilliant event to attend and be a part of - watch out for great things coming from Liverpool…", "date": "2015-11-01"},
{"website": "Novoda", "title": "Why can’t you just test on a simulator instead of getting real devices?", "author": ["Jonathan Taylor (Head of QA)"], "link": "https://blog.novoda.com/simulators-vs-real-devices/", "abstract": "Alright, you’re a mobile software test engineer and you want to get some test devices in house.  But your boss has the purse strings pretty tight, as they do sometimes.  That’s fine.  It’s up to you to convince the powers that be that you need them.  Let’s go into the meeting prepared with some counter points against the “Can’t you just test on simulators?” argument. First off - Basic terminology An emulator is a virtual device that tries to replicate not just the appearance of the software under test but also the underlying hardware and os as well. A simulator is a virtual device that tries only to replicate the appearance and behaviour of the application as if it were running on a real device. For the most part when testing Android apps in a virtual environment, you’re going to be using an emulator.  For the most part when testing iOS apps in a virtual environment, you’re going to be using a simulator. Let me also say that simulators and emulators have their places in testing.  There are absolutely some tests that you want to run on them, depending on where you are in the dev cycle.  Say you’ve got a feature in development that has simple acceptance criteria that says GIVEN the user has opened the app, WHEN they click on the ABC button, THEN the user should be taken to XYZ screen.  Since this functionality definition is simple and not platform dependent, we’d have high degree of confidence that validating in an emulator would suffice. You want to run broad stroke tests, high-level functionality and common gestures in emulators/simulators.  But where do they fall short? Argument No. 1 - Performance will be different No matter how accurate the virtual representation is, it’s not the same hardware your application was designed to run on.  And your application's performance will vary between real device hardware the hardware that is being mimicked.  And that’s not to mention performance testing scenarios that aren’t reflected at all in the virtual environment.  What happens to when your app puts too much strain on battery consumption?  Or your application is running away with the CPU and causing temperature to spike? Argument No. 2 - Hardware differences make testing harder There are other testing scenarios that hardware differences make difficult if not impossible to work around.  If your app makes use of 3D Touch or Force Touch , Touch ID , pairs with a bluetooth device, or just uses the front facing camera.  There’s likely a workaround for any of these in the testing arena, but they come at a development cost.  Other limitations can be found when testing for different types of connectivity. GPS locations can be mocked but it’s much harder to reproduce specific events like accelerating, signal loss, or real driving conditions. Likewise for NFC, casting to a Chromebox or other device, using Airplay etc. Accessibility testing ( TalkBack and VoiceOver ) is also much more efficient on real devices, as emulators require and additional apk install. Argument No. 3 - Simulating network speeds is a fallacy That brings me to network conditions.  Yes, yes.  I know.  You can simulate a chatty network, dropped packets, reduced speeds.  Let me share some of my speed tests I ran on an Android 7.0 Samsung S8.  Comparing a real 3G connection to one that has been set up proxied through Charles , with the connection throttled down to simulate a 3G network speeds [1] I did 3 benchmark speed tests setting my device to use only 3G connection Average download speed:  8.30 Mbps Average upload speed:  2.15 Mbps Real 3G connection Then I set my phone to use WIFI, connected to a proxy (Charles) on my laptop. Average download speed:  52.50 Mbps Average upload speed:  30.57 Mbps Proxied, no throttling Then I set my phone to use WIFI, connected to a proxy on my laptop and throttled the connection down to what the tool says is 3G speed. Average download speed:  40.30 Mbps Average upload speed:  0.89 Mbps Simulated 3G Ok, the upload speed is drastically reduced, but it’s only about 40% of an actual 3G upload speed.  And what’s up with the download?  Again, the speed is reduced, but not to 3G speed. Now for most features this is probably going to be fine.  More often than not, the feature in my experience has an acceptance criteria that says something like “feature should work on slower network speeds”, so proxying in this way would, to some degree, be acceptable testing conditions.  But if you want to test at 3G speeds, you have to walk outside, smell some fresh air and test on actual 3G speeds . Argument No. 4 - Designing on simulator is not a thing And then there is design.  Can’t tell you the number of times that a designer has pinged me and asked “Hey, have you got the new Motoroid Pixie SG 9000?\"  From the design team's perspective, it's absolutely essential to have real devices for their growth and marketing, creative and product design activities, including: Pre-visualisation prototyping Device variation testing (resolutions, adaptive layout concepts) Designing and prototyping visuals and motion/interactions while connected to a device through mirroring tools (eg. Sketch , Principle , or Flinto ) User testing of visualised work Without access to test devices, designers are forced to use their own personal devices.  This basically means that they are designing for a single platform and are forced to find workarounds in designing apps for other platforms at the expense of speed and efficiency. Argument No. 5 - Collaboration is easier One of the best reasons I can think of for having a real device is collaboration.  When you’ve found a bug while testing, it’s nice to be able to just walk over to the developers desk, and show them the behaviour that you’re seeing.  Usually eliminates the response “Works on my machine.\" In Summary The take away here is that we’re not designing and developing applications to be run simulators.  We’ve developed these apps to run on real devices in the hands of real users. I understand that the speed tests were not a scientific experiment, I have no idea what else was happening on the wifi at the time, or background processes on the device, caching between tests, etc.  But I still think the point is valid. ↩︎", "date": "2017-11-02"},
{"website": "Novoda", "title": "Android Things - Hackster.IO Competition IoT possibilities", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/android-things-iot-proof-of-concept-devices/", "abstract": "HacksterIO recently launched a competition with Google & NXP to explore new AndroidThings IoT concepts. Here we run through the Novoda entries and get some insights from the authors. Novoda wanted to explore the AndroidThings platform and understand what it could offer in terms of its space in the IoT ecosystem and all the connected products in the cloud that could be leveraged to create an interesting connected IoT solution. Google, Hackster, and NXP are inviting you to submit a project that showcases your use of Android Things and Google technology. Android Things lets you build professional, mass-market products on a trusted platform without previous knowledge of embedded system design. We’d love to see what you can build with our latest release. Office meeting room hot seats This project is about highlighting the use of seats and meeting rooms. I wanted to bring to peoples attention how rooms are used and how often seats are sat upon. I also wanted to bring a bit of psychology into the picture so that people are conscious about the seat they are choosing to sit in and what the history of that seat is. To then perhaps see if this influences future choices. Key technologies involved include: AndroidThings i2c protocol Google Cloud IoT Core Google Pub/Sub Firebase I really enjoyed investigating weight sensors for this project. I think the most interesting thing about the final product, is now we have a lot of data about our meeting rooms and seats. It leaves the door open for others to use Data Science to infer new things from the dataset. A companion app is used to show the layout of the meeting room and a heat map of the most active seats for that room over the last 8 hours. With each seat glowing from a range of dark blue (not sat on recently) to white (someone has just left that seat). The companion app shows a blueprint of our office, and although it is a proof of concept, it has a lot of potential to really engage users in the meeting room use. Future ideas include being able to select meeting rooms or filter by the least used room/seats. You can see the full project details (and how to make your own!) here . Android Things Word Clock What about a clock that speaks the user own language? Here it is! The core concept of this clock is that during the day, only the letters needed to create the current time in words will be on, while the others will be off. This project is a perfect example of Smart Home device, providing smooth integration with the Google Assistant which allows a natural interaction using voice commands, all in a final package that it is nice looking and can be placed in any living room. Key technologies involved include: AndroidThings, Arduino UART protocol Actions on Google Google Smart Home Firebase While working on this project I was able to reuse technologies and components I already knew with previous IoT platforms, like Arduino, and have them work with Android Things. Doing so I could use languages and tools I'm more comfortable with (like Kotlin and Android Studio) while at the same time being able to create a more tested and clean project code. When developing, we talk about the \"happy path\" to get a feature working. It's always important to think of the things that can go wrong. In case of missing internet connection after a reboot, an appropriate error state will be displayed. The \"sad path\". The integration with the Google Assistant through Smart Home was probably the most interesting part of the project. It allowed me to create a device that can be controlled directly from the phone, or other Assistant-enabled devices like a Google Home, in a natural way using voice interaction. You can see the full project details (and how to make your own!) here . Smart Dog Bed The idea of this project is to use weight sensors and put them under your dog's bed, then using an Android Things board we're able to measure the data from the sensors and we can analyse it to measure the sleep quality of your dog, as well as how often did your dog get up during the night and how much does your dog sleep during the day. I use an activity tracker which helps me understand the quality of my sleep and I wanted to take this idea and apply it to the life of my dogs, so I decided to create this smart dog bed that can do all of that for my dogs. Key technologies involved include: AndroidThings, Espruino for initial tests I2C protocol Firebase Database Bella knows there's something funny going on with the bed I've learnt a lot while working on this project, I had the chance to use the Firebase Database on Android (which I had never done before), as well as understanding all the different hardware components involved – load cell gauge, analog to digital converter, op-amp, you have to really understand what each component does individually first to be able to put it all together. As an initial step for simplicity, we can use an ESP32 running Espruino for a quick prototype. it's essentially an Arduino board that can be programmed with javascript. It includes a 12-bit ADC so it was very easy to write a quick test to ensure the wiring was correct. Once that was confirmed I could include the AndroidThings board and wire it up to the ADC and the Load gauge like below. Schematics for the Android Things device The project could be extended to record more data and use the Google Cloud Functions for Firebase to be able to collect more insights from the hardware readings, such as tracking the weight of the dog, showing a timeline view of the events captured, etc. It's an area with a lot of potential. You can see the full project details (and how to make your own!) here . We create the projects to explore the uses of AndroidThings, it was great fun to create IoT projects. All three ended up using Firebase and it really showed us the power of having a connected IoT board that can easily leverage Googles cloud infrastructure. Hold tight to see what we make next.", "date": "2017-11-09"},
{"website": "Novoda", "title": "The Novoda Craft University - Part 2: The Teaching", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/ncu-part-2-the-teaching/", "abstract": "This blog post explains how a high level of craft is kept up and shared amongst our 40 plus engineers. Part two explains what the NCU is and how it integrates into your journey. We’re a digital product agency, and we try and deliver the best value for our partners through mobile apps and digital transformations. We’ve found through excellent communication, constant feedback loops and the art of crafting, we can deliver value over an over for partners. If you haven't read part one yet, you can find it here . Novoda Craft University We’ve created Novoda Craft University (NCU) to help people with their journey. This is the company trying to support the learning of our individuals. Your company should be helping you with your journey. They can give you options and opportunities, opinions and a choice of directions. Employees can take this journey together. Helping support one another, learning and teaching. Improving.. as a cult , as a team. What is the NCU? This is our guidebook to help people begin and continue their journey. It’s an attempt to show learning from multiple different aspects. To help direct and show the different areas learning can happen in. It has 5 Modules, each targets a different learning area. We set it out in a very target orientated manner. Module 1 - Platform & Process First, is learning the Platform and Process. We want people to understand our processes when they join the company. This means learning how the teams collaborate over GitHub, with pull requests, comments etc. It also means understanding the Novoda code style guidelines. Understanding project layout and Novoda conventions. We want people to take a fresh look at the platform they are going to work on by implementing a simple app or library.  Each person tries to learn something they didn’t know or refreshes old knowledge about the platform. They could take an online Course, for example, Udacity or Coursera. They could do a Google code lab for a new feature. Or anything else they want, we try to apply structure but then leave the learning very open-ended. We also have them read “Clean Code” by Uncle Bob and then share back some insight or knowledge they have acquired. Writing code and reading other people’s code is a major part of a developer’s day. Some statistics even indicate that reading other people’s code occupies a larger percentage of a developer’s time than writing code. Therefore, it makes sense that a software crafter should hone their skills relating to writing code. The better code one writes, the easier it is for everyone to read , understand, and maintain that code. Sharing is usually done in the form of a hack and tell. This is where one person gives a brief talk 10-20mins about what they have learnt on a Google Hangout, and everyone in the company is invited to come along if they want. It’s also recorded for those that can’t make it. The idea is twofold, first, it encourages the person to share their learning and learn how to explain what they have learnt. Also, it encourages others to seek out learning and be enthused by what they see. Module 2 - Best Practices While Module 1 emphasized platform knowledge and platform coding strategies, different programming languages have their own sets of best practices and standard conventions. The first task in this module is to read one or more books from a list of what we at Novoda, consider to be some of the best language specific programming books. We have a buddy system as well for discussing what book would be appropriate. Then we ask the person to take a learning from the book and enact a change on one of our projects or open source repositories. Which encourages code reading as well as refactoring and cleaning up. Finally, we ask the person to take part in a Kata usually run by their buddy in the company, this show’s them the kata process. A code kata is an exercise in programming which helps programmers hone their skills through practice and repetition. A simple task is setup and participants repeatedly solve it, perhaps with different rules or set of tools they need to use. Module 3 - Testing Code written for consumer applications is never a static entity. Requirements and designs often change many times midway through development and, even when they don’t, there’s never any guarantee that the code written will work as designed because errors (bugs) do happen, no matter how experienced and knowledgeable the developers are. In light of that realisation, two questions come to mind: How can we write code that is as bug-free as possible? How can we write code to account for design changes, in a way that keeps the code clean, maintainable, understandable? Testing. Like Module 2, this module requires a book. This time around, they’re asked to read Growing object-oriented software, guided by tests, by Nat Pryce and Steve Freeman. Additionally, it is your turn to run a coding kata. For running a coding kata this book by Emily Bache is amazing. The Coding Dojo Handbook. It has lots of examples of Katas but also discusses how to set up the space and organise the event. Module 4 - Agility There is one aspect of true software craft that we have not addressed yet: being part of a team. Large projects are generally far too complex to be developed in isolation and involve a team of people with various responsibilities and skills, from different departments and, sometimes, even from different organisations. At Novoda we always try and explain, “we don’t do agile”, we are agile . This means being flexible in our approach to work and offer value rather than just offering an inflexible solution. So this module was really hard to put across, we’ve come up with a list of books that attempt to convey this but I feel the more important part of the module is the practicality. We ask for them to run a retrospective and a workshop. Running the retrospective allows them to take a step out of their normal role and see how the team acts from the other side. I’d like to point out two things, first Scrum: a Breathtakingly Brief and Agile Introduction, has only 50 pages with each page having 4-5 words on each page. It really is brief, and it represents how we see agility and being reactive and agile should be. The other is ‘the nature of software development by Ron Jeffries’, this really cuts to the chase about what I was saying about value. Do your processes and practices bring value? If they don’t why do you do them. Great book. The workshop is a flexible meeting. Workshops are group discussions, usually with a specific methodology and a specific expected outcome. Typically these aim to either define, clarify, or spread knowledge of aspects of the product being developed. So for example, a workshop could involve a prioritisation session for new features, or brainstorming with the 5 whys about a recurring but non-fatal error that should be fixed. We believe each person should have the skills to run one of these meetings, to be able to solve problems through collaboration and group discussion. Rather than always flying solo. Module 5 - Professionalism Novoda is a service provider, crafting products and providing guidance based on the partners brief and our own experience. Our clients will generally have greater knowledge of their brand, target market and domain, while we are likely to have a higher level of expertise in the platform and ecosystem they want to target. This requires constant communication on the client’s needs, internal demands, potential solutions and trade-offs. Communication is hard. A software crafter will have developed knowledge of coding, testing and platform specifics. Now we expect them to give effective explanations, use appropriate language for the audience they’re addressing and present written and spoken communications in a positive manner to partners and colleagues. It’s important that we can discuss technical problems together, understand other people's points of view and contribute constructively to meetings. Module 5 aims to help build professional behaviour and communication skills in each of these areas, in order to achieve and maintain a level of professionalism. We have a choice of books for module 5 and are always interested in learning about new titles. We have Sandro’s great book, that describes professional behaviour and your career. We have Getting to yes, which can help with discussions and conflict resolution. The other half of this module is a reflection on past behaviour. We’ve identified common scenarios where communication issues can occur. We ask these to be considered in the context of your own experience, thinking about how you might adapt your approach and develop your communication skills in the future. The problem areas we find are: Giving constructive feedback on pull requests Discussing a technical problem Speaking with colleagues Speaking with partners Proposing alternatives (aka saying no) Here we ask for a blog post or a H&T to show the importance of one of these behaviours. One more point, each of our chapters has references to previously completed works by other students. This shows potential ideas, but also that the NCU is a collaborative effort and others have been where you are. Completion is celebrated and everyone is notified. We do give a little celebration gift when people complete, but it is (or should) never be expected. Learnings Learning is a journey, so what have we learnt through the creation of the NCU. When trying to make complex & iterative mobile products, it's one thing knowing what to do, but it’s another teaching it to others. Mobile is a young field, some best practices have now been established but there is a constant evolution in the approach. With writing the NCU down, we found two issues: We had to define what best practices where for us and cement our beliefs We had to then maintain this documentation and ensure it is kept up to date. It is extra overhead but we find it worthwhile for new starters. Building Android & iOS products can be like working on two separate islands.  However, at Novoda our cross-platform-ness works in terms of our architecture decisions and discussions around the domain. Creating the NCU for Android and iOS wasn’t a perfect process. The different teams seem to have different cultures. Learning was gone about in different ways, and best practices across platforms aren’t at the same level. This is an ongoing challenge for us, we’re taking feedback and iterating, but it’s always important to think about the bigger software picture, make sure you haven’t taken the wrong abstraction and are now in too deep. We are trying to take a step back and look for similarities all the time. Read part three here: https://blog.novoda.com/ncu-part-3-the-learning", "date": "2017-10-19"},
{"website": "Novoda", "title": "Safer SWIFT – Moving the security game to the level of programming languages", "author": ["Fabia Schäufele (IOS SOFTWARE CRAFTER)"], "link": "https://blog.novoda.com/safer-swift/", "abstract": "With software becoming evermore present in our private and professional lives it is probably not surprising that software applications, with all their weaknesses, have also become increasingly interesting to hackers. There exist many different ways to hack applications and crack accounts, e.g. eavesdropping or sniffing network traffic, social engineering and phishing, brute force attacks, or using built in backdoors, to name just a few. Various efforts have been undertaken to reduce the risk for users to fall prey to such attacks. However, a lot of those efforts focus on changing user behaviour and fixing the (visible or hidden) interfaces that a specific technology offers. Put another way, security risks in software applications are often dealt with retrospectively by fixing a breached entrance point. – But what if we started looking for weak spots at a much earlier stage: the respective engineers and their tools. Does the choice of tools, like for example what kind of programming language to use, influence the security of a certain technology? Most certainly yes. Available programming languages have been designed with different goals in mind, come with different strengths and weaknesses, but they haven't always been developed with security in mind C has been designed to favor performance, affordability, and ease of implementation over safety. The C family of languages has inherited those tradeoffs. Those were deliberate choices and they still make a lot of sense in many circumstances today. Our OSs run fast thanks to them. WWDC2017, Session 407 – Understanding Undefined Behavior – with Ada Spark (a language that is used in safety-critical / high-security domains) seemingly representing a rare counter-example. Current programming languages also give their respective users different degrees of freedom in how to use them. The C programming language is terrible. […] I mean 'terrible' in the 'awe-inspiring dread' sense more than the 'bad' sense. C has become a monster. It gives its users far too much artillery with which to shoot their feet off. Copious experience has taught us all, the hard way, that it is very difficult, verging on 'basically impossible,' to write extensive amounts of C code that is not riddled with security holes. Evans 2017, TechCrunch The problem here seems to be that using a language like C requires the user to know a lot about it, about undocumented assumptions and about possible side effects and to always keep them in mind. A lot of our current development languages do not help much (in the way of) enforcing good security practices or avoiding known security issues. So why do developers keep making the same mistakes? Some errors are caused by legacy code, others by programmers’ carelessness or lack of awareness about security concerns. However, the root problem is that while security vulnerabilities , such as buffer overflows, are well understood, the techniques for avoiding them are not codified into the development process. Even conscientious programmers can overlook security issues, especially those that rely on undocumented assumptions about procedures and data types. Evans and Larochelle 2002, IEEE SOFTWARE , p.42f While it is impossible, and also undesirable, to codify all knowledge a programmer has to have about a language into the language itself, do we maybe need safer programming languages that protect their users from making choices that could have unintended consequences and lead to insecure products? The designers of the Swift language seemed to think so. Now, safety is a design choice in SWF [Swift]. […] While you can write code fine-tuned for performance in SWF, this language makes different tradeoffs and was designed to be much safer by default. WWDC2017, Session 407 – Understanding Undefined Behavior How is Swift safer by design? Let me give you three examples: overflows, switches and optionals. Overflows Using overflows for security exploits is a very popular technique among hackers. Buffer overflows happen when a programme tries to put unexpected, malformed data into a buffer in memory that was initially created for a smaller amount of data. Writing past the end of the allocated buffer can overwrite adjacent data, which can in turn lead to crashes or corrupt data, or – when writing into areas that contain executable code – to the execution of malicious code. The Heartbleed bug comes to mind (even though it technically isn't a ‘classic’ buffer overflow exploit, where data in memory is overwritten with the intent to execute that code; it rather is about reading more data from memory than originally intended); more examples can for example be found in the vuldb.com or the Common Vulnerabilities and Exposures database. By default Swift is designed to be memory safe . Swift can bridge Objective-C and many C types, though, and provides the possibility to call C and Objective C functions. The C language is especially prone to overflows. So, applications written in Swift can potentially suffer from the same vulnerabilities. Buffer overflows, both on the stack and on the heap, are a major source of security vulnerabilities in C, Objective-C, and C++ code. [...] Because many programs link to C libraries, vulnerabilities in standard libraries can cause vulnerabilities even in programs written in  'safe' languages. Apple Secure Coding Guide Swift works hard to make interaction with C pointers convenient, because of their pervasiveness within Cocoa, while providing some level of safety. However, interaction with C pointers is inherently unsafe compared to your other Swift code, so care must be taken. Apple Swift Blog, Interacting with C Pointers When using 'pure' Swift, over- and underflows cause a runtime trap. This means that your application will rather crash than continue running in an unexpected state. Swift is designed to prioritise safety and minimise these errors. Swift’s default behavior of trapping on overflow calculations may come as a surprise to you if you have programmed in another language. [...] The philosophy of the Swift language is that it is better to trap (even though this may result in a program crashing) than potentially have a security hole. Swift Programming: The Big Nerd Ranch Guide , p.28 This is an example of an integer overflow in Swift: As y is an Int8 , 10 is also assumed to be an Int8 ; but as Int8 can only hold values from -128 to 127 , the expected result ( 130 ) does not fit in there, which crashes the programme. A lot of other languages than Swift would not trap here. They would instead wrap-around so that the (unexpected) result of above's calculation would have been -126 . A malicious user could exploit code with wrap-around behaviour by entering for example a negative number when a positive one is required. If the applications expects an unsigned number but gets a negative number, that number might be interpreted as a very larger number (because of the before mentioned over-/underflow wrapping behaviour). Allocating buffer for the unexpectedly large number could then potentially lead to an overflow. Wrap-around behavior could also be achieved in Swift by using overflow operators; these operators need to be intentionally applied, though and aren't the default. Switches Switch-case-statements are one possible tool (among others) to control code execution by structuring it. Programmers can specify different blocks of code that will be executed under certain conditions. Whenever a certain value is found to match that of  a case , the block of code in that case statement will be executed. Usually a fallthrough from one case to another is the default, even though a break is most often the desired option. Forgetting to put a break at the end of a case can therefore be another common source of security issues. Not being a 'real' switch-case-bug example but also touching on the overall concept of flow control, I want to mention the GoTo-Fail here. A lot of the blame in this incident can be attributed to wrong indentation, and probably also missing brackets, in the if-statements. Let me show you a scary but totally possible example of a switch-case in Objective-C: switch (foo) {\n    case 0:\n        NSLog(@\"stuff\");\n    case 1:\n        NSLog(@\"more stuff\");\n        break;\n    case 2: {\n        NSLog(@\"different stuff\");\n        break;\n    }\n    case 3: {\n        NSLog(@\"other stuff\");\n    } break; {\n    case 4:\n        NSLog(@\"stuffy stuff\");\n    } break;\n    case 6: NSLog(@\"stuff stuff\");\n    case 8: NSLog(@\"and stuff\"); break;\n    default:\n        break;\n} If-statements, switches, and other control flow tools should be as clear as possible, so that the flow of code can immediately be discerned. They should also be easily implemented, e.g. it should be really hard to forget a bracket or a break at the correct place. Swift is doing that with switch-case-statements. You don't have to explicitly put a break at the end of a case anymore because Swift made breaking the default. switch statusCode {\n    case 400:\n        errorString = “Text for error 400”\n    case 401:\n        errorString = “Text for error 401”\n    case 403:\n        errorString = “Text for error 403”\n    case 404:\n        errorString = “Text for error 404”\n    default:\n        errorString = “Default error text”\n} [Languages like C or Objective-C] require a break control transfer statement at the end of the case’s code to break out of the switch. Swift’s switch works in the opposite manner. If you match on a case, then the case executes its code and the switch stops running. Swift Programming: The Big Nerd Ranch Guide , p.38 Fallthrough s are still possible in Swift, but they have to be explicitly added, which makes it less likely that they happen by accident. switch statusCode {\n    case 400, 401, 403, 404:\n        errorString = “There was something wrong with the request.”\n        fallthrough\n    default:\n        errorString += “ Please review the request and try again.”\n} Null pointer Accessing a null pointer is another really common source of software bugs that can be exploited (see for example CVE-2016-6604 or VULDB-ID-96306 ). A null-pointer-dereference occurs when a pointer with a value of NULL is used as though it pointed to a valid memory area. Tony Hoare, a British computer scientist and the inventor of null , apologised a couple of years ago at a conference for having introduced null references back in the 60s: I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years. Hoare 2009, InfoQueue conference When using C, trying to access whatever such a null pointer points to leads to (usually unwanted) undefined behavior. (Objective-C works a little bit differently. It largely avoids null pointer exceptions by simply ignoring them. As it is built on the C language foundation, though, it also knows some cases where null pointers cannot be ignored,  like for example adding them to an Array or a Dictionary) So, to be able to correctly deal with null , programmers have to insert null checks into their code. The problem is, that this potentially has to be done with every reference, as there is always the possibility that it might be null . (Again, Objective-C functions a little bit differently; it lets you add nonnull annotations. This was added to the language retrospectively, though; see Xcode 6.3 release notes .) In Swift such a null check would look something like this: if message != nil {\n\tprint(message)\n} When using Swift, programmers do not have to do such checks all the time because Swift uses Optional types to deal with the concept of nothingness. The Optional type indicates whether a constant or variable can at any point equal nil , or not. It can be used equally (and consistently) with any variable type. If a variable is not an optional it can be assumed to never be nil. Non-optionals can always be used safely. Optional variables, the ones that can be nil, are marked with a ? to show that they are of a different type. This immediately indicates if a nil check is necessary, or not. To use an optional it has to be unwrapped. Unwrapping can either be done safely by using if let or guard let ... else , or unsafely, by adding a ! to the variable; this is also called force or implicit unwrapping. When you force-unwrap an optional, you expect to get a value, and the rest of your code is written assuming there is a value to work with. If the optional is nil, there is no value. The only reasonable thing Swift can do is immediately stop the program. If your program did continue, either it would crash when it tried to access a nonexistent value or, worse, it could continue to run but produce incorrect results. (Both of these possibilities come up in less-safe languages like C.) Swift Programming: The Big Nerd Ranch Guide , p.238 Safe unwrapping example: func displayErrorMessage(_ optionalMessage: String?) {\n\tif let message = optionalMessage {\n\t\tprint(message) \n\t}\n} Calling displayErrorMessage() with a String as parameter prints out that String; calling it with a nil value does not do anything. Conclusion The examples discussed illustrate how Swift tries to be a safer language than previous ones by 'tricking' developers into using the more secure option, making it the default one. The possibility to circumvent the safeties, which Swift imposes on its developers, exists in all three examples, though. Swift gives its developers ways to override the safeties if they want to, but forces them to make a conscious decision about that. This can, in the long run, help to make software more secure. Of course, we cannot catch all (and especially new) bugs with that approach, but by using safer languages we can at least develop software that is less prone to bugs that are well known, like the ones discussed above. And this does not only apply to new developers who are still learning; even experienced who know about potential problems and pitfalls can't always catch all the errors, which could cause security issues later on. Developing safer languages, the community should however keep in mind that the rules, which are baked into the language, should be there to help developers do their job. If languages are getting too strict, programmers might tend to override the safeties and make everything worse. Making languages secure therefore not only involves technical aspects - actual usability of the language influences security of software just as much.", "date": "2017-11-07"},
{"website": "Novoda", "title": "Retain returning users with Android’s app backup (part 1)", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/android-backup-and-restore-returning-users-part-1/", "abstract": "Retaining users when they migrate to a new Android device, or when they reinstall your app, can be tricky. If you don't craft a smooth experience, they might just drop your app and go to a competitor. Fortunately, Android Data Backup APIs can help! It’s autumn, it’s new phones season, and I guess quite a few amongst you bought, or received, a new Android device in the last year or so. It certainly was so for me — I have sent my Nexus 6P in for warranty and was lucky enough that Google sent me back a new Pixel XL. As I set myself to the generally painful task of migrating to my new device before sending the defective 6P back, I noticed that in the intervening year since I’ve last done this the process has become way smoother. Once connected the two devices with a cable, almost all the apps and accounts I had were migrated to the new device effortlessly. Had I not had a working 6P at all, a cloud restore would’ve provided the same convenience (albeit at a much slower pace, I suspect). The nicest part? Most of my system and apps’ settings were migrated automatically, too! Setting up my new Pixel took me less than an hour, instead of several hours. It had been quite some time that an Android device made me say \"wow\"! How is this all possible, you might be wondering? It’s all down to more apps adopting one of the Android backup mechanisms . Google, for one, are doubling down their efforts to correctly backup their apps’ data — although they have a rather spotty record as of yet. The launcher can carry over the homescreens organisation, Drive and siblings will skip their onboarding, and so on. But not all of the Google apps still work well. I’m looking at you, Play Music and Google Maps ಠ_ಠ Why are you not automatically re-downloading my offline music and maps? I find those examples a bit sad, especially since most other Google apps, and many third-party apps, will successfully restore their settings on a reinstall, through the app backup, account transfer and data sync capabilities that Android offers — more on that later. To be clear, I don’t mean to suggest that Maps and Play Music should store their cached data in the backup — there is not enough room for that, obviously. What they could to though is to backup the list of content they had cached, so that when they’re restored they can re-download it all automatically, or at least prompt the user if they want to re-download all those things. Of course Play Music and Maps aren’t alone in the \"too good to backup their settings\" camp. It’s very annoying as an user to open an app after they’ve migrated to a new device or reinstalled it, just to find out all their configurations have been lost. It makes even less sense when you consider that, if you keep backup and restore in mind when developing it’s extremely trivial to implement it. If you were not as tidy with your internal storage you might have to refactor things a bit, but in my opinion it’s still worth it. Cool, but why should I care? Even if you were not interested in making the best app ever, you probably still care about user retention. Here’s where backup and restore come to your help. When users buy new devices (every 1-3 years generally in Western countries) and you don’t allow your app to be backed up, it will not automagically show up on their new device. If your backup settings are wrong, they’ll have to sit through an onboarding they don’t need, and then will have to re-set all the configuration options your app has lost along the way. Maybe they’ll even have to log in to apps all over again. That’s super frustrating and can take the joy out of getting a new device even for the geekiest amongst us — and I count myself in there! Users hate doing this, and they’ll be way more likely to uninstall your app, or switch to a competitor. Properly supporting backup and restore in your app takes very little time and has some solid pros and basically no cons. Even your manager and marketing should be able to immediately see a return on investment. What makes the UX tick… …is, unsurprisingly, clever usage of cloud-based backup services. In addition to the usual data syncing facilities such as SyncAdapter s, web APIs, or Firebase RTDB/Firestore, Android has some rather powerful APIs for backing up and restoring apps’ private data in a secure and effortless way: Key/Value Backup Auto Backup They aim to provide the same end result — happy users that keep using your apps on their new devices — but they have substantial differences in their scope and how they work. Key/Value Backup Not everyone knows it, but Android has been able to backup and restore app settings since the very early days of Froyo. This is because starting with Android 2.2 (API 8) Google introduced the Key/Value Backup API; as the name suggests, this works by storing a set of Key/Value pairs into a backup transport that is then queried when reinstalling an app. The backup transport on all devices with Play Services that I am aware of is provided by Google’s Android Backup Service. I am not familiar enough with the GMS licensing terms to know whether this is a mandatory requirement or if OEMs can swap it out and use another provider, but I haven’t seen any doing it. The documentation seems to imply it is not a requirement, though, so I suspect OEMs simply cannot be bothered rolling out their own thing (luckily for us users!). As far as I know non-GMS devices, such as Amazon Kindle Fires, do not provide any backup transport at all — possibly because they don’t care about these flows that much, or maybe because it’s a pure infrastructure cost that they cannot justify. As developers and users though you don’t have to care much about this. All you need to know is that, if a backup transport is available, all your data will be backed up securely and restored as needed. If no transport is available, or if a different transport than Android Backup Service is available, then the system will do the right thing by either not backing up your data, or send it wherever it is supposed to go. Implementing Key/Value Backup This mechanism is opt-in; developers need to specify the android:allowBackup=\"true\" attribute on their apps’ manifests, and implement a BackupAgent . The backup agent must then be declared in the manifest using the android:backupAgent attribute. If you want to take advantage of the Android Backup Service on Play Services devices, you’ll need to obtain an API key and put it in your manifest, too. If you don’t do this last crucial step, Key/Value Backup won’t work! For example: <application android:label=\"MyApplication\"\n             android:allowBackup=\"true\"\n             android:backupAgent=\"MyBackupAgent\">\n    ...\n    <meta-data android:name=\"com.google.android.backup.api_key\"\n        android:value=\"AEdPqrEAAAAIDaYEVgU6DJnyJdBmU7KLH3kszDXLv_4DIsEIyQ\" />\n</application> With the Android Backup Service Key/Value Backup, you can store up to 5 MB of data in Google’s cloud. This is normally more than enough to back up your shared preferences contents. The BackupAgent is a generalised backup API that can be used to provide a custom backup logic, including migrations from older versions of your backup data (e.g., a user’s last backup was on a previous version of the app that had a different schema for the data). While most apps will just want to back up their SharedPreferences , some might want to back up other configuration data that is stored elsewhere, such as in plain text files or databases. At this point I need to remind y’all that with great power comes great responsibility: don’t use the backup services to backup non-configuration data! If all you want to do is backup entire files, you can extend BackupAgentHelper , which provides a greatly simplified version of the API. In there, you want to add one or both of its two subclasses, FileBackupHelper and SharedPreferencesBackupHelper . If you want to backup entire files, though, Key/Value is probably not the best choice for you. There is also a little known library that the Backup and Restore team at Google have open sourced recently in AOSP, libbackup . While I have not used it yet, it’s supposed to let you backup only some keys that are matching a given regex, and to make testing easier. Again, this I have learnt from the Backup and Restore team, but I have not had a chance to try it out and assess its capabilities. It’s unfortunately not documented either, which makes it complicated to peek into without using it. Hopefully this will improve in the future, and maybe we’ll see it getting on GitHub and JCenter (yes, I very well remember the fate of Volley). For more information on Key/Value Backup, please refer to the documentation and to the series my fellow GDE and friend Mark Allison wrote back in 2012 on his blog . In the second and last part of this series, we’ll see how we can get apps’ data backed up with even less effort than with Key/Value Backup!", "date": "2017-11-16"},
{"website": "Novoda", "title": "Ideas for conferences to attend: What is out there?", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/ideas-for-conferences-to-attend-what-is-out-there/", "abstract": "To predict forwards you have to look at the past. In 2017 Novoda attended over 50 conferences. What were they? Check out our list to see if you want to go next year. Data analysis underpins a lot of what we do at Novoda. Diving into our conference and training budgets has helped us understand more about learning and developing at Novoda. We’ve been to a total of 55 events in 2017. Being able to look back at the data for the year helps us plan for next year, then maybe with a few more years under our belt, we will start to see trends developing in conference attendance. This list is an aggregation of conferences and training that have gone on across Novoda. That's people from engineering, product, agility, operations and marketing. Everyone is encouraged to use their training budgets to learn in their field of interest, but also further afield in new technologies or areas they are less comfortable. AA Days ADDC Barcelona Agile Cambridge Android Makers AWS Summit Canvas Certified Scrum Product Owner Training CHI 2017 CodeMobile Design conference MUXL Developer Days Krakow Devfest London 2017 DevFest Berlin DevFest Budapest DevFest Veneto DevTernity conference in Riga Droidcon Berlin Droidcon London Droidcon Turin Firebase Dev Summit 2017 in Amsterdam FrenchKit Google Developer Expert Summit Google Actions Workshop in Hamburg Google Agency Day Google Cloud Next, London Google Home Workshop Google I/O Google Playtime Berlin GOTO Berlin 2017 HalfStack Conference Heise devSec Infiniteconf 2017 - the conference on Big Data and Fast Data Interact Conference iOSDevUK 2017 Kotlin Conf Lead Dev London Mobile Forum Minds Mastering Machines - The ML & AI - Intelligence conference Mobile UX London- Designing for Alexa & IoT MTPcon MobiConf Novoda Assembly Prolific North Live PyData Quality Jam Reimagine Work SAFE Training LDN Socrates DE Socrates UK Software Craftmanship London Swift Aveiro Try Swift NYC conference Udemy Online Courses UIKonf Women Who Code Workshop XConf 2017 Manchester Attending events is a great way to solidify skills you already have but also to see areas of interest that you might not have realised exist in your field. It's also a great way to meet other people who have similar interests or can share their knowledge whilst you give yours. Using a data-driven approach to decisions helps to make choices easier (or at least quantifiable). We hope to gather more data on our conference attendance, and we hope this list might encourage you to look further afield for conference or training events to attend.", "date": "2017-12-05"},
{"website": "Novoda", "title": "Writing a dialogflow bot", "author": ["Florian Mierzejewski"], "link": "https://blog.novoda.com/writing-a-dialogflow-bot/", "abstract": "The idea was to try to build a chat bot and see how far we can get in a week and a half. Let's see what we did, how, where we got blocked and what are the lessons we learned along the way. Spoiler alert: we went pretty far. Some people at Novoda already experimented a bit and created some chatbots on their own, slack bots, Alexa skills and other Google actions. All of them were pretty simple and only a handful were actually dynamic (i.e they didn't require any backend work). For our little experiment we decided to build a small bot that is able to give us information about a tv show and when it airs on tv. After asking around it seemed that the perfect choice was to use api.ai (now dialogflow ) to create this bot as we could just start to play with the interface to create a static bot and later move to a proper backend to have something a bit more dynamic. The real plus is also to be able to export this bot to multiple platforms (here we exported to google assistant and slack). First step is to create your project, by following this that should be pretty straightforward. You can then play a bit with the static responses and see how the basics work. When you are tired to have the same static response, we can start to dig deeper. I highly recommend to read all the documentation concerning the key concepts to understand better what is possible and what's not, especially the bits regarding the context. An easy way to start hacking around is to expose your localhost to the internet using ngrok . Here is a simple piece of code written using Node.js, it starts a server on the port 5000 and return a well formatted response according to the action, parameters and contexts thanks to our magical resolve method. import express from 'express'\nimport bodyParser from 'body-parser'\n\nconst app = express()\napp.use(bodyParser.json())\napp.use(bodyParser.urlencoded({ extended: true }))\n\nconst server = app.listen(5000, () => {\n  console.log(`Server listening on port ${server.address().port}`)\n})\n\napp.post('/webhook', async (request, response) => {\n  const requestBody = request.body.result\n  console.log(requestBody)\n  const result = await resolve(requestBody)\n  return response.json(result)\n}) We can then expose our localhost using ngrok: $ ngrok http 5000 Under fulfillment we can then use the generated URL that points to our localhost and start playing! This is how our intent looks like (don't forget to check Use Webhook under fulfillment in your intent as well) Don't worry about the context for now and @tv-show is just a simple entity containing a few tv shows we want to be able to pick up. After you retrieve your dynamic information, the response need to be as simple as: return {\n  speech: 'spoken response (google home for instance)',\n  displayText: 'written response (google assistant or slack for instance)'\n} Testing our new intent on the console this is what we get This is already good but we can do better! Depending on the platform, when sending our response, we can include an extra data object that contains specifics for each of these platforms. In our case, we want to support better slack and google so we include them both: data: {\n    slack: {},\n    google: {}\n} For slack it's pretty easy thanks to the extensive documentation slack: {\n    {\n      attachments: [\n        {\n          title: show.brand.title,\n          text: show.brand.summary,\n          image_url: show.brand.image.href\n        }\n      ]\n    }\n} For google it's a bit harder as the documentation is pretty sparse, mixing both the actions sdk and the api.ai one, some dead links here and there and some not really clear errors, this one has to be my favorite: {\n    \"response\": \"Televison guide isn't responding right now. Try again soon.\\n\",\n    \"audioResponse\": \"//NExAASW...\",\n    \"debugInfo\": {\n        \"sharedDebugInfo\": [\n            {\n                \"name\": \"ExecutionResponse\",\n                \"debugInfo\": \"Failed to...\"\n            }\n        ]\n    },\n    \"visualResponse\": {}\n} Anyway, after some trial and errors, here the structure for the google card: {\n      expectUserResponse: true,\n      richResponse: {\n        items: [\n          {\n            simpleResponse: {\n              textToSpeech: textToSpeech\n            }\n          },\n          {\n            basicCard: {\n              title: show.brand.title,\n              formattedText: show.brand.summary,\n              image: {\n                url: show.brand.image.href,\n                accessibilityText: show.title + ' poster'\n              }\n            }\n          }\n        ]\n      }\n    } How about we now want to know when this show is airing? We could just ask When is The Big Bang Theory starting? but wouldn't that be even better to just have to ask When is it starting? . In other terms, that would be pretty awesome if our bot could keep some context. When we are returning a response that is linked to a question containing a tv show, such as What is SHOW about? we add a contextOut array with a context object which is going to be send back to us in the next requests: return {\n  speech: message,\n  displayText: message,\n  contextOut: [this.contextForShow(show)],\n  data\n} contextForShow(show) {\n  return {\n    name: 'show',\n    lifespan: 5,\n    parameters: {\n      name: show.brand.title\n    }\n  }\n} You'll need to create two intents: one that take a context and one that doesn't but then expect to contain a tv show in the question: On the code side, you can then have the same function handling the creation of the response, trying to fetch the tv show name either from the context or from the parameters: const show = parameters['tv-show'] || this.extractParametersFromContext() This is where we stop. I hope this short glimpse into what's possible gave you a good starting point on your chatbot adventure!", "date": "2017-12-12"},
{"website": "Novoda", "title": "Cucumberish - BDD testing framework for iOS applications + sample application.", "author": ["Bart Ziemba"], "link": "https://blog.novoda.com/cucumberish-bdd-testing-framework-for-ios-applications-sample-application/", "abstract": "Recently, I had the opportunity to collaborate on a project where we decided to change the way we work and apply Behaviour Driven Development practices into our mobile teams. Project team used that methodology not only in the way we were cooperating and communicating but also to drive our tests. I was responsible for finding the solution that would help with implementation of UI automated tests for iOS application. In this post you can find out what approach and tools team decided to use. What is more, you can check out the iOS app I built with the BDD testing framework and run some automated UI tests. What is Behaviour Driven Development? There are plenty of books that explain that methodology fully but I would like to express what it means to me in a few words. Personally, I find BDD as an agile methodology that focuses on the collaboration and communication between business and development teams. Its main goal is to create a shared understanding of what is expected from the application's behaviour and also to align business requirements with the code. In order to reach that point the team needs to speak a common language. Fortunately, there are various tools on the market that help to achieve that. What is Cucumberish? Cucumberish is the test automation framework that drives execution of the tests specified in feature files and written in Gherkin language.The framework integrates well with newest versions of Xcode and the implementation of the tests can be written in both Objective-C or Swift. Why Cucumberish? There are other tools as well! While I was doing some investigation on the possible tools we could use to automate our UI tests in connection with BDD approach I came across these: Calabash -  was always my first thought when talking about BDD on iOS until I found this article . It turns out that Xamarin, creator of Calabash, has decided to not to support and maintain the framework after iOS 11. The main reason is that the market's focus is on the native frameworks such as XCUITest or Espresso for Android. Gherkin XCTest - the second tool I found was Gherkin XCTest framework. To be honest, I have not tried it out. The main reason behind that is the lack of support for tagging on feature and scenario levels. Tags allow you to organise your test suite and select which specific test scenarios you want to run. Lack of that important functionality couldn't allow me to use it. How does Cucumberish work? Tests are written in the .feature files in accordance with the best practices of the Gherkin syntax. Example Scenario: Scenario: Ability to tap on buy button\n    Given user launches the application\n    When user fills in all the text fields\n    Then user is able to tap on buy button When the steps of the scenario are ready. Each of the steps are implemented individually. Example of a step implementation: Then(\"user is able to tap on buy button\") { _,_ in\n        self.verifyBuyButtonIsHittable()\n    }\n\nfunc verifyBuyButtonIsHittable() {\n        let buyNowButton = application.buttons[\"BuyNowBtn\"]\n        XCTAssertTrue(buyNowButton.isHittable)\n    } When running the tests, Cucumberish creates an XC test suite for each feature and an XCTestCase for each scenario and maps the steps with the code implemented for individual step. How to add Cucumberish to your app project? The creator of the framework suggests a few ways of adding Cucumberish to your project: Setup Cucumberish with Cocoapods Setup Cucumberish manually From my experience, I can say that the manual steps always made my setup successful. Setup via Cocoapods doesn’t create Objective-C bridging header file that is necessary to run tests whereas manual setup provide with that information. Additionally, I would also recommend to everyone who wants to run their tests on physical devices to disable bitcode. Otherwise Xcode main complain about it and tests won’t be executed. I normally set it up in Podfile by adding this: post_install do |installer|\n    installer.pods_project.targets.each do |target|\n        target.build_configurations.each do |config|\n            config.build_settings['ENABLE_BITCODE'] = 'NO'\n        end\n    end\nend Sample app For the purpose of this blog post and Udemy's iOS course that I follow,  I have built sample MiraclePill app together with the Cucumberish framework. Feel free to check it out and see how the UI tests have been implemented. To run the tests: Run pod install in your project directory. Open xcworkspace file. Select MiraclePillUITests target. Select device or simulator from the list. Run cmd+u . 🎉 Sum up I found Cucumberish framework very friendly to work with. It is easy to debug tests that are failing. What makes it great is also the fact that tests are written in human readable language. It means that every one without technical skills will be able to understand what features of the application are covered with automated tests. I hope Cucumberish will be supported and maintained especially given that the future of the, so far, very popular Calabash framework is unknown. Happy testing!", "date": "2018-01-04"},
{"website": "Novoda", "title": "What are some of our engineers up to in 2018?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/what-are-some-of-our-engineers-up-to-in-2018/", "abstract": "With a new year comes a new start. At Novoda we are always learning but the new year is a great milestone for starting a new project. What are some of our engineers up to in 2018? 2017 was an amazing year, we spoke publicly about our Novoda Craft University course for new starters. We started exploring data science, machine learning and IoT with AndroidThings . We’ve been to a total of fifty four conferences. Also, we released a bunch of awesome apps working with Channel 4, Immoscout, CCleaner, News UK, Schindler and YourMD. Let’s see what happens in 2018. Francesco Pontillo Google Assistant has received several new features during 2017, and it can now enable companies to do so many new things that weren’t even conceivable in the past. From easy interactions to complex conversation flows, we can now create great voice and text assistant-based products to enrich users experience. I’ll take on the challenge to experiment with Dialogflow and Google Assistant in order to cover every edge case, provide help to other developers and run code labs at as many events as possible. I will also release some tutorials and libraries for Assistant to help fellow developers onboard the paradigm and API in a better and more productive way. Dialog Flow - Getting Started Paul Blundell I’ve been very guarded about Kotlin in 2017, wondering about the maturity and the adoption rate, but judging by the amount of online material that is appearing in blogs etc, even if I do think Java 8 is the way forward, 2018 is where I am going to concentrate on learning Kotlin paradigms. I’ll do this through making sure any side projects I create will be 100% Kotlin. My blog posts I write this year will have Kotlin examples. For online resources I will start with the Kotlin koans and progress from there. Kotlin Koans Bart Ziemba In 2017 I was mostly focusing on applying Behaviour Driven Development into project that I'm currently on. I believe it's a great approach to align business requirements with what the development team delivers. It's also a great way of working to gain a common understanding of the product within cross-platform teams. I will try to do this by helping my team to understand the main concepts of the BDD and apply them into our day-to-day work. What's more, to make sure that what we deliver to users is working correctly, I will focus on applying automated tests driven by Cucumberish framework for our iOS application. Cucumberish test automation framework for BDD Berta Devant Machine Learning, AI and Augmented Reality were the main focuses of WWDC 2017 for Apple, launching new tools that make it super straightforward and easy to use for iOS developers in a native way using Swift. For 2018 I want to continue taking on the challenge of understanding how these frameworks work and how we can add them seamlessly to any project. > Core ML: Machine Learning for IOS Build your first ARKit Demo Andrei Catinean 2017 was an interesting year for Firebase, launching a lot of new products which integrate seamlessly into the whole ecosystem. With the release of Cloud Functions for Firebase, Firestore database, Analytics predictions and easy A/B testing, I see Firebase as being a great toolset to have in your skillsbox as an engineer. In 2018 I will continue improving my Cloud Functions knowledge by experimenting with different serverless setups. Moreover, I would like to research and slowly adopt in some of our projects Data Driven Development by making use of the A/B testing framework and Analytics Predictions feature that Firebase provides. Cloud Functions for Firebase Firebase Predictions Niamh Power 2017 at Novoda has given me the opportunity to explore many new technologies. One of the most interesting to me has been Flutter, a new and refreshing approach to cross platform development using Dart, which I have been learning as part of the Novoda Craftsmanship University. In 2018, I am looking forward to exploring Flutter and Dart further, and investigating its viability for large scale projects. Its ease to pick up, and speed compared to other cross platform solutions has piqued my interest and I am certainly curious to see if it gains traction in the developer community in the coming months. Google codelab on Flutter What are you going to learn in 2018?", "date": "2018-01-03"},
{"website": "Novoda", "title": "Testing Views in Isolation with Espresso", "author": ["Ataul Munim"], "link": "https://blog.novoda.com/testing-views-in-isolation-with-espresso/", "abstract": "In this post, we'll show you how and why you should use Espresso to test your custom Views on an Android device. Note: this blogpost refers to an old version of the espresso-support library. The latest version can be found at its GitHub repo . You can use Espresso to test entire screens or flows at once. These tests launch an Activity and perform actions as a user would, including waiting for data to load or navigating to other screens. They're useful because you need end-to-end tests to validate common user flows. These automated tests should run at regular intervals, enabling you to spend manual QA time performing exploratory tests. That said, these aren’t tests that you can run frequently. Running the whole suite can take hours (imagine a suite that includes verifying offline sync for media content), so you might choose to run them at night. It's difficult because these types of tests incorporate multiple points of potential failure. Ideally when a single test fails, you want it to fail because of a single logical assertion. Most (or a lot) of the regressions you can introduce are in the UI. They might be subtle so we don't notice them when adding new features, but eagle-eyed QA teams often do. It's such a waste of time. What can you do? Let's look at using Espresso to test that you have correctly bound data to Views. At Novoda, the Views we write are mostly extensions of existing View and ViewGroup classes on Android. They typically only expose one or two extra methods, which are used to bind callbacks and the data object/view model, like this: public class MovieItemView extends RelativeLayout {\n  private TextView titleTextView;\n  private Callback callback;\n\n  public void attach(Callback callback) {\n    this.callback = callback;\n  }\n  \n  public void bind(Movie movie) {\n    titleTextView.setText(movie.name());\n    setOnClickListener(new OnClickListener() {\n      @Override \n      public void onClick(View v) {\n        callback.onClick(movie);\n      }\n    });\n  }\n} They group logical parts of the UI together and often encompass naming conventions from the business domain. You will rarely see ‘raw’ Android Views in Activity layouts in our work at Novoda. Let’s write these View tests in a BDD style, like \"given MovieItemView is bound to Edward Scissorhands, then title is set as Edward Scissorhands\" or \"given MovieItemView is bound to Edward Scissorhands, when clicking on view, then onClick(Edward Scissorhands) is called\", etc. Couldn't you have caught these regressions with unit tests? Why do you need Espresso to run these tests if you’re using a presentation pattern like MVP or MVVM, which can be unit-tested? First, let’s go over the presentation flow and describe what you can test to see how the Espresso tests can augment it. Presenters subscribe to data producers that send events Events can be of type loading , idle or error , and may or may not contain data to display Presenters will forward these events to \"displayers\" (“View” in MVP) using methods like display(List<Movie>) , displayCachedDataWhileLoading(List<Movie>) or displayEmptyScreen() , etc. The implementation of displayers will show/hide Android Views and do things like moviesView.bind(List<Movie>) You can unit-test the presenters completely, verifying that the correct methods on the displayers are called with the correct arguments. Can you test the displayers in the same way? Yes, you can mock the Android Views and verify you're calling the correct methods on them. It won't be at the correct granularity though: your displayer could create or update the adapter of a RecyclerView or ViewPager, but this gives you no assurances of what the item/page is displaying Android Views are setup in code with attributes inflated from XML (layouts and styles); verifying method calls isn't enough to assert what's displayed Setting up for the tests Let’s use the espresso-support library to get started. Add the dependencies to your build.gradle file (available from JCenter): debugCompile 'com.novoda:espresso-support-extras:0.0.3'\nandroidTestCompile 'com.novoda:espresso-support:0.0.3' The extras artifact includes the ViewActivity , which needs to be part of your app under test. You can use this Activity to hold a single View, which you can test with Espresso. The core artifact (containing custom test rules) only needs to be included as part of your androidTest dependencies. The ViewTestRule is used in a similar way to the ActivityTestRule . Instead of passing the Activity class that you want to launch, you should pass the layout file containing the View you want to test: @RunWith(AndroidJUnit4.class)\npublic class MovieItemViewTest {\n  @Rule\n  public ViewTestRule<MovieItemView> viewTestRule = new ViewTestRule<>(R.layout.test_movie_item_view);\n  ... You can specify the View type for the root of the layout with ViewTestRule<MovieItemView> . The ViewTestRule extends ActivityTestRule<ViewActivity> , so it'll always open ViewActivity . getActivityIntent() is overridden so you can pass R.layout.test_movie_item_view to ViewActivity as an Intent extra. You can use Mockito in your tests to substitute for the callbacks: @Rule\npublic MockitoRule mockitoRule = MockitoJUnit.rule();\n\n@Mock\nMovieItemView.Listener movieItemListener;\n\n@Before\npublic void setUp() {\n  MovieItemView view = viewTestRule.getView();\n  view.attachListener(movieItemListener);\n  ...\n} ViewTestRule has a method bindViewUsing(Binder) , which gives you a reference to the View so you can interact with it. While you could access the View directly with viewTestRule.getView() , you want to ensure any interaction with the View is performed on the main thread, not the test thread. @Before\npublic void setUp() {\n  MovieItemView view = viewTestRule.getView();\n  view.attachListener(movieItemListener);\n  viewTestRule.bindViewUsing(new ViewTestRule.Binder<MovieItemView>() {\n    @Override\n    public void bind(MovieItemView view) {\n      view.bind(EDWARD_SCISSORHANDS);\n    }\n  });\n} Ready to test Apps only really do two things, from the user's point of view: they display information they respond to user actions To write tests for these two cases, you can start by asserting that the correct information is displayed using standard Espresso ViewMatchers and ViewAssertions: @Test\npublic void titleSetToMovieName() {\n  onView(withId(R.id.movie_item_text_name))\n      .check(matches(withText(EDWARD_SCISSORHANDS.name)));\n} Next, you should make sure that the user actions correspond to the correct event being fired, with the correct arguments: @Test\npublic void clickMovieItemView() {\n  onView(withClassName(is(MovieItemView.class.getName())))\n      .perform(click());\n\n  verify(movieItemListener)\n      .onClick(eq(EDWARD_SCISSORHANDS));\n} That's it, I hope you find this useful. In a future post, I'll cover using Espresso to test your Views for TalkBack support.", "date": "2017-04-04"},
{"website": "Novoda", "title": "If cross platform is the answer, make sure you’re asking the right question", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/if-cross-platform-is-the-answer-make-sure-youre-asking-the-right-question/", "abstract": "Native mobile development is dead. Long live React Native, Xamarin and the new kid on the block, Flutter. Why write twice, when you can write once, run anywhere — or at least, \" Learn once, write anywhere \"? Cross-platform solutions seem like an ideal solution to a number of issues, but are they really? Here at Novoda we work with a range of partners, big and small, from multinational corporations to disruptive startups. In recent years we've seen a trend where larger organisations are considering a move away from native development to cross-platform solutions like React Native or Xamarin. However, often we find that the reasons behind these technical solutions are not of a technical nature. Your product may have issues… In our experience when a cross-platform framework is considered for adoption it is because of a set of issues that the team — or, more often, management — wishes to address. In no particular order, we have: \"We're reinventing the wheel and wasting money\" Often management perceives there is an inefficiency in having several teams implement the same features over and over again on all the supported platforms, and the promise of \"write once run anywhere\" is a very intriguing promise that is hard to ignore. This might manifest itself in reasonings such as: In our corporation we ship several different apps under several different brands. Each of these brands might have an app on iOS and Android, as well as a web presence. But we keep solving the same problem, in marginally different ways, on each platform. This means we end up with several different implementations of each feature. For example, we have twenty different takes on the login screen, all of them talking to the same backend. Maybe if our mobile teams used a cross-platform framework they wouldn't need to reinvent the wheel. We can simply implement things once and start reusing each implementation  across all apps. In the end this often comes down to cold, hard cash too. It's no secret that staffing several specialist teams is expensive and often recruitment-intensive, as experienced, qualified mobile developers can be hard to find. The idea goes that using a single language and platform allows to have a smaller team, since every feature need only be implemented once and all redundant developers can either be used to increase velocity, or let go. \"We need more velocity to deliver on time\" Tying in to the previous issue we have the common \"we're moving too slow\". The idea is that cross-platform frameworks by default increase the throughput of a team since you can share code, and almost every single keystroke a developer hits will count towards getting a given feature across the finish line for all platforms. We have very tight deadlines that are dictated by marketing and depend on five other teams. We're outputting at a constant high velocity on all our teams, but they're split across all platforms. If we keep this velocity it will still not be sufficient to deliver on time. Maybe if we used a cross-platform solution like React Native, our frontend web and mobile teams could become one team, and ship features faster on all platforms. The reasoning in this case is that velocity is an extensive property, which means its magnitude is additive for subteams. Hence if three teams each have a given velocity, if they're fused in a single team their velocity will be near the sum of the three velocities. Cross-platform frameworks seem a natural for this kind of \"agile hack\". \"Our teams are moving at different paces\" Sometimes the symptom that managers want to address is that teams move at different paces: How comes that our desktop team implemented this in a day, iOS takes a week and Android two weeks and a half? It's exactly the same feature on all platforms, it even looks the same. I'd imagine it should take the same time for them all. Maybe if we used Xamarin, we could have everyone doing their implementation with the same basic APIs, possibly sharing their code too. It would seem logical at a first glance that implementing the same feature should take approximately the same time for any two teams of a comparable size, but that is often not the case. Projects that have interlocked releases across-platform cannot help but adapt to the slowest team, and working around the problem by making the code and implementation the same for all teams seems to be a good idea. ...but are you addressing the root cause? As you put each of those issues under closer scrutiny, you'll probably feel a tingling at the back at your mind that tells you those are just symptoms of bigger underlying problems. Whenever a team output is seen as too slow — often just a gut feeling, but sometimes backed by data — management will start looking at solutions that should increase velocity. The actual problem is often laying a bit deeper underground, and might not be seen or understood by those calling the shots in this sort of drastic changes. A senior manager might not be aware, for whatever reasons, that the teams are moving slowly because of process or staffing issues, so they assume that it is a technical problem. In our experience though, technical issues are rarely the root cause of such inefficiencies. What we see more often is that the issue lies somewhere else. It's extremely common in such cases to see \"human\" causes triggering the cascade that is behind the symptoms. Sometimes it's plain simple human nature: someone is too proud, or too scared, to admit they're wrong and something they (or someone else) do is hurting the team. Other times it's lack of coordination amongst interdependent teams, or maybe broken communication. Or maybe the people working in one or more teams is exhausted after burning out because some arbitrary or tight deadline forced them into a death march and crunch mode. Team leads, middle management and developers aren't infallible. That is why, for example, we run retrospectives in all our projects and insist on everyone in teams to take part in them; the first step in fixing problems is to talk about them. The sooner the better, too: a post-mortem after things crashed and burned can provide useful insight, but we should do everything we can to correct course as soon as possible whenever we feel something is not working. But sometimes, when a job is at risk, or when the surrounding organisation has a culture of finger-pointing, the problems are swept under the rug and (un)willingly misrepresented. This means technical issues get blamed for what amounts to very non-technical causes. Unsurprisingly, this leads to unhappy teams having to adopt technologies they don't necessarily like, in addition to the initial problems. Cross-platform tools are often misunderstood in their scope and capabilities, and this creates a distorted perception that they can help in these situations. Unfortunately some (want to) believe that a culture and a process problem can be solved by a tool, and that it's the current business and language differences that are holding back cross-team collaboration. However, if multiple teams across a corporation who are writing Swift for iOS, or Java for Android, are failing to share code — and, more importantly, share domain knowledge — it's unlikely that introducing a cross-platform framework or any other technology change will fix it. Improve your structure Instead, the problem may lay in several other places. The company environment might discourage effectiveness, because of a high degree of bureaucracy or because of indirect, delayed communication. The broken window theory formulated by criminology translates very well to development teams and working environments too. If a team fails to take care of the small things, then eventually it will neglect the big things too. If your TODOs are never addressed, if the code style is incoherent, if issues disappear in a sea of other tickets when they're added to your tracker, if corners are cut on a regular basis to \"save time\" and tech debt is continuously accrued but never addressed, your teams will struggle to be passionate, be productive and be efficient. They'll produce bad code, bad designs, bad processes which will compound each other and make matters worse, in a self-reinforcing cycle in which team members care less and less about what and how they work on. The solutions to these problems are not as easy as just adopting a new tech stack, and may involve going through a painful process before things become better. Take inspiration from how other companies addressed their issues. Team and productivity pains are extremely common and every single organisation hits them at some point. There is no shame in having growing pains, as long as we admit there are pains and try to address them. One of the most famous reorganisation strategies is Spotify's agile restructuring in tribes, squads, chapters, and guilds. Squads are vertical teams and chapters are horizontal specialisations inside of a tribe, while a guild is a cross-tribe organism that leads the way with regards to a specific topic (e.g., security). We adopted the guilds system in Novoda when facing our own growth process pains, and it's working very well. You can learn more about it on Spotify's blog . What worked for Spotify and Novoda might not work for your company, though. It's very important that you choose a structure and processes that fit your environment, rather than trying to force an alien structure on it. Coordinate teams and their throughputs One of the symptoms we mentioned is the different paces at which development teams move on different platforms. While this is perceived as a problem, it is somewhat intrinsically linked to how different teams and platforms work. The most common reason why teams work at different paces is that they have to deal with different codebases, architectures, and tech debt levels. A common case is that the iOS app has a well-maintained codebase and is leading the feature set, whereas the Android codebase has been hastily thrown together and has never seen a coherent architecture — or the other way around. The latter team is likely dealing with massive amounts of tech debt that slows them down, tight coupling and low testability of the code which makes any change complicated and risky, and a lack of architectural clarity. Making any changes to it is going to be a monumental task, and the velocity will be extremely low. Listen to your developers. They will raise such issues whenever you ask them for estimates on a feature. Make sure you build into your process and schedules enough time for them to get to an acceptable level of tech debt before moving to the next big chunk of work. For example, allow them to schedule a set amount of maintenance in each sprint or dev cycle, and define low-risk timeframes — such as right after a release — to do bigger tasks, such as improving the architecture and fill in the gaps left by cutting corners to get to release. Remember that crunch modes can only be sustained in short, rare burst and only should happen in exceptional circumstances. Death marches are the best way to alienate your team, make them unproductive, and risk their health, all while still missing deadlines and features. Setting realistic expectations while planning is vital to be able to deliver on time and on scope, and this is why the whole team should be part of the planning. Imposing arbitrary deadlines is just as detrimental as changing scopes without allowing for different deadlines; even if your teams are following agile practices it doesn't mean they can deliver arbitrary features at arbitrary time. It means they can either deliver a given feature with a well defined, immutable scope on time, or deliver vaguely defined features with changing scope eventually , possibly sooner than with a waterfall process. Focus on value Another common fallacy that brings managers and teams to look at cross-platform frameworks is that it'll allow them to save money. Unfortunately, that is rarely the case. We really appreciate the frameworks that don't focus their marketing message on the (often false) promise of saving money, but rather on the fact that they can bring value. The Flutter team for example often says that what they bring to the table is the ability to deliver more value for the users, and hence for businesses. With Flutter, their theory goes, developers can attain increased velocity. With a higher velocity, the customer and user is getting more value for the same amount of money, in the form of more features, more experimentation, and more solutions for users. They never focus their message on sharing code, or on lowering costs. In reality, programming languages and tech stacks are one small piece of building an app. No cross-platform solution will isolate your team from the complexities and subtleties of the platforms they run on. Anything beyond trivial apps, and your team will inevitably have to deal with the quirks of the OS that's hosting them. Where that line of triviality lies, depends on the framework and how they're employed. For example Xamarin can have that line further or closer to zero, depending if you use Xamarin.Forms or not. For others, it varies on the APIs you have to use. As a result, you will still need to have at least one developer that is an expert on each platform in the team to be able to be fully productive. Depending on the specific framework, you might have difficulties staffing a team. This might turn out to be more expensive than anticipated, nullifying any theoretical gains from sharing code. It's a careful balance to be found and it is not for everyone. We don't believe the need for platform experts will ever go away, considering how no cross-platform solution so far has ever found a way to get rid of that. Consider the 20 years run of Java, the poster child of \"Write Once, Run Everywhere\", or Mono — the ancestor of Xamarin, or any other technology. There is always a situation in which you need to, directly or indirectly, interact with the underlying platform outside of the \"safe space\" of the framework. What this all means is that choosing a cross-platform SDK because it is supposed to save you money is very likely to be a bad idea. What you should aim to get out of a cross-platform team is value for your users and, by extension, to your business. To obtain value, you need to be aware of your teams' processes and state of mind, and still be aware of what kind of value a framework can bring, and in which conditions. Get value out of cross-platform This does not mean that all those cross-platform frameworks have no place in our toolboxes. What it means is that we need to treat them like tools — which is what they are — rather than solutions. If you have a problem that can be solved with such a tool, and a team that can pick up the new framework and be proficient with it, and you are aware of the compromises your choice involves, then you can get great value out of these tools and get to a satisfactory solution. The mere existence of these frameworks shows that someone thought they could be the right tool to solve their problems. Facebook created React Native because they had a lot of web frontend developers that were familiar with ReactJS, and couldn't staff mobile teams quickly enough for their enormous growth. They created mixed teams of ReactJS and mobile developers, and gave them a tool that allowed them all to work on mobile apps. This is why React Native works so well for them. Besides having orders of magnitude more engineers than most companies out there that they can throw at problems, they have troves of developers that are familiar with the paradigm and language, or familiar with the underlying platforms. The same goes for AirBnb which is using it heavily and to great success. AirBnb managed to find problems that fit the strengths of the tool; Facebook had built the tool to work in their specific situation. The dozens of corporates that successfully employed Xamarin to build tools and projects had many .Net developers at their disposal. The agency that developed the Hamilton app in Flutter didn't actually have much in the way of Dart developers, but they had the perfect kind of project for the framework: relatively low complexity, heavily branded UI, using Firebase, to deliver very quickly. And they all have one thing in common: they used the right tool for their problems — or, in Facebook's case, they made a tool that fit their problems. Since almost nobody is Facebook and we cannot afford to create tools from scratch every time we have a problem, all we can do is to be sure that we need a new tool in the first place, and then only in that case pick the one that works best for us. If you don't have the know-how in house for this sort of decisions, you can reach out to an agency like Novoda. We have helped several of our clients understand all the ramifications of their choices over the years, and would be happy to work with you on finding the best solution. Conclusion Before choosing a tool, no matter what its nature is, it's important to first thoroughly understand the problems and pain points that you are trying to alleviate. If and only if your problem is of a technical nature, then you should proceed to pick the tool that best helps solving it. But if your issue is rooted elsewhere, then no matter what technology you adopt, you'll be doomed to repeat the mistakes of the past. After all, technologies can't create a process or a team. React Native, Flutter, Xamarin: a comparison They told you it would be impossible to choose a cross-platform framework from a diagram — they were wrong", "date": "2018-01-30"},
{"website": "Novoda", "title": "Exploring chat bots on the Amazon Alexa; to infinity and beyond", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/alexa-space-quiz-skill/", "abstract": "Recently Amazon announced a contest to evaluate the best Alexa skills, running on the Echo and optionally the Echo Show whilst targeting kids under the age of 13. Novoda is always up for a challenge. Here is what we built. At Novoda we have a mission statement: Together on a journey of learning and growth . We wanted to build a skill that can help everyone learn more about astronomy and space, so we decided to create an Amazon Alexa Skill that kids can use to learn new things and also have a bit of fun. Our skill logo: Space Quiz What it does This skill's aim is to help kids (but also adults too!) to learn more about space by asking them some questions and giving them the correct answers when they make a mistake, as well as giving some extra information about the topic. The skill is very simple and easy to use, as the game begins it asks a question and gives the different possible answers, if the user gets the answer right then they get a point. After a predefined number of questions the round ends and the skill gives the user the final score, then they can decide to play again or stop the skill. This really is a Minimal Viable Product for what can be done with the Amazon Alexa platform. We hadn’t worked in this environment before and wanted to make sure we could get something completed in the timeframe of the competition ( 3 weeks ). All the while making sure we learnt about Alexa and the platform for our own knowledge, and to share back inside Novoda. We’ll be making a lot more chatbots in the future. How we built it Our code is based on the Trivia template provided by Amazon and added a few changes to make it work with the latest Alexa Skills Kit SDK version. We also improved the conversational flow, so that the user would receive additional information related to the question's topic after the answer. This way we kept our original idea of building a skill which was both fun and and practical in learning new facts about space. Amazon provides the ASK-CLI which is an easy-to-use command-line tool to initialize, clone, deploy and interact with a Skill; it came in really handy when testing the skill since it allowed us to deploy it by running the ask deploy command. While the skill worked great already with a voice-only Echo device, we took it one step further and leveraged the display of the Echo Show. We used Render Template and some beautiful Creative Commons, space-themed, images as background for the displayed questions. We applied a darker overlay to these images to always ensure great legibility for the text. You can see the skill in action in the following video: Space Quiz works great with the Amazon Alexa ecosystem of devices, in the future we will also make it available for the Google Assistant. This will greatly increase the potential user base, thanks to the increased selection of supported devices available: from other smart speakers, like the Google Home or the JBL Link, to traditional Android phone, Android TV and Android Auto. To do so we might extract most of the data for the game, including questions, answers and follow-up sentences, to a shared source. Firebase Cloud Firestore could be a good solution for that. Dialogflow will handle voice and context recognition, allowing us to drive the conversation and provide an increasing level of difficulty depending on the conversation flow. If you want to know more about how to start creating a Dialogflow chat bot, we have you covered . All the code powering Space Quiz is open source, feel free to have a look at our Alexa Space Quiz repo . You can try the skill yourself by enabling it from https://www.amazon.com/Novoda-Ltd-Space-quiz/dp/B078SD9RWL .", "date": "2018-02-05"},
{"website": "Novoda", "title": "Kotlin on iOS", "author": ["Tobias Heine (Android Developer)"], "link": "https://blog.novoda.com/kotlin-goes-ios/", "abstract": "In our last blogpost we deep-dived into Kotlin’s multiplatform project feature and discussed how we can use it to build backend, Android and web applications written in Kotlin. Today we will investigate how we can add an iOS client to such a setup using Kotlin/Native and how easy that is. Kotlin/Native Kotlin/Native is a tool that enables us to compile Kotlin code for platforms without a JVM or a JavaScript engine. In our example we will use it to build an Objective-C framework and integrate it into a XCode project. Tooling Kotlin’s multiplatform projects use Gradle as build tool and JetBrains published a plugin for Kotlin/Native. apply plugin: 'konan'\n\nkonan.targets = ['iphone', 'iphone_sim']\n\nkonanArtifacts {\n    framework('KotlinGameOfLife') {\n        enableMultiplatform true\n    }\n}\n\ndependencies {\n    expectedBy project(':common')\n} Using the konanArtifacts closure we can specify the type of artefact we want the compiler to build. Besides many others it also supports framework , which will generate an Objective-C framework. Later on we will have a look how we can include such framework into an existing XCode project. Besides that, we also need to explicitly enable the multiplatform support. In similar fashion to the artefact we can specify a konan.target for a specific platform. By default the compiler will build the artefact just for the host, meaning the computer that is used for building. As for any other platform module within a multiplatform project we also need to specify the dependency to the common module. The plugin generates a couple of tasks, in our example compileKonanKotlinGameOfLifeIphone and compileKonanKotlinGameOfLifeIphoneSim which will generate the Objective-C framework. So all we have to do is run gradle build on our multiplatform project to generate that artefact, which will be located in build/konan/bin . Actual declarations for iOS Our multiplatform project implements the Game of Life where the business logic and presentation layer live in the common module. Part of that is an expected class GameLoop which has an actual implementation for each platform. interface GameLoop {\n\n    var onTick: () -> Unit\n\n    fun startWith(intervalMs: Int)\n\n    fun stop()\n\n    fun isLooping(): Boolean\n}\n\nexpect class GameLoopImpl() : GameLoop Since our GameLoop requires some sort of asynchronicity the question is which API we can use to implement it on the Native side. The JVM implementation uses RxJava , while the Javascript version uses the window from the browser environment. For Kotlin/Native it might be tempting to look at Kotlin’s coroutines , but unfortunately these are not yet available on this platform. Another option would be to rely on a C library using Kotlin/Native’s interoperability feature . This feature analyses C headers and generates bindings that can be called from Kotlin. Luckily Kotlin/Native comes already with bindings for a few iOS libraries, for example the NSTimer . A list of all available bindings for iOS can be found here . package com.novoda.gol\n\nimport platform.Foundation.*\n\nactual class GameLoopImpl : GameLoop {\n\n    private var timer: NSTimer? = null\n\n    override var onTick: () -> Unit = {}\n\n    override fun startWith(intervalMs: Int) {\n        timer = NSTimer(NSDate(), 0.5, true, {\n            onTick()\n        })\n        NSRunLoop.currentRunLoop().addTimer(timer!!, NSRunLoopCommonModes)\n    }\n\n    override fun stop() {\n        timer?.invalidate()\n        timer = null\n    }\n\n    override fun isLooping() = timer != null\n\n} The drawback of using interop is that we’re missing the usual level of IDE support; things like autocompletion are unavailable when using such bindings. Integrate the generated Objective-C framework into XCode This is fairly easy. Open XCode, select the tab General and click on + next to Embedded Libraries . In the chooser dialog select Add other and choose the generated framework which is located under build/konan/bin . That’s all. Next we can just import the framework into any Swift file and access your Kotlin code. import UIKit\nimport KotlinGameOfLife\n\nclass UICell: UIButton {\n    \n    let position:KGOLPositionEntity?\n    \n    init(frame: CGRect,position:KGOLPositionEntity) {\n        self.position = position\n        super.init(frame: frame)\n    }\n    \n    required init?(coder aDecoder: NSCoder) {\n        self.position = nil\n        super.init(coder: aDecoder)\n    }\n    \n} Please note that the generated files are prefixed with your framework name, that has been specified in the build.gradle , as shown above. In this example KGOLPositionEntity is generated from PositionEntity.kt . The KGOL prefix we chose stands for Kotlin Game of Life . Conclusion Kotiln/Native finally adds support for targeting iOS and other platforms, which was previously missing from Kotlin’s featureset. The integration into an existing multiplatform project is fairly easy and will hopefully enable a lot of teams to share their code among the different mobile clients. You can find our example project here on GitHub. If you have any question, don’t hesitate to contact me on twitter or get in touch with the JetBrains team using the official Kotlin Slack .", "date": "2018-02-15"},
{"website": "Novoda", "title": "XConf Manchester", "author": ["Niamh Power (iOS & Android Software Crafter)"], "link": "https://blog.novoda.com/xconf-manchester/", "abstract": "Back in July, Joe and I travelled across from Novoda's Liverpool office to Manchester for ThoughtWork’s day long conference, XConf . One of the great features of XConf, is that it is also run in Hamburg a few days earlier. This is an interesting way of running a conference, as it allows a group of people who would otherwise never meet to potentially interact online regarding similar topics of interest. Being based in the Liverpool office, we don’t get the chance to visit the same variety of conferences as our London counterparts. Though that isn’t to say the quality isn’t there, and XConf certainly delivered. The range of topics on offer for a two-track conference was great, covering topics from Artificial Intelligence to VR. Two talks particularly stood out to me, and these covered topics on Voice Interfaces and language. Firstly, we had Conversational AI and Natural Language Processing (NLP): It is All in the Intent from Pinar Wennerberg and later in the day we had Evolutionary Interfaces: Exploring from Voice to VR from James Linnegar. It was fascinating hearing about the issues surrounding making Voice assistants more user friendly, and the techniques behind crafting any trigger words and follow up questions. Another interesting talk was All Roads Lead to DevOps , from Erik Dörnenburg, Developer and Head of Technology at Thoughtworks. An interesting topic for me, as it has always been a bit of a buzzword at previous companies I have worked at. It was great to hear the issues the speaker felt were prevalent in the industry, how these are similar to the issues felt when Agile was first becoming a widely used technique, and the ways in which teams can combat issues in the workplace. Overall, the conference was great for a single day, and it was great to get out of Liverpool for a change of scenery! The opportunity to meet others in the community that so often crosses over into ours was great, and we got some good interest in the Google Developer Group that we run in Liverpool once a month. I’ll definitely be keeping an eye out for Xconf 2018!", "date": "2018-03-02"},
{"website": "Novoda", "title": "Wine, cheese and code: what dotSwift taught us about remote working and team bonding", "author": ["Berta Devant (IOS Developer)"], "link": "https://blog.novoda.com/dotswift-conference-ios/", "abstract": "On 29th of January, the entire Novoda iOS team attended the dotSwift conference in Paris, France. We came from Berlin, London, Liverpool and Barcelona to spend a weekend there, both to learn and meet other developers, and to spend some much needed bonding time together. dotSwift is the IOS part of the dot conference series . If you have never attended a dot conference, they are very short and focus primarily on giving you techniques and tools to help in your day-to-day development. The venue was the ‘Théâtre de Paris’, which makes for a fantastically unique setting. ‘Théâtre de Paris' where the conference takes place, with the most comfortable conference seats ever! The conference is a bit different from the norm. Not only does it last just one afternoon, but that one afternoon is packed full of deeply technical 18min talks, as well as 4min lighting talks, each of which are recorded Videos here . Each talk ends with the MC (Daniel Steinberg - a very funny guy and a great presenter!), asking the speakers some questions, which always provide some extra insight into the subject matter. Shout out to Daniel Steinberg for being a genuinely talented and funny guy! Unusually, the conference does not announce the topics of the talks, just the speakers themselves - so, you never know exactly what you are going to learn until you get there! While this might seem strange, it seems to work really well, probably due to the calibre of speakers that the conference has. In my opinion, it makes the dotSwift conference a Must Do, if you live and work in Europe. You can see the full speaker line up here A weekend in Paris The Novoda team was made up of both newbee and returning visitors to dotSwift. But regardless, one thing we were all there for was a weekend of team building and 🧀 and🍷. Nothing like sharing some delicious cheese and bread Because the conference is so short, it was a great one to attend as a team, and we even managed to fit in a couple of days together before things kicked off on the Monday. At Novoda, it doesn’t matter if you’re in the Berlin or London office, we all chat all day long - staying in touch on Slack, visiting the other offices from time to time and even have a iOS guild meeting every two weeks where we can all sit down and talk [1] . But there’s nothing quite like all being together in one place! The entire Novoda iOS team having dinner with two added extras Being able to just sit down over dinner and drinks, talking to colleagues about non-code related things was fantastic. It means that you get to know your coworker as more than just a Slack name or a voice while pairing or a name on a PR; you get to know them as people, have funny (and sometimes weird!) conversations, find common interests, as well as find solutions to work problems we are facing. For example, some realized over the weekend that they all shared a common love for mathematics and mathematical models, and a Slack channel was then created to share that passion with the rest of the company. Two others shared a mutual frustration about their lack of Functional Programming practical knowledge with Swift, and so set up a date to run a kata inside the company to fix that. These are the things that only surface after you start to get to know someone, or when the perspective or environment has changed. Having an entire weekend where we could relax, explore and just enjoy each other's company -  not as developers but as people - was incredibly valuable for the dynamics of the team. I think we might need a bigger staircase 😄 New dotSwift 2018 threads For this edition of dotSwift, one of Novoda's designers, David, created this really cool tshirt design to commemorate the entire team being there together, and our mutual love for the language. They went down a storm, and overall, it could not have been a better conference, since not only were the speakers all top of the line and the talks very interesting, one of the core developers for Swift Core Team even came to speak. Our Swift Novoda T-shirts were a big hit on the conference floor Here are some of our favorite talks from the conference, together with why we loved it and what we learnt from it: I loved the last talk of the day, the one given by Ben Cohen, who is a developer for the Apple Swift Core Team out of San Francisco. Not only was it very educational, reminding me about features of Swift I often forget to use, but he also showed how these features get implemented into the language, how to create a good proposition for Swift 5 and he urged us to collaborate and be part of the discussion. His talk was very well done and delivered, he taught me something about Swift I can use and at the same time erased some of the fears about collaborating for open source Swift. 💯 Berta Devant My favorite talk was Elements of Functional Programming by Paul Hudson, which introduces an idea of how a good functional code helps to avoid state on your code and makes it easier to test. His talk is a very playful and entertaining introduction to swift functional programming, worth every minute of your time. You can watch it here Eduardo Urso Swift supports important concepts of functional programming. We tends to design solutions arounds objects because most of us have a object-oriented background. On his talk Graham Lee introduces an interesting codification using functions as domain objects. I recommend to have a look at his talk if you are already familiar with the functional operators of Swift and want to exercise your brain to thinks also Functional, and not only Object-Oriented. You can find Graham Lee slides from his talk here Giuseppe Basile It was definitely a difficult task picking out a favourite talk from the day, but for me, Ellen Shapiro’s talk on the power of protocols in Swift was particularly interesting. It is something that is very relevant to our day to day programming, and I have already been using some ideas from this talk in my work. Furthermore, learning about Sally Shepherd’s experiences with accessibility, and how this affected her use of mobile apps was fascinating, and the app that she showcased that countered any visual impairment was incredible. It was also great to see such a diverse group of speakers, from such different backgrounds, which helped compliment such a variety of talk topics. Niamh Power \"What if I told you, you could invent your own buckets\" - this message from Ben Scheirman's talk 'Buckets of Code' really resonated with me. In his talk he explains how we can make the most of well known design patterns, like MVC, by adding new objects that complement these \"buckets\" and that give a more appropriate home to code that doesn't really belong in either of them. Fabia Ben Cohen talk was definitely inspiring for everyone who is interested in taking the first step and starting contributing to swift language. He walked us through the process of creating real swift evolution proposal which he submitted just a couple of days before his speech. Ben showed us how we should think in terms of making swift proposals meaningful to others, computational performant and well named.I was fascinated by his process of thinking, attention to detail and enormous knowledge. Pawel For me, the best talk was Graham Lee’s on functional programming. It focused a lot on the theory of FP and what really pure FP boils down to, not just describing how map or filter work. My favourite bit was how a collection of items can be represented using just a function, which I thought was really crazy! It was unusual and has definitely made me curious about learning more about advanced functional programming, and how it intersects with maths. Alex Peter Steinberger’s Binary Frameworks in Swift lightning talk was awesome and included an impressive amount of detail in 5 minutes. I enjoy understanding low level details, and Peter’s talk inspired me to more closely follow Swift’s evolution in this respect. His talk was a great starting point for learning about what ABI stability means for different types of Apple ecosystem developers. One broad advantage that ABI stability will bring is that the Swift standard library can be shipped with the OS instead of with every single app. This will reduce each app’s binary size by a few MB and allow smarter standard library loading — yielding significant storage, memory, and startup time improvements for your users. As far as waiting for ABI stability, binary framework developers are the most impacted (they should wait for both ABI stability and Swift Module Format stability before easily adopting Swift). Another interesting fact he shared was that the way arrays are represented in memory has changed from 24 bytes to 8 bytes during Swift’s evolution. What this means is that if a new Swift array (8 byte) is passed to Swift code compiled for 24 byte arrays, uninitialised memory will be read 😱⚡️. Meghan Kane The lesson learnt Novoda has been growing and the iOS team along with it, which is fantastic, but has made it difficult for all of us to meet in person, and to really spend enough time together to build a relationship. By taking an opportunity for professional development and combining it with team building (and some cool new tshirts!), we made the most of our time together, and it proved to be hugely rewarding - and an experience we hope to repeat next year! 🎉 We'll be there to enjoy it together - hopefully with less flooding of rivers, and maybe with the chance to attend some of the workshops as well. Watch the recorded talks here See you again, dotSwift! Guilds aim is to acquire and spread knowledge inside the company ↩︎", "date": "2018-02-26"},
{"website": "Novoda", "title": "Towards Feature Teams", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/towards-feature-teams/", "abstract": "At Novoda, we pay close attention to what’s going on in terms of development and process best practices. In 2012 Spotify popularised the feature team approach , and from that point the idea has been revised, rebuked and modified. Throughout this post we will talk about our own experience with feature teams, based on what we achieved working with various partners, including SoundCloud and ImmobilienScout. What is a feature team A feature team is a long living team rather than a task-force, which disbands after the work has been completed, aiming to complete many end-to-end customer features one by one. The knowledge needed to deliver a full stack feature is distributed within the team by pairing between the different platform specialists. Such a team works cross-functionally and across different components. It should cover all responsibilities needed in order to ship a specific feature.This multi-disciplinary team will include product owners, user experience and visual designers, quality assurance and developers. They own a feature from the beginning to the end: defining, prototyping, implementing and shipping it. The opposite of a feature team would be a component team. Usually discipline-fixed. These teams are focused on  a specific component that is reused among features, like the persistence, and usually don’t change their scope. Feature team adaption Quite frequently a “mobile platform team” is completely segregated between designers and developers working in separate time frames and areas. Designs are done completely up front and there are usually once or twice a week catch-ups between designers and developers. In such a setup the feedback cycle between developers and designers is more than one week. This communication issue is multiplied by the fact that designers and developers are working separately in different rooms. Such a team setup could work at a small scale, when there is not enough manpower to form teams around single features, but becomes less and less manageable with a growing number of people and teams. Furthermore technical limitations are discovered too late, leading to unnecessary loops in the delivery process. How can we migrate these disparate platform teams towards more in sync and communicative feature teams? Split the platform teams into smaller feature teams. This enables the teams to focus on features and the necessity of a closer collaboration with designers becomes obvious because the feedback cycle becomes shorter. At this point it's worth reviewing the seating arrangements to bring the team closer together and organize planning and review meetings together with developers and designers. Create teams that can manage themselves and work on their feature set independently across the different platforms. Such teams should include platform specialists, designers and a feature owner . From now on each team should follow its own agile process including storyboards and ceremonies. One of the challenges in a feature team setup is how to deal with cross-boundary functionality like persistence. There should be an understanding of core components among the feature teams, which should be developed by the feature teams as needed. Working on features as well as doing cross-boundary refactorings locally enable the feature teams to evolve the application architecture very effectively without blocking other teams. There can be an incorrect assumption and false hope to create a team responsible for core components. This is not advised as it will lead to knowledge silos around the core of a system, contrary to the idea of a feature team. A core team acting vertically on the full tech stack will most times operate in maintenance mode rather than working on real customer requirements. Working model impact In a more traditional team setup, collaboration between different development teams as well as between designers and developers tends to be complicated or non-existent . Traditional communication can be time consuming, since dependencies between components require additional planning effort because teams are separated. When a feature is developed across multiple teams, no one feels responsible for the whole feature, but only for their part. And if a fundamental problem arises it’s not clear which team is responsible for addressing it. This can cause unnecessarily long release cycles and reacting quickly to a change in requirements is not possible. Furthermore, designers and product teams tend to do too much up-front work in such setup. Work that is very likely to be wasted effort due to changing requirements or technical limitations discovered during the development phase. When organised by feature, the team is able to solve problems by themselves. Forming smaller teams focussing on features leads to closer, more frequent communication between the different parties (developers, designers, product owners, etc). This has multiple observed benefits: More pairing: code becomes easier to understand as a whole by everyone and logic is shared across implementations Improved bug hunting: developers feel more comfortable fixing bugs in other platforms because they have already developed an interest on it Increased code ownership: people are more willing to own the whole feature stack’s code working this way Higher productivity: Less context-switching and more agile communication makes everyone more productive Less feedback cycles: Since developer and designer are collaborating from the beginning on, unnecessary round trips due to technical limitations in the design can be avoided Speeding up release cycles Focussing on a Minimum Viable Product (MVP), driving the iterations using Key Performance Indicators (KPI) and user feedback instead of Feature creep . Working with MVP/native prototypes allows a team to quickly gather user feedback and to verify the effectiveness of a feature. This can be achieved by using A/B Testing and feature toggles. A feature team can start working behind a feature toggle allowing the teams to work efficiently without blocking a potential releas e. This means the application is always releasable and the features being developed can immediately be merged into the codebase without affecting productivity. When the feature is ready, it can be enabled for a small percentage of users to be compared with KPIs in order to validate the assumptions. A real life example would be a task to add a new way of signing in on the mobile client. In a platform or component driven team setup the mobile team would implement just their part. But what about the server side? Is everything needed for this task ready? Would that break? This information is needed as it blocks the progress and that’s not something a single person might be able to figure out on their own. With feature teams there is no need to chase the person down from the backend team who had implemented it server side. Instead the feature team being responsible for this part would deal with this request. Since the sign-in is their domain, this task would never be blocked by other work on the backend, because the team focus only on this part of the stack. Scalability Multiple teams working on the same product (and possibly the same codebase) brings the question of scalability. How can teams be flexible enough to modify their structure as needed or form new teams for new features? The team structure should support as many small and focused teams as possible. There is no need for a hard limit on the number of teams. To support concurrent feature development and to facilitate scaling, the application should be split into modules based on domain concepts, once these separate concerns have been identified. Modular separation enables higher throughput for the feature teams that will be able to work safely on different pieces of the app at the same time without major side-effects. These modules should also have smaller local components with single responsibilities. This allows fewer dependencies between modules, so changes in a module can be done without minimal impact in other areas of the product. Aligning teams Having backend and frontend teams separated causes several problems. It leads to teams focusing more about the platform they own instead of a feature across boundaries. The communication among teams is more difficult and can be even counterproductive in case the teams do not share the same domain language. Instead, the backend specialists should be encouraged to work closely with frontend and mobile specialists, ideally sharing the same office space. Involvement in regular standups is key to effective communication, so these should be considered mandatory for the whole team. Pairing sessions between specialists can help identify issues early on and encourage discussion to find solutions that work for everyone. Pairing also encourages the knowledge transfer between platform specialists which will in the end lead to full stack developers. This people are then capable of moving to other teams as needed. When backend and frontend devs are in the same team, they mostly care more about each others’ boundaries and support each other, for example by writing tests. Integration tests written by the frontend team could be used as well by the backend team as part of their release process. When they are different teams, on the other hand, it is easier to blame the other team when something is broken. Conclusion The transition towards feature teams, especially in an environment with entrenched legacy processes, takes time, effort and requires two important things: an extensively planned migration and long-term commitment at all levels of the organisation. Processes need to be aligned, teams need to be split up and reformed — from a physical point of view as well as from a logical one, product needs to be on board, pressure to ship needs to align with the workload. Everyone needs to embrace the change long enough to allow it to work. Closer collaboration allows individuals and teams to rapidly react to change and will shorten the time to market . If you believe in empowering those with the knowledge to make an impact and enabling teams to master their skills, whilst setting visionary and achievable goals then feature teams are the most favourable setup for your technology company.", "date": "2018-03-08"},
{"website": "Novoda", "title": "(Code) Sharing is caring - An Introduction to Kotlin Multiplatform Projects", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/introduction-to-kotlin-multiplatform/", "abstract": "In this blogpost we will deep dive into JetBrains multiplatform projects feature, why you should consider using it and how you can do so to level up your product development. With Kotlin 1.2 JetBrains introduced a new experimental feature called multiplatform projects . This allows you, as things stand today, to share your code across any JVM or JavaScript based client. This can be your React based web-frontend, an Android client, a desktop client or whatever comes to your mind. TLDR; as proof of concept we have implemented the Conway’s Game of Life as Multiplatform Project which can be found here on Github. Why sharing code? The main idea of multiplatform projects is to share code between different teams and platforms that work on the same product. Code sharing is almost as old as writing code. Nowadays almost every software project uses some kind of shared code: networking, persistence, data handling or even tooling. For almost every use case, each platform offers a large quantity of libraries. However, when it comes to sharing code between different platforms within the same organisation we see much less effort. Why is that? The most obvious reason is the incompatibility between the different technology stacks: the backend teams deploy their microservices in Go or Scala, the web team is catching up with the latest JavaScript frontend framework and the mobile teams are dealing with Java/Kotlin and Objective-C/Swift to write their native Android and iOS applications. While the big players may afford to maintain such a big number of teams and technologies we can see lately a trend towards cross-platform frameworks, especially on mobile. Depending on your needs, such tools might make sense. But trying to cover multiple platform SDK’s like Android or iOS with a single framework also comes with downsides such as UX challenges due to platform specifics, lack of tooling support and flexibility. In contrast, Kotlin’s multiplatform project focuses on sharing the logic and leaving the platform specific concerns to the platform specialists and SDK’s. Especially for frontend platforms this makes total sense. Why should a company decide to spend her budget on implementing the same business logic and the same MV’X’ design pattern on different platforms when she can have it all from the same source? On top of that, modern programming languages like Kotlin & Swift are becoming more and more similar which makes it easier for iOS developers to adopt Kotlin and have the whole team share the same codebase. It’s not just about pure development costs, another big benefit is to share the knowledge across your different teams. How often did you see that the web team spent ages to implement all the corner cases of your business logic exclusively for the web while your mobile teams are running into the same cases on the different platforms. All this could be avoided by sharing the logic and have each team truly owning the same code. This is basically where the idea of feature teams is coming from and a Kotlin multiplatform project makes it super easy to have such team structure in an organization. We can also see the positive effect on code quality in relation to shared code. The more users a library has or the more people contribute to it, the more effort is usually spent into stability and high test coverage. The same can be applied to code being shared within the same organisation. If you know that three other teams are relying on your code, you better do it right. This means that in the end, sharing code results in cleaner code, less code and ultimately less bugs. What code should be shared? The most obvious part to share is your data models that represent your real world entities. Think of all the duplication you could get rid of by sharing your Data Transfer Objects , Data Access Objects and the whole conversion mechanism. This sounds nice, but in the end it’s just data representation. Multiplatform projects show their real power when they can share the business logic, but this a little bit tricky. It requires you to model your business logic properly. This is a complete topic on its own where tools like Domain Driven Design come into play. Furthermore, you need a clear separation between your business logic and platform concerns like user interfaces, hardware sensors etc. A good starting point could be to design your application in such a way that you could easily port it from mobile to web and vice versa, hook it into a simple terminal client, or to quote Uncle Bob “Your delivery mechanism is just an implementation detail”. For frontend applications you could even go a step further and try to generify your presentation layer so it can be shared. This is definitely a thing since all the fancy architectural design patterns you have heard of, like MVC, MVP, MVVM or MVI have the goal of separating the rendering logic from the business logic and making your views passive . The key value of having such architecture is to increase testability. It becomes much more valuable when you share the core business logic with proper test coverage across platforms. Unlike the other solutions, Kotlin's multiplatform project does not try to standardise the platform dependant needs like threading, I/O etc. Instead, it gives you flexibility by allowing you to very easily use different implementations of abstract concepts within the shared code. This also allows you to share the test code independently of the platforms making sure that the implementations satisfy the expectations by running these tests not just once but for each platform. How does it work? Set up a Multi Platform Project You can easily create a multiplatform project using the project wizard from the latest Version of IntelliJ IDEA with at least Kotlin Plugin 1.2 installed. What you get is a common main module, a JVM and JavaScript module with both depending on the main module. You should treat each platform module as a common module which enables you to have multiple clients per platform. E.g. you could have a desktop and an Android client where both are based on a common-jvm module. The underlying build system supported is Gradle , which makes it really easy to configure your multiplatform project according to your needs. The dependencies between a common module and platform module is defined by using the keyword expectedBy in the platform modules build scripts. This signals that the actual platform specific implementations are expected and can be used by a common module. Expected Declarations and actual Implementations Kotlin multiplatform project only adds expect and actual keywords to the language. Expected declarations are defined in the common module and the platform specific modules use the actual keyword in the concrete implementations. In traditional abstractions, you would have different names for the platform specific implementations. Unlike in Kotlin, the common module and platform specific modules need to have exactly the same fully qualified names. This allows you to easily create and use such classes in a common module. // Common\nexpect class GameLoop {\n\n    var onTick: () -> Unit\n\n    fun startWith(intervalMs: Int)\n\n    fun stop()\n\n    fun isLooping(): Boolean\n} // JVM\nactual class GameLoop {\n\n    private var gameLoop: Disposable? = null\n\n    actual var onTick: () -> Unit = {}\n\n    actual fun startWith(intervalMs: Int) {\n        gameLoop = Flowable\n                .interval(intervalMs.toLong(), MILLISECONDS)\n                .subscribe {\n                    onTick()\n                }\n    }\n\n    actual fun stop() {\n        gameLoop?.dispose()\n        gameLoop = null\n\n    }\n\n    actual fun isLooping() = gameLoop != null\n\n} The expect keyword is not limited to interfaces and abstract classes. It can be put into any top-level declaration available in Kotlin. It can, for example, be applied to top-level functions. You simply expect the counterpart of the expected class or top-level function to be available in each platform (and provided via the actual keyword) and directly use it in the common module. The expect declarations cannot have implementation code and the actual declarations must have the exact names implemented in their respected platform. Limitations Kotlin common modules can only use the Kotlin language with the common version of the Kotlin standard library (kotlin-stdlib-common). Although Java and JavaScript interoperability is really good with Kotlin, the common module cannot have any Java or JavaScript code which also prevents you to depend on any platform specific library in the common module. But at the same time the expect keyword allows you to do the abstraction very easily to make use of different but similar libraries for each platform. Native and therefore iOS support is still missing, but has been already announced by JetBrains. On February 14, 2018 JetBrains released Kotlin/Native 0.6 which adds support for multiplatform projects. In this blogpost we explain how to add an iOS client to such setup. A common concurrency model is not yet available since Kotlin’s Coroutines are still experimental and only available for the JVM. On the other hand, there are discussions going on whether it’s worth to port RxJava to a pure Kotlin version or to come up with a Reactive Kotlin multiplatform project abstraction over already established RxJava and RxJS libraries. Alternatives There are the obvious Cross Platform Frameworks like Flutter which is based on Dart and has been lately released by Google, and React Native which is based on JavaScript, developed by Facebook and heavily used by AirBnB and others which provide a facade around the Android and iOS SDK. Since you need to develop against the facade SDK in such platform, you may lose flexibility to be able to use platform specific APIs. Also such frameworks unify the UI development which can be challenging due to platform specific UX patterns. Besides that there have been already similar initiatives like Google’s j2objc or web toolkit . One success story for these tools is, for example, that Google used them to port their Inbox application to the web and iOS . These tools also allow you to share the business logic and data models by writing them in Java and making them usable in different platforms. These approaches are mostly unidirectional meaning that only the common pure logic is shared and it is harder to use platform specific declarations in the shared code. To be able to have platform specific code usable in the shared code, you would need to have your own abstraction with multiple wrappers for each platform which makes it harder to setup. Conclusion JetBrains is pushing hard for Kotlin and that’s good. Kotlin multiplatform projects are a great initiative towards truly cross-platform, even cross-tier development with all its benefits. Write code, do it once and deploy it on all your platforms. Once iOS is supported this will be a true alternative to the existing cross platform frameworks. We’re super excited about that. Our implementation for Conway’s Game of Life as multiplatform project can be found here on github.", "date": "2018-01-31"},
{"website": "Novoda", "title": "Novoda Design does Mobile UX Conference London 2017", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/muxl2017/", "abstract": "The Mobile UX London Conference (MUXL), founded by Naveed Ratansi has developed from a series of design meet-ups focussed on mobile user experience. The design team at Novoda were lucky enough to attend their third annual conference with a diverse set of topics spanning research, analytics and emerging technologies. As well as an impressive lineup for talks, there were two workshops tracks deep-diving into chatbots, design strategy, innovation and expert critique techniques. Our very own Alex Styl treated us to the Inclusive Design In Action workshop, highlighting the importance of environmental considerations in design. For a moment, we were a product team at an all-new radio start-up asking the questions What if a person’s hands are busy cooking? What if their eyes are focussed on driving? or What if they’ve left their headphones at home ? This emphasised the reality that a disability is simply a mismatch between a person’s abilities and their environment. Our highlights To summarize all of the talks and workshops would be a lengthy read so here are some highlights from the team. I enjoyed every talk, workshop and moment I was in the MUXL, it’s great that people share their knowledge and experiences and we learn from each other. My favourite talk was given by Jan from Trainline, he talked about how to build a data driven product, one of his quotes was ‘The most important design work is often not sexy’ which I couldn’t agree with more, I believe design should have meaning, even for visual, especially in this agile high-velocity development environment. It was a day full of eyebrow-raising talks. I admired the way Airbnb interviewed users worldwide, from which there emerged a new dimension of user testing, coupled with the challenges of designing for this kind of audience. One of the most captivating talks was about storytelling through VR, which communicated it’s essence in a very enjoyable way. It was one of those things you wish you could experience again and again! It was great to see such a diverse set of topics at MUXL this year. Hollie Lubbock's talk on designing with data was especially interesting emphasising the need to use data to inform, define and measure the success of design. Talking of machine learning, she framed the machine as an 'essential member of the design team'. \"It was a day full of design discussions and ideas around User Experience! I was really happy to see mature talks on conversational interfaces and data in terms of UX design. The one that stood out to me was Stratis Valachi’s’ The Future of Conversational Experience discussing about the differences of Visual and Conversational Interfaces and how combining both can produce new interesting interface solutions. Overall, MUXL left me with lots of ideas to explore and interactions to investigate.\" All-in-all, we had a great day! From the Design team at Novoda, thanks to all at MUXL Conference for organising a fantastic line-up and hosting a great group of design folk.", "date": "2017-11-21"},
{"website": "Novoda", "title": "Onboard your users with Lottie of Spritz", "author": ["Francesco Pontillo"], "link": "https://blog.novoda.com/onboard-your-users-with-lottie-of-spritz/", "abstract": "Creating an onboarding for your app can get slightly tricky when combining Lottie with a ViewPager. What if we could get rid of all the nuances and just import an animation, drop some configuration settings and enjoy life? In our previous \"Whole Lottie Love\" blog post, Chris Basha showed us how to leverage Lottie to deliver pixel-perfect animations for onboarding screens with some basic setup and minimal coding. Let’s take the next steps to simplicity. Introducing Spritz In order to maximise code reuse and deliver high quality onboarding animations, here at Novoda we developed Spritz , an Android library that lets you effortlessly attach a ViewPager to a LottieAnimationView . Spritz will do the heavy lifting, automatically triggering animations when the user enters a page and transitioning to the next page as they scroll. How to make a perfect Spritz So what do you need to create a beautiful animation for your onboarding? a Lottie animation, containing all the steps for your onboarding pages and transitions in a sequential way a ViewPager with the appropriate number of pages set a splash of code Then do some stirring, and that's it. Really, you don't need anything else. Let us guide you to show you all the steps to prepare the tastiest Spritz ever. Lottie animation, your prosecco In order to make a successful Spritz that will make your guests jealous, your animation has to have one \"step\" for each transition between pages in the ViewPager . When the transition to a new page has completed, you can optionally start a new \"autoplay\" step that will be played once the ViewPager has settled on the page. For instance, if you have 3 pages the structure of your animation will be something like: The duration of each \"autoplay\" and \"swipe\" segment is not mandated, and can differ from page to page. Keep track of the duration of each segment, and make sure they appear sequentially in the After Effects comp. It’s good practice to avoid still frames on “swipe” segments, as the user will see no reaction as they’re sliding across the screen. Since this segment will usually be animated based on the user swipes, it’s a good idea to use a linear interpolation in “swipe” sections, as it will follow the user’s gesture matching its speed more closely. An example of how the animation is triggered by swiping through a ViewPager : as soon as the swipe animation completes, the autoplay starts._ Once you have created such a comp in After Effects, you can export it to Lottie as explained in Chris’ blog post . ViewPager, your glass Add a ViewPager to your layout, right after your LottieAnimationView . Remember the ViewPager has to be on top of the LottieAnimationView , or touch events will be handled in a weird, unreliable way. <RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n  xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n  android:layout_width=\"match_parent\"\n  android:layout_height=\"match_parent\">\n\n  <com.airbnb.lottie.LottieAnimationView\n    android:id=\"@+id/animation_view\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"wrap_content\"\n    android:layout_centerInParent=\"true\"\n    app:lottie_autoPlay=\"false\"\n    app:lottie_fileName=\"my-lottie-animation.json\" />\n\n  <android.support.v4.view.ViewPager\n    android:id=\"@+id/viewpager\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\" />\n\n</RelativeLayout> A splash of code The last step is to tell Spritz what are the steps that we baked in the Lottie animation. Let's start by retrieving a Spritz builder by setting the target LottieAnimationView : Spritz.Builder builder = Spritz.with(lottieAnimationView); Let's then use the builder to set the steps with the appropriate durations of each \"autoplay\" and \"swipe\" sub-steps. This is done by calling withSteps() with our comp’s SpritzStep s: Spritz.Builder builder = Spritz.with(lottieAnimationView)\n    .withSteps(spritzStep1, spritzStep2, spritzStep3); Creating the steps is easy, too. For example, a step that has an autoplay animation of 1 second and a transition animation of half a second would be built in the following way: new SpritzStep.Builder()\n    .withAutoPlayDuration(1, TimeUnit.SECONDS)\n    .withSwipeDuration(500, TimeUnit.MILLISECONDS)\n    .build(); The offsets between one step and another (within the full animation) will be automatically calculated by the library, which means that adding one step in between your pre-existing ones, or changing their order, won't introduce a significative diff in your code outside of the withSteps() method call. In the end, just call builder.build() to retrieve your configured Spritz instance. A full Spritz configuration will look somewhat similar to: spritz = Spritz.with(lottieAnimationView)\n    .withSteps(\n        new SpritzStep.Builder()\n            .withAutoPlayDuration(1, TimeUnit.SECONDS)\n            .withSwipeDuration(500, TimeUnit.MILLISECONDS)\n            .build(),\n        new SpritzStep.Builder()\n            .withAutoPlayDuration(500, TimeUnit.MILLISECONDS)\n            .withSwipeDuration(500, TimeUnit.MILLISECONDS)\n            .build(),\n        new SpritzStep.Builder()\n            .withAutoPlayDuration(500, TimeUnit.MILLISECONDS)\n            .build()\n    )\n    .build(); Stirred, not shaken Once you have created your Spritz configuration object, simply attach the ViewPager in onStart() : @Override\nprotected void onStart() {\n    super.onStart();\n    spritz.attachTo(viewPager);\n    spritz.startPendingAnimations();\n} We are also calling spritz.startPendingAnimations() to make sure that any autoplay animation in the currently selected page is animated correctly. Symmetrically, don't forget to detach your ViewPager from the Spritz instance in onStop() : @Override\nprotected void onStop() {\n    spritz.detachFrom(viewPager);\n    super.onStop();\n} Congratulations, you just made your first onboarding screen using Lottie and writing (almost) no code! Get a taste of Spritz Here's how a simple yet tasty animation looks like in our demo application: So go ahead, install our demo app, \"Taste of Spritz\", from the Play Store and try it yourself! To use the library in your app, simply include the following Gradle dependency: dependencies {\n    implementation('com.novoda:spritz:1.0.0')\n} Conclusions Spritz is a work-in-progress library, meaning the API can change while we improve it, but fear not: as you have noticed, the configuration needed to integrate an animation with a ViewPager is so minimal that any change would require only a few minutes. You are encouraged to participate in the development of Spritz by giving out your ideas, opening bugs and — even better — submitting Pull Requests at our novoda/spritz Github repository ! The real spritz Wait, were you here for an actual Spritz recipe? Alright, that’s easy: Add some ice into your glass Add 3 parts of prosecco Add 2 parts of Aperol, Campari, Rabarbaro or any other suitable Italian liquor Add a splash of soda Drop a slice of orange in it Stir Enjoy! Remember that spritz is an aperitif, just like onboarding is for your app!", "date": "2017-11-17"},
{"website": "Novoda", "title": "Firebase Summit'17", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/firebase-summit17/", "abstract": "The Firebase Summit 2017 took place in Amsterdam Francesco, Rui and Luis were there and they've shared with us the talks, tips & tricks they found more interesting about that great event Extending Firebase to the web James Daniels , Erik Haddad James & Erik describe the scenario of two frontend developers that want to build an app -  onSnapshot - to aggregate the most interesting news about Firebase. However they have to overcome not having any backend developers on their team... With a small set of requirements, that include the ability to store articles and comments, a live visitor count on each article and caching, we’re shown how some of what the Firebase ecosystem has to offer makes it possible to fulfill these requirements with great ease. Code-wise, using AngularFire makes it easy to interact with all the Firebase services being used. Storing articles and comments is achieved through Firebase’s new database solution: Firestore. A JSON document store that allows us to store collections of key-value pairs (JSON documents) which can contain collections in themselves as well - perfect for managing articles and comments. Just like Firebase’s first database offer, it’s real time but it bundles new features like a more powerful query system and shallow query results. onSnaphot's Firestore structure to store articles and the respective comments. The auth system is provided by Firebase as well, having the app wrap up what AngularFire offers by default, like the anonymous login and Google login. Having every user automatically sign in , anonymously, makes it easy to apply server-side rules (for the database) and it allows for an easy transition from a user being signed-in anonymously to when the user signs in with their account. Another key take-away is the way they leverage the way RTDB works with connections , in order to build an article view count. Whenever a user is viewing an article - signed-in anonymously or through Google - a reference is stored in RTDB, mapping the article id to that user’s id: /articleVisitors/articleId/userId . An onDisconnect() operation is then set on this reference so that when it’s triggered (server-side) it auto-removes itself. This covers a few scenarios: browser crashes, users closes the browser, etc. However, if the user just navigates away, RTDB still maintains a connection, so it’s necessary to remove the reference when this particular Article view is destroyed. To finally get the view count, Cloud Functions are used to monitor changes on these articleVisitors/articleId/userId references. When one occurs, the view count - stored in /articleViewCount/articleId - is updated. Finally, Firebase also offers hosting, making it easy to serve your web app through a powerful CDN. Make sure you check the full talk here . Actionable insights with Firebase Predictions Jumana Al Hashal , Subir Jhanb Jumana and Jhanb presented Firebase Predictions, a machine learning powered feature that analyses past user behavior to try to predict the future one. For example, it can predict when users will churn or not spend in your app, so you can take action and improve your retention or app revenue How does this work? A training data is fed into a neural network structure using TensorFlow. Then the model is asked to take the entire historic period of events and generate predictions for the upcoming 7 days. These predictions are \"labels\" which get assigned to users: will-churn , will-spend , will-not-churn , will-share , etc. In the meantime, you set up rules in your Remote Config (or push notifications) console using those labels. Let's say you want to offer a discount price to users that will possibly churn. In that case you'll set up a Remote Config condition saying: when <user has label will-churn> then <show discounted price> is true In this way, when the Firebase Predictions algorithm flags users as will-churn they will automatically get into that bucket and the Remote Config value will change for them, revealing the feature you want to show. To know more about Firebase Predictions check out this very detailed post by Joe Birch https://medium.com/@hitherejoe/exploring-firebase-predictions-fa22d093f98d A/B Testing and More with Firebase Laurence Moroney , Arda Atali The new Firebase A/B Testing feature will let to run proper A/B test experiments, allowing you to define specific audiences for your experiments, multiple variants, targeting percentages and so on. The goal of your experiment must, of course, be selected; this is what you are aiming at improving with your changes. You can select one primary goal and multiple secondary ones, even if you should keep your experiments as focused as possible in order to avoid bias in users’ flow and in your decisions. Results are very easy to analyse, in a simple table providing goal metrics for every variant, including the control group. The Firebase console will also highlight what the best variant for the primary goal is, so you don’t even have to necessarily look at the numbers and percentages: Firebase does everything for you. You can also start a notification-specific experiment, without writing a single line of code: just access the Firebase Console, and start an experiment from the Notifications panel. Your user groups will then receive different notifications according to the variant they fall in: goals and outcomes work as just the same. Introducing Cloud Firestore Jonny Dimond, Sarah Allen , Alex Dufetel Cloud Firestore is the latest database offering from Firebase. You can see it as a spiritual successor to the Real Time Database. Like its predecessor it bundles together useful features like offline capability and real time updates. However, unlike RTDB it offers much more powerful querying and data structuring capabilities. Example structure showcasing a structure with collections (“Articles”), documents (“Article”) and sub-collections (“comments). RTDB makes querying hard since it’s not possible to perform queries over more than one property, due to its underlying data structure. Unless you structure your data in advance, with all the querying you’ll be doing in mind, you’re forced to go through most of it in order to find what you’re looking for. At the same time, RTDB’s scaling capabilities will hit a roof at around 100k simultaneous connections, forcing you to partition your database through other (new) projects. Both these issues are solved in Firestore: querying is much more powerful and scalability is, according to Google itself, already better than what RTDB offers and will eventually reach a point where it shouldn’t be a concern - i.e. you won’t have to worry about sharding at all as resources should scale appropriately when necessary. Focusing on querying, RTDB would only allow you to sort and filter data based on one of the following: value of one of the child’s properties, the key of children or the value of children. You’d then have to work with filters to get the query results you were looking for. Improving on this, Firestore offers compound querying, enabling you to chain multiple queries in one go: saiyansRef\n    .where(“planet”, “==”, “earth”)\n    .where(“scouterLevel”, “>”, 9000) Be aware that such querying requires you to define indexes through Firestore’s console. If you fail to do so, the official SDK will let you know! Another important detail to consider is that query results are shallow by default, which means that when you retrieve a document you won’t be getting all of the sub-collections that document might contain. This is in contrast to the way RTDB works. In order to perform data validation, RTDB would require you to write some validation rules. Let’s say you want to validate a lat-long value that’s part of your data structure. You’d need to write a special rule for this. With Firestore, as it supports a few rich data types this is automatically done. Regarding security rules, an important thing to keep in mind if you transition from RTDB do Firestore is that this type of rules do not cascade by default on Firestore, when they do on RTDB. If you’re familiar with Google’s Cloud Datastore you’re probably spotting a few similarities. This happens because Firestore essentially relies on the same technology and infrastructure that’s behind Datastore. If you’re looking for more info be sure to check the official announcement, over at The Firebase Blog . Write production quality Cloud Functions code Thomas Bouldin , Lauren Long Cloud Functions, one of the most popular additions to Firebase, can be triggered by multiple types of events: users authenticating on your platform, events on the Realtime Database or on the newer Firestore, specific Analytics tracking events, storage uploads, even Crashlytics events. More commonly, though, you would trigger Functions through a simple HTTPS invocation. Integrating Cloud Functions with the Hosting feature of Firebase, you can also render dynamic contents, no longer only static Web pages! Using on the Firebase SDK for NodeJS , you can easily write Cloud Functions leveraging NodeJS 8 native support for promises, through async/await keywords. The Firebase SDK also exposes Typescript interfaces that help you write code more efficiently and less error-prone, given its “compile”-time safety. The biggest takeway of this talk was Local Server , a tool for Firebase Cloud Functions that allows you to run those functions on your machine, with no configuration change at all! Local Server also lets you serve hosting pages, so that you can test your Website entirely on your machine, allowing for easier integration and end-to-end tests. And if you want to test a generic Cloud Function that, for example, listens to Firestore or Analytics events, you can start the Local Shell , a fully-fledged Node shell, that loads your functions into the environment, so you can call them with any sample data you want. BigQuery for Analytics Todd Kerpelman BigQuery is super powerful, but it's hard to use. The main mistake people make is assuming the analytics data exported from Firebase into BigQuery is a flat row per event and this is not the case. Every row is a full json object, that contains properties with simple values: strings, integers, etc; but also others with more complex data, like arrays. In this talk, Todd showed us a lot of useful tips and tricks for real-life use cases of BigQuery. The main takeaway was the UNNEST function. With UNNEST you can unwrap an array creating a new row for each element of the array. This is very useful to query events and user properties. For example, say you have an event with multiple parameters and want to filter by only one of them. In this case, if we want to select only those analysis_completed events with type equals to cache our first attempt will be to do something like: SELECT\n  event_dim\n  \nFROM\n  \"my-table\"\nWHERE\n  event_dim.name = \"analysis_completed\" But this returns an error. The issue is that event_dim is not a single object, is an array of objects. And, while some of those objects will have a property called name that will be equals to analysis_completed , event_dim itself doesn’t. That’s why we first need to unwrap the array into individual rows (one per item of the array) before we can query it. The full query we wanted to do will be something like: SELECT\n  event\n  \nFROM\n  \"my-table\",\n  UNNEST(event_dim) AS event,\n  UNNEST(event.params) AS param\nWHERE\n  event.name = \"analysis_completed\"\n  AND param.key = \"type\"\n  AND param.value.string_value = \"cache\" Read more about UNNEST in this great blog post: https://firebase.googleblog.com/2017/03/bigquery-tip-unnest-function.html Overall the Firebase Summit was a great event, and we cannot wait to put to use some of the things we have learnt. Bring on the next summit.", "date": "2017-11-14"},
{"website": "Novoda", "title": "React Native, Flutter, Xamarin: a comparison", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/react-native-flutter-xamarin-a-comparison/", "abstract": "In this article we investigate the current state of things for the more popular iOS and Android cross-platform frameworks. Read on to get a definitive answer on what each framework can and cannot do, and how they compare. The ultimate goal of a cross-platform framework should be to enable a team to write a single codebase and deploy it across multiple platforms, sharing as much code — and thus, the theory goes, time and money — as possible. That way we can provide our partners with help on choosing which tool might best support them to achieve their goals. There are several cross-platform frameworks available, so which are worth comparing to create a fair overview of their current state? The original scope was a technical in-depth investigation of Flutter to gauge its capabilities, pitfalls, and how easy it was to pick up. We did a similar investigation for React Native last year, which was well received. As we started planning we realised that there wasn't an easily digestible, high level overview of the main options on the market today. So we set our sights on creating one. There are many tools in the cross-platform solutions marketplace: Xamarin React Native Flutter Progressive Web Apps (PWA) Kotlin Native J2ObjC/Doppl Ionic2 Cordova/PhoneGap/Titanium Unity ...and several others Of that list, we picked the two most popular and established solutions on the market, Xamarin and React Native, and the new kid on the block, Flutter. Xamarin is one of the oldest cross-platform frameworks available, it's mainly used in enterprise environments and has gathered plenty of success stories over the years. React Native has shown a lot of popularity in recent years, mostly from the ReactJS and web community. Being written in JavaScript has fueled its adoption rate in those circles and it has been employed in some notable products.  Flutter has been promoted heavily by Google since I/O 2017 and has generated significant interest in the development community for its novel approach and interesting tech stack. Investigation methodology It's important to set out a fair way to compare these frameworks, not limiting ourselves to comparing them from a technical standpoint to each other and to native code. We looked at each one in terms of backing and community, future plans, satisfaction level of developers, maturity of the platforms, and simplicity of staffing a team. Of course, we couldn't overlook the technical side. In this case, instead of giving a high level overview, we picked a few areas that we deemed most important when working with these frameworks. The team investigated how well testing is supported, the ease of creating custom UIs and the requirements for platform-specific code and UIs. We also looked at how they deal with native APIs, sensors, hardware and third party libraries and services. This is not intended as a judgement of these technologies — they each have their pros and cons — but rather guidance about which technology might be best for a project. We find this sort of discussion often stems from a gut feeling, a \"shiny object\" impulse, or simply from the desire to see if shaking things up improves a tricky situation. Xamarin is the oldest of the three frameworks at 5 years. It is open source but it's developed behind closed doors and doesn't receive much external input on the code. It doesn't have the best reputation for tooling, stability and performance among mobile developers but .Net enterprise developers seem happy. There is no publicly available Xamarin or Android/iOS specific roadmaps but there is one for Xamarin.Forms . Being the oldest and only paid for framework reflects well on its exhaustive documentation . It also has a ~4,000 users strong Slack community , plenty of meetups and solid support on Stack Overflow. Finding .Net engineers shouldn't be difficult but sourcing .Net engineers with the necessary native mobile knowledge may prove harder. Xamarin provides wrappers to several third party libraries, which are generally a great time saver since they're a bit of a pain to create — especially on iOS where there is no automated tool to generate a starting point. Many of these extensions are collected in the Xamarin Components library, where you can find bindings for Play Services, Firebase, Adobe Analytics and several other widely used services. Most bindings are free but some are paid for, e.g., the video playback component. That said, the vast majority of the wrappers have low user ratings, even the ones provided by Xamarin themselves. Xamarin offers two ways to develop apps. You could decide to create the UI using Xamarin.Forms, a cross-platform port of Windows.Forms, .Net's original UI library (accompanied by WPF over time), or create completely distinct per-platform app modules, in which the only shareable code is the \"plain\" .Net core business logic. Due to the nature of Xamarin, you may have an (almost) entirely shared UI code if you use Xamarin.Forms and don't need to provide platform-specific UIs. Forms has no specific feature to present a different UX based on the platform on which it runs, but has some minor degree of integration with native views if it's necessary — it's possible to inject per-platform native views in an XAML layout. This should be fine for minor, ad-hoc integrations but may have issues with scaling. If the project involves per-platform UIs there is no option to share most UI code and logic, but there is a better ability to interact with native components and APIs. Business logic that can be completely abstracted away from platform details can still be shared. Xamarin can leverage NUnit, an excellent framework for unit tests that is native to C#. Creating mocks is also easy, which makes the testing side at the Component level comfortable. On the other hand, Xamarin code is split into iOS, Android and possibly Forms; this may require duplicating test code for each platform. On the iOS side, it offers a profiler for checking the performance of the application. This tool is not available for Android. Xamarin shines when it comes to UI integration tests. A few years ago, Xamarin bought Calabash, one of the leading UI testing frameworks for mobile, and created the Xamarin test suite. This suite is fully featured, robust, and easy to use. React Native was publicly released as open source on GitHub in 2015 and is the most popular of the three frameworks on Stack Overflow and Google. Despite being a semi-mature framework, there is no clear roadmap, just a dedicated page on GitHub which lists placeholders. It has a large community with (at the time of writing) a ~10,000 user subreddit , ~7000 user Discord chat and strong support on Stack Overflow. Staffing will be easier than Flutter but React Native developers certainly aren't commonplace; finding native mobile developers who also have knowledge of React and JavaScript could be a tall order. React Native supports the majority of Android and iOS native APIs. Due to its large developer community, even if there is no official API available, there are plenty of 3rd party libraries to choose from. In terms of hardware-specific APIs, React Native lacks a number of them, but again, third party libraries are available. React Native doesn't have official APIs for: Bluetooth, biometrics, NFC, camera and sensors. However, it does have APIs for WiFi and location. React Native doesn't really offer a drawing API for custom graphics. You are invited to compose existing components whenever you need a custom solution, but it's not trivial to have fully custom drawing without resorting to platform native code. Building simple UIs is fairly straightforward, given that the development team is familiar with React components. If you need to have different per-platform UIs, React Native can manage distinct implementations for Android and iOS. It is possible, although not trivial, to mix and match React Native UI with native UI in the same screen, and it's also possible to have only a subset of an app's screens created in React Native. Developers have all JavaScript frameworks available for testing at unit level. However, when it comes to UI/automation testing, the situation is not as bright. Although a number of third party libraries are available, there is no clear path to follow. Flutter has been publicly available for less than three years but it only started gaining visibility in the development community around a year ago. Flutter is a technology developed by Google that builds upon Dart and a portable C++ engine to implement a reactive UI framework. Despite Dart not receiving much love in the Stack Overflow developer survey , early blog posts have been positive towards the use of Flutter. Our own exploration of Flutter at Novoda has also been generally positive. There is no well defined roadmap for Flutter but if you look hard enough you can find their current milestones and specific details on what they're working on in this issue discussion . The Flutter team can be found for support in a ~880 user subreddit , ~1200 user Gitter.im , ~740 user Google Group and on Stack Overflow. Staffing a team won't be easy as the platform is so new and Dart is a niche language, but it can be easily picked up by Java and Kotlin developers. Upskilling an existing mobile team should be easier for Android developers as Flutter can integrate with IntelliJ and Android Studio. iOS developers would need to get used to the language and tooling, which might be a bit more unfamiliar. Apart from Bluetooth and NFC payments, most hardware and sensor APIs are supported, but some of them are currently in a very early stage of development. A lot of plugins already exist, but some areas are more in flux than others. For example, (at the time of writing) inline video support and dynamic inline maps are still in development, but full screen and static maps are supported. With performant, custom UI being the main focus for Flutter, the framework is geared to providing a deeply customised UI, which doesn't feel \"alien\" to users of a native platform. Flutter provides a full UI stack implementation that does not use native UI components, to be able to allow the required degree of customisation, portability and performance. If you want a custom UI for each platform you need to detect the current device and decide which layout to build, which could become extremely tedious. It's important to note that it is possible to embed a FlutterView very easily in a native Android layout, to mix it with native UI in the same screen; this is possible on iOS as well, but this has not yet been documented. When it comes to testing, it becomes clear that Flutter is a new framework. Dart offers an excellent unit testing framework which can be utilised and Flutter provides you with a great option for testing the widgets on a headless runtime, at unit test speeds. However, running integration tests is somewhat complicated. Lastly, Flutter allows you to run performance tests with their flutter_driver package. So, where are we at with the current state of cross-platform frameworks in native mobile development? Xamarin certainly seems to have come a long way since its early stages and has a lot of success stories to support it. React Native has the following of a large community and a few good success stories behind it, such as AirBnb, SoundCloud Pulse and Artsy. Flutter shows some interesting potential, also easily enabling embedding as part of a native UI so it can be added to existing apps and grow from the inside out. They could all be used in some way to create an app, that's for sure, but could they be used viably for a large scale commercial app? Definitely not yet. Most large-scale success stories for React Native see it being used to implement well defined features and augment a solid base of native code. What kind of app would you use them for then? Probably short term apps for movie releases, or one-off promotions — Flutter's Hamilton app is a great example. The possibility to apply some of these frameworks for quick prototyping could also be interesting. Conclusion Cross-platform frameworks are not a silver bullet . They only work well when used with full awareness of their limitations and strengths, in a context that makes them shine. The potential difficulties in staffing cross-platform experienced teams is just one factor to consider and shouldn't be overlooked. The situations in which these tools provide the least value are those in which they are adopted with the misconception that using this framework will fix problems that stem from process or staffing issues . The many success stories of each of the platforms show that they can prove valuable and effective in tackling specific problems . These instances are usually when there is a competent team working on a low complexity project with no need to reach outside the \"safe space\" defined by the framework boundaries into native APIs, third party libraries or hardware-driven features. Are you interested in building an app for your product? Want to get more of our insights on this topic? Contact us to chat mobile and let us help you pick the best technology to achieve your goals. They told you it would be impossible to choose a cross-platform framework from a diagram — they were wrong If cross platform is the answer, make sure you’re asking the right question", "date": "2018-01-25"},
{"website": "Novoda", "title": "They told you it would be impossible to choose a cross-platform framework from a diagram — they were wrong", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/they-told-you-it-would-be-impossible-to-choose-a-cross-platform-framework-from-a-diagram-they-were-wrong/", "abstract": "In this article, we visualise the current state of things for the more popular iOS and Android cross-platform frameworks. Read on to see a definitive answer on what each framework can and cannot do, and how they compare. Choosing a cross-platform SDK because it is supposed to increase speed or decrease costs is very likely to be a bad idea. What you should aim to get out of a cross-platform team is value for your users and, by extension, to your business. To obtain value, you need to be aware of your teams' processes and state of mind, and still be aware of what kind of value a framework can bring, and in which conditions. Here we are presenting one way to help inform your decision making when attempting to create or pivot, a mobile product. It is in no way a holy grail or one size fits all answer. For further background information or if you want to find out more details, check out some of our other posts for a more in-depth discussion on the issue of cross-platform frameworks: If cross platform is the answer, make sure you’re asking the right question React Native, Flutter, Xamarin: a comparison We decided to make this cross-platform choice diagram to try and explain the thought process that should happen with this decision point. It's not a process to do on a whim and we hope the diagram sheds light on potential areas you may not have considered in your deliberations. Download as PDF The diagram is free to download, print, share and use — the diagram is licensed under the Creative Commons Attribution-ShareAlike 4.0 International licence. Just don’t take out the Novoda logo, basically, and you’ll be alright. We hope it'll spark discussions and help you have an informed choice. We’ve highlighted some areas of interest that we think may be useful when looking into cross-platform frameworks like Xamarin, Flutter and React Native. Conclusions Choosing a solution for your mobile products is a complex task. Involving the evaluation of cross-platform frameworks means considering a lot of tools and a lot of potential programming languages, ecosystems and talent pools. What you should aim to get out of a cross-platform team is value for your users and therefore value for your business. To obtain value, you need to be aware of your teams' processes and state of mind, and still be aware of what kind of value a framework can bring, and in which conditions. We hope this diagram helps you towards that goal.", "date": "2018-02-22"},
{"website": "Novoda", "title": "Banishing Burnout", "author": ["Niamh Power (iOS & Android Software Crafter)"], "link": "https://blog.novoda.com/banishing-burnout/", "abstract": "In this blog post, I’m going to discuss how you can best tackle burnout as both an employee and employer. I will touch upon how Novoda approaches these challenges, my own personal experiences, as well as advice from members of the developer community. I hope these thoughts can help you if you are struggling; feel free to reach out to me on Twitter if you need any advice. Last November, Novoda hosted their first annual ‘un-conference’ in Lisbon, Portugal. The idea of an un-conference is that potential speakers can submit a topic and attendees vote on what they want to hear about on the day. The three formats available were talk, workshop, or discussion. One of the key themes of the day that emerged was mental health in the workplace. Burnout: physical or mental collapse caused by overwork or stress. Before attending the unconference, I had been taking on more and more “extra-curricular” activities at work, and with deadlines approaching on my project, I was really struggling with stress. The resulting discussions that took place regarding mental health were fascinating, and really humbling to hear how many other people struggle with this in their day to day lives. It was also incredibly relieving to know that I wasn’t the only one having stress related issues, which aided me in finding a resolution. Mental health is still a somewhat taboo subject and conversations like the ones we had at the unconference should happen more often. With this blogpost I hope to kick-start a discussion on workplace stress with the hope that it becomes a topic we can begin to approach with ease, both within and outside of Novoda. Following the unconference, I made a particular effort to try and understand the steps I take to battle burnout, and I found there are three points to consider when approaching your own mental health. Get these right and you’re in a much stronger position to be able to jump any hurdles that come your way. Communicate If you can feel yourself getting burnt out or stressed my first piece of advice would be to talk to your employer. Initiating that first point of contact is a crucial first step and in my case, I was completely supported and only received offers of help. This is only possible in certain companies however, so it depends on your circumstance. Having a healthy support network of friends and family that you trust to be able to talk to about any struggles you are having can be really beneficial and can help if talking at work is problematic. I often found just chatting about my day to day stresses very therapeutic, and ensured I didn’t overthink, or lose sleep, over potentially menial problems. Trying to get involved in more extra curricular activities outside of work can also help, as this gives fresh perspectives, and a broader friendship circle. Switch off Switching off and allowing yourself proper resting time is vital to maintaining a healthy work-life balance. I find one of the key techniques for allowing yourself to disconnect from work is to set a strict time after which you don’t touch your laptop. This is especially important if you work remotely, as it is far too easy to continue working into the night. One technique I employ is to have separate work profiles on my machine which prevents any unwanted notifications after work time. Enabling ‘alarms only’ mode on my phone after work and before I start (6pm - 8am for me) every day has also been particularly helpful. Furthermore, taking up a creative hobby such as painting has been incredibly beneficial to my personal mental health. I would advocate this to anyone struggling with burn-out. In the past, I have also known a few people to take an extended break from the day-to-day grind of work. It doesn’t even need to be a holiday abroad, just a change of focus can really help “cleanse” your mind. The opportunity to do this is of course incredibly fortunate, and many don’t have this chance. Prioritise your sleep According to the Mental Health Foundation, a good amount of sleep is vital to maintaining a healthy mind . This is difficult if you have a large commute or other commitments so it is a case of balancing this carefully. One key purchase can be a seasonal affective disorder lamp. This is a light you can set to gradually turn off in the evenings and turn on in the mornings. My personal experience is that this results in you waking up much more naturally and also aids in falling asleep if this is something you struggle with. As part of this blogpost, I put out a call to other developers in the community for their experiences and advice on mental health and burn out. The response was fantastic. Thanks to all those who did One of the key points of advice I found was to have a healthy life outside of work. Ensuring you exercise and eat well gives you a great foundation to work on your mental health. Taking value in your weekends is also imperative - getting away from home every now and then can do wonders for feeling cabin feverish and can help stop the temptation to work outside of working hours. Within your working environment, reminding yourself that at the end of the day, your job is indeed just a job can help you put everything into perspective. Your mental wellbeing is your priority. If the culture is toxic or you’re feeling undue pressure don’t be afraid to start looking at other options if you are able to. Additionally, a common snagging point is when developers start moving to more of a senior role. Finding the balance between being more of a mentor / manager and writing code is really difficult, so it is really important to have clear communication with your manager as to what the expectations are. Don’t feel afraid to discuss possible improvements with more senior figures if you’re struggling with the work load or are unsure of the next step to take. It takes time, but being able to separate issues at work so you aren’t staying up all night thinking about them is a really vital skill. There are always going to be difficulties, but it is more how you approach them and deal with them mentally that is important. Getting a grasp on good time management can be really helpful too. By time boxing your work, you can give yourself more discipline and give realistic expectations to your colleagues. This of course needn’t apply to work only. Other issues that could be troubling you, like personal finances, can be addressed by setting aside some dedicated time for them. You will often find out that a brief but focused session can resolve concerns that have been looming over you for months. In addition to this, creating a list each morning of everything you would like to achieve by the end of the day, and then being able to pick this up the next day can help you to visualise the scale of your work, which is often smaller than you might think. This also benefits your evenings, as less of your mental capacity is spent worrying about what you need to do tomorrow. For employers, having a friendly and open HR department works wonders. Advocating for a culture of openness and support can ensure employees feel comfortable opening up when they’re struggling. At Novoda, we are planning on meeting fortnightly to discuss techniques on the best ways for the company to tackle any problems people may be facing. In all of our offices, we have break-out areas with more relaxed seating, games consoles and an arcade machine. This helps to foster an environment that encourages people to take breaks from their work and relax. By making it clear that mental health is as important as physical health, and that taking days off when everything’s a bit overwhelming really helps. When employees are encouraged to work late and there is little tolerance for taking time off, burn-out is the result, as well as deeply unhappy workers. This also can lead to a toxic culture that is far more competitive than healthy. Take these steps, and you’ll be well on your way to creating a company culture that is inclusive and open and attracts a fantastic group of people. Personally, I only recognised I was struggling after I heard of other's experiences at the un-conference. This gave me the confidence to talk to those at work who could help, and it’s been great to have that line of communication open for whenever I need it. My main point of advice if you are struggling with any mental health issue, would be to talk to someone close to you. Talking to someone you trust makes it real and allows you to start making a plan to tackle it head-on. I hope this blog has given you some confidence to take this step or to make the moves to avoid any problems in the future. Finally, the points I’ve raised in this blog talk to those seeking ways to help general day to day stresses, which can ease the root causes of mental illnesses. This doesn’t apply to all, and sometimes counselling and / or medication can be the best way to help. If you aren’t sure, be sure to go and speak to your doctor, or a healthcare professional and they will be able to point you in the right direction. This page from the NHS in the UK provides some really helpful phone lines, that can offer more professional advice. For those outside of the UK, there are many charities that provide help and advice, and also some fantastic online resources, such as BetterHelp for more relevant advice. It is particularly important to note that more than one approach could apply to each person, especially if they are experiencing various different symptoms. For example you might benefit from medical treatment but could also need to improve your lifestyle, exercise more, sleep more, or just build better habits. Don’t treat this as a chore (“I already did one thing, therefore I can ignore the remaining difficulties I’m facing”). Use every possible means of improving your life quality and you’ll be on your way to thrive!", "date": "2018-03-13"},
{"website": "Novoda", "title": "Retain returning users with Android’s app backup (part 2)", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/android-backup-and-restore-returning-users-part-2/", "abstract": "In the previous post we saw why one should care about backing up and restoring their apps’ data, and one possible way of doing it. In this second and last part, we’ll see an easier way to backup data, and how we can ease migrations/reinstalls even more by preserving login information, too. Auto Backup If you think that implementing Key/Value Backup is a bit of a pain, with the whole API key and BackupAgent thing to do, there’s great news for you. What if I told you that your app can get backups without any effort? Here’s the thing, on Android 6.0 and later (API 23) the crafty team at Google added the tremendously underrated Auto Backup APIs. Now, calling it an API is a bit exaggerated in my opinion, since there’s barely some XML involved, and nothing else. No API keys, no code, it just works. What is this wondrous Auto Backup then? Simply put, it means that if you target API 23 or later your app’s private data folder is automatically backed up to the user’s Google Drive, in a separate storage container that can hold up to 25MB of arbitrary data . Best of all, it doesn’t even count against the user’s Drive storage quota. Implementing Auto Backup Now that I have gotten the easy stuff out, you’ll probably be wondering that’s cool, but is there anything I need to do then? Do I need to care about anything? The answer is — unfortunately for my idyllic vision of zero-effort backup outlined above — yes, there is something you should do . First of all, while Auto Backup is smart enough to avoid backing up cache folders, you likely have something you don’t want to back up in your app’s data folder. Examples would be databases, downloaded assets, install or device-specific IDs, sensitive information about the user, and FCM tokens. For the latter there is a very helpful Lint warning to remind you to add exclusions, so at least that’s covered, but the rest is on you. Many apps [citation needed] target Android 23+ and have stuff backed up that should not be. Don’t be one of them! You will save yourself some serious headaches. How do you do decide what to back up then? The default behaviour on Marshmallow and later for apps that target 23+ is to back everything up, as we mentioned. If you wanted to opt-out of auto-backup (don’t!), you could set android:allowBackup=\"false\" in your app manifest. If you want to be a good dev, on the other hand, you would specify an XML resource for the android:fullBackupContent attribute. An Auto Backup configuration XML is very simple to write. It consists of a list of inclusions, or of exclusions. If you specify inclusions, everything else is excluded automatically. If you specify exclusions, whatever is not mentioned explicitly is included. Depending on what you store in your app’s data folder, either one might work better for you. You can also decide that you need more flexibility, in which case you can set android:fullBackupOnly=\"true\" in the app's Manifest and implement a BackupAgent that will be invoked to decide how to behave when restoring. At that point you can easily do things such as excluding a device specific identifier, by just clearing its value in onRestoreFinished() instead of having it in a separate file and excluding the whole file. Be warned though, there are some serious limitations when running a BackupManager — it will run in a so called \"Restricted Mode\" in which content providers are not started, you don't have your custom Application subclass, and so on. See the documentation for further details. Auto Backup will by default not restore a backup that was created from a newer version of your app, because it assumes it may not work for you. This might be a problem if your user is migrating to a new device that has an older version of your app preinstalled, because it would mean that nothing is restored. If you are confident you can handle forward compatibility, you can use android:restoreAnyVersion=\"true\" to override the behaviour. While Auto Backup is super convenient, it is not as flexible as Key/Value Backup unless you implement a BackupManager . For one, it cannot handle migrations or other custom logic. If that’s what you need, you’re better off with Key/Value Backup. Lastly, remember that Auto Backup is only available on Android 6.0 and later, so if your minimum SDK level is below 23, you’re leaving all those users out in the cold. What to use then? If there’s one thing you should take out of this blog post, is that there is really no reason why you shouldn’t have backup support in your app . Whether you end up using Key/Value Backup or Auto Backup, is really up to your use case and your app. This summary should help you make a choice: Key/Value Backup Auto Backup minSdkVersion 8 — Froyo minSdkVersion 23 — Marshmallow 5MB max 25MB max Requires API key (for Android Backup Service) No API key required Requires code implementation ( BackupAgent subclass) No code required (configure via XML) Supports migrations/custom logic Doesn’t support custom logic If you are developing a game, you also have the option of using the Google Play Games APIs to persist your game saves across installs using Saved Games . Remember that enabling Auto Backup will also enable adb backup , which will include or exclude files based on the XML configuration. Also note that, starting with Android 8.0, Key/Value Backup will be used by adb backup too and will enable it implicitly (but it's not the case before Oreo). One more thing If you remember, at the beginning I mentioned that users hate to have to log in again to all apps when they migrate to a new device. That’s something Backup and Restore cannot really help with, unfortunately. But that is not game over! There’s other APIs one can use to keep people signed in and happy, reducing friction and making the UX better. For example, there is Smart Lock For Passwords which I wrote about a couple of years ago. Smart Lock for Passwords — yeah, I know, long name! — allows you to sign users in with a seamless zero-click experience by using auth tokens. If you only rely on username and password, it can still offer a one-tap sign in flow which greatly simplifies things for users anyway. If you cannot implement Smart Lock for Passwords’ great signin flows, and your users are running Android 8.0 or later, then you can also take advantage of Auto-fill , which again requires almost zero work, except from providing it some hints as to what data type your form accepts. Lastly, in the \"new device\" scenario, if you have an account backed by the system AccountManager , then you can use the brand new Account Transfer API from Play Services, that allows you to transfer an account from an old to a new device during the out-of-the-box wizard. This is it. Now go and make sure that the next time I get a new phone I don’t have to re-configure any apps 💛 Many thanks to the Android Backup & Restore team at Google for their help and guidance in writing these articles. Thanks to Wojtek Kaliciński‏ for pointing out an inaccuracy in the Auto Back up section.", "date": "2017-11-23"},
{"website": "Novoda", "title": "Android P Slices: the missing documentation — part 2", "author": ["Sebastiano Poggi (Android GDE)"], "link": "https://blog.novoda.com/android-p-slices-missing-documentation-part-2/", "abstract": "We've been looking in-depth into what Android P Slice s are and how you can write an app to host them. In the second part of this series we'll be exploring the other side of the coin and see what a Slice is made of, and how to create a SliceProvider to expose your Slice s to other apps. In the first part of this series we have seen what Slice s are, what is their possible use, and how you can host slices from other apps into your own. If you haven't done it yet, you should go read the first part of the series, as this article assumes you're familiar with its contents: Android P Slices: the missing documentation — part 1 Where do Slice s come from? As we have seen in the first part of this series, slices are emitted by an app that contains a SliceProvider . A SliceProvider is nothing but a good old ContentProvider , although you would never be able to tell by its API: public abstract class SliceProvider extends ContentProviderWrapper {\n\n  public void attachInfo(Context context, ProviderInfo info) { /* ... */ }\n\n  public abstract boolean onCreateSliceProvider();\n\n  public abstract Slice onBindSlice(Uri sliceUri);\n\n  public void onSlicePinned(Uri sliceUri) {\n  }\n\n  public void onSliceUnpinned(Uri sliceUri) {\n  }\n\n  public @NonNull Uri onMapIntentToUri(Intent intent) {\n      throw new UnsupportedOperationException(\n              \"This provider has not implemented intent to uri mapping\");\n  }\n\n  // ...\n} A SliceProvider implementer only needs to concern itself with emitting slices and dealing with pinning and unpinning, whereas the underlying code will take care of handling URI permissions, ensuring the caller package has received permission from the user to show slices from it, etc. Since it's a content provider, a SliceProvider has to be declared in the AndroidManifest.xml as such: <provider\n    android:name=\"com.android.mypkg.MySliceProvider\"\n    android:authorities=\"com.android.mypkg\" /> Note that according to the documentation, the provider's authority is supposed to match the application package in which the provider is located, although I am not sure if that is a strict requirement or is just a recommendation. The code in the slices-core compat package doesn't seem to enforce any such requirement, in any case. You can also add an <intent-filter> to the manifest entry for the provider if you want host apps to be able to bind to slices by intent in addition to by URI; in that case you need to override onMapIntentToUri() in your SliceProvider implementation, and provide the mapping from intents to URIs. How it's made: slices The simplest scenario is the one in which the host app has the user's permission to host slices from a provider and it calls SliceManager.bindSlice() : In this case, the SliceProvider will initialise itself, call onCreateSliceProvider() to let the implementation initialise itself, and then immediately call onBindSlice() . This method is tasked with returning a Slice for the provided URI, which is then passed to the host application. As mentioned earlier, all other implementation details are hidden away from a provider implementer: enforcing URI permissions, verifying that the user has given explicit permissions to the host app to show a certain other app's slices, and so on. Provider implementers have thus no control over things like the appearance of the slice(s) that is used to request the user permission. That special slice is instead created by the slice framework, either in androidx.slice.compat.SliceProviderCompat (if running on a pre-P device) or in android.app.slice.SliceProvider if slices are natively supported by the OS. As you can see from the flow diagram above, if the host app is using bindSlice() it will have to re-call it once the permission has been granted. There is no API for checking whether a user has granted the permission, unfortunately. At this point, the permission check will succeed in the framework, and the provider's onBindSlice() will be called, emitting the actual content that will then be passed to the host app. Pinning slices In addition to the basic scenario where slices are directly bound to, there is a more interesting scenario in which a slice is pinned by a host on a provider. Pinning a slice only means that to indicate to a provider that there are one or more hosts that are interested in the data of a slice, and that it should notify of updates to the underlying data using ContentResolver.notifyChange() in a timely manner. Pinning a slice does not mean that you get updates to its data, as slices are static snapshots, but rather only signals the interest into a specific slice to its provider, so that it can for example subscribe to underlying data changes on its side, and then publish fresh data when observed via a SliceLiveData or a callback. This in practice means that providers that want to support pinning need to implement onSlicePinned() and onSliceUnpinned() and, in there, register/unregister observers to the data which makes up the pinned slice, so that when it changes it can notify host apps of the updates. The slices framework will transparently take care of re-binding updated slices on the host side and calling the registered listener(s) — or LiveData<Slice> . As mentioned previously, pinning is implicit on the host side whenever a listener is registered, or when using SliceLiveData to observe a slice. Since pinning does not survive reboots, hosts are in charge of persisting their pinned slices however they see fit and re-pin them on BOOT_COMPLETED , or whenever they need to. In the current sources for the compat slices libraries there are several TODOs in this area, including one pointing at providing a notifyChange(Uri, Slice) which should simplify things considerably both for developers and for the framework. Loading content for slices A SliceProvider needs to return a slice as quickly as possible; this means that any blocking disk or network I/O to create a slice is strictly forbidden. Providers should whenever possible keep a memory cache of their pinned slices so that they can serve them quickly. If a slice contains data that needs to be loaded from disk or from the network, it should omit it from the initial slice and instead mark each pending item as isLoading (all setters have overloads for it) and begin loading it in the background. Once the loading is completed, it should re-compile the slice and notifyChange() so that hosts can request a new copy of the slice with all the data. Anatomy of a Slice Slices are static tree-like data structures. Everything inside of a slice is either a SliceItem , or a property (hints, specs, …). When a (sub)slice is constructed internally, it can be assigned: A SliceSpec which says what kind of slice it is (currently, it's BASIC , LIST or MESSAGING ) A series of \"hints\", which are strings describing slice properties and subtypes, so that the renderers know what to inflate and how to handle it A series of subitems, stored as a list of SliceItem s SliceItem s can have a series of hints just like slices SliceItem s have a format, expressed as a SliceType , which is one of: FORMAT_SLICE , FORMAT_TEXT , FORMAT_IMAGE , FORMAT_ACTION , FORMAT_INT , FORMAT_TIMESTAMP , FORMAT_REMOTE_INPUT Slice items can have a subtype which provides additional data beyond the SliceItem 's format All contents of a slice except for hints and specs are slice items A slice has a series of convenience \"typed\" adders for subitems (actions, text, etc), which are internally represented as a SliceItem with a specific FORMAT_* type When it comes to creating a slice, there is no direct API on Slice itself. Instead, a slice can be constructed only from a TemplateSliceBuilder , an abstract class that only has two concrete \"top-level\" implementations: ListBuilder and GridBuilder . There is another implementation, MessagingSliceBuilder , but that is restricted to the support library group and is not accessible to users of the support libraries. If you look at the Slice&Dice sample app on GitHub, you will find an example of both a host and a provider apps; they both provide an example of what can be currently achieved with slices, with a bunch of workarounds for present issues and sample code for three different slices. It all starts with a list The most common scenario, and the one you’ll find in the sample app, is using a ListBuilder to create a slice that has one or more rows of content. A list can contain several kinds of items: If you want to have a look at how all the various items are rendered, you can grab the demo app and select the Demo slice . \"Regular\" list row A \"regular\" list row represents the most plain and common item type in a slice list. They are composed of: A title item A title item can be: a timestamp, an icon, or an action A title and a subtitle text An end item An end item can be: a timestamp, an icon, or an action A primary action (triggered when clicking the row) All icons in a \"regular\" row are forcefully tinted when rendered in a host, as mentioned in the previous part of the series, so you should only use monochromatic images. Header row A header row is a specialised version of a \"regular\" row, and all lists have one. You can setHeaderRow() explicitly, but if you don't, the first item in the list will become a header, regardless. A header can have: A title and a subtitle text A summary subtitle — replaces the regular subtitle when rendered in small mode A primary action — displayed as an end action (headers are not clickable) And they appear to all be optional, although it may just be that the HeaderBuilder implementation used under the hood is lacking proper validation. Header rows look much like regular rows (they end up both being instances of RowView when inflated in a SliceView ), but they have slightly larger text, and they don't have title or end items. Headers are not supposed to scroll away, and they should be representative of the activity the data originates from. All icons in a header row are forcefully tinted when rendered in a host, as mentioned in the previous part of the series, so you should only use monochromatic images. Grid row A grid row is the most complex type of row as it contains a set of cells (with a maximum of 5). They cells will not wrap on multiple lines and will not become a carousel if there are too many of them. Grid rows contain: A set of cells The maximum number of cells is 5, all subsequent ones are ignored when rendering A cell contains: An image A title text and a \"normal\" text — defined as just text A content intent, which is the same of an action but with a different name — not sure why this is The cell image has one of these ImageMode s: LARGE_IMAGE , SMALL_IMAGE , ICON_IMAGE Icon images are tinted and small A \"see more\" cell, or a “see more” action You can't have them both The \"see more\" cell counts towards the maximum of 5 cells, although it's not documented The \"see more\" cell looks and works exactly like any other cell, so I am not entirely sure why you would need one A primary action The way a grid is rendered depends a lot on the contents. The height of the items in the row, in particular, depends on a few factors: Main condition Secondary condition Tertiary condition Cell height All cells have only images Only one cell Mode is SMALL 120dp Mode is not SMALL 140dp More than one cell All images are icons 60dp Images are not all icons 86dp Cells don't only have images Cell has two text lines and mode is not SMALL Cell has image 140dp Cell has no image 60dp Cell has less than two text lines or mode is SMALL All images are icons 60dp Images are not all icons 120dp Range row A range row is a specialised kind of row, which has a progress bar to represent a value in a range. A Range row contains: A title text A maximum value — the minimum is fixed at zero just like with a ProgressBar A value, which must be in the [0, maximum value] interval Input Range row An Input Range row is the interactive counterpart to a Range row, which has a SeekBar instead of a ProgressBar . An Item Range row has: A title text A maximum value — the minimum is fixed at zero just like with a ProgressBar A value, which must be in the [0, maximum value] interval An action which is triggered whenever the user changes the SeekBar value There is currently a bug for which this is invoked whenever assigning a slice that contains the Input Range to a SliceView , which makes the Input Range unusable ( #75004842 ) See More row/See More action A See More row is just a \"regular\" row with an additional hint marking it as See More. There is currently no difference in rendering or handling for See More rows from any other \"regular\" row. As an alternative to a See More row you can specify a See More action, which is wrapped in a subslice under the hood. Action A list slice can have one or more actions associated with it. An action has: A PendingIntent , which is the actual action wrapped by the object An icon, with a content description A title Two flags: Toggle — to signify that this action is an on/off switch Checked — used with Toggle, sets the status of the action A priority — a positive integer where the lowest priority ranks highest in sorting Actions are shown: In an \"actions row\" when the view is in large or small mode and there are two or more actions Actions in the action row are shows as clickable images, optionally tinted, no text Actions are supposed to be sorted by priority but, as of the alpha1 implementation, they're not (yet) The actions row is supposed to be shown at the bottom of the slice contents At the bottom of the header row if the view is in large mode At least, that's the theory. In testing, actions are not shown in any of the three modes ( #74889520 ). What I think is still missing While slices are an exciting new territory to explore, I feel like there's many things they still need to get in their API and implementation to be truly useful and not just a pipe dream, hindered by a restrictive implementation. The main missing opportunity I can see is the ability to provide \"public\" slices — slices that every app can attach to without having to ask for permissions to do so. If the slice doesn't expose any sensitive data or behaviour, I don't really see a reason to require an explicit user permission to bind to it. For example, a weather app that likely doesn't expose any private information through a \"London weather\" slice shouldn't require a permission to provide that slice. On the other hand, a \"Weather at the user's location\" might require a permission since it involves the position permission. It's a tricky one to handle, granted, but the potential payoff is great. ( #76021782 ) The APIs need some polishing too, in particular for Kotlin users. The current builder APIs don't validate the contents of slices, allowing providers to craft slices which can make a host app crash. ( #76021783 ). Besides, while the presence of a consumer-based API for builders is welcome, a Kotlin-friendly API which simplifies the slice creation would be very welcome on top of the regular Java one. ( #76031961 ) On the rendering and customizability side, I would like to see a few things in the next previews: Improved grids support ( #74889524 , #76021784 ) Allow to control the style and size of cells Allow to have more than two lines of text Allow more than 5 items, and show as carousel if they don't fit on one line Visually distinguish the \"see more\" cells from normal cells Allow slices to specify their items' background colours, or at least, define styles that can be overridden for slice items on the SliceView side ( #74917012 ) Make list headers more visually distinct from normal rows ( #74917014 ) Allow for images in start/end items not to be tinted ( #76031962 ) Add new types of slice items ( #76021785 ) E.g., an item that only contains an image Allow slice items to define their Gravity ( #76031963 ) E.g., should an item be start-, or end-aligned? Or should it fill all the row? Allow action listeners to hijack actions, not just to listen to them but also to handle things by themselves ( #76031964 ) E.g., if the app uses Custom Tabs, it should be able to display web content there instead of in the system browser Conclusions Slices are an extremely interesting new API, full of potential. Google keeping mum about the feature and not saying much about it seems to hint to them sandbagging in anticipation for some big reveal in DP2, which should drop during Google I/O in early May. It's still an immature API and its implementation is quite buggy, but those are things that are bound to get sorted out before the final P release. Having a compatibility library also means the API can evolve separately from the system's implementation, like Fragments do nowadays. In fact, one might wonder why even bothering to release a platform version of slices, but there might be some piece of the puzzle that is still missing to make sense of it. The inaccessible Messaging slice is very intriguing, as one can imagine scenarios in which messaging chats can be integrated in other apps. For example, imagine a helpdesk SDK or Facebook messenger providing you with a way to integrate their services in your app directly. I can envision, with some tweaks to slices, that in the future apps will be offering all sorts of integrated UI to other apps. A social sign-in button, a cart and checkout for e-commerce, and integrations into Assistant are just a few of the many things one could do with Slices, assuming they evolved a bit. If you are curious about slices and want to get your hands dirty, head over to the Slice&Dice sample app repository and start playing around with the code! Many thanks once again to all those that helped me with this post!", "date": "2018-03-22"},
{"website": "Novoda", "title": "try!Swift Japan - Part 1", "author": ["Berta Devant (IOS Developer)"], "link": "https://blog.novoda.com/try-swift-japan-part-1/", "abstract": "At the beginning of March I had the pleasure of attending try!Swift Tokyo 2018 in Japan ⛩. If you have never attended a try!Swift conferences , they are an immersive community gathering with curated talks by the organizer and top of the line workshops the days before and after the conferences. They hold 3 conferences every year: one in New York city, one in Bangalore and one in Tokyo. I had the pleasure of attending try!Swift New York in previous years,  so I knew the format and what I could expect from the conference - or I thought I did! In fact,  try!Swift Tokyo managed to exceed all of those expectations. While I knew the talks were going to be good, since the speaker line-up looked really impressive, I highly underestimated the effect that being in Tokyo would have on the conference. And don’t worry if you were unable to go, I’ve highlighted a few of my favorites talks later in part 2 of this post ! On the first day we got to visit Tokyo with some conference attendees Another Point of View Japan is an incredible country to experience, full of warm people, incredible food and amazing design. And the conference was just as impressive: the organisation was faultless, the food and drink was delicious, and the talks, speakers and attendees were all excellent. 800 people attended try!Swift Tokyo 2018, and out of those 800, only 200 were foreigners like me. That was an incredible experience, because even though most of the time I’d be having a conversation in English with someone from Japan, and translation could be a bit difficult, it did help me to realise that the problems they encounter in Japan can be a bit different from ours. There was a tatami (a type of mat used as a flooring material in traditional Japanese-style rooms) zone to rest and watch the talks - what other conference has that? A couple of things that stood out to me was the importance of Networks and API, and how subtly different their design approach to apps was. As developers, we know we need to build robust network layers to be able to handle high traffic and poor internet conditions. But apparently, in Japan people do not use Wifi that much. Whether they’re living in the city or not, they expect fast loading times over the cell network.  Also their design approach was a bit different. Most of the companies I talked to were keen to implement new products and tech in their app, even if those were initially out of the scope of their business model. This is quite different to the European approach, where the focus is on delivering what’s in the brief. Take for example LINE , one of the top apps in Japan and one of the sponsors of the conference. LINE started as a messaging app, but now doubles as a camera and as asocial network (with a recent livestream feature).  You can also use it to pay for things online and in enabled stores, and while I was there they showed us a prototype of an IOT device for the home that they’d developed in-house, similar to Alexa or Google Home. The speakers are designed to look like two of LINE’s most well-recognised characters from the app This translated into some very interesting tech talks about deep knowledge of Swift Intermediate Language (SIL) and/or how to use Machine Learning to optimize manga compression. How to get shy devs at your booth? Ask them about frameworks and architecture The Talks On the first day we were given a translation earphone box for the rest of the conference, since some talks were going to be given in English and some in Japanese. The conference had set up real-time translators, so you could sit down and watch the talk while listening to real-time translations in English. It was a really good set-up and it worked really well, but it was sometimes a couple seconds slower than the speaker and some metaphors and adjectives got lost in translation (that meant some of the notes from Japanese talks were a little shorter). You can see the full talk line-up on the try!Swift page and the videos of the entire conference will be uploaded to their youtube channel and if you want to see my opinion and notes on some of the talks checkout the second part of this post Conclusion try!Swift was an incredible conference and experience, it allowed me to meet loads of new people - and in particular, those from a culture and language completely different than mine. And not to mention the fact that  I also heard some amazing talks, that will help any future project that I am assigned. Check out part 2 of this post to see my notes on some of the talks So, if you have the chance of going next year I would highly recommend you take the trip, keep and open mind and bring some business cards with you 😉 We traded flavours of Kit Kats for stickers And met some amazing people!", "date": "2018-03-30"},
{"website": "Novoda", "title": "Introduction to Redux in Flutter", "author": ["Xavi Rigau"], "link": "https://blog.novoda.com/introduction-to-redux-in-flutter/", "abstract": "Redux is a unidirectional data flow architecture that makes it easy to develop, maintain and test applications. In this post I'll explain how you can start writing mobile apps with Flutter using the Redux architecture. It was only last month that Google announced Flutter graduated to the beta stage and yet since then the interest for this framework has grown very rapidly. Flutter is a really interesting piece of technology that can prove very useful in many situations both for indie developers as well as software companies. It is currently being used by Google which makes for a promising future. Flutter allows for very quick iterations, it's developer-friendly and it's multi-platform. Let’s walk through the high level architecture of Flutter and then move onto Redux. Flutter Widgets In Flutter, every UI component is a Widget , you compose your whole application UI with Widget s that contain other Widget s, even your application class is a Widget . Widgets can either be a StatelessWidget or a StatefulWidget : Flutter StatelessWidget StatelessWidget is a very simple Widget that doesn't have any mutable state, therefore it needs to be recreated with different parameters in order to display different data. An example of a StatelessWidget could be a row in a to-do list: this Widget would get the to-do text and the “done” flag as constructor parameters and then display them. Changing the to-do text, or the done flag, requires you to create another StatelessWidget . Flutter StatefulWidget StatefulWidget is useful when building a UI based on some mutable state. In this case the Widget gets recreated every time the state is mutated, therefore reflecting the state change in its Widget tree. The StatefulWidget has to create State objects that will hold the mutable state, in addition to creating the Widget tree that StatelessWidget s also have. An example of a StatefulWidget would be the container for the to-do list items: this container list would extend StatefulWidget and it would create a ToDoState . This ToDoState is where the list of to-dos would live and where we'd create the Widget tree (i.e. ListView ). Once there's a user action (when a to-do gets added, removed, etc.) then we'd update the list using the setState in the State object, which would rebuild the Widget tree, showing the added (or removed) item. This makes for a nice separation of things that change vs. things that do not. But it also has its downsides: When there's state that has to be shared in multiple pages, it needs to sit in the app Widget and then it has to be passed down to each screen's Widget tree, requiring boilerplate code. Multiple Widgets become tightly coupled when there's a user action that has to modify the shared state because the actions have to be communicated up in the Widget tree. Tightly coupled Widget s aren’t reusable and it can be hard to modify the Widget tree if you're planning to make a UI change. To counteract and avoid some of these downsides, we can turn to Redux. Redux Redux is an architecture with a unidirectional data flow that makes it easy to develop applications that are easy to test and maintain. In Redux there's a Store which holds a State object that represents the state of the whole application. Every application event (either from the user or external) is represented as an Action that gets dispatched to a Reducer function. This Reducer updates the Store with a new State depending on what Action it receives. And whenever a new State is pushed through the Store the View is recreated to reflect the changes. With Redux most components are decoupled, making UI changes very easy to make. In addition, the only business logic sits in the Reducer functions. A Reducer is a function that takes an Action and the current application State and it returns a new State object, therefore it is straightforward to test because we can write a unit test that sets up an initial State and checks that the Reducer returns the new and modified State . Redux Middleware The above at first glance appears to be straightforward, but what happens when the application has to perform some asynchronous operation, such as loading data from an external API? This is why people came up with a new component known as the Middleware . Middleware is a component that may process an Action before it reaches the Reducer . It receives the current application State and the Action that got dispatched, and it can run some code (usually a side effect ), such as communicating with a 3rd-party API or data source. Finally, the Middleware may decide to dispatch the original Action , to dispatch a different one, or to do nothing more. You can learn more about Middleware here . With the Middleware , the above diagram would look like this: Redux in Flutter Taking all this to Flutter, there's two very useful packages we can use, making it really easy and convenient to implement Redux in a Flutter app: redux : the redux package adds all the necessary components to use Redux in Dart, that is, the Store , the Reducer and the Middleware . flutter_redux : this is a Flutter-specific package which provides additional components on top of the redux library which are useful for implementing Redux in Flutter, such as: StoreProvider (the base Widget for the app that will be used to provide the Store to all the Widget s that need it), StoreBuilder (a Widget that receives the Store from the StoreProvider ) and StoreConnector (a very useful Widget that can be used instead of the StoreBuilder as you can convert the Store into a ViewModel to build the Widget tree and whenever the State in the Store is modified, the StoreConnector will get rebuilt). Show me the code I’ve created a basic to-do list app to demonstrate the concepts discussed above. Let’s go through the important parts. First, the main.dart file (which is our app’s entry point) defines the application Store object from an initial State , a Reducer function and the Middleware . It then it wraps the MaterialApp object with a StoreProvider that takes the Store and can pass it to its descendant Widget s that need one: void main() => runApp(ToDoListApp());\n\nclass ToDoListApp extends StatelessWidget {\n  final Store<AppState> store = Store<AppState>(\n    appReducer, /* Function defined in the reducers file */\n    initialState: AppState.initial(),\n    middleware: createStoreMiddleware(),\n  );\n\n  @override\n  Widget build(BuildContext context) => StoreProvider(\n        store: this.store,\n        child: MaterialApp(\n          // Omitting some boilerplate here\n          home: ToDoListPage(title: 'Flutter Demo Home Page'),\n        ),\n      );\n} The AppState class contains the list of to-do items and a field to decide whether or not to display the TextField to add a new item: class AppState {\n  final List<ToDoItem> toDos;\n  final ListState listState;\n\n  AppState(this.toDos, this.listState);\n\n  factory AppState.initial() => AppState(List.unmodifiable([]), ListState.listOnly);\n}\n\nenum ListState {\n  listOnly, listWithNewItem\n} In order to display the to-do list, we define a ViewModel class that contains a view-specific representation of the data we need to display, as well as the actions the user can do. This ViewModel gets created from the Store : class _ViewModel {\n  final String pageTitle;\n  final List<_ItemViewModel> items;\n  final Function onNewItem;\n  final String newItemToolTip;\n\n  _ViewModel(this.pageTitle, this.items, this.onNewItem, this.newItemToolTip, this.newItemIcon);\n\n  factory _ViewModel.create(Store<AppState> store) {\n    List<_ItemViewModel> items = store.state.toDos\n        .map((ToDoItem item) => /* Omitting some boilerplate here */)\n        .toList();\n\n    return _ViewModel('To Do', items, () => store.dispatch(DisplayListWithNewItemAction()), 'Add new to-do item', Icons.add);\n  }\n} Now we can use the ViewModel class to display the to-do list. Notice that we wrap our Widget s inside a StoreConnector which allows us to create the ViewModel from the Store and build our UI using the ViewModel : class ToDoListPage extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) => StoreConnector<AppState, _ViewModel>(\n        converter: (Store<AppState> store) => _ViewModel.create(store),\n        builder: (BuildContext context, _ViewModel viewModel) => Scaffold(\n              appBar: AppBar(\n                title: Text(viewModel.pageTitle),\n              ),\n              body: ListView(children: viewModel.items.map((_ItemViewModel item) => _createWidget(item)).toList()),\n              floatingActionButton: FloatingActionButton(\n                onPressed: viewModel.onNewItem,\n                tooltip: viewModel.newItemToolTip,\n                child: Icon(viewModel.newItemIcon),\n              ),\n            ),\n      );\n} In the code above we define that when the user presses the ‘Add’ button, we’ll dispatch an action of type DisplayListWithNewItemAction which indicates we need to modify the application state so that we display the TextField that will let the user create a new to-do item. The action class is defined as: class DisplayListWithNewItemAction {} And here’s the Reducer that for this action: AppState appReducer(AppState state, action) => AppState(toDoListReducer(state.toDos, action), listStateReducer(state.listState, action));\n\nfinal Reducer<List<ToDoItem>> toDoListReducer = // Boilerplate ignored\nfinal Reducer<ListState> listStateReducer = combineReducers<ListState>([\n  TypedReducer<ListState, DisplayListOnlyAction>(_displayListOnly),\n  TypedReducer<ListState, DisplayListWithNewItemAction>(_displayListWithNewItem),\n]);\n\nListState _displayListOnly(ListState listState, DisplayListOnlyAction action) => ListState.listOnly;\n\nListState _displayListWithNewItem(ListState listState, DisplayListWithNewItemAction action) => ListState.listWithNewItem; This is a simple example but demonstrates the concepts explained above. The full source code for this to-do list app can be found in Github To conclude, using this architecture in Flutter apps keeps all the concerns well-defined and separate from each other. However, having only tried this in pet projects ( due to Flutter still being beta ), I'd like to see if it'd scale well in a large projects, as having one State for the whole app makes me think you'd end up composing this State from smaller State objects. Further refactorings for using Redux at scale, could include organising the Redux components in subfolders by feature ( /home , /settings , etc.), even though most examples I've seen use subfolders for each component ( /middleware , /actions , etc.). Last but not least I'd really like to try and build a simple web front-end using AngularDart or even add desktop support with Redux, to see how much code can be reused between the mobile client, the website and the desktop client. Thanks for following along. If you have any comments, suggestions or want to discuss then talk to me on Twitter 💬", "date": "2018-04-03"},
{"website": "Novoda", "title": "try!Swift Japan - Part 2", "author": ["Berta Devant (IOS Developer)"], "link": "https://blog.novoda.com/try-swift-japan-part-2/", "abstract": "At the beginning of March I had the pleasure of attending try!Swift Tokyo 2018 in Japan ⛩. If you have never attended a try!Swift conferences , they are an immersive community gathering with curated talks by the organizer and top of the line workshops the days before and after the conferences. They hold 3 conferences every year: one in New York city, one in Bangalore and one in Tokyo. I had the pleasure of attending try!Swift New York in previous years,  so I knew the format and what I could expect from the conference - or I thought I did! In fact,  try!Swift Tokyo managed to exceed all of those expectations. While I knew the talks were going to be good, since the speaker line-up looked really impressive, I highly underestimated the effect that being in Tokyo would have on the conference. I wrote a little bit about the effect that Japan had on the conference in part 1 of this post ! Talk highlights Here’s a few of my favorite talks from the conference... SIL for first time learners by Yusuke Kita A Lightning talk about how the swift compiler optimizes code, it looks like an intermediate swift code and works before the compiler. It was very interesting to take a look at how swift works under the hood, and to take a look at intermediate swift. I plan on following up with some reading material on SIL to learn more about how Swift compiler works. You can find the slides from his talk here Optimizing Swift code for separation of concerns and simplicity by Javier Soto Javier's talk was on one of my favorite topics: how do you write code that does more than just work? He started the talk with a very good point: we read more code than we write, and because of that, we need to write code that can be readable by others. Simplicity Conciseness Clarity I liked his talk a lot, because it had some practical examples on how to solve common problems in building complex apps in Swift. I did not 100% agree on some of his uses of enums, but it is true that they are very powerful and people need to use them more. One example I loved was how he created his own Layout Guides structures, and those returned safe area or normal layout guide, without having to add if IOS 11 is available code on all of your views. Another tip I really liked was the idea of overriding the method hitTest for UIButtons to extend the surface of the button that the user can interact with, instead of making the button artificially bigger. And a tip I did not know about is that when using UIStackViews , if you set one of the views inside the stack view to hidden, the stackView will rearrange the space on itself for the visible objects. Neat! You can find the slides for his talk here Should Coders Design by Sash Zats Sash, a former designer and now developer at Facebook, started his talk by expressing that design is NOT how things look but rather how things work. That was a very good point to kick things off with, because it immediately gave context to his talk, which was about why coders should care about design. Here are some of his reasons for why coders should be a bit more knowledgeable about design: To be more independent: if you know design, you can unblock yourself from undefined behaviour situations when coding, and help other coders in your team when they get blocked. To find solutions to the actual user problem, instead of just improving interfaces and code Think broader Look more closely at small details Think younger (young generations question every detail, you should too) To avoid habituation: when a designer has seen an app or a design too much, they can stop noticing the little details.  A coder with design eyes might be able to help break that. To use the latest API properly: as a coder you are up-to-date with the latest API, and if you know some design, you’ll be able to convey how to properly implement that to your team At the end, he left us with this quote Think of the user intention. Code for that and learn from failures. That is where design can help coders Coffee breaks are much better when you have your own barista right at the conference Event driven networking for Swift by Norman Mauer Norman is a senior developer at Apple and has been working on network implementation using Swift language. At TrySwift Tokyo, he unveiled the project he and his team have been working on for the past few years: SwiftNIO a low level networking framework that sits under frameworks like Vapor and Kitura and helps writing network protocols directly in Swift. Before he unveiled the framework, he talked about the problem he and his team were trying to solve, and explained a bit about what mulithreading in IOS app and swift looks like. You can catch the full video of his talk on the try!Swift youtube channel if you want an intro into the farmework read this post by Mickaël Rémond. Swift Pi by Kate Castellano Kate showed us on her amazing demo and presentation on how to set up a Raspberry Pi to work with Swift code. You can check her entire Swift Pi project on GitHub and the slides from the talk here. She had a couple of tips that could save a bunch of time to anyone intending to use Swift on a Raspberry Pi: You can only use Swift 3.1 as of right now, not Swift 4 You need to have Ubuntu 16.04 to be able to install Swift binaries You need SwiftyGPIO to access the pins of the board The code and the demo looked really interesting - not only because we could use the language and the frameworks we are used to to also create iot code, but because it aligns with the tendency of porting Swift everywhere. That’s a decision that I believe will make Swift a more stable language, and open Swift developers to a whole new host of opportunities regarding how many things we can create with Swift. Multiple realities by David Hoang Designing Experiences With Augmented Reality by David Hoang David talk about designing AR experiences was especially interesting to me, since I have recently been investigating how to work get started with Arkit and even implemented machine learning into an ar experience . I was really looking forward to finding out how to get the best user design and experience out of the technology, and histalk did not disappointed.  David explained t how he created an app to measure kids. He talked about how he pairs them with a 3m model of an animal of the same height as them, making the whole experience of going to the doctor much more enjoyable for children. He started by discussing  3 possible altering reality technologies around today: VR: virtual environment that shuts down reality AR: digital environment in the real world Mixed reality: augmentation that reacts to the real world He highlighted that before you start prototyping an AR app, you need to make sure your problem would not be better solved by mixed realities or VR. And to always keep in mind that a great AR experience seamlessly integrates the digital world with the physical, improving our understanding of reality. This are some of his tips for designing an AR experience: Use real life situations as inspiration for AR Think in 3D and prototype in 3D Get constant feedback from users from the very first stage of your prototype Take it out of your lab, use it in the world. Reality is the biggest View Controller you will ever work with David Hoang This is just a few of the talks I went to, but there were many more! Here’s some slides and resources for some of the other ones: Exploring Clang Modules by Samuel Giddins Diamond of Variances by Vu Nhat Minh UI Testing for fun and Profit by Sarah Olson Writing Blockchain Clients in Swift by Tamar Nachmany Solving the expression Problem by Brandon Kase Using Swift to Visualize Algorithms by Ben Scheirman Charles for IOS by Karl von Randow Super Resolution with CoreML by Kentaro Matsumae , library on GitHub Conclusion try!Swift was an incredible conference and experience, it allowed me to meet loads of new people - and in particular, those from a culture and language completely different than mine. And not to mention the fact that  I also heard some amazing talks, that will help any future project that I am assigned. So, if you have the chance of going next year I would highly recommend you take the trip, keep and open mind and bring some business cards with you 😉 See you next year!", "date": "2018-04-05"},
{"website": "Novoda", "title": "Q&A - Automation Testing Framework, Native or Cross-Platform?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/should-we-use-cross-platform-testing-frameworks/", "abstract": "Welcome to another Novoda ‘2-minute Q&A’. This series gives you insights into some of the challenging scenario's and ideas that we deal with day to day in the world of Novoda's Engineering, Product and agile Software Crafting. Today we have JT our Head of Quality Assurance, having managed to peel him away from his testing responsibilities for two minutes, and we have just one question for him. I can see the excitement (or is it bewilderment) on his face as he wonders what exactly we will be grilling him over. To cross-platform or not cross-platform When looking for automation testers for apps running on both the iOS and Android platforms. Should you look for people with the experience in cross-platform testing frameworks to avoid having to do both native iOS and Android solutions? Myself and all the Novoda test automation engineers have a lot of experience with all sorts of different frameworks, including cross-platform frameworks such as Appium as an example. From my experience, everything you put into getting Appium to work as a cross platform testing framework isn't worth the effort. Is there an effective solution that is cross platform, or should you stick to native testing frameworks? While I've read some blogs online (light on the details) about folks who've gotten a solution to work for them using Appium, I've yet to see any real world implementation live up to the hype.  There will always be differences in either the user journey through the app, the implementation, or simply page layouts that force an Appium developer to make one exception after another in the test cases. So what should you do to implement test automation across iOS and Android? When we recommend folks looking to implement a test automation framework for both their iOS and Android apps, we recommend to put Espresso in place for Android and XCUITest in place for iOS. Either of those frameworks are significantly faster execution time than Appium Either framework will happily live alongside the app codebase in a single repo But wouldn't appium mean only having to do the work once? A tester will spend more time trying to bend Appium to their will, than they would have spent if they had just used one of the native frameworks. iOS engineers will already be familiar with XCUITest and Android engineers will already be familiar with Espresso. Therefore making code reviews and pairing for your automation engineers faster and easier. If engineers aren't skilling up on cross-platform, where should they spend their time? The real skill set for an automation specialist comes in creating test scenarios and building test pipelines that provide information about the health of the app and quickly identifying risks to achieving product goals. Not in being able to identify page objects on one platform or another. A test automation engineer should be able to implement a native framework on both platforms. Conclusion Thanks to JT for sharing his insights. If anyone reading this wants to ask Novoda some specific questions, we are always happy to make more in this series or answer your questions - contact us here . We're always open to sharing our knowledge and offering insights in relationship to your business. We can review your current testing frameworks, offer hiring insights, or discuss your process and show you some Espresso, XCUITest implementations working with your app.", "date": "2018-04-10"},
{"website": "Novoda", "title": "Shopping Hauls @ Omni-a-Porter", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/shopping-hauls-at-omni-a-porter/", "abstract": "Online-only retailers used to be at a disadvantage to bricks and mortar (B&M) stores, as actually trying on clothes has steadfastly remained the preferred shopping experience. But through convenience and familiarity of online shopping, the distinct advantage of physical stores is slowly fading. Unconstrained by footfall at store locations, online-only retailers, such as ASOS or Thread, have innovated with regular curated deliveries, giving buyers the option to pay only for what they decide to keep. In doing so, online companies have turned their weaknesses into strengths -  and as a result are producing more touch points for customer engagement. This is a key learning for B&M, as they must now ask themselves how they can compete with Amazon, who are expanding their data-led approach into high street stores. How are B&M stores using near future technology to link up their in-store customers with their the online presence? Headstream research advises that great stories trigger purchase intent for 55% of shoppers, and so with that in mind, I’ll expand with a story. Summer shopping haul Pulling out a host of tired colours from her wardrobe , Lily realises she is an entirely different person as the first rays of summer brighten her bedroom. A Summer Shopping haul is her only cure. Lily shouts aloud to Alexa: “Alexa, I’m going to need some summer clothes”. What follows is some lively to and fro, as Alexa and Lily discuss shops and outfits based on some on-point observations Alexa has made about current trends, and through having compiled years of Lily’s volatile fashion preferences across all Apps, sites and channels. A week later, Lily is in town and a Notification reminds her that not only has she got a free schedule for 3 hours right next to her favourite store, she has also been offered some outstanding vouchers which would make her trip very worthwhile indeed. If Lily is like the 54% of users behind Apadami’s 2017 survey, incentives are the main reason she has the app installed on her phone. There remains no better motivator than vouchers to get people into stores, with coupon usage raising an average of 5% every year. Shopping staff greet Lily by the door to discuss her unique preferences and offer Lily a beaconised basket as she walks into ‘Omni-a-Porter’.  Customisation features are expected by Lily, and are woven throughout the store via a comprehensive network of Aruba beacons, on shelves and baskets. These trigger key shopping moments, and for the benefit of the store staff, Lily’s continually improved profile is instantly updated and shared across Omni-a-Porter channels. As she moves around the shop, the Omni-a-Porter shopping assistant remains in constant contact with Lily while she scans and discusses items in-store, through a multitude of Recommendation systems working upon her profile. The human assistant is only one touchpoint, with an AI assistant filling in the presence gaps. When she dwells by a particular rack for longer than usual, a human shopping assistant comes over and picks up where the algorithmic assistant left off. Lily and the assistant discuss the garment’s fair trade qualifications, as well as whether there are any other sizes in stock. She decides to leave the jeans for now, and keeps looking for the t-shirts for which she has coupons. Omni-a-Porter will be retargeting Lily with the jeans she left behind over the next few weeks, sharing some more tantalising info about them with her. When Lily finally finds a t-shirt she likes, she scans a barcode, and checks the washing instructions, as she strides towards the fitting room. After putting on the t-shirt, she looks in the mirror. It is as she suspected: green is not her colour. She air swipes through yellow, then red, then purple - each time, her reflection’s colour of the t-shirt appears differently. Lily decides the blue t-shirt is perfect, so she pays for it using a fingerprint on her phone, then shares her emoji-laden mirrored reflection on Instagram. As she walks out of the store, a shop assistant hands her a bag: Your blue t-shirt, size 12, Lily. Have a nice day! So how has the above addressed touchpoints for customer engagement? Customer Composites: Customer Profiles are in a state of constant refinement. All channels of data acquisition and marketing must share and contribute to understanding a single picture of the customer, regardless of whether they interact through a click, tap or their voice. Voice Actions: Current voice interfaces are limited, but given the pace of improvement, it’s important to start participating in that customer conversation. Advances in voice interface codebases and ML will soon develop Voice interfaces into personable, natural and even desirable recommendation experiences. Targeted Vouchers: Retailers love the incentive and retention hook of vouchers. As customer profiles evolve, Retailers should ask themselves whether their vouchers are keeping pace with the opportunity. 25% of a public surveyed by Valassis in 2016 saved £5-£10 a month, while a further 28% saved £2-£5. In-store targeting: Shop floors are designed so customers move efficiently towards the best stock. This customer pairing refinement needs to dynamically evolve. Dwell time in front of displays need no longer be just a passive observation, when products like Skyfii offer realtime in-store feedback. There is no better time to get creative with a buyer than when they have the product in their hands! Shop floor engagement: Customers expect bespoke engagement online, so why not on the shop floor? Truly personalised experiences could offer a personal shopping experience that has previously been the domain of only the wealthiest of shoppers. Even when they are reluctant to speak to a human shopping assistant, a Shopping AI can keep in touch via any device and follow up with them once they are out the store. Fitting room engagement: Smart Mirrors may soon appear in the Home , but they are already improving personalised shopping in spaces such as in the Hajuko Puma store , with products like Memory Mirror . The full range of customisation options should always be available for the customer to view, regardless of on-site stock limitations. Unlimited Payment options: Solutions such as Worldpay Total tie together all possible payment options, which means that whatever the customer’s preferred method of payment, you can oblige. Data-led Retailers are revolutionising their businesses by starting conversations with their customers at home, leading them to their store, and out the door with a purchase.", "date": "2018-04-13"},
{"website": "Novoda", "title": "Getting started with Google ARCore on Android", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/getting-started-with-google-arcore-on-android/", "abstract": "Google’s ARCore platform can create virtual objects, blending them with the real world through your device’s camera. Follow along as we explore some key ARCore concepts and delve into creating an augmented reality app from scratch. With the release of ARCore, Google has provided us with a brand new platform for building augmented reality apps for Android, but how does it work and where do we get started? ARCore makes use of three main technologies to achieve this: Motion tracking: ARCore can detect and track visually distinct features in the real world to understand it’s position in space Environmental understanding: This allows the platform to detect flat surfaces like the floor or tables Light estimation: ARCore understands the lighting of the environment allowing you to adjust colours to give virtual objects an increased sense of realism Augmented Reality vocabulary you need to know There are five main concepts to understand before diving into the details. Feature points When you move around with your device, ARCore uses the camera to detect “visually distinct features” in each captured image. Those are called feature points . ARCore uses these points in combination with the device sensors to figure out your location in the space and estimate your pose . Pose ARCore uses pose to refer to the position and orientation of the camera. It needs to align the pose of the virtual camera with the pose of your device’s camera so that virtual objects are rendered from the correct perspective. This allows you to place a virtual object on top of a plane , circle around it and watch it from behind. Plane When processing the camera input stream, apart from feature points ARCore also looks for horizontal surfaces, like tables, desks or the floor. Those detected surfaces are called planes . We’ll see how you can use these planes to anchor virtual objects to the scene. Anchor For a virtual object to be placed it needs to be attached to an anchor . An anchor describes a fixed location and orientation in the real world. By attaching the virtual object to an anchor, we ensure ARCore tracks the object’s position and orientation correctly over time. An anchor is created as a result of a hit test when tapping on the screen. Hit test When the user taps on the device’s screen ARCore runs a hit test from that (x,y) coordinate. Imagine a light ray coming from the point your finger touched and going straight into the camera’s view. ARCore will return any planes or feature points intersected by this ray, plus the pose of that intersection. The result of a hit test is a paired collection of planes & poses  that we can use to create anchors and attach virtual objects to the world. Here's a screenshot of the sample app featuring all those concepts What do you need to start developing an ARCore Android app? Android Studio 3.0 or higher A device with Android 7.0 or higher Camera permission ARCore feature requirement You can develop ARCore Android apps the same way you develop normal Android apps. There’s no need for any extra tools. What changes is the way you render your app’s views, because we are going to be rendering virtual objects on top of our camera image stream. For that we need to use a SurfaceView and OpenGL The sample In this example we are going to showcase a simple app that does all of the following: Check ARCore dependencies (ARCore SDK and Camera permissions) are satisfied before running an ARCore Session Display detected Feature Points Display detected Planes Capture user’s taps to perform Hit Tests Create Anchors Attach virtual models to anchors This flowchart provides a high level overview of what the app does: What are the main classes you need to understand? When you develop an app for ARCore it behaves pretty much like a game: there is a surface in which you draw updates every frame. That is what happens inside the green box in the above diagram. The ARCore main entry point is a class called Session . A new Session is created when the Activity starts and it’s responsible for handling AR system state and session lifecycles. This means every time draw frame is called you need to ask this Session object for the AR state: are there any feature points? Any planes? The sample app draws a background, feature points, planes and 3D objects. This is the sequence of operations executed for each draw frame call: Update ARCore data: in this block we’re grouping together several housekeeping operations we need to execute to ensure AR is running correctly: session.update() this provides you with a Frame , which you can use for... frame.getCamera() this provides you with Camera , which you can use for... Calculate projection and view matrices. These matrices would be used later on. Draw the background: take the Frame from step 1 and use it to draw the background of your app. This is basically the image we are receiving from the camera. If we remove everything else and only keep these two steps what we will end up having is a surface view displaying the camera view. Draw tracked feature points: take the Frame and ask for the “current set of estimated 3d points attached to real-world geometry” . Then it draws this point cloud using the camera projection and view matrices as reference. Draw planes: ask the Session for all the trackables with type equals to Plane and draw them (For now only Plane and Point are considered Trackable , but this makes us think more types will be added in the future). Draw models: this step can be divided in two: Check if the user has tapped the screen, a plane has been hit and create an anchor at the hit point. Frame has a method hitTest that receives an Android MotionEvent and returns the list of successful hits for that event. Then it’s just a matter of iterating through that list and creating an anchor if the hit is a Plane . Iterate through all of the existing anchors and draw a virtual model at the anchor’s coordinates. Can you use an emulator? Yes, the emulator will generate a virtual house interior scene where you can move around and simulate plane and points detection. Can you test it? As ARCore was released by Google very recently documentation in relation to automated test support is currently non-existent and material surrounding the topic across the web just as scarce. However, this doesn't mean we can't test our ARCore applications. Unit testing forms the foundation of the testing pyramid and is traditionally where the majority of our automated test coverage/effort originates. When it comes to testing our ARCore based applications this should be no different. The code we're writing is still Java after all. Moving up the testing pyramid we come to integration and user interface testing. Unfortunately, there’s isn’t currently a clear path towards driving user interaction with our ARCore application programmatically, however, purpose built strategies and tools to accomplish this should become more apparent and available as time passes and the technology is adopted on a wider scale. In the meantime, we can focus a larger proportion of our testing effort towards manual based exploratory testing . Pro’s and con’s ARCore technology used to detect planes, points and anchoring 3D models within a space is impressive. Using all of these features is easy and intuitive once you learn all of the vocabulary and related terminology. The main pain point we found for Android developers was rendering 3D models. The basic sample project uses openGL which is not the nicest thing to work with. It operates at a very low level and you’re required to have a good understanding of matrices transformations and complex 3D space maths to work with it. There are third party 3D rendering libraries available for Android that you can use instead of raw openGL. None of them is officially recommended by Google, so is up to you to choose. On top of that there’s no Android Studio integration for any of them, which doesn’t help in making things easier. Something else to worry about is the fact that we didn’t find a strategy to automatically test ARCore apps. We’d love to see how ARCore evolves in the future. We hold high expectations for this technology and hope to see easier ways to render models in the future as well as  better Android Studio support or integrations. Learn more! If you want to learn more check the Google quick start for Android and this presentation from GDG Europe’17 Finally, if you’re curious on how to do AR on iOS check out https://blog.novoda.com/getting-started-with-arkit/ by our colleague Berta", "date": "2018-05-01"},
{"website": "Novoda", "title": "Apps for Hotel Managers and Cleaning Teams", "author": ["Kevin McDonagh"], "link": "https://blog.novoda.com/apps-for-hotel-managers/", "abstract": "A large hotel is, at heart, a vast logistical operation. Guests check in and out of a network of rooms all requiring daily attention. Rooms, restaurants, receptions and conveniences, all require time sensitive processes managed by a small team of organised staff who can benefit from feeling of continuity between their processes. Let’s take the essential (but often taken for granted) role of cleaning. Daily, sometimes hourly cleaning tasks are required throughout hotels such as Malaysia’s First World Hotel, which boasts a mammoth 7351 rooms!  The three biggest challenges of the cleaning industry are quality, motivation and high employee turnover of staff. So how are Apps and IOT technologies helping in these challenges? Cleaning shifts at Hotel d’Data To explain, I take you on a behind-the-scenes visit to Hotel d’Data which, of course, benefits from the 20.4 billion IoT connected devices predicted by Gartner by 2020. Here,  rooms work with staff to keep themselves clean. Each room in this gargantuan hotel is aware of its temperature, last cleaning check, minibar status, etc. Davina is our hotel manager today. It’s 4am, and she’s already sent out a notification to those registered as available on the Cleaning Staff app that she’s going to need an extra four cleaners this morning. Four new cleaners respond from the comfort of their home, without a round robin of phone calls, and they have now been recruited. Of course, the ‘Cleaning Staff’ app is fictional, but apps like Humanity , Wheniwork or Kronos streamline much of the pain of shift planning, allowing the workforce to be actively involved in the resulting schedule, which in turn, allows for more flexibility and greater ownership of the result. As the staff begin to arrive, Bluetooth beacons, such as those offered by Aruba , record each person’s arrival. Each beacon triggers location-specific events and features of particular apps, as people move around the hotel. When the cleaning team leader arrives, Davina receives an alert to welcome them and outline the finer points of their responsibilities. When staff are likely to be one of the 1.5bn active monthly WhatsApp users who can instantly contact their most distant relatives, they of course expect to to be able to instantly speak with and locate their colleagues when working within the same building. One of these cleaners, Bill, isn’t familiar with the hotel layout. However, location-based asset tracking ensures he can find everything he needs for his tasks. It’s important to track high value items, such as defibrillators and fire extinguishers, so that they can be located and checked, but it’s also just handy to find where the light bulbs are kept. Bill is directed to clean rooms 307-317 via Meridian’s indoor wayfinding, and then uses his smartshift bracelet to open the door.  Bill brought a light bulb, because the IOT room was in two-way communication with him before he visited and so he knew to pick one up from supplies. He observes that the “bathroom mirror is scratched” and Voice Recognition sends his observations to the caretaking team. After room 307’s astonishingly depleted mini bar is restocked, a quick Augmented Reality sweep around the room confirms the hotel’s preferred room arrangement - right down to the objects on the desk. Bill has cleaned the room, and the door locks behind him. When all the rooms are cleaned, Davina, our hotel manager, sees the status in realtime and before Bill logs out of the hotel, Davina rates his  team leader’s efforts today 5 stars at the end of his shift. An end-to-end tracking of regular health and safety steps can dramatically lower insurance premiums, because it is so much more reliable than a pen and paper. So how has the above addressed three big concerns for Hotel cleaning ? Quality: Documented Standards are accessible to all via the Apps. And moreover, tools such as quick reporting and AR checks can help improve the results, such as when Bill checked the layout. Companies  like Ikea are already embracing this to considerable effect in their instructions delivered via AR. Motivation: It is often wrongly assumed that Apps dehumanise working, but in fact, the opposite can be true.  Close messaging contact with colleagues and quick access to information is empowering, while feedback from peers can make their time more rewarding. High employee turnover: Apps such as ‘Cleaning Staff’ can be downloaded at the employee’s leisure, and contain authorised GPS and fingerprint validating information for security.  Onboarding is de-risked through digitising standard practices and lead time is reduced through integration into the hotel Asset tracking and Wayfinding. Overall, the positions of the Hotel Manager and the Cleaning Team are improved through transparency between their processes. Meanwhile, the data shared will help both parties improve the overall customer experience.", "date": "2018-04-13"},
{"website": "Novoda", "title": "2018’s Top Digital Trends for the Retail Industry", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/2018s-top-digital-trends-for-the-retail-industry/", "abstract": "Digital technology is revolutionising the retail industry, but not in the ways that were predicted a few years ago. It’s no longer about which channel will emerge triumphant, but instead, about how online and offline can work together to benefit both the shopper and the retailer.  By taking the lessons learned online, such as those regarding tracking the customer journey, and applying them to the bricks and mortar (B&M) business, a lucrative future on the high street can be secured. So just what can future technology offer a forward thinking retailer? We look at some of the trends that are destined to set the shopping experience alight. Trend: Digital signage Digital displays allow retailers to dynamically adjust display points, including on-shelf pricing and marketing screens. It’s this deceptively small change away from static pricing that has the power to revolutionise the pricing paradigm. It will allow shops to dynamically alter prices, perhaps to be more competitive or to take advantage of a short-term limited supply, as well as to trial what kind of information encourages shoppers to buy, from the ethical origins of the product to stock availability. This can all be done from one central control point, such as from the head office, or can be changed on a per-customer basis. Surge pricing is common in other areas, such as transport, but so far, has not been seen much on the high street.  While not using digital displays, one company that has trialled dynamic pricing is Marks & Spencers , who lowered the prices of their sandwiches before noon to encourage shoppers to buy their lunch in advance. The use of dynamic displays also opens up exciting opportunities for tailoring promotions for customers, working in tandem with Bluetooth beacons and customer data, to show shoppers personalised discount vouchers or other relevant information to push them towards a purchase. This has already been trialled by Coca-Cola who have used the Google Cloud platform to show targeted advertising to customers walking past one of their branded displays, based on data from their smartphone. It’s by using a combination of beacons to track shoppers, sales assistants with instant access to information regarding both products and shoppers, and digital signage to dynamically adjust the information shown to those shoppers, that the Connected Store will become a reality. The ultimate aim is a friction-free shopping experience - something that online giant, Amazon, have been early proponents of.  In their Amazon Go store in Seattle, customers can walk out of the store with their purchases without ever going near a checkout, thanks to their advanced tracking technology, which registers exactly what a customer has picked up while shopping. The future of shopping is a much-loved discussion here at Novoda - you can read about our imaginings of the ultimate Omni-a-porter omnichannel shopping experience to find out more. Trend: Geolocation Retailers have long known that location is everything, but while that maxim remains true, it now refers to the shoppers themselves, rather than the location of the shop. There’s simply no better time to target a potential customer with discounts or vouchers than when they’re close by. As technology progresses and data speeds increase, proximity marketing will become and increasingly hot topic, with the market predicted to reach USD 52.46 Billion by 2022. This type of marketing has gained traction quickly. A recent study found that 84% of millennials want to receive messages from a retailer while they’re in their store, a number which remains high, at 74%, in the general population. And if millennials are a retailer’s target audience, there is more good news for B&M stores - 82% of millennials still prefer shopping IRL to online. Bluetooth beacons provide a lightweight and relatively low cost way of tracking the movements of individual customers. Video game retailer, Gamestop, uses beacons to map different areas within the store, so that as a shopper makes a beeline to, for instance, RPG games, they can be targeted with content relevant to that interest, via push notifications on the GameStop app. We also looked at the effect of beacons on hotel logistics recently, highlighting how they could streamline the intricate daily cleaning requirements for any large operation. In Australia, alcohol retailer, Dan Murphys , has set up a 400m geofence around each store, which alerts staff when a shopper with the app enters the area, so they can get their order ready for collection. While this is an example of geolocation being used to create a frictionless experience for those who have already purchased, it’s easy to see how this can be adapted to more proactive marketing purposes. Trend: Voice assisted shopping Smart speakers were the most popular Christmas gift in 2017 , with both Google and Amazon discounting their products in an attempt to gain market share. Canalys is forecasting 70% year-on-year growth for the product category, and Gartner estimate that 30% of web browsing sessions will be done without a screen by 2020.  Early signs indicate this is technology that fairs well long-term, with most people still regularly using theirs long after the initial out-the-box excitement fades. So, with a new product category on the horizon, what does that mean for retailers? 44% of smart speaker users already use it to buy groceries at least once a week, and Amazon naturally benefits from this trend. But Google has partnered with Walmart in the US in an effort to compete, allowing customers to buy any item available from Walmart via the Google Home speaker using Shopping Actions . This has now been extended to several other retailers, allowing customers to add items from several stockists into one central Google-powered shopping cart. In the UK, 51% of people access retail sites via both their desktop and mobile devices, and average 5.3 hours per month browsing those sites, according to Comscore’s Global Mobile Report. . But Smart Speakers could change this, as a new addition to the omnichannel experience, and the potential is huge. As customers become more used to adding things to their shopping lists, searching for suppliers and making purchases via voice, the retailer is invited into the home as never before. The canny retailer will be able to guide the shopper towards a purchase from the very moment they first think about buying one of their (or their competitor’s) products. Trend: Augmented Reality AR technology is gaining traction, and it’s predicted to be the next point of consumer inflection, changing behaviour around the globe.  As the hardware becomes cheaper to manufacture and better designed, take-up will increase significantly. IDC predict nearly 95% growth in the industry in 2018, with a continued five-year compound annual growth rate of 98.8%. One of the retail pioneers in AR technology is IKEA, who already help customers see what pieces would look like in their own home via their IKEA Place AR app. It was designed using Apple’s ARKIT , which we recently experimented with here at Novoda.  According to an interview in Wired , forthcoming features will include the ability to let people scan furniture in real life to be shown similar IKEA products, and an AI shopping assistant, which will make product recommendations based on your space. AR also suits fashion brands, allowing customers to virtually try on items. Covergirl used AR so that shoppers could try on their make-up  and then ‘buy the look’ from within a smartphone web browser. Charlotte Tilsbury have done the same thing using a ‘Magic Mirror’ in store. It’s this ability to let customers try products virtually that presents such an exciting opportunity, particularly for marketing purposes. Imagine being able to try on and then purchase a pair of sunglasses, after seeing them advertised in a magazine. It removes the one significant barrier to online shopping -  namely, the question around what a product would look like on the shopper. Our own research shows that 30% of users already use Snapchat filters to overlay a mask over their face, and it’s easy to imagine integrating purchasing functionality into that app for advertisers.", "date": "2018-05-11"},
{"website": "Novoda", "title": "How modern Facilities Management and Smart Technology can help businesses reach their sustainability targets (and what it means for the rest of the office)", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/how-modern-facilities-management-and-smart-technology-can-help-businesses-reach-their-sustainability-targets-and-what-it-means-for-the-rest-of-the-office/", "abstract": "Sustainability is firmly on the business agenda around the world, and the efficiency of buildings has natural come under the spotlight. They account for nearly 40% of the world’s energy use, 70% of electrical consumption and 40% of the world’s greenhouse emissions.  Clearly, a sustainable future for buildings everywhere will make for a better environment, and that brings a whole new set of challenges for anyone in Facilities Management (FM). By integrating smart technology into retail premises, offices and plants, it’s possible to constantly measure thousands of important and ever-changing events. In gathering data and benchmarking what should be happening, buildings can self-regulate to fix any fluctuations automatically, and alert FM when human intervention is necessary. This human intervention, in turn, can also be streamlined by technology, to make sure that anything that’s not operating efficiently can be quickly fixed or replaced. Smart meters can provide real-time data across a number of buildings, an individual building or even a single room. This collects the Big Data needed for FM to effectively benchmark,  providing a real-time snapshot of performance and energy usage at any point in the day. This constant monitoring has proven to have a positive effect on the bottom line. A recent study showed that FM spends 30% more on an non-benchmarked building, and 15% more per square foot on maintenance, as a direct result of inefficiencies that have been allowed to develop. But a more sustainable building doesn’t just mean lower energy bills - it can boost staff morale and performance, as well. WorldGBC’s recent report showed that there was a reduction in employees taking sick days in ‘green’ buildings, as well as an increase in productivity. Of particular note was a saving of £200,000 for engineering consultancy, Cundall. Their new-build office focused on improved indoor air quality, through continuous monitoring of carbon dioxide (CO2 ) and volatile organic compounds (VOCs), and resulted in a 58% reduction in absenteeism, as well as a 27% reduction in staff turnover. And while most people’s vision of a smart building probably involves an abundance of glass and chrome, designed in the most modern of styles, smart buildings don’t have to be designed from scratch. In fact, one of the most iconic historical buildings in the world, the Empire State Building , has become somewhat of a poster child for smart sustainability.  Sensors have been installed throughout the building, measuring CO2, motion, and room temperature, so that air quality, lighting and air conditioning can be monitored and controlled remotely. Skanska USA, who occupy an entire floor, have reportedly reduced their electricity spending by 57%, compared to their previous office. The Smart Office future for… Maintenance An inefficient air cooling system uses a lot of power to very little effect, whether it’s as a result of irregular maintenance or a broken part. An AC filter needs changing once a month if it’s running every day, and an old filter adds 5-15% to energy usage, as well as shortens the lifespan of the unit. By simply adding a barcode to each unit, anyone doing maintenance can quickly scan it, to have the filter change noted in the system. This will then automatically alert the team after 30 days when it needs changing again. For more on how technology is set to revolutionise the maintenance of buildings, check out our trip to Hotel d’Data. The Smart Office for… Office workers Measuring desk occupancy in real-time can create a workable  ‘hot-desking’ environment, allowing employees to check on arrival at a centralised screen or on a phone app to see which desks are free, who is occupying those around them, and even displaying the level of noise in that area. With flexible working on the increase, efficient space utilisation can become a challenge. Long-term, this room and desk occupancy data can be used by FM to make informed decisions about the size and type of office space needed. The Smart Office for… Security By issuing every member of staff and visitor with a smart tag, it’s possible to know exactly where each person is in the building at any time. This is important for health and safety (and so will lower insurance premiums), as well as R&D, where certain areas are off-limits for all but the necessary personnel. It’s a lot more efficient than manually signing in guests,  issuing them with a paper pass, and asking a member of staff to come to reception to greet them. This smart-tag data also feeds into the same system that monitors lighting and air cooling, so that the rooms in the building are automatically made comfortable when occupied. The Smart Office future for… Architects The effect that technology will have on the way architects design buildings is almost limitless - everything from the placement and size of the carparks to the material used for the windows will be influenced. This is already being seen in many new buildings. The new headquarters of Deloitte in Amsterdam, for instance, has been designed to integrate 28,000 sensors micromanaging humidity, light and temperature to precisely replicated the feeling of being outside on a warm day. In the future, a truly smart building will employ technology selectively, to best suit the needs of its users. Betsey Dougherty, co-founder of Dougherty + Dougherty Architects in California, says “A smart building should allow you to get better faster if it's a hospital, learn more if it's a school, be more creative if it's an office.”", "date": "2018-05-15"},
{"website": "Novoda", "title": "We went to Google I/O 2018, here's what we are excited about", "author": ["Team Novoda (Joint Thinkers)", "Sebastiano Poggi (Android GDE)", "Ataul Munim", "Alex Styl", "Niamh Power (iOS & Android Software Crafter)", "Daniele Bonaldo (Android, Wearables and IoT GDE)", "Tim Craven"], "link": "https://blog.novoda.com/we-went-to-google-i-o-2018-heres-what-we-are-excited-about/", "abstract": "As every year , a selection of Novoda team members headed to Shoreline Amphitheatre last week for Google I/O 2018. They attended sessions, talked to Googlers about our clients' problems and needs, and lived and breathed all that's new in the many Google products we use daily. Here's what they found exciting. Google has been very busy this year cooking up a great Google I/O. The massive refresh to Material Design, all the new AI-powered features in Assistant and Maps, Android P and its many new smarts, are just some of the headline announcements that the various teams dished out during the keynotes and the various sessions. What are the Novodans that attended the conference particularly excited about? Sebastiano Poggi Android, Identity, and Flutter GDE There are so many things to get excited about that I really don't know where to start. Both designers and developers have something to rejoice about. I think Material Theming is a huge step forward in the maturity and versatility of Material Design; the preferred design system for Android and Google products is now a design system for design systems. I'm looking forward to seeing what people can come up with this new found flexibility and freedom, and all the tools that were announced. When it comes to Material Design, there's even more exciting news: Flutter , the extremely promising cross-platform app framework, is now an official Material Design implementation. Just check how many of the design talks at I/O prominently feature Flutter! And that's not all. The Flutter areas at Shoreline have been constantly full of people, and the codelabs have consistently been amongst the most used by the I/O crowd. There's a lot of buzz around this piece of technology and I'm amazed at how fast things are moving since Eugenio Marletti and I did the first public talk on the subject last year! But I'm still an Android developer at heart, and I'm also very glad that Android P is not as minor of a release as the first developer preview might have suggested. Besides all the new Material goodness, which makes P a Pleasure to use (pun intended!), there's a vast amount of new APIs for developers to play with. Obviously, my favourite is the Slices API , together with the App Actions . Lastly, I really want to do a shout out to the digital wellness features Google is introducing in Android P. I'm very happy to see them being attentive to the risks of addiction to our phones every one of us is facing daily. Ataul Munim Android Developer I’m really pleased that accessibility hasn’t fallen by the wayside in the recent drive with AR and VR. Christopher Patnoe and Ran Tao opened on Day One with the aptly-named session, Accessibility for AR and VR , introducing both AR/VR as well as accessibility concepts to a packed tent of attendees. What I found interesting (and reassuring!) was the familiar advice given to designers to build inclusive experiences. I had a quick peek around at the Design & Accessibility sandbox too, where the Lookout team were showcasing a new app for vision impaired users . The app combines computer vision with some custom gestures to facilitate hands-free usage and it’ll be released pretty soon for you to try yourselves. Not everything was super-flashy. As part of the Jetpack library of… libraries, the Android Support Library has been updated to AndroidX , basically a repackaging of the existing tools with some updates. The intention here is to have separate versioning for each artefact guaranteeing binary compatibility through major versions and also renaming artefacts so it’s clear what’s inside each one. I think I’ll need a diagram of all the new libraries and plugins with “X” in it before I feel comfortable, though. Daniele Bonaldo Android, Wearables and IoT GDE Since its launch, Google Photos allowed users to easily store, search and share an unlimited amount of pictures, which couldn't be however accessed by third-party apps. As a passionate photographer, I’m really excited about the newly announced Photos API . While still in developer preview, this API allows developers to show users' pictures from Google Photos directly in an app. Even more interesting is that it will be possible to programmatically upload new pictures and create albums with rich information, including labels, maps and location. Niamh Power Android & iOS Developer At I/O this year, there were over 15 hours of sessions relating to Firebase . Since Google’s acquisition of Fabric last year, I’ve been particularly interested in the progression of Firebase as a platform as it brings on more of Fabric’s suite. A particularly exciting announcement as an iOS developer was the availability of ‘Test Lab’ on iOS devices. Test Lab allows you to remotely test your application on a range of devices, which is really important to remove the chances of device-specific bugs, or UI issues. With this now being available on iOS, projects with both platforms can reliably know the state of their apps, and more cross-platform team collaboration can be... Additionally, on the side of Analytics, the Firebase console has been adjusted so you can now see the total statistics across both mobile platforms together if required. This mitigates the common segregation of the platform implementations and allows for a clearer picture of the overall state of your app. Even more excitingly, MLKit is now available as an integration with Firebase, giving access to a set of APIs, allowing your app to recognise text, detect faces, scan barcodes, label images and recognise landmarks. You can run these either locally or in the cloud depending on your requirements. The expansion of features available on Firebase is really exciting as an app developer, especially when working across both platforms. Having such a plethora of tools available so easily is a fantastic resource, and I’m really excited to see the developments over the next 12 months. Alex Styl Product Designer This year’s I/O had something for everyone to enjoy. The conference featured lots of technical talks, as well as product and design ones and even inspirational ones. The one thing that I am excited the most has to be Android’s new layout builder, the ConstraintLayout . Not only it allows the developer to create those nearly impossible to implement designs, but it gives so much flexibility towards how they can achieve it. ConstraintLayout now brings a new API called Helpers which allows you to create custom behaviour for a specific view or a group  of views, which can be reused across different designs (composition over inheritance). In addition to all these goodies, the team is also working on a subclass of ConstraintLayout , the MotionLayout . As the name suggests, it provides a flexible way of creating animations across yours screens by specifying keyframes, similar to modern animation software. The one talk to left me inspired has to be Designing for inclusion where John Maeda went through some bits of the Design in Tech Report 2018 pointing out why inclusion is such an important topic not just for people with special needs, but for you as a designer, a professional, a human being and an ageing person. The same talk featured Hannah Beachler, Black Panther's Production Designer, going through the process behind designing the futuristic world of Wakanda. Last but not least, Google I/O featured research talks and I cannot express my appreciation enough for that! It was great to receive insights for research done within Google about Material Design and how each component became to be. If you want to learn more, I encourage you watch the Material Metrics talk . Tim Craven Client Services Lead For me, what’s really interesting to see in this year’s I/O is the amount of time and energy Google are investing in Augmented Reality. Google announced two big new features that are going to shape AR applications: Cloud Anchors, and Augmented Images. Cloud Anchors is a cross-platform feature that will help synchronize group AR activity. Put simply, cloud anchors allow multiple people to be involved and interact with the same Augmented Reality experience simultaneously. Why is this good? Previously an AR experience, in my opinion, has been a fairly lacklustre solitary experience. It’s has been limited to one person looking at one small phone display. If you compare this solo experience to that of a VR headset then AR is left wanting. VR can create a whole world for an individual and therefore a multitude of exciting experiences. However, this new feature opens up new opportunities in which people can experience AR with their friends across multiple devices. This adds a much needed social element and it opens up a world of opportunity for AR. Augmented Images are a new feature that allows an application to activate 2D images into 3D images very simply. They work by detecting certain images in real time and then render 3D assets on top of them. Use cases include education and media but the main and most interesting use case is in advertisements. This feature means advertisers will be able to turn stationary adverts around the world into far more engaging experiences. In conclusion, these new features show that ARCore is being taken very seriously by Google and AR is becoming more and more likely to have widespread adoption in the near future. You can get a more detailed overview of these new features, and others, on the \"What’s new in AR\" talk .", "date": "2018-05-17"},
{"website": "Novoda", "title": "TestBash Brighton 2018", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/testbash-brighton-2018/", "abstract": "On the 16th of March part of the Novoda QA team - Bart, Jonathan and Sven - went to the TestBash conference in Brighton. It was our first time attending TestBash organised by Ministry Of Testing and we would like to share our thoughts on the talks and activities we found most interesting. Bart Ziemba Mobile Software Tester / QA My favourite talk was called \"Communities of Practice, the Missing Piece of Your Agile Organisation\" by Emily Webber . It was not strictly related to testing but rather about how testers can better interact and learn from each other while still being a part of cross-functional, Agile teams. Emily proposed that it can be achieved by building communities of practice which are groups of people who share a concern or a passion for something they do and learn how to do it better as they interact regularly. Wenger-Trayner I liked very much the presented journey from silos teams of project managers, designers, developers or testers sitting separately, through cross-functional teams, finishing on communities of practice. At Novoda we are running guilds, which are groups that focus on various areas of expertise. Therefore I particularly enjoyed and found useful the part on the ways companies and employees can benefit from participating in communities of practice, and how building and running them can be improved. Benefits for a company: Possibilities to try out new things in a safe environment without concerns that could appear while trying it out when building for production. Members share knowledge that reduces duplication. Community creates its own practices and approach to tackle various. problems which helps to scale and sell the services to potential clients. Creation of collective knowledge base. Benefits for employees: Increased confidence as you get support from community. Opportunities to learn from others. How to build successful communities of practice: Every member should feel a sense of community. Common effort of community, not individuals. Safe environment and space where members of the community could express their ideas and feelings without any limitations and fears. Communities should have their leaders. Leaders need sufficient time to lead the community. Set up common values, goals, mission of the community. These enable the community to keep moving forward. Meet often and systematically. Create backlog of tasks you want to work on. I hope that learnings taken from Emily's talk will soon be applied to our Testing guild at Novoda and others too. To help with that we are going to get Emily's book so everyone can find out how to build successfull communities of practice. Jonathan Taylor Head of QA I really liked a talk that a presenter named Rosie Hamilton gave on Discovering Logic in Testing.  I found what Rosie presented interesting for a number of reasons. Firstly, she related everything back to game testing, which I’ve never done.  Secondly, she presented different types of logical reasoning, the origins of various philosophies of logic, and how testers apply logic everyday. It started out a bit like a semester at University.  We’re going to need some basic terminology so that we’re all on the same page.  Questions in logic are propositions.   We use logic in order to prove that the proposition is either affirmed or denied, that is to say judged to be true or false.  A theory that tries to answer a question is a hypothesis.  And a hypothesis is based on the probability that it can be proven and can be weak or strong. Got it?  Ok. Deductive Reasoning The first kind of logic introduced was Deductive Reasoning.  Testers, and everyone else for that matter, will use this type of logic in problem solving every day.  Conclusions drawn from this kind of logical reasoning are very strong, but in order to use deductive reasoning to logically solve a problem, the hypothesis must be true.  The example that Rosie gives of deductive reasoning: The vending machine number for crisps is 06. The cost of crisps is 50p Hypothesis: if I put in 50p and press 06, I’ll get a bag of crisps In a testers world: I found a crashing bug on every version of Android Nougat. I’ve got a Samsung S8 with Android Nougat on it. Hypothesis: I’ll see the app crash on this phone. However, it is possible that the input statement was not correct in the first place. Inductive Reasoning Inductive Reasoning is basically the opposite of deductive reasoning.  From specific observations, we can make broad generalizations.  And then we draw conclusions based on the data we’ve recorded. At this point Rosie used a whole bunch of 19th century philosopher John Stuart Mill’s Methods of Inductive Reasoning to determine the root cause of a bug found in a game called Rift.  I wouldn’t do them justice to recount them but she used: Direct Method of Agreement Method of Difference Joint Method of Agreement and Difference Method of Concomitant Variation Method of Residues See?  Sounds a lot like a Uni class.  But as far as software testing goes, it boils down to this: What is common to each failure Spot the difference between success and failure A mixture of both those things The more broken it is the more something has happened Abductive Reasoning Which brought us to Abductive Reasoning and introduced another 19th century philosopher, Charles Sanders Peirce , who thought Mill’s was full of it.  Oooh, Philosopher throw down. It’s widely misquoted that Sherlock Holmes had amazing powers of Deductive Reasoning, when he in fact had Abductive Reasoning powers.  Abductive reasoning used a best guess, or inference to the best explanation. The example that Rosie first gives: Sherlock see someone has a sun tan. He abduces that they have recently been away on holiday. He see that there is a band of skin around their wrist which has no tan. He abduces that this person usually wears a watch, however they are not wearing their watch today. Classic Sherlock Holmes. An example bringing it back to software testing: I’ve observed a bug in the Android application running Nougat when a notification is received. I’ve observed the bug on multiple devices running Nougat I do not observe any bug in notifications when the same Android application deployed on any other os version I’d abduce there is something in the way that the app is calling the Nougat notification bar. The Logic of Testing We’re all testers in one capacity or another at this conference, but I’d be surprised if there were more than a few that had classical training in logic and philosophy.  That’s what I found most interesting about this talk, it gives names and categories to the types of thought processes we use all the time to test, find bugs, find root causes, and solve problems. Paraphrasing Rosie here: When we find a problem interacting with an application and try to cause it to happen again, we are collecting data. When we are generating ideas about why this is happening, we are generating hypothesis. When we are proving or disproving we are collecting more data. That data could cause the ideas to change. When we are finding the simplest explanation, this is the simplest explanation that satisfies the data. If the problem can’t be explained. Collect more data. See if Hypotheses might change. Report problem.  The side effects from doing all this is gaining strong observation skills and strong reasoning skills. Sven Kröll Testing Toolsmith I adored the talk of Matt Long who was talking about the topic of programmable infrastructure and why we should test it like our production code. Thanks to the DevOps movement teams are working closely together. Unfortunately the Quality Engineers are often not involved in this. Programmable infrastructure becomes a pattern which is used by more and more projects, and the team members become very acquainted with the topic. However, it seems that history repeats itself. In the early days of development - testing was a discipline which was not very common. This lead to a lot of untested code which in turn led to unmaintainable code. It appears that Programmable Infrastructure is following the same path. The talk tried to tackle this exact problem. He started with pointing the differences between DevOps and Programmable Infrastructure: ...DevOps is about culture, teams, and processes while Programmable Infrastructure is a collection of tools, techniques, and everything which belongs under the umbrella of Programmable Infrastructure. The Problem Matt continued by showing us the problem he had encountered at his previous position where the team build a broker for cloud applications and that he was the person who should test it. He could break down the problem into two parts - a web testing part which is, in the end, a daily routine for a seasoned tester and the infrastructure part which was utterly new to him. The main things he thought could go wrong were: It doesn't even deploy It does deploy, but it is configured wrong It is unusable for the users. It became undeniable that testing an application and testing a programmable infrastructure have some similarities, which brought him to his next point: Tooling. The Tooling His first tool of choice was a linter which helped in keeping the project sane in an ever-changing context. The next step up in the pyramid were the unit tests which he said you could implement with a bunch of different tools depending on your tech stack. However, he strongly points out that whatever you do - bash scripts are always the worst option, as they tend to be overcomplicated. The integration test tools he used made the next level of confidence. He was showing some examples of frameworks which he identified during his projects like Serverspec, Goss and the native test solutions with a cloud provider SDK. The summary He summarised the advantages, disadvantages and the things which could go south. The good parts were that we have tests for each layer like in testing for production code and that it is very much doable. The more annoying things were that testers would need to have another framework to maintain and it ended in a lot of context and language switches which made it slower. Lastly, he pointed out that infrastructure can be very slow and expensive which can lead to management doubting whether it makes sense to continue. All in all, I was very interested in the topic and could learn a lot. I hope that I will have the chance in one of my next projects to use the techniques I've learned. UnExpo The unexpo was a new tryout to modernise the expositions which are common on conferences. It is usually a place where companies try to sell their services and tools or try to lure testers in their nets. Richard Bradshaw tried it with a new approach. While unconferences become more and more common unexpos are very much a new thing. He invited us - the participants to create a stand with a topic we liked to talk about or share. Sven started a booth at the unexpo where they wanted to share the ideas of architecting an automation solutions with other testers. Other stands were about developer/tester relationships and people looking for jobs. Another exciting one was about the dojo and how testers are learning and a small game of stapling. I found the idea of an unexpo very promising, and I hope that this will become another tradition at test bashes. Openspace Every test bash is followed by an open space where some of the participants prolong the feeling of a test bash on a Saturday and exchange a lot of topics. The format of an open space is as the name already suggests less strict and up to the people who join. We started in the morning by announcing the topics which went from playing test related games like risk storming knight rider or playing exploding kittens to more serious discussions of test techniques and tools. My programme was very much divided in playing some games about testing to a debate on pairing and how to introduce it. I also attended a discussion about automation in testing and on the one hand how we can tackle, but also on the other hand how we can enable testers to create better automation solutions. I enjoyed it very much and could learn a lot at this event. However, I feel also very good that I could also share some of my knowledge with lesser experienced folks. Sum up All in all, Brighton's test bash gave us lots of new ideas we would like to apply to our day by day work. What is more, it allowed us to interact with a great community of testers from around the world. We are looking forward to the future test bashes.", "date": "2018-05-22"},
{"website": "Novoda", "title": "Android Testing - Google I/O 2018 - What's new? - Part 1", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/whats-new-in-android-testing-google-i-o-2018-sum-up/", "abstract": "Among almost 200 talks from the Google I/O 2018 that are an amazing source of knowledge I have selected ones that are related to testing. Put some time aside and check out Google’s latest offerings  in the Android Testing space. Bart Ziemba Mobile Software Tester / QA Pre-launch testing in Google Play Console Great talk by Richard Gaywood and Justin Broughton about pre-launch report and app crawlers available on the Play Console and Firebase Test Lab. Use them to test your APK before releasing it to your clients. How does it work? APK file is being uploaded to Google Play Console alpha or beta track Firebase Test Lab. APK is being installed on high-end and low-end physical devices. Robo crawler explores your app by performing gestures or inputing text to in your app as users would do. Results of these exploratory activities are recorded and gathered into the pre-launch report available in Play Console. App crawler is not only able to find crashes, performance or security issues but can also record videos, take screenshots and share the logs of its execution. Pre-launch report gives you the summary of any issues found by app crawler and also prioritises them. It even gives you the recommendation on whether to go-live or not. Signin handling If your application requires logging in, app crawler can handle that for you. It can either log you in automatically with your Google account or you can configure the login credentials that app crawler will use after detecting logging screen. Deep Links If your application uses deep links, they can be now testes with pre-launch reports. You can add up to 3 deep links into your pre-launch report configuration and app crawler will test them for you. Robo Script Robo Script can help you to drive the behaviour of the app crawler within the the areas of the application that are too complicated for it to test such us complex text forms. Robo Script can be recorded by Espresso Test Recorded which is part of the Android Studio. Accessibility Too little tapping area of the button, too small font size, too low contrast or missing TalkBack annotaions are just some of the accessibility flaws that can be detected by pre-launch reports so that development team can improve it. To me, pre-launch reports sound like a great way to find the issues with your application at a very early stage of the development and later on  deliver high quality applications to your customers. More details about pre-launch report: Use pre-launch reports to identify issues One of the most exciting announcements regarding the Feirbase Test Lab is starting the support for the iOS devices. It's a great news for those who work on cross-platform projects and want to stick to one cloud testing service. There is a possibility to sign up for the beta here . Full talk: Autonomous and customized pre-launch testing in the Google Play Console (Google I/O '18) Jonathan Taylor Head of QA Best Practices for Testing Actions on Google Full talk: Best practices for testing your Actions (Google I/O '18) Presented by Aylin Altiok, product manager for Actions on Google and Nick Felker, developer of programs engineer for the Google assistant and IOT. Before we get too far, we might want to check out another video from the conference, an intro to Actions on Google OK, so now we know that Actions on Google is the developer platform for Google Assistant.  Basically, voice commands that app developers create to be used anywhere Google Assistant is, such as on Google Home.  And why we want to enable our apps to integrate with Actions.  The people at Google clearly think that assistants is the way forward in computing. Right off, I enjoyed the opening of this talk \"Let's talk about why testing is so important.\"  As a tester, you've piqued my interest :) Aylin starts off with some data on looking at Play Store statistics about folks that give one star reviews so often mention stability and bugs in their comments.  And that a large majority uninstall if they see stability issues.  On flip side, 5 star reviews very often mention usability and stability in their reviews. Take aways : Testing will improve stability > will lead to better reviews > will lead to more downloads. Monitor your play store reviews.  So much information in there. Then we get into an example of how to build an action using DialogFlow .  DialogFlow takes care of all the natural language processing and machine learning business when you are building your action.  Basically, you come up with the phrases that a user might use to interact with your service, and use DialogFlow to pick out the key parts of the phrase and map them to an intent. Take away : Building an action doesn't appear that difficult with the tools they've provided.  I'll have to give that a go. Nick steps up to start talking about the part we're here for.  How are we going to test our action.  Once you've got your action, first step in testing is to use the Actions on [Google Simulator] ( https://developers.google.com/actions/tools/simulator ) you access through the Google Action Console.  The tester can specify various input text or voice commands to be sent to the action, and you can see the translated request input and corresponding responses.  The tester can continue sending text or voice commands as a contextual workflow through the app, and verify each response from the service. Take away : While this seems pretty easy to execute, feels like black box manual testing.  Wouldn't want to have to do this over and over, doesn't seem exhaustive. Let's talk about how to make this testing more repeatable and exhaustive.  Next they introduce the Automated Testing Library for Actions on Google.  The testing library is built on Node.js and supports all the existing testing infrastructure. The library will allow us to send unstructured queries to the system (users can say anything), retrieve the appropriate SSML (Speech Synthesis Markup Language) responses. Ok, so here's a sample node.js test script that Nick uses to demo how easy it is to test: What this is going to do is send the users queries to the service and parse the responses, just as we'd expect. Let's break down what's happening in and pick apart the basics: A couple of declarations at the top.  They're going to use the test assertion library for node, Chai , and the logging library for node.js, Winston .  They've also got a declaration for the coordinates a place to be used as the location we're going to query for. Then they included the testing library actions-on-google-testing .  This is the nuts and bolts that is going to allow them to easily send queries and check the responses. They also need to include in the test script the credentials for accessing the actions, which can be grabbed from the developers console [ https://console.developers.google.com/ ].  Should look like: { \"client_id\": \"my-client-id\", \"client_secret\": \"my-client-secret\", \"refresh_token\": \"my-refresh-token\", \"type\": \"authorized_user\" } Now, they start the test, named \"Find trail in Glassboro with card\" and set the users Lat/Long using action.setLocation. Next they start the conversation with the service the same way a user would, calling \"trail blazer\" and once they get a response, send their first query to the service.  They send the text version of just what a user would send to the service \"Find trails nearby\", and wait for the response.  The response is going to be that the service needs to be granted permission to use our street address, to which they reply 'yes'. After that they validate that the first thing the service responds with is the correct park name, and that the second thing the server responds with is a question if there's anything else.  They also validate the cards that are returned for the title and sub title. Full talk: Best practices for testing your Actions (Google I/O '18) Take away : That's it.  Pretty straight forward.  If you have any familiarity with testing with node, none of this should be black magic.  Build you test just the way the user would send questions to the service and validate the responses. Stay tuned, we are preparing Part 2 for you!", "date": "2018-05-29"},
{"website": "Novoda", "title": "What today's retailers need to do to survive", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/what-retailers-need-to-do-to-survive/", "abstract": "We are living through the biggest period of disruption and change ever to impact the retail industry.  We seem to hear it every week: profit warnings, CVA’s and once-great retail businesses going into administration.  And running in parallel with this turbulence is an incredibly exciting period for retail technology. In short: there’s a lot going on. To try to make sense of all this, Novoda hosted a retail roundtable dinner. Held at the Hoxton Hotel in Shoreditch, the event gathered a group of senior retailers from some of the biggest and best known names on the High Street to discuss the challenges and trends in retail. Here, we share the insights. Disruption In Retail Disruption is everywhere, but from where did this group feel the greatest source of disruption was coming from? The views were varied and insightful - many people highlighted the sheer pace of change as one of the biggest disruptors, along with the ability to move from the physical to the digital and back, in the seamless manner that today’s consumer demands. There were some larger issues - such as constantly changing strategic direction, consolidation, being able to connect all the dots and having the ability to deliver great rich content - and some more specific ones, such as machine learning and IoT. It quickly became clear that disruption has many faces! Amongst the topics discussed during the evening were: What does a truly frictionless shopping experience look like? Much is written about ‘frictionless’ shopping and it became clear that it means different things to different people, and also depends largely upon the journey. Whilst a weekly grocery shop needs to be quick, easy and frictionless, a luxury item purchase can perhaps afford to have ‘friction’ deliberately built into it. What can big data and AI can contribute to the omnichannel journey ? Big data is here - the question is how to access it, interpret it and use it in ways to deliver real value back to the consumer. At the moment, many retailers feel they are drowning in data. It’s through AI (we think of this as Augmented Intelligence, rather than the misleading ‘Artificial’ Intelligence), that we are able to sift through this data. The result is only the useful data, which can help us, rather than hindering us through the sheer volume of it. With good data, it becomes possible for retailers to convey a real sense of purpose that is consistent across all of their channels and touchpoints, and in keeping with the brand at all times. How are the demands of today’s consumer creating the new age of loyalty? Loyalty is dead; long live loyalty! The demands of today’s consumer are such that we are at the same time loyal and promiscuous. One minute shoppers are fiercely loyal, and the next, seemingly having shopping-affairs up and down the High Street! With that in mind, retailers are shifting away from an ordered world of linear planning - this is an approach that simply no longer works. With so many retail structures built in a very hierarchical manner, command and control has never seemed so out of place. Younger start-up retailers have recognised this from the outset, and have never had the hierarchical structures that now need to adapt - it’s simply built into their culture to approach planning with flexibility and fluidity. This puts them in a strong position against the more established players. For them, it is one of the biggest challenges they face. What Trends Will We See In 2018? The evening concluded with a literal roundtable, where the subject ‘Trends Of 2018’ was discussed.  There were some interesting views. The first was perhaps the most insightful and surprising: are we, in reality, moving away from a digital life? With the recent headlines regarding online privacy, many people have recently considered their online life in a way they perhaps hadn’t before. Have we already reached a pivot point where more and more will opt out of an intensely digital world? The discussion already having been taken in a surprising direction, this was followed up: could technologies such as Artificial Intelligence in actuality be blind alleys? Perhaps it would be better to invest in staff and the human interaction rather than digital? Echoing that theme, it was raised that if grocery retailers really know their customers, why do we not have VIP lanes at supermarket checkouts? The retail industry still has much to learn from others, such as hospitality and airlines. Data was never far from the topic of discussion and as a trend in 2018, it was cited as being an enabler to innovation. And another fascinating insight: the value of time. As we become more and more engrossed in consuming digital content, the value of time will become more and more evident. We discussed investing in staff and the experience -  both how the shop floor is utilised to its fullest potential,  and somewhat paradoxically, how digital is fuelling the in-store experience. Automating the mundane seemed a great way to express the ever-growing potential for automation in many aspects of retail, allowing humans to ‘do something more interesting’. The last trend highlighted is perhaps the one which keeps many retail businesses looking over their shoulder: that the awakening that Amazon is really here has come too late for many retailers. Conclusion The sheer pace of change and innovation is bewildering. Where to place your bets, where to make those investments – and at pace: these are key to survival. For those retailers still clinging to the belief that past working practices will continue to work, a nasty shock is just around the corner. Old ways no longer work, and they’re no longer relevant. It’s time to throw away the rule book; tear up those well established command and control structures and have a sense of purpose. Andrew Busby is a retail analyst, founder of Retail Reflections and an IBM Futurist", "date": "2018-05-31"},
{"website": "Novoda", "title": "Community in action", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/community-in-action/", "abstract": "For the team here at novoda, a community is more than just a collection of people - it’s about sharing, whether that’s interests, knowledge or a common goal. That’s why shared learning is something that makes up part of almost every day. Team members regularly hold their own lunchtime ‘Hack and Tells’ to share whatever they’ve most recently learned, be it a new Apple/Google product or a How To session on combating burnout. We even have our own education programme - the Novoda Craft University - which all our engineers learn and then graduate from. We’ve also learnt that knowledge-sharing works best when it extends beyond just novodians, which is why we run or help run meet-ups around Europe - from Design Lab Berlin , to Barcelona’s Women Who Code , to Londroid , which is now the 3rd largest Android meet-up globally. You’ll also find us regularly attend conferences around the world. They provide an unmatched chance to spend an intense few days learning and speaking to others who are as passionate as us about app development, so are always worth the journey. That’s something that iOS engineer, Berta Devant, enjoyed when she wrote about her try!swift Japan experience, and her Parisian dotswift visit. So, with all that focus on community, we knew that when we opened our second UK office, in Liverpool, we wanted to make it a priority to not just have a physical presence in the city, but to give something back to the community we were joining.  We were keen to help both current and budding engineers, designers and coders to develop their skills, and volunteer our time, space and expertise wherever it was needed. Here’s just a couple of community projects that we recently got involved with and invested in... DoESLiverpool DoESLiverpool is an active community of makers, designers, coders, engineers, and many others, who together make up one of the most dynamic groups in Liverpool. Having been founded in 2011, they were on the hunt for new premises, to accommodate their ever growing number of members. They found the perfect spot at Tapestry - a warehouse in the newly minted Fabric District (the city’s regeneration means that companies have a wealth of industrial warehouses that can be turned into digital hubs). When they created a crowdfunding campaign to raise the necessary money, we quickly volunteered to part-fund the project. We got to know about their mission after meeting them at GDG Liverpool , and taking advantage of their old DoESLiverpool space and equipment when we wanted to make something for MakeFest Liverpool. Today, they’re fully funded, and in the process of making the move - and we can’t wait to work with them to run training sessions, host events and support the talent that makes up the DoESLiverpool community. Design Club We also got the chance to support the next generation of designers, through the brand new Design Club. Founded in 2017, they’re focused on teaching young people design thinking as a life skill.  As well as the creative side, this include soft skills, like collaboration, empathy and problem-solving. Volunteer-run, it’s in answer to the lack of educational opportunities for budding designers (design has recently fallen out of fashion in education, often in favour of coding). Again, this presented us with a fantastic opportunity to deliver on our community promise.  With novoda’s commitment to education and supporting future generations, how could we not become founding partner in this exciting and badly needed educational resource? Work has just begun with Design Club, but we are excited to work with them to build educational resources, host workshops and get involved with mentorship programs. We’ve seen so many benefits to supporting our local community (both geographically or based around similar interests), and can’t recommend it highly enough. And it doesn’t have to be a financial commitment - community groups are often looking for volunteers, speakers and even space to meet. Whatever form your support takes, it’s likely to be very warmly received - and is guaranteed to give you a warm glow, too! And if you’re interesting in more information around our communities and events you can email us direct on events@novoda.com", "date": "2018-05-30"},
{"website": "Novoda", "title": "Documentation Deep Dive: How we updated Flutter for iOS Developers", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/documentation-deep-dive-how-we-updated-flutter-for-ios-developers/", "abstract": "One of the great things about working at Novoda is that we often get to use the expertise we’ve gained to help the wider developer community. Sometimes that takes the form of attending meet-ups and speaking to other developers, while at others, it becomes something more formalised. Last month saw us working on the latter, when Novoda was commissioned by Flutter to write Flutter for iOS Developers. Flutter is Google’s mobile app SDK for crafting high-quality native interfaces on iOS and Android. It’s come a long way in the last 12 months, moving from early alpha to production ready beta. To keep up with this epic pace of development, Flutter needed to update the documentation, and we were on hand to make that happen. The documentation itself lives on the Flutter website . As a company that often uses Flutter, with engineers who are constantly exploring new technologies, Novoda was ideally placed to help update that. We have several team members who regularly use Flutter, including Sebastiano Poggi, one of the few GDEs in Flutter Google Developer Expert for Flutter and Identity, and Niamh Power, an iOS engineer who has hosted training sessions and spoken at meet-ups on the subject. In short – we had the right team to take on this challenge. Like any Novoda project, we approached the task with an initial meeting to discuss the scope of the work, before deciding together how we would split it up. This allowed us to leverage our normal toolset and use the best practices that underlie all Novoda projects. One change from the norm was our use of GitHub ‘Projects’ feature. This is a lightweight version of our normal story tracker, Jira, and we found it lent itself well to small self-contained documentation tickets, that didn’t necessarily need estimating or detailed logs or discussion attaching. When a ticket was complete, it was then reviewed by three different engineers, before a pull request was made on the Flutter website repository. This meant that any mistakes were spotted early. Making sure that people in different roles reviewed the documentation further helped with this - different viewpoints often means that different errors are spotted, and ultimately, all are caught. We weren’t starting from scratch with the documentation – Flutter for Android Developers and Flutter for React Native Developers already existed, which gave us a great base to start from. In fact, we could use the Android document to form the skeleton from which to build out the iOS documentation, allowing us to duplicate the topics that were similar, and maintain consistency in layout between the two, which would benefit developers reading both later down the line. For instance, the Navigation section could be duplicated, whilst adding Localisation of Strings which was iOS specific. We adjusted the order of topics between the Android and the iOS version, due to some iOS topics being more closely related than in its Android counterpart. This allowed us to cater for the typical iOS engineer’s base iOS framework knowledge, and to create a more readable document for the intended audience. You can find the results online: the full document is here, and the github repository here. . Now the documentation is live and Flutter development work is continuing, there are always more updates that can be done, but getting to this point provides a baseline of documentation that any new engineer to Flutter can use. In fact, due to the speed of Flutter’s development, there are already some new features that we can add, regarding Web Views and maps in the near future. Flutter at Google I/O After we completed the work, Novoda attended Google I/O.  While there, it quickly became apparent that Flutter was a hot topic of conversation, despite no big feature announcements being made. There were plenty of talks on the subject, which have definitely enticed a whole new group of developers to try the platform out. There was also the Could, Firebase & Flutter tent, where you could find members of the Flutter team willing to answer questions regarding the technology. For the Novoda team, it was great to meet the people who had been reviewing our pull requests and providing feedback on the work. The future of Flutter Here at Novoda, we’re excited to see the next year for Flutter. It’s got so much potential, and is such a fun and fast way to develop great looking apps. Dart as a language is relatively easy to pick up, and by using an IDE like IntelliJ, you’re helped along the way. There are also some fantastic codelabs straight from Google on getting started, if this post has piqued your interest. You can check them out here. .", "date": "2018-06-12"},
{"website": "Novoda", "title": "WWDC 2018 Recap: what did Novoda think?", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/wwdc-2018-recap/", "abstract": "June has come around again, so for iOS developers this means one of the most exciting events of the year — Apple’s developer conference, WWDC! For the lucky ones that get to go to San Jose, it’s an opportunity to meet up with old friends, make new ones, and get to spend valuable time talking to Apple engineers. For those who don’t get a ticket, there’s no worry — all of the sessions are recorded to watch whenever it suits. Meghan Kane was able to fly to SF and attend WWDC this year 👩‍💻 Our team of iOS developers pay close attention to what Apple announce, in order to figure out what will be valuable for our clients and what we can do to push our projects forward. But what our developers really think, and what were the sessions that caught our attention? Alex Curran IOS Developer Overall I felt that WWDC was good but didn't have much of a \"wow\" factor. Now that we're on the 12th version of iOS, there isn't much that Apple can really improve or change. One thing I like about iOS 12 is the Digital Wellbeing tools, and I hope this is something Apple will improve in future iterations of iOS. I wish they would make more focus on the development tools however, as I feel they're still lacking. My favourite talk so far was A Tour of UICollectionView , as it was a good overview of the tool. It included a lot of optimisation tips and some interesting thoughts about how to make scrolling really smooth and how you can accidentally impact the performance of your app. I wish they'd covered layouting, as that is always the bit I struggle with, but I definitely understand how to use collection views better. A Tour of UICollectionView Guiseppe Basile IOS Developer For Apple, iOS is now a stable platform and they did a great job on the optimisation of the performance. The accusation on the planned obsolence are now the past, the new iOS 12 runs on the same devices as 11 and they also focused a lot of performance, actually making the life of older devices even longer. My personal favourite is Siri Shortcuts . Siri Shortucts is a new way to expose the actions available on your apps to Siri. These actions can be performed by the user using customizable voice commands, or they can be combined with other actions to create useful workflow. The most interesting thing is that Siri will learn to predict these actions observing how the user use your app, and it will show these actions in Spotlight, Lock Screen on on the Watch Faces. Introducing Siri Shortcuts Meghan Kane IOS Developer I was lucky enough to be in San Jose during WWDC week this year. It’s a special week, because we get to learn about the new improvements Apple has been working on for the past year and it’s somewhat of an intense reunion with developer friends (old & new) from all around the world. My favorite WWDC 2018 talk was “Metal for Accelerated Machine Learning” (presented by Anna Tikhonova). New this year is Metal’s expressive graph API to describe your neural networks simply in order to do training your laptop’s GPU (it will target several types of GPUs). While there are many good options for training a neural network (e.g. the high level Turi Create by Apple or TensorFlow by Google), Metal is a solid option for training that offers flexibility high performance, and simplicity. Additionally, MPS can be used to power training with Turi Create or TensorFlow. This allows you to harness the performance gains offered by some GPUs (e.g. as she showed using an external AMD Vega GPU). It is clear that Apple is investing more resources, and thus, their future into Metal, so this is an interesting space to watch. In addition to presenting the new ML-related improvements in Metal, Anna gave a thoughtful overview of the steps involved in training a ML model. If you’re looking for a concise explanation of convolutional and recurrent neural networks through the lens of an Apple developer, don’t miss this talk. This is a refreshing break from Apple’s standard “black box” approach to explaining ML. I’ve been hoping Apple would move away from the “black box” explanation approach because it is too opaque, oversimplifies ML, and inhibits developers’ learning journey into ML. So, a shoutout and big thank you to the Metal team for this highly educational talk! “Metal for Accelerated Machine Learning” Eduardo Urso IOS Developer The WWDC keynote this year wasn't full of great and heartbreaking news but apple did do an awesome improvement on iOS 12 making it lot more performant than the older versions, also Xcode 10 has got a great enhancement on refactoring as well as on the source control tools. My favorite talk so far is Creating Custom Instruments as they dived deep into how instruments work under the hood. We can now use StandardUI and Analysis Core frameworks to create our own instruments which allows you to create a custom configuration(instruments) that will be able to analyze and profile your application, the same way all the built in instruments do. Creating Custom Instruments Fabia Schäufele IOS Developer So, WWDC definitely wasn't about blockbusters this year, which doesn't mean that it was boring. One announcement that excited me as a developer this year was the promise to release an iOS-to-macOS porting framework soon. Being able to develop for both platforms at the same time more easily opens up new possibilities for both existing and new features. Another development I, as a user, really liked is the continued commitment to data privacy. For example stronger protection for data transfers via devices’ Lightning ports, reduced tracking and fingerprinting options in Safari, and more convenient two-factor authentication. If you want to learn more or find out how you can improve your apps, watch Better Apps through Better Privacy . Better Apps through Better Privacy Pascal Drouilly IOS Developer This years was not a dev focused WWDC like those where CollectionView, Autolayout, StackView or Swift were introduced. It was more user facing changes. I will give them a pass this year because of the groundwork done on stability, speed and preparation for iOS to run on MacOS. But I expect a big announcement next year! I especially liked 3 sessions that were more revision but good ones: Data You Can Trust Swift Generics Embracing Algorithms The last one with this wonderful conclusion: No Raw Loops! Berta Devant IOS Developer I liked the Keynote this year because they seem to be focusing more on stability over shiny new features but Apple still did no address one of the biggest issues for me which is the lack of acknowledgement to the entire developer community. Apple has had a track record of selling new API as this big leap for developers when other systems already implement them, and even worse sometimes without acknowledging the contributions of Open Source community. And I still keep waiting for a year that Apple announces a developer program similar to GDG for Android. Where you can test your skills and get a community of ios developers back by Apple. But I did love some of the talks and the new things they introduced at WWDC 2018. My personal favorite one is What is new in ARKit 2? and Integrating Apps and Content with AR Quick Look . With new ARKit 2 API they proved that Augmented Reality is not a gimmick API anymore but a really powerful piece of technology that can completely change how users interact with our apps every day. They brought 3D object detection and Image detection that follows the image even when in movement, all of that running on the user's phone and completely integrated into ARKit. They also brought environmental texturing that allows for reflective models to reflect the AR scene completely automatically done by ARKit. They also improved world tracking and added the functionality to be able to share your entire world map and scene with other users or persist your session and experience. All of this changes and how powerful the code behind is show that Apple has a clear commitment to AR and that it will keep pushing it to get more apps crossing into a new reality. What is new in ARKit 2? Bart Ziemba Mobile Software Tester / QA I was particularly interested in what Apple is going to announce regarding testing. There are a few things that got introduced and demoed with Xcode 10. They are placed in a new, little menu Firstly, test parallelization. With Xcode 10, Apple decided to change the approach to parallel testing - Parallel Destination Testing - that’s been introduced in Xcode 9 by applying Parallel Distributed Testing. Instead running tests one after the other, Xcode will create a virtual clones of the same simulator and will simultaneously distribute and execute the tests classes to them. Thanks to that the execution time will decrease massively. This feature is supported in Xcode and xcodebuild. Secondly, test ordering. Be default, tests run accordingly to their name. It means they always run in the same order until you rename them. By selecting “Randomize execution order” tests will run without specific dependencies. It will push users who write and organize test as deterministic as possible. At last, test selection. With the Xcode 10 users will be able to control which tests they want to run within the scheme. Thanks to that user can decide to run only particular tests for one scheme or skip them", "date": "2018-06-21"},
{"website": "Novoda", "title": "Tech For Good: How individuals and the industry can drive positive social change", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/tech-for-good/", "abstract": "When your job doesn’t appear to have any immediately world-changing effect, it can be easy to feel powerless in an increasingly complex world.   But everyone has the potential to do good for their community and beyond, and working in technology, that’s particularly true. At Novoda, we’ve believed in making positive social and environmental change part of our mission since day one, which means we’ve been faced with many of the challenges that this presents any business trying to scale. That’s one of the reasons we co-hosted an event with banking start-up, Monzo, around Tech for Good . The idea was to both inspire individuals to take action, as well as to provide some ideas as to how businesses at different stages could do so. Doing good: as an individual It was apparent right from the beginning of the evening that there’s something about the tech community that breeds side hustle.   Perhaps it’s because it suits the inquisitive mindset of Novodans that most of us have at least one side project going on at any one time. Ryan Bateman, Novoda’s Head of Product , recently spent a weekend on a hack project to nudge users towards offsetting their carbon emissions when they bought a flight through Monzo. He used flight data from the API to work out the length of a flight, the resulting emissions and how much you’d need to donate to offset that - which would appear in your Monzo feed as ‘Offset your flight to X?’. What comes next is still up for discussion: enabling donations from within the app, creating a ‘Pot’ within someone’s account to store charitable donations, or something else entirely.   There’s a lesson for us all in not being intimidated by the task of building a complete solution - it took just two days to build something significant, which now has the potential to be built upon by someone else. Working in the tech industry means that we’ve all got skills that can seem like witchcraft to the uninitiated, and those are skills that most charities would love to take advantage of.  Take chatbots: This Guardian article discusses how they could transform how charities interact with people - in everything from educating people to soliciting donations - but smaller charities often don’t have the skills needed to make these kinds of projects a reality (and even the bigger ones will find it difficult to prioritise these kinds of projects).  Volunteering your time to build something efficient for a charity close to your heart can have a real impact for the people they help. Novoda CEO, Kevin McDonagh , has himself worked with Apps for Good , teaching children as young as seven how to code.  The resulting apps truly changed lives - Cattle Manager went on to win awards, and was made by a group of school children who lived in a rural farming community, who wanted to help the farmers manage their herd digitally rather than via mountains of paperwork. Kevin shared his main lesson from his time with Apps for Good: that the real purpose of mentoring or teaching isn’t just about teaching someone a new skill - it’s about giving them the confidence to try something new. Whether they go on to become a master coder or not is a secondary concern to teaching them what it’s like to experiment, fail, iterate and try again. Hackathons are something we’re all familiar with, and it’s becoming more common to make sure the resulting hacks have social or community value.  We worked on a hackathon with RNIB , a charity that helps people affected by sight loss, and it was one of the best we’ve ever run. One of the teams hacked the Sky listings to make them accessible to those with limited sight, which RNIB then took to Sky and had integrated -  so, that one weekend has had a positive effect on partially sighted people all over the UK. (A side benefit for the Novodans that attended was to see the different ways that partially sighted people use technology. It gave us all a much greater appreciation of accessibility, and, as a result, it’s something that we consider in our projects much more today.) Doing good: as a company Whether you’re a start-up looking for funding, or years into running a company, focussing entirely on the business side of things is always tempting. But we’ve found through experience that embedding a desire to do good right from the very start means that your social impact grows as your business does. Any business with resources has the ability to offer them to others. Community groups will often be looking for space to hold events, and sometimes, lending your time to organise something can have the largest impact of all. Providing a place for human connections to be made is as an incredibly important social endeavour. At Novoda, we organise Londroid , Europe’s biggest Android meet-up, which provides anyone in the developer community with a chance to connect with likeminded people. Providing truly flexible working can feel like a risk for a business with ambitious targets, but we’ve found that the pay off can be huge.  By allowing sabbaticals and offering proper flexible working, businesses create a structure that allows employees to do things to support their own mental health, whether that’s spending three months volunteering abroad, taking Fridays off to work with a local charity or just sharing the childcare burden with a partner. Not only does this make you an attractive employer when it comes to finding the very best talent, that person’s return is likely to come with the kind of renewed enthusiasm that only comes from stepping away from the office environment for a while. Alex Curran is an iOS and Android lead at Novoda , who recently took a one month sabbatical to build an app called Aid. He knew that there were people like him who wanted to give to charity, but who found it difficult to quickly find a suitable charity to donate to. The resulting app made it easy for people to find out about a charity that aligned with their interests, and to pay them in just two clicks, reducing the friction between that moment of wanting to do something good, and the actual donation. He’s now in the process of testing Beta versions for both Android and iOS - if you’re interested in trying them out, you can message him here . Monzo themselves have a social mission as part of their business: making banking accessible for everyone. There are two million people in the UK without a bank account, often because they don’t have the documents that banks require. Without a bank account you can’t rent somewhere to live. Don’t have somewhere to live? You won’t have the necessary documents to open a bank account. It’s catch 22. Monzo believe technology can improve that situation, and so fundamental to their product has been building the technology to let people open an account without a wealth of documents - in fact, it takes people just a photo of their ID and a short five second video of them to open one. So while it’s certainly in Monzo’s interest to have as many customers as possible, they can also have a profound effect on those two million people without a bank account, who will be able to access services and function in society as never before. It’s this kind of social mission integrated into a broader, more commercial, business plan that is increasingly common in today’s start-ups.  People, whether they’re consumers or employees, want to know about how a company is giving back, and it’s those that have a degree of formalised thinking around this that will attract the best talent. And on a personal level, giving back is a way of offsetting what might feel like a 9-5 spent decidedly not changing the world. Because realistically? We can’t all work for the Red Cross - and that’s ok! If you’d like to learn even more about what those in the industry have been doing to change the world, check out the entire evening’s presentation:", "date": "2018-07-17"},
{"website": "Novoda", "title": "ADDConf: Two disciplines, two locations, one talk", "author": ["Berta Devant (IOS Developer)", "Chris Basha"], "link": "https://blog.novoda.com/addconf-talk/", "abstract": "Last week we  were invited to speak at ADDConf in Barcelona , Spain. ADDC, the App Design and Development Conference, aims to create an opportunity for mobile designers and developers to meet, find new ways to work together and get inspired in an open, inclusive and collaborative space. At Novoda we strongly believe in collaboration between product design and development. Having a team with good communication between both disciplines improves the product development process and leads to a better result. So when we heard about a conference that tackles both, it seemed like the perfect place to present our experiences with Augmented Reality. ADDConf on day one During the two days of ADDC, there were talks aimed at designers and talks aimed more at developers, but there were no separate tracks for the talks, designers listened to developers and developers learned from designers. This was a very interesting format because it puts people in the position to learn new things from completely different perspectives. One of the talks about designing experiences for women without using the colour pink was received very well especially by developers who had never really thought about colour theories and adaptable design. Preparing a talk for designers and developers This created a unique challenge for us: Creating a talk that could benefit everyone in the room, designers, product people and developers, with the aim of bringing them forward on their understanding of Augmented Reality. Since we had been working on an AR investigation together, we were well prepared to talk about both sides: Chris had paired with Berta on the ARKit code and Berta had helped Chris with designs constraints for the experience. We wanted to say so much about the topic, but this was a introduction talk on Augmented Reality that did not allow enough time to go really deep on either design or development. Chris presenting how to design a model 💪 The biggest challenge when preparing this talk was making sure that we gave the same amount of importance and time to each subject, to make a compelling and interesting talk whilst striving for as little technical or design specific points as possible. Another challenge was the fact that Chris is based in Novoda’s London office and Berta is based in the Barcelona office. We do a lot of remote work at Novoda, but creating a conference talk remotely was a very new experience to both of us. Creating the talk remotely Putting together a talk and a slidedeck while working from two separate countries was a unique challenge, but because both of us had participated in the investigation on Augmented Reality, we were confident in our knowledge and the content we wanted to talk about. Opening Statement of the talk 👩‍💻👨‍🎨 We started out with a rudimentary skeleton of the talk. The basic idea was that we were going to split it into two main parts: designing the experience and then building the app. The roles were clear since the beginning. After laying out the skeleton, we continued by adding more detail to our topics and targeting specific areas that would benefit the attendees of the conference, designers and developers alike. We ended with a simple format: an introduction to AR technology, followed by a walkthrough of creating 3D models for the experience, building the AR app, ending the technicalities with a few UX practices, and finishing up with an encouraging message to get the audience started. Having all our topics figured out, we started drawing out the slides. We tried to keep the slides as concise as possible, but at the same time very descriptive of what we were saying on stage, with supporting points and imagery. Types of AR slide of the talk Check out the full slides from the talk on Slideshare To get ourselves ready and familiarised with the talk, we rehearsed our parts alone. The first time we rehearsed together was when we met up in Barcelona. Before we went on stage we had rehearsed together about 3-4 times, and we felt very confident with our talk. Turns out, we did great! The talk Augmented Reality has been a hot topic for a couple of years, but with the introduction of new APIs by both Google and Apple we can now place an AR experience in every user hands, allowing your users to interact with your app or data in a complete new way, augmenting their reality and, with it, what they can do with apps and how much time they spend on them. For the talk we prepared a presentation with some of the knowledge and experience from these past 6 months of investigation into AR technology and design. Berta started the talk by explaining what AR is and what types of experiences you can build and how important is to choose and narrow down your purposes and types to be able to build a cohesive AR experience. Berta Presenting what you can build with AR Chris took over the talk explaining how to design models and assets for AR experiences and how different it is from designing common 2D screens. Chris presenting the importance of shadows Berta then went over a little bit about the technology behind AR SDKs from both Google and Apple smartphones, what they had in common, where they differ and which one to use depending on the project. Finally, Chris finished the session talking about how to design AR experience from a UX perspective: besides the model, you need to think of the experience with a complete different type of design process than for normal apps. He gave a few good UX tips and tricks for fluid interactions. Chris Presented how to design UX experiences for AR If you want to know more about Augmented Reality and what we have been working on at Novoda check out Augmented Reality: From Design to Development , where Berta and Chris will walk you through their AR talk and explain a bit how to build for AR technologies. Don't forget to have fun 🚀", "date": "2018-07-23"},
{"website": "Novoda", "title": "IoT at Liverpool Makefest 2018", "author": ["Paul Blundell", "Xavi Rigau"], "link": "https://blog.novoda.com/iot-at-liverpool-makefest-2018/", "abstract": "Liverpool MakeFest is a family-friendly event celebrating low level technologies, hardware, hacking and making. Here's our review on what we did and what we learnt. We participated with a stand at the 2018 event and built an immersive Internet of Things gaming experience, called Dungeon Crawler to engage with those attending and learn more about Redux, Firebase and Android Things for ourselves. We have been participating with MakeFest for the last 3 years and every year it gets bigger and better. We had a lot of people visiting our stand all with a wide range of knowledge, which made for some great conversations. People played the games and were really interested in what Android Things as well as the Redux architecture is all about. Most attendees of MakeFest were families with children aged 7–17, which is why we created an IoT game and gave away Android Things prizes to spark their interest. We wanted to build something that would be engaging to people of all ages, something that we would want to play with ourselves if we had one in the office. So we decided to build an IoT dungeon crawler game. We based our game on the original Line Wobbler and on Twang which is an open-source version of the game, built for Arduino. Dungeon Crawler uses an LED strip as a display, where the player, enemies, and other game entities are represented as different LED colours. The player can be controlled with a joystick that’s built using a spring from a doorstop, which includes a gyro & accelerometer sensors to determine the movement speed or if the user is attacking. Paul playing a sneaky game before MakeFest ended We ported all the game logic to Java/Kotlin running on Android Things and once it was working with the hardware, we decided to take it one step further to explore how we could make the experience even more engaging. We considered multiple options, but one that really caught our attention was having a replay feature. After someone plays a game, the game stores the replay as JSON in Firebase in order to replay it later on. Using the Redux architecture enables this feature as we store a list of game states (one for each frame) as the game progresses, and at the end of the game, we store the list of states in a Firebase Database. Additionally, we built a companion app, which can run on a regular Android device. The companion app observes for new game replays in Firebase and when there’s a new one, it pulls it and replays it on the separate tablets screen. The companion app replaying a game MakeFest is such an enjoyable experience, all of the parents and kids that come along are so engaged or interested in technologies. It bodes well for the future of the industry. Personally making our DungeonCrawler game work was a big challenge in understanding low level details but also modular architecture and I’m glad we pushed it to the edge, as it was great learning for myself and watching the kids faces light up when they won the prize was amazing. Paul Blundell This was the first time at MakeFest and I’ll definitely repeat. Initiatives like this one are very much needed to get kids interested in new technologies and in the ‘maker’ spirit. I was able to sneak out of our booth a couple of times to see what was going on around MakeFest and I was impressed to see all the different technologies being showcased in a way that kids were able to interact with and get excited about. And seeing how excited (and competitive!) some were when playing our game was priceless. Xavi Rigau Some of the other interesting stands we noticed included: CPC, they had an awesome kids crane that they had retrofitted with new controls and more powerful motors. There was a brilliant DIY built ‘hammer bell’ game that the kids could wack, and a table of magnified cells that used all recycled materials to show cells at a gigantic scale. DYI built hammer bell game On the top floor there was a drone section where kids had to program the drones using building blocks from a Scratch-like language and someone had made a punchcard reader that would create Minecraft structures shaped with the wholes in the punchcard. Really great stuff! Kids learning about robotics by playing Attending MakeFest had two aims for us: We love helping the community, and seeing young people engaging with technology is a real inspiration. We enjoy helping to spark creative ideas and showing how easy it can be to get into technology. Secondly, we are always learning and trying to push the boundaries of our own knowledge. Working with hardware is not a daily occurrence yet for most of our projects, so working on a new project for MakeFest really pushes us to learn new things. We can't wait for next year. Each year we learn more and want to make it more awesome next time. We already have some bigger and crazier ideas, the trick is to start planning right now. If you have a platform or product and are interested in collaborating, feel free to reach out to us. Stay tuned! Our maker badge! Want to build this project yourself? Check out the project on hackster.io", "date": "2018-07-26"},
{"website": "Novoda", "title": "The (re)making of a Download Manager", "author": ["Team Novoda (Joint Thinkers)", "Daniele Bonaldo (Android, Wearables and IoT GDE)", "Ryan Feline", "Ferran Garriga (Senior Software Craftsman)"], "link": "https://blog.novoda.com/download-manager-v2/", "abstract": "Android allows developers to add download functionality to their apps, but if you want to extend that behaviour, or customise it to your liking — that’s not easy. That’s why we created our improved version of the Download Manager . We started working on a fork of the AOSP Download Manager, but there were a number of factors that led us to create our brand new version of the download-manager . First of all, at the time of creation the standard AOSP library did not allow writing to private storage. Second, the nature of AOSP code can at times be confusing to maintain and add features to, which is particularly troublesome to new developers contributing to the project. The original download-manager from AOSP did not allow clients to implement batching of downloads, meaning that each downloading file would have had its independent notification. The lack of batching required hacks in client applications, that led to flickering notifications when quickly showing and hiding individual ones after each file was downloaded. The new version of Download Manager hinges on the concept of downloading batches of files; now, multiple file requests can be grouped into a single download batch. With this approach, client apps can make it opaque to the end user that a download is composed of multiple separate items, instead having a single update notification displayed for the whole batch. Notifications are now also built into the library: Download Manager v2 intelligently shows notifications that are persistent, dismissible (stackable), or hidden, based on the download state. Notifications can be fully customised, through the NotificationCustomizer interface. Client applications can customise what information to show in each notification, or hide entirely the notification for some specific status. An application, for example, might be interested in showing a notification only while a download is in progress, hiding all other notifications, while another might want to notify the user also when the download has failed, or is completed. The original version of the download-manager made use of ContentResolver s and ContentProvider s to provide access to their data, a sign of it’s time, this design pattern has fallen out of use, in favor of newer, more flexible, less boilerplate solutions. In the new version, this pattern has been replaced by a persistence layer based on Room . Now, all data is treated as private, except for what is part of the public API. In doing this, we improved how client applications can query the status of current and queued downloads: while the original download-manager relied on the BroadcastManager mechanism to dispatch updates, now there is a public API to fetch the status of all downloads, and to register a callback for updates to a DownloadBatchStatus . The frequency of these updates can be throttled through the use of the FileCallbackThrottle interface; time- and progress-based throttles are already built into the library, but client apps can define custom ones as well. The client app can also specify constraints regarding the network connection to be used: downloads will start only when the device is connected to the desired network type (metered or unmetered), pausing and resuming automatically when the network status changes. The old, AOSP-based, download-manager is used by several published applications and we thought about that when working on the new version. A migration interface is currently in development to ease the transition between the first version and the new, completely rewritten v2. In the meantime, it’s already possible to migrate the downloads handled by the old Download Manager into the v2, if you upgrade your app. To perform a migration a client should inform the library about completed and partial downloads. It’s possible to do so by providing a CompletedDownloadBatch and passing it to downloadManager.addCompletedBatch() or a VersionOnePartialDownloadBatch passing it to downloadManager.download() . It is possible to see the full migration flow in the demo application . We originally had planned an automatic migration mechanism, but client applications had complete freedom to use the database columns in any way they wished, and this makes a \"one size fits all\" solution impossible to provide. The demo app, showing how to use most of the features of the library Architectural Design While working on our fork of the AOSP Download Manager we faced several difficulties when we tried to fix bugs, extend its functionalities, add tests and in general with the readability and maintainability of the code. Based on that experience, we wanted to achieve the following: Clear and good separation of concerns so that we have small classes with very a narrow focus Prime performance by avoiding unnecessary disc operations as much as possible Allow the library to be extended by the client apps Use the tell don’t ask principle, so that classes expose the minimum amount of information and encapsulate all the logic in them Use interface wherever possible to avoid leaking or depending on implementations Provide common functionality out-of-the-box including notifications, recovery on network failure and migrations from the previous version of the library Provide a demo application with good examples on how to use the library Hard lessons we learnt A task like downloading a file looks very simple, but it is actually a very complex one to achieve properly. There are many things that can go wrong and many statuses to consider. Besides that, our requirement of downloading batches of files in bulk adds lots of complexity. Writing to disk too fast might block your device , or make background tasks too slow. Too frequent callbacks might completely collapse the system. We have implemented a throttle for our download callback because otherwise having a callback that will end up with a UI notification every frame will completely grind the system and the host app to a halt. Updates on downloads progress are impacting the UI, and can happen very fast, very frequently, so it’s important to be careful. Backward compatibility and data migrations . As a download library we wanted to provide some sort of continuity between the previous version of the library and the new one. It wasn’t acceptable to offer a new library that ignored any already downloaded (or queued) content by a previous version. Thus, even though we wrote this new version from scratch, we spent a considerable amount of time thinking, implementing and testing a migration process that means updating an app to the new version would not mean losing all the previously downloaded data. Threading is complex, and robustness is important . It's easy to run a task in a thread and wait for its result. But it gets complex when you need to interact with a thread immediately and move on to the next task in a queue of tasks that are batched. For example, a file might be downloading in a background thread but the user might want to pause or delete it. Changes on statuses from background threads to the main thread and back can happen extremely fast. You have to be very careful with the flow of your states and how those statuses might change in the middle of an execution. Storing statuses in memory is faster than using disk persistence, but we had to consider what happens if the client app is suddenly shutting down. How are you going to recover the pre-crash state, and what’s considered an acceptable loss of data? Testing is hard and the real world is harsh . We spent a considerable amount of time smashing buttons in the screen trying to make the demo application to crash and we succeeded way more often than we’d hoped. By now we are true pros at it! Even with the most rigorous development and testing regime, there will be many scenarios you didn’t think of that you need to go and fix. Some of them might have a considerable impact on your implementation. If you find that something is fundamentally wrong, you might have to go back to the drawing board, sketch all your system again and think a better way out. Recovery is tricky . There will be many different reasons for which a download might fail. It could be a network issue, a server error, or your device has run out of space, or even more obscure errors such as out of memory and concurrency errors. Think of a clean way to recover your system when the library starts after a crash or a bug. We made the decision of not writing every single status change in the persistence layer, but we write the important events that will allow us to find out what went wrong and how to recover from that status. If we managed to intrigue you, head to the official repository and give our new Download Manager a try!", "date": "2018-09-18"},
{"website": "Novoda", "title": "Organising icons in design systems", "author": ["Chris Basha"], "link": "https://blog.novoda.com/organising-icons-in-design-systems/", "abstract": "The design team at Novoda has been dabbling with Design Systems in Sketch lately. Design Systems work as a unique source of truth for every imaginable component of an app’s interface and are quickly becoming an essential part of UI design as they allow easy management and maintainability of design files that inherit from it. We’ve been learning as we go, and we absolutely love them. Over the course of a few months, as we’ve been building design systems, we made sure to document our findings internally. In this blog post, I’m going to share how we organise and manage icons in a design system to help make your design process more efficient. Free tip of the day: The secret is to think like a developer! First things first, what’s the problem? A Design System should be built with reusability and adaptability in mind—that’s its purpose in the first place. In retrospective, this is exactly what I completely disregarded when I started adding icons into the design system I was building, much to my surprise. I started with a simple shape in an artboard, and a colour override on top, so I could change its colour on the fly. I ended up using this method for most of my icons, but I was slowly starting to realise that it was becoming increasingly difficult to handle multiple sizes, multiple layering, exportability and even cases where the same shape was used for different icons but in different orientations (such as arrows). A lot of problems started to surface, and a sense of tech-debt was creeping behind me. ‘Special cases’ were required every time I had to use an icon that we already had but in a bigger size. They would fail to appear on Sketch’s override menu too, which would result in more work. Zeplin only picks up the defined specs of a symbol , so that if you change its size or contents dynamically through the override menu, Zeplin disregards that and the export turns out to be all wrong. A lot of ambiguity was introduced when the burden of choosing the right icon was left up to symbols housing those icons, as we wouldn’t have clearly defined styles. Here’s what we came up with… High-level solution Before diving into specifics, let’s first get an idea of what exactly it is that we’re doing. When you’re working with a design system, cases where you’ll have to use a standalone icon will be pretty rare. That is because, ideally, the system will already include a component that houses an icon, so you will only need to change the icon through the override menu. What we want to achieve is to have a way to build icons but also edit them easily if we need to, while also simplifying our workflow when we use them in our design files. The system’s breakdown might go a little bit like this: Starting from the smallest components and going up to the biggest, there’s Colour symbols, Icon symbols, Text symbols, Components that combine all of the previous, and maybe a sticker sheet where components themselves are combined. When you paste from the sticker sheet to your design file, all components will remain linked to the Design System. Now, we’re focusing on a low-level element, namely the Icon symbols. We’ll break them down into 3 parts: Raw elements, which will be the source of truth Templates, where sources get combined Final icons, where templates are used to create every icon in the icon set 1 — Have a single source of truth for every shape Every icon shape should be its own symbol, with a colour override on top. Doing that is simple; turn your shape into a mask and place a colour symbol on top, aka an override. The colour override symbol can be a single colour, a gradient, or even your own arrangement of multiple layers. This icon symbol will be the single source of truth for that shape. Use a consistent size for these, something like 24x24 pt. Remove the fill from the shapes so that the colour override doesn't clash with the shape's colour when there is some transparency. Any sort of decorations, such as a circle background that goes behind a shape, should be a standalone symbol as well because even decorations deserve their own source of truth. If you have identical shapes but with different orientations (such as a thumb up and a thumb down), keep only one of them as the source of truth. This is so that when you need to edit the shape, all icons of that shape will be updated too. Since these are the most low-level building blocks of the icons, editing these shapes will affect all other symbols which they are linked to. 2 — Introduce templates This is where we assemble the types of icons. Are your icons 24x24 pt? Make a template of that, and throw one of the icons inside. Do you have some icons that have a background circle? Make a new template that includes the circle and an icon on top. Are some of your icons 72x72 pt? Hell, make a template for that too. Don’t be afraid to use templates inside templates if you need to. The way this benefits us is that we’ll have clearly defined styles for every type of icon, so we never need to assemble icons on the fly, or remember the exact specifications for a style. It will also give you a good idea of the different styles of icons you use in your product! 3 — Use the templates to create every possible icon Now that the templates are ready, use them to create every icon in the icon set. For example, if you have four icons that use the 'Icon on Circle’ template, make a symbol for each one of them using the template as a base. Now, lock the template layer inside, and make that symbol exportable. In this way, you’re securing the final appearance of the icon and making it ready to be used by developers exactly in the way they would need to. The icons in the system will reflect perfectly the icons in the code repository. Finally, all you have to do is use the highest level icon (step 3) in your symbols, and everything will work perfectly. Zeplin will work flawlessly, the developers will love you, and you will save yourself a lot of headaches.", "date": "2018-08-16"},
{"website": "Novoda", "title": "Designing experiences for professional collaboration", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/designing-experiences-for-professional-collaboration/", "abstract": "As product designers, we spend our days crafting experiences. We take care to understand people, their environment and their needs to make these experiences as enjoyable as possible. This is a mindset that can be particularly powerful when considering professional collaboration. Take the simple act of meeting to exchange ideas: what if instead of merely arranging your next meeting, you were to design an experience for everyone involved? The way a collaborative session is designed directly impacts attention, engagement and motivation; it’s the difference between fostering indifference or driving ambition. Where do I start? Practice active listening Talk to those people most impacted by the topic. Ask intentionally open questions and practise the art of active listening. This means making a conscious effort to listen attentively, paying attention to emotion and delivery. A deep understanding of current experiences will allow you to design a customised, impactful session that covers the most pressing issues. Eg. The team appears to be lacking clarity on product vision and prioritisation, causing frustration and reduced motivation. Define purpose & goals From your listening sessions, conclude the key topics or challenges that should be covered. Consider what the session should ideally achieve and how the outcomes of the session might benefit those people involved. Eg. The purpose of the session is to understand current working practices around product vision definition and prioritisation, identify challenges and define first steps towards improvement. Determine success Considering the discussions you’ve had so far, define what success might look like for this session. Consider ideal outcomes in terms of reflections, discussions, learnings and action points. Defining clear goals will help to keep the session focussed, provide a guideline for reference throughout the session planning and help you to reflect on your achievements. Eg. Success for this session would be every team member having had the opportunity to discuss the challenges around product vision definition and prioritisation, and define immediately actionable tasks to improve working practices and reduce frustration. Once you’ve understood the people and the situation, identified the purpose of the session and defined success, you can start to plan your approach. How should I structure the session? To find the optimal session structure, think back to what you learnt during your active listening preparation, and the goals and success metrics you decided on as a result. Different structures are suited to different outcomes - here are some examples to help you choose the right one for you. Impromptu networking Good for: Getting acquainted When forming a new team, allow time to get to know one another. This will help everyone to feel more comfortable working together. Impromptu networking is a Liberating Structure that helps teams to share challenges and expectations whilst building new connections. The activity is driven by two engaging questions that help attendees clarify and focus their thoughts through structured discussion. The first question should be self-reflective and the second focussed on the wider group. Eg. What unexpected superpower do you bring to this project? What are the advantages of shared ownership in a team? 1-2-4-All Good for: Understanding objectives, sharing ideas or exploring challenges. When kicking off a new project or initiative, a shared understanding of goals and success is essential. It helps teams to work together towards the same objective(s) with a stronger sense of ownership and achievement. 1-2-4-All allows time for personal reflection on a topic prior to group discussion. It’s a technique used to gather individual thoughts or ideas and filter those most prominent through discussion. To determine high-level goals, you can ask the group to consider what they personally would like to achieve. These ideas can then be shared to form a collection of goals that can later be developed into SMART goals (specific, measurable, attainable, realistic and timely). Eg. What would success be for you? There are also more traditional techniques for defining product objectives such as impact mapping or consumer journey mapping. 25/10 Crowdsourcing Good for: Generating and prioritising ideas in large groups 25/10 Crowdsourcing works particularly well for large groups, inviting participants to come up with their biggest, boldest idea for a solution to a challenge that is then swapped around the room and marked by others from 1 to 5, leaving a total score that can be used to prioritised ideas by perceived impact. Eg. Score between 1 (this is not an idea you’d get behind) and 5 (an idea you’d like to be involved in making a reality). For more specific product ideation techniques, this article gives a great breakdown of team set-up, preparation and session facilitation, considering value vs. impact mapping. What I Need From You (WINFY) Good for: Defining roles, responsibilities & expectations When forming a new team, it’s important to understand what you can expect from others and what they can expect from you. What I Need From You is a workshop format that asks individuals what they need from each other in order to be successful in their own roles. This helps individuals to better understand their responsibilities and get clarification on what they can expect from others. Eg. (from Design) Product team, what I need from you is a clear product delivery roadmap. What, So What, Now What? (The 3 Ws) Good for: Reflecting and improving The 3 Ws Liberating Structure helps groups to reflect on an experience, consider impact and define actions by asking three simple questions ‘What?’ (what happened?), ‘So What?’ (what was the impact?) and ‘What Now?’ (what are we going to do about it?). Eg. What? Design weren’t included in planning. So what? They didn’t know their priorities.  What now? Include designers in all planning sessions. Fun retrospectives is a great resource for facilitating retrospectives and FunRetro is a great tool for accommodating remote participants too! What should I do after the sessions? Session summaries allow you to communicate the value of your time together, allowing the group opportunity to reflect on outcomes and actions. Regular sessions and follow-ups can help teams to form a habit of continuous reflection and improvement. Applying these experience design principles to professional collaboration has changed the way I plan and facilitate collaboration. Liberating Structures have enabled me to design simple, productive and impactful sessions for multiple cross-discipline teams. The structures have helped our teams to consider challenges in new ways, using the frameworks to guide strategic thinking and develop a stronger sense of ownership. A big shout-out to Tasman Papworth, a good friend and Agile Companion here at Novoda who has mentored my journey towards more impactful collaboration to the point I can share my learnings with you.", "date": "2018-09-25"},
{"website": "Novoda", "title": "Future Legends: closing the gender gap in tech", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/future-legends-closing-the-gender-gap-in-tech/", "abstract": "Future Legends has a mission: to close the gender gap in technology. The organisation, founded by Managaing Director, Cassandra Melvin, offers girls between 6-16 years old free workshops to explore programming, design, digital photography, data visualisation and leadership empowerment. The workshops are primarily taught in English, German and Arabic to cater to the primary languages spoken in Berlin. Cassandra was kind enough to welcome me to the team to support workshops, design new curriculums and get to know the amazing, inspirational women behind this initiative. Our mission is to empower young girls to become future leaders and innovators. Through free, hands-on and exploratory workshops and courses, we equip girls with 21st century skills and encourage them to become creators and designers with technology. Cassandra Melvin, Founder & Managing Director of Future Legends It’s been a pleasure to support the Data Visualisation and Designing A Mobile App workshops over the past few months. The effort and care put into the organisation and preparation of these workshops is outstanding. Future Legend learnings When designing workshops for young people, design a relatable curriculum with clear structure and expectations. A fun and engaging experience will help the girls to enjoy the session, stay interested, inspire creativity and most importantly have fun! Here are some things that I’ve learnt from our educators and girls along the way. A warm up exercise encourages the girls to be more comfortable and confident before getting started Limiting new vocabulary to 5 key words or phrases helps the girls to learn the most important terms without being overwhelmed Making new vocabulary or processes relatable helps the girls to make sense of a new concept Clear communication and visual examples help to keep attention and interest The creative abilities of young people are astounding and inspirational! We will be hosting a Mobile App Design workshop at Novoda’s Berlin office in November, so keep up to date with future events on the [Future Legends]( https://www.facebook.com/pg/futurelegendsberlin/events/?ref=page_internal page and invite your future legends to be leaders and innovators! If you’re interested in getting involved yourself, feel free to get in touch below or get in touch through Facebook .", "date": "2018-09-27"},
{"website": "Novoda", "title": "Investigating Headless CMS solutions for Firebase", "author": ["Tom McArdle (Test Automation Engineer)"], "link": "https://blog.novoda.com/investigating-headless-cms-solutions-for-firebase/", "abstract": "At Novoda we’ve recently been delving into the world of headless CMS solutions, specifically those using Firebase storage. But what are they and why would you use one? Join us as we find out more. What are headless CMS solutions? A back-end only content management system. In our case, a CMS that interfaces with and uses a Firebase database and storage as a backend (either a Realtime or Cloud Firestore database depending on your chosen solution). Why use a headless CMS on top of Firebase? In a nutshell, user experience. Let’s face it, adding and managing content by entering data directly into a database is extremely tedious and error prone. Using a CMS for this task not only makes the experience a bit more bearable, it also allows non-technical users to chip in and perform tasks themselves without breaking anything, whilst also freeing up developers to crack on with what they do best, developing. It’s a win-win! Investigation We put three solutions through their paces, here are our findings. PushTable Homepage: PushTable.com Price: Free The first and simplest solution we took a look at was PushTable. PushTable is a free offering from Trellis Technologies Inc. that provides a neat spreadsheet-like interface that feels familiar and intuitive, allowing users to get up and running quickly and easily. Linking a Firebase project took no more than a couple of minutes and setting up data models for different types of content and adding new content was a breeze. The search and filtering capabilities also came in handy as we added more and more content. PushTable only currently supports Firebase Cloud Firestore databases. This is great on one hand as it works out cheaper to run than a Realtime database of similar size. On the other hand, Cloud Firestore databases are currently still in BETA, meaning you won’t be able to backup and restore your database for the time being. There’s also no user creation or management features, potentially opening the door for non-technical users to make mistakes, for example, modifying a data model rather than adding content to the data model. Key features and benefits: Cloud Firestore database for data storage Content management through content models Search and filter functionality for content and data Multi-project support Documentation and support available Flamelink Homepage: Flamelink.io Price: Free, Paid The second and arguably best all-round solution we took a look at was Flamelink. Flamelink offers free and paid plans depending on whether or not you need multiple users and user creation and management features. It has a sleek, intuitive interface that will feel familiar to anyone who has any kind of experience of using a CMS such as Wordpress or Ghost. The video walkthroughs provided with the docs were helpful and concise and made linking a Firebase project an absolute doddle. Flamelink uses schemas rather than content models for adding and managing content, but in reality it’s very similar to PushTable in this regard and we were able to add a bunch of content relatively quickly and painlessly. Search and filtering was again handy as we added more content. Flamelink only currently supports Realtime databases (i.e. the opposite of PushTable), great if you want to backup and restore your database, but comes in a little more expensive than running a Cloud Firestore database. A key area that really differentiates Flamelink from the rest of the pack is it’s user creation and management features. User access rights/privileges can be configured in in a way that allows non-technical users to add and manage content, whilst removing the possibility of them accidentally modifying content schemas, thus breaking things. Although this is a paid feature it’s a huge bonus in our book and one of the key reasons you would use a headless CMS in the first place. Key features and benefits: Realtime database for data storage Content management through schemas Firebase Storage for media upload and storage User creation and management (privileges and access) Search and filter functionality for content, data and users Multi-project support Multi- environment support (e.g. dev, staging, test, production) Multi-language support Backup and restore (via the product and Firebase) Documentation and support available Firebase CMS Homepage: alexabbott/firebase-cms Price: Free The third and final solution we took a look at was firebase-cms, an open source solution created by Alex Abbott. Truth be told we found it a little difficult to get up and running. The project hasn’t been updated for almost a year and some of the setup and configuration instructions are out of date or result in error messages. Adding and managing content was fairly straightforward, however, content was limited to posts, pages, products or product categories. User creation was also available, a really nice bonus, but again was limited to specific user roles including super-admin, admin and editor. Taking a look through the issues currently open for the project highlighted a number of security worries, and taking into consideration the overall inflexibility of the solution when compared to PushTable or Flamelink it becomes hard to recommend firebase-cms for commercial use. Key features and benefits: Realtime database for data storage Content management (posts, pages, products, product categories) User creation and management (super-admin, admin, editor) Backup and restore (via Firebase) Documentation available Summary After taking the reviewed solutions for a spin the verdicts are in and our final recommendations are as follows: If you’re looking for something simple and free you can’t go wrong with PushTable. If you’re looking for something fully featured and don’t mind paying a small monthly fee for the extras then go for Flamelink. We can’t recommend Firebase CMS as it's not up to date, has security issues outstanding and is less flexible than other solutions currently available.", "date": "2018-10-23"},
{"website": "Novoda", "title": "Tricks & treats to make UI testing less terrifying (part 1)", "author": ["Fabia Schäufele (IOS SOFTWARE CRAFTER)"], "link": "https://blog.novoda.com/ui-testing-part-1/", "abstract": "This series will cover a couple of obstacles or complications you might run into when writing UI tests for iOS and will give advice on how to write cleaner tests. In the first part we will look into possible issues with testing custom views; the second part will explain how to correctly access views in tests; the last part will finally show how you can use page objects to structure your tests in a sustainable way. Testing custom views It is quite probable that the application you are testing displays custom views. And it is also very likely that several of those classes inherit from the base class UIView and not from some more specific class as e.g. UIButton . In that case – if you do not want to completely rewrite those classes and inherit from something else – you need to first make them \"visible\" as appropriate UI elements before you can correctly use them in UI tests. Making a custom view \"visible\" is largely the same process as making it accessible , meaning making it \"visible\" to users that use features like VoiceOver and other assistive technologies. UI elements are accessible when they can properly describe themselves to assistive technologies. Most standard view classes from UIKit do that pretty well by default; for example, you don't have to tell a UIButton that it should \"report\" as a button with a certain state. When dealing with custom view classes, though – classes that most often only inherit from UIView or that use classes from UIKit in a way that was not intended – then we need to do some extra work and supply that information ourselves. To be able to do this you need to know a little bit more about the Accessibility API. The UI Accessibility API The UI Accessibility API mainly consists of two informal protocols, UIAccessibility and UIAccessibilityContainer , and the UIAccessibilityElement class, but we will mostly look into the UIAccessibility protocol here. All standard UIKit controls and views implement the UIAccessibility protocol by default; this means they report their accessibility status ( isAccessibilityElement ) and supply additional descriptive information about themselves when asked. The attributes that describe an accessible UI element differentiate one view from another. Apple's API defines the following attributes: accessibilityFrames accessibilityLabels accessibilityHints accessibilityValues accessibilityTraits Accessibility elements provide content for accessibility frames by default because a UI element on the screen must always know about its position in a user interface. Accessibility elements also come with standard accessibility labels ; accessibility hints are optional and need to be set by the user on elements that perform an action. The values set as labels and hints are used by features like VoiceOver to determine what to read to the user. They should never be used as a unique identifier in UI tests as they should be localized and should be descriptive to a user (see part 2 ). Setting accessibility values is optional as well, and only used when the element’s contents are changeable and cannot be described by the label. The most important attribute for us, from a testing perspective, is the last attribute: accessibility traits . Traits describe aspects of an element’s state, usage, or behavior. Apple tells us to use the following traits to characterize elements: Not Enabled Selected Static Text Search Field Keyboard Key Button Image Summary Element Link Plays Sound Updates Frequently None You will need to set one or several of those traits on your custom views to be able to correctly access them, either from UI test or from assistive technologies. Setting traits on custom views Let's assume we have a custom radio button using a basic UIView with a changing image for each state ( selected / unselected ). This button is not a \"real\" button, though; it is just a view that can look like a button depending on how it gets rendered, but it has no state. class RadioButton: UIView {\n    init() {\n        [...]\n    }\n    func render(state: SelectionState) {\n        if state.isSelected {\n            image = Assets.selectedImage\n        } else {\n            image = Assets.unselectedImage\n        }\n    }\n} So even though the view might look like it has a selected and an unselected state, the accessibility API cannot tell. When trying to determine the state of our radio button in UI tests we would probably do something like this. if radioButton.isSelected {\n    radioButton.tap() // will never be called\n} This will compile, as isSelected a property that can be found on all XCUIElements . Calling isSelected on our radio button will always return false , though, even if the \"button\" looks selected and our code works as expected. The reason is that UIViews aren't \"visible\" accessibility elements by default, as isAccessibilityElement is false by default. In that case, all traits (and other attributes like the label), which a UI element might have, will be ignored in tests as well as by assistive technologies. View classes like UIImageView , UILabel , or UIButton on the other hand have isAccessibilityElement set to true as the user of an app is expected to interact with those elements on a screen. As our radio button is subclassing from the more basic UIView we have to set that flag to true ourselves. class RadioButton: UIView {\n    init() {\n        [...]\n        isAccessibilityElement = true\n    }\n    func render(state: SelectionState) {\n        if state.isSelected {\n            image = Assets.selectedImage\n        } else {\n            image = Assets.unselectedImage\n        }\n    }\n} Make sure not to set this property to true on views that should not be visible to assistive technologies, though. When e.g. using a UIView as a container view that that doesn't have any functionality we would need to make sure isAccessibilityElement stays false . If we need access to this container view in UI tests we should use the accessibility identifier (see part 2 ), and not this property. View classes like UIButton also come with certain pre-set accessibility traits. Those are the traits that we need to give to our custom radio button view in order to make it a \"real\" button. We should try to choose the best description, meaning the best combination of traits (compare list of traits on last section), for what the element does in our application. If a button is for example used to open something in Safari it would make sense to give it the trait of a Link . It is also possible to combine traits; we can for example use Button and Plays Sound together. To be able to ask our custom radio button its state, we need to set its accessibility traits like this: class RadioButton: UIView {\n    init() {\n        [...]\n        isAccessibilityElement = true\n    }\n    func render(state: SelectionState) {\n        if state.isSelected {\n            image = Assets.selectedImage\n            accessibilityTraits = (UIAccessibilityTraitButton | \n                                    UIAccessibilityTraitSelected)\n        } else {\n            image = Assets.unselectedImage\n            accessibilityTraits = UIAccessibilityTraitButton\n        }\n    }\n} Now asking the radio button for its selected state in UI tests will work as expected. if radioButton.isSelected {\n    radioButton.tap() // will be called if selected\n} In this part of the article we covered the topic on how to use UI element attributes correctly for UI tests. As shortly mentioned, there is one very common mistake developers make in regards to  the accessibilityLabel attribute of a view, namely using it to uniquely identify and access UI elements. This should not be done. We will look into this in greater detail in the next part. Read the next part of this series.", "date": "2018-10-30"},
{"website": "Novoda", "title": "Tricks & treats to make UI testing less terrifying (part 2)", "author": ["Fabia Schäufele (IOS SOFTWARE CRAFTER)"], "link": "https://blog.novoda.com/ui-testing-part-2/", "abstract": "This series will cover a couple of obstacles or complications you might run into when writing UI tests for iOS and will give advice on how to write cleaner tests. In the first part we will look into possible issues with testing custom views; the second part will explain how to correctly access views in tests; the last part will finally show how you can use page objects to structure your tests in a sustainable way. Accessibility labels vs. identifiers In this second part of the article we want to look into how to correctly access view elements in UI tests. As shortly mentioned in part 1, there is one very common mistake developers make here: using the accessibilityLabel attribute of a view to uniquely identify UI elements. This should not be done. Let’s see why. Accessibility labels as “identifiers” Accessibility labels are used by features like VoiceOver to determine what is read to the user. They should be localized, so users that rely on assistive technologies can use the app in different languages, too. This requirement makes them very poor identifiers – however, as UI tests are often only run in one language, this shortcoming does not always manifest itself. But even then, using accessibility labels as identifiers in tests makes your live harder than necessary! Texts in apps can change quite a bit from release to release. Every time a string that is also used as an identifier changes, your tests break. What can we do? Unique identifiers For UI testing purposes Apple provides us with a so-called accessibilityIdentifier . It is meant to \"be used to uniquely identify a UI element in the scripts we write using the UI Automation interfaces\" ( Apple's documentation ). Accessibility identifiers aren't accessed by e.g. VoiceOver . They are part of the UIAccessibilityIdentification protocol, which consists of (a) method(s) that associate a unique identifier with elements in a user interface. The only strict requirement for adhering to this protocol is defining the accessibilityIdentifier property. Setting accessibility identifiers is very easy. In our view's initializer we simply do: someView.accessibilityIdentifier = \"some_string\" In UI tests, we can then find our view like this let someView = XCUIApplication().otherElements[\"some_string\"] or maybe like this let someView = XCUIApplication().buttons[\"some_string\"] ... depending on what type of UI element someView is. To avoid typos and confusion we suggest using static strings as identifiers. public struct AccessibilityIdentifier {\n    static let someString = \"some_string\"\n} Just have a struct somewhere with all identifiers let someView = XCUIApplication().otherElements[AccessibilityIdentifier.someString] ... which will then be accessible from inside all UI tests. Accessibility identifiers for system views Problems start when trying to set identifiers on views we only have limited control over, meaning view classes that the system (at least partly) manages for us, like bar button items. Let's have a look at some system UI elements. System UI elements in tab bars, navigation bars, and search bars All three bar classes – tab bars, navigation bars, and search bars – are UI elements provided by Apple. They are accessible by default and conform to the UIAccessibilityIdentification protocol so we can set an identifier like this: tabViewController.tabBar.accessibilityIdentifier =\n    AccessibilityIdentifier.mainTabBar\nnavigationController.navigationBar.accessibilityIdentifier =\n    AccessibilityIdentifier.featureNavigationBar\nsearchController.searchBar.accessibilityIdentifier =\n    AccessibilityIdentifier.listSearchBar Navigation bars differ slightly from the other two classes insofar as they manifest special behavior if no custom accessibility identifier is set. If no custom identifier is set, the default identifier of the navigation bar automatically becomes the same as the title of the bar. (Keep in mind that in this special case the identifier changes if the title is changed.) Tab bars and navigation bars are treated as special UI elements in tests. So in addition to the above mentioned approach they are also accessible by group: let app = XCUIApplication()\nlet mainTabBar = app.tabBars[AccessibilityIdentifier.mainTabBar]\nlet mainNavigationBar =\n    app.navigationBars[AccessibilityIdentifier.featureNavigationBar] For some reason we do not get searchBars as a group; interestingly Apple provides us with searchFields , though. The fun starts when trying to access system UI elements in those bars. Let's first look into navigation bar titles. Navigation bar titles Navigation bar title views come with the bar itself and do not need to be managed by us, except for being given a value. Unfortunately, we can neither set a custom accessibility identifier on them, nor do they come with a default one. Simply setting an accessibility identifier will compile (because title views are just UIViews when it comes down to it) navigationItem.titleView?.accessibilityIdentifier =\n    AccessibilityIdentifier.featureNavigationBarTitle ... it will however have no effect. The problem with that behavior is that we cannot easily assert on the title of a view controller in tests. As navigation bars cannot have more than one title, that problem can be worked around by setting an identifier on the bar and then asking it for its title. let identifier = AccessibilityIdentifier.featureNavigationBar\nlet navigationBar = XCUIApplication().navigationBars[identifier]\nnavigationBar.title The buttons that come with the bar are more complicated, though, as there can be several and different types. Bar button items and tab bar buttons Bar button items can mostly be found on navigation bars, but also for example on search bars. They inherit from UIBarItem ; both classes theoretically conform to UIAccessibilityIdentification . Like with the title views, it is possible to set an accessibility identifier on such an item navigationItem.backBarButtonItem?.accessibilityIdentifier =\n    AccessibilityIdentifier.featureNavigationBarBackButton\nnavigationItem.leftBarButtonItem?.accessibilityIdentifier =\n    AccessibilityIdentifier.featureNavigationBarBackButton ... but again with no effect. To access the bar buttons we have two (or maybe three) possibilities. We can either use the button label, e.g. Cancel , to find it. let navigationBarButtons = XCUIApplication().navigationBars.buttons\nlet cancelButton = navigationBarButtons[\"Cancel\"] However, as mentioned above, this solution is not very stable as the title label can change, e.g. when we switch the app's language. If we need a solution that works for several localizations we can use XCUIElementQuery.element(boundBy: 0) instead. Both approaches also work for search bar buttons (which technically are UIBarButtonItems , too) and for tab bar buttons (which also inherit from UIBarItem ). let tabBarButtons = XCUIApplication().tabBars.buttons\nlet homeButton = tabBarButtons[\"Home\"]\n// or\nlet homeButton = tabBarButtons.element(boundBy: 0) A third solution would be creating and setting customs items as bar buttons items, as this gives us more control over those items. let backButton = UIBarButtonItem(\n                    title: String.Localized.back,\n                    style: .plain,\n                    target: self,\n                    action: #selector(goBack)\nbackButton.accessibilityIdentifier =\n    AccessibilityIdentifier.featureNavigationBarBackButton`\nnavigationItem.leftBarButtonItem = backButton The third approach is far more work, though as we do not need to create custom bar buttons most of the time. In this part of the article we covered the topic on how to mark UI elements with unique identifiers and how to access them. In the next part of this series we will take a look at the wider context of UI tests, namely how to best use UI elements in automated test flows. Read the next part of this series.", "date": "2018-10-30"},
{"website": "Novoda", "title": "Tricks & treats to make UI testing less terrifying (part 3)", "author": ["Fabia Schäufele (IOS SOFTWARE CRAFTER)"], "link": "https://blog.novoda.com/ui-testing-part-3/", "abstract": "This series will cover a couple of obstacles or complications you might run into when writing UI tests for iOS and will give advice on how to write cleaner tests. In the first part we will look into possible issues with testing custom views; the second part will explain how to correctly access views in tests; the last part will finally show how you can use page objects to structure your tests in a sustainable way. UI testing with page objects In part 1 and part 2 we have covered how to best describe your UI elements, how to mark them with unique identifiers and how to access them. It is now time to take a look at the wider context of UI tests: using those UI elements in automated test flows. There are lots of ways to write UI tests and a lot of UI test I have seen in past projects were really long, hard to read (because very complex and unstructured) and quite brittle. As with any code base, abstraction can be helpful in making UI tests more manageable. At Novoda we decided to go for the Page Object design pattern to achieve this. It has traditionally been primarily applied in the context of websites (therefore the usage of page instead of screen ), but has also become popular in mobile application test automation. Using pages objects works best for apps that can be easily split into separate “pages”. So what exactly are page objects, you might ask. A page object models areas of an app's UI that its users interact with as objects within the test code. They represent a screen or a part of a screen, hide logic that should not be exposed, and expose possible actions and information the page has to offer. Exposing a stable API while encapsulating logic that manipulates the UI – and that previously live in several places in different tests – allows us to keep our tests flexible and maintainable. Applying the pattern reduces code duplication and moves logic to a single place. It establishes a clean separation between test code and page specific code. And it also makes the tests themselves cleaner and better readable because the tests themselves then only reflect the intention of the test. “PageObjects can be thought of as facing in two directions simultaneously. Facing towards the developer of a test, they represent the services offered by a particular page. Facing away from the developer, they should be the only thing that has a deep knowledge of the structure [...] of a page (or part of a page). It's simplest to think of the methods on a Page Object as offering the \"services\" that a page offers rather than exposing the details and mechanics of the page.” ( SeleniumHQ ) Time for an example. Imagine a simple Master-Detail app; it can be split into a main page that shows the list, and into another page that shows the detail when the user clicks on a listing. List Detail The main list page object and the detail page object then might look something like this: class ListPageObject {\n    var listTitle: String {\n        let identifier = AccessibilityIdentifier.listTitle\n        return XCUIApplication().staticTexts[identifier].label\n    }\n    var canGoToFirstListing: Bool {\n        let firstCell = XCUIApplication().collectionViews.element(boundBy: 0)\n        return firstCell.isHittable\n    }\n    func goToFirstListing() -> ListingPageObject {\n        let firstCell = XCUIApplication().collectionViews.element(boundBy: 0)\n        firstCell.tap()\n    }\n}\n \nclass DetailPageObject {\n    var detailTitle: String {\n        let identifier = AccessibilityIdentifier.detailTitle\n        return XCUIApplication().staticTexts[identifier].label\n    }\n    func goBackToList() {\n        let navigationBar = XCUIApplication().navigationBars.firstMatch\n        let backButton = navigationBar.buttons[\"Back\"]\n        backButton.tap()\n    }\n} Page objects may not always cover the whole visible screen, thought. A good example would be a tab bar in a tabbed application. class TabControllerPageObject {\n    func goToFirstTab() -> SomePageObject {\n        let tabBarButtons = XCUIApplication().tabBars.buttons\n        tabBarButtons.element(boundBy: 0).tap()\n    }\n    func goToSecondTab() -> SomeOtherPageObject {\n        let tabBarButtons = XCUIApplication().tabBars.buttons\n        tabBarButtons.element(boundBy: 1).tap()\n    }\n} In the current state of our page objects, a possible test could look like this: func testThatFirstListingIsClickable() {\n    TabControllerPageObject().goToSecondTab()\n    let list = ListPageObject()\n    expect(list.canGoToFirstListing) == true\n} In the test we are clicking on the right of two tabs, there we expect a list view controller with at least one listing and we check that we can click on the first listing. This test is already quite, compared to what it could have look without using page objects: func testThatFirstListingIsClickable() {\n    let tabBarButtons = XCUIApplication().tabBars.buttons\n    tabBarButtons.element(boundBy: 1).tap()\n    let firstCell = XCUIApplication().collectionViews.element(boundBy: 0)\n    expect(firstCell.isHittable) == true\n} But we can do even better. Instead of assuming that when clicking on the second tab we get shown the list view controller (which is implicit knowledge), we can introduce flows that combine different page objects into one coherent structure. How can we do that? By letting the page objects return new objects after a transition occurred. class TabControllerPageObject {\n    [...]\n    func goToSecondTab() -> ListPageObject {\n        let tabBarButtons = XCUIApplication().tabBars.buttons\n        tabBarButtons.element(boundBy: 1).tap()\n        return ListPageObject()\n    }\n}\n\nclass ListPageObject {\n    [...]\n    func goToFirstListing() -> ListingPageObject {\n        let firstCell = XCUIApplication().collectionViews.element(boundBy: 0)\n        firstCell.tap()\n        return ListingPageObject()\n    }\n}\n\nclass DetailPageObject {\n    [...]\n    func goBackToList() {\n        let navigationBar = XCUIApplication().navigationBars.firstMatch\n        let backButton = navigationBar.buttons[\"Back\"]\n        backButton.tap()\n    }\n} Our test example would then look like this: func testThatFirstListingIsClickable() {\n    let list = TabControllerPageObject().goToSecondTab()\n    expect(list.canGoToFirstListing) == true\n} Our test case now reads so well that even non-engineers can figure out what is going on here. Well, this is it. I hope you enjoyed reading this little series and that I could help clearing some obstacles out of the way. If you would like to learn more about testing and mobile applications have a look at our other post .", "date": "2018-10-30"},
{"website": "Novoda", "title": "5 ways to make use of flight time: LPL>SFO Hackathon", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/5-ways-to-make-use-of-flight-time/", "abstract": "Flying can seem like a real waste of time for some, a necessary evil needed to get from A to B, used to catch up on sleep, or chill out watching whatever in flight entertainment is on offer. I like to see it as a great opportunity for learning, read on to see how I turn my flight into a personal hackathon. I've just gotten back from the Google Developer Experts Summit . Every year, Google organises ~300 GDEs with the biggest community contributions to the annual #GDESummit. It was 2 days, 7 parallel tracks, 62 sessions, and lots and lots of conversation and fun. This year it was run at Google in California USA, whereas I work in Liverpool UK. This means a flight from Liverpool to London (1 hour 10mins) and London over to San Francisco (11 hours 20 mins). A hackathon (also known as a hack day, hackfest or codefest) is an event in which people set out to create, learn, hack together technologies and use their skills to reach an end goal in a very short timeframe and usually skipping on some of life's other necessities like sleep. I had a total of 12.5 hours to work with for my flights. I set out my aims to be broad and multiple, this allowed me to feel a sense of accomplishment and completion throughtout the flight ( and not just at the end, very important ). I aimed to learn about at least one new programming API, catch up on at least one conference talk I'd missed, tidy up at least one thing from my long list of \"I should improve this\" pieces of code and also read at least 1 blog post from my \"to read later\" bookmarks. Like with any well laid plan, the skill is in the preparation. You'll likely not have internet connectivity on your flights (or have to pay through the nose for it). Having tabs already opened and loaded with the content you want to explore helps. Downloading YouTube videos to watch (using YouTube Premium of course) offline is great. Ensuring your laptop is fully charged in the terminal beforehand, and generally expecting to not be connected will stand you in good stead. #1 Codelabs Learning a new programming API, I decided to use Google's code labs website . Google Developers Codelabs provide a guided tutorial, hands-on coding experience. Most codelabs will step you through the process of building a small application, or adding a new feature to an existing application. I had the tabs preloaded, and also clicked through them to see if there were any example code repositories or dependencies needed ( there were, and I downloaded these as zip source folders from GitHub ). I chose 5: Introduction to Sceneform, AR stickers ARCore Cloud Androids Using the Natural Language API from Google Docs Entity, Sentiment and Syntax Analysis with the Natural Language API Android Things Assistant The labs worked pretty well, some of them required registering online to enable cloud APIs, or to be online to test the code written, being on the flight I could not do this, but I could still understand how the APIs worked and have a taste of working in that area. Whilst not a 100% success I would say I achieved my aim and if I ran the codelab again when online sometime I could cement this learning . #2 YouTube videos There are so many awesome conferences always happening, and watching videos of these talks if you cannot attend in person is very informative. I have a backlog of talks and could always do with more time to catch up. For this flight, considering where I was going I chose to watch Android Dev summit talks . Those I had preloaded ready were: AndroidDev Summit Keynote 3 platforms in 5 minutes with Kotlin Optimize your app size with this one trick Android Suspenders Is your app ready for foldable phones? Re-stitching plaid with Kotlin Single Activity: Why, When, and How #3 refactoring Refactoring is a fun activity that doesn't necessarily need you to be running the application, opening emulators, loading new libraries or potentially even having an IDE open. ( These are all tasks of programming that drain a lot of battery or require a network connection. ) Refactoring can allow you to hone your code reading skills and apply SOLID or clean code principals to make your code better for yourself and for others who may use it in the future. Side note: you should still run your test suite to ensure your refactorings haven't broken anything. That's the whole point of a refactoring, right? Code Refactoring: is the process of restructuring existing computer code—changing the factoring—without changing its external behavior. Refactoring is intended to improve non-functional attributes of the software. Advantages include improved code readability and reduced complexity; these can improve source-code maintainability and create a more expressive internal architecture or object model to improve extensibility. #4 blog reading Like with conferences I have a constant backlog of reading I'd love to do. Here were my loaded tabs: Getting started with Google ARCore on Android Get your app ready for foldable phones Setup AWS S3 static web hosting using SSL AWS Lambda, Applying the decouples invocation pattern I closed my tabs after I had finished reading them, I think this was a bit of battery use paranoia and in real terms didn't gain me any real % duration increase in battery length. #5 sleeping Sleeping helps you learn , and I can't do all of the above for 12 hours straight anyway. Grab your face mask, use your neck inflatable or just lean in on your neighbour's shoulder and catch some zzZs. Your brain needs downtime to reflect on and absorb all that awesome learning. Remember to consider when any in-flight meals are going to come, it's bad form to be asleep when food is on offer, but that's a perfect time for some audible learning. Usually I would also have a physcial paper book on this list, so you can get some reading done .. perhaps when your laptop battery expires ( on a plane with no plug ). But for this trip I actually messed up and ended up checking in both my books in my main luggage, so didn't get the chance to open them, they were: Embedded Android: Porting, Extending, and Customizing Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead Having 4 or 5 different options of learnings is very important, learning depends on your mood , sometimes you have a lot of energy are on the ball and can dive into a codelab or read a blog post, later you might be getting tired and want to learn a bit passively with watching video, or alternatively feeling creative and express yourself with refactoring code. Having the choice is important and then however the day is affecting you; if the person next to you has suddenly grown massive elbows or started to snore, you have a choice and can still reach your aims. Flying from Liverpool to San Francisco is a long 14 hours, but it doesn't have to be a wasted amount of time. Embrace the lack of connectivity, set yourself small achieveable goals, and enjoy your own in-flight personal hackathon.", "date": "2018-11-12"},
{"website": "Novoda", "title": "Design & Git: Guidelines for effective design version control", "author": ["Chris Basha"], "link": "https://blog.novoda.com/design-and-git/", "abstract": "For a long time now, the designers at Novoda have been using Abstract as a version control system for our design files. Version control is something that we think every product in development should employ, and we’re excited to finally be able to do something that developers have been doing since forever. After using Abstract for a while, we have developed a few guidelines that help our Git workflow which we want to share. Most of these have been borrowed from developers! In this blog post, we’re going to talk about a few best practices, some hints & tips, and we’ve included a super handy Pull Request template. What is a Pull Request? First, let’s get some terminology out of the way. Essentially, a Pull Request (PR) is the bundle of edits you have made on the design files, that now have to be reviewed by other members of the team. You are requesting other designers to merge (or pull into the master branch) your edits. It is the final stage of a branch’s life. Abstract doesn’t really use this term, but we refer to our branches that are ready for review as ‘PRs’ anyway. A good rule of thumb is to write for someone that will be reading after a long time (even yourself). Name a branch after its purpose. To give a descriptive name to a branch you’re creating, think about the purpose of the PR that will result from it. Try to complete the sentence ‘The purpose of this PR is to…’ . It can be the addition of a feature, a redesign of a certain element, a flaw fix, etc. We write in simple present tense to save some precious characters. Examples: Add location type in map Fix system colour contrast Redesign widget layout Commit early and often. Limit commits to small chunks of related changes. If you were to address two different flaws, they would be two commits. An easy rule of thumb is to commit whenever you save (or press ⌘+S). Write a small but descriptive commit message. This is the place to talk about low-level design changes. Talk about what you did, and why you did it. If your commits are small, your titles should be too. If you need to mention more details, add them in the notes section. Examples: Add location type question and pop-up window in flow Add iOS Navigation Bar components Rename Text Grey to Text Secondary Create (or to use the correct term; open) small PRs. PRs should be small enough for a person to be able to go through all the edits and understand their nature without difficulty. Try to keep the reviewing process to less than 5 minutes. We often use Collections to bundle up all the edited screens in one place. Use a PR template (we’ve written one!) To add context to your edits, write a description in the Summary of the branch that outlines the essence of the changes. We’ve written a template that we use on all of our PRs, and we’ve added it at the end of this blog post! This PR template tackles two main areas; one, to give a general, high-level idea of what the PR is intended to achieve, and two, to describe the solution that was implemented to achieve the intended purpose in detail, along with any necessary info that someone else might need to know in order to understand the nature of the PR.  A good rule of thumb is to write for someone that will be reading after a long time (even yourself). Finally, ask for a review! At this point, your design edits should be ready to be reviewed, and the PR description that you wrote should cover all the essential details. Abstract has now added a feature that enables you to assign reviewers to a branch. Wait for their feedback, and make any changes that might be necessary. Lastly, before we merge to the parent branch, we usually upload the updates to Zeplin or Invision. Fin Thanks for reading, and we hope that these tips improve your workflow. Don't forget to check out the PR template below! If you have any tips and tricks that you would like to discuss, don’t hesitate to reach out on Twitter ! PR Template (formatted in Markdown) **What does this PR do?**\nBriefly explain the intent of the Pull Request. What value does it \nadd to our design files? Is it adding a feature, addressing a flaw, \nreworking a certain element? \n\n**Solution**\nProvide a detailed explanation of the solution. This can contain \ndetails that you might find relevant for the reviewers to know \nbefore jumping into reviewing the designs, and even decisions from \nPOs and stakeholders.\n\n**How do both mobile platforms compare?**\nExplain differences between iOS and Android, if any.\n\n**Paired with**\nMention the designer you have paired with (if the case).", "date": "2019-04-02"},
{"website": "Novoda", "title": "Kotlin Anti-Patterns - Also this is Null", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/kotlin-anti-patterns-also-this-is-null/", "abstract": "Kotlin is an awesome language, it has a succinct powerful syntax and takes many lessons from the past 20 years of programming paradigms and languages including Java to come up with a nice to use language. However Kotlin is the newcomer, and with that comes many untrodden paths and unknown gotchas. Let's explore one of these anti-patterns, for the sake of the article I've called it \"Also this is null\". “Null, the billion dollar mistake\" , Tony Hoare explains how back in 1965 he introduced the null reference and how he wished he hadn’t. I think everyone soon learns how the null idiom can be a bane to your code, the authors of Kotlin were also wise to this, creating the language idioms so that you have to go out of your way to create variables that have the potential for being null, unfortunately, it is still possible. As an example: You are wanting to act upon a variable, the nullability of the variable is under question, so you need to check before acting on it. In java you would: if (foo != null) {\n   foo.helloWorld();\n} Although this is a code smell of mutability (i.e. that variable foo has multiple states, null or not null ). It is a legitimate code block and the explicitness here is stating it can be null and conditionally blocking on that. In Kotlin we have some new constructs not known to Java. The safe calls operator looks like this ?. it allows for a method to be called on a variable if the variable is not null, otherwise the statement returns null. val b: String? = null\nprintln(b?.length) // Prints “null” The also block .also{} , calls the specified function block with this value as its argument and returns this value. Allowing you to operate on a variable repeatedly with less boilerplate. repository.also { // it: Repository\n  it.something()\n  it.someElseOnRepository()\n  it.perhapsAThird()\n} The anti-pattern we are exploring here is \"Also this is null\" and this is what it looks like in two forms, method referenced and parameter use: foo?.also { //it: Foo\n    it.helloWorld()\n} foo?.also { //it: Foo\n    helloWorld(it)\n} I’ve seen this used in a few 'hand me down' projects as a way to avoid null checks, to allow access to the variable if it is not null, and do nothing if it is null. I wanted to write a positive point for this small example above, using this coding style, however without changing the example to do a lot more inside of the also block, I cannot talk positively about it. This is a null check in disguise . The code is checking the nullability of the variable under question and then acting on it. It's a null check through and through, and yet it doesn't have a similar resemblance ( therefore you cannot at a glance fall back on your learnt knowledge ). In my opinion, this is bad. Another negative here is the variable name context switch, using the also block means your variable name internally is it ( yes you can change the name, if you want, adding more code/effort ). This name change is a micro-context switch, yes the scope is small but context switching like this, when it can be avoided is adding unnecessary cognitive load to reading the code. With this code block not having a resemblance to past learnt code patterns, it is hiding the potential to spot it as a typical code smell. If it was recognised as the null check code smell , you start to think of potential combative measures such as preferring the positive or quick return statements . Recommended ways to write a method referenced or parameter null object, if you have to: foo?.helloWorld() Allowing you to execute the method if the reference isn’t null, but not convoluting the approach with a block statement. helloWorld(foo!!) You can be very explicit about nulls, the above converts foo to a non-null type and throws an exception if the value is null. Further, don't be scared of being explicit and highlighting the smell in the code, the below is slightly different to the use of !! in that !! will cause an exception and stop the code executing whereas the below stops a block executing and continues the rest of the program, thus the former doesn’t ever expect null where the latter does. This is still compiling Kotlin ( highlighting nulls, until you refactor away that mutability of course ): if(foo != null) {\n    helloWorld(foo)\n} In conclusion, avoid the also this is null pattern, call it out when you find it and either prefer immutability or refactor to an explicit null check. The aim of Kotlin is to avoid mutability and null objects, sometimes though they are a necessary evil. The meta-learning here is to remember that a new possible way to do something is not always better, don’t do it just because you can and to lean on past experience (your own or others) whenever possible.", "date": "2019-03-28"},
{"website": "Novoda", "title": "Playing with Foldables in Android Q", "author": ["Alex Curran"], "link": "https://blog.novoda.com/playing-with-foldables-in-android-q/", "abstract": "Foldable devices look to be the next big thing in mobile development. The next version of Android — Q — is bringing native support for them, meaning that as a developer, it gets even easier to support them. The latest version of the Android Q Beta gives us ways to try our apps on a foldable, and we've taken a look. Spoiler alert — if you've developed your app well already, it's likely to look great on a foldable too! What is a foldable? Until very recently, device screens were, well, not foldable (although some got unintentionally close !). But with the introduction of the Galaxy Fold , we seem to be coming into a new era of phones that can be flipped out into tablets. The arms race for foldables is already underway, with Hauwei announcing their Galaxy Fold competitor , and even Apple filing patents for similar foldable iPhones. So how as an Android developer or a CTO, can you prepare your product for foldables? Preparing your app for foldables The good news is, if you are a well-meaning Android developer, you're probably most of the way there already! A lot of the functionality for foldables is built upon what we've had since Android 1.0 — the configuration change. Just as   rotating your phone from portrait to landscape is a configuration change, so is unfolding your foldable. Therefore, when handling a configuration change, it is especially important to keep the user's context and continuity . Previously, we could as developers be a bit sneaky and just reload everything on configuration change, but this will be really annoying and disruptive to your foldable users! There is many ways of dealing with keeping state across configuration changes, but naturally Google recommend their ViewModel pattern, from Android Jetpack. We're using this increasingly in our apps here at Novoda, and finding it a great help (especially as it interfaces with RxJava incredibly well). You can find out more about our adventures with ViewModels in our spikes repo . Supporting foldables isn't just about keeping the user where they are — but making sure they can get where they want to be. With the varied sizes of foldables it is increasingly important to make sure that your UI works not just on a portrait phone, but in 1:1 ratios, landscape, and also very small windows. For example — can the user scroll to get to where they need to be? Do elements overlap when using a small window? Are you assuming widths and heights of screens? A good solution to a lot of this is a) communication with your designers , b) considering ConstraintLayout and c) ensuring all screens are scrollable at a minimum. This will allow you to be super flexible with your UI, and tackle all eventualities. The bonus here is that you'll end up supporting split-screen on phones, and also go a long way to supporting Chromebooks as well! One big shame about the support right now is that, unlike the right-hand part of the GIF at the top of this post, there's no nice way of fluidly transitioning between folded and unfolded views right now. You could probably do something with handling configuration changes yourself via onConfigurationChanged() and animating layout changes but that brings a lot of other ways to go wrong! Let's see if Google change this in later versions of Q. Testing your app on a foldable So, you're now using ViewModel s to keep the user from getting frustrated, and you're pretty sure that your UI is going to look great on a foldable. But \"pretty sure\" isn't positive, and you don't have $2000 to shell out for a Galaxy Fold, so what now? Luckily, as of Android Q Beta 2, there's an emulator which you can use to test your app's foldability. As of writing, you'll need the Canary version of Android Studio 3.5, and you can follow the instructions here . It's not the most stable thing right now, but it is usable; and it is just as easy to deploy your app to it as any other emulator. You can even see in this GIF that an app I've not updated in ~5 years works pretty well with a foldable (and doesn't suffer from misalignment like Messages does!). Will foldables change mobile? Technology trends come and go, so how do we know that foldables will stick around? Simply put, we don't. There's no mention of it in the latest Gartner Hype Cycle, nor Thoughtworks' latest tech radar, so we're very much at the cusp of foldables emerging into the mainstream. Much like the first touchscreen phones, foldables are definitely in the \"bulky and expensive\" part of their development (although the Hauwei one looks pretty stunning). In my opinion though, a foldable display seems like a great idea — I love the idea that I could text on a small screen but then fold it out to be a big screen to read or sketch on. We'll see the technology evolve over this year — I'd be interested to see what a potential Galaxy Fold 2 would look like. I'm sure if Apple ever decide to do a foldable phone, then that would kick up enthusiasm in the field a lot more and bring foldables into the mainstream. Summing up Foldables seem to be the Next Big Thing in mobile to avoid the stagnation of the current generation's slowing innovation of devices. As a developer, it is pretty easy to support them, providing you're already doing the right things. Supporting them also gives you support for great experiences split-screen and Chromebooks as well. As a CTO or project lead, there's not a huge amount to take advantage of for foldables right now, but by preparing you can get ahead of the market when (or if!) it explodes. Resources Announcing support for foldables Getting your app ready for foldables Handling configuration changes", "date": "2019-04-16"},
{"website": "Novoda", "title": "Do you focus on the wrong product problem?", "author": ["David Gattig"], "link": "https://blog.novoda.com/do-you-focus-on-the-wrong-product-problem/", "abstract": "Photo by a befendo / Unsplash Several years ago, McDonald's decided to increase their milkshake sales. They had a lot of data and profiles for their core target audience of milkshake buyers. They invited people who fit these profiles into their offices, explained that they wanted to improve the sales of their milkshakes and asked them to give them guidance. The customers had a lot of guidance in terms of flavors, colors, and viscosity of their milkshakes. The marketing people from McDonald's would then go back and turn their suggestions into reality. The result? There was no significant improvement on sales or profit. Let us look at how McDonald's managed to turn it around. They employed a research team, who would go to a McDonalds store, watch and take notes about the people who bought milkshakes. What time was it that he bought the milkshake at? What clothes was he wearing? Was he alone? Did he buy other food? Did he eat the milkshake in the restaurant or get in the car and drive off with it? From their research, they found out that half of all milkshakes are sold before 8:30am. The typical milkshake buyer was always alone and the milkshake would be the only thing they get. They would always take the milkshake with them and drive off with the car. The research team then started to confront people outside of the McDonald's and ask questions regarding their behavior. As it turned out, they all had the same problem. They needed something they could consume on their boring morning commutes. A milkshake was making their life a lot easier in contrast to other food (bananas would be eaten too quickly, bagels and cream cheese too messy) as it was something they could drink for a longer period because of the thin straw and viscosity and would not create a mess while driving. The shakes keep people full and they can even put them in the cupholder of their car. Photo by Max Bender / Unsplash As a result, McDonald's launched a marketing program around these insights and made their milkshakes even thicker, swirling in tiny chunks of fruit to introduce unpredictability. They also moved milkshake machines in front of the counter to help customers avoid drive-thru lines. The results? They saw the great bump in milkshake sales they’d hoped for. McDonalds’ first initiative failed to produce more sales because they didn’t ask their customers the right questions. Your customers are eager to share their stories of pain and success, however, what they will actually tell you depends entirely on how they are prompted. This is where the underlying problem exists. Before research has even begun, your organisation has likely identified and proposed a problem statement. This statement will become the focus of research and is therefore fundamental to solving the problem, yet this work tends to be rushed and filled with unvalidated assumptions. For McDonalds’, the initial problem statement might be summarised to look something like this: “By understanding what customers desire in milkshakes we can improve the quality of our milkshakes and increase sales” With this problem statement in mind, researchers have directed their interviews towards identifying popular flavours and textures, but they would be unlikely to dedicate significant time to other customer needs. Even if qualitative research techniques were used, it’s likely that they would be designed based on the assumptions made in the problem statement. McDonalds’ was able to turn this story around by employing a research team that ignored these previous assumptions. Their updated problem statement might look like this: “By understanding why customers buy milkshakes we will identify the opportunities that will result in increased sales” It’s possible that for another brand, this research would have concluded that customers want tastier milkshakes, which could have increased their sales. At McDonalds’ however, the conclusion was that customers wanted their milkshakes to be more practical towards their “jobs to be done” of fulfilling a commuters breakfast. It is important that you check each word of your problem statement for all the assumptions you make before you go and talk to your customers. If you have the wrong problem statement, it will focus you in the wrong direction. Photo by Austin Distel / Unsplash Furthermore, don’t ask your customers to innovate for you. They are not experts for your product. You are. However, customers are the experts of their own pains and daily struggles and it is up to you to really listen, watch and understand the underlying problem they are trying to solve with your product or service. Be empathic. Put yourself into the shoes of your customers. Never assume you know what your customer’s problems are until you actually went and talked to them. Once you have understood their problems, challenges, or pains, you can start formulating robust problem statements and test solutions together with your customers. by David Gattig & Theo Allardyce", "date": "2019-05-22"},
{"website": "Novoda", "title": "Introducing the power of Liberating Structures", "author": ["Tasman Papworth"], "link": "https://blog.novoda.com/introducing-liberating-structures/", "abstract": "This blog post introduces and explores a set of powerful but incredibly easy to use tools called Liberating Structures. Anyone can use them to make it quick and simple for groups of people of any size to radically change how they interact and work together. There are over 30 Liberating Structures. In this blog post, we introduce seven that are easy for anyone to start using straight away to generate engagement and purpose for new teams, surface and filter everyone's ideas, identify both purpose and risks, clearly identify and commit to actions, and meaningfully reflect on an experience to generate deep insights. You can also experience the Liberating Structures first-hand at a local user group, for example in Berlin or London , and immerse yourself fully in a two-day Immersion Workshop this March. The best way to learn more about Liberating Structures is to use and experience them for yourself. You can get started today with the information here. Anyone can use them, without formal training or prior experience. Our Mission At Novoda we have a mission: Together on a Journey of Learning and Growth. Together means everyone, including us as individuals, Novoda as an organisation, and the community of partners, events, and industries that we are a part of across the world. Our journey of learning means constantly pursuing inclusive, diverse, multidisciplinary discussions and decision-making practices, so we can collaboratively find new solutions together. Enabling growth requires that we are empowering individuals and teams to pursue their own purpose and mastery, while also producing extraordinary products and transformational changes that result in success for our partners and communities. Why we use Liberating Structures Whenever a group of people comes together to explore questions, discuss topics, and make decisions, we have an opportunity to make an extraordinary impact. However having effective meetings, workshops, and agile events like retrospectives can seem elusive. People are often not engaged and may feel frustrated because they don't have a real opportunity to contribute, and because meaningful outcomes and actions are missing. We use Liberating Structures to change the pattern of interactions between individuals, teams, and organisations, so that everyone gets to make contributes based on their experiences, every discipline and viewpoint is represented, and so that those who are responsible for implementation contribute to and ultimately take ownership of the decision making process. This is really important because it achieves a lot of essential goals: Distribute power and influence to those with the most relevant experience Give everybody a voice, and ensure that every voice is heard Invite self-organization to flourish, by letting go of over-control Expanding and connecting our networks across organisations, communities, and industries Increase transparency for everyone involved, by promoting open flows of information Generate instant and open feedback loops to support continuous improvement at every opportunity Increase diversity by engaging more people and perspectives Emergent and joint discovery of new questions, and new solutions Generate trust across and between every level of every organisation Develop extraordinary self-directing teams, which produce extraordinary outcomes, and influence and form extraordinary organisations Getting Started Below is a description of what some of the Liberating Structures look like, and some of the extraordinary outcomes that they make possible. When we use multiple Structures in a sequence, called a String, their impact is far greater than the sum of their parts, as Structures expand on, complement, and magnify the impact of the others. Impromptu Networking Rather than introducing yourself to a group, Impromptu Networking invites you to meet one person at a time, tell them how you perceive the situation at hand, and also tell them what you bring to, and hope to get from, the group. When a new group of people comes together, Impromptu Networking helps people focus on the problem, recognise and articulate everyone’s potential to contribute, help people set an intention for their participation, and to build new connections quickly. 1-2-4-All A fundamental practice where everyone involved considers and writes down their own thoughts, then reviews these in a pair, then between four people, and ultimately with the whole group. This structure gives everyone a voice because every participant gets the opportunity to capture and share their point of view. Additionally, every idea is acknowledged, explored, and ultimately filtered based on its relevance and usefulness, regardless of where it came from. Answering serious questions, while having fun 9-Whys Simon Sinek emphasises the power of understanding your ‘why’ in this Ted video . This Liberating Structure simply and effectively guides us to dig deeper into our motivations by asking ‘why is that important to you?’ Using 9-Whys helps people dig down into their fundamental values, and helps them identify and articulate the purpose behind their ideas and actions. This can be incredibly powerful to reveal true motivations for individuals and help teams to identify and align with a common, fundamental purpose. TRIZ Imagine the worst, by pretending you’re all intentionally trying to fail. High energy A lot of fun Identify things that are getting in the way Very cathartic for frustrated people who see a lack of change Can be used as a pre-mortem for a kickoff Straightforward action planning Min Specs Explore everything that can be done, to identify that absolute minimum that actually needs to be done. 15% solutions Help individuals identify exactly what they can do today, with the time and resources they already have. Debriefing W³ Avoid jumping to conclusions and solutions Retros with insights and meaning This is a tricky structure if people don't have the Ladder of Inference as a reference to why it works... Advice on getting started Just start Pick one, try it, have fun, debrief afterward Say as little as possible Give the first instruction and let people focus on the task, don’t get them stuck on what to do next or if they are doing it right Give everyone the essentials Put the mechanics up somewhere as a picture, so there’s no confusion about individual, pair, or group work Display the invitation where everyone can see it, so they can refer to it whenever they need to Stick to the structure and timings Make sure people reflect individually, share equally, and don’t get sidetracked! Reflect Use W³ to get feedback from participants about how the Structure felt to them, and what was made possible that normally isn’t. This can give powerful insights into other situations the Structures may be valuable. Next steps The Liberating Structures are open-source, free to use, and available online. The best way to learn more about them really is to use and experience them for yourself. They are intentionally designed so that anyone can use them, without formal training or prior experience. Immersion Workshops An incredibly powerful way to learn about the Liberating Structures is by attending a two-day Immersion Workshop. You’ll not only experience the majority of structures in a string each day, but will be sharing the experience and learning directly from the inventors, pioneers, and practitioners who use them daily all over the world. There are Immersion Workshops running across Europe this March User Groups There are user groups in many locations, with more popping up every day. User groups are a fantastic way to explore the Liberating Structures because you normally experience two or three in a string, and can engage with the community and practitioners for advice. Please come and find us in Berlin and London , we’d love to meet you. You can find all of the Structures on the website , or you can order a copy of the book, pick one, and try it out! We will share Novoda’s experiences and insights from using the Liberating Structures on our blog, and we are always happy to talk with and help others, so please feel free to get in touch: https://www.novoda.com/contact “Liberating Structures introduce tiny shifts in the way we meet, plan, decide and relate to one another. They put the innovative and facilitative power once reserved for experts only in hands of everyone. Thirty-three adaptable microstructures make it quick and simple for groups of people of any size to radically change how they interact and work together. LS can replace or complement the big five conventional approaches that people use all the time: presentations, managed discussions, status reports, brainstorms, and open discussions. In contrast, LS are designed to include and unleash everyone in shaping their future.”", "date": "2018-02-16"},
{"website": "Novoda", "title": "Teaching app design to kids with Future Legends", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/teaching-app-design-with-future-legends/", "abstract": "This past weekend saw the third edition of the Future Legends ‘Design a mobile app’ workshop at Novoda, Berlin. We welcomed 7-9 year-old designers and innovators to learn the basics of user experience design. They learned the power of discovery interviews, explored app ideas and designed their very own apps to help their partners to have fun at the zoo! 1. Discovery interviews The children worked in pairs to interview each other about their next trip to the zoo asking the following 4 questions. What do you like about visiting the zoo? How do you like to travel? Who would you like to go with? What do you like to do at the zoo? 2. Defining the app idea After completing the interview, the children were asked to mark the things where they thought an app might be able to help before finishing the following sentence: 'I want to design an app to help my partner to…'. Each phase started with a demonstration so they could experience the task as a group before giving it a go themselves. 3. App design After learning the basics of navigation and interaction, they started to design their apps! Between all of the innovators, they helped their partners to buy tickets, navigate to the zoo, find their favourite animals and even check the queue length and quality ratings at the ice cream kiosks! Thank you to all who make this wonderful venture a reality! The kids had a lot of fun and it couldn’t have been done without the hard work and dedication of the wonderful volunteers; Kelly Jewell , Abigail Turner , Nastya Koro , Carolina Poll and Cassandra Melvin , Founder of Future Legends and all round wonder woman! Big thanks to Novoda for supporting this initiative and helping us to inspire future leaders and innovators!", "date": "2019-09-11"},
{"website": "Novoda", "title": "Achieve alignment & vision with stakeholder interviews", "author": ["Leonie Brewin"], "link": "https://blog.novoda.com/https-blog-novoda-com-stakeholderinterviews/", "abstract": "Stakeholder interviews allow you to ask powerful questions to business leaders to define future vision and drive success. When starting a new project, clearly articulated purpose and goals allow the delivery team to focus on success. Stakeholder interviews surface the primary needs of the business and collect unique insights to inform a single vision. These interviews can be just as helpful for stakeholders as they are for you, providing the structure needed to articulate a clear vision. Done regularly, they can help to capture directional changes, identify risks and ensure smooth, focussed delivery. What’s a stakeholder? Stakeholders are people who have an investment in an initiative. In this case, we’ll be talking about business stakeholders - people invested in the success of an organisation. The proportion of investment across stakeholders, whether financial, professional or emotional will directly impact the influence in business decisions: the larger the investment, the larger the influence. Which stakeholders should I interview? When selecting stakeholders for an interview, choose those people who have the biggest influence on the success of your initiative. Consider people who control the budgets, makes calls on investment or have valuable experience. These people are often difficult to get hold of and even harder to get an hour of dedicated time with so be direct and purposeful with your communication. A phone call or face-to-face will allow you to get the attention you need to explain your intentions and avoid communication being lost in emails or messaging systems. How should I get started? Before defining your interview questions, outline your goals. Consider what you want to gain, what you’d like to understand further and what your ideal outcomes would be. From these, you can design the right questions to reach your goals. What questions should I ask? Creating an interview script with your questions will ensure consistency between interviews and reduce bias. Time with stakeholders is precious so questions should be direct, clear and intentional. Sharing the topics or themes prior to the interview will allow you to make the most out of your limited time together. Here are some examples of questions we asked as we started a recent partner project. What is this project's objective, in your view? What problem are you solving for your consumers? What problem are you solving for your business? Why is this project important? What does success look like for this project? Do you have any KPIs? What concerns do you have about this project? What challenges do you see this project possibly running into? What is your role in this project? How will this project impact your day-to-day and your overall job? How should I run a stakeholder interview? Stakeholder interviews can be ran face-to-face or remotely. By keeping attendees to a minimum (you and the interviewee), you will create a more comfortable environment where your interviewee can speak openly and honestly about their thoughts, increasing the quality of insights. How should I start the interview? Start by thanking the stakeholder for their time. Explain the purpose of the interview and how you plan to use the insights to benefit the project. Listen intently to what the interviewee is saying and pay attention to how they are saying it. The emotion behind a response is often even more valuable than the words that are spoken and often the driving force behind behaviours, relationships and business decisions. To allow yourself to analyse behaviours further, you can ask the participant if they’re happy to record the session for your personal use before you get started. This will also remove the need to take notes during the session or have a third person involved. How should I finish up? Finish the interview with a debrief of the session. Repeat the purpose of the interviews and reiterate the value of their input. Explain how you will proceed from here and ask if they have any questions you can answer before finishing up.  Thank them for their time and get ready to analyse the results. What should I do with the insights? Once you’ve finished the interviews, you can watch back the interviews and start gathering insights. A simple table with the interview questions on one axis and participants on the other will give you a structure to collate and compare insights. Working through one question at a time, note any emerging themes and highlight any conflicts. This will allow you to iron out any conflicts between the stakeholders and propose a single vision for the project. Am I done? A united vision will help the team to stay focussed and motivated. It will provide a greater sense of purpose and ambition, allow for more ownership and autonomy and increase the chances of reaching success inline with business aspirations.", "date": "2019-07-09"},
{"website": "Novoda", "title": "Five hot-topic product design questions - answered!", "author": ["Tally Harman"], "link": "https://blog.novoda.com/five-hot-topic-product-design-questions-answered/", "abstract": "Design Lab Berlin and London , Novoda’s community Meetups, attended by some of the very best UI/UX product designers, had a colourful month. This June we had seriously inspiring industry experts, with decades of experience, cover hot topics and answer your questions. This article shares these resources with you, covering: Succeeding in design team leadership Keeping design consistency in agile environments Getting your team onboard with inclusive design Designing for the voice interface Designing without analytics ‘I really love Design Lab Meetup. It’s my favourite maker community. Folks are friendly and welcoming, especially with first time speakers and attendees.’ - Ataul Munim, Android Developer Berlin Design Lab hosted by Volkswagen Digital Labs Berlin blew us out of the water with experience! Our speakers had over twenty years of experience in the Product Design industry between them and their wisdom was a great hit with Design Labbers. Succeeding in design team leadership Working in a design team is not always easy. Would you agree? For a product to succeed it needs balance, direction and speed of execution. In startups, a UX designer has more freedom and flexibility but speed without communication can create imbalance or misdirection. Corporations, on the other hand have solid structures and processes ensuring balance and direction, but they lack speed. Marin Metohu, Lead UX Designer, shares what it really means to lead UX design. Keeping design consistency in agile environments How do you keep the look and feel of the product consistent in an agile environment without losing track? When establishing a business in the evolving market you need to be fast and to know your users. An iterative and systematic design process allows the team to get feedback from users as early as possible to inform and direct design decisions. Hanna Sandar, Product Designer at Collabary by Zalando, shares her insights into how to scale fast with iterative design processes. “This community has great opportunities to network with other creatives” - Bernard Kearney, Digital Designer London Design Lab hosted by BabylonHealth Design Lab London took the community to the offices of Babylon Health where we sat in their tropical surroundings and enjoyed some brilliant conversation with their design team and over 100 designers. Getting your team onboard with inclusive design If you're passionate about inclusive design how do you get your team and company onboard? This is the story of Android Developer, Ataul Munim and what Monzo did to drive inclusive design and how you can do the same. Designing for voice interface As a voice designer, how do you assess the usability of a conversational interface? Steven Chang, a designer for Babylon Health shares his experience designing and researching voice and conversational UI. In this talk, Steve stresses the need for designers to understand how users naturally communicate in order to design the best possible user experience. Designing without anayltics What do you do when you don’t have any data to create great product design? Great question! Liam Carter-Hawkins, Product Designer for Babylon Health, shares three ways in which you can gain quick data to inform better product design decisions in his talk 'designing without anayltic' ‘I attend design lab to get inspired’ - Olga Kruglova, UX/UI Designer at Financial Times And that’s a wrap for this month. Jobs If you are interested in joining the Novoda Product Design Team (the masterminds behind design lab) you can read up on the role and apply on our workable page . Want to speak? Has all this community love sparked joy somewhere inside? Do you want to share what you’ve currently been working on or have recently learnt? Message the supporting team behind the event at designlab@novoda.com . Attend an event Each month we see the community evolve, grow and thrive with passionate people, supported by inspiring companies. If you want to attend the next event in your city, check out our Meetup communities for the up and coming schedule Design Lab Berlin and Design Lab London", "date": "2019-07-08"},
{"website": "Novoda", "title": "IoT at Liverpool Makefest 2019", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/makefest-2019/", "abstract": "Liverpool MakeFest is a family-friendly event celebrating low level technologies, hardware, hacking and making. Here's our review on what we did and what we learnt. Makefest celebrates it's 5 year annivesary this year! We participated with a stand at the 2019 event and built an immersive Internet of Things gaming experience, called Wack-a-Light to engage with those attending and learn more about Kotlin, Firebase and IoT for ourselves. We have been participating with MakeFest for the last 5 years and every year it gets bigger and better. This year we brought back our Dungeon Crawler game to the stand to have alongside Wack-a-light We had a lot of people visiting our stand of a wide range of ages and all with a wide range of knowledge, which made for some great conversations. People played the games and were really interested in what Android Things is (every is suprised when they hear about a Raspberry Pi running Android) as well as the Pir matrix for this years game. Most attendees of MakeFest were families with children aged 7–17, which is why we created an IoT game. This year we took inspiration from the human habit of using materials once. We had a wine advent calender (empty) laying around the house and wanted to see if we could repurpose it for a game. we wanted to build something that would be engaging to people of all ages. So we decided to build an IoT Wack A Mole game. Basing the game on the original arcade versions (see wikipedia) . Wak-a-light uses 24 Pir motion sensors to detect hand movements over each of the holes in the box. It also has a 24 LED matrix for lighting up each hole, as well as a 8x24 led display at the front for showing another representation of the game and the score when the game was over. The game was written in Kotlin, with a game loop running at 10fps. This meant we could detect motion as often as the hardware allowed, and updated the position of our 'moles' 10 times a second. The app itself was architectured so that it could detect if it was running on a mobile device or on IoT hardware, that way it could show an game app using your touch screen if running on your phone and use the LED matrix if running on a IoT device. The game is arranged so that the 'config' of the game is a set of properties that are loaded before hand. i.e. the length of the game, the number of moles at the beginning, the respawn rate of moles, the duration of moles, is all configurable as a set config. After someone plays a game, the game stores their score and the config that they played in Firebase. A separate tablet application then loads those scores into a highscore table. A player can click on their score in this tablet and claim it as their own. To claim is as their own they need to feedback on the \"fun factor\" of the game 1-5 represented as 5 sad to smiley faces. Once that's done they can add their name for the highscore table. With this data gathered, it's now possible to correlate the config & scores of the game to it's rating. In another post, we will show you how we took that dataset and pushed it through a generated neural network to create a Machine Learning Model. This model can then be used to determine if any new game config will be a \"fun\" game to play. I really enjoy Makefest, every year their are new things and really interesting builds or people to talk to. This year a lot of the kids where very savy about Makefest and where asking questions about how to do things themselves, which is so awesome! I learnt a lot from building the Wak-a-light game, especially about user feedback and how the sensors can be tweaked to be even better next time! Paul Blundell My first ever Liverpool Makefest was lots of fun. Between seeing what other stalls had been tinkering on, and explaining both of the innovative games - I met lots of new faces and enjoyed understanding how stuff works. Our stall was pretty busy throughout Makefest, with children and parents alike participating and enquiring in both games. Games went down well with the audience and kids especially loved them. I'm feeling inspired to brainstorm my own game ideas for next years Makefest - Its amazing how entertained everyone was (myself included!) by a door-stop joystick and an LED strip. Frank Boylan Some of the other interesting stands we noticed included: TrainTrack LED strips with kinetic controllers. Object scanning lasers for recreation with 3D printing, Nixie Clock and mechano kids toys that where recycled to give them a new quirky lease of life. Grumpy Mike's electro gadgets Object scanning lasers Nixie Clock Refunctioned Kids Toys Attending MakeFest had two aims for us: We love helping the community, and seeing young people engaging with technology is a real inspiration. We enjoy helping to spark creative ideas and showing how easy it can be to get into technology. Secondly, we are always learning and trying to push the boundaries of our own knowledge. Working with hardware is not a daily occurrence yet for most of our projects, so working on a new project for MakeFest really pushes us to learn new things. We can't wait for next year. Each year we learn more and want to make it more awesome than last. Next year we want to perhaps demonstrate Machine Learning technologies in a kid accessible way. If you have a platform or product and are interested in collaborating, feel free to reach out to us @Novoda. Stay tuned!", "date": "2019-07-02"},
{"website": "Novoda", "title": "How the largest airline in Europe experimented with AR to drive sales", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/how-the-largest-airline-in-europe-experimented-with-emerging-tech-to-drive-sales/", "abstract": "One of the largest airlines in Europe, undergoing their largest digital transformation, wanted to explore Augmented Reality and how it would benefit their customers, engage more of their buyers in the buyer's journey and increase their sales revenue. One of their products, custom airplane build-outs, enables customers to design the airplane of their dreams. They work with their customer to tailor many parts of the aircraft. From the interior layout and cabin furnishings down to the fabric used for the seats, the airplane comes together piece by piece. Most of these choices are made by sifting through traditional catalogs, so a lot is left to the imagination until the late stages of the processes. For such a premium customer, surely there should be a way to provide a more premium, state-of-the-art experience. The Problem: Retailer and premium customers alike, are expecting and relying on imagination to sell the product. Showing different customisations to customers helps them find the most suitable product for their needs. However, there are cost and time barriers to doing this efficiently with a physical product. Enter Augmented Reality (AR) Augmented Reality (AR) can help showcase different customisations in real-time. This premium experience brings the customer on a journey where possibilities previously limited to the imagination come to life. Whether a playful or practical use case, AR creates an emotional bond and signifies that a brand seeks to deliver a product that goes above and beyond. This is a transferable example to explore how modern Augmented Reality could bring the customer experience to life. A customer could mix and match options in 3D right in front of them. In real time, they could explore countless different configurations, creating an immersive experience for a customer. For the business, it is an opportunity to go above and beyond to show the high quality of the brand and product, engaging with their customers in an innovative way. The Approach During the project, we applied the design thinking approach to go from understanding the problem to the final prototype. Design thinking is a systematic approach to dealing with problems and developing new opportunities. It is the ideal framework for creating innovative products that satisfy the end-user. We value and prioritise meeting with stakeholders from different roles, from design and engineering to business development. Keeping the scope focused, we aimed to learn from many perspectives to develop a shared understanding of how they deliver their custom aircraft product to customers. Together, we reviewed competitors, benchmarked other AR experiences in the aviation industry, and gained a greater understanding of their challenges. With these insights, we sketched out ideas and decided on a minimal proof of concept prototype that would be suitable for experimenting with transforming the current product to a premium customer experience. Then came time for the heart of the experimentation - prototyping! Novoda assembled an expert team, with the latest knowledge and skill set, ideal for the project. Understanding the business realities and the importance of ROI, we curated mini-sprints with lightweight planning to reduce the overhead and get the product to a validation stage as quickly as possible. After just a handful of mini-sprints, the prototype was presented back and ready for testing. The Prototype: What tech did we choose? The proof-of-concept prototype was built natively for iOS using Apple’s ARKit. We built for iOS because this operating system is more widely adopted in our target group and because we aimed at providing a high-fidelity experience together with an iPad. The 3D model of an airplane, animates into the real world, creating an experience where the airplane drives right into the live camera's view for the customers viewing pleasure. Once the airplane becomes part of the customers real world, they can explore the full range of exterior and interior products with high precision. Impact Provides seamless visual sales and marketing collateral for a luxury product Reduces time and cost attached to tailoring products to customer requirements Consulting, design and specification services reduce overheads Looking to the future In addition to expanding the items for customisation to include a robust catalog, future iterations of the prototype will unlock more of ARKit's technical capabilities such as allowing customers to have shared experiences that persist over time. An example of this evolution within the customer journey is if a customer was using the app to create a custom build of an airplane, they would be able to collaborate with a team on the same, shared experience over multiple sessions. Great projects take time to construct, so they would be able to make changes incrementally — saving and going back to the proverbial drawing board over multiple sessions. Augmented Reality opens many possibilities for enriching customer experiences to make them more immersive.", "date": "2019-06-21"},
{"website": "Novoda", "title": "Google I/O 2019 is over, here's what we are excited about", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/google-i-o-2019-is-over-heres-what-we-are-excited-about/", "abstract": "Google I/O 2019 was a blast! You can watch all 177 sessions on Youtube now. We've been talking and discussing all the new shiny stuff since last week and here's a selection of the things we're most excited about. Paul Blundell Head of Engineering ➤ Live Coding A Machine Learning Model from Scratch This was an excellent session by Sara Robinson using a tool called Colab to create a Tensor Flow ML model from scratch. Having a background in Android development I came to this with little experience but a lot of interest. From my understanding Colab is an online IDE for writing Python, it's backed by Google Cloud and runs the code somewhere ( don’t need to know ). Sara went through the steps of defining the problem, codifying that problem into python objects, then creating a model using Keras to train on that data and give a response. The actual task was to classify StackOverflow questions and automatically label them with the programming language they represented. I learnt a great deal about Machine Learning, not so much any of the theory but a lot of practical uses in model creation. Colab looks like a really easy to use tool, and this session is a great reference that I know I will be able to keep coming back to. This model creation example can be used in Android development to allow us to define and create our own models. We can then use other tools created by Google like ML Kit & Firebase to upload this model to the cloud, and have it sync’d to our devices. Thus allowing for on-device classification. I am currently building my own Model for classifying whether a game being played is going to get good feedback, this talk has given me the building blocks to make that happen. Tobias Heine Android Developer ➤ Build a Modular Android App Architecture This talk by Florina Muntenescu and Yigit Boyar is about choosing the right modularization strategy using Gradle library modules and dynamic feature modules including dynamic code loading. Modularization helps to scale a codebase by decoupling certain aspects of an application. Furthermore, dynamic features modules help the business to a/b test different implementations of a feature and can reduce the user drop off rate between installation and opening an application by shrinking the APK size. The talk compares the two strategies of splitting an application horizontally into modules by layers (e.g. IO, Domain and UI) or vertically by features (e.g. search and about) including migration strategies from one approach to another and best practices around testing and navigation. The part I found most exciting is about dynamic feature modules and dynamic code loading : Using Gradle library modules we can describe dependencies between modules, where a feature module exposes an API and a consuming module resolves it at compile time. Then, using dynamic feature modules this dependency can be resolved at runtime , meaning the binary being shipped to the user does not necessarily include all modules and dynamic feature modules can be downloaded on demand. Furthermore, the dependency has been inverted, so that instead of depending on a feature module, the dynamic feature module needs to depend on the consuming module. This requires the consumer to use reflection in order to resolve the dependency on the provided API. The talk also mentions a github sample repository demonstrating how to load and use classes from dynamic feature modules using either reflection , the ServiceLoader , or Dagger 2 . Daniele Bonaldo Android, Wearables and IoT GDE ➤ Android Jetpack: Understand the CameraX Camera-Support Library This talk introduced the new CameraX support library in Jetpack. If you ever tried to create an Android camera app, or just to interact with the current Camera API you know how hard it can be to configure it and to follow the preview and capture flow. I did that for my Do-it-yourselfie photo booth and the final code was far than clean. That’s one of the reasons that got me excited about the new CameraX API. Based on Camera 2 API, CameraX is backward compatible down to Android L and it allows to instantiate, configure and manage the camera with a more fluent and easy to follow API. The whole library is built on top of three main use cases: Preview Image analysis Capture These use cases are bound to the activity lifecycle, so that Camera X is lifecycle aware and it’s not needed to worry about starting and stopping the camera anymore. Camera X is consistent across different devices and it supports what are called Extensions . These are device-specific capabilities (like portrait, night mode, HDR, etc) and with the new library it’s incredibly easy to check if the extension is available and allow the user to enable it Luis G. Valle Mobile Principal Engineer ➤ What's new in Architecture Components? Architecture Components have grown up a lot since they were first presented in Google I/O 2016. According to Google internal surveys, they are used now by more than 70% of all the professional developers out there. This year, Google announced that Architecture Components are going to be Kotlin first from now on. This means they are going to be written in Kotlin and their APIs designed for Kotlin. Java will be supported too. These are two topics caught my attention from this session: DataBinding will have better tooling support. Now the binding class will be generated live, without waiting for compilation. That means, when you change an ID in your layout, it will change immediately in your Kotlin code. There’s going to be support for refactoring, so when you change the name of a function in your viewModel, it will change automatically in your layout. And finally, error messages are going to be improved as well, so no more mysterious “binding fail” errors. Talking about ViewModels, there’s going to be a way to access the activity saved state in the viewModel. Now, we’ll be able to get and put stuff in the saved state, so if the app process is completely killed, we’ll be able to restore everything to the way it was before. What are the things that you liked the most? Let us know on Twitter , we're excited to chat about it!", "date": "2019-05-15"},
{"website": "Novoda", "title": "Exploring IoT, My First Build - DMX Revolving Art Installation", "author": ["Thomas"], "link": "https://blog.novoda.com/exploring-iot-my-first-build-dmx-revolving-art-installation/", "abstract": "For quite some time I have been interested in working with artists to help bridge the gap of technology (specifically hardware and programming) and art, to be able to bring some of the skills and knowledge I have acquired as a software developer and apply these to the art world. Recently I had the opportunity to collaborate with two artists in Berlin who were looking to bring a sense of movement to an art installation they had built. The installation (as pictured) was a large and intricate chandelier that was to be hung from the ceiling, and the goal was to hang this chandelier from a motor that would rotate it in different directions with different speeds. It was important for us that we created a sense of organic movement, something that wasn’t a repeatable loop but instead would appear to have a mind of its own, unable to be predicted. The end result of all of this would be an installation that could continuously move and change in different ways over an extended period of time without the direct influence of an individual. The board would allow us to store and run the code responsible for the movement, the board would then communicate over a protocol the motor would understand, the motor would then receive this and spin the installation. Requirements One important factor was for us to be able to communicate wirelessly with the board. We were going to be hanging this setup from a ceiling, and once it was up, we didn’t want to have to take it down again to make changes to the code or to reboot. An ESP8266 board is a great candidate for this, they are low-cost, programmable, WiFi enabled and had enough pins to achieve what we wanted. To achieve this feeling of organic movement we needed a board that could store and run code responsible for communicating what kind of movement should happen and a way to communicate that to a motor. We were also considering adding more functionality in the future, so having a board that would allow us to add more code and components later was also important. We decided to go for the Wemos D1 Mini, which is a mini WiFi board based on ESP8266 with 11 pins available to connect to the components we were wanting to use. We also needed a motor that was strong enough to hold up the chandelier as well as have enough force to spin it in different directions with different speeds. For this, I had considered building a custom rig with a large servo motor, but for simplicity and time reasons ended up using a disco ball rotator (motor used for hanging and spinning disco balls). Last important requirement was a way for us to communicate with the motor from our microcontroller unit, the Wemos D1 Mini. We chose a motor that was able to receive digital multiplex (DMX) input to control its speed and rotating direction. For our board to communicate with our motor using this standard, we would need to connect a DMX output plug to pins on our board which could then be connected to the input of the motor. Sending different signals on this pin would allow us to tell the motor what speeds and direction it should move in. What's needed Hardware List An ESP8266 Board (I used the Wemos D1 mini) MAX485 Module TTL To RS-485 (for the DMX pin) 2-24V to 5V DC-DC Boost-Buck converter (this was used to step down  - the voltage that I was receiving from an external power source ) 3-pin XLR socket 3-pin XLR cable DC power supply Panel mount 2.1 x 5.5 mm DC barrel jack Solder and wires Disco ball rotator with DMX capabilities Equipement Soldering Iron Time to start making The first step was to find some libraries that I could use to communicate with the motor using the DMX standard. I planned to write my sketch in C++, so taking a look at a few different libraries, I decided on https://github.com/Rickgg/ESP-Dmx . The library was simple and had all the basic functionality I would need, it could write a value out along a channel that could be changed when needed. This would allow us to easily change the speed and direction with a few lines of code. Before actually soldering all the components together (which can be somewhat difficult to reverse - for a beginner like me at least) I got all the components set up on a breadboard to test that it would actually work. This was a little difficult at times to figure out due to my limited knowledge of breadboards and circuits, so I made a diagram to follow so I knew what goes where. I will definitely do the same for future projects too - makes things a whole lot easier if you have something there to reference. After setting up the breadboard, I got to writing some code to make sure everything was connected properly - and to make sure things worked as I was expecting them to. I tested simply rotating the motor back and forth over DMX. It worked! From here I was able to play around with the code to get a better understanding of how things were actually working and from that I was able to write a small program to control the rotation direction and speed. Now that everything was tested and proven that it would work, I felt confident to solder all of the components together. This was my first time doing some proper soldering and I was lucky to have somebody there to show me some techniques and tricks. If anybody else is attempting this for the first time then I would highly recommend watching some youtube videos or similar and then practice a bit before soldering anything important. With everything soldered together, it was now ready to be set up properly with the chandelier. Next step was actually hanging the chandelier off the disco ball rotator and do some live tests. With the chandelier now hanging up, we were able to test everything and tweak the code to produce the effects we were wanting - the right speeds and the right amount of turns in rotation and how often these changes would happen. We didn’t want to have to climb up and plug the Arduino into the computer every time we wanted to make a change, so we made this whole process much easier by using over the air updates (OTA’s). With the OTA’s we were able to change the code as we like and upload it over a shared WiFi connection without having to think about disconnecting the whole setup. Some learnings Breadboarding is really important. It allows you to set up and test the circuit and see if it works as expected and to confirm some assumptions. Having it all set up on a breadboard means you are able to tweak and modify it quite easily during the initial development stages. It can be difficult to reverse some soldering, so it’s best to get everything set up as you would like it before you make things more permanent. Soldering has a technique. I hadn’t really done much soldering before, and I was lucky to have somebody close by to show me the ways. It’s not too difficult to learn, but I would definitely recommend watching some videos before you try or getting a hand from somebody who has done it before. Hardware is different from software. I’ve mainly just worked with software in the past and with software one has the power of “undo”. And if there is anything that needs changing or fixing in the code, then it usually doesn’t take too much to fix this. Hardware, on the other hand, is much less forgiving. If you make a mistake you are at risk of burning out your board or components, or if you solder the wrong wires and want to change, it’s not as simple as hitting a shortcut on your computer. Next time? This was my first time building something like this, and so with that were a few learnings along the way. If I undertook a project like this again there would be a few things I would do differently. One thing I learnt quickly was to buy 3 of everything. When I was purchasing my components, I bought exactly everything I thought I needed for the project. What I didn’t factor in though was potentially badly manufactured components, or that I may break some components when I was putting everything together. I had a couple of pieces break on me, which meant I had to order some replacements each time. Next time, I’ll be ordering a few of each of these components. Generally, these components are inexpensive, and you’ll usually get a discount when you buy in bulk rather than singles. The last thing I want is having delays in the project because I’m having to go back to the store constantly. The added bonus is you now have some components on hand for your future projects! I learnt that it was important to be patient with soldering. I spent a lot of time getting everything set up and writing the code, but then I didn’t take time with the soldering. As a result, I ended up with a bit of a messy collection of boards and wires. Next time, I’ll make sure I spent some time thinking about a good setup for the wires before diving into the soldering. When I was in the process of all of this making and building I forgot to take a minute to snap some pictures of what I was doing. Fewer memories for me to look back on, and fewer things I can share with others. Wrapping up The project was completed! I set out with little understanding of these technologies and ended up with a working project that was able to do what I had planned for it. The “Hello World” moment of having the code I had written spin this motor was a really rewarding feeling. Before I started, I had only tinkered here and there with these kinds of projects. I wasn’t sure how it would all turn out, but with a bit of research, tutorials and help from some friends along the way, I was able to complete this. For anybody who is wanting to get involved with these sorts of projects, I say go for it. It can seem daunting at the start but pick something simple and go from there. I’m super excited to be working on my next IoT projects. This experience has shown me what sorts of things can be achieved through these projects. Having some knowledge and skills of putting together hardware along with a software background opens up the potential for lots of exciting projects. Photos of the installation credited to Andrea Rojas", "date": "2019-04-26"},
{"website": "Novoda", "title": "Git commands for code detectives", "author": ["Juanky Soriano"], "link": "https://blog.novoda.com/git-code-detectives/", "abstract": "Have you ever had to understand a codebase where the lack of documentation reached all levels? From the absolute absence of a wiki to completely cryptic file names or meaningless code change commit messages, the only saving grace being Git version control. This happened to me recently and I had to really deep dive using Git in order to understand how the system works, let me walk you through that investigation, it’ll be fun, I promise. In this blog post, we are not reviewing good practices for documenting projects up front, but instead, deep diving into the git tools we can use in situations where we need to deeply investigate into the history of a project, resolve the mysteries within it and ultimately regain control over it. None of the tools by themselves will solve an enigma, but hopefully, each tool will point you in the right direction. Using these tools with intelligence and perseverance will help you to resolve any puzzle. If you also take the appropriate actions upon new discoveries, especially documenting as you go, then you and everyone else on your team will have absolute control over the system. Now, let's have a look at the set of technical tools I recommend with this post. git shortlog When fighting an unknown system, the first thing you want to do is look for the technical experts who are still around you, in the team or in the community. Therefore you should understand who are the top contributors to the system. Hopefully, they will have the answers you need. git shortlog -e -n -s will print the list of contributors in the whole repository ordered by number of contributions. git shortlog -e -n -s [path] will print the list of contributors in the specified [path] ordered by number of contributions For example: > git shortlog -n -s -e \n164 Willy Wonka <willy.wonka@fakedomain.com>\n59  Indiana Jones <indy@fakedomain.com> git effort At this point, you still have no clue about what the system does, and everything you see is a bunch of seemingly random files. Perhaps they are not so random, so let's figure out which file is the one where the old team put the greatest effort in. git effort will print the number of commits that affected each file in the repository. git effort --above number will print the files affected for a number of commits greater than number For example: > git effort --above 50\npath                                       commits        active days\npath/to/file/verychangedfile.cfg           77             33\npath/to/file/anotherverychangedfile.cfg    67             32 Note: git effort is part of the git-extras package, therefore you might not have it out of the box. Sure it is available for your OS. git blame git blame is useful for those situations where you have already identified an interesting file, and you want to know who made changes on that file, and more importantly, what changes. Using git blame will output the file content alongside with author for a particular line, and commit hash for the same line. For example: > git blame interestingfile.cfg\n225c29ba (CATS 2018-09-27 03:34:38 -0500  1) all your\n225c29ba (CATS 2018-09-27 03:34:38 -0500  2) base\n225c29ba (CATS 2018-09-27 03:34:38 -0500  3) are\n225c29ba (CATS 2018-09-27 03:34:38 -0500  4) belong\n225c29ba (CATS 2018-09-27 03:34:38 -0500  5) to us git grep You might not know yet how the system works, but you are following the trail of a concept or idea and want to search across multiple files that could potentially be related to that. git grep regexp will print the lines in all the tracked files where the regexp is present. For example, you know that the word username is or was written somewhere inside the system, but you don't know where: > git grep username\n/very/hidden/path/in/system/crypticfilename.txt:\tusername=anonymous git log This is the most powerful git tool to understand the history of the system. git log will output, ordered by date, the history of commits in the repository alongside with the author, and the commit message. All of this information is great but sometimes you need more, let's look at some options to make our search more granular. git log -G This command works on a very similar fashion to git grep so you might want to use it on a similar situation. The difference is that git log -G regexp will output the commits where regexp is present on the diff associated to the commit. For example, continuing with the research about where username could be present in the system, let's see the commits are related to it: git log -G username\ncommit 9082a4abd6497e4dee348d5ccd74c472167b4955\nAuthor: Agent -007 <agent@noob.com>\nDate:   Tue Jul 18 12:36:02 2017 -0700\n\n    Add secure details to file\n\ncommit 71669696c5570f6e627f4ccc5722647e8ff14514\nAuthor: Agent 007 <agent@not-that-noob.com>\nDate:   Mon Jul 10 12:35:27 2017 -0700\n\n    Remove username and password from file Note: Sorry for the offtopic, but this is important. Obviously Mr. Agent -007 did a huge mistake by committing sensitive information into the repository, but his successor Mr. Agent 007 didn't make it better. What the last one should have done, is to remove the particular change completely from the repository history. git log --follow -- filename Aha! you did eventually find an apparently relevant file in your system and you want to understand the changes applied to it and, potentially, make sense of it. This is a very good use case for git log --follow -- filename which will output the history of a particular file. For example: > git log --follow -- file.mk \ncommit a35da6d8414b199e5e0629237ba047edd07783d3\nAuthor: Rocky Balboa <rocky.balboa@stallion.com>\nDate:   Wed Oct 3 03:27:14 2018 -0500\n\n    Enables building with a right hook\n\ncommit f07b8fdc3ea77f161e2bd1d4153a6478f19d426f\nAuthor: Rocky Balboa <rocky.balboa@stallion.com>\nDate:   Thu Sep 27 04:48:34 2018 -0500\n\n    Enables building with a left hook git log [--since=date] [--until=date] Perhaps you still need more research, a good idea might be to dig again  in the repository history but this time making some filtering by date. By specifying the --since=date or/and --until=date you can filter the repository history by date, hence a more granular search is possible. For example: > git log --since=\"27/09/2018\" --until=\"28/09/2018\"\ncommit f07b8fdc3ea77f161e2bd1d4153a6478f19d426f\nAuthor: Rocky Balboa <rocky.balboa@stallion.com>\nDate:   Thu Sep 27 04:48:34 2018 -0500\n\n    Enables building with a left hook git bisect You are starting to understand how the system works and voila, you realise there is a bug. You still don’t have enough information and want to understand what change introduced the bug. git bisect performs a binary search to find which commit in the system introduced the bug. You first select a \"bad\" commit where the system is broken and a \"good\" commit, where the system isn't broken. Then git bisect will select a commit between those two and ask you whether the selected commit is \"good\" or \"bad\" and so on until the responsible commit is identified. Let's see a simple example, suppose you are trying to find a bug currently present but which is not present on the commit 71669696c5570f6e627f4ccc5722647e8ff14514 : > git bisect start\n> git bisect bad\n> git bisect good 71669696c5570f6e627f4ccc5722647e8ff14514 This will trigger a git bisection. Now git will drive you through the bisection checking out a proposed  commit which you have to evaluate again as bad or good . Bisecting: 2 revisions left to test after this (roughly 2 steps)\n[2f9c695953cf9854c50108806c22a38ab9ae1d5a] Coding after party.\n\n## Time to test and evaluate\n> git bisect bad         ## Your test shown the system is broken Reiterate the process... Bisecting: 0 revisions left to test after this (roughly 1 step)\t\t\t\t\t\t\t\t\n[03bc5ebe773ce1ade24a60aecfdec704002b10e7] Introduce enhanced output to build system\n\n## Time to test and evaluate\n> git bisect good        ## Your test shown the system works And there you go!  Finally git bisect ends by showing you which was the first bad commit 2f9c695953cf9854c50108806c22a38ab9ae1d5a is the first bad commit\ncommit 2f9c695953cf9854c50108806c22a38ab9ae1d5a\nAuthor: Ivan Drago <drago@russianexpress.com>\nDate:   Wed Oct 5 22:22:14 2018 -0500\n\n    Coding after party. Another very interesting characteristic about git bisect is the possibility of automating the bisection by making use of an external tool that will be able to evaluate the correctness of the system given its current state. For example, you can write a script check.sh which is able to determine if the system is currently working as expected or not. The script, let’s name it check.sh , should return 0 when the current state is good and a value between 1 and 127 for when the system state is bad . Then you can run git bisect run check.sh to trigger an automatic bisection. > git bisect start HEAD v1.2 --      # HEAD is bad, v1.2 is good\n> git bisect run check.sh      # \"check.sh\" checks the system\n> git bisect reset                   # quit the bisect session Note: Actually the return code 125 for the script is reserved for when the system cannot be checked, on which case git bisect will skip the particular commit which made the script return that value. Conclusions There is no magic wand to get all the answers you need when facing a lack of documentation on a system, but that shouldn't prevent you from moving forward. As a Software Crafter, you should be able to look into your toolbox and make use of many tools in order to resolve your problem. Sometimes the solution won't come directly, but with perseverance, effort, and using the right tools you can always move in the right direction. git is a very powerful tool and you shouldn't limit yourself to the straightforward commands you use daily for development; instead push yourself to dig deeper and embrace Git’s full potential.", "date": "2019-03-26"},
{"website": "Novoda", "title": "Working with Legacy Testing Frameworks", "author": ["Jonathan Taylor (Head of QA)", "Sven Kroell"], "link": "https://blog.novoda.com/working-with-legacy-testing-frameworks-part-1-huge-checks/", "abstract": "As projects mature, they pick up a lot of history. This can lead to working with legacy integration test frameworks, which can over time become slow and flaky. After seeing a few of these we've have noticed the same anti-patterns emerge. Let’s have a look at those and how we can correct them by using \"good practice\" pattern. Sullied checks One antipattern to investigate is putting everything in the checks (with maybe some helpers around). Specifically the antipattern, where testdata, locators and flows can be found within the checks. @Rule\npublic ActivityTestRule mActivityRule = new ActivityTestRule<>(\n       MainActivity.class);\n\n@Test\npublic void signIn_ValidCredentials_loginPossible() {\n   onView(withId(R.id.main_activity_sign_in_button)).perform(click());\n   onView(withId(R.id.sign_in_activity_username_field)).perform(typeText(\"Username\"));\n   onView(withId(R.id.sign_in_activity_password_field)).perform(typeText(\"Password\"));\n   onView(withId(R.id.sign_in_activity_submit_button)).perform(click());\n   onView(withId(R.id.main_activity_sign_in_button)).check(matches(isDisplayed()));\n} Setting up checks this way leads to the problem that it will be hard to scale those. When you think about maintainability and extensibility, it is a worrisome trend that engineers are building their checks in this manner.  Because of the code duplication maintaining will eventually become a tough task which means that you'll end in the maintenance hell. Depending on the complexity of the software that you're testing it also can be complicated to understand what the check is doing in detail. The reality is that, while you do have checks running, they become either flaky or take ages. Eventually, those checks get disabled or ignored, which makes all the effort you put in not to have been worth it. What to do? First off - you'll need to identify the problematic point: Test data generation is mixed with the execution. Description of long flows. Locators are put into the check file and often duplicated. Magic Numbers can be found in the code. Unclear arrange, act and assert. Once you've identified specific problems in the legacy test code, try to isolate those and refactor your Framework. What follows are some examples and some approaches for rectifying. Extract Magic code One possibility would be to create constants representing the test data. By doing this, you could already remove some duplication and make it more transparent for other developers to read and understand. @Test\npublic void signIn_ValidCredentials_loginPossible() {\n   private String username = “Username”;\n   private String password = “Password”;\n\n   onView(withId(R.id.main_activity_sign_in_button)).perform(click());\n   onView(withId(R.id.sign_in_activity_username_field)).perform(typeText(username));\n   onView(withId(R.id.sign_in_activity_password_field)).perform(typeText(password));\n   onView(withId(R.id.sign_in_activity_submit_button)).perform(click());\n   onView(withId(R.id.main_activity_sign_in_button)).check(matches(isDisplayed()));\n} As we're already working with the test data, a next good step could be creating a non-primitive data type and fixtures which could help you to create a domain specific language in your framework. However keep in mind that it easy to set up, but it can get very messy if there is a lot of data needed to execute the checks. public class User {\n   private String username;\n   private String password;\n\n   public User(String username, String password) {\n       this.username = username;\n       this.password = password;\n   }\n\n   public String getUsername() {\n       return username;\n   }\n\n   public String getPassword() {\n       return password;\n   }\n}\n\n\n@Test\npublic void signIn_ValidCredentials_loginPossible() {\n   User user = new User(“Username”, “Password”);\n\n   onView(withId(R.id.main_activity_sign_in_button)).perform(click());\n   onView(withId(R.id.sign_in_activity_username_field)).perform(typeText(user.getUsername()));\n   onView(withId(R.id.sign_in_activity_password_field)).perform(typeText(user.getPassword));\n   onView(withId(R.id.sign_in_activity_submit_button)).perform(click());\n   onView(withId(R.id.main_activity_sign_in_button)).check(matches(isDisplayed()));\n} A different approach could be creating data provider for the data, which you could reuse in other contexts. This method is a bit more complicated to set up but is better if you have a lot of data. @Test\npublic void signIn_ValidCredentials_loginPossible() {\n   UserRepository repository = new UserRepository();\n   User user = repository.getDefaultUser();\n  \n   onView(withId(R.id.main_activity_sign_in_button)).perform(click());\n   onView(withId(R.id.sign_in_activity_username_field)).perform(typeText(user.getUsername()));\n   onView(withId(R.id.sign_in_activity_password_field)).perform(typeText(user.getPassword()));\n   onView(withId(R.id.sign_in_activity_submit_button)).perform(click());\n   onView(withId(R.id.main_activity_sign_in_button)).check(matches(isDisplayed()));\n}\n\npublic class UserRepository {\n   private static String DEFAULT_USER_NAME = \"Username\";\n   private static String DEFAULT_PASSWORD = \"Password\";\n   private static String WRONG_PASSWORD = \"12\"\n\n   public User getDefaultUser() {\n       return new User(DEFAULT_USER_NAME, DEFAULT_PASSWORD);\n   }\n\n   public User getUserWithWrongPassword() {\n       return new User(DEFAULT_USER_NAME, WRONG_PASSWORD);\n   }\n} You should decide which one of these methods is the best approach depending on your needs. A good rule of thumb will be to use fixtures if you have maximum two or three sets of data. If it becomes more or if you think it might grow you should opt-in for the second option. Page Objects A next step could be creating page objects which represent your application and extract those into classes. This method helps you again to reduce duplicated code. It is one of the most used and sometimes unfortunately misused patterns in the industry. The Page Objects should only contain the locators and methods to access them. Otherwise, they become bloated, and you may create an undesirable god class. public class MainPage {\n\n   private ViewInteraction SIGN_IN_BUTTON = onView(withId(R.id.main_activity_sign_in_button));\n\n   public void openSignInPage() {\n       SIGN_IN_BUTTON.perform(click());\n   }\n\n   public void validateLoggedInStatus() {\n       SIGN_IN_BUTTON.check(matches(isDisplayed()));\n       SIGN_IN_BUTTON.check(matches(withText(\"Sign out\")));\n   }\n  \n} By not putting the flows in the page objects - it creates a better maintainable piece of code, but it also pushes the responsibility of handling flows to the checks. By letting the checks describe all the flows, they can become quite large, which again leads to files which are hard to understand. One possible solution for this would be either to use the screenplay pattern or to create flows which are working as an interface between the checks and the page objects. Flows Flows are a form to organise activities performed in your checks into repeatable methods. They are combining different Page Objects and their respective parts into an understandable and human readable DSL which less experienced automation engineers can plug together. They can help you keeping the checks small and straightforward as well as having the flows to reach specific points of your application at one position in your code that changing this will be much easier. public class LoginFlow {\n\n   private MainPage mainView;\n   private SignInPage signInPage;\n\n   public LoginFlow() {\n       mainView = new MainPage();\n       signInPage = new SignInPage();\n   }\n\n   public void doLogin(Credentials credentials) {\n       mainView.openSignInPage();\n       signInPage.doLogin(credentials);\n   }\n\n   public boolean userIsLoggedIn() {\n       mainView.validateLoggedInStatus();\n       return true;\n   }\n\n\n   public boolean correctErrorDialogIsShown(String expectedError) {\n       signInPage.validateErrorDialog(expectedError);\n       return true;\n   }\n} Conclusion Setting up an automation solution is a simple task.  However it can get quite messy in a short while when you do not consider extensibility and maintainability. You should always think about architecture and the dependencies of your classes. In the end automation is like every other engineering task easy to learn but hard to master and you should always keep improving. To be fair, the vast majority of the input for this article came from Sven Kroell.  Thanks Sven.", "date": "2019-02-12"},
{"website": "Novoda", "title": "Embracing React Native", "author": ["Daniele Conti"], "link": "https://blog.novoda.com/embracing-react-native/", "abstract": "Over the past 6 months I’ve been working together with the team at The Times to lead the adoption of React Native in their Android app. It’s been an interesting journey, so I wanted to share our experience, including our successes, challenges and learnings. The Times is a large publishing company with many business requirements. They face challenges with keeping all of these requirements in sync across their codebases. The goal of the project was to unify their business rules and requirements into a single codebase. After considering some alternatives, they chose React Native, partly due to their very strong web background, and due to it being the most mature option at the time. The native mobile teams didn’t have previous experience in React Native or JavaScript. As part of the transition to React Native, the team organised workshops to bring everyone up to speed with the new technologies. In this project, there was a strong open source component: the various widgets were developed in a separate, public repository . These widgets are meant to be used both in Native and Web, thanks to the use of React Native Web . The navigation in the app is kept in native code, and React Native widgets are exposed to the app via a native module. This was done because we decided to maintain the same user experience quality as the current implementation, and the current solutions for React Native didn’t meet our requirements. What React Native offers is a minimum common multiple  between the two platforms, and didn’t fit with the rest of the application, nor did it work well for us. The teams at The Times are structured per feature, created as required. Usually every team is composed of an Android, iOS, Web, and backend engineers, plus a QA engineer and a product owner. What went well React introduced a very fresh and effective way of describing views in an application; the declarative approach lowers the mental strain required to understand the intended behaviour, and the state travels only in one direction, which makes it easy to control. Some problems were extremely easy to describe and solve with RN. For example, working on a modal image component in React Native made many things much easier than it would be in native Android and iOS code: in less than a day we managed to have a cross platform image component with pinch to zoom and rotation gestures, working well and fully tested. iOS Android Flexbox — the layout algorithm used in React Native — is a well established standard which has been around for over 3 years in the web world, is extremely expressive and battle tested. As a bonus, using Flexbox makes it easy to port similar layouts to web and vice versa. Challenges Not everyone is comfortable with working in multiple level of abstractions and different technologies to the ones they’re used to. Some team members weren’t initially happy about the transition, and it took time to bring everyone up to speed. To work efficiently in React Native, the team needs to possess expertise in both mobile platforms, and React and JavaScript. All this knowledge is usually not held by a single person, but needs to be expressed in the team composition and, unless properly addressed, it can create knowledge silos. Encouraging pairing between JS and native engineers can help avoiding this siloing. Integration testing on emulators/devices is tricky due to the way that React Native works. On Android, for example, the most used instrumentation testing framework, Espresso, will not be able to run on a React Native app due to the React Native’s threading model. We partially addressed this by creating an idling resource that waits for React Native to begin displaying content, but we couldn’t completely remove all the flakiness in these tests. The developer community around React Native is still very young, and when facing problems we found that not all the solutions out there worked for us. We ended up having to reimplement some 3rd party libraries from scratch. For example, we had to create a brand new SVG rendering solution using an undocumented set of React Native features. Learnings When thinking about React Native, or any other cross-platform framework, you need to ask yourself some questions. Does it work for you? Where does it work? Are its strengths and weaknesses matching your product’s? Will it be an impediment or a boost to your product? It’s also very important to consider the team composition and company culture. Do your mobile developers want to work with it? Do you have the time to invest into forming the team? Forcing React Native on developers can make them rather unhappy, and not investing in their formation can result in an unproductive team. Due to it being very young, the risk of adopting React Native is way higher than developing a native solution with well established technologies. Sometimes the solution to a problem can be trivial, possibly easier than native, but other times one small issue might take days to solve Recently, there have been some really insightful posts about companies using React Native. Airbnb published a 5 parts article on their experience with it over the course of 2 years, and Udacity posted a retrospective about their experience with it . Interestingly, both companies are moving away from React Native after years of usage and heavy investment — not (just) for technical reasons, but because it was not a good fit with their teams and companies. It doesn’t seem like they regretted it, but after experimenting for quite some time they decided that the benefits didn’t outweigh the costs.", "date": "2019-01-01"},
{"website": "Novoda", "title": "Why no one reads your app copy.", "author": ["Team Novoda (Joint Thinkers)", "David Gattig", "Theo Allardyce"], "link": "https://blog.novoda.com/why-no-one-reads-your-app-copy/", "abstract": "Photo by Murat Bengisu / Unsplash Your users won’t read most of the copy you have in your app. This might sound harsh, but after watching people for years in usability tests, you come to realize that very few users actually take the time and effort to read what you have to say. Even when you are giving them an important warning or describing the core functionality of your product, they will gloss over it.  When users skip your copy, they can become frustrated when they run into problems. Frustrated users will leave your app.  In this blog post, we will outline the 3 most common reasons why this happens and what you can do to make every word count. 1. It’s too long or boring Apps are in a battle for screen time. People download apps, not manuals, because they want to solve their problems effortlessly. Walls of lengthy text increase the effort and cognitive load of your users. Your users will skip long texts entirely and might run into problems later on. Interact, don’t tell. Make it easy for users to understand the value of your features without using words. Archimedes already knew: “The only way to learn it is to do it.” Let your users go through one interaction at a time instead of trying to explain everything. If you cannot go around using text, try to be as succinct as possible. Instead of showing all the text fields or actions in one long and daunting screen, you might also want to break up the user flow into small steps. If you make each step easier users will walk further. Most traditional usability guidelines follow this strategy. 2. You interrupt the user Pop-ups, overlays, and dialogs are the staple of user interaction, they are also the bane of user experience. There is a costly war going on every second of your user’s attention. We have become almost conditioned to dismiss anything that gets in the way of our current task, the X button imprinted in our minds like the colour red to a bull. This is a problem for designers, developers and product managers everywhere, because the standard dialog is easy to design, easy to implement and arguably difficult to ignore. Instead of interrupting, try enticing. Dismissable banners and snack bars can actually be more effective than pop-ups because they can stay on-screen permanently, co-existing with other content until your user is ready to action it. If you do need to block the user, consider adding a “remind me later” option as this also allows you to gauge user interest through tracking. Timing is everything, although it’s tempting to maximise the reach of your dialog by showing it early, you may achieve a better conversion rate by waiting for users to complete meaningful actions in your app. Make sure you are designing your communication so that it is visibly different from standard pop-ups, banners or full-screen adverts. Another good strategy is to make your users come to you for the updates. Give users a small indication within the app that a new message is waiting for them and let them actively retrieve the latest information when it suits them. 3. Your users think they already know what they need When we’re designing apps, we tend to assume that users will scan the page, consider all of the available options, and choose the best one. In reality, though, most of the time users do not choose the best option, they choose the first reasonable option. This behavior is known as satisficing, a combination of the terms ‘satisfy’ and ‘suffice.’ As soon as your users find a button that seems like it might lead to what they are looking for, there’s a very good chance that they will tap it. You can use this knowledge to make sure that your users are not skipping past elements that might help them in their journey. Consider changing the flow or show buttons only after the user performed an action. Be mindful to only use this when you have to communicate something truly important. Your users will get frustrated if you put hurdles in their way. You can also change the text on the button from the standard “Continue” to an action, like “I did x” e.g. “I have my ID ready”. This technique can help with slowing down users who are tapping quickly away to get to their goal. Conclusion You made it to the conclusion (or you skipped to it ;D ). In the end, it can be said that users do not read your copy because it is too long, it is timed badly or they just scan your copy for the crucial bits because of the sheer amount of information aimed at them. Once you are aware of these pitfalls, you can employ a range of user experience improvements to make your copy more impactful and your user’s life easier in one go. written by Theo Allardyce and David Gattig", "date": "2019-10-17"},
{"website": "Novoda", "title": "Android Dev Summit 2019 just got recommended but you wouldn't believe who by?!", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/android-dev-summit-2019/", "abstract": "Android Dev Summit is a conference with over 25 technical talks, and a chance for the Android team at Google to share the latest and greatest of what’s going on. This was my first year attending, and it lived up to the hype. Keep reading to see my recommended highlights and best bits. Learning comes in many different forms, from listening to speaker talks (live or recorded), talking to others, or hands on learning with codelabs. I like to mix between these styles and #AndroidDevSummit did not disappoint. They have many codelabs ready to be done here and most of the talks recorded here . A slide reminding us that we can all get a little MAD sometimes. I really enjoyed the talk on Developing Themes with Style, this took UI programming & code organisation back to basics, Explaining when and where to use themes vs styles, as well as some great pro tips with gotcha’s in the layout system. Relying heavily on the inbuilt symantic properties, such as colorSurface or colorError , can really help keep your UI coding clean and minimal. Recording can be found here . This talk will start with a crash course in themes and styles and how to apply styling throughout your app while isolating theme dependent resources. We’ll then walk through applied examples of using the styling system to build material themed apps and dark themes. A slide from the Developing Themes with Style talk. Machine Learning on Android is getting better and better. This isn’t the underlying theory of ML changing in anyway, but the tooling that novices like myself can use. It reminds me of the early days of Firebase and how that exploded into an amazing toolset. The talk ‘On-device ML: Tackling Complex Use Cases with ML Kit’ showed the suite of tools available to use, including not just object detection, but bounding those objects to a cropped smaller detection to allow for even more accurate description and classification. Recording not yet available . ML Kit makes it easy to integrate ML powered solutions into your apps, either through our turn-key Vision and Natural Language processing APIs or with your own custom TF Lite models. Not only can you easily tackle singular tasks like Text recognition, Face Detection or Language detection, but you can also create more complex user experiences by chaining multiple ML Kit APIs or using these in combination with your own custom models. A slide from the On Device ML Kit talk. The best codelab from the event has to be the introduction to Compose. Compose is the (pre-alpha) new Android reactive UI framework. It promises to deliver a faster, cleaner and more flexible way of creating your UI. Drawing from the successes in Web of React and Angular as well as this paradigm coming to mobile with Flutter, and perhaps just a little because of the iOS Swift UI framework . You need to install Android Studio Canary 4.0 to have access to the Kotlin compiler needed to make compose work, 100% recommend checking out this code lab . In this codelab, you will learn, What Compose is, How to build UIs with Compose, How to manage state in Composable functions, Data flow principles in Compose A slide from the Compose Deep Dive talk. In conclusion, the Android Developer Summit is an amazing event for the proliferation of knowledge. Not just for those in person, but the fact that all the talks are put on YouTube straight away, that the codelabs are available and that social media is a-buzz with insights and knowledge. This all makes this event an awesome time of year in any Android Developers calendar. YouTube playlist: Android Dev Summit 2019 talk videos Android Codelabs: Android Dev Summit 2019 developer code labs Event page: Android Dev Summit 2019 home page (Title spoiler: Yes it was I, who recommended the Summit ;-))", "date": "2019-10-25"},
{"website": "Novoda", "title": "Oh Sh*t : There is no time to test", "author": ["Paul Blundell"], "link": "https://blog.novoda.com/no-time-to-test/", "abstract": "Writing a test is easy, ensuring your project is tested, is hard. This post is here to advocate for end-to-end testing, to make controversy mainstream and hopefully convince you to join the cause. At the Android Summit, Donn Felker shared his views on ‘testing for success in the real world’. Advocating for end-to-end tests over unit tests, and that this viewpoint is controversial. The following is a summary outline of the talk: Does that code you just wrote actually work? How do you know? How do your teammates know? You did write tests for that, right? Unit? Integration? End-to-End? You only changed two lines of a legacy piece of the app… but still, how do you know this didn’t break anything? Let’s face it testing your application is difficult and tedious. Where can you get the most bang for your buck? What’s the 20% of work that gets you 80% of the return? In this session you’ll learn where you can focus your attention to gain the most traction in your testing endeavors. Juxtaposing the benefits of unit testing vs end-to-end testing. Spend 60% of your efforts, and thoughts on end-to-end tests, and the remaining 40% on the rest. You can find it on YouTube here: Testing for Success in the Real World - Donn Felker . Therefore the TLDW paraphrase position being: “The testing pyramid encourages a 70/20/10% test number split, and I don’t like that focus on the number of tests. Ask yourself where are you going to get the most bang for your buck, and that’s in the end, end-to-end tests. Adopt a different end-to-end mindset, end-to-end tests do mimic real world usage, that’s real users. Your users don’t care that your test suite passes, your users care that your application works.” End-to-end tests really are the closest thing we have to having a user sitting next to us using the app. End-to-end tests capture the user behaviour, they show all the pieces connected together and working. If you think of a timing analogy; end-to-end tests are the runtime-problem highlighter, whilst unit tests are more compile-time, or another analogy, unit tests are the canary in the coal mine for development and end-to-end tests are the fireguard whilst burning the coal at home. The testing pyramid reflects a notion of quantity, but we want to be thinking of the quality of our tests. The pyramid encourages good development practices for the individual developer, but doesn’t highlight anything of the complexity of time/effort needed to go into each layer (unit, system, e2e). How can we refactor this testing pyramid to reflect what it is needed from our apps test suite? Developers are always in a rush to deliver, whether this is a self imposed idea of ‘being in the zone’ and churning out code, or whether this is management/company imposed, with pressure of deadlines etc. The amount of effort to get a unit test up and running vs an end-to-end test on Android is minimal. Espresso and other acceptance testing technologies require due diligence, patience and a cognitive shift away from the individual lines of code being written. This isn’t an excuse, but another reason why developers will prefer to unit test, this leads to the “oh sh*t” scenario of testing the pieces but never the whole. Projects start with a single line of code, there is a setup of the project and the overhead of the gradle build system, getting it deploying on a real device etc. but the project isn’t “your own” until you write that first line of bespoke code. You could, end-to-end test this line of code. Have Espresso start your app and verify it is open (let’s say with a blank white UI). You could, unit test this line of code and validate it’s what you intended. Each of these two scenarios have a wildly different build time (as mentioned in the second last paragraph), but the point here is, at the start of the project the UI is constantly changing. The UI will be getting feedback from other departments in the company (your code architecture won’t), and so the UI is changing. This is a discouragement to start your project with end-to-end tests. They’ll be red - a lot. That takes effort. Whilst it’s the right thing to do, and very important to have that end-to-end scenario setup and tested as soon as possible. Here I am highlighting the (conscious or not) amount of effort involved and why developers (conscious or not) do not jump straight to end-to-end tests. Or written differently, when you first start a new project, you have no UI, once you do, the UI is still very malleable and not set. This is a reason why end-to-end tests may be avoided, having them at the start increases overhead & flakiness, but if they aren’t their from the start, then the pattern/process is not in place so they won’t arrive later. Without active effort. Another way to look at the pyramid is through the lense of the Pareto principle, this states: The Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes. Mathematically, the 80/20 rule is roughly followed by a power law distribution (also known as a Pareto distribution) for a particular set of parameters, and many natural phenomena have been shown empirically to exhibit such a distribution. One way to interpret this rule is to look at your end-to-end tests as that vital 20%. 20% of your testing efforts will catch and highlight 80% of your bugs. Potentially your current application testing pyramids look like the following (aka missing the end-to-end tests): You will be missing 80% of your bugs! The Pareto rule is a strong indicator to the importance of end-to-end tests. The testing pyramid as a concept is usually applied to your whole application. It’s interpreted as how your application should be tested and this can give the effect of “someone else's problem”. If you are only working on a small part of an application, say the login feature. You may (consciously or not) consider the idea of end-to-end testing as outside of the scope of your work - an application wide issue, not worth the time when just writing the login. A way to counteract this thought process is to break the pyramid down, talk about the testing pyramid at the module level, feature level, sprint level or whatever smaller unit of development/time that makes sense for you and your team. Finally, as has been advocated throughout this post, consider the payoff of the tests you are writing. When wanting to highlight the user impact, perhaps a one-dimensional pyramid is not the best way to discuss testing, but a multi-dimensional representation is needed that also considers users. How about one of the following: Consider the weighting of who gains the most from these test: Consider who would be impacted earliest when these tests break: In conclusion, the testing pyramid doesn’t have an axis for importance, or the value driven from the different testing layers. End-to-end tests are an important building block of your application, a pyramid without the top portion is not a pyramid at all! Remember the Pareto principle and that 80% of your value will be derived from 20% of your tests. Always, measure, learn and adapt to ensure the testing pyramid works for you and your project. Appendix I Pareto Principle in action: 80% of the quality of your software comes from 20% of your tests 80% of your effort will create 20% of your tests 80% of your bugs are caught by 20% of your tests 80% of your tests are redundant The 20% is concentrated in the top of the pyramid.", "date": "2019-10-29"},
{"website": "Novoda", "title": "3 Steps to make your app more magical", "author": ["David Gattig"], "link": "https://blog.novoda.com/3-steps-to-make-your-app-magical/", "abstract": "Any sufficiently advanced technology is indistinguishable from magic. - Arthur C. Clarke What is a magical experience? If you manage to optimize the mundane and delight users in the same process - you created a magical experience. Uber creates two magical experiences for users. The first one happens when users tap on their phones and a car with a driver appears in front of them. The second magical moment happens when users leave the car at the place they wanted to go while the payment happens automatically. Frictionless. Easy. Magical. Where is your magical moment? Where in your app are your users experiencing a magical moment? First ask yourself - what would your users be most disappointed by if you removed it today? Imagine Uber removing their two core features -  ease if finding a ride and frictionless payment - it would not be the same service and value proposition anymore. They would lose users fast. On the flip side, would users care that much if Uber were to remove their FAQs? Probably not. Beware the Bloatware You quickly realize that your app is made of one or two core experiences or features that represent the majority of value for your users. What about your other features you may wonder? They can create unnecessary noise and distract your users from the actual core of your app. Bloatware comes to be known as bloatware when it becomes so unwieldy that its functionality is drowned out by its useless features. Software bloat can slow down your app and confuse new users by the number of possible features and options that are presented to them. How is Bloatware created? This problem arises for example when development teams are told to blindly follow roadmaps that have been handed to them by management. Usually, the viability of these features is not questioned, tested with users, or analyzed after it was released. Even if the analytics numbers show that users do not interact with the features it is decided to keep it in. This happens because a lot of time and money have gone into the feature ( Sunk-cost fallacy ), someone in management has a political stake in the decision or, the whole development team is so busy developing the next feature that there is no time for looking back and questioning what they did last week. Fred Brooks in his book The Mythical Man-Month coined the term second-system effect (also known as second-system syndrome) that explains that bloated systems are created due to inflated expectation and overconfidence. Small, elegant, and successful systems are succeeded by over-engineered, bloated systems. How can you make your app more magical and less bloated? 1. Find your magical moment When you show your app to users, watch them closely while they go through your experience. When their eyes light up after an interaction with your app, you found your magical experience. You can ask your users how disappointed they would be if you would remove Feature A or B today. It is a better benchmark in evaluating how much your users actually care about your features than asking what features they like. Be data driven. Analyze your data and see how your users are moving through your software. This can give you a good indication of what users actually value. 2. Remove the noise You now have now quantitative and qualitative data that backs you up. Search for opportunities to trim your app of features that do not help your users achieve their goals. Slim down your app and make your value proposition clearer. 3. Double down on your magical experience Lastly, focus on the magical experience in your app and see if you can make it even more frictionless and magical for your users. This is the most important thing you can focus on. Find any opportunity to cut down the steps for users, the text they need to read, or the time they have to wait until they achieved what they wanted to with your app. The more magical and valuable your experience is for users, the more successful your app will be.", "date": "2019-10-31"},
{"website": "Novoda", "title": "Enhancing collaboration and transparency within a product team", "author": ["Anna Youngs"], "link": "https://blog.novoda.com/collaboration-and-transparency/", "abstract": "The most effective teams strive for frequent collaboration, creating opportunities for knowledge sharing and creative problem solving. When working on a project with tight deadlines, there may be hesitance to put processes into place to allow for collaboration due to the fear of distraction or overwhelming the team. Contrary to this, once a team has formed a habit of active collaboration it will happen organically, which can help to alleviate pressure and stress. A design thinking approach Design thinking refers to the cognitive, strategic and practical processes by which design concepts are developed by designers and/or design teams. Using a design thinking approach to create a collaborative environment can be starting point to maintain an aligned team. Firstly, by empathising with the team, and asking questions about their everyday problems can help you define team needs. Spending time working together to ideate on solving these problems is an opportunity for organic collaboration. Prototype the team’s ideal by documenting needs and trialling actions. From there you can test out the practices by timeboxing and then discussing the impact in a retrospective. Cross-discipline collaboration has a positive impact on team efficiency, happiness and business benefits. To build a culture of collaboration it’s important to first uncover the fundamental team needs. Here are some suggestions of team needs and actionable steps we’ve taken on projects which can help you to enhance your own teams collaboration and transparency. Effortless remote working Physical space between teammates can, on occasion, feel isolating at times. Easy-to-make mistakes, such as forgetting to include someone in a meeting can not only knock a team member’s confidence, but also takes away their opportunity to be involved in knowledge sharing. This can create silos and hinder collaboration. Be inclusive and transparent by default When reviewing the schedule for the day, have a think about the topics and meetings you are involved in. Does it look like the right people are involved? Do you think someone could provide value on this topic? By using team communication channels, highlighting that an initiative is ongoing during the day can be a small step to make someone feel more included. Get some face-time As brilliant as remote messaging tools are, there’s nothing like getting to know someone face to face. Schedule a video call with your team, with a loose agenda, to have a coffee and just chat. This doesn’t always have to be 1-on-1s, as everyone's time is precious, so make it a group effort too. Spend some time finding common ground and see what initiatives they are excited about. This works well during the start of a project, but there’s nothing stopping you kicking this off again if you start to feel the team is drifting. Try alternative ways of working Standing around a whiteboard drafting our plans as a team is great for in person collaboration, but what happens when you can’t do workshops in person? Using tools that prevent physical distance being a barrier can increase asynchronous collaboration. By using Miro, a collaborative whiteboarding tool, during ideation and planning allows everyone to be involved in sharing information and develop ideas during remote calls. By giving a team the power to take notes, add their perspective, and any existing documentation saves an enormous amount of time. Introduce a culture of ownership When there are many teams involved in building a product, there is a risk that features can be over-engineering or left to stagnate as a result of ambiguity of ownership. This can often be due to team misalignment or lack of focus. Create a holistic overview Spend some time as a team mapping out a holistic overview of all the team inviatives, away from project management tools and onto a whiteboard, real or digital - whichever works for your team. Create a responsibility map by listing out all of the ongoing initiatives, sort these into categories and use different colour post-it notes for each category. Create columns for each person and ask everyone to copy the post-it notes and paste them into their column. This allows everyone to visually see the colour balance between each category, and see what has been the collect focus. It also allows everyone to see which tasks have multiple people working on them, and can highlight initiatives that are missing an owner. This can trigger conversations about individual interests in and also act as guidance for the team to understand who to approach for specific subjects. Variety for repeptative events Reshuffling the way the team update each other can help to reinvigorate how the team communicates. Instead of catching up in a round-robin session in the morning, suggest moving it to the end of the day or try out updates via your remote messaging tools, instead of a call for a couple of weeks. Not only will the team be thankful for getting some time in their mornings back, but it gives the opportunity to be more experimental in ways of communicating by sharing screenshots, prototypes or polls. Understand Business Priorites Transparency allows teams to build trust which leads to an increase in employee engagement, and team security. Sharing the business strategy, goals and expectations also helps the team to understand what success looks like. It’s important to align expectations with stakeholders, understand the purpose of initiatives and to uncover what problems the team should be solving. Stakeholder interviews will help uncover business priorities, and make sure the team is aligned. Create a trustworthy and supportive work environment When a team grows and expands, there may be mixed feelings. Change isn’t always easy and this may cause team members to be apprehensive about collaborative work due to fear of losing ownership of current projects they enjoy. To make sure the team feel supported and has their voice heard it can be helpful to spend time with an agile coach. A coach can help give the team the right knowledge, tools and training to allow team members to reach their potential. Identify Team Prinicples A principle is defined as “a fundamental truth or proposition that serves as the foundation for a system of belief or behaviour or for a chain of reasoning.” Defining your team’s principles can be a very powerful way to inform and assess decision making and can also give teams the opportunity to consider and review their existing ways of working. To open a discussion on what has previously succeeded or failed, can help build trust and support. Introduce custom Ceramonies If you are working agile, you’re likely to be involved in scrum ceremonies. These are incredibly helpful to product teams as a whole, but sometimes don’t allow the opportunity to discuss more granular topics. Finding ways to sync-up independently from these ceremonies will give the team the opportunity to support each other more directly. Open sessions can allow people to drop by, without putting pressure on those who may be already inundated with meetings. Trial it out for a sprint, and then reflect on it during a retrospective to see if it benefited the team and helped with a shared vision. When suggesting improvements it’s helpful to focus on the positive impacts it will bring to both the team and individuals. As with all iterative processes, there is not a one size fits all solution. Therefore it is recommended to re-evaluate often to make sure that the adapted ways of working are as helpful and efficient as intended.", "date": "2019-12-19"},
{"website": "Novoda", "title": "How to make innovation labs succeed", "author": ["David Gattig"], "link": "https://blog.novoda.com/how-to-make-innovation-labs-succeed/", "abstract": "Photo by Daria Nepriakhina / Unsplash Innovation is the driving force to stay competitive in the free marketplace. A lot of companies struggle to stay innovative once they have created their first cash cow. Let us just look at Blockbuster. The film and video game rental company went from an unchallenged empire in 2000 to bankruptcy in 2010. Eventually, Blockbuster was killed off by on-demand streaming sites like Netflix, a new entrant that they saw coming from miles away. Systems and processes that benefit the company to scale the product and garner the monetary success are the same that will make it too slow and rigid to be innovative. To overcome the inertia that goes alone economies of scale, companies are opening innovation labs. These are semi-autonomous safe-spaces, formed by an organization, with a culture that is more conducive to innovation and informed risk-taking. The German car manufacturer Volkswagen alone has seven innovation labs worldwide. Volkswagen Innovation Lab BerlinTo break out of the shackles of corporate processes, more and more companies are looking into opening innovation labs. The idea is to have an independent structure of intrapreneurs that can, free from corporate life, move faster, break things, and spark innovation. At the same time, many of them do not succeed. Capgemini quotes a Senior executive at a leading global bank that up to 90% of innovation labs are failing and suggests it is \"extremely challenging\" to make innovation labs work. They are doing what Steve Blanks calls Innovation Theater: \"A competitive environment should drive a company/government agency into new forms of organization that can rapidly respond to these new threats. Instead, most organizations look to create even more process.\" - Steve Blanks After working with big clients, there seems to be a common theme about how big corporations go about innovation and most of them are making similar mistakes. Here are 5 rules to make sure that you do not fall into the innovation theater trap with your innovation lab: 1. Make the innovation lab truly independent Independence of the mothership is the core principle of an innovation lab. At the same time, many companies are not ready to go through with it and still want a measure of control over the new structure. However, these teams need to be able to operate separately from the main company. It is not enough to put them on one floor of your corporate headquarters. Move them to a separate location. The physical distance is needed to break out of the habits, processes, and controls that hold back innovation. 3M announces new 3M Design Center Then make the lab a separate legal entity with a new name, which can function independently and does not have to go through your established procurement or hiring processes. You will also see that you can attract more entrepreneurial talent if you set up another company with different branding for the innovation lab. Making it independent also frees up the innovation lab from the pressure to achieve the same level of financial success that is expected of other departments in the company. This means they can focus on smaller markets which are not very lucrative to the main business yet but could develop into one in the future. Similar to how Kodak undervalued the small digital film market because of its dominating market share in the analog film industry. Next, define one person in your company to be the sponsor of the innovation lab. Do not set up a committee. Ideally, choose someone as high in the corporate hierarchy as possible - maybe even the CEO - to be the main contact person of the innovation lab. How can you make sure that the innovation lab will work on the right things and provide insights and develop products that are relevant for your company? Let us take a look at rule number 2. 2. Give the team clear goals and a problem to solve It is crucial that you align the efforts of the innovation lab with the overall company vision. Formulate a clear problem statement that the innovation lab can focus on. You can have several teams working on the same or different problem statements. But be careful that you have identified the right product problem to solve. Once you have a goal and problem clearly defined and they are clear, specific, and broadly understood you are one step closer to what Netflix proposes as one of their core aspects of culture - being highly aligned but loosely coupled. But how can you measure that your innovation lab is coming up with the right solutions? KPMG Innovation Lab / Sydney, Australia3. 3. Measure \"number of learnings\" Big corporations are stuck in political infighting where success is brutally exploited for political gain and failure will be used as ammunition to throw someone else under the bus. In an environment where making mistakes can have serious consequences for your career the more risk-averse process-oriented people climb to the top of such organizations. However, this right and wrong mentality goes against all the principles of innovation, where you need to be able to fail fast, make mistakes in order to learn. Albert Einstein said it nicely: \"A person who never made a mistake never tried anything new.\" To move away from right and wrong thinking in your innovation lab, set up a metric that focuses on the speed and quality of new learnings generated by the innovation lab. You want your innovation lab to try a lot of new things. Only then you can truly know if something works or not. You want to make sure your intrapreneurs are more out validating hypotheses instead of having lengthy discussions in meeting rooms. You want to make sure you are measuring the right KPIs to gauge your innovation lab's effectiveness. I argue the best KPI for an innovation lab is \"number of learnings per week\". In order for the innovation lab to function this way, these metrics need to be championed by the people leading the innovation lab. Then focus on beacon projects to apply those learnings and solve high-priority problems for the company. 4. Invest in the right culture It's often said that company culture starts at the top - that in order to effect meaningful change in your organization, you need buy-in from the leadership. Make sure the person you put in charge of the innovation lab embodies the values that drive innovation and collaboration. People who are great at scaling great ideas have a different mindset from people who are excelling at creating new products. Hence, you should look at people outside of the current structures who are not entangled in the current processes or political infighting. Give the team the resources to hire great talent and design their creative space that they feel comfortable in. Verizon's 5G Lab5. 5. Trust in the team The last but maybe most important point is that you trust the team you hired to achieve the goal that you have set. The team you tasked with tackling a new and challenging problem will feel the trust. That will give them more freedom to try out new things. They are not afraid to fail. If you give them your trust to own and solve challenges for the business you will see a boost of ownership and commitment from the team. This is important as this team will face many unknowns and will hit some dead ends along the way. Conclusion Ideo, the global design agency says: Labs are not for the faint of heart. If organizational change were easy, no company would falter. But with a clear purpose and portfolio of beacon projects, a lab can focus on new explorations while the parent company delivers its core offers. It's a partnership. To sum up, if you start an innovation lab, make it truly independent from the woes of big corporate life. Challenge them with clear problems and goals so they stay aligned to the company vision. Measure the speed and quality of the learnings they generate and have them applied to beacon projects. Put the right people in place to foster a culture of collaboration and innovation. And finally, trust the team.", "date": "2020-01-22"},
{"website": "Novoda", "title": "Our top tips for remote working tools and practices", "author": ["Daniele Bonaldo (Android, Wearables and IoT GDE)"], "link": "https://blog.novoda.com/our-top-tips-for-remote-working-tools-and-practices/", "abstract": "At Novoda we fully embrace remote working: we have different offices, teams spread across several cities, countries and even continents. To efficiently communicate and work together we use a set of tools we selected and we perfected their usage over the years. Here you can find the most important and valuable ones for us and some best practices we adopt to get the best out of each tool. Instant Messaging (Slack) Communication is key in a team, and even more if team members are distributed in several locations. The tool we use the most, internally and with our clients, is Slack. We use it on a daily basis and it allows many reporting integrations to help improve the process for projects. Some of these integrations: AppFollow : Reports on any review of an app we have in the Google PlayStore. Provides a trend over time. Jira : Reports on updates to tickets, this can be configured based on what sort of notifications are required. Pull Reminders : Notifies developers of assigned PRs, comment updates and outstanding reviews. Continuous Integration Plugin: Reports CI job success and failures. Most commonly used at Novoda are Jenkins and Circle CI plugins. Calendar syncing: Can change personal status to surface when in meetings / change do not disturb settings, reminders of calendar events. Most commonly used at Novoda are Google Calendar and Clockwise . Best Practices We noticed that remote workers need to be more active in Slack to make up for their lack of physical presence. We need to actively seek out opportunities to engage with our team members and clients. Remember to notify when online / offline / attending meetings and for other status updates. Some Slack groups will have a specific channel for this. Make use of Slack integrations that can help automating your status update. Video Conferencing (Google Meet) We use Google Meet as a conferencing tool for many of the team and company meetings involving more than two people. It’s especially useful for large meetings where some attendees are at an office that has a chromebox connected to a large tv and a webcam. Another use for Meet is to have a constant connection between offices and remote workers. We call this “the Wormhole” and anyone can connect to it at any time to speak to somebody in a different office. Best Practices Remote workers need to have more of a presence in meetings, try to actively engage in conversations. It’s very important to always have an active webcam. This makes it easier for other attendees to interface with remote workers, compared to a static avatar. When organising a meeting with several people in one room and other remote attendees, it’s very important to have a dedicated, high-quality external microphone, so that voices are better captured than using a laptop microphone. Possibly the most important tip for meetings in general, remember to have only one person to speak at a time. A more company-wise practice we adopt is to have a daily standup in the morning. This is a great opportunity for everyone in the company, remote or office-based, to connect, converse, see each other, share information about different projects, even for only 10 minutes. Screenshare (Slack) At Novoda we regularly do pair programming and this technique is not at all limited to people in the same office! Tools like Slack allow one person to share their screen and have other connected people to draw to guide the person typing. Best Practices Pair programming is very powerful especially when driver and navigator swap often, to have a clear point of view. Plan your work with small goals (achievable in half a day or less) and when that is achieved swap the roles. Remember to use all the available tools at your disposal, especially the drawing one to attract attention to part of the shared screen. Agile Project Management (Jira) Most of our projects follow Agile methodologies and proper tools are needed to follow the project progress. THe most used one is Jira . Each team will typically have a different workflow but in general the story tickets are created according to the roadmap. All members of the team groom the tickets and help with prioritization before starting the actual development. Best Practices When a ticket is selected for grooming try to look at it before the grooming session to understand it properly and come up with questions and possible complications that will need to be addressed. By the time the ticket is selected for a sprint, there should be little to no ambiguity remaining. Whilst working on a ticket, try to document the progress at least once a day, with the aim of allowing anyone to pick up where you left off. VCS (GitHub) The Version Control System (VCS) we use for most of our projects is through Github . This contains the repositories we commit our code to and it might be hosted by Novoda or by each client. Associated with the VCS, we use Pull Requests (PR)  in our process to make sure each change in the code is reviewed before getting merged. At Novoda we strongly believe in the importance of open-source, and we maintain several open projects. These range from sample applications to libraries used by several other projects. The processes and practices used for these projects are not different than the ones applied to closed-source projects for clients. Best Practices We have a lot of best practices associated with GitHub and some might depend on the project. For large projects (with at least 5 people) each PR will need at least 2 reviewers to approve it before someone, other than the person who opened the PR, is able to merge it. PR description should have enough detailed information to provide context for reviewers to be able to give constructive feedback. This is especially important if we consider the longevity of a project where a detailed history can help solve problems or lack of documentation. Pull request templates can be very helpful making sure a PR description contains all the information needed by a reviewer to properly understand the changes. Google Drive suite The Google Drive suite of tools is a great alternative to file-based programs (like Microsoft Office), especially when collaborating remotely on a document. It allows multiple people to edit a document at the same time and easily address comments and suggestions. Best Practices When reviewing a document, prefer using the Suggesting mode, so that all the changes done to the document will be clearly visible to the author and they can be addressed individually as separate suggestions. Calendar (Google Calendar) Google Calendar is just an example, but it’s very important to have shared calendars for events. Best Practices Create calendar events for every meeting, even 1-on-1 or casual catch-up chats, so that all the interested people are notified about the event and the attendees are automatically set to busy during that period. Always write an agenda as the event description, so that all the invited people know in advance what to expect and are able to arrive prepared for the event. Consider lunch break and working hours in different time zones. Try creating most of the events in the core hours, common to all the people interested in the event. Ensure that all calendar invites have a link to a Meet session  (or alternative videoconferencing) so that remote people can attend. Bonus point: if the meeting is recorded using Google Meet, the video is then automatically accessible in the future by attendees straight through Google Calendar. People dashboard Being remote can make you feel lonely sometimes, especially when working alone from home. There several services trying to solve this and the common approach is to use a dashboard showing all your available colleagues. One of these services is Pukkateam , but at Novoda we created our own open-source alternative: Peepz . This is a dashboard accessible both via web or an Android app, showing all the online colleagues and it can periodically take a picture of you and display it on the dashboard. Best Practices Promote daily themes for your pictures, make plenty of funny faces and encourage others to use the dashboard. It really helps to make remote workers feel as part of the team. This is just a small set of the tools we use daily and are really improving the way we collaborate remotely. If you are a remote worker or planning to make your team more remote, these tools are a great start, but if you know any other tools you use and help making your remote life easier, let us know! Do you want to know how an integrated remote team could benefit your digital product? Get in touch and begin conversations with our experts to make sense of your mobile product and discover how our remote culture and practises could provide value to your business.", "date": "2020-01-28"},
{"website": "Novoda", "title": "What can product leaders learn from Napoleon’s Grande Armée?", "author": ["David Gattig"], "link": "https://blog.novoda.com/what-product-leaders-can-learn-from-napoleon/", "abstract": "(Image by WikiImages from Pixabay ) It was a Thursday evening. I was sitting on my couch, browsing through YouTube on my phone when I stumbled on a mini-documentary series about Napoleonic Wars. I love history, so I dove right in. One thing that starkly stood out was how Napoleon Bonaparte managed to lead and change the French army into the seemingly unstoppable Grande Armée, which dominated the European Continent between 1805 and 1815. I could not stop seeing the parallels in Napoleon's reforms to how successful product teams are organized today. The reformed army under Napoleon was a stark contrast to how the armies from the other European leading powers in the 18th and 19th centuries had been managed and were ultimately key for Napoleon’s extraordinary military successes. His army moved more quickly, was more flexible, and could react faster to changing circumstances on the battlefield. These are qualities that not only coveted by generals but also business leaders today. Businesses need to stay ahead of their competition by moving faster and react better to changes in the marketplace. Let us take a look at how Napoleon managed this feat and what product leaders around the world can learn from him. How were armies run in the past? The leadership of armies in Napoleon’s time (18th-19th century) consisted of aristocrats that had some form of military education. They would lead vast armies into the field, listen to reports from the front lines and then send an order down the command chain. If new information would come up the chain, the generals would discuss among themselves and then send a new directive, which should be executed by the soldiers on the front lines. The decision power was tightly concentrated in the top ranks behind the main body of the army. There was a great distance between the decision makers and the people who were actually facing the problem. This led to sluggish reactions on the battlefield that could proof fatal and win or lose a battle. Napoleon, however, changed how armies would fight until today. How did Napoleon transform the French army? (Source: Epic History TV Twitter ) First, Napoleon renamed the French army into the Grande Armée, giving it a new sense of identity. Then he reorganized it according to the Corps (pronounced /kɔːr/) System which was later imitated by nearly every army in the world. Each Corps was commanded by a Marshall and operated like a mini-army of 15.000 to 30.000 soldiers. They each had their own infantry, cavalry, artillery, and supporting arms and were completely self-sufficient. This meant that each Corps could, for a limited time, march and fight independently. The Corps had the authority to take their own decisions on the battlefield as long as they served the overall mission. They could react much faster to changes than the other armies in their time, because they did not have to wait for approval from above for each decision. This was a big break from the old dogma of having a big concentrated army. Furthermore, this massively increased their movement speed as the smaller units could advance through several routes, living off the land and not having to rely on slow supply routes. This speed allowed Napoleon to surprise and disorientate his opponents and win decidedly. What does this mean for product leadership? Looking at how Napoleon reformed the French army, there are three key concepts that stand out and product leaders today can utilize to build motivated and adaptive teams. 1. Structure your organization into smaller, cross-functional groups Organize your teams into small cross-functional units and give them a problem and goal that they can own. These small and autonomous teams will be more motivated and move faster as they feel more ownership of the problem they are tackling and have fewer dependencies. They can quickly experiment and test their solutions without waiting for anyone. It is important to agree together on success metrics so the team can hold itself accountable. Hire leaders (not managers) that you can trust to solve your problems and put them in charge of these teams. This trust will be felt and your teams will feel that they can try out novel solutions that could propel your business forward in ways you could not have imagined. 2. Create a clear identity for your organization or team. Decide together on a name that unites different people into one team.  A team name helps members take ownership of their tasks. They want their team to be successful and as such, it becomes far easier for them to make themselves accountable for their own tasks and those of the rest of the team as a whole. It leads to a “we” instead of a “me” culture. A common culture and shared values bolster team cohesion and build trust. 3. Communicate what you need to be accomplished As a business leader (or indeed, an army general), you cannot consume, interpret and action the plethora of information that your teams are generating on a daily or even hourly basis. If you are trying to do it, your organization or team will be too slow to react to new information as they cannot act independently from you.  Make your intention clear and communicate what you need accomplished and do not tell people what to do. You can make use of the tremendous problem-solving capacity of all your team members instead of relying solely on your own. You will be surprised by what they come up with. They are closer to the user and the problem and can come up faster with more relevant solutions. Conclusion The basis of today’s widely heralded best practices in business like using self-organizing and cross-functional teams and adopting the agile mindset to become more adaptive to new information has already been applied by Napoleon more than 200 years ago. By employing Napoleon's strategies for reforming the French Army, product leaders can create teams that are motivated, aligned and solve customer problems faster than the competition in today’s quickly changing world.", "date": "2019-10-24"},
{"website": "Novoda", "title": "Why you need to set up a proper super-system to be successful", "author": ["David Gattig"], "link": "https://blog.novoda.com/super-system/", "abstract": "Photo by William Iven / Unsplash Your system is more important than your outcome. What makes some people more successful at achieving their goals than others? How come some organizations are barely scraping by, releasing mediocre products, while others continue achieving great results? I was mulling over these questions for the last year. Then it came to me that there is one trait successful people and organization have in common: they focus on process rather than outcome. When you are creating a new solution, you follow a process that spans from the identification of a problem to the release of a solution in the end. When circumstances change, like a novel trend among your company’s user base, your current product might not be a great product-market fit anymore. However, you can use your product creation process to spin up a new product and release it. A product itself cannot change once it was released and is like a snapshot in time. Your process to create these products, however, is what actually makes you adaptive to changing environments. Your production system will be able to create a new product that achieves a better product-market fit, like an development process can release an update to existing software. In the end, the system or process that creates is more important than the creation itself. Bruce Mau says, “When the outcome drives the process we will only ever go to where we've already been. If process drives outcome we may not know where we’re going, but we will know we want to be there.” This means, as long as we set up the proper processes, even in changing and uncertain circumstances, we can be sure we create the right outcomes at the end. Thus, the system or process is superior to the outcome they create. In our professional lives we strive to improve our process, be it the code quality, the design system, or the product discovery methods. In highly functional digital teams we run regular retrospectives to look back and improve our teamwork, we analyze and act on data, we identify and eliminate roadblocks. How come we are not applying the same rigor to ourselves? If we put so much effort into setting up the right systems at work, shouldn't we take even more care when we deal with the system that sets up the system? What is the super-system? What is the system that ultimately sets up the system? It is you. You are the creator of the process that, in the end, creates outcomes. Put another way - your outcomes are daily practices which create other outcomes. For example, you may regularly read the latest articles and go to meetups about your profession. Then at work you apply your insights to create a better workflow that will make the next iteration of the product better. Does this make you a product or a system? Are you a snapshot in time, never changing, or do you constantly adapt to changes in the environment? If your answer is yes to the latter question, then we can safely presume that you are a system. You are constantly producing approaches that are solving problems, every single day. In that case you are a super-system that creates other systems in both your private and professional life, which in turn solve problems for your family or business. You are the embodiment of a super-system that puts you and your created systems forward. The importance of the super-system If we can agree that the process or system is superior to the outcome, then the super-system must be more important than the system that it creates to produce outcomes. Simply put - you are the most important part in this interplay and should treat yourself that way. A better super-system can create better approaches which create better outcomes. At the same time, people and also organizations seem to put their produced systems before themselves, like trying to write the perfect line of code in an otherwise flawed product, or creating a great product in a dumpster fire of a company. How can you expect to perform expertly when you are not taking care of yourself first? People get up in the morning, gulp down something to eat, trudge to their workspace, where they get stressed by the sheer amount of work. They put all their energy trying not to drown in work, making a good impression, hoping for a promotion. Then they return home, too exhausted to do anything else. They heat up ready-made meals in the microwave, watch some Netflix and fall asleep. And repeat. If you are caught in this daily grind, the results are constant exhaustion, deteriorating health, and a loss of focus in the workplace. You are acting like the shoemaker, trying your best to make a great shoe while you are ignorant of the sorry state of your own personal environment that won’t let you create something other than ordinary. Similar to the woodcutter who, when confronted by his boss about when he last sharpened his axe, cries out, “Sharpen? I had no time to sharpen my axe. I have been too busy cutting down trees” If you feel bad and your performance is deteriorating, not only will your worsening condition affect your own well being, but also your team and your product as well. For example, if you do not take care of your physical and mental health, you are likely becoming sick more often, missing work, thus putting additional strain on your team members, who then also will be impacted. This might lead to delayed product releases, which can hurt the company’s bottom line and might even lead to lay-offs. Build the best super-system If we want to make sure we are building the best systems and the best products, we need to start at the source: ourselves, the super-system. We need to appreciate that we are a creative machine that works best when well tended to and will break down due to overuse and lack of maintenance. Identify the sub-routines in your own life. Ask yourself which ones are adding value to you and help you feel fulfilled? Remove behaviors that are holding you back. Then learn new habits that will make your process more resilient. Create a life and routine around you that lets you be the most effective system. Do sports, eat healthily, expose yourself to new ideas. New studies show that doing sports and moving around helps your cognitive performance and creativity . “Engaging in a program of regular exercise of moderate intensity over six months or a year is associated with an increase in the volume of selected brain regions,\" says Dr. McGinnis. Also, eat food that does not make you tired but keeps your spirit up and your brain powered. For example, Eva Selhub MD writes: “Like an expensive car, your brain functions best when it gets only premium fuel. Eating high-quality foods that contain lots of vitamins, minerals, and antioxidants nourishes the brain and protects it from oxidative stress — the “waste” (free radicals) produced when the body uses oxygen, which can damage cells.” Like the food you eat, the media you consume is like a diet for your brain. The news on TV will make you perpetually scared off the outside world. The never ending waves of meaningless notifications will steal your attention. Advertising will make you feel inadequate about yourself. Different types of media can have differing effects on you. Make sure you are treating your media diet like your nutritional diet and you can become more knowledgeable, interesting, inspired, and motivated. Grow the super system and the other systems will benefit. Once you have your own system running smoothly, there is little that will throw you off-course. You will feel better about yourself and new opportunities will open up to you, if you keep improving yourself. Furthermore, the beauty of network effects is that your improved super-system even will have a positive impact on the other super-systems in your social network. Bear in mind that your life is a marathon and not a sprint. Think about what behaviors and circumstances will let you keep going the longest. If you do it right, you will be able to cover a lot more ground than the people who just blindly sprinted to the five mile marker without realizing there are 21 more miles to go. “Put yourself first. Putting yourself first doesn't mean you don't care about others. It means you're smart enough to know you can't help others if you don't help yourself first.” — Unknown Author To sum up, make sure you prioritize the super-system first. What is true for the individual can also be applied to your business and organization. Employees in the highly competitive tech industry are showered with additional perks ranging from free lunches to company retreats in the mountains. Companies in the tech industry like Google understood that the most important asset are their people. In The Hard Thing About Hard Things , Ben Horowitz urges us to “Take care of the people, the products, and the profits— in that order. When we in Novoda are brought in to help a new organisation in their digital development, we always start with analyzing the processes and systems in place and work hard to get these in order and running smoothly instead of focusing on short-term deliverables for them. All in all, take care of yourself, put your house in order and then you will be of more value to your team and employer, and have a more fulfilling life. You are not selfish to put yourself first. You are the super system with the power to transform yourself and people around you. Treat yourself like it. It is in the best interest of everyone.", "date": "2020-03-04"},
{"website": "Novoda", "title": "Fifty shades of #808080", "author": ["Ryan Feline"], "link": "https://blog.novoda.com/fifty-shades-of-808080/", "abstract": "<color name=\"grey\">#666666</color>\n<color name=\"lightGrey\">#999999</color>\n<color name=\"tooManyShadesOfGrey\">#9F9F9F</color>\n<color name=\"seriouslyTooManyGreys\">#AFAFAF</color>\n<color name=\"lighterGrey\">#CCCCCC</color>\n<color name=\"lightestGrey\">#EBEBEB</color>\n<color name=\"darkestGrey\">#1A1A1A</color>\n<color name=\"notTheDarkestButCloseGrey\">#262626</color>\n<color name=\"darkerGrey\">#333333</color>\n<color name=\"darkGrey\">#4A4D4E</color> Sequence of greys with increasingly desperate names Resource Smells This small snippet was taken from a project I have seen recently, and no, it is not contrived in any way. The colour resource file in question contained between 20-30 colours in the white to black range. When adding the primary, secondary and tertiary colours for the app it reached 40-50 colours. Most brands, with just a cursory look, have between 5 to 15 colours for their entire palette. If we were to be generous and take into account an additional 10-15 for errors and the typical range of blacks and whites, we still don’t reach our 40-50. More than likely this explosion of colours can be attributed to a lack of pairing between design and engineering. This lack likely meant that any original colour palette was not reflected in the application, this then had the knock on effect of not influencing the styles or components created in the application. Finally, resulting in the infinite colour variations you can see above. Being aware of this scale up of colours can help prevent this happening and encourages you to keep your design and implementation clean and usable, in such a way that you can iterate quickly and apply changes more easily. Not leveraging theme attributes enough is the main culprit with rapidly expanding colour resources. Theme attributes allow developers to update a single reference in a theme that can impact the whole application. Most of the colours in this file are referenced directly in the layout or style resource in which they are used, forcing a laborious refactoring process whenever a colour update is required. Constraining colours to a theme not only makes it easy to change the style of an application but it provides immediate feedback when a colour is added and is not conforming to a theme. My process for dealing with this scenario was to create a new color-palette colour resource file, deprecating the previous color resource file. The new color palette is based on the design library that is used when designing new screens for the application. Using the design system as a template enables our implementation to speak in the same language easing communication across disciplines. Following this process, with a designer and my iOS counterpart (cross-platform, yay!), we introduced custom attributes that covered the function of a colour, i.e. primaryColor, onPrimary etc. Each of these functions is then referenced in the layout or style in which the colour would normally be used. This enables a developer, or designer to update the look and feel of an application by simply tweaking the colour association of a theme attribute, in a single place. Name all the things <color name=\"cardview_title\">#FF333333</color>\n<color name=\"cardview_subtitle\">#FF666666</color>\n<color name=\"notification_text\">#FF333333</color>\n<color name=\"feedback_input_label\">#FF333333</color> Colours named after usage Don’t do it. This is probably going to cause you a whole world of pain at some point in the future. This strategy can get wildly out of control very quickly. Very quickly. Before you know it you will have well over 100 colours, some of which are just referencing the same base hex value. It’s probably obvious to say, but if you are using this strategy, you are probably not leveraging theme attributes so you are already putting yourself at a disadvantage for quick changes. If you really like the functional naming strategy, I did before I discovered theme attributes, then there are a few processes you can put in place to make this more successful. Have a base colour palette that lives close to your design system. This will allow cross team collaboration and ensure that the application is not drifting in terms of the colours that are being introduced. Have clearly defined layers for how you deal with colours. Try to avoid directly referencing colours from your base colour palette in the layout and style resource files. Instead, use the functionally named colours in these files and have them themselves reference your base colour palette. You are going to have a lot of colours with a functional naming scheme. Create separate colour resource files that can represent different screens / components. The aim of this, because you won’t be able to leverage theme attributes as effectively, is to allow anyone to pick up a screen’s colour resource files and make quick easy changes. But seriously, just try to use theme attributes Exception to the rule <style name=\"BaseButton\">...</style>\n<style name=\"BaseButton.Raised\">...</style>\n<style name=\"BaseButton.Flat\">...</style>\n<style name=\"BaseButton.Raised.Black\">...</style>\n<style name=\"BaseButton.Flat.Black\">...</style> Inheritance in styles 😭 Styles are great. Styles facilitate quick design changes for a given component if done correctly. When looking at styles I like to think of inheritance in styles as an exception to a rule 😉. You have your first style which establishes your rule, this is the style for the buttons across the whole application. I now extend to introduce my second style, if I am adding to the base style then I am extending this rule to apply to an additional component. If I modify something in the base style, I am introducing an exception to this rule. It's the exceptions that can be the most dangerous because they deviate from an expected, established pattern. They become especially dangerous when they begin to leak out of the styles and into the layout resources themselves. Making an innocuous change to a base style may not have the desired effect across the application because of these exceptions that have been introduced. The solution is rather simple. Work with your counterparts in other disciplines, add the components that the app uses to the design library. Establish the components that make up the application and introduce styles that carefully match this design library. If you do this carefully you will likely notice that you do not need to have inheritance several layers deep. And remember, use theme attributes! Who's the parent? <style name=\"BaseButton\">...</style>\n<style name=\"BaseButton.Raised.Black\">...</style>\n<style name=\"BaseButton.Raised.Grey\" parent=\"SomeOtherButton.Raised\">...</style> Dot vs parent inheritance in styles Please. Make. Up. Your. Mind. Personally I prefer dot notation because it is easy to see how out of hand your inheritance is getting. There's only really one good reason to use parent and that's when you need to extend from the platform, which can't be done through dot notation. But whatever you decide, please pick one and stick to it. Dot notation that then uses parent just shows that the inheritance hasn't been carefully considered. Conclusion A design system that is to the point and contains all of the elements necessary for a user journey can help to unify the practices of design and engineering through a common language created by both parties. This common language enables seamless communication between the different team members allowing them to iterate quickly. Switching the branding of an application, supporting light or dark mode become trivial when we begin to leverage theme attributes, keep style inheritance simple and carefully consider our applications use of a colour palette.", "date": "2020-03-12"},
{"website": "Novoda", "title": "How does remote working... work?", "author": ["Alex Curran"], "link": "https://blog.novoda.com/how-does-remote-working-work/", "abstract": "At Novoda, we are really proud of how well we integrate remote working into the normal day-to-day life of app development. It makes syncing with clients easier, and allows us to collaborate with any colleague in any of our offices. So how do we achieve such remote working nirvana? For planning... JIRA JIRA is a well-known project management tool that is also frequently used for sprint planning. Whilst some of us still pine for the day of physical boards, it is pretty clear that physical boards aren't useful when you're working in multiple locations. However, the visibility and knowledge of a physical board is important and this is what we try and bring across into JIRA. JIRA has the ease of access across all our offices and, given most people are familiar with it, it means it's really easy to integrate into all our teams. When using JIRA, we found that making it as flexible as possible for its users has great advantages. Just as a physical board can easily be adapted and changed, we allowed the team to make the changes they want to the JIRA workflow. There’s nothing more annoying that realising you’re working around your planning software. For pairing... Screenhero Screenhero is an excellent tool we've been using at Novoda over the last year or so. It allows you to set up a virtual pairing (or tripling) session with another person, or people. Think of it as screen-sharing on steroids. The best part of Screenhero is that it allows everyone in the session to control the keyboard and have their own mouse pointer — meaning you can seamlessly take over from your pairing partner, without having to ask to borrow their mouse first. Screenhero is available for Windows and Mac right now. If you know of a similar program that’s compatible with Linux then please do let us know . N.B. As Screenhero has been purchased by Slack, you can’t create new accounts at the moment. But you can get invited from anyone who currently has an account (such as all of us at Novoda). So if you’d like to give it a go, let any of us know. For personal communication... Slack Chat clients like Slack are really useful and make it far easier to have discussions cross-office. Our Slack is getting more and more exciting by the day, and here are some of the cool things our Slack is currently being used for: a bot which scrapes our Slack for external news posted after standups (more below) Github and CI integrations letting us know via Slack when someone has made a pull request, or when the build goes red We're making our Slack a more vibrant and interesting place each day, and given how easy bots are to make , we'll be making even more awesome things in the future that make our remote working easier. For team communication... Google Hangouts Google Hangouts for Business is probably our most widely-used remote empowering tool at Novoda. We have chromeboxes installed in all our meeting rooms and offices, which means setting up remote meetings and chatting across offices is a breeze. With the integration into Google Calendar it means that any meeting can be given a virtual room with which to chat between offices. If you can't make it to one of our meeting rooms, no problem — just join the hangout remotely. The wormhole We have a special Hangouts room called the wormhole, which most of the offices are connected to all day. This is great because it allows you to see all the different offices as if you had a window into them. Also if you need to talk to someone in another office but don’t want to send a Slack message you can just chat to them through the wormhole. Having this constant connection to all the other offices is important and makes you feel like the offices aren't really that far away. Until you look at the sunny Barcelona office from the rainy London one, that is! We found that using big screens helps as well, to make people look as lifesize as possible. The Novoda standup Probably the best thing we do remotely at Novoda is our daily Novoda stand-up. This is where, for ten minutes a day, the entire company gets together to chat about what is happening in both Novoda projects and in the world outside. Having this daily time allows us to see our colleagues in other offices (I miss you too, London and Liverpool offices 😢) and fosters a great sense of connection with other people. We have a standard format with our Novoda standup, and it doesn't revolve much around work. It's split into three key bits: Novoda news: where anyone can talk about what's going on at Novoda or if there are any meet-ups like Londroid , GDG Liverpool , or MaterialUp happening Project news: two people will volunteer (or be nominated) to talk about what they've been up to on their respective projects, anything interesting they've found, or any problems they've faced. This is great to share knowledge about different projects and any tools or processes they've developed. External news: anything interesting that's been going on in the world (or, frequently, outside of it). The more random, the better! All the external news is compiled using a slack bot and then sent out in an email at the end of the week with all the links, in case you missed any of it. At Novoda, remote working is at the heart of what we do and we’ve got really good at integrating it into all parts of our work lives — from design, to development, and even just chatting. Remote isn't totally perfect yet but it has allowed us to grow flexibly as a company, and allow people to work how they feel most comfortable. Do you have any great tools or processes for remote working? Or even some cool and remote-friendly Slackbots? Get in touch and let us know! Thanks to Daniele B. for the photos, and to Paul B. and Seb. P for proof-reading", "date": "2016-09-13"},
{"website": "Novoda", "title": "Delivering outcomes with distributed teams using Liberating Structures", "author": ["Kathleen Bright (Scrum Master)"], "link": "https://blog.novoda.com/delivering-outcomes-with-distributed-teams-using-liberating-structures/", "abstract": "By using Liberating Structures you can learn how to: Identify and mitigate risk across multiple teams and organisations. Effectively measure progress and demonstrate value for a distributed team. Run effective workshops, with a flexible design and toolkit. Tried and tested with: 20 people from 5 organisations, working in the same location, but usually based in 6 cities in 4 countries. We focused on identifying and mitigating risks to product delivery. 8 people, working from 3 cities in 2 countries, connecting remotely for a project retrospective. We focused on how to more effectively measure progress and demonstrate value. Using the same sequence of Liberating Structures (also known as a string) we got the different outcomes we needed. Risk workshop Measuring value retro Spiral Journal Spiral Journal What risks do you see in our work together? What concerns you? What do you hope can happen in the future? What practical first steps can you take now? How do we demonstrate our value? How could we measure our progress more effectively? If you were ten times bolder, what big ideas would you recommend for quantifying progress? What first steps would you take to get started? Anxiety Circus 25/10 Crowdsourcing What concern or risk do you see in our work together? A bold, yet practical, idea for measuring our progress more effectively 15% Solutions 15% Solutions What could you do (to address any/all of these risks) that’s completely within your control? Without extra time, money or permission, I could get us closer to measuring better by... What I designed & delivered Spiral Journal gave us the time and space to: surface our fears and concerns, and identify the most urgent and important risks generate ideas and consolidate our thinking around demonstrating impact and value 25/10 Crowdsourcing (including Anxiety Circus riff) made it possible to: identify the unspoken risks in the room, and prioritise them share and score ideas for measuring value effectively 15% Solutions enabled every participant to: take action to address risks & issues immediately improve how we measure our progress Spiral Journal for surfacing risks and generating ideas I knew I wanted something to help people come up with lots of thoughts before refining their thinking and selecting one thought. I find Spiral Journal so useful to use alone that it was easy to choose that for my teams to try. Tips & Tricks Let people know in advance if they’ll be expected to show or share their writing with anyone else, and in what ways, so they can respond to the appropriate level for them. This activity starts with participants drawing a spiral as slowly and as tightly as they can in the centre of a sheet of paper, to encourage mindfulness. When we did this remotely, I asked participants to tighten and relax their shoulder muscles instead, to include more physical movement. 25/10 Crowdsourcing and Anxiety Circus for sharing unspoken risks and ideas I love 25/10 Crowdsourcing because it’s high energy (therefore contrasts well with the quiet and reflective Spiral Journal). The unique way of anonymously sharing and scoring ideas eliminates a lot of the issues that can arise with power dynamics and hierarchies when it comes to ideation. Scoring here is about how committed you are to making the idea happen, rather than how ‘good’ it is. Therefore, it’s an extremely practical structure that’s biased towards action. When you get ideas with high overall scores, the people committed to those ideas can immediately form action groups for those initiatives. Using the ‘Anxiety Circus’ variant,  we quickly learned the previously unspoken fears in the room and got a sense for how keenly these concerns were felt. When it came to scoring, 1 meant “I understand this risk, but it's not one that concerns me” and 5 meant “Yes, I'm really concerned about that too”. Of the 8 cards with the highest scores, 3 unique risks came up. Which demonstrated that multiple people were aware of those risks and that multiple people were strongly concerned about those risks too. Doing this activity, made it possible to spring into action to address what was previously bubbling beneath the surface. Tips & Tricks Using the Benny Hill theme music worked well. You can use any music that encourages people to race around the room. Because it helps to get people moving, even whilst they’re uncertain about the novelty of this activity. If you stop the music every time you want people to swap cards (like musical chairs), it helps cue participants to what to do next and you don’t need to give as much verbal direction. Make sure to prompt people to explain their idea clearly when they’re writing it down, because they won't have a chance to explain or clarify it before it’s scored. Virtual 25/10 Crowdsourcing Trying this activity virtually was an interesting experiment that changed the fundamental structure. I created a form with a single comment box for individuals to anonymously submit one idea each. I created another form with options to give a score of 1-5 in response to a series of reference IDs. Once every participant had submitted an idea, I matched each idea to a reference ID, and shared the reference list and the second form, so that people could score the ideas So, instead of each person scoring 5 ideas, each person scored every idea. Which made it harder to think and talk about ideas that have enough backing for individuals to take action together, rather than ‘best’ ideas. On the plus side, scoring is much faster remotely (depending on the number of participants, and therefore the number of ideas), since there’s no moving around the room or swapping of cards. In addition, reading typed text is clearer than reading handwriting! I’ll continue experimenting with remote versions of 25/10 Crowdsourcing, and many other Liberating Structures. Thank you Tas for co-designing this modification with me and championing better remote working practices. 15% Solutions for everyone to take action I love 15% Solutions. Sometimes when I’m excited about an idea I generate a huge list of things to do; I might feel overwhelmed and get stuck. 15% Solutions is so simple, and so easy to use. It immediately gets to what an individual can quickly and easily do next to move forward and make progress. It makes getting started so easy and sets you up for success with a small win, which can then be a building block to further actions. 15% Solutions is so effective because as well as making it possible to break down ideas and activities into achievable tasks, it also focuses on what’s within an individual's sphere of control. Empowering people to move forward when they’re blocked, by focusing on what they themselves can do, rather than what they’re unable to do or what other people could do. Tips & Tricks Make sure to encourage participants to supportively challenge any proposed 15% Solution that doesn’t fit the requirements. They must be actions for the individual proposing the solution and be something that individual can do without additional time, money or permission. Do step in yourself if you need to, asking ‘Is that something you can do?’ etc. You may like to strengthen the 15% Solutions by following up with GROW model coaching questions. You can encourage participants to support each other by asking these questions too. For example, Will you do this? When will you do this? What could stop you from achieving this? How would you overcome obstacles? Who could help you? How committed are you to doing this, on a scale of 1 to 10? What now? You can use Liberating Structures for great outcomes, even if you’ve never tried them before You can use Liberating Structures to improve the quality of your interactions too. Better meetings are within your reach. Your role or position doesn’t matter, your experience doesn’t matter. Dive in! The brilliance of Liberating Structures is that you don’t need to be an expert. You can have no experience and get great results using them. Some of them are so simple you can read through the instructions online and use them straight away. I recommend 1-2-4-all and 15% Solutions to everyone to try immediately. The beautiful simplicity of 1-2-4-all #LiberatingStructures makes it so quick and easy to deploy in a facilitation emergency. Having done so, a group can use it again and again, and gain confidence that they will create what they need together. — LiLi Kathleen Bright ✨🌈 (@gobrightly) January 18, 2019 Learn more There’s a wealth of resources available if you want to know more: A book, a website, trello, blogs… the list goes on. More impressively, there’s a vibrant and responsive community of people around the world, willing and able to support and provide feedback. Join the Liberating Structures Slack and get into the #wisecrowds channel. I cannot recommend this Slack highly enough. It’s really incredible. Brilliant community. Go, join! There are local user groups globally, as well as virtual events. Kathleen and Tas design and deliver events as co-organisers of user groups in London and Berlin . Come and join us! Or, find events near you, including remote . Because you don’t need to be an expert to use them, it’s easy to talk about your ideas with colleagues and friends, and get their input too. Lots of LS practitioners are on Twitter too. What next? Check out my compilation of Liberating Structures resources for more. Do get in touch with any questions or thoughts about Liberating Structures, you can reach me on Twitter .", "date": "2019-03-07"},
{"website": "Novoda", "title": "Gain confidence in your investment with Product Discovery", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/gain-confidence-in-your-investment-with-product-discovery/", "abstract": "So, you have an idea for an app and you want to see if it's an idea worth pursuing? Before spending too much time and money, you want to be confident that your product will be a success in the world. You want to be sure that your service will deliver real value to consumers and subsequently, drive the success of your business. What is Product Discovery? At Novoda, we use structured workshops and research methods to guide our clients through the product discovery process. Where we seek market research, surface industry insights and analyse associated services and product offerings to better understand the market and competition. Together, we firm up business goals, explore market opportunities, surface consumer insights and user behaviours so that you can make more informed decisions - especially useful if you're looking for support or investment! Typically set over a three to six week period, this is an effective way to explore ideas, themes and theories in a collaborative team environment. Surfacing knowledge with people who really know their stuff, adding joy to the overall experience and setting the stage for collective, impactful decision making! Who are Product Discovery Workshops suitable for? Digital Product Discovery Workshops are especially valuable for individuals or businesses who are looking to better understand the market and explore how their proposition might fare in the real world. People or teams who want to take an evidence-based approach to decision making before making any big investments. This approach suits those who seek tangible next steps, detailed industry reports and expert guidance on decision making techniques to maximise the likelihood of success. What does your Product Discovery Team look like? Your team will be made up of experts from design, product leadership, engineering and key representatives from across your business. The wider the variety of participants, the more expert knowledge there is to be shared, allowing us to generate an array of ideas and opportunities, whilst keeping firmly rooted in the reality of delivery. What does your Product Discovery Workshop entail? Your Product Discovery workshop will be tailored to your needs, considering budget, time available, idea complexity and confidence in prior industry and consumer research. Together, we will explore your current position and design a workshop that perfectly suits your needs. Typically, the effort considers three primary areas: Initial Discovery, Product Definition & Prototyping & Testing. Initial Discovery Qualitative Discovery Study: Plan and conduct discovery interviews with target users to give you a deeper understanding of the product problem and user needs. Quantitative Study: A quantitative study, like a survey, to bolster your qualitative insights and provide you with more confidence in the problem statements and product hypotheses. Market Research: Conduct market research to provide insights to the latest trends, developments, opportunities, and risks for your product or service. Benchmarking competitor products: Product analysis of competitors and identify areas of differentiation for your product or service. Product Definition Impact Mapping: Helps to align teams to business objectives, test mutual understanding of goals and expected outcomes with stakeholders, focus your team toward the highest value features to deliver, and enable collaborative decision-making. User Story Mapping: User Story Mapping exercise to prioritize your product feature set helping to see the 'big picture' of your product. User Journey: Visualising the experience of interacting with your product from the customer’s point of view. Laying the groundwork for meaningful interactions and successful business outcomes and a tangible framework for further customer experience initiatives. Lean Canvas Session : Aims to provide you with an actionable and entrepreneur-focused business plan. Focusing on problems, solutions, key metrics and competitive advantages. Product Vision Board: Laying out the whole concept of the product, including target users, user needs, product definition, and business goals. Prototyping & Testing Sample UI Screens: A set of sample UI screens for your product that can be used to show your stakeholders the direction of your product. Clickable Prototype: Provision of an interactive visual prototype of the core feature of your product, a great tool to showcase how the user experience could function. Product Vision Document: Collation of all the information gathered, sum up the results of our workshops, and write up of final recommendations for the next steps in the Product Vision Document. Validate prototype with users: Conducting user tests with the prototype to bolster the confidence in the current designs and provide recommendations for future improvements. Refined Clickable Prototype: Refining the current prototype based on the insights from the user tests to provide you with a user-validated version you can present confidently to your stakeholders. Conclusion We love that people choose to share their product ideas with Novoda, with the ambition to bring them to fruition. This exciting and integral experience is designed to guide you to make more informed decisions that will define the direction of your product, giving you confidence and a clear roadmap to pursue this idea further. Ultimately, if you and your team are working as one, towards an aligned goal, it is more likely that all your efforts will be focused in the right direction and you'll get better results. To learn more about our Product Discovery expertise and how we can support your business, get in touch . Let’s start the conversation, and see if this approach is right for you.", "date": "2020-04-30"},
{"website": "Novoda", "title": "How to foster remote collaboration and culture", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/how-to-foster-remote-collaboration-and-culture/", "abstract": "We teamed up with Design Lab London , Design Lab Berlin and Foolproof , to share insight into keeping collaboration and culture alive and thriving in remote environments. During a one hour webinar, Head of Design at Novoda, Leonie Brewin and Director of Strategy at Foolproof, Tim Loo , deliver two talks covering inclusive remote practises which can shape how your team and company collaborate, improve performance and culture - and why these aspects are so important to success. 🗣Talk one: Energising teams through remote collaboration Collaboration is key to successful working environments. Through collaboration, you can increase knowledge sharing, improve transparency, enhance learning, better-align working efforts and as a result, improve the happiness of your team members. In this webinar, Leonie, Head of Design at Novoda will share some tips on making collaboration enjoyable and impactful in distributed teams. Key takeaways: The benefits, challenges and solutions for collaboration in virtual environments Speaker: Leonie Brewin Leonie is Head of Design at Novoda, a digital design agency based in Berlin and London. As a design leader, she has worked with organisations in the UK & Germany to advance careers and build meaningful digital experiences in the mobile space. With over 12 years of experience in the industry, she’s a firm believer in learning through collaboration in environments that promote career development with a healthy work-life balance. Twitter | Linkedin | Website 🗣Talk two: Practical tips to nurture and maintain distributed culture If you contribute to company culture within your business, what should you be focussed on in these times? Tim provides tips for today, tomorrow and beyond. Speaker: Tim Loo Tim is the Executive Director of Strategy at Foolproof, one of Europe’s largest specialist experience design agencies. He leads their Strategy & Planning practice, developing and deploying their experience strategy framework, methodologies and expertise across a range of global clients. As a UX strategist, he currently works with global brands such as Virgin, Shell, Lloyds Bank and AXA XL. Tim is based in London, and is a regular speaker on UX strategy in the US, Europe and Asia. Twitter | Linkedin", "date": "2020-05-15"},
{"website": "Novoda", "title": "Tools and practices to improve distributed teams day-to-day work", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/tools-and-practices-to-improve-distributed-teams-day-to-day-work/", "abstract": "Novoda and technology company Magic Lab come together to share their best advice along with tools and tips on remote environments and how to practice high performance with distributed teams in a one hour webinar for the Londroid community. Experienced Android Developer at Novoda , Daniele Bonaldo and Engineering Manager at Badoo , Jairo Gonzalez , share insight into remote practices for individuals, teams and companies, which have been tried and tested to improve performance, culture and individual happiness. With 12 years of remote working experience under Novoda's belt, and Badoo having to adapt from an in-person, in-office business model to adopting remote working practices quickly - this webinar covers two perspectives on how to implement the best ways of working. -- Talk one: Tips and tricks for remote teams Starting with one office, expanding to several countries and now with everyone working from home, the remote culture at Novoda greatly evolved over the years. In this journey, we learnt a few things about how to manage distributed teams. In this talk I’ll present some tools and procedures we adopt in our day-to-day work and that helped us improve our remote work. Key takeaways: Important tools and practices to improve a distributed team day-to-day work Speaker: Daniele Bonaldo Daniele is an Android developer at Novoda. He’s passionate about photography and wearable technologies. Before working at Novoda, he worked at i’m Spa, an Italian startup where he had the chance to play with the Android platform while developing one of the first smartwatches on the market. International conference speaker and Google Developer Expert, now he loves to work on everything related to the Android ecosystem, from Chromecast to Wear to smart mirrors. Website | Github | Speakerdeck | LinkedIn | Twitter -- Talk two: Remote work in a company without a remote work policy Last year, our processes in the mobile department at Badoo and Bumble were based on in-person, in-office communication. Interactions with the QA, product and other teams were far more complicated for those who had to work from home. Despite that, we have decided to expand our mobile team to our Moscow office and I took this experiment on myself as I'm a true believer of remote work. This led us to deal with the specificity of distance and time difference. We adapted some processes and started changing some mindsets. I’m not going to say it was easy, but definitely it was worth trying. We are very happy with the outcomes so far. But in March, when everyone was forced to work from home due to the current health situation, we already knew this was possible and just needed to extend the best practices we had in my team to the whole mobile department. I‘d like to share with you some of our processes and how we switched to a fully remote team keeping the high performance, in a company whose practices were really optimised to count on with people working in-place. Speaker: Jairo Gonzalez Jairo is a busy engineering manager based in London. When he's not leading different mobile teams, looking after his toddler keeps him on his toes. From his initial role as a full-stack engineer, he has over a decade worth of experience in social networks. He currently works at MagicLab, who is on a mission to create life-changing moments by building relationships through its apps, Bumble and Badoo. Linkedin", "date": "2020-06-04"},
{"website": "Novoda", "title": "How are App Clips going to save my day?", "author": ["Michal Wierzbinski"], "link": "https://blog.novoda.com/app-clips/", "abstract": "Here I am, on the streets of Wroclaw. Pandemic lockdown rules have been partially lifted and I can finally go and meet my friend. I don't really wanna use the train, because... well people. So I opt-in for one of those electric scooters. There are two just next to me, but they’re not owned by the company I’ve signed up for. I check the app I have installed, but there are none nearby. Ok, looked up the app for one of the scooters that are near me. Downloaded, registered, about to unlock it and happily drive... The scooter is out of battery. It's fine, it's fine. Not frustrated at all. Downloading the 3rd scooter app on my phone, going through registration, and finally, it worked! I'm driving, albeit a bit less happily, to my meeting, thinking: I can't wait for App Clips to be here... Image Credit: Apple What exactly are App Clips? App Clips are a new feature in iOS 14 presented by Apple on the last WWDC, and as you can see, I'm already hyped about them. They are fast, lightweight, on-demand portions of your application. A good App Clip focuses on allowing the user to finish their task within seconds and that's including \"installation\". The typical journey: 1. An iPhone user encounters one of the ways to launch an App Clip, this could be a QR code. 2. This user scans it and is presented with an App Clip card, which is only one click away from a sliced portion of an application. 3. After 8 hours the app is gone from the user's phone. The last part is an important bit. The App Clip is not a separate product. It's more like an extension to your existing one. You cannot have an App Clip without having an App on the App Store. WWDC2020 - Image Credit: Apple Is a scooter renting company the only place to use App Clips? Shockingly, no! Imagine going to a paid parking and instead of dealing with parking machines, you simply scan a code and press pay. What's even cooler, you don't even need to scan any physical code to start the interaction. Because the parking lot is associated with a specific location. An App Clip can be registered to appear on a place card in Maps so users can open it from there. If you're thinking, “but I have several parking lots, can I tag my App Clip with a different location?”. Yes, you can. We can even go a step further. Let's say you are providing a solution for small businesses. Be it our parking lot owners or even better, a delivery service for multiple restaurants. Not only can you have a tag per restaurant on Maps, but also, you can customise App Clips to a given business using their own branding. With COVID in mind, restaurants are looking for ways to reduce in person contact and there is a big opportunity with App Clips. Restaurants can put QR codes on each table where customers can quickly scan and get a menu pop up on their phone. What's even better, thanks to the way you can set up links, you can have a code per table. So the client can order through an App Clip, and the restaurant will know which table the order has to go to. Since most of this can be done through a QR code why bother? It’s true, opening a website with QR is still a valid approach, but there is a difference in experience. App Clips have great integration with Apple’s frameworks, and what's most important, they look like a part of iOS and can prevent the interruption of the users end to end experience. This is so exciting! Tell me more... Okay, reader, I will! First of all, it's not only QR codes that trigger the launch of your brand new App Clip; we have banners in Safari, Maps, nearby suggestions from Siri, messages and NFC marked with Apple’s new design. If that's still not enough, App Clips is based on your application development. It’s no different than developing an App. This means that developers don't need to learn any new magic tricks to implement this. However , there is a 10 MB limit on the size of the App Clip and that may cause some problems. Images, dependencies, wanting our App Clip to do too much could affect the performance. We’ll discuss this in a follow-up blog post Last words The way that we interact with apps is being redesigned and the introduction of App Clips are a step in the right direction. For users this means; no more App Store, unnecessary downloads, signing in or going through numerous tutorials. For App creators, this is a new way to help the user save time when purchasing their product, and may be a way to engage them to download the full version. It’s much easier to check out the functionality of an app, without the overhead of downloading and signing in. It's a cool feature that is well integrated into the OS, giving users a seamless experience. I, for one, am looking forward to seeing how developers make the most of this new feature.", "date": "2020-09-21"},
{"website": "Novoda", "title": "How to foster remote collaboration and culture webinar: Your questions answered!", "author": ["Team Novoda (Joint Thinkers)"], "link": "https://blog.novoda.com/how-to-foster-remote-collaboration-and-culture-webinar-your-questions-answered/", "abstract": "We partnered with the London and Berlin design lab communities and Foolproof Agency, hosting a webinar, bringing two talks highlighting the best advice on keeping collaboration and culture thriving in remote environments. Leonie Brewin , Head of Design at Novoda and Tim Loo , Director of Strategy at Foolproof , share insight into how inclusive remote working practises can shape how teams and companies collaborate, perform and interact - and why this is so important to success. During the webinar, questions for Leonie and Tim poured in which we couldn’t get to due to time constraints. In this blog, Tim and Leonie answer your questions on remote collaboration and culture. Do you have any suggestions for remote collaboration to accommodate for time differences? Leonie : If you’re using Google calendar, you can head into your general settings and set your working hours so that when colleagues are scheduling meetings, they can find the most convenient time for everyone. I’d also recommend being open and clear with your availability - if a meeting is arranged outside of your working hours, don’t be afraid to let them know and politely decline, suggest a new time or if it works for you, shift your working schedule for that day. Tim : Being respectfully firm is key to cross timezone working. Respectful in that we must be sensitive to the time zone differences and potential working context for pulling people into sessions early or late in the day and firm in being able to say no. Often the goodwill and politeness of colleagues which is important to make working sessions happen at short notice can feed resentment of being the one that is expected to compromise and sacrifice too often. Planning and retrospectives can ensure this pressure doesn’t build up over time. Tips on productivity hacks, like what is a recommended routine to produce effectively in times of quarantine and home-office? Leonie: I would advise setting up a comfortable working environment in a distraction-free environment. Establish a regular daily schedule for you and your team to stay connected whilst allowing for focussed work time. Take regular breaks (and allow yourself to feel comfortable with that), using the ‘away’ status in your chat tool to keep your team informed. Nurture reflection and continuous improvement with regular team retrospectives. Be a champion of online collaboration & transparency - share notes and outcomes from meetings and working sessions to keep everyone in the loop. What tools do you use for workshops, especially to encourage brainstorming? Leonie : At Novoda, we primarily use Miro for brainstorming sessions. We start by giving some background, posing a powerful question and encouraging people to write down any idea that comes to mind before sharing and reflecting with the group. Going back to the topic of Design Sprints, especially the remote sprints… What is one tip that you would give for running the Storyboarding session more efficiently? Leonie : We haven’t yet ran a Design Sprint fully remotely but in this situation, I would ask participants to collaborate using the drawing tool on Miro. I have a question for both Leonie and Tim; this remote working effort has really shown that we can do most without the need of an office. How should we redefine the purpose of an office - if it is not for desk space / what would the future office look like? Tim : We have a hypothesis at the moment that our offices are cultural and community assets to the business. We’re looking hard at what the purpose of office space is in the future but it seems clear that it being the home of lots of rows of desks for people to sit at probably isn’t it. We’ll be experimenting with how our offices will need to be and how it works to support the social fabric and sense of community we get from being together. Leonie : Great question! We have recently been exploring this at Novoda - even before the pandemic. We asked Novodans what they like about having an office space - and the answer was clear: Socialising and collaboration. So now, we are searching for a space that has more flexible co-working spaces to allow for flexibility, space to socialise and enjoy time together and space to host workshops, consumer research and community events. I love the Miro idea where people drew pictures of animals in 20 secs. Do you have any other ideas? Leonie : That’s a quick and simple one and lots of fun. We’ve also had a lot of fun with Drawception where each participant draws something from a prompt and the others in the group have to guess what it is! So Tim, you like guitar, photography, books ordered by colour, nirvana; how important is the backdrop for social engagement? :) Tim : That’s funny you noticed that. I’m extremely lucky to have a dedicated working space in my house and that it contains things that reflect my interests and make me happy  (ie. it’s a dumping ground for my personal stuff). Sometimes it’s a nice conversation starter - I’m very nosy in looking at the shelves and books of people I work with! It seems difficult to get representatives of the client, business, design and development into a remote design sprint. Do you have experience bringing in clients also? Leonie : Novoda works in constant collaboration with our clients so thankfully, this hasn’t been a problem for us when running design sprints. If you’re having trouble to get people involved when working in distributed locations, I would suggest touching base with them and considering how you might adapt the approach to best suit the participants that could bring the most value. What's your top tips for not getting distracted during calls and workshops? Leonie : Turn off your chat tool. I turn off Slack whenever I have a meeting or workshop (and ask others to do the same) so I can stay focussed, be respectful to people I’m collaborating with and get the most value out of the session. The majority of industry leaders are empathising with remote working and finding ways to create an effective culture with minimum impact on productivity. My question to Tim is, has this outbreak forced Foolproof to rethink its vision, mission and strategy? Or are you sticking to the same roadmap but with a different approach? Tim : Foolproof’s vision has been, for many years, to improve the lives of millions of people around the world. Our strategy is to be the experience strategy and design partner to some of the world’s biggest brands with global reach. We think that to do that we need to collaborate with clients where they are, drive value through customer centred product strategy and design & deliver digital experiences efficiency through multi shore teams. The COVID-19 crisis has accelerated the need to be able to deliver this. It’s a very challenging business environment at the moment but we believe our global nature of our group creates the possibility to extend our reach with clients and colleagues in different countries and location locations make this an exciting time. There you have it, answers to the questions we didn’t find the time to discuss during the Q&A. If you missed the webinar and would like to watch it it can be found here . Enjoy yourselves and keep striving to improve your remote working practices.", "date": "2020-06-17"},
{"website": "Novoda", "title": "How IPFS is taking on HTTP", "author": ["Simon Rowlands"], "link": "https://blog.novoda.com/how-ipfs-is-taking-on-http/", "abstract": "At Novoda we have been investigating products that preserve user privacy - and one that’s caught our attention is IPFS. What is IPFS? Here is the definition provided by Wikipedia: “IPFS is a protocol and peer-to-peer network for storing and sharing data in a distributed file system” The way we currently share data means that we rely on a centralised server; one that is owned by a single entity, which many people connect to in order to receive some data. Imagine opening a URL on your phone to download a video, the server that hosts that content may be hundreds of miles away; it may use a lot of bandwidth, data and time depending on the size of the video. If you then wanted to watch it on your computer it is likely you will open the same URL to download the same video again. There is a better way of sharing this data! Enter IPFS. Left: HTTP Right: IPFS How does it work? A device connected to IPFS is known as a node, it could be your phone or your desktop - anything capable of sending/receiving data. The proposal of IPFS is to remove the centralised server and create a decentralised network of nodes. This means that we abandon the concept of using a URL to retrieve data from a server but instead have potentially thousands of nodes across the world, capable of storing and sharing data between each other. From the previous example, once your phone has received that video file, you would be able to open the file on your computer and download the contents directly from your phone! A huge advantage of IPFS is that a file may not necessarily be stored on one node; there may be many nodes storing small pieces of that file. This means that instead of retrieving the file from one location, you actually retrieve the file from multiple nodes at once, massively reducing bandwidth. ...Still not convinced? If you’re a cynic like me, you’ll be asking the following: Q: What if a node fails during a download? A: The data will be stored multiple times across many nodes, if one fails we’ll use with another! Q: What if all of the nodes lose a piece of data? A: This can happen, especially with old files; File pinning and Filecoin are options and will be discussed below. Q: How do I actually retrieve a file without a URL? A: With content addressing. Instead of connecting to a URL (URL addressing), IPFS will retrieve the data from any nodes that are hosting content with the specified content address. The content address of a file is generated from the files content so that any change to the content results in a new address. The InterPlanetary Name System (IPNS) allows you to create a second more permanent address to a file that does not change when the content is updated. Persistence I mentioned two ways to keep data persistent; file pinning and Filecoin. File pinning is a very simple concept, you can pin content to a node so that it will be permanently stored and ignored from any garbage collection. Filecoin was created by Protocol Labs, the creators of IPFS, to incentivise the network and add resilience. It is a storage service backed by IPFS where users can buy and sell storage space in exchange for the Filecoin currency (FIL). When you store data via Filecoin it will ensure that your data is stored on multiple nodes to avoid any data loss as Filecoin can track how many nodes are hosting a piece of data. So, is this the future of file storage? IPFS and Filecoin are critical for the next iteration of the internet AKA web3 and technologies such as blockchain are supported by them. We are exploring the future of the mobile landscape and believe that IPFS has potential on the mobile platform. What do you think? If you want to share your thoughts with us you can continue the conversation on Twitter here. Thanks for reading!", "date": "2021-02-23"},
{"website": "Novoda", "title": "An evening with Ladies, Wine & Design", "author": ["Qi", "Anna Youngs", "Margarida Aleixo"], "link": "https://blog.novoda.com/ladies-wine-and-design/", "abstract": "Ladies, Wine and Design is the  brainchild of Jessica Walsh, and has grown into a worldwide movement. The ladies in question believe in the creative power of women and the need for equality in the tech and design industry, and want to see that reflected in the real world through mentorships and championing others’ work. In August, Novoda helped host a Ladies, Wine and Design Newbies event, where first timers could be part of a LWD event in a more informal and small-scale environment than one of their major events. That night there were 10 new ladies joining us, and it proved to be a fantastic opportunity to talk life, kids and career, while also giving us all time to indulge in some cheese and wine - what’s not to love! Inspired by the evening, three Novodans have taken the opportunity to share their own experiences as women working in the tech industry, answering questions they’re often posed in the quest to find work/life balance and reach their full professional potential. Daisy - How do you juggle babies, childcare and working? When one of the ladies raised this question \"Who in the room has kids?\" there was silence. In this case, it was largely because we had a big younger generation in attendance, but it's not the first time that this happen, and I find that talking about babies and family life doesn't come easy in a professional and working environment. Every time I go to meetups, conferences or simple pub chats with fellow coworkers, the same question always pops up: \"How can I build a family without sacrificing my career?\". And truth be told, after having a baby and building a family, I still ask myself that same question. Recently, I've been taking some time thinking about how best to do everything you're expected to professionally, while also growing your personal life at the same time. Of course, everyone is different and every experience is different, but I think it boils down to the fact that it’s not just about you personally - it is about the community that surrounds you, it's about your parents and friends and colleagues and neighbours. In essence, everyone is in it together! Being a parent is hard work, and you need a network around you.   To be able to rely on the support of the company you work in, your friends and family, and everyone that you interact with you on a daily basis, is incredibly useful. As long as the understanding is there, everything else falls into place. Anna - “ How do I manage my career progression, and how do I know if I’m a middleweight or senior designer?” All the attendees of Ladies, Wine & Design Newbies exchanged glances, desperately hoping that someone had solved that enigma. Clearly, all of us, at some point in their careers, had struggled with their identity as a designer. It’s unsurprising that this is an issue, given the ever-changing job roles that seem to be different from company to company. Similar job titles, such as UX designer / Product Designer can mean different responsibilities, and this is increasingly common in technology. For example, a Head of Design in a bootstrapped startup can be a completely different role at a multinational tech giant. Traditionally, seniority is judged on the number of years experience in the field, which is measured against a linear narrative. This tends to begin after graduation, leading on to internships, then an entry level job as a junior, then becoming a middleweight a few years later, eventually ending up at a senior positions after 5+ years. The inspiring aspect of working professionally as a creative is that, realistically, there is adventure and complexity in the journeys we take. It’s not always necessary to have a formal education, and a change of career is common in the creative fields. So, how can we navigate this within our workplace, and what is the best approach to this when looking for a new role? In ‘Lean In’ Sheryl Sandberg, COO of Facebook, describes her career as a jungle gym instead of a ladder. She reflects that “I could never have connected the dots from where I started to where I am today.\" In other words, we are only able to understand how we got there, once we’ve reached our destination. So how can we ever navigate our career, without the directions or even a map? Whether this has been caused by  a lack of direction in the workplace, or being a lone / freelance designer, eventually we all find stability in the traditional job titles. However, with the ever-changing labels, it's no wonder we can feel lost. As we sat in our circle, bouncing our stories off each other, the group became a compass. The diversity of skills in the room, gave us a moment to reflect  on our own experiences. By comparing how others sell their work, and the skills they bring as an employee, it helped us all to have an understanding of our own value. Unfortunately, there will never be a right answer to “How do I know if I am a middleweight or senior designer?”. Ideally, your workplace should be able to support you in your personal growth, and it is advisable to find a role somewhere that encourages growth and progression. Following our discussions it become clear that alongside your everyday work, you should make sure you also find your tribe. For us, our tribe was a group of like-minded ladies, of varying years of experience, who were willing to be open and honest. These conversations are a type of mentorship, and who knows - by attending events such as Ladies, Wine and Design, you could be sitting next to the next Sheryl Sandberg! Qi - “Should I consider an unpaid internship, or work shadowing?” When the ladies brought up this topic, I was reminded of  when I started my career as an unpaid intern 7 years ago, and I still remember the struggle it was. On the one hand, I appreciated opportunities to work with other professionals and be mentored by senior designers. On the other hand, I needed income to pay bills and I didn’t see my value being reflected in return. “Your goal may be to get a job, but your first task is to crack open the door.” said Dick Powell, chairman of  design charity of D&AD. As one of the many fresh graduates desperate for work opportunities, it’s easy to  feel depressed and question your own value if you’re offered an unpaid internship. However,  unpaid experience can be invaluable, because the real industry is so different from what we learn in university. It’s immensely valuable to observe how to manage projects, communicate with clients and have a deeper understanding of markets from those that have done it before. Being part of a reputable company, collaborative team and inspirational projects can be more important than money. However, realistically, working unpaid is not sustainable for the long term. Try to turn the unpaid internship into a job with your impressive performance and ability to learn and grow, don’t be afraid to speak to your manager about whether there is an open role, and ask for referral letters and recommendations if there isn’t. Consider unpaid internship as self investment, helping you towards a rewarding job and adding high-quality works to your portfolio. Most importantly, always be faithful and never lose your passion for design - when you’re offered a job, whether it’s paid or unpaid, consider carefully whether this will fuel that passion. Thank you to LWD for providing a platform for us to talk and share experiences. If you want to be part of the conversation go to Ladies, Wine and Design Facebook page", "date": "2018-11-15"},
{"website": "Novoda", "title": "Self Assessment Framework: How we use Miro for career progression and prepare for effective 121’s", "author": ["Anna Youngs", "Lydia Selim"], "link": "https://blog.novoda.com/self-assessment-framework/", "abstract": "We’ve all been there, your yearly review is looming with your manager and you feel somewhat unprepared. You feel that you’ve had a professionally impactful year and you feel quite pleased with what you’ve achieved, but how on earth do you convey all of that? On top of figuring out your strengths, where you’d like to grow, and what goals you’d like to set and achieve by next year? At Novoda, Lydia and Anna developed a new framework and tested out the methods during the 121's with the design team. We wanted to create a process that was interactive, asynchronous, and rewarding. It needed to be both beneficial to our direct reports as it is for us as managers, to allow everyone to get the most value out of the time we spend together. We wanted the process to be agile and iterative as we grew and adapted to the dynamics of the design team. Most importantly, we envisioned something that could be used regularly - not just every 6 months - and with enough flexibility it could adapt to different individuals and their needs. At Novoda, we have a culture focusing on learning and curiosity and it seemed important to enable that at a self reflection level too. We created a flexible framework that can be adapted to different personalities and needs. It is an organic tool you can regularly re-visit and re-adapt together over time, out of the official performance review time. It is more casual, less formal and therefore easier to use. If you manage a team, we highly encourage you to run through this process yourself before speaking to your reports about it. It will help you to build empathy and a deeper understanding of the tasks, while you support and guide others. It helps to tailor your approach and understand your direct reports better - what motivates them, what could be challenging. It will highlight patterns based on their previous experience, current role and ambitions and, as a result, allow you to more effectively advise them both on the long run and within the realm of day to day support. Having a career progression framework can help to make sure you can both support and guide everyone sensibly at a personal and professional level while also reaching alignment with the team/organisation's growth strategy. It provides a better understanding of the team dynamics and allows you to identify personal preferences, needs and motivators as well as opportunities for people to pair and mentor each other. Self-Assessment Activities This self assessment framework focuses on the person's past, present and future as well as their expectations. It helps to actively define and work towards their desired path as a designer, within their present role, team and organisation, and in general. Flexible and adaptive, it is designed to evolve with time and to be personalised. The self assessment can give a better overview of individual progression. Additionally, it unveils tangible opportunities for their manager and organisation to support their and the team's growth (e.g.who could help/mentor, what can be done short and long term, what opportunities are present to support, what initiatives could be started, etc…). It can be a way to assess against the organisation's benchmark in place and expectations for their role. There are a few impactful activities that you can do to build up your self awareness and to have more meaningful conversations with your manager: Career Timeline: Highlights and Lowlights Aastha Gaur's article \"The First Career Conversation\" as well as Jason Mesut's \"Shaping Design and Designers\" series directly inspired us to start our framework with a career timeline exercise. This introspective approach is a great tool to enable self-reflection in a tangible way and we recommend everyone to take the time to do it for themselves, regardless if you plan or not to try the self-assessment framework for you and your team. By focusing on their past career, people can easily identify what occurrences were positive or negative and use this information to inform future steps and better articulate their needs and expectations. To make informed choices about what you need to be happy and successful in your career it’s useful to reflect on your highlights and lowlights. Highlighting the emotional highs and the lows during your various jobs and employers and marking out events or themes that explain some of the peaks and troughs. Radar Chart: Skills mapping A radar chart is a method of displaying data represented on axes starting from the same point. It looks similar to a spiderweb. Plotting your confidence in your skills on a chart allows you to visually compare your skills, and to see which areas you are excelling at. This approach is great to adapt for each team depending on the skill sets that are required. Overtime you can compare charts from each review and see how your chart is developing and changing, this will show which areas you are the strongest in which is important to identify as you tend to grow skills where you are already have a natural strength or capability in due to your experience, interests, working style or personality. Reverse Goal Setting Instead of working out what you want to achieve, try to write down all the things you don’t want to happen and then focus on the areas you have control of and turn them into goals. This approach is a great way to set more achievable goals that are precise and contextual, it also is a great way to create goals in order to mitigate risk. For example, “I don’t want to make products that people struggle to use”. Your goals could be: Get feedback from users once a month on my prototypes before implementation. Share my ideas and prototypes with my colleagues to get feedback on usability. Integrate usability specific analytics to be able to track how the product is being used post release. Once you have a selection of goals, it’s good practice to turn them into SMART goals. This is a five step process to build a concise measurable goal that allows you to understand what needs to be accomplished, when, and how you know when you've been successful. This makes it easier to create more achievable goals and track your process. For each of your goals check that they are... S pecific - What objective needs to be accomplished? M easureable - What are the trackable benchmarks? A chievable - Is your goal within reach? R elevant - Does the goal matter to your organisation? T ime-bound - What is the designated time-frame? Test it out Following the success of this self assessment, all the designers within our team now have their own miro board with the self assessment tasks that they can use anytime. We’ve also rolled it out to the engineering team, who are also reaping the benefits of having a structured framework. If you would like to try it with your team you can find the Self Assessment Miro template on Miroverse . Once you’ve run through these tasks, personally and with your direct reports you’ll both be in a more self aware mindset and will find it easier to talk about what you want from your career. Reviews will become easier, and you’ll feel more confident and will have fewer barriers when asking for what you need and want. Go Further Do not hesitate to try other exercises and/or adapt the examples listed in this article to your team and organisation's specific needs. This overview of all the exercises used in the Shaping Designers & Design Teams series from Jason Mesut is a great resource to find detailed instructions and valuable content around self reflection and career progression. We would love to hear how you personalised your framework and used the exercises! Join the team If you want to be part of a team that deeply cares about your own career progression and you want to take part in defining and iterating our existing frameworks, we are looking to grow our team. We are looking for strong individual contributors with the appetite to develop strategic design methodologies, and to build your design leadership confidence to make insightful decisions for the product team and our clients. Product Design Lead - Novoda Novoda is looking for an experienced Product Design Lead to join the product design team. In this role you will be demonstrating your skills as a team leader and own design delivery and strategy across a team of designers. This role would suit someone ... Novoda", "date": "2021-02-11"},
{"website": "Novoda", "title": "Define before Diving: Product Strategy", "author": ["Anna Youngs", "Lydia Selim"], "link": "https://blog.novoda.com/product-strategy/", "abstract": "Defining your product and what you expect from it can be as important as creating the product itself. It is what allows a company to align their strategic vision with short-term and long-terms results, allowing companies to reach their users and market in a more direct and clear way, instead of producing a product whose strategy is too general and ambiguous. Lydia and Anna , Product Design Managers at Novoda, gave a talk at Codurance on the essential concepts of product strategy and the steps to a product definition, the key phases and importance of design thinking and the innovation value it adds plus research methods and tools to analyse the obtained information. We also learn about the huge value of clear communication and good practices when working with the rest of the team. This talk provides an enriching and useful insight for companies and stakeholders looking for a more effective way of making their vision a reality and wanting to know more about the components of a good product strategy. Key Takeaways Design Thinking isn’t just for designers. It’s for everyone involved in building software. People are naturally protective of their ideas, it’s only human. But the design thinking process is about sharing ideas, transparency, and challenging each other's thoughts. More goodness comes out of that collaboration than running with an idea from one person. A research approach during product definition can open up more user-centric opportunities and provide further team alignment and understanding. Ensure there are opportunities to involve different disciplines through all the stages for an inclusive a holistic strategy. Spend some time learning how to effectively give and receive feedback. It'll be easier to collaborate and mature your process. Being a good storyteller is a superpower when communicating strategy. It allows you to clearly explain the journey in an engaging way while respecting others perspectives. To learn more about our Product Strategy and Discovery expertise and how we can support your business, get in touch . Let’s start the conversation, and see how we can support you.", "date": "2021-04-07"}
]