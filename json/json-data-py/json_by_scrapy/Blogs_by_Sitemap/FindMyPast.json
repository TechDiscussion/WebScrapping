[
{"website": "FindMyPast", "title": "Missing Maps", "author": ["Laura Green"], "link": "https://tech.findmypast.com/missing-maps/", "abstract": "All introductions to Missing Maps include at least a nod to John Snow ( the 19thC doctor dealing with cholera, not the GoT hero ): tl;dr : Dr Snow plotted his cholera patients on a map, and when he saw a cluster around a street containing a water pump, he took the handle off the pump and stopped the outbreak. Read the full story about John Snow That was in 1854. Scale that up to today’s worldwide concerns, where free and up-to-date maps are a critical resource for relief organisations responding to disasters or political crises. The Humanitarian OpenStreetMap Team ( aka HOT ) along with OpenStreetMap in general, creates and provides those maps. ###Armchair mapping HOT has an online tool ( called the Task Manager ) to organise the efforts of “armchair mappers”, providing instructions and little squares of aerial imagery ready to be traced. One evening recent, this is what some Findmypast colleagues used to trace buildings in Swaziland for the Malaria Elimination Initiative to help create a comprehensive building foot print map to aid upcoming field work in 2016. The Task Manager has several tracing tools available, and the web app one ( called iD ) is very easy to learn - we were zapping through buildings within minutes of starting. Gratifyingly, our tracing immediately appears in OpenStreetMap - no extra processing required.  The HOT Task Manager provides a validation mechanism ( by remote mappers, who happen to be passing by ) but nevertheless, your own tracing is not in some slow backlog - it’s right there, immediately live and available to use! There’s also a java app called JOSM , which has a steeper learning curve, but has many advantages to speed things up, and is used by most validators. The more involved in HOT or OSM people are, the more likely they are to get into learning JOSM . iD is perfect for those first steps into armchair mapping though. ###How does tracing get used? OpenStreetMap provides daily downloads of all areas of the world (this can become hourly, for specific crises). These are used by fieldworkers, using GPS devices, or even smartphones; and NGOs analysing data for all sorts of uses: Pete Masters, project coordinator, Missing Maps : “Again and again, I hear and see NGOs , such as MSF , using maps that include OSM data in their operational decision making, their logistical planning, their disaster risk reduction programming and their epidemiological analysis. Much of this data has come from Missing Maps and HOT volunteers and a lot of it has come from people at mapping parties.” In the case of our Swaziland data, it will be used by epidemiologists to track which buildings received indoor spraying, where mosquito nets were received, and occupancy numbers etc. It’s very easy in the developed world to imagine everyone is ‘catalogued’ and ‘numbered’ - but it’s just not the case in the developing world, and this lack makes it very hard and inefficient to provide vulnerable people with the help they need. ###How you can contribute to Missing Maps Sign up for an account at https://www.openstreetmap.org/user/new Go to http://tasks.hotosm.org/ and pick a task (read through to see which are easy for beginners) when you first click on a Contribute button, you’ll be asked to allow the Task Manager access to your OpenStreetMap account. Pick “Edit with iD ” and away you go.  There’s lots of onscreen help to get you going. You could sit down with a very large mug of tea and go through the learnOSM docs - alternatively, watch out for the next public Missing Maps Mapathon or join in with the London office mini mapping events (I guess we could also do this remotely??). ###What’s it got to do with Findmypast?\nWe use many open source elements in our products, not the least of which is OpenStreetMap itself, as a “present day” layer (along with two old map layers) on our recently launched 1939 Register . This feature is something our customers like a lot.  So we want to do more of it. There are also many projects within the OpenStreetMap arena that could benefit from our involvement, both from a humanitarian perspective and general technical expertise. After all, we are a clever bunch… Laura Front End Designer, Findmypast OSM user: LollyMay", "date": "2015-12-21"},
{"website": "FindMyPast", "title": "A picture tells a thousand words", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/a-picture-tells-a-thousand-words/", "abstract": "Contents Why Diagrams? Plant UML Sequence diagrams Activity diagrams Component diagrams How to write and edit diagrams How to generate diagrams Alternatives Why diagrams? I find that diagrams are a great way of visualising complex behaviours or systems so that anyone can understand them.  In my experience stakeholders, product owners, project managers and other engineers can understand these more easily than a representation within text.  We use them as reference when discussing requirements in either architectural or project meetings. Our designers do mock ups to get quick feed back on various designs of our user interfaces.  System diagrams are akin to programmer mockups for how the final system will behave.  This can have some nice benefits as problems can be spotted early for little effort with tools, as I’ll show below. One more benefit is when new engineers join an ongoing project.  If an API or a service has a set of diagrams that describe the important functions of that service then it makes the onboarding experience a lot smoother than having to dig through the code to get an idea of what’s going on. Plant UML Plant UML is a great open source tool for creating various architecture diagrams.  They provide a domain specific language for generating diagrams.  Here’s a small example of the syntax. @startuml\nBob->Alice : hello\nBob<-Alice : hello yourself\n@enduml Which results in this sequence diagram I recently come across it after looking around for a few different ways to work on diagrams as a team.  We tried various diagramming software like Visio and Google Docs diagrams but none of these worked well for sharing within an engineering team. I went looking for a new tool with two things in mind: The diagrams should live in the code repository, and the diagrams should be text based so that the team would get the benefit of diffs and version history. Plant UML is a good fit for those two requirements and it also has the widest selection of diagram types available that I’ve found.  So far we’ve used sequence diagrams, component diagrams and activity diagrams. I’ll give examples below. Component diagrams Component are great for system architecture diagrams. This diagram shows a high level representation of how our metrics gathering system works.  We track things like the lead time between code being committed to code being released to customers. Sequence diagrams Sequence model interactions between various actors within a system. What we see below is how we perform tests at an integration level and the different interactions between the different components within the system. Activity diagrams Activity is like a logic diagram This shows us performing a while loop, while plugins are available and generating metrics. How to write and edit diagrams I write my diagrams in plain text using Atom . There are two PlantUML packages that I use: PlantUML language for syntax highlighting PlantUML viewer for realtime diagram preview as you write How to generate diagrams and show them in a Github repository Once you’ve written your diagram you’ll want to be able to generate your diagram images and make them available. To automatically generate the diagrams I created a node module called diagram-cli . Below are instructions on the dependencies you need in order to use diagram-cli . # You need to have graphviz installed to generate diagrams # if you're a windows guy choco install javaruntime\nchoco install graphviz\nchoco install nodejs.install # you then need to install the diagram-cli module npm install -g diagram-cli Once you’ve got that all installed you’ll want to go to the root of your repository and type the following commands # this will create a diagrams folder with the folders you'll need diagrams init # running this command will then generate your diagrams diagrams make In your front page readme file of your github repository add a link like this #### Diagrams Here is a [ link ]( ./diagrams/README.md ) to some diagrams In your file ./diagrams/diagrams.yml you can change your project name and also add additional properties that will be available in the handlebars file that is used for generating the README file. Alternatives While looking around I found several alternatives to PlantUML.  There’s some great options here. Mermaid is pretty cool - they do flowchart, sequence and gantt js-sequence-diagrams these look really nice! flowchart.js looks great for flow charts! There’s also this blog post which has quite a few other alternatives", "date": "2016-01-12"},
{"website": "FindMyPast", "title": "Using Pattern Matching to write a more idiomatic functional code", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/tweak-your-mind-into-functional-programming/", "abstract": "As a functional programming newbie and someone that has done OOP for a few years, I often find myself writing code in Elixir that looks more like C# or Java. Today I wanted to show how we can use the power of pattern matching to rethink how we define functions when we are writing code in Functional Programming. What is Pattern Matching? First things first, let’s see what pattern matching is. In programming we typically assign values to a variable with an = sign: var x = \"Elixir\" + \"is great\" ; Now, remember what = sign was in your Math class: x + 1 = 3 That means the two terms on either side of the = sign are equivalent . That’s what it is in Elixir: [ 1 , x , 5 ] = [ 1 , 10 , 5 ] #x = 10 The only difference here is that Elixir will also solve the equation for us. Pattern Matching in Functions Elixir will try to match a function call to a specific implementation: defmodule SimpleMaths do def sum ([ head | tail ]), do : head + sum ( tail ) def sum ([]), do : 0 end SimpleMaths . sum ([ 1 , 2 , 3 ]) #6 Here we provide two definitions for sum function. When we pass in an empty list, Elixir will pattern match into the second definition and return a 0.\nWhen we pass in a list, it will pattern match the parameter and get the head and the tail. Here we are simply accumulating the values on the list in order to perform the sum. Using Functional Programming idioms I remember when I started learning English, I learnt the grammar and vocabulary to make simple structures.\nThen I tried to build more complex structures with the former simple structures. In order to do that I applied ideas and concepts that I knew from my mother tongue into my new language. Life was so good.\nThen suddenly something arose: my English was grammatically correct, yet native speakers struggled to understand. I wasn’t really speaking English. Let’s see how this is relevant to programming. Let’s define a module in Elixir to make payments: defmodule Payments do def pay_with_paypal ( login: login , password: password ) do #pay with Paypal %{ login => login , password => password } |> validate |> do_transaction end def pay_with_card ( number: number , expires: expires , code: code ) do #pay with card %{ number => number , expires => expires , code => code } |> connectwithBank |> validate |> do_transaction end end We have two ways of making a payment, either with paypal or with credit card. That code above is me speaking Elixir in a C# way.\nIn C# I need to rely on a change in the function name to identify different implementations. Despite being grammatically correct, the two functions are going to do pretty much the same. Let’s try to be a bit more idiomatic: defmodule Payments do def pay ( :paypal , login: login , password: password ) do #pay with Paypal %{ login => login , password => password } |> validate |> do_transaction end def pay ( :card , number: number , expires: expires , code: code ) do #pay with card %{ number => number , expires => expires , code => code } |> connectwithBank |> validate |> do_transaction end end Now when calling the pay function, Elixir will use the one with the appropriate matching pattern: Payments . pay ( :paypal , login: \"juanito\" , password: \"123\" ) Notice that we have used an atom to help with the matching, it also helps with clarity of intent and expressiveness of the code.", "date": "2016-01-15"},
{"website": "FindMyPast", "title": "Put your textbooks down!", "author": ["Pete Gore"], "link": "https://tech.findmypast.com/put-your-textbooks-down/", "abstract": "I can be the worst for this. In a discussion. Talking about a certain way of doing things. Knowing exactly which chapter I’m repeating. I know the names of all the authors. And then, to really land the punch of my argument I say “this guy who is really big in the agile scene says so too” Woooah! What? Some “guy” I’ve never heard of in my entire life said that too? Oh ok, it must be true then. I’m fully bought in and I’ll change my ways immediately. I wish it were that easy. I know how frustrating it is. Being so convinced in what you believe in, and the person you’re talking to just doesn’t take it seriously, doesn’t see your point or simply doesn’t care. Knowing the theory only gets you so far And for those who say it’s tough without knowing the basic theory, I hear you. Without the textbook knowledge you may not know what to look out for. But! If all you do is spend your time regurgitating books and blogs, your argument holds a lot less weight. So what can you do? A good place to start is by trading war stories. Discussing experiences which are like for like and things where the context is completely different. – This conversation could even change your mind. Not everyone has the experience though. Don’t worry. You don’t always need it. What you do need is to make your point relevant to the situation you are in now. Build a case for the change you want to see. Show how the certain dysfunctions are negatively impacting your goals. And make your case tangible. Show the impact in a way they will be able to understand and quantify it. Now, you may be in a much better position for your solution to be taken seriously! Remember though, you can lead a horse to water but you can’t make it drink. Good luck! Reference: https://www.linkedin.com/pulse/put-your-textbooks-down-pete-gore", "date": "2016-01-29"},
{"website": "FindMyPast", "title": "Learning strategies for Developers", "author": ["Stephen Cassels"], "link": "https://tech.findmypast.com/learning-strategies-for-developers/", "abstract": "Contents Do we have a strategy Retention Learning abstract facts Learning new skills and problem solving The impact of creativity on learning Effective learning Motivation References Do we have a strategy? How do developers learn? Of course developers learn the same way as everyone else although not all learners are equal. Given that the pace of change in software delivery means we often find ourselves using a framework or language for short periods, it is worthwhile expending effort to learn how we learn. If we approached how we deliver learning-to-ourselves in the same way we deliver software products, we’d adopt practices from Lean Engineering like Kata’s , iteration, short sprints with shorter feedback loops, removing blockers, avoiding unplanned work and so on; we’d manage it professionally and as we iterate we’d get more effective and efficient at it. Two thorny issues at the heart of learning are retention and motivation , if we get good at these then we are well on the way to being good learners. Retention Inefficient learners often try a simple strategy of rote learning or memorising by repetition – like cramming for an exam – which we all know doesn’t retain well over longer periods or do much to enhance skill.  A good learning strategy for programmers on the other hand, needs to cater for skill, understanding and retention. For example, if we want to learn about a language that is new to us like Elixir or C# but we are very busy and our entire bank of learning-time happens to fall within our commute we could adopt an ostensibly efficient strategy of reading books on the topic, and for a change from reading maybe gorge ourselves on podcasts.  Thinking back to that old adage that students only retain ten minutes of a sixty minute lecture and applying that to the act of reading a book, it’s obvious we need to do more than just read it.  In order to truly learn and retain it, we need to synthesise the ideas and make them our own. Before considering that, lets think about learning facts. Learning abstract facts Facts are not easy to retain if we just practice memorising them.  We can do much better if we adopt a spaced-repetition-learning system as these have been shown to help us not only memorise but retain over long periods abstract facts like CLI, Git commands or language features.  The open source Anki product is a spaced-repetition-learning tool designed to foster efficient learning by making us practice the material we are most likely to forget using a Flashcard system. Learning new skills and problem solving Skills on the other hand is where something like www.khanacademy.org can show us the way with its clever strategy of identifying what you find hard and bombarding you with questions (and tutorials) in that problem space until you can successfully solve the problem.  At the same time it periodically questions you on things you have previously solved (often weeks ago) in order to cement that learning into long term knowledge; it utilises multiple strategies including creative problem solving and spaced-repetition-learning. Whilst Anki is great for retention of learnt (grocked) knowledge using short focused ten minute top-up sessions every day (Kata promotes daily frequency too), Khan Academy provides the additional benefit of employing our problem solving creativety to makes us synthesise ideas and concepts into new skills; this process is very efficient for retention. There are many similar models of learning via Katas on sites like CodeKata or PluralSight and its derivatives like CodeSchool (learn by doing). The key is active participatory learning – or to be all fancy about it,  Metacognition as espoused by John Flavell . To a greater or lesser extent,  Anki, KhanAcademy, PluralSight and CodeSchool all invoke this. The impact of creativety on learning What is critical about creativity is the search for meaning and this is not a clear cut linear process but one which requires constant revision of one’s thoughts and decisions of what is relevant in a search for meaning.  This sounds a bit like coding. If we take an analogy from the Arts and Social Sciences, the strategy there is to read, then summarise and then use creative writing to formulate an opinion in our own words, but why creative writing?  It’s the thought process behind formulating and arranging our own-words that is critical.  Short term memory prevents us from transcribing a 3000 word piece sequentially in one go,  instead we iterate whilst we struggle to formulate what is in our head into a cogent whole.  This polishing phase of composition - where we look for meaning by adding and re-arranging paragraphs, re-wording and so on - mirrors efficient learning. Effective learning Efficient learning then, is more than just turning up, or just reading a book and doing nothing else. It is a combination of creative thinking, bolstered by repetition and backed up by spaced repetition of things you have already learnt to transfer them or maintain them in long term memory. Matthew Syed, the author of Bounce: The myth of talent and the power of practice puts it very well when he discusses the role of deliberate or purposeful practice .  When comparing two athletes, each of whom have spent the mythical 10,000 hours (this number is commonly used as a measure of Mozart like expertise) of practice honing their craft, Matthew argues the better of the two will be the one who has spent more effort on purposeful practice. Motivation Motivation is very personal and from my experience there is no right way to motivate myself. I do find that trying new things helps as does recording my success, success breeds success…my learning needs to be fun and productive. I’ve tried Kata’s on varying sites to learn a new language (only to promptly\nforget what I learnt), indeed I admit I’ve crammed for Microsoft Exams (yip the knowledge went the same way as the Katas) but what I enjoy and retain the most is borne from building working software and blogging about it - thank you Git! It’s truly creative so I am more engaged and it has orthogonal benefits like Delivering something tangible instead of just the solution to a Kata Satisfaction from sharing and getting feedback from the developer community Its something I can augment as I add new skills Blogging about the side project helps – even if you have a readership of one you’ll learn because you become your own critic and just like rubber duck debugging it invokes the benefits of creative writing as you pursue the expression of the ideas in your head. For lots of folks it appears Katas provide sufficient motivation.  Ultimately it does not matter if you build something or just do Kata’s (indeed doing both is most likely the best) so long as we have a feed back loop that lets us verify if our learning is working for us. References Effective Learning skills - http://www.asa3.org/ASA/education/learn/203.htm Teaching Writing for Learning: Roger A, 1985, Scottish Council for Research in Education Bounce: The Myth of Talent and the Power of Practice: Syed, Matthew (2010-04-29).  HarperCollins Publishers.", "date": "2016-02-10"},
{"website": "FindMyPast", "title": "Creating cookies in Javascript", "author": ["Leigh Shepperson"], "link": "https://tech.findmypast.com/creating-cookies-in-javascript/", "abstract": "The Hints UI application needs to create session cookies that are shared between different sub-domains of FindMyPast and we need to create the cookies using JavaScript (ReactJS, HapiJS). The solution is not difficult, but it is easy to go down the wrong path. Problem We want to track users who come from partners (3rd party) and we need to do this by using cookies.\nFor example, if a partner has partnerId = 65340234-a07e-4339-b218-e40a6b3803b0 and lands on http://acceptance.findmypast.com/hints-ui/, then we expect a session cookie to be created containing the partnerId. Additionally, this cookie should also be visible from http://acceptance.search.findmypast.com/search-world-records. Solution You can create a session cookie by just setting the cookie property on the document object. To make sure it is visible from all sub-domains of findmypast.com, you need to include domain = findmypast.com . For example: document . cookie = ' partnerId= 65340234-a07e-4339-b218-e40a6b3803b0; path=/; domain=findmypast.com; If this is created on a sub-domain, then this will be accessible on the domain and also the other sub-domains of findmypast.com.\nHowever, this will not be visible from findmypast.co.uk or findmypast.com.au. To do this, you need to inspect the windows.location.hostname property and retrieved the appropriate domain. Note, to create a persistent cookie, you need to set the expires attribute to some point in the future. For example: document . cookie = ' partnerId= 65340234-a07e-4339-b218-e40a6b3803b0; path=/; expires=Mon, 11 Jun 2018 12:00:00 UTC; domain=findmypast.com; Testing: Our environmental setup used a combination of HapiJS and RequireJS. By default, the application host is localhost or the operating system default. If you want to create a cookie under findmypast.com on your local machine, you need to do two things: Open the hosts file (located at C:\\Windows\\System32\\drivers\\etc) and add a sub-domain of the domain you want to test. For example, 127.0.0.1 local.hints.ui.findmypast.com In server.js, edit the host property to match this sub-domain var server = new Hapi . Server (); server . connection ({ port : args . port host : \" local.hints.ui.findmypast.com \" }); If you run the application using gulp or otherwise, it will now be served from local.hints.ui.findmypast.com. Cookies created here will be shared with other sub-domains of findmypast.com.", "date": "2016-02-12"},
{"website": "FindMyPast", "title": "Introduction to Elixir", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/intro-to-elixir/", "abstract": "Introduction I have been trying Functional Languages for a little while now. F# is in the .Net environment so why not give it a go. Haskell seems like a different monster, so let’s give it a try, Erlang and its EVM looked amazing. I don’t really know why but even though I tried, they were not very engaging for me. Then I learnt Elixir, a new language running in the EVM. I must be honest, I am a C-guy , so I like my curly braces and my semicolons. Elixir had some Ruby-like syntax, so no more semicolons to me.\nAt the end I have come to enjoy programming in Elixir so much.\nIn this post I want to share the basics with you so that maybe you give it a chance, as I did myself a few months ago. Why Functional Programming? We have all read why FP is awesome, so I don’t want to annoy you with yet another post around FP awesomeness.\nI’d like though, to give you some context, about why this thing matters. Computers are getting wider, more CPUs, many languages don’t embrace parallel processing from their inception.\nWhat we are seeing these days is OOP embracing some FP principles, as systems become more distributed. Functional programming makes a lot easier to work with Distributed systems because of… Immutability You can’t change things. Consider this Javascript code: var ShoppingCart = function (){ items = []; this . addItem = function ( item ){ //find the item, if not there push. }; this . total = function (){ var sum = 0 ; //loop the items and sum it up } this . count = function (){ return items . length ; } }; We are exposing functions to mutate the internal state of the shopping cart, in this case items property. If you then want to add an item to the cart: cart . addItem ({ sku : \" ROVER \" , price : 3333223322 }); In Functional Programming we don’t mutate the state of the application but we transform it.\nThis means that in our previous example, you would get a new cart when adding an item to it: cart = ShoppingCart . new () cart = ShoppingCart . add_item ( cart , { sku : \" MAV \" , price : 10000000000 }) cart = ShoppingCart . add_item ( cart , { sku : \" ROVER \" , price : 3333223322 }) No Side Effects In OOP we are really focus on hiding and protecting things. To me it looks like we try not to be honest with ourselves, we hide our code so I can’t touch it later.\nThen side effects arise its ugly face. Because when you hide things, those things can do other unexpected things, like saving things in database or calling a service that deletes some data. At the end we are making our lives more complicated because things are harder to follow and maintain. With FP one function does one thing. So if we mix Immutability and No side effects we get the following benefits: Pushes you to write smaller more predictable functions. Tests are clearer, actually you don’t really need mocks. Not much debugging. Distributed programming Remember how C# deals with concurrency: async, await, tasks, locks,… you have to do that because things are mutable/side effects.\nA process may be changing something while other process tries to read that something. In Javascript we have callbacks, promises.\nIn FP, you don’t share any state, things are immutable. So having different processes talking to each other is much safer. Let’s meet Elixir I am assuming that you can run IEX (interactive elixir) in a command line. If you can’t just download it from Elixir web site or use any sort of Vagrant or Chocoltery. Let’s open IEX and… No semicolons, no braces. Elixir is like Ruby, really clean languages. #this is a comment title = \"a string\" price = 12.00 quantity = 1 Atoms Atoms are labels in Elixir: my_atom = :atom customer_charge = { :card , \"4111-1111-1111-1111\" , 12 , 19 } Tuples, lists, keyword list, map, struct #A tuple is just a container of data tuple = { :label , 1 , \"Steve\" , [ 1 , 2 ]} #Elixirists like using Atoms in the first position of a tuple to describe what's in the tuple. { :chocolate , yummies } { :fruit , [ :apple , :banana ]} #This is a list, it can contain any type of data list = [ \"one\" , 2 , 3.000 ] #A Keyword list is just a list with Key and Value pairs keyword = [ one: \"one\" , two: 2 , three: \"three\" ] #You can access a member of a keyword list like this: keyword [ :one ] #A Map looks really similar to a keyword list map = %{ key: \"value\" , another_key: \"another value\" } #But you don't have to use an atom to get one of the values map . key == \"value\" Head tail lists decomposition team = [ \"diogo\" , \"mickey\" , \"darren\" , \"kira\" , \"jae\" , \"liam\" , \"stephen\" , \"waqas\" ] #you can get the first element of a list by doing: [ head | tail ] = team #head is equal to \"diogo\", tail is a list with the rest of the elements [ head | tail ] = tail #head is now equal to \"mickey\" Functions #a typical structure for an Elixir function. def a_function ( arg ) do #do something end #call your function like this a_function ( \"something\" ) #or like this a_function \"something\" Pattern matching We have a post around Pattern matching . There are many ways to define pattern matching but I like to look at it as a Maths equation.\nLet’s look today at the following.\nIn OOP we usually control the flow of the application like this: //eat_a_treat as we'll do in OOP: def eat_a_treat ( treat ) do if treat == {: chocolate , yummies } do //yummies are bound and ready to use else if treat == {: fruit , oh_no } //eat that boring fruit end end In Elixir we hardly ever write if statements, we use pattern matching to control the flow: def eat_a_treat ({ :chocolate , yummies }) do #yummies are bound and ready to use end def eat_a_treat ({ :fruit , oh_no }) do #eat that boring fruit end When we do overloading in OOP we can define the same function but with different input parametes. If you have the same input parameters, then you have to change the name.\nIn Elixir the name of the function is the same and so is the input parameter, a tuple in this case. The appropriate function will be called using Pattern matching. Pipes In Elixir we can pass the result of a function to the next function: #without pipes def eat_a_treat ({ :fruit , fruits }) do clean_fruit = peel ( fruits ) eat ( clean_fruit ) end #with pipes def eat_a_treat ({ :fruit , fruits }) do fruits |> peel |> eat end By the way you may be wondering how the hell you return something in a function: it is really easy, it will be the last statement of the function Recursion, Pattern matching, and Head tail: head tail recursion I will leave you with some mind blowing stuff. I have the list of my expenses the last month. I want to sum them all: #this is in Pounds by the way sum_it ([ 10000000 , 1200 , 230000 , 2345.6 ]) In OOP we usually loop through the list and accumulate the sum using some sort of foreach.\nIn Elixir it is actually much better to recurse through the list, and operate it. def sum_it ([], total ), do : total def sum_it ([ amount | tail ], total ), do sum_it ([ tail ], first + total ) How do I learn more stuff? Here you have a list of resources I found useful when learning Elixir: Elixir docs, I recommend looking into the Enum module, the most comprenhensive set of functions to manipulate lists I’ve ever seen: http://elixir-lang.org/docs/stable/elixir/Kernel.html Introduction to Elixir: https://github.com/elixir-lang/plug Some really nice tutorial done by Rob Connery: http://www.redfour.io/ Jose Valim, creator of Elixir guiding you to create a really nice example: https://howistart.org/posts/elixir/1 This page has tone of resources for Elixir: https://github.com/h4cc/awesome-elixir And the slack group is really active, you can ask question they are really helpful people: https://elixir-lang.slack.com Use a lot IEX to try out code, at the end we are only writing functions. Finally I recomend that you challenge yourself by doing some practical work. My friend Richard Kotze has contributed to Open Source ( Whitebread and Eyedrops ) and Leigh Shepperson is creating an amazing Algebra package . In my case I have implemented a TicTacToe in Elixir and ReactJS. Hope this was useful, thanks for reading!", "date": "2016-03-04"},
{"website": "FindMyPast", "title": "Don't mock what you don't own", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/dont-mock-what-you-dont-own/", "abstract": "Only mock types that you own Currently I’m reading Growing Object-Oriented Software, Guided by Tests , by Steve Freeman and Nat Pryce.\nI can’t recommend this book more, I should have read it long time ago. The book is an amazing explanation on how the authors design and test object-oriented software. One of the Chapters reads Building on Third-Party code : Providing mock implementations of third-party types is of limited use when unit-testing the objects that call them. We find that tests that mock external libraries often need to be complex to get the code into the right state for the functionality we need to exercise.\nThe mess in such tests is telling us that the design isn’t right but, instead of fixing the problem by improving the code, we have to carry the extra complexity in both code and test. A practical example, an http client A few months ago I was writing a class that was using an HttpClient. Something like this: public class HintsClient : IHintsClient { private readonly HttpClient _httpClient ; public HintsClient ( HttpClient httpClient ) { _httpClient = httpClient ; _httpClient . BaseAddress = new Uri ( \"http://some-address\" ); } public IList < HintDto > GetHints ( FormattedFamilyTreeDto inputFamilyTree ) { return _httpClient . PostAsJsonAsync ( \"endpoint-url?rows=3&type=5\" , inputFamilyTree ) . Result . EnsureSuccessStatusCode () . Content . ReadAsAsync < IList < HintDto >>() . Result ; } } I have written this kind of code so many times. This code does the job, using an standard .Net class HttpClient, we post some data to an endpoint and then, if the request is successful, we read the result.\nEverything works as expected, but there is one problem: how do we test this? \nWe could try to create a mock of the http client and define all the required methods: [ Fact ] public void Must_get_hints_for_a_family_fragment () { var inputFamilyTree = new FormattedFamilyTreeDto (); var response = new List < HintDto > { new HintDto (), new HintDto () }; var responseMessage = new HttpResponseMessage { Content = new StringContent ( JArray . FromObject ( response ). ToString ()), }; responseMessage . Content . Headers . ContentType = new MediaTypeHeaderValue ( \"application/json\" ); var httpClientFake = Substitute . For < HttpClient >(); httpClientFake . PostAsJsonAsync ( Arg . Any < string >(), inputFamilyTree ) . Returns ( Task < HttpResponseMessage >. Factory . StartNew (() => responseMessage )); var hints = Sut . GetHints ( inputFamilyTree ); hints . Count . Should (). Be ( 2 ); } We are creating a fake response message which is returned as a result of an asynchronous task which is the returned value of the post asynchronous operation.\nHere we are using the NSubstitute library for mocking. I have heard so many times: Well, this class uses an http client, it has to be hard to test , or, this class doesn’t need to be unit tested, it is just using an http client. I see tests as a good friend that tell you the good things and also the bad, things that no one else would tell you but a true friend.\nIn this case, the test got very hard to write, it is telling us that probably there is something wrong with our design. Adapter pattern to rescue Adapter was described in GoF design patterns : The adapter pattern is used to provide a link between two otherwise incompatible types by wrapping the “adaptee” with a class that supports the interface required by the client. The idea is that we create an adapter object that interfaces the world outside of our application.\nWe keep this in a thin layer, as thin as possible, so that we minimize the amount of potentially brittle and hard-to-test-code.\nAs stated in the Growing object-oriented design book, this layer define the relationship between our application and the rest of the world in our application’s terms and discourages low-level technical concepts from leaking into the application domain model. Redesigning our client class Let’s create an adapter for the .Net http client: public interface IHttpClientAdapter { Task < Response > Post < TType >( string uri , TType content ); } Notice how we have simplified the returned type and how we have hidden the internal details of the http client, including the asynchronous behaviour.\nNow in the test we can mock this adapter interface: [ Fact ] public void Must_get_hints_for_a_family_fragment () { var inputFamilyTree = new FormattedFamilyTreeDto (); var fakeResponse = new List < HintDto > { new HintDto (), new HintDto () }; var httpClientFake = Substitute . For < IHttpClientAdapter >(); httpClientFake . Post < FormattedFamilyTreeDto >( Arg . Any < string >(), inputFamilyTree ) . Returns ( Task < Response >. FromResult ( fakeResponse )); var hints = Sut . GetHints ( inputFamilyTree ); hints . Count . Should (). Be ( 2 ); } Finally the implementation of our client class looks like this after the refactoring: public class HintsClient : IHintsClient { private readonly IHttpClientAdapter _httpClientAdapter ; public HintsClient ( IHttpClientAdapter httpClientAdapter ) { _httpClientAdapter = httpClientAdapter ; } public IList < HintDto > GetHints ( FormattedFamilyTreeDto inputFamilyTree ) { var response = await _httpClientAdapter . Post ( \"endpoint-url?rows=3&type=5\" , inputFamilyTree ); return response . Hints ; } } Creating our adapter layer has also led to a nice side effect: we have defined new types that define abstractions to the object coming from the outside world.\nWe have avoided one common anti-pattern, Primitives obsession .\nIn other words, in order to be able to test our domain logic, we have created an interface which in terms has led to a better design. Hopefully if we want to change the internal implementation of the client to a different technology, our domain logic and tests won’t have to change. Wrapping up, Listen to the Tests: I’d like to leave you with yet another great piece in the book, Listen to the Tests : Our experience is that, when code is difficult to test, the most likely cause is that our design needs improving. The same structure that makes the code difficult to test now will make it difficult to change in the future. By the time that future comes around, a change will be more difficult still because we’ll have forgotten what we were thinking when we wrote the code.\nOur response is to regard the process of writing tests as a valuable early warning of potential maintenance problems and to use those hints to fix a problem while it’s still fresh.", "date": "2016-04-19"},
{"website": "FindMyPast", "title": "Don't fail to fail", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/dont-fail-to-fail/", "abstract": "(The numbers provided in this report have been made up for this post and they don’t represent the reality.) I reckon, I have failed. So many times, but lately I have massively failed. Let me give you some context: we have just released an API, we have our first customers just a few weeks ago.\nDue to a number of reasons one day I looked into the revenue that API was giving. Reporting and analytics are not my areas, but why not?\nEasy peasy, I went to the database and one minute and a few sql statements later, I had the number. ” £10k” I shouted. \nMy product manager was so happy: “This is amazing Juan,  £10k, in a couple of days.” The day after these events my Product Manager came to my place: “Juan, Analytics came back to me with the number, your amount was wrong, we have made only 2GBP”. I was in shock, “let me check what happened” .\nI went back to my sql scripts and checked with my colleagues and there we go… I hadn’t filtered properly the revenue. Apparently I had to filter by a trans_status_key = 20 Some more days passed and we had a company AllHands, one of the big managers talked about the API, she gave the number: “Oblivion team has done an amazing work, we have already a revenue of £10k”. I felt like crawling into a hole and hiding. I colleague came after that: “Juan, don’t worry, you need to remember that we need to celebrate failures” . So that reminded me to that entrepreneur that was giving an interview about his successful business, he was running two very profitable companies. Someone asked how he managed to do that.\nHe answered: “I haven’t told you about the other 5 failed companies I have tried to run” .\nFailing is part of the learning process, we make mistakes, we learn from them. If you never fail, does it mean that you are not learning? I enjoy making mistakes so much. One day I will collate a list of my biggest mistakes so that I can always remember them and improve.\nWhat have your biggest mistakes been so far?", "date": "2016-04-14"},
{"website": "FindMyPast", "title": "Unit testing Legacy code", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/unit-testing-legacy-code/", "abstract": "Let’s get to the point. I have this nice little piece of legacy code: public static void Save ( object key , object value , System . DateTime ? expiry , string cookieValue , bool useRequestCookieIfNew , ICookieDomainResolver domainResolver = null ) { if (! string . IsNullOrEmpty ( key . ToString ())) { var cookie = HttpContext . Current . Response . CookiesSafe ( cookieValue ); if ( cookie == null || string . IsNullOrEmpty ( cookie . Value )) { if ( useRequestCookieIfNew ) { cookie = HttpContext . Current . Request . CookiesSafe ( cookieValue ); } } if ( cookie == null ) { cookie = new HttpCookie ( cookieValue ); } cookie . Values . Set ( key . ToString (), value . ToString ()); cookie . Domain = domainResolver == null ? HttpContext . Current . Request . ServerVariables [ \"HTTP_HOST\" ]. StripSubDomains () : domainResolver . GetDomain (); if ( expiry . HasValue ) { cookie . Expires = expiry . Value ; } HttpContext . Current . Response . Cookies . Set ( cookie ); } } Here we are setting an Http Cookie, possibly considering a bunch of parameters, including setting an expiry date if provided. We have also this function: public static HttpCookie Load ( string cookieName ) { var cookie = HttpContext . Current . Response . CookiesSafe ( cookieName ); if ( cookie == null || string . IsNullOrEmpty ( cookie . Value )) { cookie = HttpContext . Current . Request . CookiesSafe ( cookieName ); if ( cookie == null || string . IsNullOrEmpty ( cookie . Value )) { return new HttpCookie ( cookieName ) { Domain = ( HttpContext . Current . Request . ServerVariables [ \"HTTP_HOST\" ] ?? \"\" ). StripSubDomains () }; } } return cookie ; } In this case we get the cookie given the name of it. Not the worst code, yet not beautiful code, easy to follow and not many responsibilities going on. There is a problem though: there aren’t any unit tests covering it.\nSome people may argue that it isn’t a problem. For me, it is. I don’t feel like I want to touch that code. It is in a shared library, if I fuck it up, I will probably make a mess, breaking some other stuff.\nTDD is about workflow, Unit testing is about confidence. If I don’t have any unit tests around it, my confidence is, zero? We all have had to change some code like this. In my case I have to provide a parameter so that when creating the cookie, it can be created as an httponly or not . I’m planning to pass an optional parameter, isHttpOnly , and then doing something like: cookie . HttpOnly = isHttpOnly Simple code, only an assigment. I’d really like to unit test my change, but this code is difficult to unit test.\nThe big deal is this line: var cookie = HttpContext . Current . Response . CookiesSafe ( cookieValue ); And this: HttpContext . Current . Response . Cookies . Set ( cookie ); Probably I would have been a lazy guy and chunk my code in it. This time around it is different, I am doing Pair programming with Mahmut .\nHe acts as an angel on my shoulder, as a good friend telling me the things I should do, even if those things are hard to hear: even if my code if simple, I want to have confidence in my change.\nMost importantly I want to apply the Boy Scout rule . Next time someone else lands in this code she will find some unit tests so that the campground will get cleaner and cleaner. Ok, let’s try to unit test this code. How can I solve the problem of having the static HttpContext? How can I unit test it? \nWe could create an addapter for the HttpContext and mock it in our unit tests.\nIt isn’t easy in this case since the whole class is static: public static class SimpleCookieHandler {} We can’t inject an addapter through the constructor. We shouldn’t either do method injection, that would mean introducing some breaking changes, this is a nuget package used in god knows how many places. Ok, I’ll figure this out. Let’s write a unit test: public class When_saving_cookie_with_httpOnly_true { Because of = () => { SimpleCookieHandler . Save ( \"test\" , \"\" , _cookieName , true , _httpContextProvider ); }; It should_be_httpOnly = () => { HttpCookie cookie = SimpleCookieHandler . Load ( _cookieName , _httpContextProvider ); cookie . HttpOnly . ShouldBeTrue (); }; Establish context = () => { _cookieName = \"cookie-cookie\" ; var response = MockRepository . GenerateMock < HttpResponseBase >(); response . Stub ( r => r . Cookies ). Return ( new HttpCookieCollection () { new HttpCookie ( _cookieName ) { HttpOnly = true } }); var request = MockRepository . GenerateMock < HttpRequestBase >(); request . Stub ( r => r . Cookies ). Return ( new HttpCookieCollection ()); request . Stub ( r => r . ServerVariables ). Return ( new NameValueCollection ()); var fakeHttpContext = MockRepository . GenerateMock < HttpContextBase >(); fakeHttpContext . Stub ( h => h . Response ). Return ( response ); fakeHttpContext . Stub ( h => h . Request ). Return ( request ); _httpContextProvider = () => fakeHttpContext ; }; static string _cookieName ; private static Func < HttpContextBase > _httpContextProvider ; } This is the important bit: SimpleCookieHandler . Save ( \"test\" , \"\" , _cookieName , true , _httpContextProvider ); /.../ HttpCookie cookie = SimpleCookieHandler . Load ( _cookieName , _httpContextProvider ); /.../ private static Func < HttpContextBase > _httpContextProvider = () => fakeHttpContext ; Wait a second, you told me we couldn’t inject anything to avoid breaking changes.\nThat’s right, but let’s go to the method: internal static void Save ( object key , object value , string cookieName , bool httpOnly , Func < HttpContextBase > httpContextProvider , ICookieDomainResolver domainResolver = null ) {} I see… you have changed the existing method from public to internal, what about the public method? Here you have it, in the same class: public static void Save ( object key , object value , string cookieName , bool httpOnly , ICookieDomainResolver domainResolver = null ) { Save ( key , value , cookieName , httpOnly , () => new HttpContextWrapper ( HttpContext . Current ), domainResolver ); } So if you see the public method calls the new internal method that holds all the legacy code. The public method sends an anonymous function providing the actual HttpContext.\nIn our test we pass an anonymous function providing the fake http context: SimpleCookieHandler . Save ( \"test\" , \"\" , _cookieName , true , () => MockRepository . GenerateMock < HttpContextBase >()); Now we see some tests around the legacy code. I have some confidence in my code and the next person touching this code will find some ground where she could write her tests. Thanks Mahmut for pushing the quality of our code and pushing me towards becoming a better developer.\nNow I can send this to code review without the shame of having to explain why I didn’t write any unit tests.", "date": "2016-05-25"},
{"website": "FindMyPast", "title": "Change one thing", "author": ["Andy Mendelsohn"], "link": "https://tech.findmypast.com/change-one-thing/", "abstract": "This isn’t about code. This is about change. In recent months I’ve been affected by a number of things in both my personal and professional life that have led me to stop for a moment and take stock. I did some reading and some research on ‘Life, the Universe and everything’  and one of the things I found was a TED talk given by a swiss monk called David Steindal-Rast entitled “Want to be happy? Be grateful.” I recommend spending some time watching the video, the link can be found at the end. So, this is not just about code and change, it’s also about being happy and being grateful: about giving gifts and receiving gifts; about opportunities ripe for the taking. In his talk, David Steindal-Rast says happy people are invariably happy because they are grateful as opposed to being grateful because they are happy. I won’t repeat everything he says, but instead let you watch his talk and quote the following: Grateful living, that is the thing. And how can we live gratefully? By experiencing, by becoming aware that every moment is a given moment, as we say. It’s a gift. You haven’t earned it. You haven’t brought it about in any way. You have no way of assuring that there will be another moment given to you, and yet, that’s the most valuable thing that can ever be given to us, this moment, with all the opportunity that it contains. If we didn’t have this present moment, we wouldn’t have any opportunity to do anything or experience anything, and this moment is a gift. It’s a given moment, as we say. Now, we say the gift within this gift is really the opportunity. What you are really grateful for is the opportunity, not the thing that is given to you, because if that thing were somewhere else and you didn’t have the opportunity to enjoy it, to do something with it, you wouldn’t be grateful for it. Opportunity is the gift within every gift, and we have this saying, opportunity knocks only once. Well, think again. Every moment is a new gift, over and over again, and if you miss the opportunity of this moment, another moment is given to us, and another moment. We can avail ourselves of this opportunity, or we can miss it, and if we avail ourselves of the opportunity, it is the key to happiness. Behold the master key to our happiness in our own hands. Moment by moment, we can be grateful for this gift. Let’s think about “ this moment and the opportunity with all the opportunity that it contains.” If every moment is an opportunity then it’s an opportunity to change something (for the better.) It’s an opportunity to change something that affects you, and possibly others (for the better.) And, if you do that, not only will you be making the best of the opportunity, but you will also be giving others a gift. Giving others a gift make others grateful, which, if you agree with David Steindal-Rast, will also make them happy. Stop, look, and then go, and really do something. And what we can do is whatever life offers to you in that present moment. Mostly it’s the opportunity to enjoy, but sometimes it’s something more difficult. Let’s make ourselves and others happy. Let’s change one thing. Let’s make that change once a week, because in a team of 40, if everybody changed one thing, once a week, we would have 40 changes that could make us happy. 40 more gifts, 40 opportunities taken. In a company of 150 people… you do the maths. What can you change? What are you going to do with your opportunity? Anything. But keep it small and doable. Make it something that takes just an hour: a configuration file change that’s been annoying you; a code test that’s flakey; a coffee machine that needs cleaning; a chair that needs fixing; a blog that needs better CSS; a blogpost that could be shared etc… Make one change. Make someone happy. Make yourself happy. So what will you change? https://www.ted.com/talks/david_steindl_rast_want_to_be_happy_be_grateful", "date": "2016-03-17"},
{"website": "FindMyPast", "title": "Learn your tools well", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/learn-your-tools-well/", "abstract": "Learning your tools This time I’d like to talk about tools, the tools and frameworks we use every day. I often relate to the Gardening metaphor when talking about software. A gardener knows his tools, he knows when to use a certain type of trowel not to damage the plant, he knows when using a bigger type of spade so that his work will be more efficent.\nFurthermore a professional gardener knows how to take advantage of a given technique, one that may not be relevant in theory, but in a specific context may be useful. We are professional software engineers, we need to use the right tools, we even need to look into other tools in the market and then use a similar one that we have in our storage. We tend to make one mistake though: we get trapped into our own habits. Habits are good, habits are the realization that we have come to learn something that deeply that we don’t even think on doing that.\nThe trap here is that we may follow certain habits when they are not applicable or even when there is a better pattern to apply in that context. One Example is worth a Thousand words Test cases in your testing framework: Recently I was doing the FizzBuzz coding kata. In this kata we need to print a sequence of numbers but transforming some of them into “Fizz”, “Buzz” or “FizzBuzz”, given certain rules. My tests were as follows: public class FizzBuzzTest { [ Fact ] public void FizzBuzz_for_two_elements_outputs_two_numbers () { var fizzBuzz = new FizzBuzz . FizzBuzz (); string fizzBuzzSequence = fizzBuzz . Get ( 2 ); Assert . Equal ( \"1 2\" , fizzBuzzSequence ); } [ Fact ] public void FizzBuzz_for_three_elements_includes_fizz () { var fizzBuzz = new FizzBuzz . FizzBuzz (); string fizzBuzzSequence = fizzBuzz . Get ( 3 ); Assert . True ( fizzBuzzSequence . Contains ( \"Fizz\" )); } } I wanted to be explicit so that people could read my tests as documentation. While that is correct, imagine following this pattern in a larger and more complex solution.\nTests become overcomplex, we need to maintain more lines of code. In this scenario I have a single function that transforms the data: I send an input sequence and I get back an output sequence.\nIt seems the perfect match to use TestCases. I’m using XUnit test framework so that I did some research and found about InlineData, then I changed my tests like this: [ Theory ] [ InlineData ( \"1\" , 1 )] [ InlineData ( \"1 2\" , 2 )] [ InlineData ( \"1 2 Fizz\" , 3 )] [ InlineData ( \"1 2 Fizz 4\" , 4 )] [ InlineData ( \"1 2 Fizz 4 Buzz\" , 5 )] [ InlineData ( \"1 2 Fizz 4 Buzz Fizz\" , 6 )] public void FizzBuzz_sequence_for_given_number_of_elements ( string expectedFizzBuzz , int numberOfElements ) { var fizzBuzz = new FizzBuzz . FizzBuzz (); string fizzBuzzSequence = fizzBuzz . Get ( numberOfElements ); Assert . Equal ( expectedFizzBuzz , fizzBuzzSequence ); } Tests have become more concise and simple, yet they are explicit. Creating tools, test cases in Mocha. A trully proficient gardener may find that in a given context, none of the tools do a good job. \nWhat they do in that case is that they  apply patterns that they know from other contexts, even creating new tools, to solve the new problem. We, as proffesional software engineers, should learn more than one language. Even if you are a Ruby developer, by learning different languages, different paradigms, you will see other ways , you will find different solutions to similar problems.\nI ensure, you will come back to your regular environment to find out that you have become a more efficient profesional developer. Imagine our friend the gardener using always the same shovel, no matter what the context is… Probably he won’t be hired that often. A few days after doing the kata, we are doing some work in Javascript. Again in our scenario, test cases are the right approach. We use Mocha as our testing framework so we did some research and found out that Mocha doesn’t support Test cases.\nIt didn’t matter, we knew the language and we knew the concepts, so let’s try to implement test cases. Here is a test implementation of the tests above for FizzBuzz in Javascript using test cases: import FizzBuzz from ' ../fizz-buzz ' ; describe ( ' For given number of elements ' , () => { const testCases = [ { ' 1 ' , 1 }, { ' 1 2 ' , 2 }, { ' 1 Fizz ' , 3 }, { ' 1 Fizz 4 ' , 4 }, { ' 1 Fizz 4 Buzz ' , 5 }, { ' 1 Fizz 4 Buzz Fizz ' , 6 } ]; testCases . forEach (( testCase ) => { const { expectedFizzBuzz , numberOfElements } = testCase ; it ( ' generates FizzBuzz sequence ' , () => { let fizzBuzzSequence = FizzBuzz . get ( numberOfElements ); fizzBuzzSequence . should . equal ( expectedFizzBuzz ); }); }); }); Even though Mocha doesn’t have an in-build functionality for TestCases, we have learnt our tools well, in this case Javascript ES2015. Learn your tools well to reduce work. Thanks for reading!", "date": "2016-06-01"},
{"website": "FindMyPast", "title": "MapSet in Elixir", "author": ["Mahmut Surekci"], "link": "https://tech.findmypast.com/elixir-mapsets/", "abstract": "While writing some koans for MapSet I discovered some new information about it and thought I would share some of them here. MapSet is another type of collection like lists and maps. What makes it unique is it doesn’t allow duplication. If we did: set = MapSet . new ([ 1 , 1 , 1 , 2 ]) MapSet . size ( set ) The result is 2. The element 1 will only appear in there once and no more. Ordered or unordered? Another interesting thing about MapSets is that it is unordered which is true to a certain extent. MapSets are actually Hash array mapped trie . To a certain point\nit is ordered. In the case of Elixir MapSet it is up to 32 elements. Example in IEx: 1 .. 10 |> MapSet . new |> Enum . fetch ( 0 ) { :ok , 1 } 1 .. 33 |> MapSet . new |> Enum . fetch ( 0 ) { :ok , 11 } Performance MapSet are really fast and is really useful when searching for something. MapSets can check existence of an element in O(log(n)) time. Here are some stats: MapSet evaluation . Note that HashSet is deprecated in the later versions of Elixir You may want to consider using a MapSet rather than a List when searching through a collection depending on your use case.", "date": "2016-06-02"},
{"website": "FindMyPast", "title": "Sitemaps as a Microservice", "author": ["Liam Humphreys"], "link": "https://tech.findmypast.com/sitemaps-as-a-microservice/", "abstract": "Table of Contents What are sitemaps? Why do we want sitemaps? What is a microservice? Why do we want sitemaps as a microservice? Great, show us the way, O’ Benevolent voice on the internet! Sitemaps as a Microservice Oh hi there, sorry, I didn’t see you behind all my beautiful sitemaps! Well since you’re here let’s have a chat about sitemaps. What are sitemaps? Shamelessly torn from Wikipedia’s grubby sitemaps page : “A site map (or sitemap) is a list of pages of a web site accessible to crawlers or users. It can be either a document in any form used as a planning tool for Web design, or a Web page that lists the pages on a Web site, typically organized in hierarchical fashion.” If the block quote scared you and you zoned out trying to calculate the most apparently reasonable time to go for your 27th coffee break of the day then I’ll shamelessly tear the simple definition from my simple brain: “It is literally a map of the website, i.e. These are the pages on our site and how to get there.” Sitemaps come in a variety of flavours and serve various purposes, we’ll be discussing sitemaps for SEO purposes (that means sitemaps for lifeless robot eyes, not beady human eyes). The XML format is best suited to our needs as it allows us to set additional properties on each URL, detailed later. XML you say? I know. I tried to find out why XML is supported and a more lightweight language is not but there appears to be no rhyme or reason. XML is semantically rich, and elements can contain mixed content, and there are probably other benefits of using XML over something simple like JSON but none of these attributes seem to be taken advantage of in the sitemap protocol! Oh well, best get in line. For the non-believers amongst you here is a real live (example, not real or live) sitemap with annotations by yours truly on the enigmatically named properties: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- Using the industry standard XML namespace for sitemap definition, we define a urlset (read as an array or collection of urls), this is our sitemap. --> <urlset xmlns= \"http://www.sitemaps.org/schemas/sitemap/0.9\" > <!-- Within the urlset we define the urls on our site --> <url> <!-- REQUIRED: set the loc to be the fully qualified URL of the page in question. Must be less than 2048 characters. --> <loc> http://www.tinypuppies.co.uk/pup-of-the-week </loc> <!-- OPTIONAL: set the lastmod on links to signify the last time this page was modified. The date should be in W3C Datetime format. *Some* crawlers / search engines will use this property to determine if your page shows up in results when the searcher has time filters set, e.g. only show results from within the last week. --> <lastmod> 2016-01-23 </lastmod> <!-- OPTIONAL: set the changefreq on links to let crawlers know how often this content is likely to change, this will affect how often *some* crawlers revisit the page. Changefreq can be always, hourly, daily, weekly, monthly, yearly, and never --> <changefreq> weekly </changefreq> <!-- OPTIONAL: set the priority on links to let crawlers know how important this page is in relation to the rest of your site. This will affect how *some* search engines prioritise your pages in results, but only against your own pages. Priority is from 0(low) to 1(high), the default being 0.5 --> <priority> 0.7 </priority> </url> <url> <loc> http://www.tinypuppies.co.uk/breeds </loc> <lastmod> 2016-01-01 </lastmod> <changefreq> monthly </changefreq> <priority> 0.5 </priority> </url> </urlset> URLs detailed in an XML sitemap can be on the same domain or different subdomains and domains. Each XML sitemap file must be UTF-8 encoded, is limited to containing 50000 URLs, and must be under 10MB in size. XML sitemaps can be compressed using gzip. If your sitemap is exceeding the size limits, it’s time to invest in a sitemapindex. In the same way an XML sitemap lists URLs, a sitemapindex lists XML sitemaps. If your mind is wonderfully advanced and you’re still reading this rather than ctrl-tabbing to facebook to see if that special someone has liked your photo yet then you can probably see beautiful sitemap hierarchies stretching off into the sunset, well stop that. This is the real world where nobody gets to be beautiful up close (yes, I am hinting at your photo). A sitemapindex can NOT contain links to other sitemapindex files. You can submit multiple sitemapindex files to search engines but never the twain shall meet. Here’s an example of a sitemapindex file containing two XML sitemaps (no annotations required,  I’m not holding your hand through all of this and besides, it’s not rocket science): <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sitemapindex xmlns= \"http://www.sitemaps.org/schemas/sitemap/0.9\" > <sitemap> <loc> http://www.tinypuppies.co.uk/sitemap-breeds-a1.xml </loc> <lastmod> 2016-01-01 </lastmod> </sitemap> <sitemap> <loc> http://www.tinypuppies.co.uk/sitemap-breeds-a2.xml </loc> <lastmod> 2016-01-05 </lastmod> </sitemap> </sitemapindex> Search engine crawlers and the occasional strange user will look for the sitemap on your site at http://www.yoursite.com/sitemap.xml as is the convention, but nothing stops you from storing your sitemaps in a different location and submitting them to search engines manually or programmatically. As we stumble to the end of this section and the start of your next coffee break which you can hopefully drag out to lunch or home time, here are some common XML sitemap myths debunked: “Including a URL in the XML sitemap guarantees it will be indexed.” No. It’s important to note that XML sitemaps are only recommendations. The XML sitemap will not guarantee indexation of the URLs included. “If I leave a URL out of the XML sitemap it will get deindexed.” No. The XML sitemap will not exclude indexation of URLs not included on the XML sitemap. It’s merely a set of recommended URLs that, if the recommendations agree with the signals the rest of the site is sending, will lend a bit of extra importance to the URLs included above and beyond the other URLs on the site. “XML sitemaps are difficult to create and maintain.” No. In the simplest cases, small sites can easily create and post their own XML sitemaps manually using the examples above as formatting guides. For larger sites and sites that change more frequently, plugins or modules available for most ecommerce platforms can automate the creation and posting of XML sitemaps. “Posting an XML sitemap is like asking to get scraped and spammed.” No. An XML sitemap is nothing more than a list of URLs. Scrapers and spammers can easily crawl any public site they wish to generate a list of URLs and content from which to steal a site’s content for their own nefarious purposes. They certainly don’t need an XML sitemap to do it, and not posting an XML sitemap won’t keep the scrapers and spammers away. Why do we want sitemaps? Sitemaps are the cure for world hunger, and they are your best friend. Sitemaps like to give you money for nothing, and they always help you move house. Almost none of that is true, it was just a clever ploy to get you interested again. Using sitemaps has many benefits, if only there was a sensible way of listing points along with brief descriptions.. I know - I’ll use a table! Benefit Description Content Modification It’s easier to maintain higher ranks in search engines if you keep modifying content on your site; keeping it fresh and useful to the needs of your visitors. Effective use of sitemaps means Google will be alerted whenever your site’s content is modified. Efficient Crawling All content on a site should be crawled, which can take a long time. If it takes too long, crawlers are likely to give up. With a trusty sitemap in their lifeless robot hands however, the website can be efficiently and effectively crawled, which leads to better indexing of your site. Content Prioritisation Sitemaps let you prioritise your pages. This means that the pages carrying your most important content will be crawled and indexed faster than those with a less priority value. Get Discovered The main reason you invest time and money creating new content for your site is that you expect to be found on the web. Using sitemaps means your new content will be discovered by search engines a lot faster. Highly recommended for new websites or web pages and other pieces of content. Free service The best things in life are free! Except for spam mail, cut that out. Yes, submitting your sitemap to search engines doesn’t cost a penny, it only costs a poor engineering schmuck some of his greasy time, no loss there just do it! Learn About Your Visitors You can learn a lot by monitoring your sitemap reports on the various search engine webmaster tools. Errors will be displayed so that you can fix, traffic sources and even keyword searches. Using this information can help you improve your content and attract more attractive traffic which is important because nobody wants ugly traffic. What is a microservice? In the real world your face experiences, a service is a system that supplies utilities or commodities as required. For example, a telephone service supplies customers with the telephone communications utility (most of the time). Another example is a bus service supplying the dirty general public with the transportation utility. In terms of software engineering, the meaning of “service” doesn’t actually change! We add the word “Micro” because ideally these services should be small, simple, and be concerned with a single high level responsibility. Look at us, learning stuff that we already knew but wrapped in a shiny new context, how admirable. A software microservice example is a DateTime microservice. If you want today’s date, you ask the DateTime microservice, easy. One thing to keep in mind when designing, thinking about, implementing, or even smelling software microservices is the responsibility of the microservice. The microservice should be built for one high level purpose, and the interface to the microservice should reflect that purpose in a cohesive manner. For example, the DateTime microservice could have functions for getting tomorrow’s date, and getting the date in different formats. The DateTime microservice should not have functions for converting currency. The cohesive microservice principle still applies to real world services. Using the previous examples, you wouldn’t expect your phone to only work if you have bought a bus ticket. Hopefully that wraps up microservices. If you want to read further a simple google search will not be beyond your technical grasp, and should return results such as Martin Fowler’s definition of Microservice Architecture Why do we want sitemaps as a microservice? If you’re as swayable as a drunk toddler and are ready to implement all the things as microservices already then you can probably skip this section . Rather than answer the question with sitemaps specifically on the chopping board, I think it’s more useful to answer the question for any potential microservice. This way when a terrifying new microservice is proposed, you will be proud and fearless rather than shaking in your booties. To answer “Why microservices?” I feel you would be better served by looking up one of the many, many articles already arguing for or against the architecture, such as SmartBear’s What Are Microservices . In my own simple words Microservice Architecture means you can develop and deploy individual components without worrying about the larger system. Fixing a bug in the billing microservice means deploying the billing module, not everything else. Implementing a Sitemap microservice means all our products don’t need individual implementations of sitemap generation and submission. They simply make use of the utility supplied by the sitemap microservice. This means less code duplication, and more consistency in our public-facing websites. Great, show us the way, O’ Benevolent voice on the internet! Ok, on to the delicious meaty reason of this great wall of text. How I would go about implementing the sitemaps microservice.\nAt this point we aren’t too clear on the inputs to the microservice but we know the desired outputs: sitemaps! Given this scenario, “I am an FMP website, and I need a sitemap for all my lovely content pages.” What does the sitemap microservice need? It needs to know the common structure of the URLs to be built, a template if you will, e.g. http://www.fmp4life.com/inspect/transcriptions/{transcription_id} , and it needs to either be given, or be told how to get all the data necessary to build all the variations of that URL. Let’s briefly explore the implementation of a sitemap microservice which accepts the page template and a link to or an identifier for a data source. This design would mean the sitemap microservice has to trawl through the data itself to gather the page names, and the other properties. This would mean less complexity in the requesting sites but it would couple the sitemap microservice to the various data sources. An example of why this is bad is because when a new site is built it can’t use the sitemap microservice until work has been done to register the new data source with the sitemap microservice. It also means if any of the data sources move or change structure, work will need to be done in the requesting sites as well as work in the sitemap microservice. Let’s explore the implementation of a sitemap microservice which accepts the page template and all the associated data with an example.\nA full example of a sitemap request being given all the data necessary to build all variations of the URLs would be: GET: http://sitemap-service.dun.fh/sitemap BODY: { \" url_sets \" : [ { \" template \" : \" http://www.findmypast.co.uk/surnames/{surname} \" , \" pages \" : [ { \" url_part \" : \" aardvark \" , \" lastmod \" : \" 2004-12-23 \" , \" changefreq \" : \" weekly \" }, { \" url_part \" : \" aastra \" }, { \" url_part \" : \" behemoth \" , \" priority \" : \" 0.7 \" } ] }, { \" template \" : \" http://www.findmypast.co.uk/transcriptions/{transcription_id} \" , \" pages \" : [ { \" url_part \" : \" gbprs_2_india_mars \" , \" priority \" : \" 0.7 \" }, { \" url_part \" : \" prs_all_detail_209_3_d_nwkfhs_47 \" , \" lastmod \" : \" 2004-10-19 \" }, { \" url_part \" : \" tna_ccc_2c_multi \" , \" changefreq \" : \" yearly \" , \" lastmod \" : \" 2003-11-09 \" } ] } ] } RESPONSE: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <urlset xmlns= \"http://www.sitemaps.org/schemas/sitemap/0.9\" > <url> <loc> http://www.findmypast.co.uk/surnames/aardvark </loc> <lastmod> 2004-12-23 </lastmod> <changefreq> weekly </changefreq> <priority> 0.5 </priority> </url> <url> <loc> http://www.findmypast.co.uk/surnames/aastra </loc> <lastmod> 2004-12-23 </lastmod> <priority> 0.5 </priority> </url> <url> <loc> http://www.findmypast.co.uk/surnames/behemoth </loc> <lastmod> 2004-12-23 </lastmod> <priority> 0.7 </priority> </url> <url> <loc> http://www.findmypast.co.uk/transcriptions/gbprs_2_india_mars </loc> <lastmod> 2004-12-23 </lastmod> <priority> 0.7 </priority> </url> <url> <loc> http://www.findmypast.co.uk/transcriptions/prs_all_detail_209_3_d_nwkfhs_47 </loc> <lastmod> 2004-10-19 </lastmod> <priority> 0.5 </priority> </url> <url> <loc> http://www.findmypast.co.uk/transcriptions/tna_ccc_2c_multi </loc> <lastmod> 2003-11-09 </lastmod> <priority> 0.5 </priority> <changefreq> yearly </changefreq> </url> </urlset> Pro’s of this design Clear single high level responsibility for the sitemap microservice, it is only concerned with building sitemaps, it doesn’t care or know about where the data is originally from. Individual properties are settable for each page. Note the optional page properties being optional. No tight coupling to other services or data sources, this means any site can make use of this microservice with no extra work required. Con’s of this design A lot of the complexity (gathering all the page names, and their details like last modified, and change frequency) is pushed to the requesting site, complexity which is likely to be quite expensive depending on the data source. The con listed above I don’t see as an issue, this is probably the best place for that complexity as otherwise the sitemap microservice will need to know far too much about all sorts of data sources. Caching can be put in place to save on the expensive operations, or perhaps only generate the sitemap once a day. The winner is the “Give the Sitemap Microservice all the things!” approach. Using this design we should end up with a highly cohesive (you won’t need a bus ticket to request a sitemap), easily testable (due to simple functional design), fast (no external dependencies, simple operations, caching) sitemap microservice. Thanks for reading, or at least gazing zombie-like at the harsh screen with the occasional flick of the mouse wheel to make it look like you’re soaking it in.", "date": "2016-06-15"},
{"website": "FindMyPast", "title": "Deployment and Infrastructure at Findmypast", "author": ["Oleksandr Stasyk"], "link": "https://tech.findmypast.com/deployment-and-infrastructure-at-find-my-past/", "abstract": "In this talk Sash (Oleksandr Stasyk) explains what continuous deployments mean and how we do it in Findmypast.\nA walkthrough about Docker, Consul, Puppet and how we put all of them together.", "date": "2016-06-28"},
{"website": "FindMyPast", "title": "Achieving Continuous Delivery of Microservices", "author": ["Jae Bach Hardie"], "link": "https://tech.findmypast.com/achieving-continuous-delivery-of-microservices/", "abstract": "How do we achieve continuous delivery? To quote Jez Humble : Continuous Delivery is the ability to get changes of all types—including new features, configuration changes, bug fixes and experiments—into production, or into the hands of users, safely and quickly in a sustainable way. The ultimate goal of continuous delivery is to minimise the iteration time of the code-test-deliver-measure experimentation cycle. Increasing deliverable throughput in this way is the key to not only more feature work being delivered but higher quality code as well. This might seem counter-intuitive at first but code is fixed and polished through that same cycle and less time spent on deployment is more time spent on designing quality code. You can read Jez Humble’s website and/or book for the details and data. Having decided that continuous delivery is an ideal we want to pursue we had to decide how. The high-level requirements were: Software must be easily testable, which means it must be loosely coupled. Delivery must—under normal circumstances—require minimal human interaction. Delivery—from commit to production—must be fast. Preferably under 10 minutes. Rolling back a deployed feature if it is found to be broken or unwanted must be trivial. While it is not the only possible solution after deliberation we settled on a microservices architecture . Microservices enforce loose coupling, plus it’s easier to develop fast and reliable deployment pipelines if they only have to handle small packages. However, microservices introduced a new problem: if adding a feature was often going to require adding a new, independently deployed and hosted service then that process had to be fast and not require any specialist knowledge. Demanding that every single developer in the company learn the intricacies of maintaining Puppet configuration for all their services would have been impractical and more than a little cruel. We set ourselves the goal that feature teams should be able to set up a new service in under four hours, which meant: Developing services should not require knowledge of the infrastructure and changing infrastructure should not require detailed knowledge of the services running on it. If we need to change the hostname or port a service runs on it should require no changes to the service itself. All project configuration—from build process to health monitoring—must be contained within the project repository. Anything else introduces hidden dependencies for deployment that threaten to break the pipeline and require specialist knowledge to debug. The above configuration should be declarative and not require adding dependencies to the project. We didn’t want our Elixir or .NET projects to have to include a full npm configuration just to use gulp to run their build steps. We haven’t entirely reached these lofty goals but we’re much—very much—closer than we used to be thanks to containerisation, service discovery and a couple of bespoke tools. So let me introduce you to our deployment pipeline. Containerisation Spanish philosopher José Ortega y Gasset famously wrote “I am myself and my circumstances” to express that no life can be separated from the context it occurs in. In much the same way the source code of a program cannot fully describe the function of that program without the context it will be compiled and run in. Most unexpected behaviour during deployment comes from build environments being different than expected. To make deployment repeatable, we need to make a program’s context repeatable. That’s where Docker comes in. Docker essentially allows you to specify a “source code” for a program’s context that can then be “compiled” to an image and run as a container—the details are fascinating but I won’t go into them here. This means that once we have tested an image we can have high confidence that it will perform equally well in every environment it is deployed to. Additionally, Docker (through the compose utility) allows you to specify deploy configurations made up of multiple containers all linked in a private network and DNS that allows services that depend strongly on each other to be deployed and scaled together. For example, our visualization configuration storage consists of a thin API over CouchDB. The in-house API is deployed linked to the official CouchDB Docker image from Docker Hub which is accessed in the API code simply as db:5349 . Service discovery To be fully context-agnostic, deployment should be able to happen to any host on the network on whatever port the host happens to have free. This presents a challenge: how do services link up when their network locations are fluid? You need a reverse proxy (we use nginx ) and a way to keep its configuration up to date in a changing service landscape. We use HashiCorp’s Consul to store and monitor service state. Each host has a consul instance that receives information about the containers running on it from registrator , grouping them into services and tags. We currently use tags to indicate environment (integration vs production) and colour (blue vs green). Fig 1. The full service discovery infrastructure HashiCorp provides a templating system with an application that monitors consul for changes which we use to auto-generate the reverse proxy configuration and reload it when it changes. This system is extended with flags in consul’s versatile key-value store where we store—for example—aliases. With this setup blue/green deployments become a simple question of sending an HTTP PUT flipping the production alias from green-production to blue-production once all checks pass. What a deployment looks like: Build Docker image. Test that image in isolation. Push that image to the in-house image registry. Pull all images you need to deploy linked. Deploy them to a test environment. Run automated tests against the container system. Upload service configuration to Consul API (if changed). Deploy the containers to all hosts, tagged with the offline colour. Wait until they are all responding and passing automated checks. Flip environment alias to point at the offline colour. The new build is now online. What service developers need to know: The hosts they have resources assigned in. For now. Eventually we plan to move to a docker cluster where resources are allocated and monitored automatically so teams won’t even need to know that. The proxy URI’s of all services their service depends on. These are all available with descriptions in an auto-generated service catalogue and they do not change. That’s not much, compared to needing to know the IP address, port, and environment of not only the service you’re deploying but all services you’re going to need. Developer time and attention is an expensive resource and everyone is happier when it is focused on developing new features, not orchestrating dependencies on remote hosts.", "date": "2016-07-08"},
{"website": "FindMyPast", "title": "Welcome to FMP Tech!", "author": ["Andy Mendelsohn"], "link": "https://tech.findmypast.com/welcome-to-fmp-tech/", "abstract": "This (the blog, not specifically this post) is about what we do and how we do it. It’s not glossy or shiny and isn’t meant as a sales job, but it might help inform you about some of the technology we use, and maybe give us an opportunity to bang our own drum about some of the good stuff we’re doing: Like getting our tests to stop slowing us down (1hr 30 down to 10 mins) - strangling monoliths - component-driving our front-end - etc Who are we? The Findmypast Digital team (engineering, design & product) are approximately 60 people based in London, Dundee, Provo(UT - USA) and Boulder(CO - USA). We support, maintain and develop software and solutions for one of the largest family history organisations in the world. And our on-going challenge is to build and deliver the best, to provide a platform for innovation in product and to facilitate improvements in the way we surface our data, much of which is original and unique to our organisation. Like all technology-driven organisations we have more than our fair share of technical debt, and can sometimes feel like we’re at war with a legacy code-base and solutions that no longer fit where we want to be technologically. To help us fight back, we have kicked off a number of exciting projects to re-architect and re-engineer our back-end, front-end and our data repositories (and products). We will also use this blog as a place to talk about what we’re doing in this area and share what we’ve discovered. We also have had an element of legacy in the way we were organised and the processes we used. Now, we really do try to work in an agile way - and we’re not averse to changing things in the way we work in our attempt to constantly improve, but we try to do things consistently and as a team. We use whiteboards, we use Trello, we use Slack and, while we try to avoid tossing more tools into the mix, we are not averse to adding more tech that will help us improve and deliver quality (more often than not, as long as we’re -all- using it and it doesn’t stop us being nimble in the way we work). We talk to each other. We help each other and we try to be nice to each other! If something doesn’t work we try to fix it. Ultimately we believe in code craftsmanship - we want to get better at what we do, and, in turn, provide great experiences and interfaces for our users. Notice all of that ‘we’… Because in doing most of this stuff we try work as a team, and have fun as a team. At the end of the day we create stuff - we solve problems - and we build. We use multiple languages, processes, frameworks, tools, designs and our own skills to achieve our aims. Oh, and we get bored easily. We don’t like spending time doing repetitive tasks. So we’re getting into more and more automation and beginning to explore and implement a range of different technical solutions. And finally, we want to reduce unnecessary pain in our working life (and to those of our colleagues, and to those of our users!) to a minimum because we want to enjoy what we do, oh, and we don’t mind the odd puzzle to solve along the way, and to get that warm fuzzy feeling when we’ve created something cool. Jeremy Hoy,\nCTO, Findmypast", "date": "2015-09-13"},
{"website": "FindMyPast", "title": "Stubbing dependencies in commonJS", "author": ["Richard Kotze"], "link": "https://tech.findmypast.com/stubbing-dependencies-in-commonjs/", "abstract": "Unit testing is a valuable tool and it can be challenging to stub a dependency used by a module under test when applying the commonJS pattern. This is primarily  because it is only accessible to itself ( much like a private field). Enabling the ability to control the response and ensuring data reaches the code being tested are two of the many good reasons to mock and stub particular bits of functionality. An elegant solution I’ve discovered for accessing this is to use a node package called proxyquire . Proxyquire proxies the node ‘require’ function, allowing it to override the dependencies with a stub. In a test file, instead of using ‘require’ or ‘import’ for the module, we use proxyquire to load it in. The second parameter is an object which uses the key as the name/file path of the dependency and the value is the stub. In order to demonstrate proxyquire, the example below has a simple module that has been built to fetch and render a twitter feed. There are a couple dependencies that need to be stubbed, one for fetching the tweets and the other to render them. In the module to test there is some logic to check:have any tweets have been retrieved and an HTML element has been specified to output too. For some side points I will use sinonJS for stubbing the dependencies and shouldJS to assert. Twitter feed module import twitterApi from ' ./twitterApi ' ; import feed from ' ./feed ' ; var twitterFeed = function ( elementId , username ) { if ( elementId ) { let tweets = twitterApi . get ( username , 10 ); if ( tweets . found ) feed . renderTo ( elementId , tweets . feed ); else feed . renderTo ( elementId , username + ' has no tweets ' ); } }; Test Twitter feed import should from ' should ' ; import sinon from ' sinon ' ; import proxyquire from ' proxyquire ' ; describe ( ' Twitter Feed ' , function ( done ) { var twitterApi = { get : sinon . stub () }, feed = { renderTo : sinon . spy () }, twitterFeed = proxyquire ( ' ./twitterFeed ' , { ' ./twitterApi ' : twitterApi , ' ./feed ' : feed }); it ( ' to render tweets ' , function () { //arrange let expectedUsername = ' richardkotze ' , elementId = ' rich-tweets ' , expectedTweets = [{ tweet : ' This is a tweet. ' }]; twitterApi . withArgs ( expectedUsername ). returns ( expectedTweets ); //act twitterFeed ( elementId , expectedUsername ); //assert twitterApi . get . calledWith ( expectedUsername ). should . be . true (); feed . renderTo . calledWith ( elementId , expectedTweets ). should . be . true (); }); it ( ' to render error message ' , function () { //arrange let expectedUsername = ' richardkotze ' , elementId = ' rich-tweets ' , expectedTweets = null , expectedError = expectedUsername + ' has no tweets ' ; twitterApi . withArgs ( expectedUsername ). returns ( expectedTweets ); //act twitterFeed ( elementId , expectedUsername ); //assert twitterApi . get . calledWith ( expectedUsername ). should . be . true (); feed . renderTo . calledWith ( elementId , expectedError ). should . be . true (); }); it ( ' not to render anything ' , function () { //arrange let expectedUsername = ' richardkotze ' , elementId = null ; //act twitterFeed ( elementId , expectedUsername ); //assert twitterApi . get . called . should . be . false (); feed . renderTo . called . should . be . false (); }); }); Being able to access and stub these private variables significantly helps to make testing easier. Proxyquire does an excellent job of achieving this. Another library worth noting is rewire and it too allows you to stub private dependencies.", "date": "2015-10-09"},
{"website": "FindMyPast", "title": "Pair programming - from a Junior's perspective", "author": ["Mahmut Surekci"], "link": "https://tech.findmypast.com/pair-programming-from-a-juniors-perspective/", "abstract": "Pair programming eh? Knowledge sharing with each other, discussing different approaches on how to solve problems. This is the intention but, does it always end up being that way? No. Why not? I think it is some of our idiosyncrasies. Not everyone is compatible for pair programming however, there are do’s and don’ts we can follow. Do’s Time to switch\nIt’s important that the driver (person typing) is not in control for too long otherwise, it can become easy for the observer to lose focus. As a junior, I am always keen to take control too so, taking a backseat for a bit too long can get tiring. What the hell are you thinking?\nExplain your thoughts. It is not easy to tell especially if you have come up with a bright idea and it hasn’t hit me yet (almost always the case). It helps the observer follow what you are doing and not left trying to figure it out. It’s not good to do this after you’ve implemented your bright idea either because by the time you do explain it I’ve already tired myself out so much trying to figure out what you were trying to do. Now that you say it I have to process that new thought and it gets exhausting. Enough space and visibility\nYou know how you need to adjust your mirrors and seat in a driving exam (and every time you are about drive) before you start driving? It’s pretty much the same thing with pairing. Allocate proper space for both people and make sure it is easy to see what’s on the screen. Patience\nWhen both the observer and the driver are out of ideas for a problem and are thinking it is important for both of them to not feel any pressure so they can think freely. The comfort comes with knowing each other more and having spent time pairing for a certain amount of time (or can also be character and/or experience). Don’ts Try not to interrupt\nWhen the driver is explaining something it is probably best to not interrupt and wait for them to finish. As a driver it can be hard to explain the reasoning behind what you are doing as you are doing it. It makes it even harder if you start questioning it or interrupting as the observer. Even if it is incorrect in your opinion the driver should still have a chance. You can always discuss after the driver is done explaining and/or implementing then discuss a possibly better solution. Hands off\nTaking the keyboard off the driver before his turn has finished is a no no. Explain it to the driver and let the driver type it for you this way they will learn. Even if it is like teaching a kid something, be patient. Try using line numbers, pointing out examples in the code or whatever you feel may help.", "date": "2015-10-16"},
{"website": "FindMyPast", "title": "Judge the code, not the developer", "author": ["Asier Barrenetxea"], "link": "https://tech.findmypast.com/judge-the-code-not-the-developer/", "abstract": "This was originally posted at asierba.net I’ve been paying attention recently to conversations we have at work. I can hear people complaining about the code: how bad it is, not readable or difficult to make changes. They will refer to the people who wrote it. And I do it too! Even if we don’t know who wrote it. It doesn’t matter, is ‘us’ and ‘them’. “ They wrote this crazy stuff.. why??” “ Somebody had this idea of creating this hierarchy of classes. Look now, how can we extend this class without affecting this other??” “Look, a developer added an extra call to the service here. This is making our application not to perform well.” We are judging the people, instead of the code. This just creates tension between co-workers. It creates a difference between the person who wrote the ugly code and us. We are kind of saying they don’t know how to write code, saying that they are not doing their jobs properly.. ultimately we are just saying they are inferior and that we are better than them. We judge them. Let’s change our attitude, and detach the code from the people who wrote it. “This crazy stuff.. why??” “This hierarchy of classes. Look now, how can we extended this class without affecting this other??” “Look, there is an extra call to the service here. This is making our application not to perform well.” We would still be stating the same thing: the code is bad, not readable, difficult to make changes.. BUT we won’t be judging people, our co-workers, our team mates. We will be all in the same boat seeing the same problems and ultimately fixing them together.", "date": "2015-11-26"},
{"website": "FindMyPast", "title": "How I keep up to date in the software industry", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/how-I-keep-up-in-the-software-industry/", "abstract": "How I keep up to date in the software industry I find keeping up to date quite challenging in software industry.  The pace of change at times can be fierce and daunting.  Fear not though as over the years I’ve found a few easy ways to keep up. Firstly I use a couple of phone applications to follow tech news. Feedly Feedly is a news feed application, it works from atom and RSS feeds.  This is a great way to get updates to your phone from various websites without having to visit them. This delivers news items in an easy consummable list that I scan.  Feedly has an intergration with Pocket that lets me long click articles I’m interested in to save to Pocket. Pocket Pocket is an offline reader application.  This lets me read the articles I’ve saved at my leisure.  It’s also got some nice features where it suggests articles based on what you’ve read before. Newsletters I’ve recently dropped a lot of the blogs that I had in my news feed and replaced them with curated newsletters - I find the quality of the articles usually pretty good and also then I don’t have to hunt around the web for them.  There’s a lot there and I usually only pick one article from each newsletter to read each week.  I like a good cross section of the technologies I read about. Changelog Weekly Web Operations Weekly Node Weekly JavaScript Weekly Elixir Weekly Software Lead Weekly Podcasts Some of the best and most inspiring information I’ve got is from listening to podcasts, I listen to these when I do the dishes at night! The Changelog Podcast Ruby Rogues Books I also read a fair few books but these are usually more around process than directly technical.  Here’s what I’ve read in the last year that I thought was good. Getting to Yes This was an interesting book, it taught me better how to approach situations where people have strongly differing opinions and focus on the problem at hand rather than what people think is the solution. The Phoenix Project I enjoyed this book immensely and could draw a lot of parallels from businesses I’ve been a part of.  It detais an organisation going through a devops transformation and how they changed their culture and processes to be more effective. The Five Dysfunctions of a Team Similar to The Phoenix Project this is a parable of a dysfunctional executive team and goes through the author’s beliefs in what makes a high functioning team.  Both The Phoenix Project and The Five Dysfunctions help you learn by experiencing the dysfunctions first hand through the characters in the book; I think this makes the learnings resonate more deeply than an abstract discussion of the issues - as a result they are fun and easy to read. Drive: The Surprising Truth About What Motivates Us I thought this book was great, it’s a treatment of what motivates individuals.  It details intrinsic and extrinsic motivations and what can be good and bad for individuals and teams. It promotes giving high levels of autonomy but that requires a high level of trust. Leading the transformation This is a nice and concise high level treatment of adopting a devops philosphy.  A lot of this most developers will be aware of but I found it helped me glue together all the concepts into a cohesive vision of what devops is.  I recommend this if you’re at all interested in highly effective cross-functional teams that can be autonomous and deliver fast. In conclusion I go through phases of doing a lot for several months then nothing for several months.  Life is supposed to be fun so the key to it is not to feel bad if you’re not doing all this stuff.  If you do find reading/listening about tech interesting and you enjoy it then hopefully some of these resources might be of use to you. Neil", "date": "2015-11-27"},
{"website": "FindMyPast", "title": "Getting up and running with Phoenix and Elixir", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/getting-up-and-running-with-phoenix-and-elixir/", "abstract": "At FindMyPast we have been trying Elixir and Phoenix for a few months now. After creating some spikes we are about to start creating our first production API.\nIn this post I want to help you getting up and running with these technologies by using Vagrant. Why Vagrant? Vagrant lets you create and configure lightweight, reproducible, and portable development environments. The benefits are many but mainly: You don’t have to download all the packages and dependencies into your machine. You can share settings with your colleagues and you can track history of changes with Git as any other file. You can use the exact same configuration to run locally than the one used in production. All your configuration is in version control What are we going to do? We are going to create a Linux virtual machine that will run an empty Elixir and Phoenix application. We are going to do this by using Vagrant. \nWe will be able to edit the code in our regular machine and the application will be automatically updated. Let’s get our hands dirty: Download and install VirtualBox . Download and install Vagrant If you are running on Windows, turn off Hyper V feature in Windows features. Also include Vagrant in the System Path variables: setx path \"%path%;C:\\HashiCorp\\Vagrant\\bin\" Download the following Vagrant setup files, this includes Postgres, Elixir, Phoenix, and all required dependencies. https://github.com/kiere/vagrant-phoenix-postgres Open your favourite command line tool. Navigate to the root folder of the repo you have just downloaded and run the following command: vagrant up This will create a new virtual machine with all required dependencies.\nIf you get an error message, trying deleting the following code from the VagrantFile and run Vagrant up again: config.vm.synced_folder(\n  '.',\n  '/vagrant',\n  type: 'rsync',\n  rsync__exclude: [\n    '.git/',\n    '.vagrant/',\n    'log/*',\n    'tmp/'\n  ]\n) Run vagrant ssh . That will take you to the actual box you have just created. This machine has all the dependencies installed so that you can write iex and use the Elixir Interactive tool. Vagrant creates a shared folder between your machine and the box. The folder is called vagrant .\nLet’s navigate to it: cd /vagrant Let’s create an empty Phoenix project: mix phoenix.new %name_of_project% This will include live reloading by using brunch. If you don’t need it you can run: mix phoenix.new %name_of_project% --no-brunch If you just want an Elixir project you can run: mix new %name_of_project% Navigate to your new projects folder: cd /%name_of_project% Run your newly created Phoenix app: mix phoenix.server In your local machine open up a browser and navigate to localhost:4000 . You should have the Phoenix application up and running! From here you can feel free to do changes in the application in the Linux box. In my case I like that since the code is in that shared Vagrant folder with my Windows machine, I can use my regular development tools in my local environment, do changes, and it will be automatically updated in the application. Running tests mix test This command will run all the tests we have with the template. If you get the following error: ** (Mix) The database for Play.Repo couldn’t be created, reason given: psql: FATAL:  password authentication fa. FATAL:  password authentication failed for user “postgres” It means that you need to set a password for the postgres user. To solve this problem go to postgres: sudo -u postgres psql And once in postgres, run this command: alter user postgres with password 'postgres' Some more useful commands If you need to do any changes to the Vagrant configuration, say the port number, just update it the VagrantFile and run: vagrant reload Once you want to stop the application, first exit the vagrant box exit and then suspend it vagrant suspend .\nRemember not to leave the app running since we are not running the virtual machine from VirtualBox UI and files can be corrupted.", "date": "2015-12-07"},
{"website": "FindMyPast", "title": "Elixir magic for fun and profit!", "author": ["Andy Mendelsohn"], "link": "https://tech.findmypast.com/elixir-magic-for-fun-and-profit/", "abstract": "It started on slack… …with a brief slack exchange where David Elliot pasted a specific snippet of elixir describing a data structure and how to use the put_in function to modify that data: our_tasty_map = %{ fields: [ %{ name: \"rank\" , rank: 0 }, %{ name: \"location\" , location: \"\" }], type: \"how-common-is-surname\" } |> put_in ([ :fields , Access . at ( 0 ), :rank ], 100 ) |> put_in ([ :fields , Access . at ( 1 ), :location ], \"Weston Super Mare\" ) which transforms this: %{ fields: [ %{ name: \"rank\" , rank: 0 }, %{ name: \"location\" , location: \"\" }], type: \"how-common-is-surname\" } into this: %{ fields: [ %{ name: \"rank\" , rank: 100 }, %{ name: \"location\" , location: \"Weston Super Mare\" }], type: \"how-common-is-surname\" } My first thought was “yuck” because, at first glance, I hated this: put_in ([ :fields , Access . at ( 0 ), :rank ], 100 ) put_in ([ :fields , Access . at ( 1 ), :location ], \"Weston Super Mare\" ) This is certainly not David’s fault. It’s just the argument syntax required by the put_in function, and specifically the array passed as the second argument to put_in.  It’s an array of keys and/or list locations, and it was crying out for some syntactic sugar. Forgive me if I admit to immediately craving something akin to the perl5 data structure de-referencing syntax. (hey, listen, I like Perl, m’kay?) $ hash -> { \"fields\" } -> [ 1 ] -> { \"location\" } It’s a while since I’ve written any Perl, but I think this, or something similar is the syntax required to get to the data stored under the {“location”} key in the anonymous hash referenced at the second [1] element of the anonymous array referenced to by the {“fields”} key of the hash referenced to by the variable $hash. So I was really craving this: put_in ( access_at ( \"{:fields}->[1]->{:location}\" ), \"Weston Super Mare\" ) instead of this: put_in ([ :fields , Access . at ( 1 ), :location ], \"Weston Super Mare\" ) Yes, it’s more key-presses, but it feels (IMO) nicer, cleaner and easier to understand. It might not work for you, but it worked for me… And yes, to some, it feels uncomfortable when elixir uses -> to indicate the body of a function, but the string quoting of that syntax is probably enough to disambiguate it. At the end of the day this is just code generation where I needed this: access_at ( \"{:fields}->[1]->{:location}\" ) to generate (morph into) this: [ :fields , Access . at ( 1 ), :location ] Macro FTW! This is a perfect job for a macro based Meta-programming job. It’s all about code generation! Meta-programming is bad! Wicked. Evil. Meta-programming and the macros that make it easy can also do stuff that you don’t know about or expect. On the other hand…. Meta-programming is fantastic! Wonderful, amazing things. Meta-programming and the macros that make it easy can do stuff that makes magic, saves you time, makes your code more readable, etc, etc.. Elixir’s macros are not hard to apply, understand or use. They are a little surprising. But they are surprising in that they seem to just work and they work with such ease. So, I took a stab at it, and after a little fumbling I got my integration test to pass using the following code: defmodule Mapit do defmacro access_at ( data_structure_string ) do String . split ( data_structure_string , \"->\" ) |> Enum . map ( & convert_from / 1 ) end defp convert_from ( string ) do cond do string =~ ~r/\\{.*}/ -> mapped = Regex . named_captures ( ~r/\\{:(?<key>.*)\\}/ , string ) String . to_atom mapped [ \"key\" ] string =~ ~r/\\[.*\\]/ -> mapped = Regex . named_captures ( ~r/\\[(?<index>.*)\\]/ , string ) index = String . to_integer mapped [ \"index\" ] quote do Access . at ( unquote ( index )) end end end Wow. It worked. But I didn’t like it. It felt clunky. cryptic. repetitive. conditional. conditional. Yes, that was it. The conditional needed to go. But also,  that convert_from was just doing far too much. So I broke it up into smaller pieces of functional functions. The First problem was how to match once as opposed to twice and to capture everything in one hit rather than two and how to reference the matches easily: Regex.named_captures to the rescue! ~r/(\\{:(?<key>.*)\\}|\\[(?<index>.*)\\])/ This regular expression matches {:map_key} OR [n] not both (it can’t be both!). It also returns a map with both keys, one of which will hold an empty string and the other a value (depending on whether it matches a key or an index). That made calling a pattern matching function very easy. defmodule Accessit do defmacro access_at ( data_structure_string ) do String . split ( data_structure_string , \"->\" ) |> Enum . map ( & get_key_or_index / 1 ) |> Enum . map ( & convert_from_capture / 1 ) end defp get_key_or_index ( string ) do Regex . named_captures ( ~r/(\\{:(?<key>.*)\\}|\\[(?<index>.*)\\])/ , string ) end defp convert_from_capture (%{ \"key\" => key , \"index\" => \"\" }) do String . to_atom ( key ) end defp convert_from_capture (%{ \"key\" => \"\" , \"index\" => index }) do index = String . to_integer ( index ) quote do Access . at ( unquote ( index )) end end end Pretty bloody simple. access_at takes a string, splits it on the (possibly contentious) “->” and passes the resultant array to a map calling the get_key_or_index function which, surprise surprise, gets the key or index. get_key_or_index is a simple one line regex call that matches on the key or index, based on the syntax of “” or [n] and returns a simple Map of matches. Simples! The nice thing about Maps is that you don’t have to care about their order when you are using them to pattern match function arguments. And, because you can’t be a map key -and- an array index, the Map returned by get_key_or_index would have either ‘key’ or ‘index’ with a value and the other pointing to an empty string. That’s pattern matching heaven! In the case of it being a Map key, we merely call String.to_atom (because we stripped out the ‘:’ in the regex). For list elements, we take advantage of both quote and unquote in order to return Access.at with the correct index number (as opposed to calling it). See: no conditionals! And now there’s a git repo. accessit .Clone it ,\nStick the lib/accessit.ex in your lib, require it with: defmodule Mymodule do require 'accessit' import Accessit ... end There are still a few minor problems, like the fact that it’s not matching a key that’s a string, but that’s pretty easy to fix. The tests are all integration testing because we only really want to test the code that’s generated and not the meta-code or AST. So there you have it. Bob’s your uncle, Mary’s your aunt and Meta-programming is the grand wizard that heals your wounds and makes you smile.", "date": "2016-07-17"},
{"website": "FindMyPast", "title": "usher-cli - A Node.js CLI for stitching together command line interfaces", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/usher-cli/", "abstract": "Usher CLI : A Node.js CLI for stitching together command line interfaces At Findmypast we needed a way to reduce the complexity of our deployment scripts. We built usher-cli to simplify and standardise our application deployment automation scripts. Firstly let me take you back three years. At Findmypast we stored all our release build configuration and deployment scripts in Teamcity. This was breaking one of the big rules of continuous delivery “Application code and app & system configuration all in source control”.  Breaking that rule manifested itself negatively for us in two ways. Firstly our teamcity occasionally suffered outages which meant we couldn’t release.  The second and worse problem was that anyone could change the configuration and there was no change management so configurations were regularly broken.  This resulted in a lot of developer time being spent from within the Teamcity interface tweaking our configuration, trying to get our builds working again and unfortunately delivering no value to our customers. From here we started looking more at scripting the various steps and maintaining those scripts within our project source repositories. We’ve scripted with powershell (too windows specific), ruby-rake (concurrency in ruby melted my brain) and laterly gulp (Node.js). Gulp was a great step forward because it took all the automation logic out of Teamcity and moved it to within the application’s codebases. Our engineers were now more easily able to test their changes to the automation and also have a full history of changes through version control. There were some downsides to Gulp though. It was hard to test our scripts because we never took the time to fully abstract them away from Gulp itself. Between our various projects there was a lot of repetition of similar Gulp tasks and a few times we ended up in the situation that we had multiple different places to apply a fix when we found a bug. We broke out a few libraries to individual NPM packages but we never got the seams quite right so that they weren’t that reusable and transparent. Recently we started a large re-architecture of our Findmypast product and moved over to Docker containers for hosting our applications and Consul for service discovery. At this point we thought about everything that had gone before and the tooling available to us with the Docker ecosystem. We decided that the best point of abstraction was to build individual command line applications in order to encapsulate various tasks that we needed to perform as part of our deployment automation. We tried out a few tools. First we tried Ansible which we thought was great but a lot of our devs are running Windows as their OS meaning they couldn’t run Ansible without some elaborate VM setup.  We thought about straight bash scripts but we liked the model of CI tools like CircleCi and Travis where there’s a file at the root of your repository that describes how to put your application into production. This also gives a nice standardisation across all our projects; if you want to know where the deployment commands are, a developer simply goes and looks in usher.yml. At this point we decided to build Usher. Usher is built in Node and allows you to write your deployment scripts in yaml in a very similar way to CircleCi etc. We chose Node for a few reasons: it’s really easy to build small command line applications; we can make use of lots of other Node command line applications that are available in the ecosystem; finally a Node cli usually works quite well cross platform as it handles spawning child processes for the environment that you’re running under be that Windows, Unix or Mac. Have a look at our code in github or the npm package . Here’s an example of a small Usher file with a task for Elixir and Node unit tests and also an aggregate task to run all your unit tests. Usher supports full templating erb syntax, examples in our repository documentation . elixir_unit_tests : - description : Run elixir unit tests cmd : docker-compose -f docker-compose/dev.yml run --rm --no-deps api mix test environment : - MIX_ENV=test node_unit_tests : - description : Run node unit tests cmd : docker-compose -f docker-compose/dev.yml run --rm --no-deps web npm test unit_tests : - description : Run unit tests inside a container (with automatic build) - task : node_unit_tests - task : elixir_unit_tests Then you can run commands like so usher run node_unit_tests For future features we’re looking at being able to do some form of file inheritance or inclusion so that you can import various standardised tasks and allow some form of centralisation of deployment configuration. Sound off in the comments!", "date": "2016-07-22"},
{"website": "FindMyPast", "title": "Repository Driven Development", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/repository-driven-development/", "abstract": "Ok, so you’re moving to a microservices architecture. You’ve got a lot of things to think about when you’re increasing the number of moving parts within your system. Instead of one point of failure you now have a hundred points of failure! At Findmypast we use repository driven development to help manage the complexity of connecting our microservices to the many different centralised operations services we have in order to reduce the risk of operating a microservices architecture. I’m a huge fan of Sam Newman’s principles of microservices which is a great set of guidelines when getting into the world of microservices. In this post I will cover how Findmypast applies two of the principles; making microservices highly observable and a culture of automation. Sam has also talked about having a superstructure that supports microservices which I’ll describe as well. At Findmypast our superstructure is the glue that holds our suite of microservices together. Our centralised monitoring, logging, alerting, service discovery, service configuration, continuous integration, source control and feature toggling all make up our superstructure. These operations services help us to operate our microservices out in the wild and react to problems when things go wrong. However it’s a big responsibility for our developers to think about all these different operational services so we try to make it as easy as possible to hook up our microservices to the superstructure when our developers are creating new microservices. This is where repository driven development comes in. To quote the blog from codeship ‘Repository driven’ means your developers never have to look anywhere except the code and the repo to test and release software. This is super efficient, allowing teams to stay focused on their code and their projects rather than maintaining CI/CD infrastructure. We’ve taken repository driven development a little further and we’ve tried to capture all externally connected services within configuration files in the codebase of our microservice. This means that logging, monitoring, alerting, visualisation, infrastructructure, deployment and everything else is all connected up from within the codebase. In order to create a fully production ready microservice our developers can work solely within a single repository to configure everything required. To make our services highly observable we have centralised logging through Graylog and centralised monitoring with Graphite and Statsd . After capturing this telemetry about our microservices we visualise it and alert on it. Our developers can write an alerts.yml file which gets posted into our service discovery mechanism underpinned by Consul . From there we use Consul-Template in order to generate alerts that get picked up by Icinga . Our developers can write a dashboard.yml file which will automatically generate a visualisation dashboard and it will pop up on our TVs dotted around the office. Here is an example of an Icinga alert using graphite data generated the yaml config below. environments : production : host_address : flipper.example graphite : response_elapsed_time : display_name : Time in ms for response graphite_target : averageSeries(long.flipper.production._._.requestElapsedTime) reverse : false critical : 200 warning : 50 timespan : 30 This is one of our automatically generated dashboards using graphite data generated the yaml config below. data_sources : parse_all_configs_time : key : summarize(long.dasher.production.parse_all_configs,\"5min\", \"avg\") functions : dashboards : graphite_production : title : Dasher Graphite Stats widgets : parse_all_configs : title : All projects parsed type : Rickshawgraph width : 2 height : 1 properties : data-renderer=\"line\" data-unstack=\"true\" measures : parse_all_configs_time : source : parse_all_configs_time We’ve built a strong culture of automation within Findmypast and at the core of our new microservices architecture is Docker . This has sped up our delivery pipelines which are now running under five minutes for a complete test and deploy cycle for all our new microservices. In these codebases we’ve been able to fully adopt continuous delivery and trunk based development. In addition to those benefits it has enabled easy spin up of our development environments in Windows, Linux and Mac so a boon for our developers. This is great when bringing in new developers as they’re mostly able to get a working environment on day one of working here. The foundation of our new Docker infrastructure is the automatic provisioning and configuration of our Docker hosts with Puppet . Our operations team have worked tirelessly to make sure that provisioning isn’t a bottleneck and is easily repeatable. We have a bank of generic Docker hosts for testing and when we’re ready to go to production we can get a dedicated bank of hosts spun up the same day. It’s important to us that our developers can be as autonomous as possible. We feel strongly about not programming in our continuous integration tool so we built Usher . We use it to stitch together all the various command line interface tools that make up our automation pipeline. This leads to deployment being as simple as the command usher run deploy . We can test everything locally before setting up our continuous integration pipeline to just plug in the simple commands to the continuous integration software. We also make use of Docker-Compose to describe the infrastructure and environment that we wish to deploy, run or test. We have an automated service proxy that automatically surfaces our new microservices and allows us to route to them through the internal dns which means no more manually changing nginx configurations and dns entries. We’ve also been looking at Traefik as an alternative to our home grown automated service proxy which we think will plug in nicely. The best code is the code you don’t have to write! dasher : description : A dashboard view to be used by the visualization framework. root-tag : production This proxy configuration will generate a url similar to http://dasher.service-proxy.example automatically. In order to decouple our deployments from our releases we use feature toggles . You have to be careful with feature toggles and make sure to clean up after yourselves but it’s well worth the effort. Our feature toggling infrastructure is built on top of launch darkly and it’s trivial to add in toggles with our ReactJs library reactoggles . When deployment stops being something you worry or even think about, you can start releasing at will. toggles : * name : Example toggle description : A description of a toggle it’s a big responsibility for our developers to think about all these different operational services It’s worth reiterating that point from earlier and it’s not something we want to burden our engineers with. The final tool I want to cover is an internal automation tool called Overwatch which addresses this burden. At it’s base it’s a Yeoman generator which allows you to create a skeleton application, have it pushed to github, have it automatically create a continuous integration project and pipeline, and finally have it hooked up to every system I’ve mentioned previously in the post. In this quarter we want to empower our engineers to go from a blank slate to a production microservice in under ten minutes. All this adds up to a super efficient day-to-day development experience. We’re on a mission to remove all the day to day change work and let our engineers focus on delivering business and customer value rather than fiddle with half a dozen operations systems. Our goal is for our engineers to never have to look anywhere except the code in the repository they are working in to test, release and manage our software. We build our software with NodeJs , ReactJs , Elixir and Phoenix . If you like the sound of what we’re doing here please apply as we’ve got positions open in both Dundee and London .", "date": "2016-08-21"},
{"website": "FindMyPast", "title": "Testing React using Enzyme", "author": ["Richard Kotze"], "link": "https://tech.findmypast.com/testing-react-using-enzyme/", "abstract": "What is Enzyme? It’s a testing utility to assert, manipulate and read rendered React components. Why use it? Keeps the code base clean and reduced boiler code Stable rendering of components Easy to use API to assert on rendered components e.g dom.find('.element') What our code looked like before Enzyme Below is one test asserting the user name exists in the ProfileBox component. In the before function a component is rendered to the dom (using jsdom ). The setTimeout is used to wait 20ms, allowing extra time for the component to mount. (this is a hack.) In the after the component is removed for clean up and a setTimeout is used for similar reasons as mentioned previously. findDOMNode method is used to get the rendered component from the DOM, allowing access to stand DOM API which can be used to assert on as shown in the it . describe ( ' User profile ' , () => { let component , dom ; before (( done ) => { let person = { FirstName : ' Richard ' , LastName : ' Kotze ' }; component = React . render ( < ProfileBox hint = { person } /> , document . body , () => { setTimeout ( done ); } ); }); after (( done ) => { React . unmountComponentAtNode ( document . body ); setTimeout ( done ); }); it ( ' should contain name ' , () => { dom = React . findDOMNode ( component ); dom . textContent . should . containEql ( ' Richard Kotze ' ); }); }); Using Enzyme The same example as above but using Enzyme. import { mount } from ' enzyme ' ; describe ( ' User profile ' , () => { const person = { FirstName : ' Richard ' , LastName : ' Kotze ' }; it ( ' should contain name ' , () => { const profileDom = mount ( < ProfileBox hint = { person } /> ) ; profileDom . text (). should . containEql ( ' Richard Kotze ' ); }); }); Clearly, there are far less lines of code written, making it more focused and readable. Also, the Enzyme test is more reliable because in the first example, the hack creates flaky tests. There is no need for the before so we render the component in the it , but the component can be rendered in the before if needed. Enzyme API Enzyme Guide Enzyme provides three ways to render your components. Shallow: shallow(<component />) Testing the component as a unit and not asserting on child components. (jsdom or browser not needed) Full: mount(<component />) Full DOM rendering when interacting with DOM APIs or components that use lifecycle methods. ( Needs jsdom or browser envrionment) Static: render(<component />) Render React components to static HTML and analyse the HTML stucture using the Cheerio library. (jsdom or browser not needed) Common Enzyme examples In the examples below are commonly used Enzyme methods to help get started. The assert library used is shouldJS , the Chai can also be used. I find “should” helps make the asserts more readable and focused. If you are using shouldJS then you can try should enzyme to help with Enzyme assertions. The find method will probably be used in every test and is used to traverse through the DOM using css selectors to get elements. This returns a ReactWrapper . There other Enzyme selectors to find elements. The following example renders a component containing a list and the find method is used to get the list items to assert the total. const dom = shallow ( < ExampleComponent /> ); const exampleList = dom . find ( ' .exampleList li ' ); exampleList . length . should . equal ( 3 ); get(index) returns a node ( ReactElement ) giving access to React and DOM methods. at(index) returns a wrapper to access Enzyme methods. While the difference is subtle the get method is useful to check the rendered markup. In the following example, the first list item is found and checks for a css class. const dom = shallow ( < ExampleComponent /> ); const exampleList = dom . find ( ' .exampleList li ' ); exampleList . get ( 0 ). getAttribute ( ' class ' ). should . equal ( ' special ' ); To access the state and prop objects in a React component, Enzyme exposes state([key]) , prop([key]) and props() . In the following example, the component takes a profileId to get a user profile. Assert the profileId is correct on the props and the expected user name assigned in the state. const dom = mount ( < ExampleComponent profileId = \" 123 \" /> ); const exampleList = dom . find ( ' .exampleProfile ' ); dom . state ( ' name ' ). should . equal ( ' Richard ' ); dom . prop ( ' profileId ' ). should . equal ( 123 ); To simulate an event like onChange , use the simulate(event[, mockData]) method. In the following example, that event onChange is fired with a mock value, and the test asserts that the value has changed. const dom = mount ( < ExampleComponent /> ); const exampleInput = dom . find ( ' .exampleForm .userName ' ); dom . state ( ' userName ' ). should . equal ( ' typo.doe ' ); exampleInput . simulate ( ' change ' , { target : { value : ' john.doe ' }}); dom . state ( ' userName ' ). should . equal ( ' john.doe ' ); Enzyme JS is a useful tool for testing React components enabling developers to build tests efficiently. I would encourage all React app development to use this library.", "date": "2016-08-31"},
{"website": "FindMyPast", "title": "Twoface and blue green deployment", "author": ["Oleksandr Stasyk"], "link": "https://tech.findmypast.com/twoface-and-blue-green-deployment/", "abstract": "When we deploy our application into the world, we generally want to avoid downtime. This can be avoided by deploying the new version in parallel and then convincing your router or reverse proxy to change where it’s pointing to. This is a deployment automation process described by Martin Fowler as the blue-green deployment. So the theory being sound, how would you actually achieve this in practice? Getting the application ready First step is to make sure your application infrastructure allows you to have two instances running in parallel. This step might be trivial to some projects, however this might uncover some tangled up parts in your application. Containerization or at least deployment in an isolated environment might help you our here. At Findmypast we have adopted docker containerization for our deployment, meaning each application deployment results in an isolated docker container, to which our reverse proxy can routed to. When you decide to go down the docker path you quickly realize that docker alone will leave your infrastructure in a bit of a mess. So to help with configuration management and orchestrating (tying together) the docker containers we use docker-compose . This, in turn, allows us to define a name for your project, project here being a reference to a collection of services working together. Using a unique project name allows to create multiple containers running the same version of the service, and therefore bring up blue and green containers at the same time. Service discovery To allow your service proxy (or router) to know where to point, a form of service discovery is required. A popular set of tools for this is HashiCorp’s Consul and the registrator docker container . Together they automatically provide a dynamic index of your running services. Along with a name, services can have “tags” metadata values. This is very useful if you need to keep track of containers running different testing environments, and on top that it can help us to separate blue and green instances. The control of the tags as well as project name is done via the container’s environment variables such as SERVICE_TAGS and SERVICE_NODE . These variables are then read by registrator. When the values in consul change, a handy tool called consul-template can monitor and update template files based on this data. The template file could be part of a reverse proxy (in our case nginx ) configuration. So once a container is deployed and registrator container registers it with consul, the reverse proxy can automatically start routing traffic to it. Consul has a convenient feature which allows storage of arbitrary key-value pairs for user-defined data. Using this store we keep track of the current live colour per service. Deployment sequence Using our task runner tool usher , it’s possible to tie multiple command sequences together. This is where we can control our deployment commands which include the blue-green switch. Of course the deployment steps do not have to be implemented in a tool like usher or be defined as a command sequence; however, a degree of automation is highly recommended! Alternative tools/frameworks like capistrano, rake, grunt, gulp, or just plain bash scripts can help out. Regardless of implementation, the sequence of actions related to deploying with a blue-green switch can be described as: Determine the new colour to be deployed, default to something, if nothing is deployed already. Deploy a new application instance, while marking (e.g. tagging) it with this new colour. Run tests against the new instance. If the tests pass, change the active colour to be the new one. Tooling At Findmypast, we make use of tools as soon as we come across a step which could be automated. This approach certainly encourages us to not only develop a task runner like usher, but to go even further and abstract communication with various services. Since our state of the service colours is in a store in Consul, we rely on communicating with its API. This is exactly the case where the specific API calls (to read and manipulate key-value pairs) can be abstracted in a command line tool. Enter twoface . This command line tool has a very simple job of looking up the opposite value colour to the one in Consul as well as changing it. This functionality is exposed by two commands: twoface peek and twoface flip . Given the two possible values, peek prints the opposite value to standard out  and flip writes the opposite value to Consul’s key-value store. So replacing the steps in the sequence mentioned above with twoface usage: NEW_COLOUR=$(twoface peek -H my.consul.com -k myservice/colour -b blue -g green) Deploy a new application instance, while marking (e.g. tagging) it with this new colour. Run tests against the new instance. twoface flip -H my.consul.com -k myservice/colour -b blue -g green Finding the approach right for you The aforementioned technologies are our personal choice which you do not have to agree with. You are free to implement blue-green development using your favorite approach, however one piece of advice on your journey would be to make this ( and every ) part of your deployment as transparent and as automated as you can. The reason for the twoface tool was to encapsulate communication with consul and make it reusable. But regardless of where or how you choose to keep track of your state, make sure it keeps your deployment streamline! The easier it is to add and maintain blue-green deployment to your service or application, the less overhead it will add while you get to reap the benefits of rapid and more reliable delivery.", "date": "2016-09-08"},
{"website": "FindMyPast", "title": "Testing React using Enzyme - a novice's quick guide", "author": ["Laura Green"], "link": "https://tech.findmypast.com/testing-react-using-enzyme-novice-guide/", "abstract": "Writing tests for React components, using Mocha, Enzyme and Should.js, can feel a bit like trying to do this toddlers’ puzzle, inside a black velvet bag. Try this shape? Nope. How about this one? Nope. Maybe this one? Ok, that seems to work, but - I can’t see the shape it is - so I don’t know why it fitted. Time to learn what is really going on here. Our tests usually look something like this: describe ( ' Button component ' , () => { it ( ' should set the button size to large ' , () => { const renderDom = mount ( < Button size = \" large \" > Button < /Button> ) ; renderDom . find ( ' a ' ). get ( 0 ). getAttribute ( ' class ' ). should . containEql ( ' button_large ' ); }); }); Now apparently, Mocha is the testing framework Enzyme is a testing utility for React components Should is the assertion library jsdom is a JavaScript based headless browser which we render components into using the mount function Mocha is used for the describe and it lines in the example above.  These parts are always the same for our tests, so I don’t feel the need to look any further into this here. Enzyme is the rendering bit (ie, the mount ) and all the different ways of traversing the DOM and find ing various bits of what was rendered. It’s the part I’m mainly going to be looking at here. Should is the ‘assertion library’ - meaning, it’s the library used to evaluate the actual to the expected outcome - it’s the heart of the test itself really. Pinpointing what needs to be tested I want to test my Button component, to ensure the value of the size prop makes the resulting markup have a class of button_large . So, assuming I have mounted the following: <Button size= \"large\" > Button </Button> the expected markup would be: <a href= \"\" className= \"button_large\" > Button </a> This is where Enzyme comes in, to pinpoint what you want to find, and in this case, it’s the node a . But the a tag is also the only node in our mounted markup (ie, at position 0 ): find('a') get(0) For find('') you can use the following selectors: class syntax ( .foo , .foo-bar , etc.) tag syntax ( input , div , span , etc.) id syntax ( #foo , #foo-bar , etc.) prop syntax ( [htmlFor=\"foo\"] , [bar] , [baz=1] , etc.); For get() - it’s simply the position of the node, ie, if it’s the fourth node in the DOM, then get(3) because it’s a zero based index. Now we’ve found the node that we want to inspect, we need to find the specific bit that we want to evaluate within it. In this case it’s the value of a class: get(0).getAttribute('class') Obviously, this could be any valid attribute of the tag in question, such as href , type etc. And finally, the evaluation is done by Should syntax, where we need the following: should.containEql('button_large') The Should.js assertion library contains a myriad of evaluating statements, some of which we’ve used are: should.have.length(2) should.equal('Foo') should.containEql(style.Bar) should.not.containEql(\"dropdown_show\") should.startWith('button') Read Richard Kotze’s post here on the Findmypast Tech Blog: Testing react using enzyme to learn much more. The exercise of writing this has made the puzzle of Mocha + Enzyme + Should a lot clearer in my head. Did it help you?", "date": "2016-09-15"},
{"website": "FindMyPast", "title": "Elixir Maintainability", "author": ["Steven Blowers"], "link": "https://tech.findmypast.com/elixir-maintainability/", "abstract": "When writing code it is good to keep in mind the perspective of the next\nprogrammer who will read it, be that yourself or someone else, be that tomorrow\nor in six months time. The Elixir/Erlang community has created a variety of tools to improve code\nmaintainability and readability. In this blog I will be showing you how to use\nfour essential tools and also giving my views on them. A note before I get to the tools, I am a big believer in Test Driven Development\nand thorough test coverage of code. Elixir has ExUnit , an excellent\ntool for writing unit tests that run super-fast, all the tools mentioned below\nshould be used on top of thorough test coverage. Table of Contents Why listen to me? ExDoc Adding it to your project How to create documentation How to generate documentation Dialyzer / Dialyxir Adding it to your project How to run the analysis Credo Adding it to your project Configuration How to run the analysis ExCoveralls Adding it to your project How to run the analysis Summary Why listen to me? My team recently finished a project writing an API in Elixir, where one of the\ngoals was to deliver the project with high maintainability. Below I will be\ndiscussing tools that we used to increase maintainability in our production\ncode. ExDoc Github Hex This is the tool used to generate all of the official documentation in Elixir.\nIt is also the tool used to generate documentation for packages published to Hex , the most widely used package manager in the Elixir/Erlang\necosystem. Its goal is to make generating documentation for your project as easy\nas possible, and I think it does a good job of meeting that goal. Adding it to your project Add the following code to your mix.exs file: def deps do { :ex_doc , \"~> 0.13\" , only: :dev } end More detailed installation and configuration instructions can be found on the Github page. How to create documentation If you go straight to generating documentation now, you will see that ExDoc has\ndone an excellent job of creating a skeleton structure of your project and its\nmodules for you to add more information to. The way you will be adding documentation is by adding attributes to your\nmodules. For example: defmodule MyProject . Response do @moduledoc \"\"\"\n  This module will respond to a greeting.\n  \"\"\" end This uses the @moduledoc attribute to add documentation at the module level. There is also the @doc attribute which adds documentation for a single\nfunction. You may find when using this attribute with functions that use pattern\nmatching and multiple function definitions it can get a little messy. In order\nto get around this you can define a body-less definition of your function to add\nthe @doc attribute too. For example: defmodule MyAnimals . Sound do @doc \"\"\"\n  Recognises an animal given its sound.\n  \"\"\" def recognise ( sound ) def recognise ( \"oink\" ) do :pig end def recognise ( \"moo\" ) do :cow end end Besides adding useful documentation to your project, ExDoc also has a killer\nfeature, DocTest . This\nfeature allows you to run unit tests on the examples you give of how to use your\nfunctions. Here is an example of how to do that: defmodule MyMaths . Basic do @doc \"\"\"\n  This function will add two numbers together.\n\n  ## Examples\n\n      iex> MyMaths.Basic.add(3, 4)\n      7\n\n  \"\"\" def add ( num_one , num_two ) do num_one + num_two end end defmodule MyMaths . BasicTest do use ExUnit . Case , async: true doctest MyModule end As you can see in this example we include iex> in the @doc attribute on a\nfunction to describe an example of how to use a function. Then, in the\ncorresponding test file we use the doctest macro to run tests on those\nexamples. The idea of DocTest is to compliment documentation by ensuring the examples\ngiven in it are up-to-date and correct. It should not be seen as a substitute to\nunit tests. Also, just because your unit tests pass, it does not mean your\ndocumentation examples are up-to-date. Elixir provides more information on writing documentation on their website. How to generate documentation In root directory of your project run mix docs , then the documentation will be\naccessible at “doc/index.html” . For more options run mix help docs . Dialyzer / Dialyxir Github Hex Another attribute you can add to your functions is @spec , which allows you to\ndefine the types that the inputs and outputs of the function will be. This gets\nadded to pages created by ExDoc and allows the user to get a better\nunderstanding of how to use the function. For example: @spec multiply ( integer , integer ) :: integer def multiply ( a , b ) do a * b end Elixir provides more information on Typespecs on their\nwebsite. But how can you test the correctness of the types you specify? This is where\nDialyxir, powered by Dialyzer comes\nin to play. Adding it to your project Add the following code to your mix.exs file: defp deps do [{ :dialyxir , \"~> 0.3\" , only: :dev }] end How to run the analysis Before the first time you use dialyxir, you will have to run mix dialyzer.plt ,\nunfortunately this takes a while, but speeds up subsequent runs. Once you do that, just run with mix dialyzer . Unfortunately, the output from\ndialyzer leaves a lot to be desired, however you should be able to get some\nuseful information from it. A tip I have picked up is to pipe the output into grep if you are looking for output from a particular file. Credo Github Hex Credo is a code analysis tool with an emphasis on consistency within your\ncode base. It can tell you things like whether you have been consistent with\neither tabs or spaces, whether you are about to commit any TODO statements, or\nwhether a function probably has too many arguments, and many more. The display of the output is a real highlight, and greatly helps in\nunderstanding how to fix the issues it raises. Adding it to your project Add the following code to your mix.exs file: def deps do { :credo , \"~> 0.4\" , only: [ :dev , :test ]} end More detailed instructions on installation can be found on the Github page. Configuration Credo can be configured to include/skip certain directories and checks, even set\nup different profiles. Configuration is written in a file called .credo.exs which can live in the\nroot directory of the project or in the config/ folder. An example config file . How to run the analysis In the root directory of your project just run mix credo . I would recommend\nrunning it with the --strict argument to get more feedback. ExCoveralls Github Hex ExCoveralls is a tool to analyse code coverage on a project. It can provide\ncommand-line output, but also integrate with continuous integration tools such\nas Travis CI and Gitlab CI. I particularly enjoy trawling through the output using the coveralls.io service which is free for public\nrepositories. Adding it to your project Add the following code to your mix.exs file: def project do [ ... test_coverage: [ tool: ExCoveralls ], preferred_cli_env: [ \"coveralls\" : :test ]] end defp deps do [{ :excoveralls , \"~> 0.5\" , only: :test }] end How to run the analysis In the root directory of your project just run mix coveralls . Summary I hope this post has been helpful in your discovery of tools to use for\nimproving maintainability. I encourage you to click through the links I have\nprovided in this post in order to learn more about the tools I mention above. Lastly, I’d like to thank all the authors of the tools above for their\ncontributions to the Elixir community.", "date": "2016-09-23"},
{"website": "FindMyPast", "title": "Extracting a Microservice from a Monolith", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/extracting-a-microservice-from-a-monolith/", "abstract": "Ahh, one of the big questions in our industry right now… How do I take a monolith and refactor it into Microservices? Where the heck do you even start? Assuming you’re operationally ready for Microservices migration can be an arduous task. This article covers our approach to identifying and breaking off parts of our monolith. The road we’re walking isn’t a new one, it’s a well trodden path mapped out by Sam Newman who literally wrote the guidebook on Microservices . Borrowing concepts and techniques from Domain Driven Design we start by running event storming workshops which help us identify our business subdomains (or bounded contexts). Following mapping out the subdomains we annex areas of our monolith in tandem with adding complete functional test coverage that captures all associated business rules. With strict separation of concerns in place and the safety net of tests we are now ready to migrate our associated database. Given that our subdomain now has sole ownership over its data it’s now time to create our new Microservice reusing our functional tests. Finally we look to remove any redundant code from the monolith and then you’re done! Hey! Oh! Let’s go! How do we identify our business subdomains? Let me ask you a question. Assuming you have a monolith if you were to imagine your monolith in pictorial form what would it look like? Like this? Or this? At the beginning of re-architecture we run Event Storming workshops which greatly aide us to identify the various business subdomains that we want our software to model. Event Storming is described by its inventor Alberto Brandolini as … a workshop format for quickly exploring complex business domains. It’s a new technique, only surfacing in late 2013, but has become popular in Domain Driven Design circles as a way of capturing actors and events within a system. We’ve run it a couple of times and have found great value in the results it has given us. Once we have our ideal subdomains mapped out we can start refactoring the code within the monolith into well defined chunks. The intent of the refactoring is to annex complete services within the monolith. The next stage of the monolith might look a little bit like this with a couple of annexed internal services but still a good portion of spaghetti code. At this point you might be thinking but I don’t want to write lots of code in my monolith, that’s why I’m moving to Microservices. If that’s the case then read on. This seems like a lot of work to do to an application that we want to replace, why don’t we just start with writing the services from scratch? This is such a good question and one that I find challenging to argue against. There’s a reason we’re moving to Microservices and that’s because our technical debt is wildly out of hand in our monolith. Writing and reasoning about code in the monolith is a monumental task and it just seems so tempting and easy to write a new service. On the surface of it writing a new service seems much simpler but I’ll try to explain, by digging a little deeper, why I think you should start in the monolith. Here we have our monolith. The red line represents a chunk of functionality that could be refactored into a new subdomain or written as a new service. As you can see it’s intertwined with every part of the system from the view down to the database. Imagine that we write a new service from scratch that connects to the same database like so. The new service completely replaces the functionality depicted by the red squiggly line in the diagram. This poses a few interesting questions. If you change the schema of the database will your new service break if it relies on that schema? Can you be one hundred percent sure you’ve captured all the business rules and implemented them like for like replicating all functionality from the monolith? How do you get rid of that squiggly red line that represents a thread of functionality and replace it with calls to the new service? Can you be sure you’ve found everything and there’s nothing in the monolith making sneaky changes to the data managed by your new service? Quite frankly I don’t want to be the one answering those questions when something inevitably goes wrong and you’re adversely affecting the company’s revenue stream and irreparably damaging delicate customer relationships. With this in mind let’s look at an alternative approach. How do we create a subdomain within our monolith? Assuming you agree with me that we should start in the monolith we’re back to the point of having our desired subdomains identified through Event Storming. The next step is to isolate those subdomains through refactoring into annexed internal services. This is painstaking work and a substantial amount of rigour is required in performing refactoring on such a massive scale. Imagine a big messy monolith like this with lots of coupling and shared dependencies. We start off by pulling on the threads in the monolith and unpicking and unwinding them out into annexed services. Each annexed service should have a single API to access the public functions of the subdomain. This is the single point of contact with the monolith with all other code being encapsulated and hidden within the subdomain. With each new subdomain that we model we also aim to capture all the business rules in Cucumber tests which live with it. These tests are full functional tests written in ubiquitous language at an API level so that they are portable and reusable when we come to extract a Microservice. Eventually you will hopefully find yourself in a state depicted by the diagram below. At this point we’re doing a lot better but we still have a shared database and we still want to enforce this separation of concerns with a hard infrastructural boundary. Next, onto the database. How do we tease apart the database? You’ll have to find a way to break your database into chunks if you want to break off a chunk of the monolith and not have a shared database dependency. There’s a few techniques you can use here. In this example I’m assuming a SQL database migrating to a SQL database but there’s many other options out there for migrating from SQL to NoSQL that I won’t cover here. One approach we’re using is to slowly migrate data within your main database. Making non-destructive changes until you have a set of tables under a new schema that reflect the schema you ideally want. You are now in the situation of your data being completely segregated from the main database. Your annexed service is connected to those new tables under the separate schema. Finally you can migrate the schema to a new database and you might end up with something like this. The unused columns and tables in the main database can be deleted over time making sure to give teams like analytics time to update their models to reference the new database if they’re reliant on the existing data and schemas. Finally, how do we extract a Microservice from the Monolith? At this point you’ve got a couple of options. You can take your existing code and move that to run under a new project and repository. In our case we’re moving from C# to Elixir for our standard server technology. In the diagram below you can see a service that has been implemented to support the same API as the service within the monolith. This is only possible in a safe way when you have a full suite of functional tests that capture all the business rules. You can develop against these and once they’re all passing that should give you confidence that you’ve implemented a like for like service. The pink dot represents the forking point and we’re looking to use some sort of feature toggle to perform a canary release. This allows us to test that the new service stands up to real users. A great overview of this technique has been written up by github (http://githubengineering.com/move-fast/) with the usage of their scientist library. Hopefully everything is going well and you can move to using the new service for one hundred percent of users. This now represents the state of play in your system. The only thing left do is to delete the redundant code from the monolith and you’ve broken out your first Microservice. That’s it, you’re done! I sincerely hope you find this useful, I don’t want to be prescriptive, this is just our approach right now which will undoubtedly change as we learn more by doing further monolith refactorings and Microservice extractions. We build our software with NodeJs , ReactJs , Elixir and Phoenix . If you like the sound of what we’re doing here please apply as we’ve got open software engineering positions in both Dundee and London .", "date": "2016-10-05"},
{"website": "FindMyPast", "title": "Feature Toggles and A/B testing", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/feature-toggles/", "abstract": "Overview Here at FMP, we practice trunk-based development, with changes committed to the main trunk being deployed by our continuous integration server onto our acceptance, integration and finally production servers.  We commit to trunk multiple times daily, and provided that the deployment pipeline is green, that also means we deploy those changes to our production servers multiple times daily. Trunk-based development can be scary for developers and product owners; changes to the code base are deployed to live and, if a feature is not ready for release, how do we control the release of a new feature or hide a feature still in development? The answer is feature toggling and the use of feature toggles allows us to safely commit all changes to trunk without accidently exposing unwanted features to our end user. Trunk based development along with feature toggling allows us to separate the process of deployment (automated by our continuous integration server) from the process of release (configured via feature toggles). If you are unaware of feature toggles then this Pete Hodgson article on feature toggles is a good place to start. For more details on trunk based development, see this Martin Fowler article. Feature toggle implementation in FMP Feature toggles are – on the whole – used exclusively at the UI layer to either: Hide a feature that is still under development. This we would call a feature toggle. Perform A/B testing on UI elements to determine which approach works best. This we call an experimentation toggle. Feature toggles are generally not used elsewhere in the stack, the UI is our point of contact with the end user and it’s clear that any entry points into a feature still under development need to be hidden there. Code in the API and micro services layer is typically not controlled via toggles because it is far easier to hide to the code changes deeper within the stack.  We could use them, it’s just that generally we don’t need to worry for feature development. Technology choices Before we can start to use feature toggles in our UI, we needed some way of managing the available feature toggles. In fact, we had a whole shopping list of requirements for the feature toggle service including: Easily create and set the state of a toggle depending on the environment requesting the toggle E.g.: A toggle may be on for the integration environment but off for the production environment. Canary releases – Roll out a new feature to a small subset of users and then to a larger group Targeted release – Release a feature to only a specific cohort of users A/B testing – Ability to perform experiments based upon the feature toggle. An API or SDK available – we also practice repository driven development , so it’s important to have a service that exposes an API that we can leverage to programmatically create feature toggles and manage those toggles. After a lot of internal discussion, including debates on whether we should develop our own feature toggle service, we eventually decided upon a SaaS called LaunchDarkly . It fits nicely into our technology stack, ticks a number of boxes from our shopping list including an API endpoint to allow us to build automated tools in order to manage our feature toggles. It also has a number of SDKs available, including a Node JS SDK and a client-side JavaScript SDK. Our web application is built using React and GraphQL/Relay. The LaunchDarkly JavaScript SDK client is embedded in the page allowing each component the ability to query LaunchDarkly for a feature toggle state. In reality, components rarely interact with the LaunchDarkly client but instead interact with a module which wraps the JavaScript client and provides an easy method of displaying a component based upon the value of a toggle. Let’s take a contrived example and assume that we are updating our registration page. We want this work to be visible to our product owners via our internal integration server but hidden from our customers on our production server. The entry point to the new registration pages is via a Free Registration button which looks exactly the same on both the production and integration servers. First, we create a feature toggle in LaunchDarkly – let’s call it new-registration . A feature toggle created in LaunchDarkly is available for use across all environments but the state of the toggle can be configured per environment. In our example, we configure the feature toggle to be on for the integration environment and off for production. Within the React page, we would want to render either the existing registration page or the new registration page. Two button components are required here; one button component would, when clicked, navigate to the existing registration screen while the other would navigate to the new registration page. First, we bring in a module that wraps the LaunchDarkly JavaScript SDK client to avoid components having a dependency on LaunchDarkly. import { toggleAB } from ' toggler ' ; The toggler module is pretty straightforward. Here it is in it’s entirety: /* global featureToggleClient */ export const toggleOn = ( ReactComponentA , toggleName ) => featureToggleClient . state ( toggleName , false ) ? ReactComponentA : () => null ; export const toggleAB = ( ReactComponentA , ReactComponentB , toggleName ) => featureToggleClient . state ( toggleName , false ) ? ReactComponentA : ReactComponentB ; ( featureToggleClient is just a wrapper around LaunchDarkly’s SDK.) Next, we create our two buttons which are pretty similar except for the navigation address: const existingRegButton = () => < Button className = ' register ' to = ' /register ' > Free Registration < /Button>; const newRegButton = () => < Button className = ' register ' to = ' /new_register ' > Free Registration < /Button> ; We use the imported toggleAB to determine which component to render, depending on the state of the ‘new-registration’ feature toggle. If the flag is true then the newRegButton is returned, otherwise the existingRegButton is used. const RegistrationButton = toggleAB ( newRegButton , existingRegButton , ‘ new - registration ’ ); Finally, inside the render function, render the returned RegistrationButton component: render (){ return ( < div > < div > ... page content here … < /div > < div > < RegistrationButton /> < /div > < /div > ); That is essentially it from the React side of things. When the web application is running on our servers the application knows the environment it’s running under and configures the LaunchDarkly JavaScript client accordingly. So, when running under the integration environment, the LaunchDarkly client is configured for integration and gets the state of the new registration feature flag for the integration environment. In our example, the state is set to on and so the newRegButton is rendered. Similarly, when running under the production servers the client is configured accordingly and the request for the state of the feature toggle from the production environment is returned. For production users, the existingRegButton is returned and the end user is directed to the existing registration page. Once a feature is complete we can configure the feature toggle on the production environment to set the state to on (in which case all end users will see the new page) or we canary release and roll out the feature to a certain percentage of users – say 20% get a feature toggle state of on while the remaining 80% get a state of off . We can do all of this via the LaunchDarkly UI (or even programmatically via the LaunchDarkly API) without any React code changes or even a re-deployment of the code. Making a feature live to our end users is as simple as flipping a switch on the LaunchDarkly UI and that state change is reflected immediately on our web application. The toggleAB function we use in the react page can also be used for A/B experiments. Instead of having a button navigate to a different page, we might want to run an experiment on the colour of a button, or the text displayed in the button. (E.g.: does the text Register for free drive more users to register than Register here ?) LaunchDarkly has functionality to set goals and assign those goals against the feature toggles. The JavaScript client can UI interactions (such as element clicks) and sends events to LaunchDarkly when a goal has been met (e.g.: A button has been clicked). LaunchDarkly tracks page impressions, element clicks, etc and provides statistics on how many users clicked the button when the feature flag is on or off. Feature toggle maintenance One of the criticisms with using feature toggles is that the base code gets covered in feature flag calls that are never tidied up. We could also end up with a lot of feature flags that end up never being removed since developers aren’t sure if these flags are safe to remove.  The end result is a mess of code that is difficult to maintain. LaunchDarkly can help here – it provides useful information on the state of a feature toggle. The screenshot below shows the LaunchDarkly UI hinting that a feature toggle looks like it could be a candidate for removal. FMP are developing a tool – code name nagger – which interacts with the LaunchDarkly API and retrieves information on the state of the feature toggles and reports those states back to the engineers. The idea is that any stale feature toggles are reported back to the engineers via a Slack channel. The tool starts to “nag” the engineers to remove stale toggles and the nagging will get louder the longer the stale feature toggle is in the system. Nagger will fail a CI build if it detects stale feature toggles that are older than say, one month for example. Hopefully, the constant reminders about stale toggles, along with the threat of a failed build, should ensure that toggles are removed from the source code and from LaunchDarkly when no longer required. Feature toggles performance One of the concerns from the engineers around feature toggling is the performance of the 3rd party service. How does this affect page load speeds and execution times? Will the requests for feature toggle slow down code execution speed? These are valid concerns – waiting on a 3rd party SaaS is time consuming and potentially brittle. What happens if the 3rd party site is unavailable? Slow network latency? Helpfully, LaunchDarkly has been designed to be fast and resilient – it even works if the LaunchDarkly website is unavailable.  (See the LaunchDarkly FAQ for more details on performance.)  Performance of feature toggle request is fast - all toggle states are cached within the client so no remote requests are required to get the feature toggle state. Updates to the state of the toggle (for a user) are streamed from the LaunchDarkly servers to the client using server-sent events . This means that, provided we have a network connection, changes to the feature flag state set via the LaunchDarkly UI are reflected almost immediately in the JavaScript client. However, how do the feature toggle states get into the JavaScript client in the first place? As part of the page load we need to initialise the LaunchDarkly client and that does involve a remote call to LaunchDarkly. In fact, we need to wait for the client to emit the on(‘ready’) event before we can start to ask for feature toggle states. This can slow down the page load speed considerably (LaunchDarkly docs suggest 100ms or more latency here). The React component cannot render the page until the client is initialised, but a 100ms latency wasn’t acceptable. Once again, LaunchDarkly has a solution. During initialisation of the client, we can bootstrap the client with default values for the feature toggle states. Bootstrapping means that the on('ready') event is fired immediately and we don’t need to wait around for any remote calls. But, where do we get the initial set of toggle states from? We need to get the correct toggle states because we want to ensure that the correct component is rendered, but the only way to get the correct states is to call LaunchDarkly. We solved this problem using GraphQL/Relay and another microservice named Flipper. The diagram below shows the solution: Flipper is a RESTful service that, among other things, serves feature flags gleaned from LaunchDarkly. (It also allows us create feature flags, set state, etc). Flipper exposes an endpoint that returns all the feature toggle states for a user. Because it is a microservice, the Node JS client within Flipper is already initialised with LaunchDarkly and so any requests for feature toggle states are super quick.  (Response times from Flipper are < 5ms and we are working to make that faster). The React app composes a GraphQL query that defines the list of feature toggle states for a user.  GraphQL calls down to our API level to return the data. The API layer, in turn, calls down to Flipper.  The feature toggle states returned from the GraphQL query is then used to bootstrap the LaunchDarkly JavaScript client with the correct set of feature toggle states.  Average response times from the GraphQL call are typically < 30ms. So, that’s how feature toggling is performed at FMP. We would love to hear from you on how you have solved the problem of continuous delivery and trunk based development.", "date": "2016-11-15"},
{"website": "FindMyPast", "title": "FMP goes to the Elixir Meetup group in London", "author": ["German Munoz"], "link": "https://tech.findmypast.com/fmp-goes-to-elixir-meetup/", "abstract": "After work on Wednesday 25th January, a cohort of FMP engineers made their way to the Skills Matter Code Node near Moorgate tube station in London. They were on their way to participate in the London Elixir Meetup group and to hear about all things Elixir. The evening had 2 presentations scheduled, “Flexible Elixir” and “Deploying Elixir”. Flexible Elixir The first talk, “Fleixble Elixir” was by the head of Elixir at [Erlang Solutions Ltd] (https://www.erlang-solutions.com/), Claudio Ortolina. We had 2 consultants from Erlang Solutions a few months back, so we know they are Elixir experts. Claudio said the original name of the talk was “Idiomatic Elixir”, but he changed it to give people a better feel for the content, which is to show how to do things the Elixir way. They covered some basics like GenServer, keyword lists and railway oriented programming in Elixir. The team’s comments on the talk was that while it was informative, no topic was really covered in-depth. Having been doing Elixir for a few months now, it wasn’t necessarily anything new for our engineers. Nonetheless, it was useful to confirm that we’re already doing a lot of the best practices recommended by Claudio. Deploying Elixir The next talk, “Deploying Elixir”, was by Tetiana Dushenkivska, a software engineer at Inflowmatix in Southampton. The team thought this talk was more interesting since it covered tools we hadn’t used before. Tetiana covered using Distillery, a deployment tool for deploying Elixir. It lets you build your code into a .tar package and deploy to the server. It is a rewrite of an earlier Elixir release tool, Exrm. Tetiana also covered how it could be used inside Docker, thought didn’t provide much detail as her team has only started using it for this purpose. We have our own Docker solution here at FMP, but it was still interesting to see what else is out there. Tetiana’s slide deck can be found online [here] (https://speakerdeck.com/tetiana12345678/elixir-deployment?slide=1). Overall the team enjoyed the event, it was fun to get to meet the ever expanding Elixir community. They also commented on the FREE BEER (thank you Erlang Solutions!). Special thanks to our devs Elliott Jenkins and Diogo Silva for their help with this article.", "date": "2017-02-22"},
{"website": "FindMyPast", "title": "Pursuing mastery as a software engineer - part 2", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/pursuing-mastery-as-a-software-engineer-part-2/", "abstract": "Welcome to part 2, in part 1 I covered the great work of Anders Ericsson , Angela Duckworth , Carol Dweck and Charles Duhigg . Their work helps us understand how we, as human beings, learn and master new skills. They also teach us the growth mindset we must adopt in order to do so. I will now work backwards through the stages of the mastery loop as I’ve applied it to myself in learning to be a better JavaScript engineer. Mental representations These are what we seek. One could define a mental representation as a conceptual structure designed to sidestep the usual restrictions that short-term memory places on mental processing. — Anders Ericsson As you train, whether physically or mentally, you are sharpening your mental representations . For instance, when building a complex computer program it will probably be assembled with multiple techniques, patterns and libraries. If you deliberately practice increasing your skill in each technique, pattern and library, you will build finer mental representations. With continued practice those mental representations will continue to improve and become second nature to you. You might compare this to a musician practicing scales. When they see a grouping of notes on the page they can instinctively perform the actions to reproduce those notes. This ability allows them to focus on the subtlety of the sound they wish to create, not the mechanics of playing the notes in the right order. Having quick access to mental representations like programming techniques, programming patterns and in-depth knowledge of common libraries enables you to work on a higher level. What once was hard becomes easy allowing you to battle more complex problems with your ever growing arsenal of mental representations. Deliberate practice, practice, practice This is what you need to do. I know quite certainly that I myself have no special talent. Curiosity, obsession and dogged endurance, combined with self-criticism, have brought me to my ideas. — Albert Einstein Let’s focus on dogged endurance. Deliberate practice is hard work. Sitting down and focusing on one small thing, really working towards deep knowledge, is a challenge. The 5 Elements of Effective Thinking is a great book by two mathematics professors on how to gain much deeper understanding of a particular subject. Their arguments and techniques align well with deliberate practice. The authors create a mental model of the elements (earth, fire, water, air and quintessence) to help you in your learning and practice. To quote one of the authors. Deep work on simple, basic ideas helps to build true virtuosity—not just in music but in everything. — Edward B. Burger An example could be functions in JavaScript. Do you have a deep enough understanding to teach it to someone else? Could you sit down and write a good summary of that feature of JavaScript? If not spend an hour researching it deeply until you understand it from first principles. Building a bedrock of mental representations will help you with the more complex techniques as you start to stack your representations on top of each other building towards mastery. This is where our daily learning hour at Findmypast comes in. We need to support all our engineers in refining their mental representations. I’m a big believer in the phrase “Every Damn Day” and developing a habit of learning every day. This quarter I’ve been working through the exercism track on ECMAScript as my daily deliberate practice. A training program and making a scoreboard Stay the course. This is a fundamental truth about any sort of practice: If you never push yourself beyond your comfort zone, you will never improve. — Anders Ericsson Lets be honest, it’s difficult to go outside your comfort zone. It is easy to give up when things get hard. Training plans help you stay the course. You might decide to do a certain number of exercises in a week. You might decide to read a certain number of chapters of a book the following week. Keeping a scoreboard of your work towards your goal can shine a light on the progress you’re making. A common trick of athletes is to publically state their intentions. At the beginning of the season they’ll state their goals to achieve. If they fail to do their training they’ve not only let down themselves but everyone else that they have told. This makes them work towards their goals with added focus and vigor. You might not tell everyone in the office but you might tell your team that this quarter you want to improve a particular aspect of your skillbase. You could publically show your scoreboard that shows your progress towards your goals. Finally having a coach that holds you accountable for your weekly commitments towards your goal is solid step to keeping your focus. These are useful techniques to help you form a habit of practicing every day. This is the scoreboard that I created for my JavaScript learning this quarter. I set myself a goal of reading 3 textbooks and completing 30 exercises. I’ve been seeking feedback on each completed exercise from colleagues that I trust. This has helped me refine my solutions to be more readable and useful. I can already feel my skill in JavaScript growing even after only a couple of months of deliberate practice. However I’ve not been doing my prescribed reading and I’m working to find ways that I can reliably hit my weekly goal of 3 chapters a week. Setting goals and finding a coach Pick a goal. Clarity about what matters provides clarity about what does not. — Cal Newport I find identifying an area where I want to train my mind an incredibly daunting task. There’s so much that I could turn my hand to that quite often I get paralysed. As a result I end up casually dipping my toe in lots of different areas. I usually read enough to be able to talk about the concepts but never gain the depth of understanding that I crave. I want the depth of knowledge that allows me to be confident in teaching it to others. I find it difficult to stay focused and lack the mental fortitude not to get distracted and chase the shiny new thing. I find the hardest aspect is saying no to so many interesting things and picking one thing to say yes to. For this quarter I’ve said yes to JavaScript and I’m actively trying to say no to everything else. To begin with I felt like I was missing out but now I feel the benefit of my focused deliberate practice it far outweighs any initial angst. The best way to get past any barrier is to come at it from a different direction, which is one reason it is useful to work with a teacher or coach. — Anders Ericsson If you find setting personal goals a challenge then a great first step could be to get a coach. Pick someone you respect and trust who’s seen your work. Hopefully they will be able to help you identify your weaknesses. My mentor Andy (our CTO) helped me create a mind-map of the things that matter most to me. Once that was in place he helped my set my immediate goals and plans for the quarter. I liked the model proposed by Angela Duckworth in her book Grit. Below is a diagram I made to help me in prioritising what I should work on in my personal learning time. Self criticism and expert critique Seek feedback. As soon as possible, experts hungrily seek feedback on how they did. Necessarily, much of that feedback is negative. This means that experts are more interested in what they did wrong—so they can fix it—than what they did right. The active processing of this feedback is as essential as its immediacy. — Angela Duckworth Reflecting back on your deliberate practice is essential. Identifying mistakes in your thinking is core to improving. A trusted coach is a great way gain critique on your work. Having someone to explain a different approach to what you’ve implemented, or point out an optimisation you’ve missed greatly helps shortcut you to deeper mental representations. Exercism helps me generate fast critical feedback as it lets others comment on my solutions to exercises. This creates a short feedback loop to the practice I do each day. That’s it, that’s all So there you have it. Currently that’s everything I’ve learned about learning. Figure out what’s important to you. Find a coach. Build a plan of improvement. Track your progress. Deliberately practice. Actively seek critique. Make a habit. Walk firmly on the path towards mastery. If you like the sound of what we’re doing here, we build websites with React , Relay , Webpack , and Babel . Elixir and Phoenix are the core of our platform. If you love learning then we’ve got a lot in common. We look for hard working developers. Have a look at our jobs and drop us a line. I’d love to hear your comments on your learning strategies and experiences.", "date": "2017-03-06"},
{"website": "FindMyPast", "title": "A Step by Step Guide to Building a Form Using Relay and GraphQL. Part One: Mutations", "author": ["Leigh Shepperson"], "link": "https://tech.findmypast.com/build-a-form-using-relay-part-one/", "abstract": "Relay is a powerful framework created at Facebook that can efficiently talk to a GraphQL server. Nevertheless, it has a reputation for being difficult to learn, primarily for the following reason: queries are easy but mutations are hard. This is compounded by the fact that the majority of examples tend to be overly complicated and the reader can easily get lost in the details. Nevertheless, with a bit of effort, once the basics are learnt the process becomes intuitive. This tutorial is intended to give an immediate feel for creating mutations in Relay and Absinthe, an Elixir implementation of GraphQL. We will provide a step by step guide to building a simple form that uses a mutation to authenticate a user. In part one of this tutorial, we will only focus on mutations. Prerequisites The main technologies we will use are absinthe and absinthe_relay and they respectively provide us with an implementation of GraphQL for Elixir and support for the Relay framework. We will assume that the reader has enough basic knowledge of these technologies to create queries and consume them using Relay. For further information, absinthe GraphQL tutorial is the recommended place to start. Step One: Create a GraphQL Mutation In this section, we will create a mutation that has two input fields and one output field. The two inputs are email and password and the output is a Boolean called success . The resolve function will return true if the email and password respectively match “test@test.com” and “password”; otherwise it will return false . We create the mutation after this brief discussion: The payload macro from absinthe_relay provides support for creating fields that satisfy Relay’s common mutation pattern . This pattern is not very complicated; it just means that each mutation is a root field on the mutation type, it has exactly one input called input and both the input object and the output object have a field called the client mutation identifier that is used to match up the response to the request. Note, it is helpful to think of the payload as the output of a mutation. The payload macro takes care of these implementation details. At a minimum, it just needs you to name the mutation, specify the input fields, specify the output fields, and provide a resolve function. Thus, the mutation for our form looks like this: payload field :login do input do field :email , non_null ( :string ) field :password , non_null ( :string ) end output do field :success , non_null ( :boolean ) end resolve fn %{ email: \"test@test.com\" , password: \"password\" }, _ -> { :ok , %{ success: true }} _ , _ -> { :ok , %{ success: false }} end end Note that our implementation has a resolve method that always returns the success tuple. This is because the error tuple is reserved for developer errors and all validation errors should form part of the payload. We will expand on this in part two of this tutorial. Step Two: Test the Mutation in GraphiQL In order to get a feel for the mutation, we will test it in GraphiQL. At a first glance, the syntax for creating mutations is vastly different from the syntax for creating queries.\nHowever, the thing to remember is that each mutation has exactly one input called “input” and it is a root field of the mutation type. So, we just need to specify the payload fields and create the input object for the mutation. Note, we also specify that the input to the mutation is of type LoginInput . To understand more about this object, it is helpful to take a look at the input and payload types that have been created in the schema: input LoginInput {\n  email: String!\n  password: String!\n  clientMutationId: String!\n}\n\ntype LoginPayload {\n  success: Boolean\n  clientMutationId: String!\n} Thus, the syntax for the login mutation is given as follows: mutation LoginMutation($input: LoginInput){\n  login(input:$input){\n    success\n  }\n} with query variables {\n  \"input\": {\n    \"email\": \"test@test.com\",\n    \"password\": \"password\",\n    \"clientMutationId\": \"someId\"\n  }\n} Note, as part of the input object you have to specify the client mutation identifier ( clientMutationId ). This makes sense given the discussion of the relay common mutation pattern in the previous section. In fact, if you request the client mutation identifier as part of the payload, you will find that the two values are the same. Step Three: Create a Relay Mutation There are four functions that you need to know when creating Relay mutations: getMutation , getVariables , getFatQuery and getConfigs . The following relay mutation guide contains information about these functions in greater depth, but we will now give a brief illustration of these as follows: The getMutation function tells GraphQL what mutation you want to perform and getVariables returns the input variables used by the mutation. The getFatQuery function tells Relay what could change as a result of the mutation and the getConfigs function describes how Relay should handle the payload returned by the server. There are various technicalities surrounding these functions, for example, Relay intersects the information defined in the fat query with a “tracked query” to efficiently fetch data from the server, but we won’t worry about them here. (See relay mutation guide for more information) In our example, we do not want to write the success field into the client store, but we do want to have access to it so we can appropriately handle the response from the server. In this situation, the ideal type for the getConfigs function is REQUIRED_CHILDREN . Thus, the Relay mutation looks like this: import Relay from 'react-relay';\n\nexport default class LoginMutation extends Relay.Mutation {\n\n  getMutation() {\n    return Relay.QL `mutation {\n      login\n    }`;\n  }\n\n  getVariables() {\n    return {\n      email: this.props.email,\n      password: this.props.password\n    };\n  }\n\n  getFatQuery() {\n    return Relay.QL `\n    fragment on LoginPayload {\n       success\n    }`;\n  }\n\n  getConfigs() {\n    return [{\n      type: 'REQUIRED_CHILDREN',\n      children: [Relay.QL`\n        fragment on LoginPayload {\n          success\n        }`]\n    }];\n  }\n} Step Four: Build the Form Component Given the Relay mutation defined in the previous section, we will proceed to build a component that contains a form that enables us to authenticate a user. The form will have two input boxes, one for the email address, the other for the password. It will also have a submit button. At this stage, the implementation is quite simple; we just console log the response: import React, { Component, PropTypes } from 'react';\nimport LoginMutation from '../login-mutation';\n\nexport default class LoginComponent extends Component {\n  constructor() {\n    super();\n    this.state = {\n      email: \"\",\n      password: \"\"\n    };\n  }\n\n  handleSubmit(e) {\n    e.preventDefault();\n\n    const onSuccess = (response) => {\n      if(!response.login.success) {\n        console.log(\"Email or Password is not valid\");\n      }\n      else {\n        console.log(\"Success\");\n      }\n    };\n\n    const onFailure = (transaction) => {\n      console.log(transaction.getError());\n    };\n\n    const mutation = new LoginMutation(this.state);\n\n    this.props.relay.commitUpdate(\n      mutation, {onFailure, onSuccess}\n    );\n  }\n\n  handleChange(name, e) {\n    let change = {};\n    change[name] = e.target.value;\n    this.setState(change);\n  }\n\n  render() {\n    return (\n      <div>\n        <form onSubmit={this.handleSubmit.bind(this)}>\n          <input type=\"text\" value={this.state.email} onChange={this.handleChange.bind(this, 'email')} />\n          <input type=\"text\" value={this.state.password} onChange={this.handleChange.bind(this, 'password')} />\n          <input type=\"submit\" value=\"Submit\"/>\n        </form>\n      </div>\n    );\n  }\n} Part Two: Server-Side Validation In part two, we will extend this example to include server-side validation and handling transaction failure. The main change will involve modifying the getConfigs function to be of type FIELDS_CHANGE and including validation messages as part of the payload.", "date": "2016-11-28"},
{"website": "FindMyPast", "title": "SQL Server in Elixir, Part 1: Connecting", "author": ["Jae Bach Hardie"], "link": "https://tech.findmypast.com/sql-server-in-elixir-connection/", "abstract": "At Findmypast we are currently engaged in a major rearchitecture process, moving\nfrom an architecture of a small number of tightly coupled C# servers to a network\nof Elixir microservices. One of the big gains we hope to achieve from this is to\nmake our database architecture easier to modify and scale by splitting out our\nhuge and unwieldy Microsoft SQL Server databases into smaller units. Each unit\nwill be specialised for a specific task and accessed solely by one microservice. To migrate live databases with zero downtime our plan is to design our\nservices to write to both the legacy and new databases at once but continue to\nread from legacy. Then, once we are satisfied that data and responses will be\nconsistent from the new database, reads will be switched to happen from it and\neventually the dual write switched off and the deprecated legacy tables archived.\nThis means our Elixir servers need to be able to communicate with both Microsoft\nSQL Server and the open source database technologies (primarily PostgreSQL) we\nwill be migrating to using roughly the same interface. Ecto is perfect for this but there is a major stumbling block: there is no Ecto 2.0\nready adapter for SQL Server. So we are writing our own database connection layer and Ecto adapter . We could have tried patching the existing TDS adapter but we were wary of delving into a native Elixir TDS implementation that has\nbeen stale for a year. We wanted to use Microsoft’s own ODBC driver for Linux\nto benefit from first-party support and not have to maintain a binary protocol\nimplementation. So we decided to base our adapter around OTP’s existing :odbc application (this application turned out to be lacking in some glaring areas but\nthat’s a story for another time). We hope this series of blog posts will both document our process and provide\nan informal resource for other people wishing to implement new Ecto adapters. Dependencies The Erlang ODBC application is usually not installed by default, so we had to\ngo through our respective package managers to find erlang-odbc or similar.\nThis package usually depends unixodbc , the ODBC driver manager for Linux, but\nif it didn’t we’d have to install that as well. Finally, we needed the latest SQL Server driver for Linux, which we got from Microsoft’s repositories .\nNote that the driver isn’t open source. We don’t have a problem with this\nbut if you’re a purist and still want to follow along the FreeTDS ODBC Driver is a serviceable open source alternative (although I cannot\nguarantee all the example code will work with no modifications). Establishing a DBConnection DBConnection is a\nlibrary that implements much of the logic needed for handling database\nconnection pooling. The Ecto SQL adapter base which we will be using to\nbuild out SQL Server adapter expects the connection protocol to implement\nthe DBConnection behaviour, so that was our starting point. Starting from the start, the first callback we need to implement is connect/1 .\nThis function should establish a connection to the database and return a\nreference to it in the state object so it can be used throughout the\nlifetime of the connection. It has the following signature: @spec connect ( opts :: Keyword . t ) :: { :ok , state :: any } | { :error , Exception . t } To connect to a database using the :odbc application, the relevant function\nis :odbc.connect/2 . It accepts a connection string (a charlist in Elixir terms,\nof course) that will be parsed by the ODBC driver manager. We can look at the\nSQL Server documentation for the format of this string, and find: Driver={SQL Server};\nServer=myServerAddress;\nDatabase=myDataBase;\nUid=myUsername;\nPwd=myPassword; All of these should be configurable, but for ease of use we provide some\ndefaults: [ { \"DRIVER\" , opts [ :odbc_driver ] || \"{ODBC Driver 13 for SQL Server}\" }, { \"SERVER\" , opts [ :hostname ] || System . get_env ( \"MSSQL_HST\" ) || \"localhost\" }, { \"DATABASE\" , opts [ :database ] || System . get_env ( \"MSSQL_DB\" )}, { \"UID\" , opts [ :username ] || System . get_env ( \"MSSQL_UID\" )}, { \"PWD\" , opts [ :password ] || System . get_env ( \"MSSQL_PWD\" )} ] |> Enum . map_join ( fn { key , value } -> \" #{ key } = #{ value } ;\" end ) |> to_charlist Having the connection parameters default to environment variables allows the\nsame application to target different databases depending on how it’s deployed.\nThis is very useful for things like integration v production environments. :odbc.connect/2 also accepts a keyword list of options for the Erlang :odbc application itself. Of interest to us are: :tuple_row : Defaults to :on and rows are returned as tuples. This is\nvery awkward (and not what Ecto expects). We’ll turn it :off to get our rows\nas lists. This will enable us to Enum.map/2 over them, amongst other things. :extended_errors : Turning this :on returns more information on errors,\nwhich we can use to catch and handle specific exceptions. :binary_strings : You’d think turning this :on and communicating with the\ndatabase in binaries instead of charlists would make things easier. However,\nit means you have to manage character encodings instead of :odbc doing it\nfor you, so we’re leaving it :off for now. So that leaves our function as: def connect ( opts ) do conn_opts = opts |> Keyword . put_new ( :tuple_row , :off ) |> Keyword . put_new ( :extended_errors , :on ) conn_str = [ { \"DRIVER\" , opts [ :odbc_driver ] || \"{ODBC Driver 13 for SQL Server}\" }, { \"SERVER\" , opts [ :hostname ] || System . get_env ( \"MSSQL_HST\" ) || \"localhost\" }, { \"DATABASE\" , opts [ :database ] || System . get_env ( \"MSSQL_DB\" )}, { \"UID\" , opts [ :username ] || System . get_env ( \"MSSQL_UID\" )}, { \"PWD\" , opts [ :password ] || System . get_env ( \"MSSQL_PWD\" )} ] |> Enum . map_join ( fn { key , value } -> \" #{ key } = #{ value } ;\" end ) |> to_charlist :odbc . connect ( conn_str , conn_opts ) end It works! We can call DBConnection.start_link on our module and get a ref to\nthe process. There’s a bit of a problem, however. A line in the :odbc.connect/2 documentation should worry us: The connection is associated with the process that created it and can only be\naccessed through it You might think that’s OK. Surely the DBConnection process will own\ncommunication with :odbc , just as intended. The DBConnection docs explain\nwhy this assumption is incorrect: DBConnection handles callbacks differently to most behaviours. Some\ncallbacks will be called in the calling process, with the state copied to and\nfrom the calling process. What this means is we cannot rely on the logic inside DBConnection callbacks\nbeing executed in the same process that connect/1 was executed in. Our :odbc functions will return {:error, :process_not_owner_of_odbc_connection} . Sad! In the next post we’ll cover wrapping :odbc in a GenServer to keep the calls\nin a single process while still having the bulk of the logic in DBConnection callbacks. See you soon!", "date": "2017-03-14"},
{"website": "FindMyPast", "title": "FMP co-sponsors London Elixir Meetup", "author": ["German Munoz"], "link": "https://tech.findmypast.com/fmp-co-sponsors-london-elixir-meetup/", "abstract": "We’re going Meetup-CRAZY here at FMP. Our devs not only attended the Feb 23 Elixir London Meetup, we officially co-sponsored it! In the heart of the City of London’s financial district, near Finsbury Square, the London Elixir fans came together that evening at Code Node , the Skills Matter 23,000 sqft Tech Events and Community venue. We were going to do this in style. The two talks presented that evening were “Making sense of time in distributed systems” by Ju Liu and “BEAM stack under my umbrella” by Emanuel Mota. Making sense of time in distributed systems The first talk, “Making sense of time in distributed systems”, dealt with techniques on how to handle timing and communication between processes in distributed system. The team considered this a really interesting talk, though definitely an advanced topic. It was mostly theoretical, covering things like Lamport timestamps and Vector clocks . It was geektastic. The pitch Before the talks started, our own Richard Kotze (a.k.a. Mobile Richard…a.k.a. Mobile…a.k.a. Mobes), gave a brief chat about the kind of work we do at FMP, and about our technology stack. FYI, we use lots of Elixir! As well as GraphQL and React. He also gave a shout out to our mighty FMP Tech Blog . And, oh yeah, we’re hiring ! The crowd also got another brief chat from the other co-sponsor carwow. But, we’re way better. :smiley: BEAM stack under my umbrella The second talk, “BEAM stack under my umbrella”, was more of a case study around how they built a chat app using Elixir. This app was able to make a restaurant order and enabled you to track the order in-app. The team enjoyed that it was a more practical talk, focusing on how they created the app, how they handled multiple events and aggregated them in a message queue system (RabbitMQ). They felt it was an intermediate level talk, and they enjoyed how they focused on what issues they ran into and how they solved them. Like the precious talk, there was a lively Q&A session immediately after. Also, since we were co-sponsors, we got to provide copious amounts of FREE BEER at the Meetup. You’re welcome, Elixir community! Oh, and did I mention WE’RE HIRING !!! If you love all things Elixir, or fancy going in that direction as part of your next job, let us know. Also, if you want to get more information about Elixir Meetups in London, check out they’re Meetup group: https://www.meetup.com/Elixir-London/", "date": "2017-03-17"},
{"website": "FindMyPast", "title": "Pursuing mastery as a software engineer - part 1", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/pursuing-mastery-as-a-software-engineer-part-1/", "abstract": "At Findmypast we have aspirations to radically disrupt the family and social history market. Our company vision is Connect our family’s past, present and future. In order achieve this lofty goal we are growing a truly amazing software engineering team full of wonderfully talented individuals. The craft of software engineering is evolving at an unnerving speed, and as such, we must constantly and incessantly pursue mastery to stay at the forefront. We encourage and support our engineers to spend time at work learning and mastering all aspects of building software. Every engineer at Findmypast is entitled spend one hour learning each day in the way that they find most effective. In that vein I’ve spent a good portion of the last six months educating myself about how we as human beings learn and master new skills. Could we apply the same training techniques that world class athletes, musicians and chess grandmasters use to hone their abilities? After reading Peak by Anders Ericsson I strongly believe that we can. He asserts that we all have the potential for greatness, what we need is hard work and a hell of a lot of it. My interpretation of his research can be modelled as a cycle of four parts. We must start with self reflection to identify an area of skill that we are deficient in. This is probably the most difficult part as we must put our ego aside and be truly honest about our abilities. Once you’ve identified what you’re going to improve you build a training program tailored to improving that skill. Finally we undertake the training program that involves regular deliberate practice, coaching and self analysis of the skill. This leads to us forming finer mental representations in our long term memory and an improved level of performance in that area. Hand in hand with Peak is the Mindset we adopt. Carol Dweck has a very simple mental model. Do you have a growth mindset or a fixed mindset? Let me ask you a question. Do you agree with this statement? No matter who you are, you can significantly change your intelligence level. If you do then you probably have a growth mindset, if not you probably have a fixed mindset. Over the last three decades both Ericsson and Dweck have carried out exhaustive research that leads to the conclusion that innate talent is fiction. With a growth mindset and deliberate practice we can increase our skill in whatever we choose to focus our efforts on. We use the idea of innate talent to protect our egos when comparing ourselves to people who’ve mastered a skill. It’s easy for us to say that someone is naturally talented and we’ll never be that good. It has nothing to do with the thousands of hours of deliberate practice those masters have dedicated to their field of expertise. It’s so much harder to say, I’ve got a thousand hours of practice to do before I’m that good. Learning isn’t a way of reaching one’s potential but rather a way of developing it. — Anders Ericsson We all have different rates at which we can pick up new skills, but the experts assert that we are all capable of being world class at whatever we put our minds to given enough of the right kind of practice. This leads nicely into Grit which describes the people that can stick with something no matter how hard it gets. Angela Duckworth’s field of research is what she has termed “Grit”. Generally people right at the top of their game are also incredibly gritty. They have a focus and drive that allows them to pour all their effort into their goals. As I read more about their deeds I found myself wishing that I was more like them. How does one acquire grit? How do I be more gritty? She describes a technique of keeping a hierarchy of goals for yourself. They help you keep focused on what’s important to you. Your low-tier goal lead towards attaining your mid-tier goals which lead towards you attaining your top-tier goal. I’ve found this a useful technique in setting my own goals. The final part of my source material is centered around habits. The Power of Habit explains how habits work, how to change existing habits, and how to create new ones. Charles Duhigg goes into some depth on the habits of individuals at the top of their fields. I believe that building a habit of deliberate practice around your chosen skill will help you towards building mental representations that will help you perform at an ever higher level. Champions don’t do extraordinary things. They do ordinary things, but they do them without thinking, too fast for the other team to react. They follow the habits they’ve learned. — Charles Duhigg Joining together all this research can help us use our time to its maximum potential. To understand more deeply. To make better decisions. To write awesome code. That’s the end of part 1 for pursuing mastery as a software engineer. In part two I’ll write about how I’ve put some of this into practice with myself. If you like the sound of what we’re doing here, we build websites with Elixir / Phoenix and JavaScript ( React , Relay , Webpack , Babel ). If you love learning then we’ve got a lot in common. We look for hard working and ever learning developers. We give you the time you need to learn our stack. Have a look at our jobs and drop us a line. If you’d like to know more, part 2 can be found here", "date": "2017-02-24"},
{"website": "FindMyPast", "title": "SQL Server in Elixir, Part 2: Process Management", "author": ["Jae Bach Hardie"], "link": "https://tech.findmypast.com/sql-server-in-elixir-gen-server/", "abstract": "Previously on this series, we’d used the Erlang :odbc application to establish a DBConnection connection to Microsoft SQL Server. Unfortunately, the way that DBConnection callbacks work conflicts with the :odbc restriction of only\nbeing called from the process where the connection was established or its\nchildren. We need a process that holds the connection to :odbc and passes messages on\nto it. Fortunately, Elixir makes the construction of processes really easy with GenServer . Our GenServer We’ll build a GenServer that wraps the three :odbc calls we’ll need to start\nwith, connect/2 , disconnect/1, and sql_query/2 . Connection and\ndisconnection will be mapped to GenServer.init/1 and terminate/2 respectively while sql_query/2 will be our sole handle_call/3 : defmodule Mssqlex . ODBC do use GenServer def start_link ( conn_str , opts ) do GenServer . start_link ( __MODULE__ , { conn_str , opts }) end def stop ( pid ) do GenServer . stop ( pid , :normal ) end def query ( pid , statement ) do GenServer . call ( pid , { :query , statement }) end # Callbacks def init ({ conn_str , opts }) do case :odbc . connect ( to_charlist ( conn_str ), opts ) do { :ok , odbc_pid } -> { :ok , odbc_pid } { :error , reason } -> { :stop , reason } # init should return :stop on errors end end def terminate ( _ , odbc_pid ) do :odbc . disconnect ( odbc_pid ) end def handle_call ({ :query , statement }, _ , odbc_pid ) do { :reply , :odbc . sql_query ( odbc_pid , to_charlist ( statement )), odbc_pid } end end That code should be pretty self-explanatory, although a glance through the GenServer docs is never amiss.\nThe server starts up an :odbc process and maintains a reference to it in\nstate. This way, no matter where Mssqlex.ODBC.query/2 is called from, :odbc.sql_query/2 will get called from the same process (with ref pid )\nthat established the connection. We’ve also taken advantage of the fact that the calls to Erlang functions are\nall in one place to make the conversion from Elixir to Erlang strings there so\nthey don’t clutter up our main logic. The implementation of DBConnection that we began writing in the previous post\ncan then be expanded to become: defmodule Mssqlex . Protocol do use DBConnection alias Mssqlex . ODBC def connect ( opts ) do conn_opts = opts |> Keyword . put_new ( :tuple_row , :off ) |> Keyword . put_new ( :extended_errors , :on ) conn_str = [ { \"DRIVER\" , opts [ :odbc_driver ] || \"{ODBC Driver 13 for SQL Server}\" }, { \"SERVER\" , opts [ :hostname ] || System . get_env ( \"MSSQL_HST\" ) || \"localhost\" }, { \"DATABASE\" , opts [ :database ] || System . get_env ( \"MSSQL_DB\" )}, { \"UID\" , opts [ :username ] || System . get_env ( \"MSSQL_UID\" )}, { \"PWD\" , opts [ :password ] || System . get_env ( \"MSSQL_PWD\" )} ] |> Enum . map_join ( fn { key , value } -> \" #{ key } = #{ value } ;\" end ) ODBC . start_link ( conn_str , conn_opts ) end def disconnect ( _ , state ) do case ODBC . disconnect ( state ) do :ok -> :ok { :error , reason } -> { :error , reason , state } end end # State checking and checkout to connection process def checkout ( state ) do { :ok , state } end def checkin ( state ) do { :ok , state } end # Query preparation def handle_prepare ( query , opts , state ) do { :ok , query , state } end # Finishing a prepared query def handle_close ( _query , _opts , state ) do { :ok , nil , state } end # Actually execute the query def handle_execute ( query , _params , _opts , state ) do ODBC . query ( state , query ) end end We’ve introduced a number of new callbacks, which are required to be able to\nexecute a query. Let’s run through them: checkout and checkin are there to handle the process of copying state to\nand from the process that calls our implementation and our implementation’s\nprocess itself. If your state object needs to be different in each of those\nexecution environments, you can use these callbacks to implement the\nnecessary transformations. Our state object is for now just a reference to\nthe ODBC process, so no transformations are needed. handle_prepare and handle_close handle the process of preparing a query\nfor later executions and releasing that prepared query respectively. This\nwould be useful if the process of translating a query into a form readable\nby the database wasparticularly expensive or if performance could be gained\nby caching queries for execution in the database. There’s no gain there for\nour purposes, so we leave them as no-ops. handle_execute is the callback that actually runs the query when the user\ncalls an execution DBConnection function ( execute , prepare_execute ,\nor their raising ( ! ) versions). Its return value becomes the return value\nof those functions. Let’s try it out: iex > { :ok , conn } = DBConnection . start_link ( Mssqlex . Protocol , []) iex > DBConnection . execute! ( conn , \"Select 12345\" , []) ( Protocol . UndefinedError ) protocol DBConnection . Query not implemented for \"Select 12345\" Aw! OK, we’ve overlooked a few things: DBConnection requires that queries be structs implementing the DBConnection.Query protocol. handle_execute should return an {:ok, result} tuple, but :odbc.sql_query somewhat unhelpfully returns either {:updated, number_of_rows_updated} or {:selected, column_names, result} . We really need to support parametrised queries via :odbc.param_query unless we expect users of the adapter to interpolate values into their query\nstrings (which is a bad idea . But for the sake of checking that our little GenServer architecture actually\nworks we can directly call inner functions for a completely artificial test: iex > { :ok , conn } = Mssqlex . Protocol . connect ([]) iex > Mssqlex . ODBC . query ( conn , \"Select 12345\" ) { :selected , [[]], [[ 12345 ]]} It lives! (Note the :odbc result structure I mentioned earlier). In the next post, we’ll work through those three items to produce an API that\nactually works for real-world parametrised queries.", "date": "2017-03-14"},
{"website": "FindMyPast", "title": "How an intern outdid a senior engineer…", "author": ["Laurie Mills"], "link": "https://tech.findmypast.com/How-an-intern-outdid-a-senior-engineer/", "abstract": "With mentoring and coaching, could a 3rd year software intern out perform a senior engineer? Yes indeed! Last year, Findmypast decided to create our first ever software internship program which we piloted in the Dundee office. Given the time required to set it up and select just 2 applicants while still meeting business objectives, it could have been risky - especially as we are still a relatively small company. But we wanted to give it a go for several reasons: we wanted to build better relationships with the universities in Dundee we wanted to see if we could turn a third year student into a full stack developer we wanted graduates to be impressed that they would naturally want to come work for us and if it all worked well, we wanted to run it in our London office the following year Now that some time has passed, we’d like to think that we succeeded. We treated those engineers as our own from the start and we hope we taught them a few things about software development in the process. They led tasks on our product delivery teams, were exposed to lean and agile practices and were given a good understanding of our delivery pipeline set up by our tools team. As a result, both engineers were exposed to more technology and tools than you could shake a stick at! They outperformed our expectations of a 3rd year student in so many ways. They questioned our requirements, pointed out issues with coding and testing, picked up new languages and so much more. They made valuable contributions to our products and our processes and they made it a fun summer for everyone in the office. So we think it’s safe to say they are both now full stack developers. But why just take our word for it, why not read how Kim and Steven got on and decide for yourselves… Kim McCann 3rd year Applied Computing Student from Dundee University During my time at FMP, I had the opportunity to work on all three of the engineering teams in Dundee, both product delivery teams and the tools team. I spent my first month on Doozers working on a new user journey in order to increase site traffic and getting involved with the sitemaps service. When I joined, Doozers were at the start of the software development process and spending time making careful decisions about the projects technology and refining their tickets. Having the opportunity to see these processes was incredibly important since this is something I had learned about at university but had never seen in practice. There was a huge amount of new technology for the team to learn and learning this alongside them was hugely advantageous. This situation gave me the confidence to not be afraid to ask questions, something I previously struggled with and I’m very thankful for gaining this ability. The next month was spent on T-Rex creating the new customer service application and breaking down the dependencies on the legacy monolith application. T-Rex were in full swing when I joined and it was incredibly rewarding to be contributing to tickets so quickly. Whilst on the team they gave me the opportunity to lead a ticket, which involved writing the C# backend on FMP, the Elixir API and the React UI. Learning how each of the parts joined together and pairing with different members of the team on each part was an amazing learning experience. T-Rex move incredibly quickly in their development and taught me to not be afraid of taking risks and to fail quickly, a valuable lesson that I will take forward. My final month has been spent with the Spanners working creating the new tools and infrastructure necessary for the re-architecture, as well as carrying out DevOps work. Whilst there, I got the opportunity to pair on fixing the acceptance tests for FMP and fixing problems with service proxy. This taught me how to quickly identify problems and solve them. This, along with Sash’s constant reminder, taught me about the importance of logging to help identify problems which will be massively helpful for my honours project. The teams here work using Agile Software Development, mostly KANBAN. These practices were covered at university but getting to see these practices in use was very helpful and opinion changing. Previously, I found Pair Programming distracting and didn’t care much for Test-Driven Development. Now, I love Pair Programming since I can bounce my code and skills off another person to develop the best piece of code we possibly can, and I will never write any production code without tests again. Throughout my time, the company have run a number of workshops. Some of these included topics such as the SOLID principles and design patterns. These were topics that I’d never covered in great detail before and learning about these from people who use these every day has helped me grow as a developer. Other workshops included how to increase my employability by improving my CV, learning about interview techniques and analysing code tests, all led by the people who do recruitment. My only wish would be to have had the workshops more regularly to cover even more content and covering some content relevant to the teams earlier. Overall, the workshops that FMP ran for me as an intern were amazing. I appreciated how the workshops weren’t tailored towards just the placement, but on my future as a software engineer. They helped me to improve the practices that I applied to my software development, and grow as an individual, something I wouldn’t have gained elsewhere. My time at FMP has been an amazing opportunity and so much more than I expected. Being a fully integrated part of the team and contributing to product delivery has been a massively rewarding learning experience. I hope that FMP decide to continue running their internship program in future years due to the great opportunity it is for students. Most internships don’t involve even half of what FMP did. Their internship program is one of the best and I would recommend it to anyone. Steven Turner 3rd year Applied Computing student, Dundee University What I Expected Not what I got! That can have good or bad connotations, thankfully in this case it wasn’t the latter and I won’t be slating the company, but I don’t really know what I was expecting since it was my first time in a professional environment. Something more business-like and boring I guess, but what I found was the office was a much friendlier atmosphere with a wide range of personalities and a huge focus on culture. I didn’t really feel like the intern or anything. I expected to be given side projects to work on that weren’t live projects but I didn’t, I instantly jumped on pairing with team members on the actual projects. What Experience Did I Gain I worked in two different product delivery teams and a tools team; T-Rex, Doozers and Spanners. T-Rex were creating new tools for our customer service team to do their job while Doozers were creating a new landing page with facts about surnames to help increase the sites SEO traffic and Spanners were building tools for the rest of the company. I worked with people with multiple levels of experience in different areas, with a huge array of tech and it was interesting to see a few projects from the start using tech that was new to a lot of the team members at Findmypast.\nSomething I gained a HUGE amount of experience in was test driven development. It’s everywhere here. From uni, I had a rough idea of why we write tests but I missed out on so many reasons why you’d even consider TDD. Working for a tools/DevOps team was interesting. I enjoyed the idea that we were developing tools for developers, especially after using the tools myself on the two product delivery teams. The challenges instead were delivering to developers who are tech savvy and internal, ending up as tech support and pushing for their tools to be adopted by the rest of the company.\nRegardless of team, one huge thing I took away was that sometimes things just don’t work, have to be thrown away or the difficulty curve is a steep cliff upwards, I saw and experienced this first hand and felt demoralised myself. But it’s just part of the job and happens, the more times you experience it you understand it just happens and that the code you wrote yesterday is terrible compared to today. Workshops We had weekly workshops were we go taught about concepts ranging from agile principles to development practice’s, by people who used them every day. These helped bridge gaps of knowledge that was missing such as the SOLID principles which are a pretty big deal. We also had multiple workshops on concepts such as how to gather requirement’s properly and why they’re so important, task estimation, a wide array of design patterns, TDD, debugging and error tracing and agile principles such as the difference between Scrum/Kanban. We also has some more general workshops such as reviewing our CV’s and interview techniques, open source vs closed source, and the difference between working for a big company and a small company.\nOne of the more unexpected things we did was have me sit in on a phone interview. It was an interesting experience anyways. During any interview I always felt the interviewer was out to get me, but being on the on the other side in this case showed me, there were no real trick questions it makes our lives easier when the candidate is what we’re looking for. Who wants to interview 10 candidates? Overall on the program I feel it’s a huge success for me personally as it helped me gain confidence and learn a huge amount from people who’ve been in my position and helped me understand concepts such as SOLID principles which have already seeped into the way I code or work in a team using Kanban which and introduced me to many technologies. Some parts of the program didn’t go as well as we struggled to have most of the workshops in the first month due to holidays. We had regular retros where we gave feedback and acted upon it during month 2 and 3 so we could correct anything before it was too late. Conclusion That’s my summary of my internship at Find My Past I wasn’t sure at first if I wanted my summer to myself or an internship but at the end now I feel it was worth it.", "date": "2017-04-19"},
{"website": "FindMyPast", "title": "Consumer Driven Contracts with Jackal", "author": ["Oleksandr Stasyk"], "link": "https://tech.findmypast.com/jackal-consumer-driven-contract-testing/", "abstract": "Jackal saves you from outages by stopping you releasing breaking changes to your APIs! Jackal started life as a discussion around a bug introduced by changing the API of one of the Microservices without checking its consumers would handle the new API successfully. Hint: they didn’t. It has ultimately evolved to provide a standalone Microservice and command-line client to manage contract testing in a way which can be easily automated using a CI platform. Install from Jackal on npm . Integration Tests One approach to the above problem would have been to have a suite of integration API tests separate from any service which tested the services worked together by calling out to multiple interdependent services in sequence. This approach is similar to the current pipeline for our monolith, and presents a few, often quite frustrating problems: Changes must be made in two repositories when updating or extending any public API Changes must be made in quick enough succession to prevent a code change building successfully without sufficient tests, or failing due to obsolete tests which haven’t been removed Changes run through the pipeline in a branch must run against the same branch of the test project, or potentially fail Consumer Driven Contracts An alternative approach to this problem is to use Consumer Driven Contracts (CDC) . A Contract specifies the interaction which can occur between a Consumer and a Provider and aims to ensure development of a Provider is driven by the requirements of its Consumers . The advantages this provided over a suite of integration tests are: Requirements are defined and maintained by the Consumer of a service, rather than by the service itself Contracts are more explicit as well as smaller and faster to add to an existing repository than adding a suite of integration tests in a separate location Contract testing provided the ability to prevent both the Consumer of the service and the service itself from releasing broken code Jackal At FindMyPast we are currently in the process of splitting a monolithic web application into a platform of Microservices which requires a lot of API interaction. It was inevitable that a breaking change in the service communication was going to be introduced. Each of our new Microservices presents an API to be used by other services. In order to ensure each API is tested, each service has a suite of API tests which live in the same repository as the code, this encourages RDD approach . The API test suite for each service validates the code against the business rules laid out when migrating monolith functionality to a new Microservice. The problem this leaves, and the problem Jackal tries to solve, is to provide a guarantee of compatibility between a Microservice and its consumers. In an attempt to ensure contracts using Jackal are idempotent and repeatable, contracts can be defined with multiple before and after hooks to carry out set up and tear down actions. Jackal can also automatically insert random data into request bodies in order to meet requirements around unique data constraints, for example, a new registration request must specify a unique email. Development flow Our envisaged development flow for Jackal is as follows: after a contact is sent, Jackal tests the provider’s API. If the contract tests pass and the response from the provider matches the contract, successful response is propagated all the way back. If a consumer makes a breaking change to the contract, the provider no longer matches the shape and jackal responds with a failure to the consumer side. This can be used to break the test build. If a provider makes a breaking change to its own API, you can run the tests already saved from the previous successful consumer run. The breaking changes would be detected, returned back and used to break the build. In order to introduce changes, while having strict consumer contract testing process in place, the provider is required to make a non-breaking change. This could either be an introduction of a new API version or an additive change to an existing response. Installation Jackal is available from NPM and should be installed globally in order to use the client as a normal CLI utility: $ npm install --global jackal Server The Jackal Server provides five API routes which act as an interface to the service, they do not provide a RESTful API and attempts to use them in such a fashion may cause your Jackal to be mortally wounded. The Jackal Server uses a single configuration file in either JSON or YAML format and can be started using the Jackal Client: $ jackal start -c path/to/config.yaml The configuration file specifies the database file location, any additional metadata fields for logging and the server to send StatsD data to should you be so inclined. The default configuration file provides the following configuration: db : path : db.json logger : environment : development statsD : host : localhost port : 8125 prefix : jackal If the default configuration is ok for your needs then you can start a Jackal Server using the even simpler command: $ jackal start Client The Jackal Client can actually be thought of as two distinct parts: a CLI enabling usage of Jackal in a command-line environment a programmatic client capable of making requests to and handling responses from a Jackal Server Early versions of Jackal only provided a server, requiring contracts to be sent using Curl or similar to send contracts to a running instance: $ curl -X POST --silent http://jackal.url:25863/api/contracts -H 'Content-Type: application/json' -d @path/to/contracts.json In addition to the fairly lengthy command needed to send the contracts, parsing the JSON response and determining whether contracts had passed was also left to potential users and their sh -fu. By comparison, the same action can be achieved using the Jackal Client with the relatively simple: $ jackal send http://jackal.url:25863 path/to/contracts.json The response from the Jackal server is also handled and can be presented in one of several formats, including spec and teamcity formats, before an appropriate exit code is returned. The Jackal Client also allows for other quality of life improvements: Contract files can be specified in either JSON or YAML format Contracts can be split into per Provider files under a common directory for a given Consumer Contracts in a directory can be skipped by simply adding a .skipped extension to the file For convenience jackal can be sucessfully ran against a non-existsing contract file with --skip-missing-contract flag, this can be used for introducing jackal in multiple builds at once with only select builds opting into the tests by actual defining the contracts file Defining a contract The following is a YAML example contract for an iTunes Search App : itunes_search_app : itunes : search_by_term_and_country : OK : request : baseUrl : \" https://itunes.apple.com\" method : GET path : /search query : \" ?term=mclusky&country=gb\" response : body : resultCount : Joi.number().integer() results : - collectionName : Joi.string() trackName : Joi.string() statusCode : 200 This contract can also be used in its equivalent JSON format. Submitting the contract Submit the contract to your running Jackal server by running the following: $ jackal send http://localhost:25863 path/to/example/contract.yaml itunes contracts executed\n  itunes_search_app contracts executed against itunes\n\t✔ Test search_by_term_and_country OK passed for itunes_search_app against itunes Run the contract for your provider So it might be hard to convince Apple to run your contract before deploying the iTunes API server, but in a perfect world they’d be a good provider and run the following: $ jackal run http://localhost:25863 itunes itunes contracts executed\n  itunes_search_app contracts executed against itunes\n\t✔ Test search_by_term_and_country OK passed for itunes_search_app against itunes Note itunes refers to the provider name specified in the contract. Full documentation for Jackal, including an extensive and (hopefully) exhaustive usage guide, can be found in our GitHub repository . Acknowledgements Jackal initially used, and later absorbed, parts of the BBC Consumer Contracts project Guido Bellomo for kindly granting us the rights to the Jackal NPM package name Jackal was developed by Donald Robertson , Oleksandr Stasyk and Neil Crawford .", "date": "2017-05-02"},
{"website": "FindMyPast", "title": "A lightweight GraphQL (Part I)", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/lightweight-graphql/", "abstract": "This post is inspired by a talk in the latest React London Meetup, Lightweight GraphQL.\nIn Findmypast we use GraphQL and Relay in our new Titan architecture. I always felt like I had to go to the basics in order to understand what was going on in Relay.\nThat talk gave me that and I wanted to share it with my friends.\nIf after reading this post, you feel like you want to know more, you can watch the video. This post is also inspired by Dan Abramov’s, You might not need Redux .\nIn there, Dan goes to the basics of why he created Redux. In order to demonstrate it, he creates a really lightweight version of Redux as an example. This post is not about why you need GraphQL. There are many good blogs talking about that topic ( Why Graphql is the future , Graphql vs REST ).\nI must confess though, many times I have asked myself the following: Why the heck do we use GraphQL and Relay? I’ve been using Rest for many years, why do I need another tool if Rest does the job just fine… For that reason I have prepared the following example. Hopefully you can change your point of view a little :D. PhotoBooth In Findmypast we have a Frontend Guild, this consists of a group of people interested in developing their Frontend skills.\nWe sit together every week, we talk about all things Frontend for an hour, we decide what to learn next.\nWe’ve been practicing using a case study, a Photobooth app. This is my code so far. And this is how it looks (sorry for the lack of style :D): Let me show you some bits and bobs of the code. This is the render function of my main app container: class PhotoBooth extends Component { constructor ( props ){ super ( props ); this . state = { currentPhoto : 1 }; } render () { return ( < div > < Header /> < Thumbnails currentPhoto = { this . state . currentPhoto } photoSelected = { this . photoSelected } / > < PhotoControls previous = { this . previous } next = { this . next } / > < Photo currentPhoto = { this . state . currentPhoto } previous = { this . previous } next = { this . next } / > < /div > ); } } We’ve got a Photo component where I render the big image: import PhotoStore from ' ./photo-store ' ; export default ({ currentPhoto }) => { const photoMetadata = getCurrentPhotoMetadata ( currentPhoto ); return ( < div className = \" row \" > < div className = \" col-lg-12 \" > < h2 > { photoMetadata . title } < /h2 > < /div > < Image className = \" row \" src = { ` ${ PhotoStore . baseUrl } / ${ PhotoStore . bigImage } / ${ photoMetadata . src } ` } title = { photoMetadata . title } / > < /div > ); }; function getCurrentPhotoMetadata ( currentPhoto ) { const photo = PhotoStore . allPhotos [ currentPhoto - 1 ]; return { src : photo . src , title : photo . title }; }; And a Thumbnail component where we show all the small photos and we allow the user to select one of them to be displayed larger: import PhotoStore from ' ./photo-store ' ; export default ({ currentPhoto , photoSelected }) => ( < div className = \" col-lg-6 col-md-offset-3 \" > < Grid > < Row > { PhotoStore . allPhotos . map (( photo ) => { return ( < Col key = { photo . title } xs = { 6 } md = { 3 } > < Thumbnail className = { currentPhoto === photo . index ? \" selected \" : \"\" } alt = { photo . title } src = { ` ${ PhotoStore . baseUrl } / ${ PhotoStore . thumbnailSize } / ${ photo . src } ` } onClick = {() => photoSelected ( photo . index )} / > < /Col > ) }, this )} < /Row > < /Grid > < /div > ); As you can see I have something called PhotoStore, but that is simply a list of harcoded photos that I take from Lorem Pixel: const PhotoStore = { allPhotos : [ { index : 1 , title : \" Baseball \" , src : \" sports/1 \" }, { index : 2 , title : \" Surf \" , src : \" sports/2 \" }, { index : 3 , title : \" Bike \" , src : \" sports/3 \" }, ], baseUrl : \" http://lorempixel.com \" , thumbnailSize : \" 200/200 \" , bigImage : \" 500/500 \" }; export default PhotoStore ; PhotoBooth with our Github pictures At the moment we are using Lorem Pixel pictures , but I thought, why don’t we make this a bit less boring and I show my colleagues’ avatars from Github.\nThat would mean making a request to Github for every colleague, going through their metadata and getting their avatar. Maybe not! Github has created a version of their api using GraphQL. With GraphQL we can request all the data we need from a consumer point of view, we define the shape of the data and it is delivered to us as JSON object.\nThis is a massive win compare to classic REST, where in order to populate a page we need to make many requests to get all the data required to render. Let’s use Github GraphQL to see this in action. Our first GraphQL query Another great advantage of GraphQL is that the documentation is a living documentation, generated from the api itself.\nThere is a tool called Graphiql where you can try GraphQL apis before using it. For Github we have it as well . Let’s play a bit with Github’s graphiql in order to get some pictures.\nFirst of all let’s get Findmypast’s avatar: query GetAvatarsQuery($login:String!) { organization(login:$login) { avatarUrl } } { \"login\" : \"findmypast\" } Let’s go through the previous code: First of all we have the operation type , in this case it is a query . Then we have the operation name . It is optional for queries but it helps debugging. After that we specify that we want to query data from the organization type. We also need to pass some parameters so we can filter the data, in this case we send login. In GraphQL these are called arguments. Instead of using a harcoded value, in the example we have used a variable.\nBecause GraphQL is strongly typed, we need to specify the field type in the argument. This is the response we get back: { \"data\" : { \"organization\" : { \"avatarUrl\" : \"https://avatars0.githubusercontent.com/u/9417510?v=4\" } } } Let’s get some more data Now let’s also get the avatars of last 3 members of Findmypast. With Rest that would mean hitting a different endpoint, forming a different request.\nWith GraphQL I can just amend the previous request like this: query($login:String!) { organization(login:$login) { avatarUrl members(last: 3 ) { nodes { name avatarUrl } } } } That is, I tell the api: in addition to the organization’s avatar, give me the name and avatar of the last 3 members of the organization. I obtain this: { \"data\" : { \"organization\" : { \"avatarUrl\" : \"https://avatars0.githubusercontent.com/u/9417510?v=4\" , \"members\" : { \"nodes\" : [ { \"name\" : \"Dennis Ideler\" , \"avatarUrl\" : \"https://avatars2.githubusercontent.com/u/497458?v=4\" }, { \"name\" : \"Yosuf Ali\" , \"avatarUrl\" : \"https://avatars3.githubusercontent.com/u/10884239?v=4\" }, { \"name\" : \"Juan Flores\" , \"avatarUrl\" : \"https://avatars1.githubusercontent.com/u/11361723?v=4\" } ] } } } } I find this way of querying an api highly expressive: I’m more worried about what data I want than how I have to obtain it. Consuming GraphQL data programmatically in a React app Execute GraphQL programmatically You can run that same GraphQL query in your favourite browser, go to the Console and type the following. We’ll be using window fetch like this: window . fetch ( \" https://api.github.com/graphql \" , { method : \" POST \" , headers : { \" Content-Type \" : \" application/json \" , \" Accept \" : \" */* \" , \" Authorization \" : \" bearer --YOUR PERSONAL TOKEN-- \" }, body : JSON . stringify ({ query : \" query($login:String!) { organization(login:$login) { avatarUrl members(last: 3) { nodes { name avatarUrl } } } } \" , variables : { \" login \" : \" findmypast \" } }) }) . then ( response => response . json ()) . then ( json => console . log ( json )) (In order to communicate with Github Graphql you need to generate a Personal token, follow instructions here ). That former code is running a post operation to Github’s GraphQL server. In the body, we provide exactly the same query and parameters that we used in Graphiql. Now let’s move that into a function that we can execute in our code: function executeGraphQLQuery ( query , variables = {}){ return window . fetch ( \" https://api.github.com/graphql \" , { method : \" POST \" , headers : { \" Content-Type \" : \" application/json \" , \" Accept \" : \" */* \" , \" Authorization \" : \" bearer --YOUR PERSONAL TOKEN-- \" }, body : JSON . stringify ({ query : query , variables : variables }) }) . then ( response => response . json ()); } export default executeGraphQLQuery ; Connect your GraphQL queries to React components Now let’s use our new function to build a High Order component (HOC). This will allow us to decorate our React components with the possibility to get data from GitHub: import React from ' react ' ; import executeGraphQLQuery from ' ./execute-graphql ' ; const withGraphQL = ( query , { variables } = {}) => Component => class extends React . Component { state = { loading : true }; async fetchData (){ try { const json = await executeGraphQLQuery ( query , variables ); this . setState ({ loading : false , error : null , data : json . data }); } catch ( e ){ this . setState ({ loading : false , error : e }); } } componentDidMount (){ this . fetchData (); } render (){ return < Component {... this . props } loading = { this . state . loading } data = { this . state . data } error = { this . state . error } /> ; } } export default withGraphQL ; When the HOC is mounted it goes and fetches data by using the previous function. Since the function is asynchronous, we use the nice coming ES7 syntax with async/await and try/catch.\nAfter we get the data we update the HOC state and pass it to the rendered Component. By using this High order component, any Component can get data from GitHub, it will be injected as props. Using withGraphQL in our Photo Booth: Let plug this into some of our component.\nHere is our Photo component decorated with the HOC, we tell it to get the name and avatar of the first 3 members of Findmypast: const query = ' query($login:String!) { organization(login:$login) { members(first: 3) { nodes { name avatarUrl } } } } ' ; const params = { variables : { \" login \" : \" findmypast \" }}; export default withGraphQL ( query , params )( Photo ); This is great in the sense that you can see the data requirements together with the component being rendered.\nNow the Photo component will get that data as props: const Photo = ({ currentPhoto , data }) => { if ( ! data ) return null ; const photoMetadata = getCurrentPhotoMetadata ( currentPhoto , data . organization . members . nodes ); return ( < div className = \" row \" > < div className = \" col-lg-12 \" > < h2 > { photoMetadata . title } < /h2 > < Image className = \" row \" src = { photoMetadata . src } title = { photoMetadata . title } / > < /div > < /div > ); }; function getCurrentPhotoMetadata ( currentPhoto , photos ) { const photo = photos [ currentPhoto ]; return { src : photo . avatarUrl , title : photo . name }; }; Please notice that for now while data is undefined, I don’t render anything. Otherwise the data comes in data.organization.members.nodes . Let’s take a look at the Thumbnail component using GraphQL: import React from ' react ' ; import { Grid , Row , Col , Thumbnail } from ' react-bootstrap ' ; import withGraphQL from ' ../graphql/with-graphql ' ; const Thumbnails = ({ currentPhoto , photoSelected , data }) => { if ( ! data ) return null ; return ( < div className = \" col-lg-6 col-md-offset-3 \" > < Grid > < Row > { data . organization . members . nodes . map (( photo , index ) => { return ( < Col key = { photo . name } xs = { 6 } md = { 3 } > < Thumbnail className = { currentPhoto === index ? \" selected \" : \"\" } alt = { photo . name } src = { photo . avatarUrl } onClick = {() => photoSelected ( index )} / > < /Col > ) }, this )} < /Row > < /Grid > < /div > ) }; const query = ' query($login:String!) { organization(login:$login) { members(first: 3) { nodes { name avatarUrl } } } } ' ; const params = { variables : { \" login \" : \" findmypast \" }}; export default withGraphQL ( query , params )( Thumbnails ); In the bottom part you can see how we decorate Thumbnails with the HOC. We provide the exact same query as before.\nIt means that we can loop through the organization members and render a Thumbnail for each of them. And this is how the app looks with these changes, my random PhotoBooth turned into a Findmypast Photobooth! This is it for now. With this basic example we have been able to query data from Github by using GraphQL and render it in our Photobooth React app.\nWe haven’t used Relay but we have understood how Relay queries data from GraphQL servers.\nRelay hides the nitty-gritty aspects for us so we can focus in our application. You have the full code of the example in this link . In the next blog post we will continue developing our Lightweight GraphQL. We will include mutations, fragments and much more so that we can understand what Relay is doing for us.", "date": "2017-05-02"},
{"website": "FindMyPast", "title": "Technical debt", "author": ["Neil Crawford"], "link": "https://tech.findmypast.com/technical-debt/", "abstract": "I was chatting with my wife a while back and I flippantly dropped technical debt into our conversation. She asked me to describe it to her. I couldn’t. Well not in a way that made any sense whatsoever! Now that I’ve had some time to think I’m going to give it a bash. In the traditional sense technical debt is described as decisions that we make to speed us up but then slow us down later. You take a deliberate decision to do something a bit hacky, a bit dirty, but you get that feature out the door. Though, if you need to change that feature the cost of change is going to be high. Here you understand you’re making a tradeoff of flexibility and quality for speed. However in my experience engineers use technical debt in a much broader sense. It might be deployment pain where you have to babysit a deployment to production. It might be flaky acceptance tests that you have to rerun a few times to get to pass. It might be that the code you’re working with is structured in a way that you can’t extend. Whatever it is, it wastes your time . Most of these problems in your platform won’t have been deliberate trade-off decisions so aren’t in the strictest sense technical debt, but they still have the same effect, they still waste your time. So now we have a grasp of what technical debt is - anything that wastes your time, I want you to imagine a world without it. Conjure up a picture of sitting down to write a feature and the entire experience being utterly and completely frictionless. Everything you need to know is easy to find, easy to understand, and you get some really meaningful work done in what feels like no time at all. How amazing would that be? Let’s switch gears and start thinking about how to get a little closer to the idyllic scenario above. To start tackling technical debt I need some mental models to help in thinking about the causes of technical debt. I find it useful to categorise engineering work into four types. Fire fighting , where you drop everything and react to the problem at hand. Change , where you have to make changes to existing code, tests, production hosts. This is like the tax you have to pay to get work done. Innovation , the fun stuff, where you build something that will drive new business and customer value. Improvement , where you make improvements to your system of work to reduce the amount of change and fire fighting work you have to do. How does thinking in these terms help? I believe tech debt is very closely linked to change and fire fighting work. If innovation is what drives new customer value, then fire fighting and change work cost you an opportunity to innovate by wasting your time. But why do we find ourselves spending so much time fire fighting and doing change work? Why do problems build up in our codebases? A relentless drive to deliver features can leave little time to go back and fix problems with the platform. A lack of ownership of a codebase can make it difficult to improve when no-one believes it’s their responsibility to go in and fix a problem. A lack of support for continuous improvement where you don’t feel safe to spend a day fixing a problem. A lack of agreed standards can make you think that what you’ve done is good enough, when in fact it might not be. All of these examples could lead to an increase in change and fire fighting work. There is an opportunity cost here, the time you spend fire fighting and doing change work, you can’t spend innovating, that time has gone. However investigating the time you spend on wastes normally leads to discovery of rich veins of technical debt in your platform. Fundamentally, I believe you need a strong habit of continuous improvement, the organisational support and dedicated space to do the improvement work. Without this you’ll always be amassing more technical debt. The hardest thing, and I believe the first step, is for the organisation to admit the problem. This can be extremely challenging to the organisation, serious truth-telling can be hard-hitting and really difficult to accept for a lot of people. It’s admitting that mistakes have been made, and if the organisational culture isn’t strong enough to cope with truth-telling, the conversations that need to happen will not. It’s a difficult thing to have to slow down, to put aside some of your important short term goals and focus on long term sustainability. In order to support this conversation I believe it’s important to measure your delivery and the impact that tech debt is having on it. How do we measure and then make visible tech debt? The state of devops report outlines some KPIs that you might find useful. The lead time from a code commit to that code in production, deployment frequency, and lead time for user stories help measure your throughput. Change failure rate and mean time to recovery can help measure quality of service. These criteria give you a picture of the health of the platform you’re working in. Here at Findmypast our lead time for code in some legacy applications is close to 7 days, whereas in our newer applications it can be as little as 20 minutes. This shows us that there’s problems getting work done in a legacy codebase, it’s easier to have a conversation about whether reducing the lead time for code in legacy should be a priority for business or not. We could set a target of 3 days lead time and iterate towards it. Then the conversation changes to developing a capability and away from technical debt. Taking a step back and assuming the business decides to make improvement and developing engineering capability a priority, here are some things we’ve tried here at Findmpast. We have two platform teams, one focussing on mostly operational goals of making it reliable to ship code to production. The second team is focussed on tooling that makes it easier, faster and safer to deliver application code on our primary codebase. These teams have paid massive dividends in throughput and quality. However we had to be very careful not to create siloed teams, by making sure the whole engineering team felt involved in the decisions. This is still a constant challenge that we work on every day. But with the right approach this can reduce a lot of the wasted work for developers every day. In addition we’re now creating dedicated time and space for product delivery teams by giving them 20% of their iteration backlog to plan their own work towards a technology platform goal. This work is prioritised at the same level of importance as product work and gives the teams space to make improvements and address the causes of their fire fighting and change work. The final piece is to celebrate the improvements you’re making as a whole. We have all hands meetings on a regular basis and it’s important to share progress and celebrate any successes you’ve had as a team. It keeps technical debt and continuous improvement in the forefront people’s minds. It keeps us balanced when considering long term sustainability alongside launching great new functionality for our users. Business is an infinite game, it never ends, and as such we need to equally consider our long and short term goals. If you like the sound of what we’re doing here Have a look at our jobs and drop us a line.", "date": "2017-12-06"},
{"website": "FindMyPast", "title": "GRAPHical Family Trees", "author": ["Alex Clark"], "link": "https://tech.findmypast.com/graphical-family-tree/", "abstract": "This post uses a graph database to show your family tree to you in a new way and shows you how to query it too. It serves as a basic introduction to Graph Databases and shows what a powerful tool they are. What are we doing? At Findmypast we have a team of engineers working on Search. We recently had the chance to use Neo4J (a graph database) to do some interesting work. It’s worth sharing what Graph Dbs are, what they can do, and some of the fun you can have with them. If you want to play along, there’s a github repo for those in FMP, or a zip file containing everything you need to get started. Interlude… First, what’s a graph database? What’s a graph? This is a graph isn’t it? No. This is not a graph. Well, not for the purposes of this blog-post anyway. This is perhaps more technically known as a chart. Charts might have X and Y axes, or segments of pies, or pretty bars and wiggly lines… But these are not GRAPHS. A Graph could be defined as being made up of vertices which are connected by edges. A graph may be undirected, meaning that there is no distinction between the two vertices associated with each edge, or its edges may be directed from one vertex to another. Fascinating stuff, I’m sure you’ll agree, but let me put that another way. A graph is a group of nodes connected by relationships. All of a sudden this starts to sound a bit like a family tree where the nodes represent people and the relationships represent, er, the relationships between them. Back to the story. Getting started. If you want to try this as well, there are a few hoops to jump through first. If you just want to skip to more pretty pictures, go to the next section. :-) Prerequisites You’ll need Docker for this preferably running on Ubuntu (otherwise you’ll have to work out the pathnames stuff yourself) You’ll also need a few other things installed: Clone the github repo or download the zip create a directory called tmp at the same level as Neo4J Ruby: sudo apt-get install ruby Ruby-dev: sudo apt-get install ruby-dev Some gems: sudo gem install gedcom and sudo gem install neography A gedcom file: Gedcom files are a structured text document format that can contain people and relationships from a family tree. From Findmypast you can export your family tree as a gedcom file by visiting the view all trees page and clicking the middle export tree icon next to your tree. If you don’t have a family tree, you can use mine or use the sample Harry Potter one in the directory (wizard!). I think that is everything! Setting up. First, you need to convert your gedcom file to a format that can be imported into a graph database. Run this: cd neo4jfmp/import/gedcom/\n\n./gedcom2csv.rb <Name of you gedcom file without the .ged extension> My ruby-fu is not great and there could be bugs here with your gedcom. Here be dragons - You have been warned. cd ../../../\n\n./startneo.sh /media/psf/AllFiles/Users/aclark/Projects/Docker/tmp/ That last command will need the path to your tmp folder, not mine… And it might take a while to run first time. You should now have an empty graph database waiting for you! Go to the Neo4J database page and you should see something like this: OK, let’s import your tree… In the input box at the top, paste the following (you’ll find this snippet in Neo4J/neo4jfmp/cypher/import_persons.cyp file along with all the other snippets we’ll run) USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM 'file:////gedcomImportPersons.csv' as record FIELDTERMINATOR '|' WITH record CREATE ( person: PERSON { gedcomId: record.gedcomId }) WITH person , record SET person.gender = record.gender , person.label = record.label , person.firstName = record.firstName , person.lastName = record.lastName , person.yearOfBirth = toInteger ( trim ( record.yearOfBirth )), person.yearOfDeath = toInteger ( trim ( record.yearOfDeath )), person.birthPlace = record.birthPlace , person.deathPlace = record.deathPlace WITH person , record , CASE WHEN record.gender = \"M\" THEN [ 1 ] ELSE [] END AS Males FOREACH ( x IN Males | SET person: M ) WITH person , record , CASE WHEN record.gender = \"F\" THEN [ 1 ] ELSE [] END AS Females FOREACH ( x IN Females | SET person: F ) WITH person , record , CASE WHEN record.gender = \"U\" THEN [ 1 ] ELSE [] END AS Unknowns FOREACH ( x IN Unknowns | SET person: U ); and run it. Once that finishes, run this ( import_families.cyp ): USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM 'file:////gedcomImportFamilies.csv' as record FIELDTERMINATOR '|' WITH record MATCH ( start_person { gedcomId: record.START_ID }), ( end_person { gedcomId: record.END_ID }) WITH start_person , end_person , record FOREACH ( ignoreMe IN CASE WHEN record.TYPE = \"CHILD\" THEN [ 1 ] ELSE [] END | CREATE ( start_person ) - [ :CHILD { empty_label: '' }] -> ( end_person ) ) FOREACH ( ignoreMe IN CASE WHEN record.TYPE = \"SPOUSE\" THEN [ 1 ] ELSE [] END | CREATE ( start_person ) - [ :SPOUSE { empty_label: '' }] -> ( end_person ) ) FOREACH ( ignoreMe IN CASE WHEN NOT record.TYPE = \"SPOUSE\" AND NOT record.TYPE = \"CHILD\" THEN [ 1 ] ELSE [] END | CREATE ( start_person ) - [ :RELATED_TO { empty_label: '' }] -> ( end_person ) ); Now this ( create_root.cyp ). Change the name to be the root person of the tree (probably you!): MATCH ( you { firstName: \"Alexander\" , lastName: \"Clark\" }) WITH you LIMIT 1 SET you: ROOT ; All done! Now you’re ready to do the fun part. But first! Let’s check everything has worked and you can see your tree. MATCH ( n: ROOT ) RETURN n ; This should pop up a little circle. Double click it and more little circles appear. You can click on each node and see its properties at the bottom. If you don’t like the colours, click on the star icon on the far left of the screen and drag Neo4J/import/gedcom/style.grass onto the dotted box. The yellow relationships represent parent-child reclationships and the green ones represent spouse relationships. OK. Let’s start with something basic. Like showing everyone in your tree! Click the cog icon at the bottom left of the screen and change Initial node display to 1000 Now run this: MATCH ( n ) RETURN n ; Boing! up pops everybody. You can make the window fullscreen with the diagonal arrows in the top right hand corner of the result pane. There’s a zoom bottom right to let you see everyone at once: OK, that’s silly and doesn’t tell us anything. How about the path between the youngest person and the oldest person in the tree? (These are in queries.cyp ) MATCH ( youngest ) WITH youngest ORDER BY 0 - youngest.yearOfBirth LIMIT 1 MATCH p = shortestPath (( oldest ) - [ :CHILD * ..25 ] - ( youngest )) WHERE oldest.gedcomId <> youngest.gedcomId WITH nodes ( p ) as rels ORDER BY oldest.yearOfBirth LIMIT 1 UNWIND rels AS rel OPTIONAL MATCH r = ( rel ) - [ :SPOUSE ] - () RETURN rel , r ; At the bottom of the image, my daughter Eloise was born in 2011. At the top, her great-great-great-great-great-grandfather was James Thorpe born in 1778 :-) There’s quite a lot going on in this query. I’ll break some of it down: In cypher (the language for querying Neo4J) you get data out by using the MATCH...RETURN query. We start with MATCH ( a_variable_name_you_make_up: A_LABEL_FROM_THE_DB ) then RETURN a_variable_name_you_make_up ; There are ORDER BY and WHERE clauses to re-order and filter our query. The label you specify can also filter the results. You can use F as a label to get all the females for example. Cypher works like a pipeline of data. You start with a lot of records at the start and then filter or group it, passing it onto the next stage of the pipeline with the WITH clause, after which you can run more MATCH ing and filtering / grouping. Graphs work by pattern matching an input against one or more nodes and relationships. You can give it a pattern and ask it to find that pattern in the graph. In the above example, I’ve used the shortestPath function to tell me the quickest way to get from one node to another in the graph. Nodes and relationships between nodes are represented in ASCII: (var_node)-[:RELATIONSHIP_LABEL *..HopsNumber]->(another_var_node) shortestPath() takes this relationship pattern, looks for all occurrences of it in the graph, and the returns the one which has the fewest hops form the start to the end. How about your most distant relative? MATCH ( n { firstName: \"Alexander\" , lastName: \"Clark\" }) WITH n MATCH p = shortestPath (( a ) - [ * 1 .. 1000 ] - ( n )) WHERE a.gedcomId <> n.gedcomId WITH p WHERE length ( p ) = 8 RETURN p ; (Your mileage with this one may vary. You’ll need to change the name for your tree, and change the 8, increasing it until you don’t get a match. There’s probably a better way of finding the longestPath, but this seemed reasonably clear) I’ve used a shorthand in the query of putting the properties I’m filtering by into the MATCH clause, rather than use a WHERE clause. So Jane Thorpe at the top is my most distant relative that I know about at the moment. Most popular first names in your tree? MATCH ( n ) RETURN n.firstName , count ( n.firstName ) as occurances ORDER BY occurances DESC LIMIT 10 ; Here, data comes back as a table rather than nodes as I asked for a count aggregate and just the firstName property. John is fairly predictably the most common, but there’s an Isabella in the list which is similar to my daughter’s name Isobel and my name is in the top ten. Final challenge. My daughter asked me how often her name appears in our tree. Simple, I thought: MATCH ( n ) WHERE n.firstName = \"Isobel\" RETURN n ; That should give me all the bouncing balls with her name. Oh, only one: But I know there are others! Oh, they spell it differently (or it’s a variation) MATCH ( n ) WHERE n.firstName = \"Isabella\" RETURN n ; But what if I hadn’t known that or there were misspellings in my tree? Because Neo4J is built on Lucene I can do a fuzzy search: MATCH ( n ) WHERE apoc.text.fuzzyMatch ( n.firstName , \"Isobel\" ) RETURN n ; Here, I’ve made use of a plugin of stored procedures called apoc , but dammit, still only one result! What’s going on here? Oh. it seems that in recent releases of Lucene, to make fuzzy matching faster it’s now limited to two characters being different or missing between the two texts being compared. So Isobel -> Isabella has 3 changes (the ‘o’ to an ‘a’, the extra ‘l’, the extra ‘a’) OK, can’t use fuzzy matching out of the box. Google search… Ah! phoneticDelta ! Let’s have a look through the list of built-in procs and see that: CALL dbms.procedures () YIELD name , signature , description as text WHERE name = 'apoc.text.phoneticDelta' RETURN * apoc.text.phoneticDelta(text1, text2)\nyield phonetic1, phonetic2, delta\n\nCompute the US_ENGLISH soundex character\ndifference between two given strings OK, looks promising. Soundex is a way of quantifying how similar 2 bits of text are. Try this: MATCH ( n ) CALL apoc.text.phoneticDelta ( n.firstName , \"Isobel\" ) YIELD delta WITH n , delta WHERE delta > 3 RETURN n ; (delta here means how similar they are with a 4 being the max.) Yay! all the nodes! But how are they related to Isobel? MATCH ( n ) CALL apoc.text.phoneticDelta ( n.firstName , \"Isobel\" ) YIELD delta WITH n , delta WHERE delta > 3 MATCH p = shortestPath (( a { firstName: \"Isobel\" , lastName: \"Clark\" }) - [ * 1 .. 1000 ] - ( n )) WHERE a.gedcomId <> n.gedcomId RETURN p ; Nice. Final thoughts Family trees lend themselves very well to Graph Databases and Neo4J is a great tool. There’s LOADS more to do on this. I could write a proper plugin for doing name matching better. Or do something with the birthPlace location data to analyse how my family moved over time. Or load in other trees and see if we have any overlap in our relations Or work this up into tool for the site to display trees. If you want to learn more about graphs and Neo4J there are lots of googleable resources. If you got stuck, drop me a note and I’ll see what’s wrong. Or send me your gedcom and we can go through it together. Alex Clark\nSearch Developer, Findmypast\naclark@findmypast.com\nwww.findmypast.co.uk", "date": "2018-02-20"},
{"website": "FindMyPast", "title": "Do not WIP", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/do-not-wip/", "abstract": "I’m sure you are familiar with the term WIP , just in case Work In Progress. \nIt relates to something that is not finished yet, there’s a bit more way to go before completing it.\nI’ve been using Trunk-based development for some time now. We commit and push straight to master at FMP. Crazy, right? I had been doing my branches and Pull requests in Github for a few years.\nOne day someone came: We are going to end with Pull requests, if we do Pair programming we should push to master, without a formal Code review process. I must admit, I disliked the idea. \nFor me Pull Requests were the best way to share knowledge in an offline way, it was a way to get feedback from great developers and a way to keep the quality of our code as high as possible. After more than a year of Trunk-based development, now it is my favourite way of working. \nThere are many challenges a team faces when pushing straight to master, it is a cultural change and there is a massive change in the way the team operates.\nOne of the aspects of working in master is keeping it in a healthy state. We were quite lucky the day Dennis Ideler joined our team, his massive sense of professionalism meant that I was going to learn loads. One of the areas has been Version Control. The following happened not long ago.\nThat day I had been doing Pair programming for the whole day, it was late and there were some pending changes in my computer.\nMy colleague wanted to get these changes in Github so he could continue working the day after from his computer.\nAlthough the changes were incomplete, tests were passing and I really wanted to go home. Once more, my lazy side showed its ugly face: Let’s create a branch, and commit this changes as “WIP”. The day after, we started by getting the branch in my colleagues computer and merging it into master. My lovely “WIP commit” went into master. \nThat moment could have easily gone by, but suddenly I overhead: Why is this WIP in master? This should have never gotten into master. The shame and guilt came quickly but with my cheeks flushed, I asked why that was a problem. We have a chat about how every commit needs to be a fully working part.\nWhen we try to solve a problem we break that problem into subproblems. It is ok to commit a finished solution to a subproblem, but not Work in progress .\nWorking in master means that every commit is pushed to live in a matter of a few minutes. \nAlso everything we commit will be pulled by dozens of devs, they need to get something working, not something in progress. I asked how he could have solved the problem: 1. Create a branch: Name it with the name of the ticket (if you have one) and with the thing that is “In Progress”. git checkout -b Ticket123/unfinished-post 2. Commit the changes in this branch: Give some context to help your colleague remember and push the new branch to your VCS. git commit\ngit push origin Ticket123/unfinished-post 3. In the other work station, pull latest master and get the branch . git fetch\ngit rebase master\ngit checkout Ticket123/unfinished-post 4. Uncommit the “Work In progress”, with the following command: git reset --soft HEAD^ I have that command with an alias in my gitconfig git uncommit . That will move the branch pointer to the previous commit but leaving the working directory state unchanged. 5. Switch to master: git checkout master You will find the changes that you did in the other computer, pending to be staged in master. \nNow you can continue the work and finish the WIP. Then your changes can go into master. Thanks for reading and thanks to Dennis Ideler for being a mentor on many things during this time in Findmypast. UPDATE An alternative way to perform this operation is by running: git merge --squash branch_name That will unstage and uncommit the changes from the branch_name into the current branch for example, master. Thanks to Regis Bittencourt for teaching me this.", "date": "2018-02-22"},
{"website": "FindMyPast", "title": "The Many Facets Of Solr", "author": ["Alex Clark"], "link": "https://tech.findmypast.com/solr-facets/", "abstract": "This post introduces a few basic elements of the Solr Search Platform and highlights how faceting can ‘bucket’ results into counts What are we doing? At Findmypast we user Solr as a search index for our historical records data. Censuses, births, marriages, deaths and a whole host of other data is searchable via a single Solr cloud. Our 2.7 billion historical records (that’s all the records excluding the newspapers from the British Newspaper Archive which are indexed in Solr separately) are easily queryable in Solr and we can aggregate result counts in a highly flexible way using Solr’s faceting features. Getting started If you’d like to play along at home with this blog, there’s a Github repo for those who have access to Findmypast’s account and a zip download for those who don’t. You’ll need docker to run the examples, but I’ve included all the relevant output in the blog to save you the trouble. If you’re running the examples, after cloning or downloading, if you’re on Unix or Mac you should be able to just launch the ./run.sh file and everything will take care of itself. On Windows, you’ll need to change run.sh to run.cmd Two minute intro to Solr I can’t do justice to everything Solr can offer, but here’s a taster of the major features Full-text search: We’re not just talking about doing searches that start with or contain some text; searches can make use of fuzziness, synonyms and proximity for example. NoSQL features: You don’t need to make all your data conform to the same structure in Solr. Highly scalable: You can spread your search index out over multiple nodes and machines. Search faceting: This is what the bulk of this post is about! Bootstrapping Solr Solr comes with a bunch of built-in defaults that I’ve made use of in this blog. Because Solr has NoSQL features, you can store whatever combination of fields each record might have for that document and only those fields. Each document in Solr can look very different to any other document. For example, DocA might have a Forename field and a YearOfBirth field, and DocB might have a Surname field and a YearOfDeath field. Both can be stored in the same index without the annoyance of null columns that a relation database would need. Even better, if you post data to Solr using a prescribed format for field naming, it’ll work out what type of field you mean and how best to store it. For example, send a field called gender_s and the _s suffix tells Solr this field is a string that it should index and store. _i says the field is an integer, _f is a float and so on. _ss means the field is a string, but that it can contain multiple values. (It’s the plural of _s ) _is - multiple integers, _fs - multiple floats and so on. In production, this isn’t advisable as it’s best to tell Solr what sort of fields to expect and how to store and index them. But for getting started, it’s ideal. In this demo, I’ve used a few million rows from the 1881 census of England, Wales and Scotland and imported them into Solr using the default schema Solr ships with. If you run the accompanying demo file, it bootstraps Solr using the out-of-the-box config, spends a few minutes formatting the 1881 data dump for import and then posts the data to Solr. Just over 4 million records import at around 4 minutes on my laptop - this goes to highlight just how much speed is crucial to Solr and a million rows a minute import in a Docker VM is pretty quick! Querying Solr At Findmypast, querying Solr is doing over http and the output is JSON making it a breeze to integrate. At its most basic, a query to Solr using just a URL might pass just a few parameters: …q=country_s:scotland&rows=10 The ‘q’ parameter is our query that asks for all data where the field country_s is scotland . The ‘rows’ parameter limits our results to the first 10 matches. The ‘q’ parameter can make use of boolean logic (amongst other features) so a query might look like this for records marked with the country ‘Scotland’ and the gender ‘Male’: http://0.0.0.0:8983/solr/recordsets/select?q=country_s:scotland+AND+gender_s:m&rows=10 Results come back in JSON format. In a Chrome browser, the plugin JSON formatter makes the responses easy to read. There is a MASSIVE amount more you can do with querying, but I want to talk a bit about faceting the results. None of what I have to say here is revolutionary or new, but it’s a quick taster of Solr’s faceting all in one place Solr Facets Facets aggregate or ‘bucket’ your results into counts. You get a bunch of results and instead of doing work in an application to group them by fields or formulas, you get Solr to do it. There are a few different types of facets that all provide a different shape of result. For each type, I’ve included an example that shapes the included 1881 census dataset, but I’ve stopped it delivering the base results by supplying the parameter rows=0 . What we’re interested in here is the faceting of results rather than how we might construct a complicated query for results. Simple Terms Facet Example Our first example simply groups the results by the birth year of the records. http://0.0.0.0:8983/solr/recordsets/select?\n  q=*:*&\n  rows=0&\n  facet=on&\n  facet.mincount=1&\n  facet.field=birth_year_i The first two parameters say we want to query all data, but not return any documents from the actual data. facet=on is required to switch on faceting facet.mincount=1 says we only want facet results where we actually get a number of records returned affected by our query. facet.field=birth_year_i is the main bit where we say we want counts from the result split by the ‘birth_year_i’ field. So this query is saying for the sample data in the 1881 Census, which years were they born in? The results look like this: { \"responseHeader\" :{ ... }, \"response\" :{ ... }, \"facet_counts\" :{ \"facet_queries\" :{}, \"facet_fields\" :{ \"birth_year_i\" :[ \"1881\" , 120771 , \"1880\" , 108354 , \"1879\" , 108150 , \"1878\" , 104087 , \"1877\" , 101469 , ... ] } ... } } The facet_counts : facet_fields section in the results shows us that 1881 is the most common year followed by 1880, 1879, 1878 and so on. The most common age for people in the sample 1881 Census was 0! OK, that’s done, but now let’s arrange that data by decade. Give us counts of people in the Census grouped by their birth year decade. For that, we can use a Range Facet: Range Facet Example: http:// 0.0 . 0.0 : 8983 /solr/recordsets/select?json= { query : \"*:*\" , limit: 0 , facet: { ages: { type: range , field: birth_year_i , start: 1770 , end: 1890 , gap: 10 , other: all } } } Sidebar… I’ve switched querying style here a little to use the new JSON API endpoint that the latest version of Solr uses. As well as being a bit easier to read, it has type-safety in the parameters and becomes clearer with more complex queries. Back to Range Facets… Range facets can have start, end & gap numbers to bucket the data and a selection of other parameters you find by googling. The results look like this: { \"responseHeader\" :{ ... }, \"response\" :{ ... }, \"facets\" :{ \"count\" : 4120178 , \"ages\" :{ \"buckets\" :[{ \"val\" : 1770 , \"count\" : 6 }, { \"val\" : 1780 , \"count\" : 479 }, { \"val\" : 1790 , \"count\" : 8789 }, { \"val\" : 1800 , \"count\" : 53921 }, { \"val\" : 1810 , \"count\" : 145564 }, ... ], \"before\" :{ \"count\" : 0 }, \"after\" :{ \"count\" : 0 }, \"between\" :{ \"count\" : 4120178 }}}} Each decade is listed with how many people were born in that year who appeared in the sample data for the 1881 UK Census. Let’s see how many different birth years there are in the data and what the average age of each person in the Census sample is: Metrics Facet Example: http:// 0.0 . 0.0 : 8983 /solr/recordsets/select?json= { query : \"*:*\" , limit: 0 , facet: { birth_years: \"unique(birth_year_i)\" , avg_age: \"avg(age_i)\" } } Here, I’ve applied some formulas to the facets: unique() tells us how many different values there are and avg() tells us the mean average value: { \"responseHeader\" :{ ... }, \"response\" :{ ... }, \"facets\" :{ \"count\" : 4120178 , \"birth_years\" : 106 , \"avg_age\" : 25.62985434124448 } } So there are 106 different birth years and the average age was 25.6 There are lots of other metrics functions available as you might guess! Facets of Facets… Facets can get really powerful when you group one set of facet results by another field. Imagine a hierarchy of fields such as Country=>County=>Town. You can produce a result-set that shows aggregation by each level in the hierarchy. http:// 0.0 . 0.0 : 8983 /solr/recordsets/select?json= { query: \"country_s:scotland\" , limit: 0 , facet: { counties: { limit: 1000 , type: terms , field: county_ss , facet: { towns: { limit: 1000 , type: terms , field: town_s , missing: true } } } } } Let’s break this down a little: First, facet the domain of results by aggregating by the county_ss field. Then for each of those values, facet the results by each town_s in that county_ss . Also include a count for any records that have that county_ss but a missing town_s field (Remember, solr is flexible so you don’t need to have a value for every field for every document). The results look like this: { \"responseHeader\" :{ ... }, \"response\" :{ ... }, \"facets\" :{ \"count\" : 115556 , \"counties\" :{ \"buckets\" :[{ \"val\" : \"midlothian\" , \"count\" : 45818 , \"towns\" :{ \"missing\" :{ \"count\" : 0 }, \"buckets\" :[{ \"val\" : \"leith\" , \"count\" : 38588 }, { \"val\" : \"edinburgh\" , \"count\" : 7230 }]}}, { \"val\" : \"ayrshire\" , \"count\" : 13990 , \"towns\" :{ \"missing\" :{ \"count\" : 0 }, \"buckets\" :[{ \"val\" : \"saltcoats\" , \"count\" : 5090 }, { \"val\" : \"stevenston\" , \"count\" : 3552 }, { \"val\" : \"largs\" , \"count\" : 3178 }, { \"val\" : \"stewarton\" , \"count\" : 1355 }, { \"val\" : \"landward\" , \"count\" : 815 }]}}, ... ] } } } For each county we can see not only the count of records at this level, but also the breakdown of towns and the counts at that level too. Although I’ve restricted the query to just 2 levels here (County and Town), there’s no real limit - You could easily produce results for Continent => Country => County / State => District => Town => Street Conclusion The last set of examples where we nested one set of facets within another, hints at some real power in Solr where any type of facet can be nested within another: A Terms Facet with a nested Range Facet could show you the counts of ages or people within each county for example. Or a Terms Facet with a nested Metrics Facet using max(age__i) could show you the oldest person in each town, county and country. There are lots of different ways to facet data - most of which you can find somewhere on Findmypast - We use Term faceting in the Category lists on a Country or World Search. We use Range faceting when displaying the publication years of our newspaper records. We use Nested faceting when displaying the list of datasets with record counts in the categories on our Dataset A-Z. Solr’s powerful faceting features drive some of the most flexible searching on Findmypast. Alex Clark\nSearch Developer\nFindmypast\naclark@findmypast.com\nwww.findmypast.co.uk", "date": "2018-03-01"},
{"website": "FindMyPast", "title": "TDD, what do you expect?", "author": ["Juan Flores"], "link": "https://tech.findmypast.com/TDD-what-do-you-expect/", "abstract": "TDD, Test driven development.\nIf you don’t know what it is I envy and salute you.\nThese days TDD is like one of those other terms, eg. Agile, MVC, Lean, SOLID.\nWe love acronyms in our profession, and as developers we need to know them all, even if we don’t use it.\nIn my opinion that is because it is used by people that don’t fully understand the value of them and they use it as a weapon against us.\nThey are even making millions by using them, can you imagine? Worst of all, we have created a culture of you need to know this so we’ve ended with a massive population of people repeating the acronyms without really knowing them: Me, of course I know TDD, I use it every day. Comment of a person who has just checked in some to Github without a single unit test. For me TDD always was about stepping into the unknown. \nSimilar to a baby who starts with her first steps. She sees something spiky in front, she doesn’t put the whole foot in it. She approaches the surface, while building an expectation ( how is it going to feel? Cold? Is it going to hurt? How much? )\nIn order to prove that expectation she slightly touches the surface with the big toe and checks if the expectation still hold. That’s what we should be doing with software, do baby steps while we step into the unknown. \nTDD helps me in that regard, I implement one little step at a time as I learn more about the system. \nI know some people are proud to say they never follow TDD, maybe I’ll be the same if I were to build exactly the same system over and over again (that’s why Dave Thomas does not do TDD anymore is proud to be a not TDD person). TDD: the Triple-A Arrange, Act, Assert, AAA. I was introduced to TDD by following that practice: Arrange: Establish the context where something is going to happen. Act: Make that thing happen. Assert: Did that thing happen? I even put comments in my code to let others know that I was following TDD: public void Some_test_name_I_will_rename_in_the_future_because_now_I_am_too_lazy_to_think_about_it () { //Arrange //Act //Assert } I think I even checked some code to Version Control with those comments. Sometimes I see people leaving those comments in. In this post I wanted to show you a subtle but different way to write your unit tests. What is the problem? Again and again, when I pair with people we start with the Arrange part of the test. \nWhat is the problem with that? With TDD one of the goals is to write the minimum code that satisfies a expectation. If we start by writing the Arrange we have already broken the rule: we don’t have yet an expectation and we have some code. People tend to start the Arrange of some new test by copying the arrange of a previous test. In many occasions we leave some code that wasn’t really needed. In some frameworks like Mocha in Javascript, some part of the Arrange is defined in a before ( beforeEach ) that only allows to set a function. That is even worst since we can’t name the Context properly. What do you expect? For some time now, I have been practicing a different way to do TDD. We still follow the AAA but backwards, starting with the Assert. \nIt seems silly but I found multiple advantages. It seems easy but you’ll have to fight against your normal tendency. Let’s do an example: When a customer buys a subscription with Paypal we store his email in a table. If we renew his subscription later on, we don’t save his paypal email again as this wasn’t a manual payment.\nWe found a bug in a situation where we need to retrieve his paypal email but we are getting an empty email. Let’s fix this bug together. The code is in C# and the tests are written in Machine Spec, let’s follow those for now. This is the buggy code: public string GetPayPalEmailByMemberId ( int member ) { return this . _transRepository . FindAllSubscriptionsForMember ( member ) . OrderByDescending ( m => m . DateAdded ) . FirstOrDefault ( m => m . PaymentProvider == PaymentProviderEnum . PP )? . PayPalEmail ; } I know, it is legacy code, it takes a bit to understand (although we’ve seen worst). Let’s explore (and fix) this code through Unit tests. Expectations, the What As sometimes happens with Legacy code, the class that has the bug doesn’t have any unit tests around it. \nLet’s start with an empty file and we put empty blocks that will compose the test: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal { Establish context ; Because of ; It should_find_their_paypal_email ; } } We have described what we expect, now we can nail the expectation: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal { Establish context ; Because of ; It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( \"paypalEmail@fmp.com\" ); } } You don’t see it but my intellisense is painting the variable _paypalEmail with red, it doesn’t exist. \nAt this point proably you are feeling the urge to create that variable, you are fighting against the autocompletion and that doesn’t feel good. Keep going! In a moment you’ll see the value of this. Act, the How Now we need to retrieve that paypal email, we are ready to invoke the logic that needs to be tested. public class when_user_has_bought_subscription_through_paypal { Establish context ; Because of = () => _paypalEmail = _paymentRepository . GetPayPalEmailByMemberId ( _memberId ); It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( \"paypalEmail@fmp.com\" ); } We now have called the buggy piece of functionality. We will retrieve the paypal email for a given member using the payment repository. There are a couple more variables not yet defined, the code does not compile, but that’s fine, we are still building our test. Arrange, the When In order to be able to find a member’s paypal email we need a payment made by that member.\nOur payment repository has a dependency in a module that returns all the payments made by a member.\nWe will make the Arrange happen by mocking that other module to return a payment made with paypal: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal { const int MemberId = 123 ; const string ExpectedPaypalEmail = \"paypalEmail@fmp.com\" ; Establish context = () => { var paypalTransaction = new MemberTrans () { PayPalEmail = ExpectedPaypalEmail , PaymentProvider = PaymentProviderEnum . PP }; var memberTransactions = new List < MemberTrans >(){ paypalTransaction }; MockOf < IMemberTransRepository >() . Stub ( x => x . FindAllSubscriptionsForMember ( MemberId )) . Return ( memberTransactions . AsQueryable ()); }; Because of = () => _paypalEmail = _paymentRepository . GetPayPalEmailByMemberId ( MemberId ); It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); } } When asked about transactions for a member, the mock will return a transaction with the expected member.\nWe have defined a couple of constants to remove duplication and make the code easier to read. Finally we need to get our subject under test, sut , the payment repository.\nTo do that we are going to inherit from an utility class that will create our SUT with all the dependencies mocked. It is an internal implementation but it is not difficult to guess how it works. public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { const int MemberId = 123 ; const string ExpectedPaypalEmail = \"paypalEmail@fmp.com\" ; Establish context = () => { var paypalTransaction = new MemberTrans () { PayPalEmail = ExpectedPaypalEmail , PaymentProvider = PaymentProviderEnum . PP }; var memberTransactions = new List < MemberTrans >(){ paypalTransaction }; MockOf < IMemberTransRepository >() . Stub ( x => x . FindAllSubscriptionsForMember ( MemberId )) . Return ( memberTransactions . AsQueryable ()); }; Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); static readonly string _paypalEmail ; } } TestsContext will provide a sut that matches the generic parameter provided, in this case the type we want to test, PaymentRepository, \ni.e. sut will be an new instance of PaymentRepository. We are ready to run our test and… it passes! What? Shouldn’t your test fail and then make it pass. Right, yes and no. Since this is legacy code without any test, before fixing the bug I want to have a safety net of tests that check the expected behaviour.\nThese tests also allow us to understand a bit better how the legacy code behaves.\nThis methodology is described by Michael Feathers in his book, Working effectively with Legacy code . More importantly we have found our way to writing this test the opposite way to the Classic TDD way, Arrange-Act-Assert. Even though our Arrange has 9 lines of code, we know it is the minimum code to ensure that will verify the expectation.\nIn my opinion it is not hard to understand this code since it is not loaded with setup code that we don’t really need. Assert-Act-Arrange Since the main focus is the expectation, we can do one more thing to make our code easier to follow. Let’s rearrange the elements of the test: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); Establish context = () => /.../ private static string _paypalEmail ; const int MemberId = 123 ; const string ExpectedPaypalEmail = \"paypalEmail@fmp.com\" ; } } For the person learning TDD now this change may seem crazy and it goes against our traditional thinking.\nFor the new person to this code, this test is a lot easier to understand as it focus in what we expect from our test module. Reproducing the bug Now that we have a basic happy path cover we can add a unit test that reproduces the bug: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { [...] public class and_the_subscription_has_been_renewed { It should_find_their_paypal_email ; Because of ; Establish context ; } } } I have omitted the previous test to make it easier to follow to the reader.\nNotice how I have added the new test inside the other test. The reason is that we are composing our scenarios with other scenarios, redefining the initial statement with a more specific case. Let’s add the Assertion: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { [...] public class and_the_subscription_has_been_renewed { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Because of ; Establish context ; } } } And the Act: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { [...] public class and_the_subscription_has_been_renewed { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); Establish context ; } } } I know it looks the same, let’s wait to have a failing test to refactor. We are ready to create the specific Arrange of this scenario. \nWe know that when a user is renewed after buying a subscription through paypal, there will be a later transaction for that member without a paypal email: Establish context = () => { var paypalTransaction = new MemberTrans () { PayPalEmail = ExpectedPaypalEmail , PaymentProvider = PaymentProviderEnum . PP , DateAdded = DateTime . Today . AddDays (- 10 ) }; var renewalTransaction = new MemberTrans () { PayPalEmail = null , PaymentProvider = PaymentProviderEnum . PP , DateAdded = DateTime . Today }; var memberTransactions = new List < MemberTrans >() { paypalTransaction , renewalTransaction }; MockOf < IMemberTransRepository >() . Stub ( x => x . FindAllSubscriptionsForMember ( MemberId )) . Return ( memberTransactions . AsQueryable ()); }; In this case I have only put the Arrange part to make it easier to read.\nLoads of duplication going on. It is good to remember that when following TDD it is ok to have bad code for a while, we have the refactor step to improve the code structure. I try to run the test and I get this error from the test runner: ERROR Machine.Specifications.SpecificationUsageException: \nThere can only be one Because clause. Found 2 Becauses in the type hierarchy Now it is time to refactor our two tests to make the runner happy: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { public class and_it_is_the_only_transaction { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); Establish context = () => /.../ } public class and_the_subscription_has_been_renewed { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); Establish context = () => /.../ } private static string _paypalEmail ; const int MemberId = 123 ; const string ExpectedPaypalEmail = \"paypalEmail@fmp.com\" ; } } We have separated every scenario in two subclasses, still duplication but we can run the test.\nIt fails! Expected : \"paypalEmail@fmp.com\" But was : [ null ] Now we can go and fix the code: public string GetPayPalEmailByMemberId ( int member ) { return this . _transRepository . FindAllSubscriptionsForMember ( member ) . OrderByDescending ( m => m . DateAdded ) . FirstOrDefault ( m => m . PaymentProvider == PaymentProviderEnum . PP && ! m . PayPalEmail . IsNullOrEmpty () )? . PayPalEmail ; } Specifically we are interested in those paypal transactions that contain a paypal email, since those are real payments by the user. Finally let’s remove that ugly duplication in our tests: public class PaymentRepositoryTests { public class when_user_has_bought_subscription_through_paypal : TestsContext < PaymentRepository > { public class and_it_is_the_only_transaction { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Establish context = () => { MockMemberTransactions ( new List < MemberTrans >() { _paypalTransaction }); }; } public class and_the_subscription_has_been_renewed { It should_find_their_paypal_email = () => _paypalEmail . ShouldEqual ( ExpectedPaypalEmail ); Establish context = () => { MockMemberTransactions ( new List < MemberTrans >() { _paypalTransaction , _renewalTransaction }); }; } Because of = () => _paypalEmail = sut . GetPayPalEmailByMemberId ( MemberId ); Establish context = () => { _paypalTransaction = new MemberTrans () { PayPalEmail = ExpectedPaypalEmail , PaymentProvider = PaymentProviderEnum . PP , DateAdded = DateTime . Today . AddDays (- 10 ) }; _renewalTransaction = new MemberTrans () { PayPalEmail = null , PaymentProvider = PaymentProviderEnum . PP , DateAdded = DateTime . Today }; }; private static void MockMemberTransactions ( List < MemberTrans > transactions ) { MockOf < IMemberTransRepository >() . Stub ( x => x . FindAllSubscriptionsForMember ( MemberId )) . Return ( transactions . AsQueryable ()); } private static string _paypalEmail ; private static MemberTrans _paypalTransaction ; private static MemberTrans _renewalTransaction ; const int MemberId = 123 ; const string ExpectedPaypalEmail = \"paypalEmail@fmp.com\" ; } } Now we have different contexts for different scenarios.\nFrom here we can even add further tests to ensure that the legacy functionality behaves as expected. I think I’ll do that right after finishing this post. Recap That was a long example, thanks for getting this far.\nI hope you could follow and see the advantages of rearranging your AAA when doing TDD. If you want to take only one thing from this post, take this: Every now and then change the way you do things, you will find something you may not expect.", "date": "2018-03-04"},
{"website": "FindMyPast", "title": "Searching for Solr", "author": ["Alex Clark"], "link": "https://tech.findmypast.com/searching-for-solr/", "abstract": "This post discusses how we take our data from its raw state to something that joins the rest of our records in a sub-second search index for nearly 3 billion records. Background At findmypast, we have the ability to flexibly search our billions of records using a Solr index. But just how does the data get in there and what have we done to make the Search experience so adaptable and responsive? We’ll take a journey with a record to see how it starts from ink on a page in a book, to the key to discovering your past. (Maybe!) A Long Time Ago… Over the last few hundred years various hand-written records have been collected about people. From censuses to birth records, army medal records to passenger lists, court documents to electoral rolls, there are billions of hand-written or typed lines with names, dates, places and events. Organisations like The National Archive, Family Search, and Billion Graves all work with us to supply raw data to us to publish on our site. The raw data comes in all sorts of formats: Database dumps; PDF books; CSV files; Images. We use techniques like OCR + image processing, teams of transcribers copying text into a computer, and database import scripts to turn that raw data into a digital format we can start working with. Stage 1 - The Gold Dataset Data really begins life for us in Search once it is turned into ‘Gold’ data in a MySQL database. (Although the process of scanning and OCR of delicate paper pages from books which are decades or sometimes more that a hundred years old, almost faster than you watch is interesting too - but another post…) The gold databases take all sort of shapes - there’s no real prescribed structure except for a few key fields like a date and a title for the dataset. Let’s start with a record for Malcolm Reginald Clark (b. 1925), mentioned in the 1939 Register as an evacuee along with his brothers Douglas and Peter just before the outbreak of WWII. He’s my grandfather. He’s listed as a row in a table in our MySQL Gold Data store. First, we assign the record a unique record id that is made up from a supplier code, a code for the dataset, and an Id within that dataset. TNA/R39/2647/2647E/006/18 Next, we attach to the record the Id of any image that accompanies the record. TNA/R39/2647/2647E/006 And any other keys that might tie that record to another (for example, a household Id that other people who lived with them also share) 3494960 Then there are a whole bunch of cleaning and standardisation processes that take place that make sure any place-names, dates, names, and a myriad of other fields follow prescribed patterns and spellings of our other data. Our Gold Data is ready to be used. Stage 2 - The Dataset Our Data Team create a metadata definition that describes how we should index and search all the information within this dataset. At findmypast, our search is powered by Solr which is an amazingly flexible storage solution that allows us to store many different shapes of data together and search them ‘as one’. For our record in the 1939 Register, we decide to make a lot of information from the record searchable by the user: FirstName; LastName; Full BirthDate; Location; Gender; Marital Status; Occupation; Other Household Member Names; Official Ref. Number. A few fields like Ledger Code are judged not useful for searching and are left as display-only fields. The columns in the Gold Data table are mapped to the fields we want to use in our Solr index using a simple SQL statement: SELECT forename AS FirstName , LastName , DATEPART ( DAY , BirthDate ) AS BirthDay , ... FROM 1939 _Register Stage 3 - The Ingest We’re ready to get our data into Solr. We use our Ingest Tool to select our data from the MySQL Gold Data and send it to our Ingest API that will store it in Solr. The Ingest Tool runs the SQL statement and sends batches of messages to the API containing hundreds of records at a go to store. Each record is represented as a JSON object: {\n  FirstName: \"Malcolm Reginald\",\n  LastName: \"Clark\",\n  BirthDay: 26,\n  ...\n  LedgerCode: \"EPCF\",\n  RecordMetadataId: \"1939_register\"\n} Our API parses this message and, using the metadata for the dataset, works out what needs to be sent to Solr. It also augments this information with extra data such as the category & sub-category, an estimated BirthDate, & DeathDate (where not specified explicitly in the record), and the Record Provider information. Using the metadata it works out the field types for each field too. A message to Solr might look like this: {\n  FirstName__fname: \"Malcolm Reginald\",\n  LastName__lname: \"Clark\",\n  BirthDay__int: 26,\n  ...\n  KeywordsPlace__facet_text: [\n    \"England\",\n    \"Isle of Wight\",\n    \"Newport\"\n  ],\n  RecordMetadataId: \"1939_register\"\n} The suffixes to each field tell Solr how to process and store it. It sends posts to Solr with the information to store. And here’s where it gets more interesting… Solr Search Strategy Setup Solr is very flexible and because it follows a noSQL approach, you can store different shapes of document all together. In addition, because it uses a Lucene index, it can do more than just store simple strings, numbers, dates etc. Strings can be stored as-is with little intelligence, or as processed text that can be broken down into token words, synonyms added, plural words stemmed to the singular, and a whole host of other manipulations. Our record for Malcolm Clark is stored with all the other records. His last name is stored along with synonyms like Clarke, his first name is broken down into two terms, one for Malcolm and one for Reginald, everything is lower-cased for ease of searching,\nand fields that are multi-valued are separated out. Solr Shards Solr documents largely live in isolation from each other. In a relational database there are foreign keys that tie records together. In a graph database, there are relationships that tie nodes together. You don’t get that concept (much) in Solr. Because of this, sharding (or the act of splitting a data store down into smaller pieces) is very much a first-class citizen. Sharding is powerful in Solr and there are a few choices you can make to maximise the efficiency and speed you get. You can opt to leave it all up to Solr. Simply tell it how many shards you want to split the index into and it will create little indexes for you that all work together as a cloud. When you store data, fire it at the cloud and let Solr decide how to balance the load and where to store it, based on a hash of the key. The downside of this approach is that all parts of the cloud have to reply to a query. If you search for documents where the LastName equals Clark, all shards have to give their response before a final count can be returned. But this serves very well for relatively small indexes of a few million records. The load is spread out over a number of Solr nodes and so long as there’s not too much load on them, they all respond very quickly. For larger collections such as ours, you can impose your own sharding strategy. At findmypast, we looked at the shape of queries to our site and realised that most people search our records using a name of a person they’re looking for. So we decided to shard our index by LastName. Names beginning with AAA-ARB are stored in shard 1, ARC-BAC in shard 2, and so on up to WIZ-ZZZ in shard 48. Records without a last name go into a ‘Default’ shard. We split our index into 49 separate smaller surname-based indexes! We do this to make our searches faster. When a search comes in with a surname, we only need to query a small part of the total to look for records. If a search comes in without a last name, it’s still fast as we benefit from parallel processing in our smaller search indexes. But it’s even more efficient than that Because Solr works as a load-balanced cloud, we have multiple copies of each shard - the one doing the least work is chosen to respond. In total we have 156 separate shards just serving record searches. Then there’s 8 shards for addresses and image searches, 106 shards serving our hint queries from our Family Tree. And a whole other set of infrastructure serving our British Newspaper Archive records. Here’s a crazy screenshot showing a representation of our record-search Solr cloud: Next Stage - Searching Hmm. There’s still a huge amount to talk about. Filter caches, Riak document stores, Solr plugins, Memcached searches… They all contribute to making our Search platform as fast and as extensive as it is. This is going to need a ‘part 2’ post to go through them. Malcolm Clark is there waiting to be discovered and we’ll go through how we do that next. But I hope you can see from this high-level look at Search that we have a big investment in the platform that means we can give customers what they want in a fraction of a second. Alex Clark\nSearch Developer\nFindmypast\naclark@findmypast.com\nwww.findmypast.co.uk", "date": "2018-03-18"},
{"website": "FindMyPast", "title": "Co-author commits with Git Mob", "author": ["Richard Kotze", "Dennis Ideler"], "link": "https://tech.findmypast.com/co-author-commits-with-git-mob/", "abstract": "Command line app: VS Code extension: Great teams build great software. Part of what makes a team great is their ability to work well with each other.\nIn modern software development, collaboration is key. We’re big fans of Extreme Programming at Findmypast. Many of our teams benefit from pair and mob programming.\nThis is when two or more people share a computer to write code. In our experience, working like this helps us keep focus,\nincrease code quality, reduce defects, and create a shared ownership. Documenting code collaboration When we work together, we should document that. It’s more than just giving credit to others, it’s also\ninforming everyone about who was responsible for introducing changes to a codebase. Keeping a clear record of your\nwork has lasting value - ever stumbled on an old commit only to be left with unanswered questions?\nIn addition to explaining in your commits why a change was introduced, it also helps to document who implemented the change in case there needs to be a follow up. We use GitHub, and GitHub has always been keen on collaboration. Their early slogan was “social coding”.\nWell, in January 2018, GitHub added support for commit co-authors and it’s great. 1 2 Co-authors even get attribution in their contribution graph. Adding co-authors to your commit can be done manually by appending a Co-authored-by trailer for every\nco-author when writing a commit message. A commit message title\n\nA commit message body.\n\nCo-authored-by: Jane Doe <jdoe@example.com>\nCo-authored-by: Thomas Anderson <mr.anderson@example.com> Why we built Git Mob If you collaboratively code and rotate co-authors often, you may find the list of co-authors a chore to manage.\nIt’s a cumbersome and error-prone process to manually record everyone who’s worked on a change, especially when it’s\nlimited to a specific way that GitHub understands. For people who pair or mob program daily in their work environment,\nthere has to be a better way where some of this would be automated. There are existing CLI tools that help to keep track of co-authors in Git. 3 We’ve used many of these over the past few years, but these tools were developed before the Co-authored-by trailer\nwas supported by Git and GitHub. Instead the usual approach they took was to record one person as the author, and the other\nas the committer. 4 This has some drawbacks, such as co-author support being limited to two people at any time, and\ncommits losing their co-author metadata when history is rewritten (e.g. when using a command such as git-rebase).\nSome of these tools introduced workarounds to help with those problems, and though it helps, they’re still noticeable\npainpoints in the user experience. We couldn’t find a tool that worked with the new commit message trailer, so we decided to build Git Mob during our\n20% time that we get at Findmypast. Git Mob is a command-line tool to manage co-authors and to easily add\nthe metadata to commit messages. Getting started with Git Mob Install Git Mob with npm . npm install --global git-mob Once installed, the commands git mob and git solo will be available. Create a JSON list of co-authors in your user profile directory called ~/.git-coauthors . { \"coauthors\" : { \"jd\" : { \"name\" : \"Jane Doe\" , \"email\" : \"jdoe@example.com\" }, \"ta\" : { \"name\" : \"Thomas Anderson\" , \"email\" : \"mr.anderson@example.com\" } } } Note that a person’s initials are used as the key in the “coauthors” object. When using the git mob command, initials are given as arguments to identify co-authors. $ git mob jd ta\nRichard Kotze <rkotze@example.com>\nJane Doe <jdoe@example.com>\nThomas Anderson <mr.anderson@example.com> First output line is the author set on the user machine, and the remaining are the co-authors. The git solo command clears co-authors when working on your own. $ git solo\nRichard Kotze <rkotze@example.com> Try Git Mob Try Git Mob VS code extension and here is the Git Mob repository . People who spend a lot of time writing code together will benefit most from this tool.\nThose who code solo most of their time won’t need this. You can take a look at our Git Mob workflow on how to use it. Please try Git Mob and let us know what you think.\nThe project is still in its early stages and might have bugs or lack some features you’re looking for.\nWe encourage you to submit feedback and contribute so we can make it better together. Footnotes The Co-authored-by metadata in commit messages is an agreed-upon trailer used by git-core contributors .\n  GitHub uses this metadata to find the GitHub profiles of the co-authors, then visualises the data. https://github.com/peterjwest/git-pair, https://github.com/hiremaga/git-mob, https://github.com/git-duet/git-duet GitHub has quietly supported multi-author commits for years, but the process wasn’t as friendly as it is now. ↩ It’s worth noting that GitHub hasn’t created a new standard for recording co-authors. ↩ Here’s a few of those projects: https://github.com/pivotal/git_scripts#git-pair, https://github.com/chrisk/git-pair, ↩ You can view the author and committer of commits with git log --format=full . ↩", "date": "2018-04-12"},
{"website": "FindMyPast", "title": "GraphQL Schema Modeling: Part 1", "author": ["Jae Bach Hardie"], "link": "https://tech.findmypast.com/graphql-schema-modeling-1/", "abstract": "The power of GraphQL is that it allows a client to query all parts of a system simultaneously, flexibly and safely. The difficulty of GraphQL is that you now need to publish a schema that describes every public-facing part of your system while being consistent, coherent and understandable. This is often a positive challenge — the need to publish a consistent public API can guide backend development to better patterns — but it is a challenge nevertheless. At Findmypast we’ve been early adopters of GraphQL and have recently become early adopters of schema stitching as a way to keep large schemata manageable. This is Part 1 in a series of posts about patterns and pitfalls we have seen when authoring schemata. Pattern: Entities and Value Objects An Entity is an object defined by its identity. The data associated with the identity might change but the entity is still the same “thing” and is treated by the system as the same. For example, a User would be an Entity because you can imagine properties of that User changing (email address, name, address) while it still referring to the same individual. Entities are modeled as GraphQL Objects with an id field: type User {\n    id: ID!\n    name: String!\n    email: String!\n    address: Address\n    activeSubscription: Subscription\n    familyTree: FamilyTree\n} Many GraphQL client frameworks will leverage the id field for automatic cache updating and normalisation. Even if they don’t it’s useful to ensure that clients can treat two entities with the same ID as the same object by having them follow a few rules: An ID must uniquely identify an entity for that specific entity type. Watch out for accidental id clashes or two parts of the system using the same entity to represent data from non-synced data sources. Given an ID and entity type the client should be able to fetch any field from that entity. It’s possible to work with entities that don’t fulfil this requirement but you will quickly run into trouble designing resolvers if entities aren’t independently queryable. Value objects, meanwhile, are objects which are identified not by an identity field but by their value. Two value objects are the same object if they contain the same data and different if they do not. The most important consequence of this is that it is impossible to mutate a value object: if you change any data, it is no longer the same object. All scalars are by necessity value objects but GraphQL Objects may also be value objects. Take the Address object above, modeled thusly: type Address {\n    streetAddress: String!\n    postcode: String!\n    county: String!\n    country: String!\n} This is a composite value object. The only operation you can perform with an Address is to create a new one. Do not be fooled by the fact it has child fields. Those fields might be other composite value objects or even entities! That doesn’t mean it needs to be converted into an entity. Pitfall: Entities masquerading as Value Objects A common pitfall is to express something as a value object in the schema when it is an entity in the backend model. Take FamilyTree above. This is a simplified model in which a user can create only one family tree on the service (it’s the kind of feature we provide in Findmypast). Let’s say that when creating it for the first time there was no use case that required the tree to be separate from the user object, so we modeled FamilyTree as a value object and were done with it. Saved us writing code we didn’t need to fetch a tree by ID after all. However, design smells quickly start appearing when designing mutations. Say we want users to be able to add family members to their tree: type Mutation {\n    addPersonToTree(userId: ID!): AddPersonToTreePayload\n}\n\ntype AddPersonToTreePayload {\n    success: Boolean!\n    message: String\n    updatedUser: User\n} Having to use the user ID to update a tree already feels janky and then we realise that the mutation has to return a whole User to update the client. If we tolerate this and keep going down this path, eventually we’ll end up having to implement multiple trees per account or the capability to share trees with other users and we’ll be faced with a model that simply cannot be used for that but would now be very expensive to refactor. A little extra work making entities up front can save you a lot in the long run. Pitfall: Non-idempotent queries GraphQL queries should be idempotent . That is, if executed with the same variables and the same context they should return identical results. What does this have to do with entities and value objects? Well, remember that entities are identical if they share an ID while value objects are identical only if they contain the exact same data. Errors in modeling objects can easily introduce dangerous non-idempotency to your queries. Imagine that in the family tree example above we still resist remodeling FamilyTree as an entity. To share a tree, we just give it a sharing ID and make a Query that fetches a tree to display to a guest user: type Query {\n    sharedFamilyTree(sharingId: ID!): FamilyTree\n} We have now introduced non-idempotency into the system. The tree owner could be making modifications to their tree, causing the same query with the same variables to return different results. In the client we would need to have a way to trigger a refetch to update the view. However, since the object does not have an ID it is not enough to have a reference to the FamilyTree to request an update. The UI component needs to know what sharingId was used originally to fetch it. This can lead to incredibly messy client code where pieces of additional information not logically part of any object have to be passed around everywhere. It can also make caching and schema stitching harder in subtle ways. Pitfall: Entity cannot be fetched by ID This represents the case where you do make an object an entity but decide against making it queryable by ID. This often happens when interacting with backend services that don’t expose their entities, making it very laborious to add that kind of query. Saving yourself the effort can sometimes be justified but I’ve more often seen such a short-sighted decision come back to bite the team that took it. If an entity cannot be fetched independently: Returning an updated object from mutations must now be done manually in every case, rather than relying on the query-by-ID endpoint. There’s no easy way of resolving a reference to the entity elsewhere. If a Record can only be fetched as part of a Search then what do we do when a User contains a list of Record IDs as ViewedRecords ? With the query-by-ID endpoint we could easily resolve those IDs into Record objects during a query. If a UI component wants to provide the user an option to refresh the data it cannot do so with a reference to the entity, it also needs the full query the entity was originally fetched with (the same problem we got with idempotency failures). Conclusion Spend some time thinking about your design before you start writing a graphQL API. Make things entities with unique IDs even if it’s a little extra work. And stay tuned for Part 2 where I’ll talk about pagination strategies.", "date": "2018-11-07"},
{"website": "FindMyPast", "title": "FMP Recruiting DO’s and DON’T’s", "author": ["German Munoz"], "link": "https://tech.findmypast.com/fmp-recruitment-do-donts/", "abstract": "We’ve been doing a lot of hiring here recently at FMP, both in London and Dundee (and we’re still hiring! ). Along the way we’ve noticed some patterns (and some anti-patterns!) and we thought it would be useful to share with the wider interwebz in the hopes that people will get something useful out of it.\nWith the caveat that each company will hire differently and has different standards, here are some DO’s and DON’T’s from the FMP Engineering team: DON’T apply for the wrong job. This one should be obvious, but you’d be surprised. Target only the jobs you think you’d be a good fit for.\nThis mistake is more common for recent graduates or people seeking a career change, since both will have a lack of experience. There’s no easy way to solve this, but it CAN be solved. You can volunteer with a charity or non-profit and create a software project for them, you can build and release your own mobile apps, create a web app just for fun and put it on a personal web site, etc, etc, etc. If you don’t have the experience, get creative. We like creative. DON’T have any typos or errors on your CV. Sorry guys, I know this makes us look like petty, grammar Nazis. But we’re not. If you don’t take enough care to check the spelling or check that there aren’t any errors on your CV, what’s to say you don’t have a similar disregard for your code or regular work tasks? So make sure it’s error-free. Read it out loud, ask someone to proofread it for you, etc. A CV is a first impression, it should be spotless. DO prepare. A lot. Lack of preparation can take many forms. Being late for the interview; not knowing much about the company; not researching the technologies we use (they’re usually on the job spec); not remembering details about your past roles/projects that are on your CV, etc.\nIf you put in the work and prepare for an interview, it shows. We like people who prepare. It shows us that you care and that you will likely be prepared when you come into work every day. DO ask questions. If you’ve taken the time to prepare (see above), you should have loads of questions for the company and each of your interviewers. You should at least do this to avoid accepting a job offer from a crap company! Make a list of things that are important to you and create a list of questions based on that. Are you passionate about continuous delivery? Pair programming? TDD? Are you wild about a particular form of agile methodology? Show us what you care about. DON’T lie. I know, another really obvious one. But again, don’t lie. We can tell. Enough said. Technical questions DO use TDD We’re a TDD company. We’re serious about it. If you are too, we’ll be a good fit. If we send you a tech test and the answer doesn’t include a battery of unit tests, this will be bad. Also, all the tests should pass! DO keep it simple When answering technical/coding questions, resist the urge to over-engineer. Focus on the given problem. What’s the simplest solution that can answer the problem? DO implement what you’re asked for Another one that seems obvious, but it’s actually very easy to get side-tracked in a coding interview. Keep focused on the task you are given. Your ability to focus is part of the interview as well. At tech job fairs DO prepare. See above. You’ll make a much stronger impression if you know who will be at the fair and prep beforehand to speak to them. Do your homework! DO be smart about how you market yourself. Are you a developer? Great. What kind? Java? C#? Front end? Back end? Full stack? Javascript? React? Know how to sell yourself and be as specific as possible. Trust us, being “general” (i.e. ambiguous) will not work in this setting. DO summarise what you do in a few sentences. There are loads of people at job fairs. You need to get your spiel down to a few sentences explaining what you do and what you’re looking for. If you know which companies you’re aiming for and know what they’re looking for, this should be straightforward. Good luck with the job hunt. And now that you’re a job seeking expert, take a look at our open positions .", "date": "2018-12-04"},
{"website": "FindMyPast", "title": "Highly Available Postgres Databases", "author": ["Andy Wilde", "Leo Cardoso"], "link": "https://tech.findmypast.com/highly-available-postgres-DBs/", "abstract": "High Availability (HA) means that users can run their critical production services without worrying about a database becoming unavailable due to the failure of a database component. It improves the reliability of running a database by minimizing downtime and ensuring that the service is never down for more than a few seconds in the event of a failure. A HA database cluster includes a source database instance (master) and one or more replicas (slaves). If the master becomes unavailable, an automatic failover is initiated to one of the slaves, with the slave promoted to the new master. When the old master becomes available the HA solution, should implement automatic recovery to synchronize it with the new master and so become another slave. We looked at a number of solutions for implementing HA with Postgres. One solution Pgpool recommended in Postgres’s own Wiki turned out to be a disappointment due to poor documentation, flaky performance and the lack of support for automatic recovery. An alternative commercial solution ClusterControl had some nice features, but the free version didn’t support automatic failover and recovery, and the premium version was prohibitively expensive.  The next option investigated was Patroni , an open source solution  which had good documentation and from testing proved to reliably implement automatic failover and recovery . A shame this wasn’t in the list of recommended solutions on the Postgres Wiki! Patroni Patroni is a solution developed by zalando , an e-commerce company based in Berlin. It is used by GitLab, Red Hat, IBM Compose and Zalando, among others. It can run on VMs (or bare-metal), docker containers or kubernetes. For simplicity, we tested Patroni on VMs. Patroni runs on the same machine as the databases so it can control the postgresql service. Patroni supports three synchronisation modes: Asynchronous mode , Synchronous mode , and Synchronous mode strict . Asynchronous mode is performed by postgres streaming replication. It does not rely on slaves to update the master, so there is the possibility of the databases getting out of sync in case the master goes down. Synchronous mode is controlled by patroni and prevents the former from happening as it keeps the slaves in sync with the master. However, when there are no slaves, the master is switched to standalone, and will accept write commits. This might cause inconsistency if a slave comes up, as although the slave is not going to be promoted to master, it will still be able to serve out of sync data for reads 1 . Synchronous mode strict is also controlled by patroni but the master will not accept commits unless there is a slave it can synchronise with, preventing the former issue. Consul, HAProxy & Keepalived For HA it’s important to store the state of the cluster in a reliable and distributed manner. We already use Consul for this purpose with our Kubernetes cluster, so it made sense to re-use this solution for Patroni. For a database cluster it would be cumbersome to manually track the endpoint a client should connect to. HAProxy simplifes this by providing a single endpoint forwarding connections to whichever node is currently the master. An additional benefit is that HAproxy can be configured to provide a read-only port that will connect to slave, allowing distribution of read-only queries across the cluster. At this stage if we only used one instance of HAProxy then this would be a single point of failure and our solution would not be HA. The answer, as always, is to have multiple HAProxy instances, but to use Keepalived to provide a virtual IP that points to the live HAProxy . Backups HA does not obviate the need for backups. For example a malicious query might wipe your important data and this would instantly be replicated to your slaves. Barman is our tool of choice for performing backups of Postgres DBs. The full backups are performed overnight with continuous archiving of the write ahead log files allowing point-in-time-restore. We configured Barman to use an SSH connection on HA proxy and a postgres connection for WAL archiving. We hoped to use a read-port port on HAProxy, so that continous archiving load would occur on a slave. Unfortunately, Barman does not support this configuration 2 . Migration to the cluster In order to migrate an existing single DB to a HA cluster we developed Ansible 3 scripts to install Patroni , configure Consul , Barman and HAProxy . The Ansible playbooks were executed many times on a test cluster to prove their idempotency and gives us confidence in the migration process. The migration followed these steps: Create two new VMs. Run the postgres playbook on the current database node which will now become cluster master. Run the postgres playbook on the VMs which will become the cluster slaves. Run the HAProxy playbook to configure HAProxy to provide read-only and read/write ports. Configure Barman to take backups. Switch clients to use the read/write or read-only ports provided by HAProxy. Following the migration the architecture of the cluster looked like: Overall we believe the final technology choices and implementation will provide a reliable HA solution, and that developing using infrastructure-as-code principles and a test cluster that we could easy rollback allowed a very smooth migration of a live production database. Footnotes In our tests, we verified that extremely large write commits can be a problem for the synchronous mode case when there is just a standalone master. In this case, when a slave is restored, it will have problems to synchronise because the write-ahead logging file will not have the data anymore. So, a manual recover has to be performed. We assume that, if necessary, the write-ahead logging file size can be tuned as well as the maximum_lag_on_failover which defines how far a slave can be from the master for the automatic failover to happen. ↩ According to Barman “When backing up from a standby server, the only way to ship WAL files to Barman that is currently supported is from the master server.” ↩ We used the patroni role developed by Kostiantyn Nemchenko plus our own additional role to perform some custom configuration. ↩", "date": "2019-04-25"},
{"website": "FindMyPast", "title": "The Journey to Kubernetes High Availability - Part 1", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/road-to-k8s-ha-part-1/", "abstract": "Findmypast (FMP) software engineering practises and operational practices have changed considerably over the last few years. We’ve moved from a “classic” monolithic web service, where deployments to production were applied possibly weeks after the end of a sprint, to a microservices approach where we have continuous deployment to production and the majority of the FMP service is managed by Kubernetes. We didn’t just jump straight into Kubernetes, the journey along that path was long and winding. These set of posts document the journey through that long and winding road. The Monolith Back in days of yore - 2015 - The Findmypast web service was a C# ASP.Net MVC web application with a SQL server backend. Pretty standard stuff back then. The code base was split into the main FMP website along with a number of common library assemblies. Development process was sprint based. We branched off the master, coded for a couple of weeks and, if the feature was ready, we would push the changes to our integration environment. Our QA team would test the feature changes on integration and, if all went well, changes in the branch would be merged into master and then deployed into production. We used blue green deployment to deploy changes to production. 20 Windows servers running IIS were allocated to the green bank, while another 20 were allocated to the blue bank. In the example above, the blue bank is serving web site traffic while the green bank is offline. Deployment involves updating the green bank with the latest changes, running automated tests against that bank before configuring the load balancer to switch colours and direct traffic to the green bank. In case of disaster, rollback of the site was as easy as switching back to the offline colour. Scaling the site involved deploying the same code to both banks and configuring the load balancer to use all hosts in both banks. (This we call cyan deployment and was used when we expected a lot of traffic to be hitting the site.) So, why did we change? We had a number of challenges that we wanted to overcome with the above approach: We couldn’t scale the service effectively. Sometimes the servers underutilized. At other times, we need to bring in the whole bank of 40 servers to service the load. We couldn’t automate scaling of the service depending on load. Our release velocity was slow. Features could take weeks to be deployed to production. Defects found with the feature after it was pushed to production were investigated by the engineering team responsible for the feature. However, those feature changes would have been made weeks earlier - it’s hard for engineers to context switch from their current task to debug code written weeks ago. The distance between the code being committed and finally deployed into production meant that it was sometimes hard to determine what code change was the root cause of a problem. That was compounded with the fact that a deployment release would have involved multiple branches being merged into trunk. Identifying which branch/feature caused a problem wasn’t always straightforward. And, of course, different teams working on different branches can cause merge hell if teams are working close to the same codebase. We wanted to deploy to production faster as well as spot and fix problems earlier. Also, we wanted “developer joy” - no painful merge conflicts, deployments that passed automated tests and went straight into production. So, we started the process of migrating from the monolith to a microservice architecture. If you’re interested in our process of how we approached breaking apart the monolith, check out Extracting a Microservice from a Monolith by Neil Crawford. The microservice architecture Our initial architecture looked a little like this: (You may also want to watch the Deployment and Infrastructure at Findmypast given by Sash from Findmypast which also discusses this infrastructure.) That’s a busy wee diagram, so let’s break it up a little. Service Virtual Machines (VMs) Each microservice had a number of Linux virtual machines, each of which had docker engine installed. Each service node typically had four docker containers running: A blue service container A green service container A registrator container CAdvisor container to provide container instrumentation. (Not important to this discussion and not on the diagram!) In our first incarnation of the infrastructure, we continued to use blue-green deployment. Service 1 VM , in the diagram above, has the blue container as the live/online colour while Service 2 VM has green as the online colour. Briefly - we’ll cover this in more depth later - we would easily swap between the two colours by routing to a different container in the VM. The registrator container was responsible for monitoring the docker containers running on the VM and notifying the service registry ( Consul ) when containers were added or removed. Registrator would register the docker container IP address, external port and other metadata - such as the colour of the container. Continuous integration Our CI pipeline (Teamcity) was responsible for building a tagged docker image for a service, publishing that image to our internal docker repository and deploying the image to the service VM. Service Registry and Routing The heart of the new infrastructure is the Consul service registry along with the NGINX reverse proxy. Consul is a registry that holds: The service name, specifically which docker containers are associated with a service; The IP address of those containers; The exposed port of those containers; Metadata about the container, such as the colour. Here’s an example of a payments app service within Consul: This service is tagged as a production service, along with the blue/green colour. It lists the nodes that the service related docker containers are hosted on. Consul also supports health checks on the node (VM) level as well as the service level. If a node becomes unhealthy, then that node will be marked as unavailable. Similarly, if a docker container becomes unheathly then that specific container on the node is marked as unavailable. So, Consul knows about a service and which healthy docker containers are associated with the service. But, Consul is a registry, it holds this information but it doesn’t provide routing. (Actually, it is a DNS server as well, but DNS looks up IP addresses, but we also need the docker container port information which DNS doesn’t provide.) How do we use the information in the registry to route requests to the correct docker container? To achieve that we use NGINX. The NGINX configuration file is generated on the fly by consul templating . Changes in the Consul service registry are picked up the consul templating application running on the NGINX server. It then regenerates the NGINX configuration from a template file that reads the registry and updates the NGINX routing accordingly. Blue-Green Deployment Consul also holds a Key-Value store. We used the KV store to save information about the state of a service - including which colour is currently online. When we want to switch which colour is serving live traffic, we update the relevant key in the KV store. That change is picked up by consul templating, which re-writes the NGINX configuration to route to the new colour. The CI pipeline is responsible for the deployment: The CI gets the current offline colour from Consul, builds, tags and deploys a docker image to the service VMs, overriding the existing offline docker container with the new container; Automated smoke and acceptance tests are run against the new offline container; If all tests pass, Teamcity updates the colour key in the Consul KV store, forcing consul template to update the NGINX config. The flip of a colour takes a few milliseconds while NGINX reloads its configuration. Scaling Deployments Scaling the new services was not ideal. Each service had a dedicated number of VMs associated with it and each of these VMs only held the blue/green containers for that service. Scaling the infrastructure meant provisioning new VMs and then deploying the service containers onto the new VM. Why did we go down the road of having only one set of service blue/green containers on a VM? We could have added many more service containers into the VM and registrator would have easily picked up on the container tags and registered with Consul. For that matter, why did we choose one service per VM, rather than using something like Docker Swarm ? The reason was stability; At the time, we found that docker swarm wasn’t production ready. For example, containers that crashed out of their memory limits would sometimes bring down the entire node, wiping out every container on the node. With enough VMs in the cluster this may not have been a big issue, but other issues with docker swarm forced us to keep it simple. It wasn’t elegant, but it worked. Conclusion FMP started off with a typical ASP.Net MVC stack and started the progress of breaking up the monolithic codebase into microservices. Our development and deployment processes changed, we started to use Trunk Based Development and Repository Driven Development . We updated our CI pipeline to use shared deployment code, so all microservices were deployed in the same way. This enabled us to quickly roll out changes to the deployment pipeline to all services. All of these new approaches helped us to deliver more quickly into production. We committed small code changes and often, all changes pushed to trunk were deployed to production (after automated tests gave us the green light to go to production). Continuous deployment was becoming the norm within FMP. Code that wasn’t production ready was hidden behind feature toggles . So, we were moving faster with regards to our development process, but the infrastructure still wasn’t strong enough. The next blog post tells the story of the dark age and the continuous struggle we had with the deployment infrastructure.", "date": "2019-05-23"},
{"website": "FindMyPast", "title": "The Journey to Kubernetes High Availability - Part 2", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/road-to-k8s-ha-part-2/", "abstract": "Findmypast (FMP) software engineering practises and operational practices have changed considerably over the last few years. We’ve moved from a “classic” monolithic web service, where deployments to production were applied possibly weeks after the end of a sprint, to a microservices approach where we have continuous deployment to production and the majority of the FMP service is managed by Kubernetes. The first blog post detailed the start of the journey, where we have started to pick apart our monolith code base and start to use microservices to deliver the FMP service. This second blog post continues the journey… The Dark Days of Winter 2017 At the end of the last blog post, we had an infrastructure that used Consul as a service registry and NGINX acting as a reverse proxy into our docker containers. For a while, everything was fine, but as more microservices came online we started to see more and more failures with deployments. These failures were generally due to infrastructure issues. These were dark days, with the team spending a lot of time fire-fighting issues with the deployments. We had a whole host of symptoms, including: NGINX routing to wrong containers, or not routing at all; Smoke tests on deployed containers failing, meaning the active live colour wouldn’t be flipped; The active colour being flipped but not reflected in the NGINX configuration. The root of all evil the problems was consul templating. It (apparently) couldn’t cope with the amount of changes within Consul itself. It either crashed out when generating the NGINX template (which it did frequently) or simply got slower and slower. Monitoring of consul template showed a large amount of messages being received. The end result was that NGINX configuration was invalid, which then caused the routing issues. NGINX as a central routing point was a single point of failure. In those dark days, most of the teams’ time was spent investigating consul template (and restarting consul template). We had to find a quick solution and a number of options were discussed: Docker Swarm. Once again we looked at - and rejected - Docker Swarm to manage the containers. We were still concerned about docker memory issues taking out a whole node; A NodeJS and Elixir service discovery client was discussed. The idea here was that services, rather than relying on DNS to find a service, would use the service discovery client to determine the name (and port) of the service to talk to. This idea was rejected, partly because it would involve a lot of retro-fitting to existing microservices, but mainly because we didn’t feel it was the responsibility of the microservice to “discover” how to talk to another service. Essentially, it should just use an URI and let the infrastructure sort it out; To counter the above argument, a side-car model was discussed where we would inject a service discovery side-car into either the docker container or the host VM. The side-car would intercept network requests and route then accordingly. This also was rejected, it was complicated and no-one in the team was a networking expert. The solution the finally settled on wasn’t universally loved, but it had the benefit of being a quick win, easy (in relation to the other options) to implement and had minimal impact on existing microservices. Here’s what we ended up with: The solution isn’t a million miles away from the earlier infrastructure. The main change is that rather than having NGINX/Consul templating as a single point of failure, we’ve shifted the load balancer (running as a docker container) down into each service VM. Consul templating need only update the configuration for that specific service, so the load on consul templating is spread across each service. In this architecture, consul templating was much more stable. A major change - which isn’t obvious from the diagram - concerns service to service routing. How does a service call another service and route to the correct load balancer (which are now hosted on a range of VMs)? The approach here was to use Consul Service Registry as a DNS server. Recall that Consul will hold all information about all the docker containers registered on the VMs, including the new NGINX load balancer containers. A URL such as production.service1.consul.service would resolve using Consul (via round robin) to one of the VMs running the service service1 . The downside was that we needed to expose those load balancers on a specific port on the VM - we choose that to be port 3333. Services could interact with our services using a a url in the format {environment}.{service-name}.consul.service:3333 . The hard coded port, in particular, really upset a few people in the team. It’s not service discovery, they would cry. However, it was a necessary evil in order to get the solution in place with very little pain. The diagram also doesn’t make it clear the redundancy built into moving the load balancer down into the VM. Each service had at least 3 VMs associated with it: A service URL was resolved using the Consul as the DNS service. In the above example, we have three VMs, with a load balancer exposed on port 3333 and IPs of 172.21.54.1 , 172.21.54.2 and 172.21.54.3 . Consul DNS will round robin through those (healthy) IPs. Should one of the nodes fail, or the load balancer health checks fail, then the IP is removed from the healthy upstream list and Consul won’t return that IP back from the DNS request. From a microservice point of view, the changes required to make this happen were minimal: Update any service URLs in the source code to use the new consul.service:3333 addresses; Deploy a load balancer as part of the deployment process. The latter was made easier by the automated tools we had already developed in house to scaffold up services and automate the CI build pipeline. Towards the end of 2017, all microservices were updated with those changes, everything settled down and deployments were no longer failing. :grin: With the deployment stabilised, we could turn our attention elsewhere. Hmmmm….what’s that shining over there in the distance? Could it be the fabled container orchestration tools Kubernetes? Why Sir! I do believe it is! Part 3 of the journey is about to begin!", "date": "2019-06-24"},
{"website": "FindMyPast", "title": "The Journey to Kubernetes High Availability - Part 3", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/road-to-k8s-ha-part-3/", "abstract": "Findmypast (FMP) software engineering practises and operational practices have changed considerably over the last few years. We’ve moved from a “classic” monolithic web service, where deployments to production were applied possibly weeks after the end of a sprint, to a microservices approach where we have continuous deployment to production and the majority of the FMP service is managed by Kubernetes. The second blog post detailed the issues we encountered with our service discovery and deployment infrastructure. This third blog post continues the journey… The Journey to Kubernetes With our deployment and service discovery infrastructure in a reasonable state, the team could finally afford to spend some time getting on with the day job. However, the day job included improving the developer experience, making deployments easier, debugging easier, etc. We turned our attention to looking for improvements to make. At the time, our focus was not service orchestration tools like Docker Swarm or Kubernetes, but a more general problem; engineers were finding it hard to trace requests through the system, from microservice to microservice to microservice. Our logging guidelines specified that certain headers - including the correlation id - be added to every request so that these common headers can be centrally logged, in the hope that the logs can easily find all log entries associated with a specific user request. In reality, not all services forwarded the headers correctly. Also, not all services added the correlation ID as a field to the log entry. Log entries, also, were sometimes not helpful, either the log entry didn’t contain any useful information or there were too many log entries, adding a lot of noise to the system. The team was focused on improvements to help engineers (especially the on-call engineer) to tracing requests through the system. Around the same time, the team were using their 1 day a week tech debt time to investigate orchestration tools like Docker Swarm. (FMP allow engineering teams one day per week to work on non-feature work tasks, such as fixing technical debt or investigation of new tools.) On tech debt days we spent time installing and investigating tools like Docker Swarm, Nomad and Kubernetes. During these weeks, we actually looked at Kubernetes and rejected it as too hard to learn and focused instead on Docker Swarm. (We also installed and investigated Nomad for a short while). Outside of tech debt days, our focus on request debugging led us to OpenTracing . This looked attractive, it provided a visual means of tracing distributed requests through our system, along with metadata such as the URL request, status code, time taken for a response, etc. During our investigation of OpenTracing, we setup a Jaegar server and updated a couple of our NodeJS microservices to support distributed tracing with Jaegar. The results were promising but a worry was that each microservice would need to be updated to support sending requests to Jaegar. It also added another external dependency for the microservice as well. We would also have to spend some time writing our own open tracing client in Elixir as well, since a number of our microservices were developed in Elixir. Istio Service mesh Around this time, we also came across Istio . This was a service mesh that sat on top of Kubernetes. A service mesh provides a whole host of goodness that extends the networking that comes with Kubernetes (and Nomad) with traffic management, policy enforcement and telemetry including distributed tracing and network visualisation tools. The nice thing about Istio was that these services are pretty much invisible to the microservice. In fact, the only changes required by the microservice to support distributed tracing with Istio were to forward on certain request headers. After that, the logging of requests to Jaegar was handled by Istio. This approach was much more attractive - minor changes to the microservices were required, but we would get a bucketful of extra tracking goodness including service telemetry, network topology visualisation with Kiali , distributed tracing, fault injection (useful for chaos testing!) and advanced routing. So, we had a choice. Go down the home brew approach and implement distributed tracing using our bespoke tools or move to Istio. The choice was pretty easy, were we thinking about orchestration tools anyway, so the choice to use Istio on top of Kubernetes was made. At the time, Istio was still in beta, but we figured that by the time we got a real Kubernetes cluster up and running Istio would have matured. (Also, we could pick and choose which bits of Istio we wanted to install - it wasn’t an all or nothing choice.) Hi ho, Hi ho, it’s off to Kubernetes we will go! We started down the path to Kubernetes quite slowly. We started with creating some virtual machines from our HyperV cluster and slowly went through the process of installing Kubernetes using the kubeadm tool. Progress was fairly slow at the start, the main issue was getting the networking within Kubernetes sorted out. Root causes of the issue was the Flannel manifest installed by kubeadm was out of date and the CNI plugin was also out of date. Once both components were updated, networking was working as expected and the rest of the installation went through smoothly. We carefully documented each manual step required to create the cluster from a vanilla virtual machine and reproduced those steps over and over again until we had them nailed. The manual steps were used to automate the provisioning on the cluster using Puppet. By the middle of April 2018 we could spin up a Kubernetes cluster via Puppet. Initially we have 2 clusters: A playground cluster. That was used to try out news ideas, new tools, updates to deployment scripts, etc. We broke that one quite a lot, but it was easy to spin up a new one. A staging cluster. Clearly, before we even thought about serving live traffic via Kubernetes we needed a lot of testing. The staging server was used as our testing ground, not only for how Kubernetes performed, but a test bed for how we would migrate our existing microservices across to Kubernetes. Our testing strategy was pretty straightforward: Update the deployment scripts for the microservices to deploy into the existing infrastructure as well as the staging cluster. We used Helm to deploy into K8s and our tools to scaffold up services was updated to add the Helm deployment scripts into a microservice. Use Consul to route traffic into both the old infrastructure as well as Kubernetes If all is well, route all traffic for the microservice into Kubernetes. We migrated each of the services one-by-one, upgrading the service deployment scripts and testing how the service performed in Kubernetes. Our infrastructure diagram now looked like this: In the diagram, we have a Kubernetes cluster. Two of the worker nodes in the cluster have the Service 1 pods running. They also have another pod which is a bespoke Consul registrator. This pod listens to API events from the control plane master node and, when a new service is deployed, registers that service with Consul. We have a Consul registrator running on each of the worker nodes (in reality these were nodes that handled ingress into the cluster). Kubernetes services then routed the request to a pod. Actually, in the interests of making the above diagram clear, the reality is a little different. Recall from the last blog post that the load balancer is listening on port 3333 for the service. All microservices talk to each other on port 3333 . In order to route into K8s, every load balancer not only has details about the containers running in its own VM, but also has knowledge of all the other service containers in the other VMs, as well as the IP addresses of the Kubernetes worker nodes. (Basically, the load balancer holds a list of all heathy microservice IP/ports registered in Consul). Each load balancer could route to any other container running that service, regardless of where the container was located. Requests hitting any load balancer would round robin between the service VMs and, of course, the Kubernetes nodes. Another useful tool was Kubernetes annotations. In order for service traffic to be routed from Consul to the Kubernetes cluster, the registrator in Kubernetes would check the status/existence of an annotation that told it whether to register/deregister the service with Consul. By modifying the annotation, we could dynamically control whether the service (in Kubernetes) was registered in Consul. E.g.: $ kubectl annotate service my-service fmp.register-consul=true --overwrite would tell the Kubernetes Consul Registrator to register that service, while fmp.register-consul=false would de-register the service. This was incredibly useful during initial testing. We could also route directly into the Kubernetes cluster - by-passing Consul - using a service address such as service1.integration.service.dun.fh . This would route directly to the service running in Kubernetes and the usual Kubernetes service configuration / pod configuration would ensure that the request was handled by a heathy pod in the Kubernetes cluster. The end game was to remove the old service.consul:3333 address and route directly into Kubernetes using the new service.dun.fh address. However, we couldn’t migrate over the new address until a microservice was completely divorced from Consul and deployed onto into Kubernetes. Breaking up is easy to do Once a service was stable within Kubernetes, we could start to de-commission the old service VMs and remove the dependency on Consul for service discovery. Decommissioning meant: Deleting the VM Updating the deployment scripts, removing the recently deleted VM from the deployment target Once all service VMs were removed, the only route to the service was via the service1.integration.service.dun.fh Kubernetes address. This however, gave us a problem - not all microservices would have been updated to use the new address - in fact most still used the older service.consul:3333 address. We need a way to handle both the deprecated 3333 address as well as the new Kubernetes address. Recall that the 3333 address was routed via Consul DNS to a load balancer on the service VM, But, we didn’t have any service VMs any more, we switched them all off. Our solution to that problem was to have a (temporary) NGINX proxy that proxied the 3333 addresses into Kubernetes. This NGINX proxy was registered in Consul, so as far as Consul was concerned it was just another service endpoint. The proxy stayed in place until all services had been fully migrated into Kubernetes and all services had replaced any 3333 URLs with the new Kubernetes equivalent. At this point, the infrastructure for a deployed service was a lot simpler: In the example above, service 1 is fully within Kubernetes. Service 5 is talking to Service 1 , but it’s still using the deprecated service.consul:3333 address. In that scenario, Consul (acting as  DNS server for the service.consul addresses) returns to the service the IP of the NGINX Kubernetes Proxy. That in turn will proxy that request into Kubernetes. However, if Service 5 had updated it’s URLs to use the new Kubernetes address (like Service 6 in the diagram above) then Consul isn’t used and the request is routed directly into the Kubernetes worker node. A couple of points to note here: The NGINX Kubernetes Proxy was only in place to support those microservices that hadn’t been updated to using the new address yet. Once all services had been updated, this proxy was switched off. The Kubernetes address of service1.integration.service.dun.fh was intended for those services outside of the Kubernetes cluster to access services inside the cluster. Calls to services within the cluster didn’t (shouldn’t!) use this address - the Kubernetes address of service1 or service1.integration . Using the full service.dun.fh address would only route of the cluster in order to route back in again - madness! Blue green no more… Another change made with the move to Kubernetes was to drop the existing blue / green deployment and replacing that with the Kubernetes rolling update deployment strategy. We could have gone down the blue / green route, but it would have meant that the resources needed for a service would double (one set of pods for the blue colour and another for the green colour.) But, more importantly, it wasn’t actually required. Blue / green in the old deployment system allowed us to verify that updates to a service were performing as expected by deploying the recent updates to the offline (backstage) colour, testing against the backstage and, if everything worked, flipping the blue / green colours in Consul. In Kubernetes, we could achieve the same result by using the rolling deployment strategy. For example, let’s assume that service1 has 4 pods running in Kubernetes and we want to deploy a new version. When we deploy the new version, Kubernetes will terminate one of the existing pods. Routing to the other three pods will still work as normal, so our service is up and available, albeit with one less pod. A new pod is created with the latest version of service1 . If it passes all its health checks, then the new pod is added to the service endpoints and another of the older pods is terminated and the process repeats until all pods have been updated. If the new pod doesn’t pass its health checks, then the deployment is stuck and nothing else happens - the existing pods are still alive and the service is still responding to requests. We detect a stuck deployment and issue a rollback - the new pods are deleted and a new pod is created with the previous image (version) of the service and everything is restored back to its state before the deployment began. Our deployment scripts actually use Helm to package and manage our deployments, including rollback of stuck deployments. Rollout to production By the end of August 2018, all microservices had been migrated across to our staging Kubernetes cluster and Virtual Machine infrastructure (for those microservices) had been decommissioned. But, this was just our staging cluster - the live site was still being serviced by the VMs with blue/green deployment. It was time to move into production. We provisioned a Kubernetes cluster with: 1 control plane (master) node: 8GB RAM and 4 CPUs 8 worker nodes: 32GB RAM and 16 CPUs The specifications for the cluster were pretty much double that of the specs we used for the staging cluster. The Kubernetes metrics produced for the staging cluster also guided us in estimating resources required for the worker nodes. (The worker nodes are also virtual machines, so changing memory or CPU is trivial). At the time of writing, CPU usage across the cluster is 11%, memory is 30%, so we have a lot of capacity in the cluster. e.g.: We can easily drain a worker node (move the pods to another node) in preparation for maintenance on the node. Once again we performed a staged migration of services into Kubernetes, updating the deployment scripts to deploy the microservice into Kubernetes and the old VM/blue/green infrastructure. Traffic was routed both to Kubernetes and the VM infrastructure. Once we were happy the microservice was stable in Kubernetes, we decommissioned the VMs for the microservice and routed all production traffic into Kubernetes. We’d already been through this process a few times before when migrating to the staging cluster, so the migration to production was pretty straightforward and pretty painless. By the end of October, all of the microservices were migrated across to the production Kubernetes environment and the vast majority of the older VM infrastructure was gone. :grin: A quick time line of events showed the fairly rapid progress we made to using Kubernetes to manage our deployments and production traffic: Mar - Apr 2018 - Investigation into distributed tracing tools and the agreement to move ahead with Kubernetes/Istio May 2018 - K8s provisioning scripts created and the Kubernetes staging cluster created Aug 2018 - All microservices migrated across to the staging cluster Oct 2018 - Production cluster created, all microservices migrated to production cluster. By this point things were looking pretty good - we could automatically autoscale services when needed, production infrastructure was greatly simplified (8 worker nodes rather than 100’s of VMs), we started to leverage goodies like Prometheus metrics and alerting for services. At this point, our infrastructure looked like this: We still have our C#/.Net stack - bits of that are still serving production traffic. Most of the traffic is served via Kubernetes (8 worker nodes rather than the four in the diagram) with service pods running on a variety of worker nodes for redundancy. Most services have at least 3 pods to ensure uptime should a worker node become unavailable. Critical services have auto scaling to ramp up the number of pod replicas should we need it. The only problem with the above diagram is that we have a single point of failure with the master node. How we migrated to the high availability Kubernetes is the subject of the next post.", "date": "2019-07-24"},
{"website": "FindMyPast", "title": "The Journey to Kubernetes High Availability - Part 4", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/road-to-k8s-ha-part-4/", "abstract": "Findmypast (FMP) software engineering practises and operational practices have changed considerably over the last few years. We’ve moved from a “classic” monolithic web service, where deployments to production were applied possibly weeks after the end of a sprint, to a microservices approach where we have continuous deployment to production and the majority of the FMP service is managed by Kubernetes. The third blog post detailed how we migrated to deploying our production services into Kubernetes. This fourth blog post details how we configured our production Kubernetes environment for high availability. The Journey to High Availability Kubernetes For a few months, everything went well with the Kubernetes cluster. Our automation helped us upgrade Kubernetes on production a number of times without incident, until - of course - we had an incident with the Kubernetes master control node. A reboot of the control node failed to start the server correctly - the root disc was marked as read-only. After 10 mins or so, the worker nodes started to complain that they couldn’t talk to the API server and services began to fail. The outage didn’t last too long - 20 mins or so - but once service was restored our attention turned to avoiding this situation in the future. Clearly, we needed high availability in case the master node threw more issues at us. At the time, our Kubernetes cluster looked like this, One master control plane node with one API server and the etcd backend database held on the master with 8 worker nodes: ![Kubernetes cluster, right now](../images/2019-10-17-migrate-k8s-ha/right-now.png “”) We have a few single points of failure here: The master node itself could become unavailable, killing both the api server and the cluster data store saved in etcd. The api server pod could become unavailable. The etcd pod could become unavailable. To mitigate these SPFs, our end goal is to update the cluster to an external etcd toplogy with three control plane nodes and external etcd cluster: “”) While upgrading the cluster, we wanted zero (or near zero) downtime for the website while the upgrade was taking place. To mitigate the blast radius of the changes, we took a phased approach to upgrading the cluster. Each phase could be completed in isolation and easily rolled back if anything went horribly wrong. The upgrade phases were: Add a load balancer between the worker nodes and master control plane. Extract etcd database to a separate high-availability cluster. Add new control plane nodes. We took a rather cautious approach to the planning and execution of the phases: For each phase, we identified and practised the steps to take to complete the phase. The manual steps taken were documented and members of the team reviewed the documentation by following the steps. Unclear guidance was improved. Once we had a reproducible set of (manual) steps we could take to upgrade the cluster to HA, we automated as much as possible (using Ansible scripts) to reduce the number of human mistakes. A new guide was created - the playbook for each phase - that described step by step what scripts to run and what manual steps to take to complete the phase. Again, that playbook was reviewed by the team by following the steps in the playbook, updating unclear instructions where necessary. When we’re ready to perform the upgrade, we had a playbook for each phase and each team member had completed the update to HA on our playground Kubernetes cluster at least once. We were ready to go! Phase 1: Adding a load balancer This, on the face of it seemed like an easy thing to achieve. We already have an NGINX server in place, so adding new upstreams that proxied onto the Kubernetes master control plane nodes to the NGINX configuration was straightforward. However, the Kubernetes API server refused to accept requests proxied via NGINX. We need to create new API server certificates that included the IP address of the NGINX server in the Subject Alternate Name (SAN). Once they were updated, we copied the new certificates to the worker nodes and updated the kubelet configuration on each worker node to use the NGINX load balancer as the API endpoint. Finally, because our cluster is spun up using kubeadm , we needed to update the controlPlaneEndpoint property in the kubeadm configuration to ensure that future Kubernetes upgrades would find the correct API server endpoint. At this point, our setup looked like this. It’s a bit odd with the load balancer pointing at one control plane node, but that’ll change when we add more control planes: “”) Phase 2: Creating an external etcd cluster Next phase was moving the etcd database from the existing control plane node onto an external etcd cluster with three nodes. The plan was to add three etcd nodes to the existing etcd and, once the replication magic was completed, remove the etcd instance from the control plane node, leaving us with an three node etcd cluster external to the Kubernetes cluster. The general steps to achieve this were: We provisioned three VM’s with etcd installed on each of the VMs. Create new SSL certificates for etcd - new peer and server certificates were created. The certificates were signed with kubernetes CA certificate. Each host was added into the cluster one host at a time. To do that: On the etcd host, stop the etcd service and remove the existing configuration folder. (Typically found in /var/lib/etcd folder.) Adding a host was a call to etcdctl from within the etcd pod running on the Kubernetes cluster. We opened a shell into the pod and issued the command to add a new member: $ ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key member add \"fh1-k8s-etcd01\" --peer-urls=\"https://172.21.14.91:2380\"\n\nMember 4ea98ab081f843d9 added to cluster 6090d9ccc522c5c4 On the etcd host, start the etcd service: systemctl start etcd . That should quickly start up and join the cluster. Once all hosts were added we have a cluster with four etcd instances. Final step was to remove etcd from the control plane node: The API server on the control plane needed to be updated to know about the new etcd hosts. That information is saved in /etc/kubernetes/manifests/kube-apiserver.yaml on the control plane host. The -etcd-servers property needed to be changed to point to three new etcd hosts. Editing this file will automatically restart the API server, so we made sure that the cluster still behaved as expected after this change. On any of the new etcd hosts, we used etcdctl to remove the etcd instance associated with the control plane node. First, we needed to get the instance ID: root@fh1-k8s-etcdt01$ ETCDCTL_API=3 etcdctl --cacert /etc/etcd/certs/ca.crt --cert /etc/etcd/certs/peer.crt --key /etc/etcd/certs/peer.key -w table --endpoints=fh1-kubet01:2379,fh1-k8s-etcdt01.dun.fh:2379,fh1-k8s-etcdt02.dun.fh:2379,fh1-k8s-etcdt03.dun.fh:2379 endpoint status\n\n+-----------------------------+------------------+---------+---------+-----------+-----------+------------+\n|          ENDPOINT           |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |\n+-----------------------------+------------------+---------+---------+-----------+-----------+------------+\n|           fh1-kubet01:2379  | bdf1c0c0d197886d |  3.2.24 |  5.5 MB |     true  |        28 |     134209 |\n| fh1-k8s-etcdt01.dun.fh:2379 | 61325c8d446f52f3 |  3.3.10 |  5.1 MB |     false |        28 |     134209 |\n| fh1-k8s-etcdt02.dun.fh:2379 | d365bb78153d1e05 |  3.3.10 |  5.5 MB |     false |        28 |     134209 |\n| fh1-k8s-etcdt03.dun.fh:2379 | 40544edaa48f23dc |  3.3.10 |  5.5 MB |     false |        28 |     134209 |\n+-----------------------------+------------------+---------+---------+-----------+-----------+------------+ In the example above, fh1-kubet01:2379 is the control plane node. To remove that from the cluster: root@fh1-k8s-etcdt01$ ETCDCTL_API=3 etcdctl --cacert /etc/etcd/certs/ca.crt --cert /etc/etcd/certs/peer.crt --key /etc/etcd/certs/peer.key member remove bdf1c0c0d197886d That removes the control plane etcd instance from the etcd cluster. Finally, we remove the etcd pod from the control plane node: root@fh1-kubet01$ cd /etc/kubernetes/manifests\nroot@fh1-kubet01$ rm etcd.yaml Kubelet running on the control plane node will pick up that the manifest is missing and remove the etcd resources. Problems found along the way Of course, whe investigating and creating the playbooks, the above wasn’t quite as straight forward. etcd does appear to have some quirks that make it slightly harder to get the configuration setup correctly. Configuring etcd cluster state Configuration of the etcd hosts is quirky - if we don’t get that right then etcd will fail to start and log messages about the cluster state being inconsistent. This all boiled down to the ectd ETCD_INITIAL_CLUSTER configuration property. Initially, we configured that property to be the addresses of all the etcd hosts. E.g.: ETCD_INITIAL_CLUSTER=\"fh1-kubet01=https://172.21.16.61:2380,fh1-k8s-etcd01.dun.fh=https://172.21.14.91:2380,fh1-k8s-etcd02.dun.fh=https://172.21.14.92:2380,fh1-k8s-etcd03.dun.fh=https://172.21.14.93:2380\" That, however, was wrong. When we tried to add fh1-k8s-etcd01 to the etcd cluster, the only members in the cluster were fh1-kubet01 and fh1-k8s-etc01 . The other etcd hosts were not members. To get the etcd instance to join the cluster correctly, we needed to provide different values for the ETCD_INITIAL_CLUSTER , depending on the hosts. So, for fh1-k8s-ect01 , it would be: ETCD_INITIAL_CLUSTER=\"fh1-kubet01=https://172.21.16.61:2380,fh1-k8s-etcd01.dun.fh=https://172.21.14.91:2380\" While for fh1-k8s-etcd02 it would be: ETCD_INITIAL_CLUSTER=\"fh1-kubet01=https://172.21.16.61:2380,fh1-k8s-etcd01.dun.fh=https://172.21.14.91:2380,fh1-k8s-etcd02.dun.fh=https://172.21.14.92:2380\" The ETCD_INITIAL_CLUSTER value for a etcd host had to match the members added to the cluster at that time. First etcd node joining the cluster We also found an issue when we added the first etcd node into the cluster. The etcd pod on the K8s cluster entered a crash loop backoff state and kept restarting. Because of that, the etcd node wouldn’t start up (because the pod had crashed). We could get the etcd service started up and added to the cluster, provided we started the service when the pod was running between crashes! If we were pretty quick about adding the new member to the cluster and then starting up the etcd service on the host was fine. If we were slow (> 15 secs perhaps) then the pod went into a crash loop. Failing to join etcd instance to cluster - part 1 Joining the fh1-k8s-etcd01 node initially failed. etcd won’t start on the new host and the log entry was: -- Logs begin at Fri 2019-04-26 08:03:04 UTC, end at Tue 2019-04-30 07:17:49 UTC. --\n...\nApr 30 07:04:06 fh1-k8s-etcd01 etcd[195108]: could not get cluster response from https://172.21.14.51:2380: Get https://172.21.14.51:2380/members: dial tcp 172.21.14.51:2380: connect: connection refused\nApr 30 07:04:06 fh1-k8s-etcd01 etcd[195108]: cannot fetch cluster info from peer urls: could not retrieve cluster information from the given urls\nApr 30 07:04:06 fh1-k8s-etcd01 systemd[1]: etcd.service: Main process exited, code=exited, status=1/FAILURE\nApr 30 07:04:06 fh1-k8s-etcd01 systemd[1]: Failed to start Etcd Server. It couldn’t connect to etcd on the master: Get https://172.21.14.51:2380/members: dial tcp 172.21.14.51:2380: connect: connection refused The problem was that the etcd configuration for our Kubernetes production cluster is different than that on the playground cluster. Looking at the /etc/kubernetes/manifest/etcd.yaml on playground we get - among other things - these IP addresses configured when etcd starts up: - --advertise-client-urls=https://172.21.16.61:2379\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --client-cert-auth=true\n    - --data-dir=/var/lib/etcd\n    - --initial-advertise-peer-urls=https://172.21.16.61:2380\n    - --initial-cluster=fh1-kubet01=https://172.21.16.61:2380\n    - --key-file=/etc/kubernetes/pki/etcd/server.key\n    - --listen-client-urls=https://127.0.0.1:2379,https://172.21.16.61:2379\n    - --listen-peer-urls=https://172.21.16.61:2380\n    - --name=fh1-kubet01 Note that most of the IP addresses here are the host IP address. The same file on production - --advertise-client-urls=https://127.0.0.1:2379\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --client-cert-auth=true\n    - --data-dir=/var/lib/etcd\n    - --initial-advertise-peer-urls=https://127.0.0.1:2380\n    - --initial-cluster=fh1-flagship01=https://127.0.0.1:2380\n    - --key-file=/etc/kubernetes/pki/etcd/server.key\n    - --listen-client-urls=https://127.0.0.1:2379\n    - --listen-peer-urls=https://127.0.0.1:2380\n    - --name=fh1-flagship01 And the IPs here are all localhost. etcd on the control plane node isn’t listening on the host address, which is why the new etcd host failed to communicate with the existing etcd on the control plane. We updated the Kubernetes etcd configuration on the control plane node to use the host IP address and tried again… Failing to join etcd instance to cluster - part 2 Updating the 127.0.0.1 IP addresses to use the host address didn’t fix the problem. The etcd host still failed to connect through to etcd on the control plane node. This time the log entry was: error validating peerURLs {ClusterID:c9be114fc2da2776 Members:[&{ID:a874c87fd42044f RaftAttributes:{PeerURLs:[https://127.0.0.1:2380]} Attributes:{Name:fh1-flagship01 ClientURLs:[https://0.0.0.0:2379]}} &{ID:e64f069657cd588 RaftAttributes:{PeerURLs:[https://172.21.14.91:2380]} Attributes:{Name: ClientURLs:[]}}] RemovedMemberIDs:[]}: unmatched member while checking PeerURLs (\"https://127.0.0.1:2380\"(resolved from \"https://127.0.0.1:2380\") != \"https://172.21.14.51:2380\"(resolved from \"https://172.21.14.51:2380\")) This took a while to diagnose. The problem was that etcd will use a stored version of the advertised peer URL, rather than use the IP specified in the configuration. When we changed the configuration from the 127.0.0.1 to the host IP, etcd was still picking up the stored 127.0.0.1 address for the peer URL. We could see that when we listed the members. $ kubectl config use-context production-admin@kubernetes   \nSwitched to context \"production-admin@kubernetes\".\n$ kubectl exec -it -n kube-system etcd-fh1-flagship01 sh\n/ # export ETCDCTL_API=3\n/ # etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key member list -w table\n+------------------+---------+-----------------------+---------------------------+---------------------------+\n|        ID        | STATUS  |      NAME             |        PEER ADDRS         |      CLIENT ADDRS         |\n+------------------+---------+-----------------------+---------------------------+---------------------------+\n| ec22143f96f83c45 | started | fh1-flagship01        | https://127.0.0.1:2380    | https://127.0.0.1:2379    |\n+------------------+---------+-----------------------+---------------------------+---------------------------+ The fix here was to tell etcd that the peer has a new URL. See https://coreos.com/etcd/docs/latest/op-guide/runtime-configuration.html#update-advertise-peer-urls for more information on this. Once these quirks were ironed out, our Kubernetes production cluster looked like this: “”) Phase 3 - Adding new control plane nodes This wasn’t too difficult. The steps were: Provision two new virtual machines Copy certificates and configuration files from the existing control plane nodes to the two new nodes Issue the kubeadm join command to add the new control plane nodes to the cluster. Update the NGINX load balancer and add the two new control plane nodes to the Kubernetes upstream. That was pretty much about it. At this point we had our final HA setup! “”) Summary Our journey along the road to deploying our services to a high-availability Kubernetes cluster has been long, winding and rough in a number of places. But, IMHO, the journey has been worth it. Our experience with running Kubernetes has, on the whole, been positive. Our ability to auto-scale our services is nice. We can respond automatically to more demand on the web service. We’ve reduced the number of virtual machines running our web service down to manageable number. Kubernetes (and Prometheus) allows us to easily instrument how the cluster and the services are performing. Upgrades to the Production site and - on the whole - pretty painless. Both Istio and Kubernetes upgrades can be applied with little downtime. On the downside, Istio and distributed tracing has not been so successful. For tracing requests, each pod needs to have the Istio sidecar injected. However, we are finding a lot of routing configuration issues with the sidecar. Because of this we don’t use the sidecar in production, but we do have it enabled on our staging cluster where we are continuing to test the sidecar configuration. I hope you also enjoyed the journey!", "date": "2019-10-28"},
{"website": "FindMyPast", "title": "Solving DNS lookup failures in Kubernetes", "author": ["Mike Thomas"], "link": "https://tech.findmypast.com/k8s-dns-lookup/", "abstract": "Our journey along the Kuberentes (K8s) road continues with the story of how we diagnosed and solved random, intermittent DNS lookup failures between K8s pods and also between pods and services external to our K8s cluster. The problem Findmypast have 80+ microservices running in the K8s cluster each of which have at least 3 pods running per service. Many services have horizontal auto scaling enabled, which means the service may have many pods running depending on load. We don’t have a huge K8s cluster, so it was surprising to see NodeJS services starting to log messages such as: Error: getaddrinfo EAI_AGAIN some-external-host Or RequestError: Error: getaddrinfo EAI_AGAIN k8s-service And, it wasn’t just the NodeJS services that logged domain lookup failures, our Elixir based services were also complaining: Service HTTP Error: nxdomain, Message :nxdomain, Url: https://www.findmypast.ie Engineers were raising the issue since these domain lookup errors were affecting their error rate metrics and, in some cases, their service alerts were firing. Also, we were also getting reports that communication between services was causing issues; in particular one service that we call Titan Marshal, was complaining about timeouts when talking to another service called Flipper. Oddly, we didn’t see the same behaviour when Titan Marshal was calling other services. We saw timeouts, but not on the same scale. At the time, the timeouts and the DNS lookup failures seemed unrelated. Flipper is one of our central services that receives a lot of traffic - it manages our feature toggles and typically gets ~150 requests per sec. Services use Flipper to get the feature toggle state for a user - a failure to return the toggle state means that a user could, conceivably, get two different toggle states during their visit. The feature toggle is used to allow/deny access to certain features on the site, so a user seeing two different states could get different behaviour or features. In fact, we did see some (small) evidence of that happening on production. So, services were intermittently timing out or failing to resolve a DNS lookup. This was particulary bad with services calling Flipper, so it was time to dig deeper. Gathering data - DNS lookup failures Before we dig deeper, let’s have a (simplified) look at how the DNS lookup is handled in K8s before the fix: DNS lookup is handled by the coredns pods. We currently have two of these pods running, both are running on master nodes. When a pod (Titan Marshal in this instance) sends a HTTP request to another service (Flipper) then a DNS lookup is sent to the coredns pods which will return one of the Flipper service pod IPs. With that in mind, we first looked at the logs from the coredns pods. After all, if we are getting domain lookup failures then these should be coming from the DNS service, right? Well, wrong it seems. coredns logs were not reporting any particular problems and the coredns_dns_request_duration_seconds_bucket Prometheus metric exported by the coredns pods were reporting <6ms response times for the 99th percentile. While we didn’t immediately rule out coredns , it was removed from the list of top suspects. Next, we looked at the distribution of the DNS lookup failures but these were random. For example, a service might log DNS lookup failures that would last 60 seconds or so, but other services were fine. (Or, rather they would report DNS failures but at a different time.) However, when a service did start to log the failures, the failures were not associated with one pod, but spread across a number of pods. Pods were spread across worker nodes, so the failures were not localised to a particular worker. But, other pods on those worker nodes weren’t affected, which suggested that it wasn’t an inherent issue with the node networking but something else. We also noticed that some services suffered from the DNS lookup problem much more than others. Some services hardly suffered at all. Again, this suggested that the issue was not inherently due to K8s networking but, perhaps, more to do with the docker image or the service configuration. Services used different docker base images - e.g.: node:slim , node:lts-slim or node:alpine images. Some services specified a specific version of the Docker image, like elixir:1.6.5-alpine or elixir:1.7.4-alpine . We were aware of K8s issues with DNS in Alpine and DNS issues with Alpine images but there was no obvious reason as to why one alpine image, for example, would fail while another would work almost flawlessly. Even with the same docker base image, we had discrepancies across services. Thoughts were turning toward configuration with the particular service. We knew that NodeJS, in particular, may have performance issues with DNS lookups : Though the call to dns.lookup() will be asynchronous from JavaScript’s perspective, it is implemented as a synchronous call to getaddrinfo(3) that runs on libuv’s threadpool. This can have surprising negative performance implications for some applications, see the UV_THREADPOOL_SIZE documentation for more information. Also, it seems that NodeJS does not provide caching of DNS lookups . Both Titan Marshal and Flipper are NodeJS services, so all DNS lookups would end up at the coredns pods, even if request the same endpoint multiple times a second. This might explain why we got DNS lookup failures bunched around the same time. Given the DNS cache issue, and the thread pool performance issue we suspected that the reason that some of the NodeJS services were failing was due to configuration. We also suspected that the Elixir services were failing for similar types of reasons, but didn’t dig deeper into the Elixir side of things. To summarise: DNS lookups affected both NodeJS and Elixir services; The lookup failures affected multiple pods in a service, but did not affect other pods; Some services rarely failed with DNS issues, which suggested the issue was local to the service; NodeJS had performance issues both with the UV_THREADPOOL_SIZE as well as no caching of the DNS reply; coredns was responding quickly and not reporting problems At this point, thinking was that something was going on inside the Docker image that prevented the outgoing DNS request ever reaching coredns or we had some subtle issue with the Flannel network layer inside of Kubernetes. Gathering data - Timeouts At the same time we were looking into the DNS lookup failures, we were also digging into the 500ms timeouts between Titan Marshal and Flipper. Specifically, why so many timeouts to Flipper when calls to other services from Titan Marshal hardly ever failed? The Flipper service is well instrumented, incoming requests are instrumented, all outgoing HTTP calls are instrumented and all the figures showed the Flipper was performing well within its Service Level Objective. The timeout from Titan Marshal to Flipper is 500ms, but we rarely saw latency spikes that took that long in the Flipper dashboard. We knew we were having DNS issues, so we decided to instrument how long the DNS lookup and TCP connect times were taking for Flipper. This RisingStack article had a good explanation of how to get these timings out of a NodeJS HTTP request, which we added to Flipper and exported the timings as Prometheus metrics. The results were…surprising. It appeared that Flipper could sometimes take up tp 8 seconds to perform a DNS lookup. Even during “normal” operations, the 99th percentile latency of the DNS lookup was ~128ms. We added the same TCP network instrumentation to other services to see if these services were exhibiting the same behaviour, but it appeared that only Flipper had this issue. The DNS timeouts was the reason why Titan Marshal was timing out calling to Flipper. The graphs below shows the results of Titan Marshal calls to Flipper (via Apollo engine stats) and the TCP instrumentation from Flipper. The increase in DNS lookups times correspond nicely with the timouts recorded by Titan Marshal. So, while we knew the reason why Titan Marshal was timing out, that still didn’t explain why only Flipper was taking so long to perform the DNS lookup. (Also, it doesn’t explain why the long DNS lookup times weren’t reflected in the request duration metrics produced by Express Prometheus Bundle module . That requires further investigation.) We compared the base Docker image of Flipper with other NodeJS services that didn’t have the DNS lookup issue; even when using the same base image and the same versions of dependencies, Flipper still took way longer than other services to perform the DNS lookup. At this point we began to suspect that the long DNS lookup times and the DNS lookup failures were symptoms of an underlying problem within the K8s cluster. Time to dive deeper… CPU throttling in K8s Investigation into poor server performance lead us to an number of interesting articles about how Kubernetes manages the CPU request and the CPU limits for a pod. Let’s take a look at what K8s documents about CPU request and CPU limits: When using Docker: The spec.containers[].resources.requests.cpu is converted to its core value, which is potentially fractional, and multiplied by 1024. The greater of this number or 2 is used as the value of the --cpu-shares flag in the docker run command. The spec.containers[].resources.limits.cpu is converted to its millicore value and multiplied by 100. The resulting value is the total amount of CPU time that a container can use every 100ms. A container cannot use more than its share of CPU time during this interval. The CPU limit is subtle - essentially if you specifiy a CPU limit for your pod, that value will be used to calculate how much time your container can use within a 100ms period - once the time is up the container is throttled until the next period. This CPU throttle can degrade the performance of latency critical services. A quick “fix” for this issue is too simply remove the CPU limit for the service. While that helped, a little, it didn’t really sort the underlying problem of slow DNS lookup times. For more information on CPU throttling in K8s, see: https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718 https://github.com/kubernetes/kubernetes/issues/51135#issuecomment-373454012 https://www.youtube.com/watch?v=UE7QX98-kO0&feature=youtu.be&t=3560 The latter YouTube video is an deep dive into Linux CPU scheduling with Dave Chiluk. Long story short, a scheduling bug in Linux has been fixed with release 5.4 of the Linux kernel. Linux networking race conditions Further reading and digging into various connection and DNS timeouts led us to other articles with similar DNS and connection timeout issues. We tried some of the simplier solutions too see if they would help the DNS lookup, but still no joy. Some solutions involved patching tc , libc , musl , etc which was not an avenue we wished to go down without knowing if the patches would really solve the problem. However, we did determine that the Linux kernels on our K8s hosts appear to be suffering from a DNAT race condition - the race condition has been fixed in 5.x of the Kernel. For more information on the DNAT race condition: https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts https://github.com/kubernetes/kubernetes/issues/56903 https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/ https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02 https://github.com/weaveworks/weave/issues/3287 The solution The solution was pretty straightforward - we installed a K8s cluster add-on called Node Local DNS cache . This runs on each of the K8s worker nodes and caches DNS requests from the pods on that particular node. DNS lookups now look like this: Instead of (potentially) hopping out to another worker node to get DNS resolution, pods query the DNS on the worker node itself. Since this also caches DNS lookups, the end result is faster DNS resolution for pods. Installing the DNS cache was easy and, even better, no changes to existing services were required to use the DNS aside from a pod restart. Since we’ve added the cache we’ve seen: No instances of domain resolution failure. Flipper DNS lookup times are comparable with other services. Large reduction in Titan Marshal to Flipper timeouts Summary Debugging K8s issues like this can be hard, especially when the problems are intermittent. Instrumentation of your service is invaluable when diagnosing issues like this. After all, who’d have thought that DNS lookup would be so slow (for one service!). “If it moves, instrument it” is a well known mantra and one we’ll continue to take on board for the future. The DNS Local Cache solution is a mitigation, we haven’t really got to the bottom of the various issues, but the suspicion is that the DNAT network race condition is contributing to, if not the underlying cause of, the issue. Our next stage is to upgrade Linux on all the K8s hosts, at the very least that’ll fix the CPU throttle and the DNAT bugs.", "date": "2020-03-11"},
{"website": "FindMyPast", "title": "Buy or Build?", "author": ["Gavin Henderson"], "link": "https://tech.findmypast.com/buy-or-build/", "abstract": "Should you buy software or build it yourself? Software teams are often faced with the decision between buying in software solutions or building their own. The example I will use is one that we are currently faced with at FindMyPast. At FindMyPast we build software that connects our customers to their past by helping them find their ancestors in our rich datasets. Like most companies, we need a way to identify users securely. So we are asking the question: “buy or build an authentication system?” When you face this question you typically have three options: Build a bespoke solution Host an open source solution Or pay a SAAS company to deal with everything for you Hopefully when you approach this decision as a company you will be doing it with people from across the entire business. The decision will often come down to money, which makes sense, however if you ask the wrong questions you will end up with a solution that only benefits you in the short term. I’ll run through a few questions you might hear and why I think they are flawed and I will end with how to frame the question to get the best decision that works long term. What is the cheapest option? When you go to answer the question of what is the cheapest you have to start by obtaining the cost of each option. It is easy to get the price of a SAAS option, you just go to the price page or contact sales. Importantly this price you get is totally accurate, you don’t have to worry about over or underestimating. On the other hand it is very hard to measure the total cost of building and maintaining your own solution so it tends to get underestimated. Also humans are just bad at giving estimates and the bigger the project the worse the estimate. To get the cost of building your own first you have to decide what you want to build. It is likely that you will not build all the features that the SAAS product is offering as the immediate value of them might not be obvious. However, in reality the features you are suggesting to leave out might be features that the SAAS business has added after extensive research and is actually one of their most popular features. So your comparison already starts out unfair as you are not giving an equal comparison. So to answer this question you end up comparing solutions which are of massively varying quality and you are looking at estimates which have widely different confidence levels. Not a recipe success. If you hear this question I encourage you to push back, and try and reframe the situation with the final question. What is the most cost effective option? This is a much better question to be asking, mainly because it shows consideration for maintenance cost however it still has issues. When you are looking at how cost effective each option is you need to think about how long you want this solution to last. Do you expect this to be in place for 6 months or 10 years? Is it going to be an area of active development for your company or is it something you want to set up and forget? It is easy to measure the ongoing hosting cost but that is probably the smallest ongoing cost. However there are so many uncertainties that are impossible to capture. What if a new standard is released? Do you implement it or stay on the old standard? What if the open source library becomes unpopular, do you start maintaining it yourself? What if the open source solution uses a database tech no one is familiar with? The stars may align and you might not have any problems but more than likely there will be unforeseen problems that you couldn’t possibly capture when thinking about how cost effective the different options are. In the example problem we have at FindMyPast looking at the cost effectiveness we will probably find that hosting an open source option is far more cost effective than building our own. You only have to pay maintenance costs and if the open source solution is popular it is likely in active development and maintenance, none of which you have to pay for. The ongoing cost of an open source solution will probably be lower than the ongoing cost of a SAAS product. However, you mitigate a lot of the uncertainty by going with a company who is motivated to solve those problems by the money they receive from customers. What is the most profitable option? This is the best question to ask as it now leaves room to acknowledge opportunity cost. No one can argue that the answer to this question is not important, ultimately profit is what drives the business. What is the opportunity cost? Google says: “the loss of other alternatives when one alternative is chosen” - lexico In our case our case the opportunity cost is the money that should have been made on the features you could have made instead of building your own system. This is again pretty much impossible to measure but it is very important to recognize the size of this cost. Now that we are asking what option is more profitable we can begin to make an informed decision. One way to make profit is by adapting and innovating faster than your competition. When you are thinking about building you need to decide if the part of you application under question is something you can use to beat your competitors and make more money. If the answer is no, then you should buy. If we look back again at the example of FindMyPast wanting a new authentication system. A robust authentication system is very important to have for us, and we can’t operate without one. However we will not win customers by innovating in the authentication space. We will win customers by innovating in the family history area. If we acknowledge that all time spent working on authentication is waste because it is not competing then anytime spent now or in the future we on authentication is waste. So we want to minimize that work as much as possible to direct efforts to areas that we can beat our competitors at and gain more profit. So it likely makes sense for us to buy an auth platform so that FindMyPast engineers can focus purely on family history and think about authentication as little as possible. You might be thinking, won’t asking this question always lead you to buy? Well a lot of the time it will. Your company should focus on delivering direct innovation in their respective area and not reinventing the wheel at expense of delivering value. Sometimes you will ask this question and the answer will truly be to build it ourselves. An excellent example of this is search at FindMyPast. We have our own customer searching setup, something which many SAAS businesses are well established at. However, we asked the question: can we use innovation in this area to beat our competition? The answer is definitely a yes, we have billions of complex records so we have a very unique problem and also the speed of searching and indexing will lead directly to a better customer experience. This proves that asking the question “what is the most profitable option” will not always lead you to buy but in fact lead you to the correct answer. Still not convinced? Another reason to buy is documentation! Everyone hates to write documentation but the only thing worse is working with a microservice without any documentation. Buying means you don’t have to write documentation and most SAAS companies put a lot of resources into making their documentation very robust. Another reason to buy is admin panels. Admin panels are often an afterthought and are not given the same usability consideration as the customer facing application is. For SAAS businesses the admin panel is the customer facing application so they will invest heavily making the admin panels top quality. In conclusion, when you come to this decision you need to reflect on whether or not building will give you a competitive edge, if it wont spend as little time as possible on it. Spend the time you would have been building on out moving the competition. Sources: Why your estimates are always wrong The art of doing twice the work in half the time Unicorn Project Opportunity Cost", "date": "2020-08-04"},
{"website": "FindMyPast", "title": "My Onboarding Experience ", "author": ["John Parsons"], "link": "https://tech.findmypast.com/my-onboarding-experience/", "abstract": "Background So far, this year has been a disruptive one for sure. It may seem like an unusual time to start a new job, particularly the recent uncertainty and with so many graduates entering the industry at the same time. However, this was the exact situation that I found myself in just a few months ago. I interviewed for several software engineering positions earlier on in the year, one of which was at Findmypast. After a very positive interview experience I decided that this was a company that I really wanted to work for. With lockdown putting a hold on hiring for many, I was delighted to receive an offer of employment from Findmypast for the position of a Junior Software Engineer. During the run-up to my start date I had the opportunity to meet the engineers on my new team as well as ask some questions. It was really great to be introduced to my colleagues in advance and gain some insight into the specifics of the tech stack that I’d be working with. I feel that these meetings really helped me form a clearer picture of what and who I’d be working with on a day-to-day basis. The Onboarding Prior to starting, all the hardware that my position required arrived at my flat the week before. This allowed me some time to get setup and configure my machine in advance. As when starting any new job there’s often a slight feeling of nervousness and even more so with the prospect of onboarding remotely. Questions began to enter my mind such as: What do I do if I get stuck? Who do I reach out to for help? However, my first day soon put an end to any worries I may have had. Speaking of my first day, right from the get-go I was assigned a buddy who took me through the procedures and the work methodologies employed by the team. Following this, I worked with my buddy to set up my development environment before starting to work on a ticket from the backlog. I soon learned that Findmypast have a strong focus on delivering frequent product updates via continuous integration. A key mechanism enabling this is trunk-based development, whereby code is directly commited to master and tested through a pipeline before being deployed shortly after. The fact that I was writing meaningful code on my first day was awesome and the sense of achievement when my work was later deployed to production was something else! Aside from the development side of things I attended an induction for new-starts which provided me with a good overview of the various areas of Findmypast, both technical and non-technical. It was also a great chance to become familiar with whos who within the company as a whole. Highlights I’ve been having a lot of fun at Findmypast and I thought it would be a good idea to share some of my highlights with you. First of all I was particularly impressed with how structured the onboarding process was overall. Even though I was joining a relatively new team at Findmypast, the experience felt as though it was well practised. The onboarding related tasks were divided into convenient checklists, categorised by first day, week and month, making it easy to see exactly where I was at and what I had left to do with regards to onboarding. Before starting my job, I hadn’t practiced pair programming in an industry capacity. And so I was a little unsure as to what to expect, especially when pairing remotely. However, I soon came to appreciate benefits that paring can offer. It is an awesome method of sharing knowledge, providing context and improving skills (and even more so for a new start like myself). Because of this, techniques and best practices are circulated amongst everyone, thereby passively upskilling the team as a whole. During my time pairing, I have always felt encouraged to share my thoughts and ideas. Questions have always been welcomed and the team really wants to do everything they can to make sure that everyone is on the same page. Another great thing about working at Findmypast is the inclusion of Learning Time. Each day, an hour may be taken for individual learning. Many engineers use this time to learn a new technology or to further develop their skills. Recently, my team has been doing some work with GraphQL, and so I have been spending my learning time practicing using the technology by building a small fullstack app. I seem to have got myself into a nice sort of loop where what I do within my learning time helps me be better during pairing time, and what I do when pairing helps be during learning hours. Wrap-up In conclusion, I’m learning a lot at Findmypast and having a great time doing it. I’ve had the opportunity to work in some really interesting areas and with some fun technologies. I’m grateful to have had such a smooth and yet comprehensive onboarding and I feel that it’s equipping me well as a new engineer. I can certainly recommend it! If you are interested in joining the team, please head over to the Findmypast Careers Page .", "date": "2020-08-14"},
{"website": "FindMyPast", "title": "Findmypast Onboarding Experience", "author": ["Miroslaw 'Mirek' Majka"], "link": "https://tech.findmypast.com/fmp-onboarding-experience/", "abstract": "I have recently joined Findmypast in Dundee as a Senior Software Engineer and I’ve been happily developing solutions in some of my favourite tech stacks ( Node , C# , Docker to name a few). With a very volatile job market in 2020 I wasn’t exactly expecting much apart from a steady job. However I was in for a treat. Not only did the company prepare a comprehensive onboarding experience but also the day-to-day work has shown that remote software engineering can be done right. Here are some of impressions from joining Findmypast . Onboarding The company had set up specific day, week, month, quarter goals so that the amount of information would not be too overwhelming on the first day.\nThis has proven very effective in getting up to speed with specific technologies, team methodologies and the overall company policies.\nI was lucky to have my company-wide induction on the first day getting some exposure to the business side of things and better understanding what we are actually delivering to our customers.\nThe management was open to answering questions about the current ways of working and reporting. Remote Work In 2020 it has become apparent that the term “home office” is here to stay for a while.\nWhen joining a new company you have to know if working from home can be done effectively or not. Thankfully, the teams at Findmypast know how to use tools such as Slack and Zoom efficiently.\nThe company culture allows for meaningful communication between individuals and teams.\nWorking time is well-structured and lets engineers get into their “programming zones” without much distraction.\nThe leadership team allows for a good degree of independence among development teams.\nYou are not left in the dark either as regular company updates let you know how the business is doing and what direction we are heading into. Code Development Using gitflow previously when developing software solutions I was initially sceptical about trunk-based development . However the combination of Pair Programming , continuous integration, test and deployment pipelines facilitates this methodology.\nThe codebases adhere to clean code principles and unit testing is encouraged.\nYou take responsibility for the piece of code you are adding or changing and you see it through all the way into production. Given my QA Automation background I was excited to see Cypress in action for acceptance criteria browser testing. \nI was not disappointed. It’s a fantastic tool for rapid test development that allows your programmers to quickly verify if their core works or not. Pair Programming I’ve pair-programmed before in my career but it was never the standard way of working in any other company. \nMost managers see this technique as wasting time when 2 programmers who could be working on separate tasks at the same time. At Findmypast this is seen as adding value and it’s hard to deny it when after a week of reviewing code and exchanging experiences you find that the solution contributed in the repositories is of sound quality. Not only do you end up sharing the technical knowledge, eliminating the anti-pattern of “tech silos”, but also it leaves very little time for procrastination.\nYou can switch the “driver” roles and carry on developing effectively, regardless if you are based in the same office or remotely.\nHaving core pair programming hours facilitates discipline and you are never left to wonder “what next to do?” See Pair Programming at the Agile Alliance. The Kit Final point to mention is the equipment that the company provides. \nWith a plethora of programming languages used you sometimes need to run multiple IDEs at the same time.\nSome of them better supported on Microsoft-based systems, others are easier to set up in Linux environments. The laptop came with both Windows 10 and Ubuntu Linux giving me multiple options on how I want to develop the software. \nBeing an old-school Linux user I was glad to be back in my natural environment.\nWith enough processing power and RAM under the hood running all the software required is a doddle! If you would like to join the team, please head over to the Findmypast Careers Page .", "date": "2020-08-17"},
{"website": "FindMyPast", "title": "My onboarding experience at Findmypast", "author": ["Conor Sturgess"], "link": "https://tech.findmypast.com/my-onboarding-at-fmp/", "abstract": "I have recently joined Findmypast as a Junior DevOps Engineer working with tools such as Kubernetes, Docker, Puppet, Ansible, Prometheus. I have created this blog post for anyone interested in joining Findmypast and would be interested in some generic questions answered. What Personal Equipment are you given? I was offered a choice of three operating systems: Windows, Linux, or macOS. My laptop also arrived a few days early giving myself plenty of time to set up and get my favourite tools ready. How was your Onboarding process? My onboarding was different from the usual process as it happened during the lockdown. Initially, I was worried about how I could get to know colleagues, understand the culture, and learn the general structure of the company. But something which stood out was how well the Findmypast team kept in contact once I had accepted the offer. They hosted Zoom calls and invited the team with mini introductions to get to know each other before my official start date. This helped to get a better understanding of who I would be working with and an idea of the team and company structure. Also, once you officially start you are invited to a presentation with all new joiners hosted by the executive team, where they discuss their role, departments, and company mission. Additionally, you are invited to a one-to-one introduction call with the executive team to get to know one another. What it’s like as a new joiner in DevOps at Findmypast? In this current situation working with FMP as an engineer is remote. Before joining, I had concerns with how working remotely would go and whether or not this would be an effective approach to learning and getting up to speed in a new role. From the first week, I could see how Findmypast had adopted remote working effectively and efficiently. We hold daily stand-ups to discuss our progress on tickets, review work, and plan the day. Tools such as Zoom and Slack are also utilised for effective communication. Once you join there is also a set of tasks that have already been created for your first month, so you have something to work on from the get-go. Findmypast is also very big on pair programming which I’m finding immensely valuable to exchange experiences and knowledge. We switch between a “Driver” and “Co-Driver”. The driver leads the session whilst the “Co-Driver” inputs and provides additional knowledge to the task. A huge benefit at Findmypast is they are big on learning and personal development. We are given a study hour each day - an hour which we are encouraged to take and use to learn about something especially interested in. Recently I have been using this hour to deep dive into Puppet to catch up to speed. We also hold True North days where we each research a specific tool of interest and share the results. Have you worked on any exciting tasks since joining? Yes, many. At Findmypast we have a range of different Linux and Windows nodes, and to my surprise, I have enjoyed working on tasks with our Windows Server nodes. Previously having only used the Linux terminal, it was a welcoming change jumping into Powershell at Findmypast. My Powershell knowledge was built on installing and configuring Kubernetes through the terminal and automating the installation of certain packages and file creations. There are a lot more tasks, but to summarise I have listed some below: Updating our integration and production Kubernetes clusters to v1.17.4. Updating nodes to Ubuntu 20.04 and Windows Server 2019. Installing Kubernetes onto Ubuntu nodes. Dockerize Linux and Windows applications to deploy onto Kubernetes. Chaos testing the resilience of services to test for any failures. Contributing code to our Node JS applications. Configuring and creating ETCD clusters. Automating the installation and deployment of our nodes through Puppet and Ansible. What do you enjoy most outside of DevOps at Findmypast? What I enjoy most is how friendly and welcoming everyone is and employees are also given a lot of recognition for work with a specific #shoutouts channel. Teams also get the opportunity to present their achievements and any show-off what they are working on to the company every Friday during our weekly demos. Because of social distancing and remote working, we have hosted events over zoom. These range from company quizzes or colleagues just joining on a Friday for socialising. There are also various slack channels to join\nfrom #gardening, #gaming, #book-club, and #wellness. Overall, I am enjoying my time at Findmypast and continue to look forward to helping create a product for our customers. If you would like to join the team, please head over to the Findmypast Careers Page .", "date": "2020-08-28"},
{"website": "FindMyPast", "title": "Accessibility First", "author": ["Laura Green"], "link": "https://tech.findmypast.com/accessibility-first/", "abstract": "Creating pages “mobile first” is a technique that’s very familiar to front end engineers. A few extra considerations could transform this technique into “accessibility first” while still maintaining the principles of “mobile first”. Primarily, “accessibility first” is using the right semantic structure to outline your page from the outset. This will make your site understandable for everyone, using the broadest range of devices. What are landmarks? The use of ARIA landmark roles supports keyboard navigation, and provides a way to understand the structure of a web page for screen reader users. Using landmarks, assistive technology users can quickly navigate around sections of a web page. You can deliberately assign landmarks (eg role=\"main\" ) or why not get them for free by using the HTML elements <nav> , <header> , <main> , <aside> and <footer> . The <section> and <form> tags will need the addition of an aria-label to get a default ‘role’ assigned. Aim to have your entire page ‘chunked-up’ using landmarks to give assistive technology users the best access to your site. Try using a screenreader , or more simply, just install the Landmarks extension to understand how landmarks define a page’s structure. Using landmarks (Here, I’ll be using the scenario of a ‘designed’ page being handed over to engineers). Firstly, consider the purpose of a web page: Which part of the design contains the primary aim? The primary aim should be wrapped in a <main> tag (it will automatically acquire the default landmark role of “main”). What is the purpose of everything else on the page? Content that is for a secondary purpose should be wrapped in <section> tags (with appropriate aria-labels). Any forms should have an aria-labelled <form> tag. Ensure <nav> , <header> and <footer> tags are used. Everything else could be an <aside> . If you’re about to use a <div> for a large area or to contain a specific context of a page: consider using the above tags instead. For further details on tags and landmarks see HTML Sectioning Elements from W3C. Example design Primary aim : choose a package and move on to payment page. A <main> tag contains the page heading, subscription choices and the associated form elements. A <form> tag contains the actual choices and selection form elements (should also have an aria-label) Secondary aim : provide a detailed list of the subscription package features. Wrap in an aria-labelled <section> tag. Standard <nav> and <footer> areas. Image above: A page design, marked up with HTML sectioning elements The markup outline for this page <nav> nav </nav> <main> <h1> the page heading </h1> <form aria-label= \"choose subscription\" > the form, with valid form elements (any fancy design enhancements should\n    happen after the basic structure is correct) </form> </main> <section aria-label= \"compare our memberships\" > comparison table </section> <footer> footer </footer> Summary This almost seems too simple. But start with the right semantic markup, sprinkle some appropriate aria-labels, and you’ll have a very good starting point for creating an accessible page that’s understandable and usable by everyone, regardless of different abilities. Links 10 free screenreaders from Usabilitygeek Landmarks extension HTML Sectioning Elements from W3C Resources for developers from W3C", "date": "2020-09-03"},
{"website": "FindMyPast", "title": "Learning Swarm", "author": ["Gavin Henderson"], "link": "https://tech.findmypast.com/learning-swarm/", "abstract": "A Learning Swarm is designed to be a highly effective way to rapidly generate shared knowledge within a team on a topic they previously had no experience in. The output of a learning swarm can also serve as a resource for others to gain the same knowledge as the team who ran the learning swarm making it even more useful. Here’s an outline of the areas will cover in this article: Timebox Scope and Project Swarm Time! Closing the Swarm Example Join Us! Timebox To run your own Learning Swarm you first need to decide upon a timebox, which is the time you are willing to dedicate an entire team towards learning the topic. It depends on the size of the topic but for it really to be worthwhile I would recommend around 3 days. You can always review it after 3 days and decide to extend it by a day at a time if need be. Having a timebox for the swarm also really helps people focus on getting their work into a complete state as soon as possible and then iterate on it from there. A timebox also reassures managers that you have limited the time you won’t be delivering direct customer value, even if your timebox has to be large it is still reassuring to have it Scope and Project Once you have decided on how long and when you are going to run the swarm you can start thinking about what you are going to do during the learning swarm. First you should come up with a scope of what you want to learn, it might also be helpful to say what is explicitly out of scope. An example of your scope might be ‘GraphQL’, you might want to specify that although GraphQL uses the network ‘network protocols’ is out of scope of the swarm. After you have defined your scope you should come up with an idea for a project you want to build individually. In the example of our GraphQL learning swarm you might want to make the project something along the lines of, “Build a todo list app using GraphQL”. This might seem really narrow but you would be really surprised at how 4 or 5 different people can build wildly different things based on this project description. I would encourage you to go deep into an area that interests you, but make sure to stay in scope! Swarm Time! Now you know what you want to learn and build you can get swarming! Everyone in the team should go their separate ways and learn about the topic and build the project. As you go everyone should keep detailed notes about what they have been learning, these will be the main artifact of the swarm. They might also be fairly repetitive between each person but that’s fine at the end we will merge them into a single document. It is important you keep your daily stand up going because you might need to nudge each other to stay on track or you might have to help unblock someone because everyone needs a fresh pair of eyes from time to time. It’s also a good idea to share any resources you find on the topic, it’s not a competition so sharing is encouraged. Closing the Swarm When you reach the end of the timebox now is the time to come back together and share what you have learned! Setup an hour or two (depends on team size) to come together and discuss what you have learned. Everyone should give a quick informal demo of what they made and share what they enjoyed and what they found hard. It is good to record these meetings if you can for later reference. I would also suggest someone takes notes and generates a summary of the meeting. It might also be worth summarising all of the learning notes that everyone has taken into a single place. I wouldn’t spend a long time doing this, just pull all the best parts together so other people can learn from it. Now your team all knows a great deal more about a topic in a short amount of time and has practical experience with whatever you were looking into. So it is time to get cracking on with your real project! As with all processes, make it your own! Maybe run it as suggested first then tweak it so it works better for you. Example We recently ran a Learning Swarm at Findmypast so we could get up to speed on authentication, specifically Auth0 libraries as they are our chosen third party for authentication going forward. We set our project to build a React app using auth0 authentication. Surprisingly we all took a different approach. We had people focus on automating config deployment, protecting backends, serverless authentication and comparing the different libraries Auth0 provides. In 3 days we went from a high level understanding of authentication to a deep understanding of the standards as well as plenty of code samples we all learnt from. Join Us! Good luck with your own Learning Swarm! Want to try a Learning Swarm at Findmypast? Well good thing we are currently hiring lots of positions, see our job postings here: https://www.findmypast.co.uk/careers", "date": "2020-11-16"},
{"website": "FindMyPast", "title": "Using BDD at Findmypast", "author": ["Miroslaw 'Mirek' Majka"], "link": "https://tech.findmypast.com/bdd/", "abstract": "Behaviour-Driven Development has gained some attention over the last few years with some companies embracing this methodology.\nOthers are more sceptical and may see additional “code” as waste. Let’s take a closer look. Definitions Before we weigh up the pros and cons of applying the BDD approach let’s define a few concepts that may appear to be confusing at first. What is BDD? Behaviour-Driven Development, or BDD, is a software development methodology which aims to bridge the gaps between product owners and engineers, and between engineers and testers. It allows the person that has been tasked with writing User Stories to come up with business scenarios that best describe the desired Behaviour of a given system. From that point on those scenarios can become Living Documentation that anyone can look up when in doubt about what a given piece of software is meant to do. Those scenarios can, and in fact, should be used as the base for automated testing of that system which,in turn, becomes the System Under Test . Gherkin syntax Photo: Canned pickle cucumbers by Marco Verch under Creative Commons 2.0 BDD features and scenarios are written in their own text file format called Gherkin . \nThese files are human-readable and can help any party understand how a given system is meant to work. \nSee the example of an add-relative.feature file below: @StableFeatures\nFeature: Adding relatives to a family tree\n\nBackground:\n  Given I navigated to Findmypast\n  And I am logged in\n\nScenario: Adding a child to a family tree\n  Given I am  looking at my family tree\n  When I click on a family member\n  Then The person drawer appears with the selected person's details\n  When I click on the Add relative button\n  Then The relative selection dialog appears\n  When I click on the \"Child\" option\n  And I click the Continue button\n  Then The person details dialog appears\n  When I fill in the new relative details\n  And I click the Continue button\n  Then The new relative appears in my tree The Feature describes the high level summary of what a given system “can do”.\nA Feature can have multiple scenarios defined.\nThese can include “happy” or “golden” paths but also “sad” paths (e.g. “The password provided is incorrect”).\nEven those that cover past fixed bug edge cases. Scenarios in a given feature can have common setup tasks.\nThese are Steps defined under Background .\nEvery scenario defined in this feature file will start with these Steps . Essentially every action or assertion that forms a scenario can be described as a Step . Steps have to start with one of the following keywords: Given , When , Then , And .\nThe form the basis of any testing scenario for any computer system.\nThe Given-When-Then Steps echo the familiar triple-A sequence from other testing domains ( AAA = Arrange , Act , Assert ).\nIn many pieces of modern-day unit testing code you can find comments above blocks of code as such: // Arrange const subject = new StringConcatenator () const testStrings = [ ' apples ' , ' bananas ' , ' carrots ' ]; const linker = ' _ ' ; // Act const result = subject . concatenate ( testStrings , linker ); // Assert expect ( result ). to . equal ( ' apples_bananas_carrots ' ); The BDD Given-When-Then setup takes it to the next level and applies the same logic to the wider system. What is Cucumber then?? Photo: Cucumbers by Marco Verch under Creative Commons 2.0 BDD scenarios are written in the plain text file Gherkin syntax. In the Node / JavaScript world these scenarios are used in test automation by using tools such as Cucumber.js . \nThis tool allows you to write programmer-readable code that will be executed in any environment that has the relevant dependencies installed. Browser testing frameworks such as Selenium or Cypress can make use of the BDD setup: WebdriverIO Configuration BDD Example BrowserStack Test Automation using Selenium and Cucumber (Java) Cypress Automated Testing using Cucumber How is test automation wired together with all this? In order for a plain text file to be understood by Node (or any other programming language supporting this for that matter) you need to match up those step lines with Step Definitions .\nThese are actual JavaScript expressions that capture the plain-language strings and execute the code that tests the system. To use the example from the previous paragraph: // The \"Arrange\" step in BDD fashion Given ( ' The test strings and linker are set up ' , () => { this . context . subject = new StringConcatenator (); this . context . testStrings = [ \" apples \" , \" bananas \" , \" carrots \" ]; this . context . linker = \" _ \" ; }); // The \"Act\" step in BDD fashion When ( ' I call the string concatenator with the strings and linker ' , () => { this . context . result = this . context . subject . concatenate ( this . context . testStrings , this . context . linker ); }); // The \"Assert\" step in BDD fashion Then ( ' The resulting string should match the expected concatenated one ' , () => { expect ( this . context . result ). to . equal ( \" apples_bananas_carrots \" ); }); The Cucumber Browser Demo allows you to prototype valid BDD scenarios: The context is the “dumping ground” that you can use between steps to store data and results.\nThis object is usually set up outside of the Step Definitions in what Cucumber defines as the “world” or in its own file, e.g. support/world.js : class CustomWorld { constructor () { this . context = {}; } } setWorldConstructor ( CustomWorld ); Test results: The files for the demo are available here: string-concatenation.feature step-definitions.js What is living documentation? The BDD feature files can be used to generate living documentation which allows every stakeholder to see what a given system is meant to be doing. At Findmypast we are actively writing our scenarios so that they can be digested by our document store and presented as such: When setting up a Living Documentation library it is good to wire it up with the automated testing your CI/CD system of choice.\nThis will allow any Agile team to effectively know WHAT a given system is meant to be doing and WHETHER or not it’s doing it right now. Use case BDD can bridge gaps between Agile development teams, stakeholders and product owners.\nIt provides a common language in which these parties can talk about the systems they are releasing. Documenting features and bugs The BDD syntax can be the start of a new feature of a system.\nAside from the usual Acceptance Criteria in a user story you should set up the expected feature with one or more BDD scenarios.\nThis *.feature file might not be used for anything else but it will document what a given system is meant to do. It is also a good idea to collect the existing business *.feature files into a document store.\nFrom there any stakeholder will be able to determine what the acceptance criteria currently are or were in the past. Preventing bugs by running automated tests The *.feature files are the basis for writing automated tests.\nAs you develop the software and flush out technical problems your scenarios might need to change to reflect additional steps.\nSometimes you may find that steps defined in a scenario make no functional sense.\nHowever you eventually end up with a Feature with multiple Scenarios that are granular enough. The scenario steps should run as test regression and provide answers as to whether the System Under Test is working as it should, ideally in a continuous integration/testing/deployment setting. Sample BDD Feature In my past side project called Cucumber Boilerplate I have outlined how a tiny Online Shopper Express Web application can be tested automatically using a set of BDD scenarios: Feature: Online Shopper payments and available items\n\n  Background: Always open the payment page first\n    # \"Arrange\"\n    Given The \"payment-page-id\" page is opened\n\n  Scenario: Verifying that the page welcome header contains the expected text\n    # \"Act\"\n    When I get the text value of the page welcome header\n\n    # \"Assert\"\n    Then The expected header text value equals \"Welcome to Online Shopper - Payment Page\"\n\n  Scenario: Clicking on the show payment details button to reveal them\n    When I click to show payment details button\n    Then The payment details are revealed\n\n  Scenario: Clicking the Pay for Items button inside the payment frame shows the payment time\n    When I click on the Pay for Items button\n    Then The payment time text appears with a new value\n    When I click on the Pay for Items button\n    Then The payment time text appears with a new value\n\n  Scenario: Verifying that the page welcome header is different in the Available Items page\n    When I get the text value of the page welcome header\n    Then The expected header text value equals \"Welcome to Online Shopper - Payment Page\"\n\n    When I click the \"Available Items\" top navigation link\n    And I get the text value of the page welcome header\n    Then The expected header text value equals \"Welcome to Online Shopper - Available Items\"\n\n    When I click the \"Payment Page\" top navigation link\n    And I get the text value of the page welcome header\n    Then The expected header text value equals \"Welcome to Online Shopper - Payment Page\" The contents of the features directory contains all the necessary Gherkin and JavaScript code to run those test scenarios. This project uses a combination of WebdriverIO , Selenium and Cucumber to interact with the browser and the sample website but the test framework is BDD and its scenarios are listed above. The previously mentioned “world” is set up in world.js .\nThis defines the context for sharing context between the step definitions: When ( /^I get the text value of the page welcome header$/ , async function () { const page = this . context . page ; this . context . welcomeHeaderText = await page . getWelcomeHeaderText (); }); Then ( /^The expected header text value equals \" ([^ \" ] * ) \"$/ , function ( expectedValue ) { const welcomeHeaderText = this . context . welcomeHeaderText ; should . exist ( welcomeHeaderText ); welcomeHeaderText . should . equal ( expectedValue ); }); For comparison there is also “traditional” testing code using mocha .\nThese tests do exactly the same thing but with Mocha you don’t get a text file that everyone can understand and determine what the particular website is capable of. Conclusion As with any “additional” software-building methodology there are time and effort costs involved.\nYou need to set up the Gherkin feature files and may need to change your automated tests so that step definitions are clear. However once you have everything in place then adding, modifying and removing redundant scenarios becomes pretty easy. The great things about BDD scenarios is that once you have build up a big enough step base then constructing new testing scenarios requires less work because of easy step re-use. About the Author My name is Miroslaw “Mirek” Majka and I’m a Senior Software Engineer at Findmypast .\nI enjoy building and testing Web solutions of every size and proportion.\nI have build end-to-end testing solutions using C#, Node.js, Postman, Selenium and Cypress for various companies in the past.\nI believe that with BDD you can have more people in your company invested in delivering quality software than just engineers. If you’d like to join our fantastic team of engineers visit the Findmypast Careers page!", "date": "2020-12-10"},
{"website": "FindMyPast", "title": "Round up of the last year", "author": ["Alexandra O'Mahony"], "link": "https://tech.findmypast.com/round-up-of-year/", "abstract": "As the year draws to an end, I have found myself looking back on all that has happened during my first year at Findmypast. I started my first ever software developer role with Findmypast and did so in the middle of the pandemic. Along with the other new joiners, we’ve had the rather unique experience of learning about a company as it adapts to the new norms of working from home. In this blog post I’m hoping to take you through that journey and summarise what we’ve learned while adapting to our “new normal”. Lockdown and a new role Job hunting as a Junior Developer is a daunting task for anyone but when I started in February 2020, COVID19 was already beginning to make companies and Recruiters nervous. I interviewed with Findmypast just one week after graduating from the coding bootcamp, Founders and Coders (FAC). I was kept in the loop throughout the process and had several interviews in just one week. Findmypast were, and still are, in the very fortunate position of still being able to hire right the way through lockdown. When I very nervously asked “Will you still be offering me a job if we go into lockdown next week?” (yes… to think there was a time we weren’t sure…!), my interviewers reassured me that if that were the case I’d be onboarded remotely. In the month leading up to my starting date I was very nervous. COVID19 was bringing a lot of uncertainty to the job market and already people were seeing offers being pulled and redundancies in the tech sector. Findmypast were brilliant throughout this period! I was reassured consistently throughout with a ‘welcome’ card and calls with Stu, our CTO, and my onboarding buddy, Rob. I received my laptop and ethernet cable (the wifi in my Dad’s house was definitely not up to the job of zoom calls) before starting, and another monitor after starting. Onboarding in a pandemic By the time my start date came around, almost a full month later, I’d been sent everything I could possibly need and had already been made to feel very welcome. I was excited, nervous too, but mostly excited. Onboarding is difficult to get right, there’s a lot of boxes to tick and yet you want to keep it engaging for the new start. Adding into the mix that we were having to do the whole thing over Zoom because we were working remotely in a global pandemic? Well, it’s pretty intense… but it was well organised and we made sure to have regular tea breaks ☕️. I knew Findmypast would be a great place to work because by the end of my first day, I made my first ever professional commit to GitHub 🎉! I was provided with comprehensive tutorials and guides to help me through the rest of my onboarding. Fortunately, in the Engineering department we have an hour a day which is specifically dedicated to learning and so I was able to complete a lot of it in that time. I got to know my team with organised get-to-know-yous, charades evenings and through pair-programming. We’re a team of 5 engineers (with occasional help from babies or toddlers on laps 😻) and are split over two offices in Dundee and London. In some ways, I think remote working has helped us develop as a team as the London/Dundee divide no longer exists, with all our conversations happening virtually. When remote was temporary Getting to know the rest of the company took a little more time. At the end of my first week, I waved from my box in the Zoom call to the rest of the company at our weekly Friday demos. Some people I don’t work with day-to-day were also kind enough to reach out to me and welcome me personally. I’ve now joined various not-so-work-related Slack channels, with the most successful being the #Findmypast-buddies. We started buddies, because we needed a way of introducing new starters to people outside of their direct team. In the channel we use donut, a Slack app, that pairs us up randomly with others in the company. It also gives us talking points and encourages us to set up a 15-minute chat with the other person. I’ve now met people from Marketing, the People team, and Engineers from teams I don’t work with at all. It’s been great fun! Realising remote was here to stay There has come a bigger shift in the last few months, as Findmypast  and the wider tech industry, began to realise that remote working would become our new normal. Since lockdown restrictions began, the Exec team have been sharing updates twice a week. Initially these were used to keep us informed about the company status or how the newest COVID19 regulations would affect us. These have now taken a shift into updates mixed with greater participation, breakout rooms where we discuss strategy, or ideation. There’s also been a big focus on taking time for ourselves. In August we were given a day off in lieu of our summer party, and once a month we have a mental health hour, when we leave our laptops behind to focus on ourselves and our wellbeing. Within my team we’re also flexible on our hours, and as we’ve entered winter, I’ve been taking longer lunches to make sure I get some fresh air. What’s been the best part? Seeing as we’re ending the year, I think it’s important to round this off with what I’m most proud of. Throughout 2020 we’ve seen major cultural and political shifts, with Lockdown, the Black Lives Matter movement, Brexit and calls to re-evaluate aspects of our own history etc. I’m so proud to be part of a company that has pledged to make a difference in some of these areas. As part of this pledge, we’ve created a Diversity and Inclusion working group where, even as a Junior Developer, I have been given the opportunity to contribute to our strategy for diversifying our workforce , especially within Engineering. I’m immensely proud to be part of this initiative and can’t wait to see our company grow in this way. If you want to be part of a team that values diversity, encourages continuous progression and creates a compassionate working environment check out our open roles:… https://www.findmypast.com/careers", "date": "2020-12-30"},
{"website": "FindMyPast", "title": "Making history: insights from inside a genealogy product team", "author": ["Carianne Whitworth"], "link": "https://tech.findmypast.com/making-history/", "abstract": "Every product person is usually on a mission to use insights, research, and tech to make something better for users. At Findmypast, our users are family history researchers. Here’s how their needs shape our work. The product value is clear Genealogy was once a paper-based activity largely practised by experts. Then developments in tech enabled enormous amounts of historical records to be scanned and translated into searchable text that users can access at any time, no matter how far they live from an archive. It saves researchers time, money and breaks down traditional barriers to access which in turn democratises research. This - for me - is an excellent reason to go to work. The outcomes are stories Customers’ discoveries are wildly diverse and their missions inadvertently spawn all sorts of subject matter expertise along the way. Users set off looking for a great grandfather and end up with degree-level knowledge about an 18th century gin craze (the newspaper collections are brilliant for context). Ask me about the development of deep sea diving technology in the 19th century, the fight for women’s voting rights and the introduction of postcodes in the UK. The community is great The researcher community actively supports each other to solve problems, share successes and level up their skills on all sorts of topics: changing place names, meanings of old occupations and new record sets to try. This is lovely to see, but also a brilliant source of insights for product people. The work is diverse One week we can be optimising our search tech, and on another developing features to surface newspaper stories from events during an ancestor’s lifetime. Making decisions in the space between customers’ expectations, technology, and the constraints of historical data is an interesting spot to be in - particularly when thinking about novice or younger customers whose main experience of information retrieval is with digital-first content designed specifically for search engines to crawl. It upcycles unexpected records When dog licences were brought into Ireland in the 1860s to ensure owners could be held liable for their trouble-making hounds , nobody anticipated that the same registers would be used over 150 years later to help people trace the names and addresses of their ancestors (and yes you can also find out the breed, colour, and sometimes name of the dog too). Genealogy has unleashed life into otherwise redundant data, which often has its own story to tell. We will launch the 1921 census next year Censuses usually take place every decade in the UK and are a brilliant means of tracing a family through time and finding out their occupations and addresses. Findmypast has worked with The National Archives to digitise the 1921 census, which is the last census to take place until 1951 since the 1931 census records were destroyed in a fire and the 1941 census did not take place during the Second World War. It’s nice to work on things that we know are really important to researchers. And we’re growing! Would you be interested in joining us to shape our product experience? Check out our current vacancies here.", "date": "2021-01-25"},
{"website": "FindMyPast", "title": "Running ASP.NET applications on Windows nodes in a Kubernetes cluster", "author": ["Attila Molnar"], "link": "https://tech.findmypast.com/k8s-windows-nodes/", "abstract": "In the latest installment of our Kubernetes series we will continue our journey with some Windows ASP.NET containerisation and deploy them onto Windows K8S worker nodes. Is it going to be viable though? The problem We still have some services running on old Windows virtual machines. These are .NET apps written in C#. We are using blue-green deployments to release new versions which double up our resources reserved by these applications. It’s described well by Martin Fowler if you would like to read more about it. Wouldn’t it be great to move them off from those VMs and run them in Kubernetes? This way we would be able to free up some resources and only use the required amount for actual demand. We could also utilize autoscaling, it would be much more straightforward to route to these apps within K8S instead of going out to our load balancer again. Easier chaos testing, fault injection, etc.\nSo there are numerous benefits to this! Dockerizing ASP .Net Applications We will be building a docker image from an ASP.NET application. We will be using the FMP project as an example. All steps are done on a Windows machine. Installing Docker First we install Docker for windows from https://hub.docker.com/editions/community/docker-ce-desktop-windows/ . \nOnce installed, click on the Docker Desktop on the taskbar, and select Switch to Windows Containers . This might need a restart. The Dockerfile First we’ll need to build our .NET app, as usual, either from Visual Studio or the CLI. We can use the output folder to build our Docker image. Comments explain why we did each step. For reference, we are running this on Windows 10 Pro Version 1909 (Build 18363.836). # It's an ASP.NET application so we are using the microsoft/aspnet image\nFROM mcr.microsoft.com/dotnet/framework/aspnet:4.8-20200512-windowsservercore-ltsc2019\n\n# Downloading url rewrite as it's required by FMP\nADD http://download.microsoft.com/download/D/D/E/DDE57C26-C62C-4C59-A1BB-31D58B36ADA2/rewrite_amd64_en-US.msi c:/inetpub/rewrite_amd64_en-US.msi\n\n# Uninstalling Web-Stat-Compression as it's not on the list of IIS features for FMP\nRUN powershell \"Uninstall-WindowsFeature Web-Stat-Compression\"\n\n# Enabling Windows Update (wuauserv) to support .Net 3.5 installation and then installing .Net 3.5 (NET-Framework-Features)\nRUN powershell \"Set-Service -Name wuauserv -StartupType Manual; Install-WindowsFeature -Name NET-Framework-Features -Verbose\"\n\n# Installing url rewrite\nRUN powershell -Command Start-Process c:/inetpub/rewrite_amd64_en-US.msi -ArgumentList \"/qn\" -Wait\n\n# Setting powershell as the shell to avoid parsing errors on the website names and bindings\nSHELL [\"powershell\"]\n\n# Removing the Default Web Site from IIS as it's not required and it's bound on port 80 which we will need for our website\nRUN Remove-Website -Name 'Default Web Site'\n\n# Copying our website to C:/websites/FMP, the folder location was copied from the build output\nCOPY ./build /websites/FMP\n\n# Adding our website to IIS, setting its Physical Path and binding it to port 80\nRUN New-IISSite -Name \"FMP\" -BindingInformation \"*:80:\" -PhysicalPath c:\\websites\\FMP As you can see there are some interesting steps there. We had to get these right in order to get our application up and running. Building and pushing the docker image after getting the Dockerfile right was fairly straightforward on Windows. The image size is really large though: 3.69GB. This wouldn’t be practical to run with, since we’re keeping a few tags around. We have found a blog post where Microsoft is talking about 40% image size reductions. https://devblogs.microsoft.com/dotnet/we-made-windows-server-core-container-images-40-smaller/ . Unfortunately, it’s only available in insider builds which are not compatible with our Kubernetes Windows node versions. Adding Windows worker nodes to Linux based Kubernetes Clusters Now that we have a docker image we can use, it’s time to add some Windows worker nodes to our Kubernetes cluster. Prerequisites An existing Kubernetes cluster with a Linux master node the worker node can communicate with. We were working with Kubernetes version 1.19.1 at the time. A VM with Windows Server 2019 with the latest cumulative updates, because in order to have network overlay enabled we need KB4489899 and KB4497934 . Our Windows Server 2019 Build Number is 1809. Enough fixed space on the Windows worker node. 50GB at least, ideally 100GB. Make sure it’s a fixed disk and not dynamically allocated space. WinOverlay Kubernetes feature gate enabled on all of the master nodes by editing the /etc/kubernetes/manifests/kube-apiserver.yaml and appending - --feature-gates=WinOverlay=true to the argument list. Preparing the master node for Windows workers This guide is based on the Kubernetes Windows Guide recommendations. To add the first Windows worker to the cluster, we’ll have to prepare our existing Kubernetes cluster to deal with the connection.\nThey are as follows: Edit the kube-flannel ConfigMap with: kubectl edit cm -n kube-system kube-flannel-cfg Search for the net-conf.json and add 2 new fields to the Backend section. This will look like: \"Backend\": {\n     \"Type\": \"vxlan\",\n     \"VNI\" : 4096,\n     \"Port\": 4789\n   } Don’t save the ConfigMap just yet, find the cni-conf.json section instead and change its name to vxlan0 from cbr0 Get the raw version of a node-selector patch with wget from https://raw.githubusercontent.com/microsoft/SDN/1d5c055bb195fecba07ad094d2d7c18c188f9d2d/Kubernetes/flannel/l2bridge/manifests/node-selector-patch.yml and execute kubectl patch ds/kube-flannel-ds-amd64 --patch \"$(cat node-selector-patch.yml)\" -n=kube-system Add Windows Flannel and kube-proxy DaemonSets Get the Kubernetes version you are running with the following command kubectl get nodes Check if your Ethernet name is Ethernet by going to the Windows node, access the Control Panel and then Network and Sharing Center. Once there, go to Change adapter settings . In case it’s not Ethernet , please refer to the notes of the section Add Windows Flannel and kube-proxy DaemonSets on the Kubernetes Windows Guide . Replace KUBERNETES_VERSION on the line below with your Kubernetes version and run the commands curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed 's/VERSION/v<KUBERNETES_VERSION>/g' | kubectl apply -f -\n     kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml Adding a new Windows worker node to the cluster Again, we’ll have to prepare this Windows machine to join the cluster. Enable Containers on Windows Get-WindowsOptionalFeature -Online -FeatureName Containers            \n Enable-WindowsOptionalFeature -Online -FeatureName Containers\n Restart-Computer -Force Now install a specific version of Docker ( 18.09.11 ). This is because when we installed a newer version of Docker ( 19.*.* ) we had problems with kubelet pausing frequently. We also saw some PLEG errors saying something like PLEG is not healthy: pleg was last seen active 3m7.629592089s ago when describing the node with kubectl and checking the kubelet status there. It was resolved by using this specific version of Docker . Install-Module -Name DockerMsftProvider -Repository PSGallery -Force\n Install-Package -Name Docker -ProviderName DockerMsftProvider -RequiredVersion 18.09.11 Restart the Windows VM Install wins, kubelet, and kubeadm by pulling and running the script from the C:\\ directory proving the target Kubernetes version to the script curl.exe -LO https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/PrepareNode.ps1\n . \\P repareNode.ps1 -KubernetesVersion v<KUBERNETES_VERSION> Join the cluster Go to the control plane host and get the command to join the cluster root@control-plane kubeadm token create --print-join-command Execute the command from the previous step on the Windows node you are adding Note: It may be necessary to clear down the C:\\etc\\kubernetes directory before running the above command on the Windows node to enable the script to complete. Check the cluster status See if the node is ready ssh root@control-plane kubectl get nodes -o wide In case it’s not ready, check if flannel is running, mind you, it might take a while to download the image. kubectl -n kube-system get pods -l app = flannel If after downloading the image it is still not ready, please refer to the Kubernetes Troubleshooting guide . Since this is a Windows worker node, we don’t want to schedule certain pods on it. This includes the Prometheus, metrics and monitoring pods. To disable scheduling Linux docker containers on the worker nodes, we’ll need to add a taint: kubectl taint nodes <node> OS=Windows:NoExecute This will only allow deployments with the “OS=Windows” toleration on them. For the flannel networking to work correctly, we’ll need to edit the kube-flannel-ds-windows-amd64 DaemonSet in the kube-system namespace and add a new toleration in (there will be a tolerations block already): - key: \"OS\"\n  operator: \"Equal\"\n  value: \"Windows\"\n  effect: \"NoExecute\" There are other ways to stop deploying to specific nodes, but we’re using this approach for now. Should you notice any issues with the networking, you can use the Windows Kubernetes networking troubleshooting guide to debug them. Deploy with our docker image to the newly added Windows worker node Our last step is to deploy the .NET application into Kubernetes! These are our Deployment apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: fmp\n    environment: playground\n  name: fmp\n  namespace: playground\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: fmp\n      environment: playground\n      release: fmp-playground\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: fmp\n        environment: playground\n        release: fmp-playground\n    spec:\n      containers:\n        image: <our_private_docker_repository>/findmypast/fmp:v1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /api/health\n            port: http\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n        name: fmp\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        resources:\n          limits:\n            memory: 1000Mi\n          requests:\n            cpu: 50m\n            memory: 200Mi\n      dnsPolicy: ClusterFirst\n      hostname: kube-fmp-01\n      nodeSelector:\n        kubernetes.io/os: windows\n      restartPolicy: Always\n      tolerations:\n      - effect: NoExecute\n        key: OS\n        operator: Equal\n        value: Windows And Service config objects: apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: fmp\n    environment: playground\n    monitoring: prometheus\n  name: fmp\n  namespace: playground\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app: fmp\n    environment: playground\n  type: ClusterIP kubectl apply -f deployment.yaml service.yaml and we can watch our image getting pulled and pods should hopefully end up in a Running state. Troubleshooting Flannel Pod Enters ‘CrashLoopBackOff’ State If the kube-flannel pod enters CrashLoopBackOff , it might be due to a missing podCIDR configuration. Check the logs to see if an error similar to the following is present: E0302 11:39:19.978950       1 main.go:289] Error registering network: failed to acquire lease: node \"<node>\" pod cidr not assigned If you see an error similar to this, check whether the podCIDR has been set using the following: $ kubectl get node <node> -o jsonpath = '{.spec.podCIDR}' If the podCIDR has not been set, you need to get a list of podCIDR s currently configured in the\ntarget cluster by running the following (where the second line is example output): $ kubectl get nodes -o jsonpath = '{.items[*].spec.podCIDR}' 10.244.0.0/24 10.244.3.0/24 10.244.2.0/24 10.244.1.0/24 Finally, you should patch the affected node to assign a podCIDR not in the outputted list, for\nexample (where the second line is the output from the command): $ kubectl patch node <node> -p '{\"spec\":{\"podCIDR\":\"10.244.4.0/24\"}}' node/<node> patched You’ll then need to verify that the windows pods are running in the kube-system namespace. If not yet, give them a restart. Can’t route to services / kube-proxy or flannel pods are restarting/not running You will need to patch the file , then restart the node for the networking to be correct. Summary So, the initial question remains… Is this going to be viable? We can see how circumstantial it is to set up Windows worker nodes and deploy an ASP.NET app into Kubernetes. It is possible, but one would not say it is easy. After getting the application running in K8S, pods were really unstable. We were getting odd container restarts; health check timeouts and metrics were lacking as well. We decided not to go with this setup in our production systems for now. Don’t give up hope just yet! There are interesting articles out there that talk about setting up monitoring for Windows nodes and pods: https://www.inovex.de/blog/kubernetes-on-windows-2-tools/ .", "date": "2021-02-05"}
]