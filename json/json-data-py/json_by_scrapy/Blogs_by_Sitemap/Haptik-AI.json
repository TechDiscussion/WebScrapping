[
{"website": "Haptik-AI", "title": "Setting Up a RabbitMQ Cluster & Failover", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/rabbitmq-cluster-failover/", "abstract": "Tweet RabbitMQ as we all know has been around for long and is being used in more than 40% applications worldwide for Message Queuing which implements Advanced Message Queuing Protocol (AMQP). Based on Erlang Programming Language it handles clustering and failover very smoothly. RabbitMQ is used to queue messages sent between applications. For example, I could queue some messages in RabbitMQ from one part of the application and put some workers for another application below it which would process those messages as an when required and get the task done. You can read more about RabbitMQ here . Use Case Setting up RabbitMQ cluster and configuring failover. Clustering is inbuilt into RabbitMQ and for failover we will use AWS Route 53 and AWS CloudWatch . I am assuming you already have a RabbitMQ server setup on an AWS EC2 instance or any other platform. We will be creating a new RabbitMQ node and then creating a cluster. At Haptik , we believe in more reliable architectural components and hence we decided to implement this. This is not Load-Balancing. Pre-requisites 1. RabbitMQ server up and running. You can follow this if you don’t already have that: Setup RabbitMQ . 2. A new node/server could be an AWS EC2, Digital Ocean droplet or any other platform. 3. Follow the steps in the above link to install RabbitMQ server on the new node as well. Remember that versions should be same on this serve and RabbitMQ Management plugin should also be enabled for UI. It can be enabled following the steps here . You can check it running on port 15672 . You can also just bring up the new server using an AMI of the master server. Steps To Follow To Setup The Cluster Steps on the RabbitMQ server 1: 1. SSH into your RabbitMQ server 1. Let’s call it rabbitmqnode1. 2. Set hostname using following commands. hostname rabbitmqnode1 Edit the following file /etc/hosts and make the following entry there: 127.0.0.1 rabbitmqnode1 \"Public/Private IP of your new RabbitMQ server\" rabbitmqnode2 3. You’ve now set a hostname for your RabbitMQ server and told it which is the other node in the cluster you’re about to setup. View the content in the Erlang cookie file which can be found at /var/lib/rabbitmq/ using It should show content as follows: Just copy the text and keep it handy. Steps on the RabbitMQ server 2 (New Server): 1. Stop RabbitMQ server app using the command: sudo rabbitmqctl stop_app 2. Now, go ahead and replace the erlang cookie on this server. Remember the erlang cookie we previously copied from server 1? 3. Now, run the following commands one by one: sudo rabbitmqctl reset rabbitmq - plugins enable rabbitmq_management sudo rabbitmqctl join_cluster rabbit @ rabbitmqnode1 sudo rabbitmqctl start_app (Adds this server as a part of a cluster of rabbit1 (the primary rabbitmq server)) rabbitmqctl join_cluster --ram rabbit@rabbitmqnode1 (This does the same but adds it as a RAM node, meaning everything will be save over RAM and non-persistent as in Disk mode which is default) 4. Run the following commands to set HA policy and see the cluster status now and you should see all the nodes in the cluster and their mode, whether RAM or DISK: sudo rabbitmqctl set_policy - p < strong > your - < / strong > < strong > virtual - host - name < / strong > ha - all \".*\" '{\"ha-mode\":\"all\",\"ha-sync-mode\":\"automatic\"}' rabbitmqctl cluster_status You’re done. In the management console of all the nodes on the browser, you will see same graphs, messages, queues. This is an active-active cluster that we have setup. Below is an example of how the nodes would look like (I have added a third one as well which is in RAM mode ): Steps To Setup Failover For failover, I am using a simple AWS Route53 Health check, which makes a DNS switch to one of the secondary servers if the primary server fails to respond. Once the above steps are performed, you can just make a simple script to push RabbitMQ master’s status to AWS CloudWatch. There can be infinite ways to do so, I am using a simple telnet: #!/bin/bash INST_ID = i - 0584axxxxxxxxxxx cd / usr / local / nagios / etc / objects / scripts rm - rf / usr / local / nagios / etc / objects / scripts / myoutputfile telnet rabbit . xxxxxxxstagingtestwebsite . com 5672 | tee - a / usr / local / nagios / etc / objects / scripts / myoutputfile cat / usr / local / nagios / etc / objects / scripts / myoutputfile | grep \"Connected\" if [ $ ? - eq 0 ] ; then count = 1 / usr / local / bin / aws cloudwatch put - metric - data -- metric - name \"RabbitMQ staging UP/DOWN\" -- unit Count -- value $ count -- dimensions InstanceId = $ INST_ID -- namespace Rabbitmq else count = 0 fi So, if the it’s connected, I push count 1 to CloudWatch else I push 0 which means the host is down. Just make a relevant AWS CloudWatch alarm which is at alarm state when the count is 0. We will use the same alarm in AWS Route 53 health check. Just go to AWS Route 53 and setup a health check from the menu from the left hand side: Now, select Create health check : The above shall open a page where you need to enter some details. I will keep it brief here. I have chosen based on State of CloudWatch alarm because I have used the above script to note the status of my RabbitMQ server: If I choose an endpoint, it should be publicly accessible, which is not true in my case. My RabbitMQ server is not publicly accessible. Now, choosing that gives me options as below: Once you’re done with that, you can click on Next and that should create the health check for you. Now, you can see the health check status as follows: Once the above is done, i.e. the health check is configured, just create a DNS failover in AWS Route53 based on the health check we just made. (Assuming you know how to do that, else you can follow the steps here: Route 53 DNS Failover ) Hope this helps you setup a working RabbitMQ cluster. I will soon be coming up with more exciting blogs. Until then keep following Haptik Tech Blog . Posted by Ranvijay Jamwal on Dec 22, 2016 7:17:00 PM Find me on: LinkedIn", "date": "2016-12-22,"},
{"website": "Haptik-AI", "title": "Contextual Speech Recognition for Voice Assistants", "author": [" Dhiran Jamana"], "link": "https://www.haptik.ai/tech/contextual-speech-recognition-for-voice-assistants", "abstract": "Tweet Take a look at the video below, the solution in play is what we’ll be explaining through this write up. Notice the processing at the bottom of the screen grab to better understand the solution. Now that you’ve looked at the video of the solution in play, let’s understand how we get here! Haptik focuses on developing a deep understanding of natural language and domain intelligence as a leading provider of intelligent assistants, including voice-enabled ones. When it comes to speech recognition in Voice Assistants, we rely on device libraries or third party APIs that include Android default Speech Recognition, iOS Speech Recognition or API providers such as Azure/Reverie/Google. We discuss in this blog how Haptik uses domain intelligence to solve speech recognition inaccuracies across domains in third party speech recognition systems. Let’s understand the p roblem statement Improving the accuracy of third party speech recognition solutions to better cater to user intent. At Haptik, we use a layer of domain intelligence based post-processing of the output from third party speech recognition solutions to better understand and accordingly fetch results for the right user intent. For example, if a Voice Shopping Assistant user says \"Laung\" and the Third Party system's top interpretation is \"long,\" we can recover from the error and correct it back to \"Laung.\" This suggestion is made based on contextual understanding of the term which makes more sense in the grocery domain. Understanding Speech Recognition APIs Output Almost all Speech recognition APIs return the top N interpretations along with their confidence scores. Example Output if a user speaks out the query “Vaseline” Vaseline (0.342865258455), westland (0.115404672921), westlin (0.0663541182876), West Lane (0.0444444417953), wrestling (0.0444444417953), vesselin (0.0444444417953), westling (0.0444444417953), westlund (0.0444444417953), westline (0.0444444417953), West line (0.0444444417953), West plane (0.0444444417953), waistline (0.0444444417953), vestland (0.0444444417953), westerlund (0.0444444417953), West Linn (0.0444444417953) Deeper Understanding of the Problem We took a live Grocery Ordering Voice Assistant for our client as the test bed for building out this feature. Let’s take a look at the performed analysis: Types of queries being received on the Grocery Ordering Voice Assistant These were found to be largely queries by category, brand or specific products. Kayam Churan for pet Dard fruit and nut chocolate Dairy milk 3 kilos of tomato Joy lotion 50% off show maximum discount Vaseline Hide and Seek choco rolls Maggi Masala Indian flavour please add 3 Ghadi detergen Surf Q) How many queries have speech recognition issues? A large percentage (above 30%) were found to have speech recognition issues. Q) In what percentage of queries with speech recognition issues was it humanly possible to identify the correct query? Human identification was found possible in almost 75% cases. This would be the approximate upper limit  for improvement in Speech Recognition interpretation  through post processing after the response is received via APIs. The reason  behind this is simple.  If a human with domain understanding is not being able to identify the correct interpretation of the user query, a machine would definitely find it difficult. Examples of Cases with correction humanly identifiable. red chillies holes -> red chillies whole casio nuts -> cashew nuts hand seming tyzer -> hand sanitizer needless -> noodles bainjal -> brinjal send fuel oil -> sunflower oil loosu chini chahie -> loose chini chahie mujhe Les lena hai -> mujhe lays lena hai pasi paruppu -> pesarapappu Examples of Cases where user intent was not understood even by a human. Udan mata Macons Wam Shri Q) In how many cases was the best interpretation provided by the ASR incorrect while the other probable interpretations had the correct option? Around 30% of the speech recognition errors were found to contain the correct interpretation in the probables. Red Chillies holes -> absent Casio nuts -> absent hand seming tyzer -> absent Needless -> present Bainjal -> absent send fuel oil -> absent Pepsodent Baso gram -> absent loosu Chini chahie -> present Mujhe Les lena hai -> present Pasi paruppu -> absent Formulating the Solution We figured that there were three directions in which Speech Recognition could be improved after the response is received from the Third Party API. 1. Selecting the probable which is most relevant to the domain. 2. Correcting based on phonetic similarity to known domain words. 3. Learning common Speech to Text errors which were repeating in the data. Implementation & Learnings Selecting the probable which is most relevant to the domain Domain specificity means that our Conversational AI models are based on the learnt ontologies for each domain. These include all concepts important to the domain such as subcategories (e.g. balm, spread, shower gel, shampoo, toothbrush), features (e.g. spicy, loose, brown, chocolate favoured),Brands (e.g. Tropicana, MTR, Cadbury, Fevicol) and more.It also understands common query patterns in the domain - e.g. ‘I want to buy milk’, ‘mujhe hari sabjiyan chahiye’, etc as well as the synsets for common words - e.g. buy = purchase = kharidna etc.This structure  enables us to identify which query interpretations are more domain related. For each interpretation, we compute a domain specificity score. For each word in the query found as a category, brand, feature or product name word a high score is given. E.g. Cashew, mango, pickle, bourbon etc. For each word belonging to common queries but not a part of domain words, a medium score is given. E.g. i, want, buy, have etc. The highest scoring domain specific interpretation is then taken to be the correct one. Challenges faced in this approach & learnings There were 2 major challenges faced in this approach: Maintaining a balance between domain specificity and speech recognition confidence scores . Correcting as many incorrect interpretations as possible to more domain specific ones can result in correcting already correct interpretations that can really break the user experience. To avoid this, we put a threshold to only try correcting if the original query did not have a high confidence score or was not already sufficiently domain specific. This allowed us to recover back parts of the speech recognition accuracy while making sure that no additional inaccuracies were introduced by this algorithm. Domain specificity calculation can lead to a larger score for interpretations with more words. E.g. pasty Colgate got corrected to face ke colgate because the correct interpretation paste colgate had lesser number of words and consequently a lower domain specificity score. We overcame this by introducing a length normalization while computing the Domain Specificity Scores. Learnings from this project so far: Trying to correct based on phonetic similarity to known domain words. This approach is  based on the hypothesis that if we  take the interpretation of the Speech Recognition system and correct all words which were not understood to phonetically similar words in our domain knowledge, we can change some of the incorrect interpretations to correct ones. We implemented multiple phonetic similarity algorithms including soundex, metaphone etc. Among these, soundex performed best but overall we found the correction accuracy via this approach was very low. What that meant was that while phonetic correction did correct some of the words, in most cases it faltered by correcting to some other similar sounding word. Ultimately this approach was dropped from the end solution. Learning common Speech to Text errors which were repeating in the data. One of the things we saw from a visual scan of the user conversations was that people have a tendency to try repeating themselves more slowly and clearly if their query is not understood by the Voice Assistant. This was an in-depth data set to learn speech recognition errors from. We identified all queries where the user changed the query after being misunderstood by the Voice Assistant. To avoid scenarios where users indeed changed their query to something else entirely and were not really repeating the previous query, we put a restriction to consider corrections happening within 30 seconds of the first query. A phonetic similarity threshold was also added to further strengthen the restriction. This led to a significantly sized library of learnt corrections e.g. the tall -> dettol, siltod -> cinthol, dappa -> dabur, hair ji -> hair gel, painting -> pantene etc . Conclusion The above implementation was able to yield a significant increase in speech recognition accuracy compared to using the underlying domain agnostic speech recognition APIs directly. Furthermore, the team is continuing to improve this solution including usage of phrase context to prioritize corrections and improvement in terms of managing phonetics based corrections. Posted by Dhiran Jamana on Feb 11, 2021 6:11:14 PM Find me on: LinkedIn", "date": "2021-02-11,"},
{"website": "Haptik-AI", "title": "Real Time Messaging Using Mqtt", "author": [" Viraj Anchan"], "link": "https://www.haptik.ai/tech/real-time-messaging-using-mqtt", "abstract": "Tweet This post is written by Viraj Anchan, Full Stack Engineer at Haptik. Haptik is a chat application that connects users to their digital personal assistants in real time. When chatting with your personal assistant, you want as little delay as possible in order to get things done quickly. When I joined Haptik after graduating from college in May 2015, my first technical challenge was learning the messaging architecture. In July 2015, I suggested to our CTO that we shift from XMPP to MQTT. Our mobile apps (Android & iOS) and Athena (web chat tool used by our assistants) used XMPP for real-time messaging. We were facing a lot of issues with XMPP.  To point out a few, XMPP lacks built-in Quality of Service (QoS). QoS is an agreement between the sender and receiver of a message regarding the guarantees of delivering a message. Since XMPP lacked the built-in QoS, we had to build our own custom solution to ensure message delivery. Along with that XMPP session is one big long XML document and every client has to use an event-driven XML parser. All in all, XMPP was proving to be a lot of overhead and maintenance for us and we needed a better more scalable solution. In January 2016, we finally decided to shift from XMPP to MQTT. MQTT is a lightweight machine-to-machine/”Internet of things” connectivity protocol. MQTT stands for MQ Telemetry Transport. MQTT is an extremely simple and lightweight publish/subscribe messaging protocol. It is designed for constrained devices, high latency or unreliable networks. The design principles are to minimise network bandwidth and device resource requirements whilst also attempting to ensure reliability and some degree of assurance of delivery. These principles make MQTT ideal for IOT and mobile apps where bandwidth and battery power are at a premium. 5 reasons why we shifted from XMPP to MQTT 1. Less overhead and lightweight 2. Supports QoS (fire and forget, at least once and exactly once) 3. Low keep-alive traffic 4. Pub/Sub Mechanism 5. Low power usage MQTT provides 3 types of QoS for delivering messages: QoS 0 (fire and forget) – The message is delivered at most once. QoS 0 messages are not stored. Therefore QoS 0 messages are lost if client disconnects. In QoS 0, delivery of message is not acknowledged. It is the fastest mode of transfer. QoS 1 (at least once) – The message is delivered at least once. Messages might be delivered multiple times if sender does not receive an acknowledgement from the receiver. The messages must be stored at the sender’s end, until sender receives a confirmation from the receiver. QoS 2 (Exactly once) – The message is delivered exactly once. QoS 2 ensures that the message is received exactly once by the receiver. The messages must be stored at the sender’s end, until sender receives a confirmation from the receiver. It is the most reliable mode of transfer. It is also the slowest mode of transfer since it uses an advanced acknowledgement sequence as compared to QoS 1. MQTT uses a pattern called publish/subscribe. Multiple clients connect to the MQTT broker. Clients can either publish or subscribe to a topic. Topics are used by the broker to decide who will receive a message. The broker and MQTT act as a simple, common interface for everything to connect to. Our MQTT server is powered by Mosquitto . Mosquitto is an open source message broker that implements the MQTT. It is written in C. Mosquitto is easy to install and deploy. It supports TLS, Websockets and provides authentication either via username/password, pre-shared keys or TLS client certificates. It also supports ACL. Using ACL, you can configure which clients can access which topics. We decided to use Paho Python library for the backend.  Our backend maintains an MQTT connection and routes messages through our chat pipeline. Installation (mosquitto on ubuntu) sudo apt-get install mosquitto Installation (paho-python) pip install paho-python Here is a simple client that subscribes to a topic (sports/cricket/india) and prints out the resulting messages. <i><span style=\"font-weight: 400;\">import paho.mqtt.client as mqtt</span></i> <i><span style=\"font-weight: 400;\"># The callback for when the client receives a CONNACK response from the server. </span></i><i><span style=\"font-weight: 400;\">def on_connect(client, userdata, flags, rc): </span></i><i><span style=\"font-weight: 400;\">&nbsp; &nbsp; print(\"Connected with result code \" + str(rc)) </span></i><i><span style=\"font-weight: 400;\">&nbsp; &nbsp; client.subscribe(\"sports/cricket/india\")</span></i> <i><span style=\"font-weight: 400;\"># The callback for when a PUBLISH message is received from the server. </span></i><i><span style=\"font-weight: 400;\">def on_message(client, userdata, msg): </span></i><i><span style=\"font-weight: 400;\">&nbsp; &nbsp; print(msg.topic +\" \" + str(msg.payload)) </span></i><i><span style=\"font-weight: 400;\">&nbsp; &nbsp; client = mqtt.Client()</span></i><i> </i> <i><span style=\"font-weight: 400;\">client.on_connect = on_connect </span></i><i><span style=\"font-weight: 400;\">client.on_message = on_message</span></i> <i><span style=\"font-weight: 400;\">client.connect(\"localhost\", 1883, 60)</span></i> <i><span style=\"font-weight: 400;\">client.loop_forever() &nbsp;</span></i> Here is a simple client that publishes to a topic (sports/cricket/india) <i><span style=\"font-weight: 400;\">import paho.mqtt.client as mqtt</span></i> <i><span style=\"font-weight: 400;\">client = mqtt.Client() </span></i><i><span style=\"font-weight: 400;\">client.connect(\"localhost\", 1883, 60) </span></i><i><span style=\"font-weight: 400;\">client.publish(\"sports/cricket/india\", \"India wins Asia Cup 2016!\") </span></i><i><span style=\"font-weight: 400;\">client.loop(2)</span></i> MQTT has helped to make our application lightweight and ensure real-time reliable message delivery. MQTT is an amazing protocol which has lots applications in mobile, IOT and M2M communications. If you want a lightweight and reliable messaging protocol, then you should definitely consider MQTT. Wish to be a part of the amazing things we build? Look no further! Reach out to us at hello@haptik.co Posted by Viraj Anchan on Mar 29, 2016 7:00:00 PM", "date": "2016-03-29,"},
{"website": "Haptik-AI", "title": "A Day In The Life Of A Quality Analyst At Haptik", "author": [" Admin"], "link": "https://www.haptik.ai/tech/day-in-life-quality-analyst-haptik", "abstract": "Tweet In the early days at Haptik the engineers built, tested, and released software all by themselves. As the company grew and features proliferated, everybody obviously realized that this wasn’t scalable and that developers can’t test their own features. The engineers then started specializing in skill, creating more scale in the development process at Haptik and that is when I made my grand entry. I joined Haptik in September 2015 as the sole QA Engineer. I was given the responsibility to test our core products which included our apps, web chat tool and machine learning features. Being the only tester at my previous company, I had just the right kind experience and skills; so when I was given the opportunity to assure quality of products at Haptik I was excited. Being the only tester meant I also had to work with every single person in the company to make sure we are working towards the same goal and this was one challenge I was ready to take head on. Putting everything else aside, testing such a large product and environment is a long and complex process. At a startup which focuses on nimbleness and speed, requirements can come in the middle of development and it is the responsibility of a QA to make sure the existing systems doesn’t break and the new features are build as per requirements and guidelines. Before I joined the team, testing was squeezed some place in between the end of development and pre-release. After I joined, we have managed to build a culture of testing at different phases of development to ensure quality. Over the months we have learnt and improved a lot of things & still have a bunch to change. Below are some of the things we did built, and in my opinion have worked and there are also some thoughts around what else we can do to improve our QA processes. Processes that worked: Unit testing: This is a very important process to ensure quality products being shipped from development to testing. The developers need to write unit tests for the features being built and need to make sure all unit tests pass before a feature can be send to testing. Git process: Another thing of importance is to have a proper git process. We have implemented git flow by Vincent but in general having a proper process around git is very important to assure quality. Testing at different phases: It is of significant importance for testers to be involved in different phases of SDLC. We now test at various stages Before feature is complete: We try and test at every checkpoint while a feature is being developed. This helps catch bugs while a feature is being developed. Integration testing: We now have a staging environment where all the complete features are shipped and we test the app end to end to make sure new features are not breaking other old features Before release (production testing): We point our apps to production and test using production to make sure there are no surprises when the apps are released After release: Once our product is released we do sanity tests to make sure everything is working as expected. Issues we still face Smaller bugs are hidden behind bigger bugs: We have seen that small bugs get shipped into production because they get hidden behind bigger bugs. This happens because we don’t thoroughly test the feature individually before we call it complete. We can fix this by adding proper testing at different phases of the git flow process that we process. Testing time is taking long: As our products get complex it takes more time to test since we need to test all existing features along with new ones. To tackle this challenge we are starting to build automation to save time testing old features. Bugs in production not reproducible: We use crashlytics to find out crashes that users face and some of the crashes which occur (although infrequently) are hard to reproduce. At this point we try and reach out to our users over Haptik / email / call to ask them for help to reproduce the issue. All in all below here are some of the things I learnt to become a better QA Engineer: Have a deep understanding of the product and the features being built Spend some time in understanding who your users are and how they use the product Make sure to accept & acknowledge the deadlines before starting the sprint. Talk about problems out loud as it helps you think about things in a different way Rubber duck debugging actually helps a lot so try it out Ask questions, make notes and connect the information people give you to turn it into knowledge. That being said, we have now started growing our QA team. I would love to hear your thoughts and suggestions around QA and how we can build quality products. Also don’t forget, we are hiring high quality QA engineers. So if you are interested reach out to us at hello@haptik.co This post is written by Sanchita Mishra, Sr. Software Quality Assurance Tester at Haptik. Posted by Admin on May 2, 2016 7:39:00 PM", "date": "2016-05-2,"},
{"website": "Haptik-AI", "title": "Javascript: The \"Awesome\" Parts", "author": [" Admin"], "link": "https://www.haptik.ai/tech/javascript-the-awesome-parts/", "abstract": "Tweet “ The year 1995 and forth was a crazy time. Java made its entry to the mainstream and was ready to rock the world, and soon enough intelligent folks at Netscape brought the Netscape Navigator and things changed. Everybody could immediately understand that these two would change how we interact and think about computing. The first was intended to be used on all the devices whereas the latter would define a new computing system inside another. var yoJavascript = parseInt(‘Oops!’); if (yoJavascript == yoJavascript) { console.log(‘Hello Javascript!’); } else { // Civil War console.log(‘Gotcha!’); console.log( But soon enough it was observed that Java and its browser technology (Java Applets) were not optimized and finely tuned to be able to be used inside a lightweight browser environment. They were heavy, required steep learning and sometimes would not behave exactly how we wanted to. Then entered Javascript by Netscape. Java and Javascript shared synonymity in names, but they were (are) not the same; in fact they were (are) very different. Where one was (is) compiled and statically typed, the other was (is) interpreted and dynamically typed. Where one would not leave OOPS for a second, the other would not even think about it (till now). Cut to 2016, both Java and Javascript have seen a lot of changes. They have become more mature, optimized and now have a definite set of users/developers and have definite goals. Javascript, in fact, is waiting for another overhaul change (ES6) and now powers many web applications thanks to a new groundbreaking technology called the AJAX (supported by JSON). You can now consume APIs and show or change data on your browser without even having to refresh the page. This is a big benefit. But there is also a downside to Javascript. Even though everybody now uses Javascript to power their web applications, the developer community has not really accepted the language to the fullest. Plenty of us still think about it as a poorly made one which should be used just to fulfill the immediate purpose and then be left behind. There can be many reasons behind it, and to list a few: 1) The name. The name ‘Javascript’ attracts a lot of confusion. The word ‘Java’ in it gives an impression that somehow it’s related to Java and the word ‘script’ might tell you that it’s sort of a scripting language. Both, relatively are not true enough. 2) Few poorly made language choices. And the gotchas. Let’s admit it. Javascript has few poorly made language choices. It has few concepts which do not go well. Also, it has too many ways to do the same thing which is at times, confusing. Let’s take an example of creating a function which takes 2 numbers and returns the added value: Way 1: function addNumbers(n1, n2) { return n1+n2; } Way 2: addNumbers = function(n1, n2) { return n1+n2; } Way 3: var addNumbers = function(n1, n2) { return n1+n2; } Way 4: var addNumbers = function iCanNameAnything(n1, n2) { return n1+n2; } Way 5: var myFunctions = { addNumbers: function(n1, n2) { return n1+n2; } } … and many more Where all of these ways have different meaning and usage, it’s very confusing for the beginners and sometimes for professionals too to choose the best one. (If you ask us, 3 and 5 works best for us.) Apart from having these way-too-many-ways to perform something, javascript has some tricky concepts (gotchas) which needs utmost attention. Sometimes it can be a pain to debug some error caused by one of the javascript gotchas and you might get frustrated. One example can be seen at the beginning of this post. The method ‘parseInt’ returns NaN for anything apart from a number. But in javascript, NaN is never equal to itself and thus NaN == NaN will return false. Now that can be explained in a mathematical way treating NaN as infinity, could it not be designed in a better way? We gotta have a question. ( Common Javascript Gotchas is a great source for more of such javascript gotchas.) 3) The trial and error approach. Javascript comes built-in with all the modern web browsers. Now that helps developers like us to directly and easily dive into programming and building javascript applications; it has a cost. The cost of building trial and error approach. As its very easy to start coding into javascript, we sometimes do not really learn the concepts. As its just matter of seconds to change the code and see the changes reflect in the browser, developers tend to build the trial-and-error approach. Also as programmers, we like solving big and complicated problems which require us to write big and complicated algorithms and somehow javascript has failed to provide us the same. But even though javascript has its limitations and the gotchas, there is no denying that its ruling the browser ecosystem. At Haptik too, we use javascript extensively. In fact, our chat app Athena, which helps our assistants to solve your queries, is made with React (a javascript framework by Facebook). Also one of the most popular javascript libraries; jQuery powers our many in-house tools. And thus we love Javascript. But as we speak about Javascript, we take immense care to make sure we follow high standards while coding in Javascript. We learn the basics, understand the data structure (yes! Javascript has a data structure), create algorithms and code. This along with the following measures helps us to reduce the number of errors in our Javascript code and helps us to scale. Measure 1: The lints . We love (or hate :P) lints. We have lints for almost all of our languages, be it for Python (we use Python and Django for our backend) or Javascript. We have integrated these lints with our Github repo and thus any pull request tells us all the possible javascript errors we have made while coding and helps us to fight the basic mistakes. If you are interested in knowing how does that work, here’s a great read . Measure 2: Coding guidelines. We believe prevention is better than cure. And that is why we follow strict javascript coding guidelines. As we spoke before about way-too-many-ways to do something in Javascript, we have curated for ourselves the best possible ones. But being overprotective, we might not want to share those here (:P) but here’s a great article about good practices in Javascript. Measure 3: Object oriented Javascript. We are big believers of object oriented programming. Even though Javascript do not directly inherit the OOP concepts, we have workarounds to make it work like one. Here’s a great read to do so in ES5 . Measure 4: Libraries. We constantly look for great libraries in Javascript and try to use them. We believe that these libraries help us not to reinvent the wheel and also allow us to prevent the common mistakes. One classic example would be of using React in Athena. We previously used to use jQuery to power Athena and when things got messy we pivoted to React. You can read about it here . With all that being said, to make Javascript more awesome, here comes ES6! ES6 is about to release in full force (it’s already released but has partial browser support) and it has a few much needed language changes. One very welcome change would be the introduction of the keyword “let”. Now we have something which creates block level scope of variables unlike the old “var”; and many more improvement like classes and promises. Apart from the futuristic ES6, we have many present libraries/frameworks which has changed the javascript paradigm. To name a few, AngularJS , Ember.js , Backbone.js , Ext JS , React have created different design patterns and solved different use cases and are doing great. And who can not like the power of Node.js , the server-side Javascript. Now we have the power to use Javascript in server side to act like any other backend technology. This not only allows us to think and code in only one specific language for both frontend and backend, it also allows us to port the same code back-and-forth and with a little tweak your code starts working in both the places. Bingo. In fact we loved NodeJS so much, we are considering it for one of our upcoming awesome project (Surprise Surprise!) and we are finding it promising enough to replace any of our existing technology stack. And if you are not so sure about the performance of NodeJS, you might want to look at these benchmarks . So what do we think is the biggest problem with Javascript? Mindset. We, developers, do not treat Javascript to be a programming language which needs as much attention and learning as any other language. But at Haptik, every one of us believes that Javascript is just like any other programming languages and you should learn it first before implementing. Javascript has come a long way and it needs its due respect. And with ES6 it is going to be more matured and with browsers becoming more powerful we can do more creative stuffs with Javascript. So if you are a javascript junkie like we are, then do drop us a note at hello@haptik.co and we would love to hear what you have to say. We hope, together we can improve the quality of Javascript coding and the mindset too 🙂 Lastly, here’s a great watch from the man himself, who has seen Javascript fall and rise: This post is written by Nabendu Karmakar, Full Stack Engineer at Haptik. Posted by Admin on Aug 2, 2016 7:19:00 PM", "date": "2016-08-2,"},
{"website": "Haptik-AI", "title": "Taking Neural Conversation Model To Production", "author": [" Krupal Modi"], "link": "https://www.haptik.ai/tech/taking-neural-conversation-model-production/", "abstract": "Tweet Just about 3 years ago, multiple applications which were primarily backed by conventional machine learning modules got on to the wave of optimism, driven by promising results of Deep Learning techniques. One such application was Machine Translation which got significant improvement with an introduction of sequence to sequence learning method . Soon after, using deep learning for chatbots became a promising area of experimentation with neural conversation model becoming a starting point for developers and researchers . With ample amount of conversational data in place and our eagerness to toy around with new technologies, Haptik , one of the largest conversational AI platform became a direct beneficiary of this research. However, it took us multiple iterations to come up with a system that worked for us in production and added significant value to the end user experience. We built a hybrid model which combined neural conversational model with graph-based dialogue system and achieved significant improvements on existing baseline systems. You can also refer our research paper, ‘Production Ready Chatbots: Generate if not Retrieve’ which was presented at AAAI-2018 , Deepdial workshop. This post primarily focuses on why we came up with a hybrid approach and how chatbots at Haptik use this approach to respond to user’s messages in a precise and intelligent way. Before we get into the hybrid system, let’s go through some basics. What are primary approaches to build Chatbots? Retrieval Approach: Retrieval based models use a repository of predefined responses and some rule-based expression match or an ensemble of Machine Learning classifiers to pick the most appropriate response. In this category, one we use a graph-based dialogue system which helps respond to almost 70% of our user’s messages. You can refer to the details of our retrieval approach in section 3 of the research paper. Generative Approach: Generative models are trained on human-to-human conversational history and build new responses from scratch. In our case, we use seq2seq model with 3 layers bidirectional GRU encoder and unidirectional decoder (again with 3 layers and GRU cells) with attention settings over encoder states. Details of this approach are available in section 4 of our research paper. You can read more about these approaches here as well. Why did Haptik need a Hybrid of Retrieval and Generative Approach? While our retrieval model responded to the majority of user messages, it failed on complex user queries that contained a lot of spelling mistakes, deviation from the domain, code mixed queries, etc. Hence, we decided to dive into experimenting with seq2seq model to harness our historical conversational data. By training around a million conversations, we quickly got a dirty prototype ready, which was a schizophrenic chatbot. This chatbot energetically answered all types of questions, supported every conversation, but at the same time displayed disorganized thinking which was not aligned with improving end-user experience during the conversation. Unfortunately, we couldn’t use it in that shape, because it did not adhere to the following prerequisites of building a successful bot: Content, tone, and personality of the bot: There was no consistency in language and grammatical constructions used by our seq2seq model; something which is not expected of a good conversational agent. Accuracy: While it responded to queries, as expected, it also tried responding to unfamiliar intents with unexpected responses. This defeated the entire purpose of responding accurately. Alignment to a specific task: Good task-specific chatbots tend to keep conversations in a narrow domain and aim to drive it towards task completion. But our model accepted open-ended queries and engaged in endless chatting. Based on the above issues, we concluded that we needed a model which could respond to complex user queries. Also, the objective of these responses should be dedicated to putting the user back on track. And hence, navigate our graph-based dialogue system to exactly where the user left it. Hence, we started engineering a hybrid system which would use a generative model in a controlled fashion. How does a Hybrid System work? The graph-based system works for near-ideal scenarios and takes care of 70% of Haptik’s chatbot conversations. We introduced neural conversation model to respond to the remaining 30% of conversations where the users tend to deviate from ideal chat-flows. We got human agents to respond in real time, with the intent to put users back on an ideal chat-flow. Interestingly, 80% of our training data comprised of these 30% conversations, while the remainder 20% was taken from rest 70% chatbot conversations. You can refer to section 4.2 of the paper for more details on our training data generation and section 5 to understand real-time working. Following is a snapshot of the real-time working of our hybrid system: How did the Hybrid System help Haptik? Haptik processes more than 5 million chatbot conversations on monthly basis. Here’s a list of the advantages of a hybrid model and the snapshots from reminders domain: We could respond to complex queries which were not handled by our graph dialogue system: The Hybrid System catered to hinglish (code mixed data generated by mixing Hindi and English) queries and also catered to outliers: It handled spelling errors, slangs and other chat lingos used on chat by Indian users: One of the biggest advantages of being able to plug in seq2seq model was that the performance of our chatbot system was directly proportional to data. Just like every other system, our model has its own limitations which are mentioned along with results and analysis of our system in section 6 of the research paper. Based on our experience of iterating over and again to achieve a result oriented hybrid model, here is the list of few key checkpoints we would like other developers to consider while building a system which includes usage of the generative model. Clearly defined goals – As a developer and a deep learning enthusiast, it’s always fun to build generative models because sometimes the model’s behaviour is amazingly cool and brings instant gratification. But needless to say, it is important to narrow down the goal for a generative system so that you can leverage your data efficiently. At Haptik , our primary aim while building a generative system was to keep check when conversations deviate from ideal flows. Choice of domain – Domain defines the vocabulary which a model will need to understand. And always the lesser, the better. For example – In case you want to build a model for travel ticket booking, you should ideally prefer to train a separate model for flight bookings, train bookings, hotel bookings and cab bookings. Amount of Data – A ground reality of such generative models is that they need lots of data points to learn. Training data generation is one of the most critical tasks while building any neural conversation model. Getting the considerable amount of Data is difficult but a mandatory step to get measurable results. Standardisation of Data – Most chatbots are content heavy and messages contain a mix of static and dynamic data. Therefore, metadata throughout the system needs to be logged properly and critical entities should be tagged appropriately at the source. If not done right, one can easily end up in a situation where historical data is not reusable or it needs disproportionate time to clean it for feeding into a generative model. We hope that this blog helps you when you decide to plug in a generative model in your dialogue systems. We keep writing about more use-cases of generative models as we plug them on Haptik as we deploy them on Haptik Here’s a good reading list to begin with for more research on the same lines: Long Short-Term Memory networks(LSTM) Sequence to Sequence Learning with Neural Networks A Neural conversation model A Persona-Based Neural Conversation Model Production Ready Chatbots: Generate if not Retrieve The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems (2015-06) Think we did a good job? Let us know in the comments below. Also, Haptik is hiring . Visit our careers section or get in touch with us at hello@haptik.ai . Posted by Krupal Modi on Feb 27, 2018 6:42:00 PM Find me on: LinkedIn", "date": "2018-02-27,"},
{"website": "Haptik-AI", "title": "Botathon – An Overview", "author": [" Admin"], "link": "https://www.haptik.ai/tech/botathon-an-overview/", "abstract": "Tweet We’ve just come home after hosting the nation’s very first Botathon, a hackathon for building conversational bots, in Bengaluru. A 2-day long event with 43 teams full of talented individuals competing to showcase their incredibly creative ideas of using a chat interface to empower their users – is exactly what we had wanted & expected and the Botathon didn’t disappoint! Heading into the Botathon, we had received an overwhelming response for the event with 400+ applications for participation from over 10 cities across the country – with some truly amazing ideas ranging from an educational bot that will help students go through their engineering course material to a bot which empowers farmers by giving them access to various irrigation techniques basis recent soil tests in the region and reports by the government while giving them real time weather updates etc. As excited as we were to be hosting such a phenomenal group of developers looking to change the way we go through our daily lives in every which way possible, what excited us even further was the participation of a certain 72-year-old gentleman – who stayed in and coded till the very end of the 2-day event! We are truly honoured to have been able to host someone as dedicated as him at this event. On-the-ground at Botathon On the first day of the event – within the first two hours of starting registrations, we already had a spectacular turnout and a full house considering it was early in the morning on a Saturday. People were coming in from all over the country – Hyderabad, Jaipur, Delhi, Mumbai, Chennai, etc. The energy in the house was absolutely amazing! People from all walks of life – some working at big companies like Microsoft and IBM to early stage tech entrepreneurs to those working in start-ups – we had a spectacular pool of talent coming in for the Botathon. Of the 43 teams that got selected for the event, we had a lot of two-three people teams and some lone rangers who coded their way till the very end of Sunday. There were some truly amazing ideas looking to shake up nearly every industry there is. Some of the ideas are – Educational Industry: A Bot to help you study engineering, A bot to give teenagers career advice and a bot to help you take notes during class Agricultural Industry: A Bot to help educate farmers, help them book soil tests and based on the samples, get reports from government officials Entertainment industry: A Waiter bot (Brotender) that can make your restaurant experience a lot better, a bot to crowdsource the DJing on slack. Health and Nutrition: A bot to help users keep track of calories and help them with their weight goals, A bot to be your personal trainer and motivate you to go to the gym. Home Automation Industry : A bot that can control the electronics at your home with a simple message Tool bots: A bot to QA other bots, A bot to replace DevOps for you. And these are only a few of the fantastic ideas that we’ve received. Post the initial set up, we had great team of mentors going around guiding all the teams in the right direction and helping them focus on the core of the problem. Keeping the energy alive all weekend long, most of the people stayed overnight to hack out their bots. In fact, this stunning display of dedication and focus from the participants had everyone, including our prolific panel of judges, startled with the sheer amount of work they had gotten done in just 24 hours under a single roof! On Sunday afternoon – out of the 43 teams, 10 were shortlisted by the mentors to demo their products in front of the panel of judges. After receiving some fascinating demos from the top 10 teams – the panel after long deliberation finally announced the winners of the Botathon: CStrikers – Winners The duo of Swarndeep S Arora and Anshuman Dhamoon took the top prize with their bot that helps its users keep a track of their fitness goals and motivates them to achieve them by analysing their daily calorie intake and suggesting appropriate diet and workout regime. BroCorp – First Runner-up Rohin Gopalakrishnan & Arvind Kumar won the hearts of the judges and everyone around them with their focus on customer experience. Their Facebook Messenger bot – BroTender aims to automate and simplify customer interaction at bars and restaurants without the need for the long waits for waiters and bartenders to take the orders. A user can ask the bot for the menu or have it recommend one based on personalized taste in a restaurant. The bot also keeps a track of the user’s favorite meal from a previous order, splits payments with friends and will pay off the bill from the wallet integrated within. Sven – Second Runner-up Ganessh Kumar R P & Arvind Thangamani grabbed the 3 rd place at the Botathon with their incredibly simple idea for a slack bot(DJ) for the team, using which any team member can request for the song and it plays it for the team. As requests keep coming in, it queues them. the bot also accepts controls like ‘next’, ‘pause’, ‘clear playlist’ to operate on the playlist. Using machine learning to further enhance the experience, the bot also suggests songs to the team based on the songs played thus far. We were pleasantly surprised to see that the energy and the enthusiasm amongst the participants hadn’t downed even a little after 30 hours of hacking! We ended the event with a networking session on Sunday evening with some beer & snacks amongst the participants, judges and mentors. A note from our CTO & Keynote Speaker at the event – Swapan Rajdev: As the designated in-house mentor, I had the opportunity of not just listening to the top three ideas as decided by the judges, but also of going around individually talking to each of the participants at the event. I had the best of time talking to these extremely talented folks and their dedication towards the cause has given me a sense of hope and pride for the industry I’ve dedicated my life to. I sincerely hope that the participants keep pursing their bot ideas as a lot of them are very useful in the real world. Even though the event is over, I would still love to hear from the people at the botathon about how they are building bots and I am happy to help in any way possible. Posted by Admin on Dec 13, 2016 6:06:00 PM", "date": "2016-12-13,"},
{"website": "Haptik-AI", "title": "Open Source Entity Recognition for Indian Languages (NER)", "author": [" Aman Srivastava"], "link": "https://www.haptik.ai/tech/open-source-entity-recognition-for-indian-languages/", "abstract": "Tweet One of the key components of most successful NLP applications is the Named Entity Recognition (NER) module which accurately identifies the entities in text such as date, time, location, quantities, names and product specifications. There are already existing sophisticated systems for NER such as spaCy , Stanford NER , etc. but most of them are built with general purpose for a wide range of NLP applications such as Information Retrieval, Document classification and other applications of unstructured data analysis. At Haptik, we focus on continuously improving NLP capabilities of our conversational AI platform, which powers more than few million exchanges on a daily basis. These conversations are spread across hundreds of enterprise bots built for different use-cases such as customer support, e-commerce, etc. Hence, building an accurate and reliable NER system tailored for conversational AI has always been one of the key focus areas of the engineering team at Haptik. Around 3 years ago we open-sourced one of our key frameworks, Chatbot NER , which is custom built to support entity recognition in text messages. You can read more about it here . After doing thorough research on existing Named Entity Recognition (NER) systems, we felt the strong need for building a framework which can support entity recognition for Indian languages. This led us to upgrade our own NER module i.e Chatbot NER to V2 version to scale its functionalities in local languages. The primary focus of this blog is to help you get started with using basic capabilities of Chatbot NER for English and 5 other Indian languages and their code mixed form. In version 1, we had provided support for following entity types: Numeral : Entities that deal with the numeral or numbers such as temperature, budgets, size, quantities etc. Pattern : Entities which use patterns or regular expressions such as email, phone_number, PNR. Emporal : Entities for detecting time and date. Textual : Entities detection by looking at the dictionary or sentence structure. This detection mainly contains detection of entities like cuisine, dish, restaurants, city, location, etc. In version 2 we have extended support for all above entity types (except pattern entities as it is language independent) in the following five Indian languages: Hindi Marathi Gujarati Tamil Bengali Selection of the above languages was based on the availability of linguistic experts in Indian languages who helped us in curating training data to scale entities. Installation steps We have a Docker-based set up for the Chatbot NER module, which can be set up on your system in less than 5 minutes by just following the installation steps given here . API Examples Below are the Django shell examples of Hindi date detections in Devanagari and Latin script: 1. Detection in Devanagari script: >> from ner_v2 . detector . temporal . date . date_detection import DateDetector >> detector = DateDetector ( entity_name = 'date' , language = 'hi' ) # here language will be ISO 639-1 code >> detector . detect_entity ( text = 'अगले मंगलवार को मेरा बर्थडे है ' ) >> { 'entity_value' : [ { 'dd' : 12 , 'mm' : 10 , 'yy' : 2018 } ] , 'original_text' : [ 'अगले मंगलवार' ] } 2. Detection from Latin script: >> from ner_v2 . detector . temporal . date . date_detection import DateDetector >> detector = DateDetector ( entity_name = 'date' , language = 'hi' ) # here language will be ISO 639-1 code >> detector . detect_entity ( text = 'agla mangalvar ko mera birthday hai' ) >> { 'entity_value' : [ { 'dd' : 12 , 'mm' : 10 , 'yy' : 2018 } ] , 'original_text' : [ 'agla mangalvar' ] } You can follow the structure given below to make a curl request for the above example: >> curl ' http://localhost:8081/v2/date?message=अगले मंगलवार को मेरा बर्थडे है&source_language=hi' What’s Next? Our team is actively working and we will extend support for more Indian languages within the next few months as mentioned in our repository milestone. We also plan to add batch processing, more optimisations and better models going forward. We hope that Chatbot NER helps you add more utilities to your Bot and ease out the process of detecting entities. Do share your feedback so that we can improve the same. Also, if you wish to understand the details of the framework and contribute either data or code to it, then refer to our contribution guidelines . Let’s hope that this repository comes up as a powerful resource and contributes to the research and engineering community. For, any questions or support related to this repository, do leave your comments below. Also, don’t forget, we are actively hiring researchers and engineers. So, if you are interested, check out our careers page . Posted by Aman Srivastava on Mar 20, 2019 6:54:00 PM", "date": "2019-03-20,"},
{"website": "Haptik-AI", "title": "Sneak Peek Into Haptik’s Engineering Culture", "author": [" Saumil Shah"], "link": "https://www.haptik.ai/tech/sneak-peek-haptik-engineering-culture", "abstract": "Tweet I joined Haptik as the VP of Engineering a couple of months ago. The Engineering team was around 50 people at the time. Over the past few months, as I ramped up and worked closely with each team, I have been really impressed by the engineering culture ingrained within each and every engineer – the passion of solving tough problems, the mentality of helping and supporting each other and the unique set of values and personality centered on clear goals. In the past, I have worked with multiple companies in multiple roles. Every company has a different definition of culture. I have seen that culture isn’t based on any common background, a comfort or familiarity between individuals or the shared enjoyment of company perks as ping pong or craft beer. When an employee fits into the company culture, it allows them to enjoy their time, and be more productive. It gives people a framework on how to respond to the challenges facing their teams and helps them with the decisions that each person will take and own up to every day. It plays a role in our work, our meetings, and even social interactions with our colleagues. It is such that each employee: 1. Shares enthusiasm about our company’s mission and purpose. 2. Works together towards a common goal. 3. Creates a mutual understanding of how to make decisions and assess risk. 4. Celebrates each other’s success and supports each other during tough and challenging times. Haptik’s culture is built on 3 core pillars – ownership, honesty, and perseverance . “ Haptik is a workplace where even the CTO sits with interns and interns have full freedom to push code to production. Everyone is given an equal opportunity to say what they want. “ In this blog, I will talk about some aspects of Haptik’s culture which we continuously seek to emphasize and reinforce across our engineering department. We empower engineers to lead from the front and make decisions Being an engineer in a relatively young company like Haptik is about navigating through tough unknown problems. Engineers at Haptik are hungry to contribute to fixing challenging problems without caring about how dirty the laundry is. There is a lot of freedom given to each engineer for out of the box thinking. We lay a lot of emphasis on hiring people who are good at what they do, and then just let them do it. A case in point – a few of our backend systems were running on Python2 and we needed to make a move to Python3, since Python2 was getting obsolete and Python 3 offered a lot of bug fixes, better performance, and security. In spite of some of our code being a monolith, the engineer tasked with this effort did some research upfront and delivered the project as per the timeline set without any downtime and optimum testing. In a blog post on How Haptik Carried Out Their Largest Python3 Migration , the engineer has described how he came up with a multi-phase approach to solve this problem and successfully completed it . At Haptik, we have complete faith in the talents of our employees and empower them to take important decisions. This extends not just to full-time employees, but to interns as well! In fact, one of our former interns in the Machine Learning team has written about how we empowered him to learn, grow, and work on impactful projects, in this blog – 3 Haptik Internship Stories – Working with a Startup! . Quoting one of the lines by the intern in the machine learning team from the above blog – “ I never imagined that I could build something like this and learn so much in such a short time. ” We are always learning, flexible, and self-adjusting Engineers at Haptik are always excited about making an impact. They are constantly trying to innovate and try new things. Self-learning is immensely valued at Haptik. It provides them with accelerated learning and development in some of the crucial areas of tech. We also follow DevOps practices organization-wide. Managers at Haptik are continuously encouraged to work with their teams on cross-skilling and up-leveling engineers on newer technologies. We have our mobile SDK development team, that on a number of occasions, was dependent on our backend teams to make corresponding application-level changes. With the pace of their development, the team could foresee this becoming a frequent requirement and took it upon themselves to cross-train on backend technologies. By doing so, they are able to ship code faster by being fully independent and shipping code end to end themselves. This immensely improved their delivery and productivity. We had an iOS engineer who later transitioned to a backend engineer. We’ve also had a team-member who started as a developer but later went on to become a program manager – eventually transitioning to business development! We nurture the right talent and provide a platform that will be the most effective for each individual. As a company, we want to be leaders in the development and deployment of a wide range of conversational AI solutions and we are constantly innovating using our future-ready tech stack, powered by Machine Learning (ML) and Artificial Intelligence (AI). We had already built significant expertise when it came to text-based virtual assistants and wanted to develop a Conversational IVR (Interactive Voice Response) solution. The engineering team was tasked with building an IVA served over voice.t They faced quite a few roadblocks but showed tremendous perseverance and continue working on it. They tried out varied approaches and continuously improvised to come up with a successful working solution. We enjoy working together as one towards a common goal without Silos For a young company (Yes! We feel we have a long way to go and think of ourselves as a young company) in blitzscaling mode, collaboration plays a very important role in the success of the company. If the team is able to work comfortably with each other the quality of the product enhances considerably and the overhead to deliver quality products reduces drastically.  The focus on collaboration has been naturally reinforced by the work we do every day and we also leverage tools that help reduce the communication and collaboration gap such as Slack and all its plugins, Jira for task management, google drive for documents and presentations collaborations and many more. Sometimes, when the teams are working on a critical project, we set up a “ war-room ”. The idea of a war room is to physically gather the entire team into a ‘single location’ and work collaboratively. This helps to facilitate communication, problem-solving and status reporting. It increases team commitment and feeling of togetherness along with more responsibility and accountability. The walls of such rooms are covered with sticky notes, photos, wireframes, and user interface designs for easy collaboration. During our hiring, in addition to all the above cultural aspects, we also put equal weight on candidates who will be great team-players in the long-term. We evaluate such criteria by asking tough questions about situations the candidate has been in and how they handled such situations. Our ability to deliver on our commitments and solve newer technical challenges has been closely tied to how well we can collaborate, while simultaneously giving each person the space to learn and grow. We promote transparency & communication throughout the organization Communication is key at Haptik. We listen to each other with the aim of learning and working towards better scalable solutions. We value and share candid feedback and make a conscious effort to get better and grow as a business. When we communicate well with each other, we learn from our mistakes and iterate constantly. All employees at Haptik value the opinions and feedback of their peers, which helps them have a long-lasting measurable impact within and outside the organization. We encourage engineers to take ownership and hold them accountable for their work. Our open and transparent work environment allows them to express ideas and raise alerts fearlessly. Each manager schedules weekly or bi-weekly 1:1s with every individual within their teams. This meeting gives an opportunity for individuals to discuss any issues and concerns on their minds with their managers. Regular check-ins help stop larger issues from festering, allows for regular feedback and promote open communication. For the manager, these check-ins help to connect with their team on pressing issues, develop a strong relationship, and ensure that employees feel like they’re working toward their goals. We have a monthly all-hands meeting to drive transparency and alignment within various teams. In these meetings, we share business updates of the past month, drive alignment around company mission and strategy and give everyone a chance to ask questions. We also spend some time reviewing our key metrics, sales, retention, amongst others. An open Q&A with leadership helps each employee uncover the most burning issues and foster transparency. We follow an agile scrum process and have 2 weeks of sprints. At the end of each sprint, the teams schedule a sprint retrospective meeting to discuss the just-concluded sprint. During this time, the team self-identifies elements of the sprint that did or did not work during the sprint, along with potential solutions. In this meeting, each engineer is given a chance to talk about things that went for him/her, things that didn’t go as desired or planned and potential self-improvements. Managers constantly encourage each team member to be candid when communicating in these retrospectives. Talking about the success and failure of the sprint is not only the responsibility of a manager but of everyone in the team. As a part of our interview process, we closely examine candidates on being a “cultural fit” and each interviewer will assess these skills in addition to the technical abilities and ambitions of each candidate. To sum it up … The world is moving incredibly fast, and we can’t let anything slow us down. At Haptik, we foster an environment where each and every engineer feels they are treated with respect and equality. Our culture has empowered us to build a strong close-knit team who are there to support each other in success and while facing the toughest of challenges. We don’t pretend to be perfect and realize we still have a lot of work to do. We are still growing,  and the practices that we set up now will shape us as a team for years to come. As we scale up, we are mindful of the fact that we will need to reinforce these practices to an even greater degree. But, these values have helped our engineers do their best work and get them excited to come to work every day. Keeping this in mind, we feel we are definitely pointed in the right direction. We are hiring. Do visit our careers page. Posted by Saumil Shah on Jan 10, 2020 7:40:00 PM", "date": "2020-01-10,"},
{"website": "Haptik-AI", "title": "Internship Experience Of An iOS Engineer", "author": [" Admin"], "link": "https://www.haptik.ai/tech/internship-experience-ios-engineer", "abstract": "Tweet As an engineering student, a challenging internship is really important so as to get real-world experience & luckily I got a chance to work at Haptik. In this article, I’ll be sharing my internship experience at Haptik and why it is one of the best places to be an intern. I joined Haptik as an iOS Engineer Intern and worked in the iOS SDK team. Before joining Haptik, I had worked on a few projects, which helped me learn some necessary technologies but they were mostly personal projects without any deadlines or maintenance. And, it just couldn’t get better; I was assigned to a live project at Haptik. I’ll try to cover my learnings and the change in my thinking process during & post the internship in this post. On-Boarding One great experience in the first few days at Haptik was the on-boarding process. From my reporting manager to each and every member of the team helped me get on-boarded. From going through the Haptik’s Tech Jargons, the Git structure that is followed at Haptik, to the existing codebase of the iOS App and SDK. Code Maintainability While working on course assignments you had detailed requirements about what you needed to make, your sole objective was that the code you wrote, worked. You didn’t need to look back on it or explain it to anyone. All they needed to know was that it worked. But this isn’t the case when writing code professionally, changing requirements means a change in code, which could mean changing code that one wrote yesterday or years ago and finally affecting user experience. One thing I learned is that one needs to write code that can be understood by itself without someone needing to explain it & also that it is flexible enough to change behavior without breaking the system. Also, it should be easy to make changes and maintain the codebase down the line without much effort. There are a few things that can help with that: Design Patterns – When working with a massive codebase you need to write code that can be changed with time and is abstract enough that the developer updating/modifying it can work independently without knowing about the entire code base. That’s where the design patterns come in, design patterns help keep the code organized, modular and abstract enough. Design patterns break down functionality into components, splitting the implementation. These, then add abstractions as the various components don’t need to depend on the implementation of others. Since design patterns are common, any future developer trying to change anything will know where to find it and can make the necessary changes knowing that he is not breaking anything else. Documentation – You’ve already heard this everywhere, and there is possibly nothing that I can say about writing readable code that you haven’t already heard. In my experience documentation goes beyond well-written code, since developers are almost always too busy to read the entire code to understand what a function does. You need to document each & every function/variable & class, it reinforces abstraction as other functionalities are built without considering the implementation of your class. A descriptive git commit message is just as important. Make sure your Git commit message is clear and precise. This is better explained here. Unit Testing – How do you as a developer, make sure that the functionality you wrote is resilient to changes and keeps functioning as it was intended? Unit tests are the answer. It can be used to test that the code you have written behaves as expected, and help to detect any bugs in the initial phases of development before integration. It can be very costly to remove a bug if discovered late in the development stage. A set of well-planned unit tests can find the basic bugs (which may have a potentially huge impact on the system later) during the development. Haptik follows and abides by unit testing on a day by day basis and as a developer, it is a really good experience to design, think & write unit test cases for each and every line of code; 100% code coverage. Planning your work Since we are now working with deadlines, you have to be able to estimate how much time/effort a task would take. While distinguishing between hard and easy task might seem easy, it’s not that straightforward when estimating a task down to the hour. While estimating a task might help you plan it, it’s more about how estimating a task changes the way you do it. Wrong estimates might make you skip a few things and edge cases that you might have taken care of if given more time. Estimating a task requires you to plan out the details and visualize the complete picture before you start to work on it. This allows you to see all the pros and cons of an approach before you commit to it. Haptik has helped change my initial mindset to find possible solutions for the bug/task, and estimate them based on the time required considering scalability before choosing a solution. Personally, this has stopped my initial code-as-you-go approach, & now I make sure to scope out the necessary changes beforehand. This gives me a complete idea of what I should be doing before starting, thus I know the functionality that I will be adding, the number of classes that this functionality would be split into and how these classes would be interacting and what functions/ classes should/can be tested. What initially seemed like time wasted on planning the changes that “ you were gonna do anyway “ , suddenly started making sense. This led to my writing code that was more suitable for iteration, testing and more understandable. UI/ UX Yes, as an iOS Engineer, you’re given a chance to work on the UI/UX side of things as well. We have people using our software, so we need to keep in mind the user interface and end-user experience as a developer too. While one may argue that why should a Software engineer care about the UI, isn’t that the job of UI designer, software engineering is about creating something useful that people can use but if the user decides not to try out some of your product’s features because it does not work the way they want, you have basically wasted time in implementation. While having a UI designer removes most of the guesswork, you’ll often be required to make decisions on the UI. Since you are the first person to see it in action, you need to verify that the UI/UX is good, responsive and easy-to-use. Few other things I learned during the internship are: Work-life Balance: More isn’t necessarily more. If you’ve worked too many hours and are no longer efficient, recognize that and take a break. It is a marathon rather than a sprint, make sure to not exhaust yourself. There is more than enough to clear your mind at Haptik with great colleagues and regular activities. Learn to unblock yourself: There are going to be times when things are not working as you expect them to, learn to use the resources available to you, figure out the underlying implementation, and discuss with your colleagues. It is not a good idea to spend time figuring out a solution that could be solved with a mere discussion. Move fast/discuss approaches: Submit code early and submit code often. Do not wait until you’ve completed a feature to get feedback on it. Submitting/sharing your code early will give you time to work on any changes requested and prevent unnecessary effort and delays. Even better is discussing your approach before starting to work on it. Question existing code: While working with existing implementations, you are bound to come across suboptimal code. If there is anything that can be changed to make it better or any code that does not make sense, be sure to improve it. just cause it was reviewed and approved doesn’t mean that it is perfect. I got a chance to refactor a lot of old code as well. You maintain the health of your codebase via such improvements over time. Conclusion I hope all this gives you an idea of what my internship at Haptik was like. I was treated as a regular employee and everyone in the Engineering department was warm and welcoming. Working at Haptik as an intern has made me learn a lot of things and this will hopefully help me grow in my career. Visit Haptik’s career page for job opportunities . Posted by Admin on Mar 8, 2020 7:41:00 PM", "date": "2020-03-8,"},
{"website": "Haptik-AI", "title": "A Conversation with Calayer – Meeting I", "author": [" Vinay Jain"], "link": "https://www.haptik.ai/tech/a-conversation-with-calayer-meeting-i/", "abstract": "Tweet If you’ve been programming for iOS devices, you might have encountered these lines of code: view . backgroundColor = [ UIColor greenColor ] ; view . layer . cornerRadius = 8.0 ; view . layer . borderWidth = 1.0 ; view . layer . borderColor = [ UIColor blackColor ] ; If you haven’t seen code like this before try using it with any UI component in your app and check what it does. It adds a 1pt black border to the view and round its corners by 8pt. Every UIView is backed with a layer which can be accessed with the view.layer . The layer property points to an instance of either the CALayer class or any of its subclasses like CAShapeLayer , CAGradientLayer , CATextLayer etc. Layers are part of the Core Animation library and are extensively used in custom drawing with Core Graphics and Core Animation , these two frameworks make iOS apps beautiful. The image below shows where these two frameworks lie in the iOS graphics drawing engine: OpenGL , a low-level API which interacts directly with the GPU, does all the heavy lifting work of graphics processing. To make our lives even easier Apple has built Core Animation , a high-level wrapper above OpenGL so that we don’t need to write the low-level C code. Now, since we have been introduced to CALayer and have access to the low-level graphics rendering engine lets start interacting with them and create some beautiful graphics. There are various CALayer subclasses, below are the direct CALayer subclasses available in the Core Animation library: In this part of the series we will build: 1. A loading indicator with CAShapeLayer and CAKeyFrameAnimation 2. A mirror reflection of the Haptik logo with CAReplicatorLayer 3. A colorful Haptik logo with CATextLayer CAShapeLayer With CAShapeLayer we can easily draw curved paths and geometrical shapes. CAShapeLayer is mostly used in drawing custom UI components. At Haptik , we design most of the custom UIButton subclasses with CAShapeLayer . But why would you write code to draw a circle or a triangle or anything complex when you have images? There are a few reasons to write code: Almost all properties of shape layers are animatable , which gives us the freedom to  change these shapes with code at runtime, which we’ll learn how to do. CAShapeLayer is vector based, thus they are resolution independent. This can be drawn directly using the GPU so we can free the CPU for other tasks. By the end of this section, we’ll be able to show this cool animation on the screen: The animation above has three components: 1. A rounded rectangle added on the view layer 2. A gray color circle added on the rectangle layer 3. A dark gray color arc added on the circle layer The below code draws the rounded rectangle on the screen: CAShapeLayer * roundedRect = [ CAShapeLayer layer ] ; roundedRect . path = [ UIBezierPath bezierPathWithRoundedRect : CGRectMake ( 0 , 0 , 120 , 120 ) cornerRadius : 8.0 ] . CGPath ; roundedRect . fillColor = [ [ UIColor whiteColor ] colorWithAlphaComponent : 0.5 ] . CGColor ; [ self . view . layer addSublayer : roundedRect ] ; To create the path of the shape layer we used a UIKit class UIBezierPath to skip the complexity of drawing paths with Core Graphics . The fillColor property of the shape layer fills the closed region of the layer with given color. Next, we add a circle to the rounded rectangle: CAShapeLayer *circle = [CAShapeLayer layer]; circle.path = [UIBezierPath bezierPathWithArcCenter:CGPointMake(0, 0) radius:50 startAngle:0.0*(M_PI/180.0)  endAngle:360.0*(M_PI/180.0) clockwise:YES].CGPath; circle.lineWidth = 5.0; circle.fillColor = [UIColor clearColor].CGColor; circle.strokeColor = [UIColor lightGrayColor].CGColor; circle.backgroundColor = [UIColor clearColor].CGColor; circle.position = CGPointMake(60, 60); [roundedRect addSublayer:circle]; For drawing a circle we need to pass a startAngle and an endAngle . With these angles, we tell the system from where the path should start and where it should be drawn till. If we were drawing this circle with a pen, consider the strokeColor as the ink color and the lineWidth as the minimum width of the line that can be drawn with the pen. Changing the position of the layer centers it in the rectangle. To add the arc we will again use the same function for drawing a circle, but we will pass different start angles and end angles to draw it as an arc: CAShapeLayer * arc = [ CAShapeLayer layer ] ; arc . path = [ UIBezierPath bezierPathWithArcCenter : CGPointMake ( 0 , 0 ) radius : 50 startAngle : 180.0 * ( M_PI / 180.0 ) endAngle : 225.0 * ( M_PI / 180.0 ) clockwise : YES ] . CGPath ; arc . lineWidth = 5.0 ; arc . lineCap = kCALineCapRound ; arc . fillColor = [ UIColor clearColor ] . CGColor ; arc . strokeColor = [ UIColor darkGrayColor ] . CGColor ; arc . backgroundColor = [ UIColor clearColor ] . CGColor ; The lineCap determines how the endpoints of the drawn curve are stroked. To create a rotational animation in x-y plane we need to change the rotation transform along the z-axis and fortunately, we can easily do this with Core Animation . CAKeyframeAnimation * animation = [ CAKeyframeAnimation animationWithKeyPath : @ \"transform.rotation.z\" ] ; animation . additive = YES ; animation . duration = 10.0 ; animation . repeatCount = HUGE_VALF ; animation . values = @ [ [ NSNumber numberWithFloat : 0.0 * M_PI ] , [ NSNumber numberWithFloat : 1.75 * M_PI ] , [ NSNumber numberWithFloat : - 0.75 * M_PI ] , [ NSNumber numberWithFloat : 2.75 * M_PI ] , [ NSNumber numberWithFloat : 0.0 * M_PI ] ] ; animation . keyTimes = @ [ @ 0 , @ ( 2 / 6.0 ) , @ ( 3 / 6.0 ) , @ ( 5 / 6.0 ) , @ 1 ] ; animation . timingFunctions = @ [ [ CAMediaTimingFunction functionWithName : kCAMediaTimingFunctionEaseIn ] , [ CAMediaTimingFunction functionWithName : kCAMediaTimingFunctionEaseInEaseOut ] , [ CAMediaTimingFunction functionWithName : kCAMediaTimingFunctionEaseInEaseOut ] , [ CAMediaTimingFunction functionWithName : kCAMediaTimingFunctionEaseOut ] ] ; [ arc addAnimation : animation forKey : @ \"rotate\" ] ; With CAKeyFrameAnimation we can control the animation attributes like fromValue and toValue , timingFunction , calculationMode for different time intervals in the complete animation. The values array determines fromValue and toValue of the animatable property ( transform.rotation.z ) in the time intervals given to the keyTimes array. The timing functions decide how the animations start and end. You can find the complete code till this section on this CALayers-GitHub repo. CAReplicatorLayer CAReplicatorLayer is a container layer, it replicates the content added to it. It has some cool properties which can be used to instruct the container how the replication has to be done. Beautiful effects can be achieved by applying animations to the replicated content. Every contained content is called an instance . To show the usage of this layer we will create a reflection of an image. By the end of this section, we’ll be able to show the reflection of the Haptik logo like this: Let’s build this! First, we need a CAReplicatorLayer instance and on this instance, we’ll be adding an image layer of which the reflection we will be showing: // Create a CAReplicatorLayer CAReplicatorLayer * replicatorLayer = [ CAReplicatorLayer layer ] ; // Create the image layer UIImage * image = [ UIImage imageNamed : @ \"haptik_logo\" ] ; CALayer * imageLayer = [ CALayer layer ] ; imageLayer . contents = ( __bridge id ) [ image CGImage ] ; imageLayer . bounds = CGRectMake ( 0.0 , 0.0 , [ image size ] . width , [ image size ] . height ) ; imageLayer . anchorPoint = CGPointMake ( 0 , 0 ) ; // Set bounds of replicator layer // to height twice of image height replicatorLayer . bounds = CGRectMake ( 0.0 , 0.0 , [ image size ] . width , [ image size ] . height * 2 ) ; replicatorLayer . masksToBounds = YES ; replicatorLayer . anchorPoint = CGPointMake ( 0.5 , 0.0 ) ; replicatorLayer . position = CGPointMake ( self . view . frame . size . width / 2.0 , 80.0 ) ; [ replicatorLayer addSublayer : imageLayer ] ; This code is pretty straight forward, the anchorPoint of a layer is the point from where all the geometric manipulations will happen. The default anchorPoint is (0.5, 0.5) which represents the center of the layer. We want to apply a rotation from the top of the layer, so we changed it to (0,0) . With the above code, we have added an image to the replicator layer and set its correct bounds. To get the reflection we need to apply a rotation transform and translate the replicated layer to the correct position as below: CATransform3D transform = CATransform3DIdentity ; transform = CATransform3DScale ( transform , 1.0 , - 1.0 , 1.0 ) ; transform = CATransform3DTranslate ( transform , 0.0 , - [ image size ] . height * 2.0 , 1.0 ) ; replicatorLayer . instanceTransform = transform ; replicatorLayer . instanceCount = 2 ; The instanceTransform property of the replicator layer allows us to set the calculated transform on the replicated content. There are other properties of the replicator layer like instanceDelay , instanceColor which can be manipulated to get more control. Setting the instanceCount to 2 instructs the replicator layer to create exactly two instances of the added content. This is it! Running this code will give us the below output: But this is not what you expected, yes because the mirror we used earlier was blurred and so was the reflection. But if that is what you also need then add a gradient layer to your layer as shown below: CAGradientLayer * gradientLayer = [ CAGradientLayer layer ] ; gradientLayer . colors = @ [ ( __bridge id ) [ [ [ UIColor whiteColor ] colorWithAlphaComponent : 0.25 ] CGColor ] , ( __bridge id ) [ [ UIColor whiteColor ] CGColor ] ] ; gradientLayer . bounds = CGRectMake ( 0.0 , 0.0 , replicatorLayer . frame . size . width , [ image size ] . height + 1.0 ) ; gradientLayer . position = CGPointMake ( replicatorLayer . position . x , replicatorLayer . position . y + [ image size ] . height * 1.5 ) ; At Haptik , we have used the CAReplicatorLayer to create a new typing indicator. This is how it looks! If you want to download and run this code check the Github repo . And yes, Craig Federighi was online. 😉 CATextLayer Text layers are used to layout and render plain and attributed strings, but we do this usually with UILabel . One amazing usage of CATextLayer is to mask UIView . In this section we will redesign the Haptik logo as in the image below: We create a UIImageView with a pattern image and mask that pattern with the text layer: // Create the imageView UIImage * haptikLogo = [ UIImage imageNamed : @ \"Artboard\" ] ; UIImageView * imageView = [ [ UIImageView alloc ] initWithFrame : CGRectMake ( 0 , 0 , 300 , 300 ) ] ; imageView . image = haptikLogo ; // Create the CATextLayer instance. CATextLayer * textLayer = [ CATextLayer layer ] ; textLayer . frame = imageView . bounds ; textLayer . rasterizationScale = [ UIScreen mainScreen ] . scale ; textLayer . contentsScale = [ UIScreen mainScreen ] . scale ; Never forget to set the rasterizationScale and contentsScale , without these properties you might get blurry or smaller text depending on the screen resolution of the devices your app runs on. Set whatever string you want to display as a mask with the desired font: textLayer . fontSize = 100.0 ; textLayer . font = ( __bridge CFTypeRef _Nullable ) ( [ UIFont systemFontOfSize : 100 ] ) ; textLayer . string = @ \"haptik\" ; Finally, use the text layer as the mask on the image view and we are done: imageView.layer.mask = textLayer; Build and run the app and see how it looks like. The app source code till this section can be downloaded from CALayer-Github repo. Looking forward to hearing from you all. Also, don’t forget, we are hiring high-quality engineers. So if you are interested reach out to us at hello@haptik.ai . Posted by Vinay Jain on Feb 26, 2017 7:16:00 PM Find me on: Facebook LinkedIn", "date": "2017-02-26,"},
{"website": "Haptik-AI", "title": "How To Make Your Android App Work With The Ok Google Command", "author": [" Raveesh Bhalla"], "link": "https://www.haptik.ai/tech/android-app-work-with-ok-google-command", "abstract": "Tweet I am a big believer in speech interfaces, and probably use them significantly more than the average person thanks to owning an Android Wear device. In fact, I’ve often tried to plug voice to text into any app I design. For this reason, I was absolutely delighted last September when Google announced the ability to bridge the “OK Google” command to your own Android app. I had a demo up and running in barely a few hours, and we released it soon after. I was dead certain that this would be something everyone would want to add to their own app’s experience. I mean, it’s magical. Have a look for yourself with this video we made back then. Yet strangely enough I haven’t yet come across any other app that really makes use of it. And like most design patterns, the lack of usage means that this isn’t something users have taken to either. But what is truly the reason for the lack of uptake? Turns out, it’s purely a case of developers and PMs being unaware of it entirely, or of how simple it is to add in the first place. I’m going to show you how you can add this to your app in pretty much five minutes (as long as you already have a search-like experience somewhere in your app). Firstly, a primer on how this works. When a user says, “OK Google, search for <phrase> on <app name>”, Google first checks if there is an app called app name installed which has declared itself to be capable of handling such queries. If there is, the app is launched and the phrase is passed to it. And all this is managed with just a few lines of code in your manifest file and your search activity, as shown in the gist below. <activity android:name=\".SearchActivity\"> <intent-filter> <action android:name=\"com.google.android.gms.actions.SEARCH_ACTION\" /> <category android:name=\"android.intent.category.DEFAULT\" /> </intent-filter> </activity> /** * Somewhere in your activity */ String query = \"\"; if (getIntent().getAction() != null && getIntent().getAction().equals(\"com.google.android.gms.actions.SEARCH_ACTION\")) { query = getIntent().getStringExtra(SearchManager.QUERY); } In the manifest, all you are doing is adding an intent filter to your search activity, using the appropriate action. After that, your activity will be launched and the phrase passed to it. You can then obtain the phrase inside the activity by checking whether the intent’s action matches what we declared, and if so, we extract the appropriate extra. Once you have the phrase, all you have to do is perform the appropriate search within your app. At Haptik, we do this in a unique way by sending the phrase to an expert instead as a message. Not what Google probably expected this to be used as, but powerful nonetheless thanks to its being a hybrid search/texting app. If you’d love to see your favorite apps add this feature, we recommend you tweet out this blog to them. Do mention us with (@hellohaptik) as well, and we’ll try to assist you in peer pressuring them 🙂 Cheers! Posted by Raveesh Bhalla on Jun 18, 2015 4:45:00 PM", "date": "2015-06-18,"},
{"website": "Haptik-AI", "title": "Spello – The Spell Correction Library", "author": [" Aman Srivastava"], "link": "https://www.haptik.ai/tech/spello-the-spell-correction-library", "abstract": "Tweet I am sure I won’t make a spelling mistake while writing this blog. But unfortunately, it’s hard to expect the same while chatting, tweeting or commenting or social media. IVAs (Intelligent Virtual Assistants) built on the Haptik platform have to cater to such noisy queries and still provide quick and correct resolution. Hence, a fast and accurate spell corrector is one of the fundamental components of our NLU pipeline. We are happy to share our spell correction module, “ Spello ” and some of our learnings from the same. Spello is a spell correction model that uses the power of three models “ Phoneme ”, “ Symspell ” and “ Context ” in the backend to get the best possible spelling suggestion for misspelled words in a text. The Phoneme Model uses the Soundex algorithm and is responsible to suggest corrections by searching for similar-sounding words. Symspell model uses edit distance and suggests correction primarily for unintentional mistakes while typing from the keyboard. The contextual model is responsible to find the best candidate from the list of suggestions suggested by Phoneme and Symspell model for a misspelled word. Currently, Spello is open-source, ready to use for English (en) and Hindi (hi) and fast enough to correct misspelled words in less than 10ms. Available Models Below is the detailed description of the working and types of mistakes handled by each of the above-mentioned models: Phoneme Model: This model handles spelling mistakes that occur either due to wrong usage or omission of vowels in a word. For example: Check for availblity -> check for availability pls book a table -> please book a table सही मातरा में पानी मिलाएं -> सही मात्रा में पानी मिलाएं Symspell Model We are using a modified version of Symspell model to handle qwerty based errors which un-intentionally occur while typing from the keyboard. Below are some of the typing mistakes users might make in a conversation: Adjacent Character error : Misplace a character by some nearby character from the keyboard. For example: training -> trsining (here “a” get replaced by “s” which comes adjacent to each other) Transpose error : Sometimes while typing fast, we might introduce transposition between characters. For example: helping -> hleping [here “el” get replaced by “le”] Adding or deleting unrequired character: While typing fast, sometimes users end up adding extra character in a word. For example: address / addres -> addresss [here an extra “s” gets added in word] Context Model: This model helps to determine the most probable word from the list of the suggested word for misspelled words from the Symspell and Phoneme model. A lightweight n-gram probabilistic model has been trained which will find the next best word in a sentence. We use this model to calculate the overall probability of a sentence being formed for each combination of the misspelled word suggestions. For example: For the sentence, “I wnt to buk flight”, we got two misspelled words ‘wnt’ and ‘buk’ with suggestions [‘went’, ‘want’] and [‘book’, ‘buy’] respectively. This model will calculate the probability for each of the following sentences: I want to buy a flight I want to book a flight I went to buy a flight I went to book a flight The sentence with the most probability will get selected and the suggested words in that sentence will be selected as the correct spellings of misspelled words. Getting Started Install the library using pip: pip install spello Initialize the model by providing language code: from spello.model import SpellCorrectionModel sp = SpellCorrectionModel(language='en') Train the model either by giving a “list of sentences” or “word count dictionary” as training data: sp.train(['I want to play cricket', 'Kohli scores another century']) sp.train({'I': 2, 'want': 1, 'play': 1, 'cricket': 1, 'century': 5}) Run the model for spell correction: sp.spell_correct('i wnt to plai kriket') { 'original_text': 'i wnt to plai kriket', 'spell_corrected_text': 'i want to play cricket', 'correction_dict': {wnt: 'want', 'plai': 'play', 'kriket': 'cricket'} } Pre-trained models for English and Hindi are available to be used directly and you can train models in other languages as well.  For more details, follow the Github repository of our Spell correction – https://github.com/hellohaptik/spello Limitations There are two limitations of the current version of Spello (spell correction): 1. It assumes that words that are part of training data are correctly spelled words. However, the frequency of misspelled words being lesser will reduce their impact on the model if the frequency of their respective correct version is higher. 2. It attempts correction for only those words which are not present in vocabulary. For example, In a sentence “I want to by Apple”, Correct replacement for ‘by’ should have been “buy” but it will not suggest any correction for “by” as it is a valid English word and is very likely to be present in the vocabulary when you train the model. However, we have seen negligible occurrences of such mistakes in user data at Haptik and hence we avoid correcting such words to optimize the speed of processing in a production environment. In a future release, we will be adding features to address both these limitations. Do let us know your feedback and we will be happy to consider it while building our future roadmap. Haptik is hiring. Do check out our careers page. Posted by Aman Srivastava on Feb 9, 2020 7:40:00 PM", "date": "2020-02-9,"},
{"website": "Haptik-AI", "title": "Animated Vector Drawables – II", "author": [" Raj-Dixit"], "link": "https://www.haptik.ai/tech/animated-vector-drawables-ii/", "abstract": "Tweet In my previous article, I covered the basics of Vector Drawables and how to animate a few of their attributes using Animator Vector Drawables. Assuming you are through with that, in this article, we will walk through examples involving techniques that can be used to have a nice little icon animation in your app. Path-Morphing As the name suggests, this involves morphing from one path to another which is achieved by animating android:pathData , attribute of the <path> elements of any <vector> . In order to morph path A into path B , we need to take care of these first: A and B must have the same number of drawing commands . The ith drawing command in A must have the same type as the ith drawing command in B, for all i . Say, if path A has M12,13 as its 4th drawing command. Path B must have a similar M<?><?> as its 4th drawing command for Path A to able to morph into path B. Android animates the difference between the values assigned to these matching commands only. Let’s say, we need to have a play vector drawable which morphs into a pause vector drawable when tapped and back to play when tapped again: To provide one such drawable to an ImageView to do all that, we use AnimatedStateListDrawable . So, let’s have a look at as_playpause.xml. <item android:id=\"@+id/pause\" android:drawable=\"@drawable/vd_pause\" <strong class=\"markup--strong markup--pre-strong\">android:state_selected=\"false\"</strong> /> .... .... .... <transition android:drawable=\"@drawable/avd_play_to_pause\" <strong class=\"markup--strong markup--pre-strong\">android:fromId=\"@id/play\"</strong> android:reversible=\"false\" <strong class=\"markup--strong markup--pre-strong\">android:toId=\"@id/pause\"</strong> /> 1. The <item> element decides the vector drawable which will be used when the ImageView is in a given state. In this example, I have used android:state_selected to decide between the state changes. Obviously, one can provide a custom state through <declare-styleable> but I won’t go into those details here. 2. The <transition> element decides which Animated Vector Drawable to run when ImageView changes from one state, decided by android:fromId tag, to another, decided by android:toId tag. One key point to keep in mind here is that each <transition> element’s <animated-vector> drawable uses android:drawable attribute to decide the drawable that will be used to run the animation on when that transition is applied. Now, have a look at the avd_play_to_pause.xml we have here, which is the Animated Drawable that runs when ImageView changes from play to pause state: <?xml version=\"1.0\" encoding=\"utf-8\"?> <animated-vector xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:drawable=\"@drawable/vd_pause\" tools:targetApi=\"lollipop\"> <target android:name=\"iconPath\" android:animation=\"@animator/morph_play_to_pause\" /> </animated-vector> The ObjectAnimator, assigned through android:animation , is as follows: <objectAnimator xmlns:android=\"http://schemas.android.com/apk/res/android\" android:duration=\"600\" android:interpolator=\"@android:interpolator/linear\" android:propertyName=\"pathData\" android:valueFrom=\"@string/path_data_play\" android:valueTo=\"@string/pause_path_data\" android:valueType=\"pathType\" /> android:propertyName indicates that it is the pathData property that we are interested in animating. android:valueFrom is used to assign the pathData to morph from and and android:valueTo is used to decide which pathData to morph to. Since these values are path-commands, we also have to specify the android:valueType, which is “ pathType ” for pathData values. And that completes this discussion. Now, let’s move on to the next technique. Path Trimming The android:trimPathStart and android:trimPathEnd path – attributes of a vector drawable can be used to decide the sections of the path data to render on UI. As with many of the other path-attributes, these can be animated as well using Animated Vector Drawable . android:trimPathStart controls the start of the visible part, which has a default value of 0. android:trimPathEnd controls the end of the visible part, which has a default value of 1. In the example below, the heart icon is filled when tapped and it reverts back to the empty state when tapped again: Icon Animation achieved through Path Trimming I used AnimatedStateListDrawable API again to provide the ImageView a drawable that could do this job. Have a look at as_like_unlike.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <animated-selector xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" tools:targetApi=\"lollipop\"> <item android:id=\"@+id/liked\" android:drawable=\"@drawable/vd_like\" android:state_selected=\"true\" /> <item android:id=\"@+id/not_liked\" android:drawable=\"@drawable/vd_like\" android:state_selected=\"false\" /> <?xml version=\"1.0\" encoding=\"utf-8\"?> <animated-selector xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" tools:targetApi=\"lollipop\"> <item android:id=\"@+id/liked\" android:drawable=\"@drawable/vd_like\" android:state_selected=\"true\" /> <item android:id=\"@+id/not_liked\" android:drawable=\"@drawable/vd_like\" android:state_selected=\"false\" /> It has two <item> elements for both the states. If you notice, I have used the same vector drawable in both the states out of ignorance. It works fine for the purposes of demonstration, but in real scenarios, one might want to be in a different state by default and thus it would be wise to have different drawable for each state. And then there is one <transition> element which takes care of the transition between these states: <transition android:drawable=\"@drawable/avd_unliked_to_liked\" android:fromId=\"@id/not_liked\" android:reversible=\"true\" android:toId=\"@id/liked\" /> The attribute android:reversible helps Android run the same animation in reversed order when the ImageView instead changes from @id/liked to @id/not_liked. Everything else is almost the same as is in the example explaining path-morphing, except for our ObjectAnimator for obvious reasons. Let’s have a look at fill_like.xml , the animator being used here: <objectAnimator xmlns:android=\"http://schemas.android.com/apk/res/android\" android:duration=\"300\" android:interpolator=\"@android:interpolator/accelerate_decelerate\" android:propertyName=\"trimPathStart\" android:valueFrom=\"1\" android:valueTo=\"0\" android:valueType=\"floatType\" /> The android:propertyName =”trimPathStart” identifies that this animator is interested in animating the attribute “trimPathStart” . Of course, one can try other animating properties like trimPathEnd or/and trimPathOffset and go crazy with their imaginations. You can check out the complete source code available here and start playing around if haven’t already. #BuildBetterApps Also, don’t forget, we are hiring high-quality engineers. So if you are interested reach out to us at hello@haptik.ai . Posted by Raj-Dixit on May 4, 2017 12:00:00 AM", "date": "2017-05-4,"},
{"website": "Haptik-AI", "title": "Offline On-Device Ml – Text Classification", "author": [" Aman Srivastava"], "link": "https://www.haptik.ai/tech/offline-on-device-ml-text-classification", "abstract": "Tweet Machine Learning has proven to be a great advantage over the simple rule-based system. However, it comes with its own set of complexities such as training model, its size, computation, etc. As a result, it becomes challenging to use machine learning for mobile applications, where users expect a quick response. But with the release of TensorFlow lite by google, it’s now possible to ship and run any deep learning model directly on the device using Firebase MLKit. Before delving deeper into this, let’s first understand the key advantages of having an ML model on the device: No server communication and hence reduced hosting cost Offline support – Will work without Internet Speed – Speed of the task will improve as all processes are running locally Privacy – Data will reside inside the user’s device We will be using python as the backend to train and convert a model to the Tflite type. Below is an overview of the topics we shall be covering: 1. Data preparation and preprocessing 2. Building word tokenizer 3. Building a text classifier model using bag-of-words as Feature using Keras. 4. Converting Keras model (.h5) to Tflite format. 5. Creating an android application to run inference on the offline model. Data Preparation We need to first create a dataset for text classification. For simplicity, we can use SNIPS intent classification dataset with classes. You can download the dataset from here . import csv sentences , labels = [], [] with open('data.csv','r')as f: data = csv.reader(f) for row in data: sentences.append(row[0]) labels.append(row[1]) Building Word Tokenizer Since Machine Learning works only on numbers, we need to first transform sentences to fixed number representation. For this, we will create a word_index dictionary, with a mapping of each word to a unique identity number. Here we will read uniques words from a sentence list and assign them a unique index. This will then be used to convert sentences to list of numbers: sentences = [re.sub(r'.,:?{}', ' ', sentence) for sentence in sentences] corpus = \" \".join(sentences) words = set(doc.split()) word_index = {word: index for index, word in enumerate(words)} with open( 'word_index.json' , 'w' ) as file: json.dump( word_index , file ) Building a Text Classifier model We will build a text classifier (using the bag-of-words feature) using DNN architecture and bag-of-words as input feature: from sklearn.preprocessing import LabelEncoder import tensorflow as tf from keras.layers import Dense, Input, Dropout from tensorflow.python.keras import models, optimizers, losses, activations from keras.layers.normalization import BatchNormalization from keras.callbacks import EarlyStopping, ModelCheckpoint from sklearn.model_selection import train_test_split LE = LabelEncoder() def train_and_eval(sentences, label): # converting categorical label labels = LE.fit_transform(labels) labels = np.array( labels ) num_classes = len(labels) onehot_labels = tf.keras.utils.to_categorical(labels , num_classes=num_classes) setences_tokens = [sentence.split() for sentence in sentences] tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.word_index = word_index sentences_features = tokenizer.texts_to_matrix(setences_tokens) train_features, val_features, train_labels, val_labels = train_test_split(sentences_features, onehot_labels, test_size = 0.1) feature_input = Input(shape=(sentences_features.shape[1],)) dense = Dense(128, activation=activations.relu) merged = BatchNormalization()(dense) merged = Dropout(0.2)(merged) merged = Dense(64, activation=activations.relu)(merged) merged = BatchNormalization()(merged) merged = Dropout(0.2)(merged) preds = Dense(num_classes, activation=activations.softmax)(merged) model = Model(inputs=[word_input], outputs=preds) model.compile(loss=losses.categorical_crossentropy, optimizer='nadam', metrics=['acc']) early_stopping = EarlyStopping(monitor='val_loss', patience=5) model.fit([train_features], train_labels, validation_data=([val_features], val_labels), epochs=200, batch_size=8, shuffle=True, callbacks=[early_stopping]) model.save('models.h5') Run the method given below to test your model by giving a model path and word_index path: def test(sentence, model_path, word_index_path) classifier = models.load_model( 'models/models.h5' ) tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='.,:?{} ') sentences = re.sub(r'.,:?{}', ' ', sentence) with open(word_index_path, 'r') as f: tokenizer.word_index = json.loads(f.read()) tokenized_messages = tokenizer.texts_to_matrix(sentence.split()) p = list(classifier.predict(tokenized_messages)[0]) for index, each in enumerate(p): print(index, each) Converting Keras Model (.h5) to Tflite format We need to convert the above model file to Tflite format, which we will then ship to the ML kit and android device. def convert_model_to_tflite(keras_model_path): tf.logging.set_verbosity( tf.logging.ERROR ) converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file( keras_model_path ) converter.post_training_quantize = True tflite_buffer = converter.convert() open( 'model.tflite' , 'wb' ).write( tflite_buffer ) print( 'TFLite model created.') Creating the Device Application Given below is the basic flow of how the ML model works on the device. Let’s now discuss step-by-step the process we will be following to run inference. Starting your project 1. Add word_index.json and model.tflite inside assets of your android project. 2. Add the dependencies for the ML Kit Android libraries to your module (app-level) Gradle file (usually app/build.gradle ): dependencies { // ... implementation 'com.google.firebase:firebase-ml-model-interpreter:21.0.0' } apply plugin: 'com.google.gms.google-services' Also, in your build.gradle ( app-level ), add these lines, which will disallow the compression of .tflite files. android { ... } buildTypes { release { ... } } aaptOptions { noCompress \"tflite\" } } Hosting Models on Firebase Follow the below steps to host your model.tflite mile to MLKit console. 1. In the ML Kit section of the Firebase console , click the Custom tab. 2. Click Add custom model (or Add another model ). 3. Specify a name that will be used to identify your model in your Firebase project, then upload the TensorFlow Lite model file (usually ending in .tflite or .lite ). 4. In your app’s manifest, declare that INTERNET permission is required: <uses-permission android:name=\"android.permission.INTERNET\" /> Define Constants value used for Model // model name given to custom model stored on MLKit public static String REMOTE_MODEL_NAME = \"mlmodel\"; // model name given to model stored locally (can be the same as on MLkit) public static String LOCAL_MODEL_NAME = \"mlmodel\"; // file for word dict with word to index map public static String WORD_DICT_FILE = \"word_index.json\"; // file for model stored locally inside assets public static String LOCAL_MODEL_FILE = \"model.tflite\"; // input shape to your model (max value of index in word_index.json file) public static Integer MODEL_INPUT_SHAPE = 30; // number of classes for your text classification task public static Integer MODEL_NUM_CLASS = 8; Creating Model Input for Given Text This method will return a list of integers in the required shape expected by the model. Here are the steps involved: 1. Read word_index file from assets. 2. Clean the text, removing punctuations, extra spaces, etc. 3. Create a list of zeros of the size of the model input shape. 4. Split text into words, based on words present in the text, it finds the index of that word from word_index and assigns value 1 that index in the list of the above-created zeros. Code for the above implementation is given below: public static String cleanText(String text){ String clean_text = text.toLowerCase(); clean_text = clean_text.replaceAll(\"[.,:?{}]+\", \" \"); clean_text = clean_text.trim(); return clean_text; } private float[][] textToInputArray(String text) throws JSONException { float[][] input = new float[1][MODEL_INPUT_SHAPE]; JSONObject word_dict = new JSONObject(readJSONFromAsset(WORD_DICT_FILE)); String clean_text = cleanText(text); String[] words = clean_text.split(\" \"); for (String word : words) { if (word_dict.has(word)) { int index = word_dict.getInt(word); input[0][index] = 1; } } return input; } Run Classification Call run inferencemethod with the above-processed model input. It returns the label (int) with the maximum confidence score. public class MLModel { float[] probabilities = new float[Constant.MODEL_NUM_CLASS]; public void configureHostedModelSource() { // [START mlkit_cloud_model_source] FirebaseModelDownloadConditions.Builder conditionsBuilder = new FirebaseModelDownloadConditions.Builder().requireWifi(); if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.N) { // Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle(); } FirebaseModelDownloadConditions conditions = conditionsBuilder.build(); // Build a remote model source object by specifying the name you assigned the model // when you uploaded it in the Firebase console. FirebaseRemoteModel cloudSource = new FirebaseRemoteModel.Builder(Constant.REMOTE_MODEL_NAME) .enableModelUpdates(true) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build(); FirebaseModelManager.getInstance().registerRemoteModel(cloudSource); // [END mlkit_cloud_model_source] } public void configureLocalModelSource() { // [START mlkit_local_model_source] FirebaseLocalModel localSource = new FirebaseLocalModel.Builder(Constant.LOCAL_MODEL_NAME)  // Assign a name to this model .setAssetFilePath(Constant.LOCAL_MODEL_FILE) .build(); FirebaseModelManager.getInstance().registerLocalModel(localSource); // [END mlkit_local_model_source] } private FirebaseModelInterpreter createInterpreter() throws FirebaseMLException { // [START mlkit_create_interpreter] FirebaseModelOptions options = new FirebaseModelOptions.Builder() .setRemoteModelName(REMOTE_MODEL_NAME) .setLocalModelName(LOCAL_MODEL_NAME) .build(); FirebaseModelInterpreter firebaseInterpreter = FirebaseModelInterpreter.getInstance(options); // [END mlkit_create_interpreter] return firebaseInterpreter; } private FirebaseModelInputOutputOptions createInputOutputOptions() throws FirebaseMLException { // [START mlkit_create_io_options] FirebaseModelInputOutputOptions inputOutputOptions = new FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.FLOAT32, new int[]{1, MODEL_INPUT_SHAPE}) .setOutputFormat(0, FirebaseModelDataType.FLOAT32, new int[]{1, MODEL_NUM_CLASS}) .build(); // [END mlkit_create_io_options] return inputOutputOptions; } public float[] runInference(float[][] input) throws FirebaseMLException { FirebaseModelInterpreter firebaseInterpreter = createInterpreter(); FirebaseModelInputOutputOptions inputOutputOptions = createInputOutputOptions(); // [START mlkit_run_inference] FirebaseModelInputs inputs = new FirebaseModelInputs.Builder() .add(input)  // add() as many input arrays as your model requires .build(); firebaseInterpreter.run(inputs, inputOutputOptions) .addOnSuccessListener( new OnSuccessListener<FirebaseModelOutputs>() { @Override public void onSuccess(FirebaseModelOutputs result) { // [START_EXCLUDE] // [START mlkit_read_result] float[][] output = result.getOutput(0); for (int i = 0; i < MODEL_NUM_CLASS; i++) { probabilities[i] = output[0][i]; } //                                probabilities = output[0]; callback.onSuccess(probabilities); Log.d(\"Success prediction\", \"\" + probabilities[7]); // [END mlkit_read_result] // [END_EXCLUDE] } }) .addOnFailureListener( new OnFailureListener() { @Override public void onFailure(@NonNull Exception e) { Log.d(\"Error prediction\", e.toString()); callback.onFailure(e.toString()); // Task failed with an exception // ... } }); return probabilities; // [END mlkit_run_inference] } } I hope the above helps you in getting started with ML on-device. Please do try the above and let us know if you have any feedback. We will be sharing more details in the following blog. Haptik is hiring. Do visit our careers page . Posted by Aman Srivastava on Nov 5, 2019 7:36:00 PM", "date": "2019-11-5,"},
{"website": "Haptik-AI", "title": "Using Reflection in Android", "author": [" Rajas Ashtikar"], "link": "https://www.haptik.ai/tech/using-reflection-in-android/", "abstract": "Tweet When designing an Android Application, one of the important things to remember is the download size of your application. Huge download sizes discourage the users to download and install the application. The factor of size restriction becomes worse if you are designing a library, as developers will not want to bloat their application because of the size of your library. Hence, to keep the library size less, it is important to bundle related functionality together while modularizing the other unrelated functionalities. This allows the developer integrating your library to pick and choose the functionality they want to avail from your library while keeping their application sizes in check. At Haptik , we divided the entire SDK into 3 major modules: 1. Core: which will enable the basic chat flows like “Daily Quiz”, “Reminders” etc in the client’s application. 2. Travel : along with the basic chat flow the developers can also enable the functionality to make travel bookings using the Haptik UI flow, by integrating the Travel module of Haptik. (If transactions are needed, Payments module needs to be integrated as well) 3. Payments : Haptik also offers options to make Payments over its platforms, like “Recharge” for phones operators etc. Modularizing the functionalities simultaneously increases the complexity of integrating the library. T he developers using your library will have to keep in mind that they should initialize all the modules of the library before using it. Failing to do so, may cause the library to crash. To solve the above problem one of the approaches is using REFLECTION. What is Reflection? In object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods ( Wikipedia ). Let’s understand Reflection in Java with a simple example: // With reflection to instantiate an object Object foo = Class . forName ( \"complete.classpath.and.Foo\" ) . newInstance ( ) ; // Alternatively you can also use the below: Object foo = Foo . class . newInstance ( ) ; Method m = foo . getClass ( ) . getDeclaredMethod ( \"hello\" , new Class < ? > [ 0 ] ) ; m . invoke ( foo ) ; Using Java Reflection you can check Java classes at runtime. Checking for classes is often the first thing you do when using Reflection. You can obtain the following information from the class: Class Name Class Modifies (public, private, synchronized etc.) Package Info Superclass Implemented Interfaces Constructors Methods Fields Annotations, plus a lot more information related to Java classes. For a full list, you should consult the Doc for java.lang.Class. Why use “reflection” to create objects instead of the “new” operator in Java? – “ new ” operator is used to create objects as shown in the below example: Foo foo = new Foo ( ) ; foo . hello ( ) ; Here, the limitation is that if you use new operator there is no way to define the type of the object at runtime. Let’s see how we can do this using Reflection: Since the type of object needs to be created at the runtime, the availability of the class at the runtime needs to be checked: try { Class cls = Class . forName ( \"developer.r47.module1api.Module1FactoryClass\" ) ; //this will get the public constructors for the class Constructor [ ] publicConstructors = cls . getConstructors ( ) ; //this will get all the declared constructors for the class Constructor [ ] declaredConstructors = cls . getDeclaredConstructors ( ) ; //this will return only the public methods Method [ ] methods = cls . getMethods ( ) ; //this will return all the methods declared in the class Method [ ] declaredMethods = cls . getDeclaredMethods ( ) ; //this will get all the public fields for the class Field [ ] publicFields = cls . getFields ( ) ; //this will get all the declared fields for the class Field [ ] declaredFields = cls . getDeclaredFields ( ) ; Object clsObject = cls . newInstance ( ) ; //to get a specific method at the runtime pass the name of the method Method method = cls . getDeclaredMethod ( \"getModule1Functionality\" ) ; //since the method in question is static the object on which it needs to be called is null //else you can pass `clsObject` from above as the parameter to the method below Object object = method . invoke ( null ) ; //take action as the module you were looking for exists } catch ( ClassNotFoundException e ) { //add your fallback mechanism here e . printStackTrace ( ) ; } catch ( IllegalAccessException e ) { //add your fallback mechanism here e . printStackTrace ( ) ; } catch ( InstantiationException e ) { //add your fallback mechanism here e . printStackTrace ( ) ; } catch ( NoSuchMethodException e ) { e . printStackTrace ( ) ; } catch ( InvocationTargetException e ) { e . printStackTrace ( ) ; } You can explore more details about using reflection to get more properties of a class here . What if the access to a constructor or a field or a method is private? You can access a “ private ” method using: constructor . setAccessible ( true ) method . setAccessible ( true ) field . setAccessible ( true ) Object clsObject = cls . newInstance ( ) ; //to get a specific method at the runtime pass the name of the method Method method = cls . getDeclaredMethod ( \"getModule1Functionality\" ) ; //since the method in question is static the object on which it needs to be called is null //else you can pass `clsObject` from above as the parameter to the method below /*the invoke(Object, obj, Object... args) method takes the object on which the method needs to be * invoked as a parameter, and the arguments that are required by the method you are trying to invoke * as the second argument*/ /* * It throws `NoSuchMethodException` which you will have to handle*/ Object object = method . invoke ( null ) ; So, you can check for the presence of the dependency in the build.gradle and internally initialize the class. If the class is not present you can also have a fallback mechanism. To get access to the code for this project, visit: https://github.com/rajas47ashtikar92/reflection-sample-app We would love to know what you think about this approach and if there is any better way to tackle the problems of evergrowing sizes of mobile applications/SDKs. In our next blog, we will talk about some other things we did to reduce our android SDK size. Posted by Rajas Ashtikar on Oct 10, 2018 7:27:00 PM Find me on: LinkedIn Twitter", "date": "2018-10-10,"},
{"website": "Haptik-AI", "title": "Serverless CSV Uploader For API Integrations", "author": [" Parvez Alam"], "link": "https://www.haptik.ai/tech/serverless-csv-uploader-for-api-integrations", "abstract": "Tweet At Haptik, thus far, we’ve developed Conversational AI solutions for a number of enterprises across sectors. Most of the bots we have developed so far have custom use cases which require API integrations . There are some use cases where the bot response is completely dependent on the user input. Some of the use-cases are as mentioned below: 1. Documents required for loan type asked by the user. 2. List of all available franchisee location for a given pin code. 3. Create a ticket in client CRM when user post grievance. 4. Calculation of premium for the term or investment plans 5. Fetch the test report status There are more such examples, but for now, I’ll discuss the one use case where the user asks the bot for report status. In the above diagram, I’ll walk you through where API integration happens: The moment when our message pipeline receives a message it goes through the following stages: 1. Language Detection 2. Domain Classification 3. Intent Detection 4. Name Entity Recognition 5. Deep Learning Layer 6. Business Logic 7. Multi-lingual support for ML models 8. Bot personality & tone adaptable to context 9. Data preprocessing 10. Returns Bot Response As can be observed from the above diagram, Named Entity Recognition returns entities from the message. In pointer E in the diagram, we use these entities as params to hit the client API, parse the API response, and return the bot response in the required format (Carousel, Quick Replies, Action Button Elements). Problem Statement While working with many enterprise businesses we noticed that some enterprises don’t always have APIs for use cases in which data doesn’t change frequently. for eg: franchise location or required document for some loan type. To solve this problem, we used to: 1. Ask enterprises to give us CSV dump for the data which we used to upload in MySQL table. 2. Query the table for the given parameters. 3. Return the response in appropriate HSL(Haptik Specific Language, which we use internally to create UI elements, like a carousel, quick replies, action buttons, etc) format. But with the above approach, we faced the following problems: 1. Repetitive process, for each enterprise. The data fields were different so we were maintaining different SQL tables for each client. 2. Frequent code release required on addition/deletion of new data field: Since schema was required to change every time some new data field was added or removed, a new code release was required for that. 3. Non-Scalable: considering the number of clients being added this was not a scalable way to handle the problem. Solution We moved to a more skeleton based approach and let the values for response etc. be dynamically generated from the bot builder . We decided to leverage AWS lambda , S3, and MongoDB for this problem. AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of the Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code. 1. Configure an S3 bucket to trigger a lambda function every time when someone uploads a CSV file. 2. Triggered Lambda function which parses the CSV data fields creates the schema, validates the data for each field, uploads the CSV to and dump the data in mongo. 3. We have also made use of Lambda Layers which helped us access more packages like Pandas etc. 4. Render the error or success response when CSV upload gets completed or send an email accordingly. Sample code for the lambda function import os import logging import json import datetime import boto3 import pandas as pd import requests from io import StringIO from pymongo import MongoClient from mongoengine import StringField, DateTimeField, Document, connect from botocore.exceptions import ClientError SENDER = \"parvez.alam@abc.com\" AWS_REGION = \"us-west-2\" RECIPIENT = requests.post( url=os.environ['CONTENT_STORE_URL'], json={ \"message_type\": \"client.csv_upload.email_recipients\" }).json() # The subject line for the email. SUBJECT = \"Client CSV Upload Status\" s3 = boto3.resource('s3') # Set client client = MongoClient('mongodb://{host}:{port}/'.format( host=os.environ['DB_HOST'], port=os.environ['DB_PORT'])) # Set database db = client['test'] # Logger settings - CloudWatch logger = logging.getLogger() logger.setLevel(logging.DEBUG) connect(os.environ['DB_NAME'], host='{host}:{port}'.format( host=os.environ['DB_HOST'], port=os.environ['DB_PORT'])) Class CSVData(Document): \"\"\"Schema to store hdfc csv data. \"\"\" field_1 = StringField(max_length=255, required=True) field_2 = StringField(max_length=255, required=True) field_3 = StringField(max_length=100, required=True) field_4 = StringField(required=True) field_5 = StringField(required=True) field_6 = StringField(required=True) field_7 = StringField(required=True) created_on = DateTimeField(default=datetime.datetime.now) meta = { 'collection': 'csv_data', 'strict': False, 'auto_create_index': False } def send_email(body_text): \"\"\"Send Email \"\"\" # The character encoding for the email. CHARSET = \"UTF-8\" # Create a new SES resource and specify a region. client = boto3.client('ses', region_name=AWS_REGION) # Try to send the email. try: #Provide the contents of the email. response = client.send_email( Destination={ 'ToAddresses': RECIPIENT, }, Message={ 'Body': { 'Text': { 'Charset': CHARSET, 'Data': body_text, }, }, 'Subject': { 'Charset': CHARSET, 'Data': SUBJECT, }, }, Source=SENDER ) # Display an error if something goes wrong. except ClientError as e: print(e.response['Error']['Message']) else: print(\"Email sent! Message ID:\"), print(response['MessageId']) ################################################################## ##------------- Lambda Handler ---------------------------------## ################################################################## def lambda_handler(event, context): \"\"\"Event handler method which will get triggered when new csv file get uploaded for hdfc client Args: event (dict) -- Lambda event details context (obj) -- Context of environment in which this lambda function is executing \"\"\" response = { 'status': False, 'message': '' } logger.info(\"Received event: \" + json.dumps(event, indent=2)) logger.info(\"initializing the collection\") logger.info(\"started\") bucket = event['Records'][0]['s3']['bucket']['name'] key = event['Records'][0]['s3']['object']['key'] #obj = s3.get_object(Bucket=bucket, Key=key) obj = s3.Object(bucket_name=bucket, key=key) try: csv_string = obj.get()['Body'].read().decode('utf-8') # Read csv file as string except Exception as e: logger.exception(str(e)) response['message'] = str(e) send_email(\"Csv file is invalid\") return response logger.info(\"Bucket {} key(s)\".format(bucket, key)) df_rows = pd.read_csv(StringIO(csv_string)) # Convert string to dataframes if df_rows.empty: message = \"uploaded csv is empty\" logger.debug(message) send_email(message) logger.debug(df_rows.columns) column_order = ['List of Columns'] csv_columns = [column for column in df_rows.columns] logger.debug(csv_columns) # Before insertion remove the existing data from collection db.csv_data.remove({}) for index, row in df_rows.iterrows(): data = CSVData( field_1 = row[0], field_2 = row[1], field_3 = row[2], field_4 = row[3], field_4 = row[4], field_5 = row[5], field_6 = row[6] ) data.save() # The email body for recipients with non-HTML email clients. message = \"CSV Upload has been completed and data has been inserted successfully\" send_email(message) response['status'] = True response['message'] = 'Data has been inserted successfully' return response Advantages: 1. Lesser cost 2. Code release only required when we need to change the MongoDB query. 3. Any person in the company who has access can use the tool and configure the validation, so less developer intervention required. 4. Makes life easier for Bot builders. Future Scope: UI tool so anyone can configure the schema and required validation to upload the CSV. Hope this blog helps you understand how we improved the process of building custom bot solutions for clients and used serverless Technology to achieve the same. We welcome your comments and feedback and would love to hear from you. We are building more solutions like this so follow us. We are hiring. Do visit our careers page . Posted by Parvez Alam on Jul 3, 2019 7:29:00 PM", "date": "2019-07-3,"},
{"website": "Haptik-AI", "title": "Why We Chose (To) React!", "author": [" Vaibhav Nachankar"], "link": "https://www.haptik.ai/tech/why-we-chose-to-react", "abstract": "Tweet Chat is the most socially used interface in this day and age. It is the most convenient form of communication available. We all use at least one chat application daily, may it be on our mobile phones or desktop. Haptik is one such chat service application available on Android and iOS which connects people to their very own digital personal assistants. As a user you can ask your assistant to book a movie ticket , make your dinner reservations , wake you up at 5 am in the morning, and much much more, on chat itself. We have a team of dedicated in-house assistants who are available 24×7 to support and help the users complete these tasks. This is why we built Athena! Athena, named after the Greek goddess of Wisdom (we are all Greek mythology Geeks here), is Haptik’s web chat tool used by our assistants to resolve queries and complete user’s tasks. We power Athena with all the information and tool integrations that we think can make their job quicker and convenient. The first version of Athena was made out of pure javascript and jQuery with the backend powered by Python-Django. We used Django templates for rendering HTML with jQuery being used for any DOM manipulations. This worked for us for a good 3 months, but things started blowing up soon after. The problem with any real-time application is that things change every second. Every time a message was received we had to: append the messages to the thread update the counter of messages change the state of the chat bind new event listeners and do 10 other things… We ended up doing heaps of expensive DOM manipulation on our app for every incoming message. To top that, our codebase was already bloated. That was the problem of using a framework like jQuery which is amorphous and unstructured, with a tightly coupled architecture. We knew we had to decouple the frontend from the backend to make it scalable. Enter, REACT We evaluated a bunch of frontend frameworks, with Backbone-js, Angular-js and React-js being the front-runners and finally chose the latter. React is Facebook’s very own open source javascript library for building User Interfaces. It is the V(iew) in the traditional MVC architecture. Unlike Angular-js which has a steep learning curve, React is very easy to grasp and we started coding in days. Here are few things we loved about React: JSX React code is usually written in JSX, the extended javascript syntax programming language, using which you can write HTML type of components within Javascript. It makes the React code readable and easier to write. <i><span style=\"font-weight: 400;\">// MessageItem.js</span></i> <span style=\"font-weight: 400;\">var MessageItem = React.createClass({</span> <span style=\"font-weight: 400;\">   propTypes: {</span> <span style=\"font-weight: 400;\">       message: ReactPropTypes.object,</span> <span style=\"font-weight: 400;\">   },</span> <span style=\"font-weight: 400;\">   render: function() {</span> <span style=\"font-weight: 400;\">       var message = this.props.message;</span> <span style=\"font-weight: 400;\">       var messageItem =</span> <span style=\"font-weight: 400;\">    </span> <span style=\"font-weight: 400;\">   &lt;div className=</span> <span style=\"font-weight: 400;\">                {cx({</span> <span style=\"font-weight: 400;\">                  'from-assistant': message.direction == 'from-assistant',</span> <span style=\"font-weight: 400;\">                  'from-user': message.direction == 'from-user',</span> <span style=\"font-weight: 400;\">                })}&gt;</span> <span style=\"font-weight: 400;\">                &lt;div&gt;</span> <span style=\"font-weight: 400;\">             </span> <span style=\"font-weight: 400;\">   {message.text}</span> <span style=\"font-weight: 400;\">                &lt;/div&gt;</span> <span style=\"font-weight: 400;\">           &lt;/div&gt;  </span> <span style=\"font-weight: 400;\">        return (</span> <span style=\"font-weight: 400;\">           {messageItem}           </span> <span style=\"font-weight: 400;\">   );</span> <span style=\"font-weight: 400;\">   },</span> <span style=\"font-weight: 400;\">});</span> One way data-binding Unlike most frameworks, React uses a one-way data flow mechanism. React does not let the HTML to update your component. You can only use triggers and events to change the component <span style=\"font-weight: 400;\">// MessageComposer.js</span> <span style=\"font-weight: 400;\">var MessageComposer = React.createClass({</span> <span style=\"font-weight: 400;\">    getInitialState: function() {</span> <span style=\"font-weight: 400;\">   return {text: \"\"};</span> <span style=\"font-weight: 400;\">    },</span> <span style=\"font-weight: 400;\">    _onChange: function(event,value) {</span> <span style=\"font-weight: 400;\">  var value = event.target.value;</span> <span style=\"font-weight: 400;\">       this.setState({text: value});</span> <span style=\"font-weight: 400;\">},</span> <span style=\"font-weight: 400;\">    render: function() {</span> <span style=\"font-weight: 400;\">       return (</span> <span style=\"font-weight: 400;\">      &lt;div className='messageBox'&gt;</span> <span style=\"font-weight: 400;\">               &lt;input</span> <span style=\"font-weight: 400;\">                   type=\"text\"</span> <span style=\"font-weight: 400;\">                   placeholder=\"Start Typing...\"</span> <span style=\"font-weight: 400;\">                   onChange={this._onChange}</span> <span style=\"font-weight: 400;\">                   value={this.state.text}/&gt;</span> <span style=\"font-weight: 400;\">           &lt;/div&gt;</span> <span style=\"font-weight: 400;\">      );</span> <span style=\"font-weight: 400;\">   },</span> <span style=\"font-weight: 400;\">});</span> This is our React Message Composer component. As you can see the <input> field can only be updated via the render() function and not by the HTML itself. It can only be updated using events, in this case the onChange event. Component Reusability Because of the modular nature of react, it is very easy to use and reuse the same components throughout your app. <i><span style=\"font-weight: 400;\">// MessageThread.js</span></i> <span style=\"font-weight: 400;\">var MessageThread = React.createClass({</span> <span style=\"font-weight: 400;\">    </span> <span style=\"font-weight: 400;\">getInitialState: function() {</span> <span style=\"font-weight: 400;\">        </span> <span style=\"font-weight: 400;\">return {</span> <span style=\"font-weight: 400;\">        messages: [{body:\"Hey\", direction:\"from-user\"},</span> <span style=\"font-weight: 400;\">                       {body:\"I need help web checkin\", direction:\"from-user\"},</span> <span style=\"font-weight: 400;\">                       {body:\"Sure, please share your PNR\", direction:\"from-assistant\"}],</span> <span style=\"font-weight: 400;\">        };</span> <span style=\"font-weight: 400;\">    },</span> <span style=\"font-weight: 400;\">    render:function() {</span> <span style=\"font-weight: 400;\">        var messageItems = this.state.messages.map(function(message){</span> <span style=\"font-weight: 400;\">            return  &lt;MessageItem message={message}/&gt;</span> <span style=\"font-weight: 400;\">        });</span> <span style=\"font-weight: 400;\">        return(&lt;div&gt;</span> <span style=\"font-weight: 400;\">                  {messageItems}</span> <span style=\"font-weight: 400;\">                  &lt;MessageComposer/&gt;</span> <span style=\"font-weight: 400;\">               &lt;/div&gt;</span> <span style=\"font-weight: 400;\">        );</span> <span style=\"font-weight: 400;\">    },</span> <span style=\"font-weight: 400;\">});</span> The Message Item is a react component which is re-used over iteration of the message list. Virtual DOM Another cool feature with React is the in-memory cache data structure that is created using javascript for updating and modifying the DOM. React computes all the changes in its cache and then updates the actual DOM. Because of this when a user messages, instead of making unnecessary and expensive DOM updations, only components like the Message , Message Counter, and other relevant components are updated. Watch this video to learn more about React’s Virtual DOM . (source: youtube/ LispCast ) Note that React is just the View part of the architecture. We use Flux by Facebook which complements React’s unidirectional flow. But, more on Flux in future posts. Facebook announced React-Native last year, which is their framework for making hybrid native apps for iOS and Android. Since then many apps on both the stores have started converting from pure native to hybrid models. React is certainly a framework you should keep your eyes on! If you’ve had an experience with React and would like to help us build at Haptik send in your profile at hello@haptik.co and join the react team! 🙂 P.S: If you want to connect the dots on this post, simply Download Haptik & you’ll know what the hell we were talking about. Posted by Vaibhav Nachankar on Feb 18, 2016 6:02:00 PM", "date": "2016-02-18,"},
{"website": "Haptik-AI", "title": "Haptik introduces HINT3 to benchmark performance of Dialogue Agents", "author": [" Gaurav Arora"], "link": "https://www.haptik.ai/tech/haptik-introduces-hint3", "abstract": "Tweet Intent Detection is a vital part of the Natural Language Understanding (NLU) pipeline of  Task-oriented dialogue systems. Recent advances in NLP have enabled systems that perform quite well on existing intent detection benchmarking datasets like HWU64, CLINC150, BANKING77 as shown in Larson et al., 2019 , Casanueva et al., 2020 . However, most existing datasets for intent detection are generated using crowdsourcing services. This difference in dataset preparation methodology leads to assumptions about training data which are no longer valid in the real world. In the real world, definition of intent often varies across users, tasks and domains. Perception of intent could range from a generic abstraction such as “Ordering a product” to extreme granularity such as “Enquiring for a discount on a specific product if ordered using a specific card”. Additionally, factors such as imbalanced data distribution in the training set, diverse background of domain experts involved in defining the classes makes this task more challenging. During inference, these systems may be deployed to users with diverse cultural backgrounds who might frame their queries differently even when communicating in the same language. Furthermore, during inference, apart from correctly identifying in-scope queries, the system is expected to accurately reject out-of-scope queries, adding on to the challenge. Dataset Domain Number of Intents Number of Queries Train Test Full Subset In Scope Out of Scope SOFMattress Mattress products retail 21 328 180 231 166 Curekart Fitness supplements retail 28 600 413 452 539 Powerplay11 Online gaming 59 471 261 275 708 HINT3 Datasets and its statistics Hence, to accurately benchmark in real-world settings, we release 3 new single domain datasets, each spanning multiple coarse and fine grain intents, with the test sets being drawn entirely from actual user queries on the live systems at scale instead of being crowd-sourced. On these datasets, we find that the performance of existing systems saturates at unsatisfactory levels because they end up capturing spurious patterns from the training data. Similar failures have been reported in computer vision also when Google’s medical AI was super accurate in lab but in real life it was a different story . Few examples of queries from our dataset which failed on all platforms are shown in the table given below: This figure shows few examples of test queries in SOFMattress which failed on all platforms, L: LUIS, H: Haptik, D: Dialogflow, R: Rasa. NO NODES DETECTED is the out-of-scope label. There are 3 major contributions based on our paper: We open-source 3 new datasets to accurately benchmark intent detection performance of task oriented dialogue systems in the real world. We evaluate 4 NLU platforms: Dialogflow , LUIS , Rasa NLU and Haptik on HINT3, highlight gaps in language understanding and discuss tradeoffs between performance on in-scope vs out-of-scope queries. We propose a novel 'subset approach' for evaluation by extracting one representative sample for each natural 'class' of examples based on mutual entailment scores. This figure presents results for all systems, for both Full and Subset variations of the dataset. Best Accuracy on all the datasets is in the early 70s. Best MCC ( Matthew’s Correlation Coefficient ) for the datasets varies from 0.4 to 0.6, suggesting the systems are far from perfectly understanding natural language. In this table, we consider in-scope accuracy at a very low threshold of 0.1, to see if false positives on out-of-scope queries would not have mattered, what’s the maximum in-scope accuracy that current systems are able to achieve. Our results show that even with such a low threshold, the maximum in-scope accuracy which systems are able to achieve on Full Training set is pretty low, unlike the 90+ in-scope accuracies of these systems which have been reported on other public intent detection benchmarking datasets like HWU64, CLINC150, BANKING77 as shown in Larson et al., 2019 , Casanueva et al., 2020 . We’ve made our datasets and code freely accessible on GitHub to promote transparency and reproducibility. You can also check out our paper \" HINT3: Raising the bar for Intent Detection in the Wild \" accepted at EMNLP-2020's Insights workshop for more details. Posted by Gaurav Arora on Nov 18, 2020 12:52:49 PM", "date": "2020-11-18,"},
{"website": "Haptik-AI", "title": "Open-Sourcing The Nlu ‘Swiss Army Knife’ For Conversational AI", "author": [" Saransh Mehta"], "link": "https://www.haptik.ai/tech/open-sourcing-nlu-conversational-ai", "abstract": "Tweet We recently open-sourced a repository called Multi-Task learning which can help with multiple conversational AI tasks. In this blog, we have explained why such an architecture can change the way we build conversational AI systems and the thought process behind building it. Entrusted to build the Next-generation conversational AI, we had some obvious questions to contend with. How do you define “a good conversational ai “? What does it comprise of? What are the indispensable cogs in this machine? Drawing parallels to the human brain and its conversational ability, was inevitable  in this discussion. One of the résponses :  “it should have Natural Language Understanding (NLU) + Natural Language Processing (NLP) + Natural Language Generation (NLG) abilities “,  was met with an eerie silence, followed by a thought-provoking counter question. “ Does the human brain handle conversations the same way? If yes, How does the transition between the three, seem so fluid? Almost unnoticeable? Why do we not feel any lag between the 3 processes? “? Following a bit of research,  it was not very difficult to find out that there are 3 areas in the brain which do exhibit similar behaviour . Wernicke’s area: mainly involved in the understanding and processing of speech and written language. The NLU equivalent? Angular gyrus : helps with memory retrieval, context understanding, and general cognition. NLP (pragmatics, context understanding /knowledge retrieval) equivalent? Broca’s area has an important role in turning your ideas and thoughts into actual spoken words.  The NLG equivalent? And? There was no way for us to profess with certainty how these three operate in sync, for humans to comprehend a query and respond back with the appropriate answer. But an argument in the context of intrinsic, extrinsic and germane cognitive load, gave us some validity to our assumption that ‘ maybe ’ general conversations do not exert cognitive load large enough for the aforementioned lag across these processes to be noticeable. The Argument was beautifully underpinned by an example of a bilingual conversational exercise where a bilingual user is being asked a question in one language but needs to respond in a different language. The ‘Understanding act’ in a foreign language, ‘processing/retrieval part’ in the native language, and responding again adds a bit of extraneous cognitive complexity, causing the minuscule but noticeable lag, enough to ascertain that we could assume the fundamental pipeline for conversational AI to look something like: NLU->  NLP -> NLG The mind works by ear Language Understanding is easier than a generation, in general. Lots of people comprehend a second/third language really well, but can’t speak with similar fluency. In fact, the study of brain activities against the sound using the Magnetoencephalography (MEG) technology has demonstrated that it is even possible for human infants to start to learn/understand 2 languages together, while still not being able to speak any. That pretty much made it clear to us, we needed to take up NLU before the rest. With NLU encompassing syntax, semantics, and pragmatics, we zeroed down on the bare minimum requirement for a functional NLU system for conversational AI. Each component plays its own role in ensuring the virtual assistant sounds intelligent and not vulnerable to very common failures. Each cog in this machine is designed to either detect, extract, or infer from the query to intelligently respond to the user. The Centipede’s Dilemma “The centipede effect occurs when a normally automatic or unconscious activity is disrupted by consciousness of it or reflection on it. For example, a golfer thinking too closely about her swing or someone thinking too much about how he knots his tie may find his performance of the task impaired.” Considering the spectrum of functionalities expected(as seen in the diagram above)  to be developed for an effective natural language understanding required in a conversational system, putting it all together can be no lesser than ‘the centipede’s dilemma’ for an AI developer. Imagine the complexity of the pipeline if each component was driven by GPU-hungry transformer-based architecture. Imagine the need to upgrade to the latest SOTA encoder, every few weeks. Needless to say, it can easily bog down most developers. There was a dire need for a utility which is capable of performing multiple conversational tasks without hogging on computational resources and be versatile enough to fit the needs of different data formats. Having a closer look, it’s probably a bad idea to make a separate model for each task and carry along the burden of millions of parameters each time. We found a simple yet effective way to do this is by having a pre-trained encoder model act as pivot and then build task specific headers on top of it as required. Vola! one can now get rid of those heavy encoders required for each task by clubbing multiple tasks together. The following illustration demonstrates the gist of how multiple tasks can be trained using one single shared encoder: The Swiss army knife – Introducing Our Multi-Task NLP In a bid to make life easier for NLP developers, we started to put together a toolkit which is capable of single/multi task learning and also aiding them with the development/ deployment chores. We feel it’s the ‘ swiss army knife ’ moment in our conversational AI journey. The following are some major development concerns which this toolkit addresses: 1. Enhanced maintainability of conversational pipelines Does a multitude of NLP models in your pipeline make it look like a jigsaw puzzle? Do you struggle to put the pieces together and make it work? Well, no more! With our toolkit, you can build a single model capable of performing multiple tasks hence sparing you the pain of managing ‘n’ number of models in one place. 2. Reduced GPU infrastructure consumption for deployment It takes a lot of engineering effort and infrastructure cost to deploy and maintain a pipeline containing multiple transformer-based GPU hungry models. Using this toolkit, tasks can be easily clubbed together to keep the model count low while keeping the tasks capabilities intact. 3. Faster inference for multiple conversational components Is the inference latency requirements  restricting your conversational system from having  more features/ capabilities ? We provide a simple inference pipeline which can batch (pack) inputs of multiple tasks together for simultaneous inference. This can reduce inference times significantly alongside reducing resource consumption. 4. Better learning with comparatively lesser data As the learning space for multiple tasks are shared due to the presence of a single pivot model, similar tasks put together can learn better with lesser data. There’s more! 5. Versatility to cater diverse NLU requirements . Diverse conversational use cases have diverse needs too! For example, textual entailment and intent detection have different data format requirements and belong to different task families. A down-side to this is every time a different family of tasks is to be modeled, a new setup is required to be done. Our toolkit provides versatile data formats and encoder model types to fit the diverse needs. We believe most of the NLU tasks can be accomplished if formulated as one/ multiple of these task families. a. Single sentence classification b. Sentence pair classification c. NER/ Sequence labeling 6. Easy setup of train/ infer for tasks with no need to code. So, will the toolkit be difficult to use? Not at all! One can train a model in three simple steps. All the selections, be it the data format or encoder model choice, can be done just by changing names in the task file defined for training, so no need to code for it!! And not to forget, the super simple infer pipeline which can directly load and infer upon a multi-tasks or single-task model. The Humphrey’s law Jeff Sutherland’s version of Humphrey’s Law states that “users don’t know what they want until they see working software” We’ve open-sourced the toolkit so everyone can use it for building great conversational systems. To demonstrate the effectiveness of our toolkit, we’ve curated some ready to use examples of conversational tasks, to be trained over some of the popular public corpuses. Below are some sample use-cases: 1. Answerability prediction (single task, single query classification task family ) 2. Textual entailment (single task, Query pair classification task family ) 3. Intent detection + Named entity detection + Fragment detection (Multi – task setup , multiple task types) 4. Query grammatical correctness (Single task, Single query classification task) 5. Named entity recognition + part of speech tagging ( Multi-task, Sequence labeling task family ) 6. Query type detection (single task, single query classification task family) 7. Sentiment analysis (single task, single query classification task family ) 8. Query pair similarity ( single task, Query pair classification ) Details for these can be found here . Whether you’re an AI practitioner getting started with NLP for conversational AI, or a seasoned NLP expert, you can easily build your conversational use case(s) as a single task or put multiple tasks together to train/ infer a single model. To experience how simple it’s going to be, pick up an example notebook, and get going in 3 steps without any need to code!! We’re looking forward to adding support for generative tasks as well. This will remarkably enhance the toolkit’s robustness to model conversational tasks. NLP developers can go through the step by step guide present in our documentation to understand bits and pieces of the toolkit. We encourage them to experiment around with different tasks and encoder types. In case, one faces any issue, please feel free to raise it on our https://github.com/hellohaptik/multi-task-NLP . What lies in the future? Multi-task-NLP is just one of our many initiatives in the quest to find simpler, effective ways to build better conversational AI systems. The future of AI agents is to be multilingual, multi-tasking, multimodal. Multi-task learning did open a new avenue for building efficient NLU models, but at present each task is essentially independent. For NLU-NLG duality, the Multi-task learning paradigm will need to be extended for a dependent , ordered tasks. There is empirical evidence that people with multilingual skills are better at multitasking and they can learn comparatively quicker than monolingual individuals. .As Computational neuroscience discovers better theoretical abstractions and mathematical models for language abilities of the human brain, it might pave the way for better learning algorithms and network architectures which can solve conversations at scale, with near-human accuracy. Posted by Saransh Mehta on Jul 23, 2020 7:44:00 PM", "date": "2020-07-23,"},
{"website": "Haptik-AI", "title": "Experience Of A Frontend Intern At Haptik", "author": [" Shivang Bhandari"], "link": "https://www.haptik.ai/tech/experience-of-a-frontend-intern-at-haptik/", "abstract": "Tweet I’ve been interning at Haptik for almost 10 months, as a Frontend Engineering Intern working in the Platform team. The journey has been full of learnings and ups, & downs, and has been one of the best experiences of my life so far. Through this blog, I want to share my experience and my insights on what makes Haptik one of the best startups to work with. SPOILER : This post will be more journey centric and not a guide to how you can land an internship. SimranJot Singh has been one of the people who continue to inspire my journey as an engineer and he’s been a great example of how hard work takes a person places. None of this would have been possible without him. It’s OK to ask and be WRONG When I had just joined, I was a pretty scared kid watching all these highly productive and experienced developers being Usain Bolt when it came to shipping code. All I could hear were terms which made no sense to me (Django, Celery, Message Queuing, webservers, Release pipeline, etc.) and I was bamboozled to my core. My first mistake was trying to pull things off without errors. I wanted everything to be perfect, no bugs, no wrongly named variables – without having the experience to pull off any of it. I was hesitant to ask my team to help me out with the most basic of problems I was facing ( they might think they hired a chump who doesn’t know how to code 1+1=2 ). Gradually, my team members sensed my hesitation and they just came over to help me out. I consider myself to be very lucky that I had such amazing people around me who understood what I was going through and made me realize it is OK to not know everything and its OK to be wrong and to ask for help, as that’s a part of the growth process. Till you don’t accept your own shortcomings and problems, no one can give you a solution to it . Play around Usually, whenever anyone starts off with their career in programming its specific to a single domain – that’s how you land a job in the first place. But now that you’ve already reached that stage, there’s no barrier that forces you to stick to your domain. As long as your work is delivered, you can play around with whatever you want for as long as you want, and that’s the silver lining very few people notice. It’s fun to change swings once you get settled into the routine. Make sure you challenge yourself not just to perform better at what you already do but also explore things you’ve never done before, retrospect on your experiences and learn from them. I initially joined as a Front-End Intern at Haptik but ended up working on Python Frameworks for my first month and went on to contribute to some of the fairly complex APIs written at Haptik during our platform upgradations. I explored computer vision, Backend, UI/UX a bit with the designers, usually brainstorming with them whenever a new feature’s design was being discussed, I hung out with the DevOps Engineers, solving some issues together, learning tricks and quietly understanding how to do things the DevOps way. Also, tried to touch the product side; trying new things was a lot of fun and kept the wheel rolling. T.E.A.M. (Together Everyone Achieves More) As surprising as it may sound, I was always considered as a core team member throughout my internship at Haptik (and never asked to bring coffee, contradicting the stereotype!) and I was always given the opportunity to contribute to ideas and voice my concerns. I would ask questions while brainstorming and one of the engineers of our team would help me understand how things work. Whenever I had a concern regarding product, the Program Managers at Haptik would step in and explain how things work on their end and what’s the impact of the decision being taken and so on. If I’ve raised a concern, if it’s valid it’ll be taken into full consideration. If not, someone will explain to me why it’s not the best way to do things. It is very important to trust your team members and make sure that they trust you. At the end of the day, the team is a family which runs together and has your back when things go south. Always respect and support the team. Never hesitate to ask for help, and always step up to help someone if you know you’ve got the right solution for a problem. Summary So this is my version of how things should be. Here are some thoughts you should try to keep with you, and which will help you to push through as they helped me in my rough patches: 1. The world won’t die if you make a mistake. People might get angry, you might get a little roast, but think about it, it’s worth learning. 2. If you do what you’re supposed to do on the day you don’t feel like doing it at all, make sure you reward yourself for it. No one knows the battles you’re fighting in your head better than you, reward yourself. 3. While hustling is expected from you, don’t forget to take some moments and party. It is not the mountain ahead of you that wears you out, it’s the pebble in your shoe — Muhammad Ali. I hope my experiences help you with your journey. If you ever want to strike up a conversation feel free to reach out to me here and I’ll help you with everything I can. For more wisdom read SimranJot’s Blog here , he’s shared his valuable experiences which have helped me through my intern journey. Do check out the Haptik careers page. Posted by Shivang Bhandari on Aug 2, 2019 7:33:00 PM Find me on: LinkedIn", "date": "2019-08-2,"},
{"website": "Haptik-AI", "title": "How We Simplified API Testing At Haptik", "author": [" Sanoop Nair"], "link": "https://www.haptik.ai/tech/how-we-simplified-api-testing", "abstract": "Tweet Haptik is a conversational AI company and we build Intelligent Virtual Assistant solutions (IVAs) that enhance customer experience and drive ROI for large brands. When we build these solutions for large brands, we closely integrate with their systems (via APIs) to source relevant information and provide contextual data to a customer interacting with the brand’s IVA (eg: checking available loyalty points, tracking order status, canceling of a prior order, etc) APIs are a software intermediary that allows two applications to talk to each other. In other words, it is the messenger that delivers your request to the provider that you’re requesting it from and then delivers the response back to you. Since we are relying on the client’s systems to relay information back and forth, it is very important for that system to be consistent, reliable, and respect the contract for message delivery. To deliver a high-quality solution is our goal and we take a lot of measures to ensure the systematic and performant running of our IVAs. But, no solution can be completely problem-free and we’ve had our own share of challenges while integrating with client APIs. First Problem Stateme nt We have a support team that is tasked to triage any problem faced by our customers while interacting with the IVA. Quite a few times, after analyzing the issue reported by a customer, we would find that our IVA wasn’t able to answer the customer’s query because of a client API failure: Client/Customer API being down Client API contract changed Client API didn’t return the results within a fixed expected response time To mitigate this risk when our IVA is dependent on an external system for it to be of high quality, and not have any of our customers experience a degraded product experience, we decided to build a system that would periodically perform proactive quality checks and validate the correctness of a Client API. If the API’s are not working as per the defined SLA, we would notify the client so that corrective action can be taken before a customer experiences a problem. Second Problem Statement A few channels that we use to deploy our bots on eg: WhatsApp, Facebook Messenger, can only be tested end-to-end via an API. Considering the above problems, we decided to focus our efforts on API testing. What is API testing? API testing is in many respects like testing software at the user-interface level, only instead of testing by means of standard user inputs and outputs, you use software to send calls to the API, get output, and verify the system’s response. Pros: Time Efficient: API testing is less time consuming than functional GUI testing. It requires less code and thus provides faster test coverage than automated GUI tests. Interface independent- API testing: It provides access to the application without a user interface. This ensures testing the application code to the core which helps in building a bug-free tool/application. Language-Independent: The data transfer modes used in an API test case is XML or JSON, which are language-independent. Cost-Effective: API testing provides faster test results and better test coverage, thus resulting in overall testing cost reduction. We were looking at multiple tools for our testing and finalized on karate. Why Karate? Karate Framework is the only open-source tool to combine API test-automation, mocks and performance-testing into a single, unified framework It is built upon cucumber which is a BDD framework, where we can write tests in normal - English language that anybody can easily understand. No coding knowledge required to write tests in karate. Easy to maintain. You can write custom code in java or JS and use that function/method in your feature file so it is flexible in that way. Sample test: In the below test case we are getting the user details from an API and validating the user’s name and user’s country: Java Runner File: Feature File with test cases: Folder Structure We created client wise folders containing a runner file (Java Class to run a test) and a feature file with test cases pertaining to that client. Ensuring the security of client information: We use tokens as a handshake mechanism and ensure proper authorization with a client system. We work with clients to get environment-specific tokens so that we can reduce the risk when we are testing the client APIs in a pre-production environment vs using the client API in a production environment. We take security very seriously within our development process and it was critical for us to ensure proper access to these tokens as a part of our framework when we were using it to test a client API. We created a KeyStorage.java class to manage API auth keys or any other secret tokens. In dev/staging, we have a local config.properties file and for production, we use the Environment Keys management tool to securely manage the tokens and ensure no unauthorized access or no exposure to any secrets. Depending on the environment in which you are running the test, the framework will use relevant keys. KeyStorage.java: public class KeyStorage { private static Properties prop; private static String FILE_NAME = \"config.properties\"; private static KeyStorage single_instance = null; private static String currentEnvironment; private KeyStorage() { } // static method to create instance of Singleton class public static KeyStorage getInstance(String environment) { if (single_instance == null) { single_instance = new KeyStorage(); currentEnvironment = environment; if (!\"production\".equalsIgnoreCase(currentEnvironment)) { try { InputStream input = new FileInputStream(FILE_NAME); prop = new Properties(); prop.load(input); } catch (Exception e) { e.printStackTrace(); System.out.println(\"Exception occured while reading from properties file\"); } } } return single_instance; } public String getValue(String key) { if (!\"production\".equalsIgnoreCase(currentEnvironment)) { return prop.getProperty(key); } else { return System.getenv(key); } } config.properties: FACEBOOK_USER_API_GUID=sdsdjuhcksidsdsdsduhdjsASDSD FACEBOOK_USER_API_AUTHTOKEN=842djhsgsdg82u= karate-config.js: config.getKeyValue = function(keyName) { var KeyStorage = Java.type('implementation.KeyStorage'); var ks = KeyStorage.getInstance(env); return ks.getValue(keyName); }; Reporting: There is a concept of ‘hooks’ in cucumber and it is supported in karate to some extent. Hooks, which are blocks of code that run before or after each scenario. You can define them anywhere in your project or step definition layers, using the methods @Before and @After. Cucumber Hooks allows us to better manage the code workflow and helps us to reduce code redundancy. To track the failures of any API, we used an ‘afterscenario’ hook of karate that executes after every test scenario and checks if the karate.info.errorMessage is null or not. If the errorMessage is not null that means the test case is failed. At Haptik , we use Sentry as Application Monitoring and Error Tracking Software. If any test case fails in production, we log the error message in Sentry and alert relevant teams. var afterScenarioHook = function() { // the JSON returned from 'karate.info' has the following properties: // - featureDir // - featureFileName // - scenarioName // - scenarioType (either 'Scenario' or 'Scenario Outline') // - scenarioDescription // - errorMessage (will be not-null if the Scenario failed) var info = karate.info; var errorMessage = info.errorMessage; var scenarioName = info.scenarioName; var featureFileName = info.featureFileName; if (info.errorMessage != null) { var msg = \"'\" + scenarioName + \"' in '\" + featureFileName + \"'failed. Error Message: \" + errorMessage; karate.log(msg); var SentryLogger = Java.type('implementation.SentryLogger'); var logger = SentryLogger.getInstance(env); logger.sendEvent(msg); } else { var msg = \"'\" + scenarioName + \"' in '\" + featureFileName + \"' passed\"; karate.log(msg); } } public class SentryLogger { private static SentryLogger single_instance = null; private static String currentEnvironment; private SentryLogger() { } // static method to create instance of Singleton class public static SentryLogger getInstance(String environment) { if (single_instance == null) { single_instance = new SentryLogger(); currentEnvironment = environment; if (\"production\".equalsIgnoreCase(currentEnvironment)) { KeyStorage ks = KeyStorage.getInstance(environment); Sentry.init(ks.getValue(\"SENTRY_DSN\")); Sentry.getContext().addTag(\"environment\", environment); } } return single_instance; } public void sendEvent(String message) { if (\"production\".equalsIgnoreCase(currentEnvironment)) { Sentry.capture(message); } else { System.out.println(\"&#91;SENTRY] \" + message); } } } For dev/staging, we would generate an HTML report. The report shows a list of tests that were run from an individual feature file, test completion status, time taken to complete the tests, etc Running the tests Manually via the command line Execute the karate tests for specified runner file: mvn clean test -Dtest=<name of the runner file> Karate.options gives an option to run a specific feature file or a folder in which all the feature files are contained @tags feature allows users to run some particular tests or skip some tests  in a feature file. You can specify the tag name in the runner file and karate would execute only that test which has that tag in the feature file and skip the rest. mvn test -Dkarate.options=”–tags @SmokeTest classpath:src/implementation/facebook/” -Dtest=FacebookTestRunner Feature file: Karate.options gives an option to skip a specific feature file. Eg: @ignore (or any other tag-name) added on top of the Feature File and adding ~@<tag-name> in the command mvn test -Dkarate.options=”–tags ~@ignore classpath:src/implementation/facebook/” -Dtest=FacebookTestRunner Users can also specify the environment on which the tests are running and write custom code for each environment in the karate-config.js file. mvn test -Dkarate.env=<name of the environment> -Dtest=<name of the runner file> var env; env = java.lang.System.getenv('ENVIRONMENT'); if (!env) { env = karate.env; // get system property 'karate.env' if (!env) { env = 'dev'; } } karate.log('karate.env system property was:', env); var config = { env: env, myVarName: 'someValue' } All the above is automated via Jenkins. We can also schedule to run the tests periodically using a CI/CD tool. In our case, we used Jenkins to run the tests every hour. The Jenkins job would download the latest version of code from git, run the test, and send the mail attaching the report to the concerned team. Fetching the latest code from the repository Triggering the runs every hour Running the test using maven command As a part of the Post Execution, we are attaching the test report and sending it via email. Conclusion: At Haptik, teams were not using any specific tool for API testing – few APIs were tested using custom tools, some using Postman or some APIs were not being tested at all. Karate makes API testing simple and it is integrated with the BDD framework which makes reading and understanding tests easier. Our plan is to implement karate for all our API testing within engineering. Along with doing production regression testing, we also plan to integrate it with our PR merge process via Jenkins. This blog was co-authored by @saumil-shah References: Karate doc: https://intuit.github.io/karate/ Posted by Sanoop Nair on Aug 18, 2020 9:55:00 AM Find me on: LinkedIn", "date": "2020-08-18,"},
{"website": "Haptik-AI", "title": "Audit User Activity in the System", "author": [" Prathmesh Ghadge"], "link": "https://www.haptik.ai/tech/audit-user-activity/", "abstract": "Tweet For a company to remain agile, engineers have access to multiple servers across various environments. This helps people be more independent and reduce dependencies on other teams. While this is helpful, it is important to have checks and controls in place that will prevent people from abusing this. At Haptik, to control this we wanted to monitor all user activity on all servers. We also log all important activity happening on the platform . For that purpose, we set up a pipeline to collect logs and push them on to a common dashboard for auditing purposes. I will discuss a simple pipeline with you all in this blog. Types of Activity Logs 1. User activity logs on servers (SSH and initiated commands, files edited, etc.) 2. User activity on our Bot Builder Platform (Who edited what) 3. User activity inside Python Shell (IPyhton logs) Technologies Used Ansible [v2.7] Ansible is an open-source software provisioning, configuration management, and application deployment tool. It just requires that systems have Python (on Linux servers) and SSH. Filebeat Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify collects log events and forwards them to either to Elasticsearch or Logstash for indexing. R-ELK Stack [v6.x] R-ELK is the acronym for three open source projects: Redis, Elasticsearch, Logstash, and Kibana. “Redis” is used as a buffer in the ELK stack. “Elasticsearch” is a search and analytics engine. “Logstash” is a server‑side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a “stash” like Elasticsearch. “Kibana” lets users visualize data with charts and graphs in Elasticsearch. ELK stack setup steps are present here . Pipeline As shown in the above diagram, all the user activity data is collected and push to our central logging ELK server. Creating users for servers is controlled through Ansible and a Jenkins job that helps us do that. We also, use OpsWorks to create users on AWS machines but wanted to use a solution that is more cloud agnostic. For our platform, we have a separate permissions model. What the user does on the platform, we try to push some important types of the user activity log and helps us audit who did what. (Who made changes to what bot, some of this is still WIP). Steps to Setup 1. User Creation on Servers We use Ansible to manage users on all our VMs (servers). All the users that we create are added to the developer’s group. - hosts : all connection : ssh user : ubuntu gather_facts : yes strategy : free vars_files : - users . yml tasks : - name : Add users and configure ssh access user : name : \"{{item.name}}\" groups : developers , docker state : \"absent\" shell : \"/bin/bash\" with_items : \"\" ignore_errors : yes become : true become_method : sudo - name : Add ssh keys for users authorized_key : user : \"{{item.name}}\" state : \"absent\" key : \"{{item.key}}\" with_items : \"\" ignore_errors : yes become : true become_method : sudo users : - name : prathmesh key : \"ssh-rsa AAAAB3Nza....userh@haptik.com\" hosts : [ \"host_name\" ] The above is available as a Jenkins job. We use it to create any user and give him/her ssh access to specified servers. The inventory file & the permissions file is maintained by us on a different reliable data store. We can control what type of access the users will have on the servers. 2. Bash History Setup Following are the steps to consolidate bash history for all the users into a single file : Edit the system-wide BASH runtime config file: sudo - e / etc / bash . bashrc Append the following command to the end of that file : export PROMPTCOMMAND = 'RETRNVAL=$?;logger -p local6.debug \"$(whoami) [$$]: $(history 1 | sed \"s/^[ ]‌*[0-9]‌\\+[ ]*//\" ) [$RETRN_VAL]\"' Set up rsyslog based logging with a new file: sudo - e / etc / rsyslog . d / bash . conf Contents: local6 . * / var / log / commands . log Restart rsyslog: sudo service rsyslog restart Configure log rotation for the new file: sudo - e / etc / logrotate . d / rsyslog Append the following to the end of that file : /var/log/commands.log 3. auditd Setup Set up auditd on all the servers sudo apt - get update sudo apt - get install auditd Configure auditd rules sudo vim / etc / audit / audit . rules Contents: Sample conf here . Restart auditd sudo service auditd restart Some more about auditing Linux servers here . 4. Filebeat Setup Install filebeat Configure filebeat #=========================== Filebeat prospectors ============================= filebeat . prospectors : - input_type : log # Paths that should be crawled and fetched. Glob based paths. paths : - / var / log / commands . log - / var / log / auth . log # - <path to python_shell logs> #-------------------------- Redis output ------------------------------ output . redis : hosts : [ \"<ELK_Server_IP>\" ] key : \"filebeat\" db : 1 timeout : 5 logging . level : debug filebeat . modules : - module : auditd log : enabled : true var . paths : [ \"/var/log/audit/audit.log\" ] 1. Python shell logs are set up separately via Code. We use iPython settings to create the logs for shell sessions, write it to a file and then Filebeat pushes the logs to the ELK server. 2. audit logs for the application/bot platform are also written to a file on the host machines which we again push to the same ELK stack. That has a separate Filebeat which runs on each and every application server. Restart Filebeat: sudo service filebeat restart 5. Logstash Setup We are going to leverage Logstash from the ELK server. Following are the steps to configure Logstash for audit logs : Create a new configuration file: sudo vim / etc / logstash / conf . d / audit . conf Contents : input { redis { host = > \"127.0.0.1\" port = > 6379 data_type = > \"list\" key = > \"filebeat\" codec = > json db = > 1 } } filter { if [ source ] == \"/var/log/audit/audit.log\" { grok { match = > { \"message\" = > [ \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): %{GREEDYDATA} pid=%{NUMBER:[auditd][log][pid]} res=%{WORD:[auditd][log][res]}(\\')?\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): pid=%{NUMBER:[auditd][log][pid]} uid=%{NUMBER} old auid=%{NUMBER:[auditd][log][old_auid]} new auid=%{NUMBER:[auditd][log][new_auid]} old ses=%{NUMBER:[auditd][log][old_ses]} new ses=%{NUMBER:[auditd][log][new_ses]}\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): %{GREEDYDATA} acct=\\\"%{DATA:[auditd][log][acct]}\\\" %{GREEDYDATA} addr=%{IPORHOST:[auditd][log][addr]} %{GREEDYDATA} res=%{WORD:[auditd][log][res]}(\\')?\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): (%{GREEDYDATA})?a0=\\\"%{DATA:[auditd][log][a0]}\\\"( %{GREEDYDATA})?\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): (%{GREEDYDATA})?a0=%{WORD:[auditd][log][a0]} %{GREEDYDATA} items=%{NUMBER:[auditd][log][items]} ppid=%{NUMBER:[auditd][log][ppid]} pid=%{NUMBER:[auditd][log][pid]} (%{GREEDYDATA})?comm=\\\"%{DATA:[auditd][log][comm]}\\\" %{GREEDYDATA} (%{GREEDYDATA})?key=\\\"%{DATA:[auditd][log][key]}\\\"\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): %{GREEDYDATA} acct=\\\"%{DATA:[auditd][log][acct]}\\\" %{GREEDYDATA} res=%{WORD:[auditd][log][res]}(\\')?\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): %{GREEDYDATA} res=%{WORD:[auditd][log][res]}(\\')?\" , \"type=%{WORD:[auditd][log][record_type]} msg=audit\\(%{NUMBER}:%{NUMBER:[auditd][log][sequence]}\\): (item=%{NUMBER:[auditd][log][item]} )?%{GREEDYDATA}\" ] } } mutate { remove_field = > [ \"message\" ] update = > { \"type\" = > \"auditd\" } } } if [ source ] == \"/var/log/commands.log\" { grok { match = > { \"message\" = > [ \"%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:syslog_hostname} %{DATA:user}: %{DATA:group} \\[%{NUMBER:pid}\\]:\\s*%{NUMBER}\\s*%{DATA:command}\\s*\\[%{NUMBER:command_exit_code}\\]\" ] } } mutate { remove_field = > [ \"message\" ] update = > { \"type\" = > \"commands\" } } } if ( \"shell_\" in [ source ] ) { #TODO #Improve the grok filters by combining them if possible grok { match = > { \"message\" = > \"%{GREEDYDATA}\" } } grok { match = > { \"source\" = > \"%{GREEDYDATA}shell_%{DATA:user}.log\" } } mutate { update = > { \"type\" = > \"shell\" \"user\" = > \"%{user}\" } } } if [ source ] == \"/var/log/auth.log\" { grok { match = > { \"message\" = > [ \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\\[%{POSINT:[system][auth][pid]}\\])?: \\s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\\[%{POSINT:[system][auth][pid]}\\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\\[%{POSINT:[system][auth][pid]}\\])?: new user: name=%{DATA:[system][auth][user][add][name]}, UID=%{NUMBER:[system][auth][user][add][uid]}, GID=%{NUMBER:[system][auth][user][add][gid]}, home=%{DATA:[system][auth][user][add][home]}, shell=%{DATA:[system][auth][user][add][shell]}$\" , \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{GREEDYMULTILINE:[system][auth][message]}\" ] } pattern_definitions = > { \"GREEDYMULTILINE\" = > \"(.|\\n)*\" } remove_field = > \"message\" } mutate { update = > { \"type\" = > \"auth\" \"user\" = > \"%{user}\" } } date { match = > [ \"[system][auth][timestamp]\" , \"MMM  d HH:mm:ss\" , \"MMM dd HH:mm:ss\" ] } } } output { elasticsearch { hosts = > [ \"0.0.0.0:9200\" ] index = > \"audit-%{+YYYY.MM.dd}\" } } As per the requirement, you can add different indexes for different data. For more understanding of output/input visit Elastic’s website. 6. Viewing it on Kibana Dashboard Below are the steps to view logs: 1. Go to ELK server’s Kibaba URL. 2. All the logs are pushed to audit-* index on ES 3. All auditd logs are sent with type: auditd 4. All Bash command history logs are sent with type: commands 5. All Python shell logs are sent with type: shell 6. All SSH auth logs are sent with type: auth Hope this helps you set up a similar pipeline for you as well. This is a very high-level overview of a data pipeline to collect user activity log. We have internally put more audits in place which we will share soon. You can even push these logs to archived long-term storage like AWS S3, Azure Blob Storage, etc. and this data can be fetched as and when required. We are hiring. Please visit our careers page and let us know how you are going about maintaining security practices at your company. We will soon be back with a much detailed blog around what other practices we are following. Posted by Prathmesh Ghadge on Jul 26, 2019 6:57:00 PM Find me on: LinkedIn Twitter", "date": "2019-07-26,"},
{"website": "Haptik-AI", "title": "CustomType, An Android Library For The Typography Enthusiast", "author": [" Raveesh Bhalla"], "link": "https://www.haptik.ai/tech/customtype-android-library", "abstract": "Tweet All across our tech stack at Haptik, we’ve benefitted enormously from the Open Source community. And every one of us, right from day 1, had a personal goal of wanting to give back in some form, big or small. Today, we have the privilege of announcing our first contribution: CustomType for Android. Being a messaging app, typography plays an important part of our design. However, using custom typefaces within an Android app isn’t easy: there are limitations which prevent us from specifying the typeface in the layout XML file itself, and also performance issues to be handled while creating the Typeface objects in the Java code. To handle these issues, we had written CustomType in the middle of last year. While in truth it’s a very, very small library (the aar is only 5kb), it’s impact is huge since it handles the two issues outlined earlier. And it does so with minimal programming. In fact, while implementing in Java, CustomType decreases the amount of code you need to write. All you need to do is <span style=\"color: #a9b7c6;\">view.setTypeface(CustomType.</span><span style=\"color: #a9b7c6;\"><i>getTypeface</i></span><span style=\"color: #a9b7c6;\">(</span><span style=\"color: #cc7832;\">this,</span><span style=\"color: #6a8759;\">\"fonts/typeface.ttf\"</span><span style=\"color: #a9b7c6;\">))</span><span style=\"color: #cc7832;\">;</span> You can also use custom typefaces from the XML files, though this would require you to use a bundled custom TextView instead of the standard TextView. <span style=\"color: #e8bf6a;\">&lt;in.raveesh.customtype.TextView </span> <span style=\"color: #bababa;\">android:text=</span><span style=\"color: #a5c261;\">\"@string/lorem_ipsum_short\" </span> <span style=\"color: #bababa;\">android:layout_width=</span><span style=\"color: #a5c261;\">\"wrap_content\" </span> <span style=\"color: #bababa;\">android:layout_height=</span><span style=\"color: #a5c261;\">\"wrap_content\" </span> <span style=\"color: #bababa;\">app:typeface=</span><span style=\"color: #a5c261;\">\"fonts/RobotoCondensed/Light.ttf\" </span> <span style=\"color: #bababa;\">android:textSize=</span><span style=\"color: #a5c261;\">\"@dimen/typesize_headline\" </span> <span style=\"color: #bababa;\">android:textColor=</span><span style=\"color: #a5c261;\">\"@color/text_primary_light\"</span><span style=\"color: #e8bf6a;\">/&gt; </span> We’ve also bundled a few useful resources, such as Lorem Ipsum of three different lengths, and the type sizes as per the Material Design spec and colors that the spec recommends for type. To get started with CustomType, head over to the repository on Github . Posted by Raveesh Bhalla on Jan 13, 2015 5:41:00 PM", "date": "2015-01-13,"},
{"website": "Haptik-AI", "title": "The Haptik Code Executor", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/haptik-code-executor", "abstract": "Tweet Virtual Assistants built today are expected to not just help with frequently asked questions but actually furnish end to end query resolution. In order to be able to provide a holistic & unified experience to end-users, virtual assistants need to be able to carry out these tasks instead of guiding the user or providing a DIY manual. To support the execution of such activities, Virtual Assistants require deep integrations with multiple systems & tools. The user’s intent triggers a flurry of activities in the background which are executed by the virtual assistant in real-time. The user only has to interact with the virtual assistant and the virtual assistant does the job of interfacing with various systems – CRM, LMS, IAM with the help of APIs. Integrating APIs is a time consuming and yet unavoidable activity in the process of building virtual assistants. Integrating these APIs also required to follow a full SDLC (Software Development Life Cycle). At Haptik, we soon realized that one of the biggest bottlenecks the bot developers faced was the requirement of infrastructure to run code that could integrate these APIs into the IVA. We spent the next 3-4 days in research and came up with a working solution of overcoming these problems. We wanted to find out ways to overcome this bottleneck while still maintaining proper logging, error handling, and reducing the time and effort. FAAS – Functions as a Service is something what we wanted to build out and integrate. Below is all the Research we conducted and how we finally ended up building the solution. Some of the paragraphs in the blog would also show how we were thinking about each and every phase etc. Research Phase This particular project was divided into 7 major parts: 1. Identifying the backend tool to execute the code. 2. We would go ahead with already available tools or services in the first phase, with refactoring and improvements in V2. 3. Identifying API contracts for this microservice ( It ended up being a nano-service ). 4. Move existing code in our Integration repository to this setup via backend with the help of our Implementation team. 5. Sandboxing the code executor (for different environments, support for frameworks, etc.). Building both testing and production executors. 6. Make it easy to add and maintain libraries 7. Returning an API endpoint which will run on certain triggers during IVA flow 8. Making this process simple even for a non-developer. Criteria for Technology selection 1. Initial Setup time & Management of the new service2. Costs 3. Creation of function and easy execution of code 4. Easy integration with our IVA/Bot builder 5. Scalability 6. Ease of Deployment of new code & Monitoring 7. Security We came up with 7 possible solutions. We will start with solution 2-7 and then come to solution 1 which we in the end also locked down on. The below tools are actually amazing to solve many use-cases. In this particular scenario did not suit our use-cases. Solution 2 – AWS Cloud 9 This is AWS’s Cloud code Editor solution where you can collaborate on writing code and finally deploy the code as an API Gateway Endpoint, which can be put directly into our Dialog Builder tool while building an IVA. Pros Simple to use Cons 1. AWS console access required for everyone who is Building an IVA and wants to integrate Code/APIs. 2. Manual effort of publishing to API Gateway. 3. Management overhead. 4. You can read more about Cloud9 here . Solution 3 – OpenWhisk (4k+ stars on GitHub) OpenWhisk is an Apache Incubator Project. Below is based on when we were researching. Pros 1. Open-source 2. IBM uses this on their platform. 3. New code could be added via an API and that piece of code can be called via an API. 4. Support for logging available out of the box. 5. Supports environment variables and custom packages. Cons 1. No good UI to manage. 2. Infrastructure management – When new functions are created, it creates Docker containers in the backend and they require infrastructure to run. Not just maintenance but scaling would need to be thought out. 3. Open-source, and requires regular upgrades, which could become a significant overhead given that the team is quite lean. 4. OpenWhisk does not provide versioning by default. The application code will have to handle it by giving an appropriate package and action name. If you want more details around OpenWhisk, do mention in the comments, or connect with me. I would be happy to share more details. We got a prototype of this up and running and got 20 sample functions running as an API as well. Solution 4 – Fission (4k+ stars on GitHub) Fission is a framework for serverless functions on Kubernetes – Add Link. This was another open-source solution that was being used by the community. It had a great UI and made management easier as well. The cons were similar to OpenWhisk on the management side. Pros 1. 100 ms cold start seemed to be quite fast. 2. Maintains a 3 pool container environment by default. 3. Support for custom container images on the go. 4. Full Kubernetes support & namespaces. 5. UI support Cons 1. Management & Infrastructure 2. Keeping up to date with the latest releases. 3. HTTP API for getting metadata missing. So, if I need details for a function I created, it can be done via a subprocess call. Fission CLI needs to be present. Solution 5 – OpenFaas (15k+ stars on GitHub) OpenFaaS is Kubernetes-native and uses Deployments, Services, and Secrets. For more detail check out the “faas-netes” repository. Pros 1. With UI 2. Available as a Managed service OTB – Costs higher Solution 6 – Kubeless (5k+ stars on GitHub) Kubeless is a Kubernetes-native serverless framework that lets you deploy small bits of code (functions) without having to worry about the underlying infrastructure. According to them, they are better than Fission and OpenWhisk. Again similar Pros and cons like other tools we discussed above. Finally, we dropped it. Solution 7 – Simple Docker containers running code Containers could be used with ECS task scheduling which is triggered when API call is done, OR by keeping the task running at all times. However, this defeats the purpose of going serverless, so we dropped this idea as well. And now for the Final Solution and the solution which was the first we came up with: Solution 1 AWS Lambda + API Gateway Using AWS Lambda reduced the overhead of managing infrastructure and going fully serverless. We finally chose to go ahead with the AWS Lambda, API Gateway based Hybrid solution. We have already done most of the research here and things seem to be working. The way this would work is : IVA/Bot Builder -> Machine Learning Pipeline (execution trigger) -> Code-Executor service -> AWS API Gateway -> AWS Lambda We divided the project into 2 major phases. The first phase was the Rollout with the basics of code execution. The second phase was to integrate without Dialogue Builder so that any IVA builder could easily use this and view logs for the same. Phases of Development Phase v1.0 Outcome : Should be able to write an integration function and execute it on a FAAS. No versioning will be supported in V1. 1. List existing integration functions we could port to code executor first 2. Get an idea of what all resources and packages are needed by integration functions. 3. Integrate this function with the pipeline and probably test with the bot 4. Fix Container image / Layer of packages that are to be used (Packages basis above research) 6. Test functions locally 7. Limit amount of resources a function can use CPU, Memory, Disk 8. Stdout of the lambda function (API Gateway identifier) 9. Show current code in the UI as well Additionally, some good-to-haves: 1.How can we inject environment variables? (ENV Manager/KMS/Vault etc.) 2.Bot/IVA level store env keys on Mogambo or an ENV 3.How do we access Datastores? 4.Come about with a deployment plan 5.Monitoring for execution failures We really wanted to get v1 for code executor up and running within a few days. So for this, we discussed that we should go with a more cloud agnostic open-source method but on further evaluation, we realized that handling deployment for these open-source options is quite complex to get started with. This is a process that is typically done by companies like IBM which have very large teams. And also the open-source methods sometimes lead to management overhead. Technical Implementation The major change was that we deployed the backend for this code executor pipeline in a separate repo as a separate nano-service. This was a Django project and could be deployed on any cloud platform later in a scalable way. The repo mostly contains all Boto3 code and some pseudocode which we took reference from a couple of places which saved us some time. The deployment solution for this service is a Hybrid one. We planned to deploy this on AWS itself and also test it out on Azure on-the-side. We are on our Journey to migrate lot of our Workloads to Azure so we wanted to try out how this would work in an Azure specific setup. (Later in version 2/version 3 we can try and move to Azure Functions etc. Python3 support was in Beta on Azure Functions that time) Also, later if we were to move to open-source methods, we can directly make changes in this service itself while still maintaining the API contracts our Bot Builder was expecting. The whole idea is to get V1 out there ASAP. Phase v2.0 Outcome : Fully functional code executor with least management overhead. 1. Figure out versioning & Scaling & Executability 2. Logging – Viewing logs of the Code for testing and also when it runs in production 3. Libraries/Methods Pipeline – Devs can keep adding packages 4. We built a common Haptik Code Library Repository to handle this. 5. Local Testing – Dev testing and logs like Mogambo 6. Versioning for Functions and APIs 7. Give developers a standard Docker image they can use to test functions () 8. Datastore method to access Datastores Code Executor Flow & Consumption of APIs Initial flow 1. The UI interface, (Bot Builder for now) has a code block. The IVA Builder user will paste 2. Python Code inside the block and click save. 3. On the UI interface, there is a Syntax checker that will check the code Syntax and save the code thereafter. 4. Environment variables are passed along with the code save. 5. On Clicking save, the Code executor create_function API call is made which: 6. Pushes the saved code to S3 as a ZIP & create a function on Lambda along with the CloudWatch logging setup. 7. Also, using CloudFormation, an API gateway stack is brought up and API URL is returned as HTTP response as described in our internal documentation. 8. The API URL is the one and only way to execute the function. POST call to API URL for invoking the Lambda Function. 9. Execution Logs of every Code Executor blog is visible on our IVA Builder itself. Some Problems that we faced 1. Default limits of AWS on number of Lambda functions that can be created 2. Default limits of AWS on the number of API Gateway endpoints that can be created 3. Adding/removing libraries on the go in the Layers of Lambda. Some additional changes we made: Dropped usage of API Gateway Instead of executing the API Gateway, using restricted AWS credentials, we build functionality to trigger the Lambda function directly, and this way we never had to expose any public/ private endpoint. The function name was anyways available to us in our IVA/Dialogue builder tool. New flow Support to make Lambda functions run within the VPC Earlier all functions were executed outside the VPC and whitelisting for the outgoing IP via NAT could be done. Running within VPC has some cold start issues but things have been working fine so far, and creating functions inside VPC is optional. Stats 1. We have over 5000+ code executor blocks in our IVA builder and all run at scale without any intervention from our Engineering teams. 2. A few $$s spent for millions of executions per month. 3. Accessible for our cloud Agnostic platform deployments on Azure too 4. We are planning to add more security-related features and easier ways to add more packages and support even more frameworks in the future. See the code executor in action & how we have productized this further here . Further, I wanted to thank @viraj-anchan for his valuable contribution to this Project. Posted by Ranvijay Jamwal on Jun 29, 2020 7:43:00 PM Find me on: LinkedIn", "date": "2020-06-29,"},
{"website": "Haptik-AI", "title": "The Haptik Open Source Challenge", "author": [" Raveesh Bhalla"], "link": "https://www.haptik.ai/tech/open-source-challenge", "abstract": "Tweet As we’ve said in the past, we at Haptik are highly grateful for the open source community for the role it has played in getting us where we are, and are always looking to contribute back with work such as Pacemaker. However, while we’ll continue shipping code, we really would like others to get involved as well. For this reason, we’re launching the Haptik Open Source Challenge. Every once in a while, we’ll pick up one of our libraries and invite the community to contribute to it. Contributors stand the chance of winning cash prizes and jobs at Haptik. Today (September 17, 2015) we’re starting the first Open Source Challenge, inviting contributions for Pacemaker. The library helps fix a severe bug with Google Cloud Messaging, which more and more apps are relying upon these days. For details on the bug and the library, we recommend visiting the GitHub page . Your goal, if you do accept the challenge, is to build out a functionality that allows multiple apps on the device to use Pacemaker, without harming the user’s battery life. You need to devise the most appropriate strategy, whose goal is to minimize heartbeats sent, while ensuring all applications on the device function properly. We have set a cash prize of Rs 5,000 for the most significant contribution to this challenge. The contributions will be judged on the algorithm, completeness, and ultimately whether or not we believe the pull request is ready to be merged. Additionally, we would be offering interviews to a large number of contributors to join our Android development team. So, if you are interested in joining our team, this is a great way of getting yourself noticed. The challenge is open until 12 noon on 21st September, 2015. So go ahead, fork us, do a good deed, and once you’re done, submit a pull request. If you have any questions, E-mail me (Raveesh Bhalla) or reach out to me on Twitter . Posted by Raveesh Bhalla on Sep 17, 2015 5:59:00 PM", "date": "2015-09-17,"},
{"website": "Haptik-AI", "title": "Netlify = Frontend Productivity++", "author": [" Shivang Bhandari"], "link": "https://www.haptik.ai/tech/netlify-frontend-productivity", "abstract": "Tweet At Haptik , we recently added Netlify to our frontend CI pipeline for Front-End Pull Requests and I’m going to walk you through how it has helped us increase developer productivity for Frontend Engineers. The Problem When one looks at a PR, how does one know what it would look like if the PR were to be merged? Every Front-End change, no matter what the size, has an impact either on the UI or the UX of a product. When building on a product that has matured over the last 2–3 years, a developer might not always know what repercussions of a small change could be. Sometimes you might have a task to redesign an existing component/feature, and your PR would include a lot of CSS changes. To the person reviewing the PR, these are changes which the reviewer/tester can’t visually verify and they need to trust you with your implementation. Moreover, let’s say these changes do get merged, but the design/product team then finds some issues with it. In that case, the whole process repeats of raising a PR, changes that are not verifiable to the reviewer, etc. It is also a very tedious process for a QA to verify the changes and ascertain that the feature has been built properly. Netlify to the Rescue The fact that setting up Netlify is very easy and developer-friendly is a huge advantage. To begin with, it is also free to use. Platform Tools at Haptik connect and rely on backend services for the Data. Each developer has a server assigned, with the help of which we go about building features on a day to day basis. Sometimes, there are features that require both Front-End and Backend changes and these are usually tested by pulling the backend changes on the server and linking the front-end changes to it. Now, once a developer raises a pull request, Netlify allows us to track changes and make builds per commit on the PR. We deal with over 20–30 commits, which get tested in a day on Netlify builds, and are then finally merged in our codebase. Netlify links point to the developer’s dev server. At Haptik, all developers have their own development machines so that they can run and develop applications with all dependencies taken care of. Also, for each new commit, Netlify automatically builds the changes and serves them: In our experience, it has increased our productivity by a decent amount and we are able to function with more efficiency and fewer conflicts. It surely has been a good addition to the process for the Frontend Team at Haptik. Netlify Flow Netflix has improved the overall speed of testing and pushing changes to production. The total time saved per PR change is around 10-15 minutes; thereby also reducing the number of bugs being pushed to production. We have seen a reduction of 50% of the bugs in production in the last 3-4 months. It also gives the Product managers a chance to see how the new feature would look to the end-user. Below is a basic flow of how the pipeline for Frontend works: Impact Summary Process Refinement Before introducing Netlify builds, we would often run into scenarios where the shared branch we use to release our tools would end up with some untested code because we lacked a resource to test it out before merging it into the branch. As a result, the branch would get blocked if there were any bugs and releases would be blocked as well. After Netlify, we were able to make the shared branches cleaner. Reduced Bug Rates Pushing changes to the release branch without a proper testing environment would often lead to bugs that aren’t accounted for. With Netlify we were able to make the shared branches bug-free with proper testing before the PR gets merged, making it easier to release rapidly and reducing the chance of shipping any missed bugs drastically. Reduced Conflicts: As every Netlify build on a PR links the Frontend to a specified server, it reduces the risk of Backend and Frontend branches conflicts and provides a reliable build to test the changes. To learn more about how to integrate Netlify in your repository you can go through their guide . Thanks for reading! If you find this story helpful, please click the 👏 button and share it to help others find it! Feel free to leave a comment 💬 below. We are hiring. Do get in touch with us via hello@haptik.ai or visit our careers page. Posted by Shivang Bhandari on Dec 19, 2019 7:38:00 PM Find me on: LinkedIn", "date": "2019-12-19,"},
{"website": "Haptik-AI", "title": "Haptik’s Analytics Dashboard For Chatbots", "author": [" Deep Singh Baweja"], "link": "https://www.haptik.ai/tech/analytics-dashboard-chatbots/", "abstract": "Tweet At Haptik, we build new chatbots on a daily basis. These chatbots generate data, lots of it. Data including, but not limited to, what messages were exchanged, what data elements were used by the bot to respond appropriately and what problems were detected along the way. This amounts to huge volumes of data generated daily. Clearly, there is a need to interpret that data into meaningful, insightful analytics This necessity led to the development of our very own data-driven analytics tool, an Analytics Dashboard . In this blog, we will talk about the capabilities of our Analytics Dashboard and what went into developing it. What is a Data-Driven Approach? Our Analytics Dashboard is a data-driven tool, one focused on information that shows its users the behaviour and patterns created by all their end users. By observing the data constantly, it becomes possible to gain real insights of what works and what does not, what people use and what is ignored, eventually helping us in making really important strategic business decisions. This dashboard is currently our source of truth , it is the go-to place to analyze and observe exactly what’s transpiring between our end users and all our chatbots. Analytics at a Glance We have a lot of data generated by various parts of our chatbot environment, all pushed to an AWS Kinesis Stream ; data from this stream is then pushed to AWS Elasticsearch using Logstash . The goal of this tool is to make sense of all the data generated by our chatbots across various businesses and clients. Businesses include but are not limited to bots created for our own Haptik App and for external clients. All the data visible inside the tool is filterable based on selected businesses, bots and over a time range. For Example, we have an entertainment bot, which chats with users, sends across jokes, horoscopes basically spreading happiness. This bot falls under the business of the Haptik app. Now we can see which components of the bot are used most frequently, what new detected keywords are causing the bot to break, what’s working and what’s not for a given time range. Tech Stack The main technologies used to create this tool are as follows: 1.Back End : Python Django , our middleware framework of choice. 2.DataBases : 1. We use Elasticsearch to store our chat/message/event data across indexes, we perform most of our data crunching at this level. 2. ElasticSearch is an open source, a RESTful search engine built on top of Apache Lucene and released under an Apache license. It is Java-based and can search and index document files in diverse formats. Refer here for Elasticsearch: elastic.co 3. We do have MySQL , but mainly to fetch user permissions and basic business information. 4. Redis is used to make access to static data, optimizations quick. 3. Front End : Mainly consists of ReactJs with Flux, along with a couple of graph plugins such as: 1. React Charts Js: https://github.com/reactjs/react-chartjs 2. React Graph Js: https://github.com/crubier/react-graph-vis 3. Ant as our UI Framework: https://ant.design/docs/react/introduce 4. Webpack to help with the build If you would like to know why we choose ReactJs for our tools, please refer to the blog here . Overall Architecture Our overall architecture is MVC based (not including various AWS components): What Analytics Do We Provide? Our overall analytics is divided into the following major components: Trends 1. Under Trends we provide aggregated information such as Total conversations, Total users and Overall automation % which basically indicates the success metric of our bots to respond to user queries without human interference 2. We also provide a graph which is data aggregations based on time, hourly or daily depending on time range selected, this is a real-time graph. Story Analysis 1. Here, we provide information as to which component (story/node) of the chatbot is invoked, how many times, along with the % of success accuracy. 2. We also provide a pie chart depicting the same information for better clarity of overall node usage across the bot. 3. Selections in the table will re-render the pie charts accordingly. Message Analysis 1. Pretty straightforward, here we provide information about what messages were sent by the user/bot/human as and when they were sent. 2. We know everybody loves excel sheets, and this data is downloadable if you have appropriate permissions Word Analysis 1. We provide a word cloud of all the keywords (important words) that were not understood by the chatbot accurately 2. They are ordered and sized with respect to the frequency of occurrence. This is easier to read and more useful as compared to the traditional fancy word clouds. It’s good to be practical sometimes. 3. This data helps us to improve the chatbot and increase its understanding so as to reply more accurately next time onwards User Journey 1. Here we provide the actual mapping of chatbot responses that were required to complete a conversation. This shows us how often certain nodes are invoked, and wherein the chat flow they most commonly appear. 2. This mainly helps us to improve the chatbot at a conversation level. What Did We Learn From This? Elasticsearch is perfect for data which does not get manipulated. Sure, it’s a little expensive when it comes to memory utilization, but its beauty lies in its extremely optimized built-in ability to crunch data over multiple levels of aggregations. (similar to ‘group by’ as provided by MySQL ) To compare, we initially used Python’s pandas to crunch our data, it took around 24 seconds for the API to complete as compared to 1.2 seconds when the crunching was leveraged to Elasticsearch, enough said . ReactJs has tremendous support and a ton of libraries to choose from, and it’s constantly upgrading to provide extremely useful features. It also allows for an extremely efficient Test Driven Development approach. We will continue to experiment with more technologies out there and make this tool even better as a benchmark for Analytics Tools. Do let us know if you have any feedback in the comment section below; we will be happy to know. Haptik is hiring . Do visit our careers section and get in touch with us at hello@haptik.ai . Posted by Deep Singh Baweja on Jan 4, 2018 6:39:00 PM Find me on: LinkedIn Twitter", "date": "2018-01-4,"},
{"website": "Haptik-AI", "title": "How React Shaped Haptik’s Frontend Story", "author": [" Nabendu Karmakar"], "link": "https://www.haptik.ai/tech/react-js-frontend-story-haptik/", "abstract": "Tweet In case if you have not noticed, Haptik now offers enterprise solutions to build and maintain AI chatbots for different industries (check out our Coca-Cola Bot ). It is enthralling to be able to build bots for different people/verticals and fulfill their use cases. But as they say, with great power comes great responsibilities. At Haptik, we have built various tools to cater to the needs of our customer. As an enterprise customer you are most likely to have a need to instantly change a reply of your bot; add more features to it or look how the bot is performing in a day to day life. Haptik’s enterprise solution offers everything. From a simple drag-n-drop Bot Builder tool to a live chat monitoring application; from a Javascript Bot-integration SDK for your website to an analytics dashboard which explains everything about your bot – we have them all. And you guessed it right – everything is built with/using React. In general, these tools are of very complex nature and have a learning edge associated with them. For example, in the following paragraphs, describes is one of our offerings to our enterprise customers and we call it Teja (Yeah, we do. Why? Cause, Andaz Apna Apna rocks. Period). This tool is a dashboard that shows all the relevant information regarding the performance of a bot. There are different ways and semantics in which way one can analyze the performance of a bot and this tool try to deliver a medium to those aspects/areas. TEJA DASHBOARD The above screenshot is from Teja (a live chat monitoring dashboard) depicting some of the key metrics for the Trains Channel at Haptik in the staging environment. As you might have noticed, there are a lot of actionable areas and a lot of data to be processed and displayed. For example, clicking on a tree node from the Data Table will change the pie chart information displayed on the left. Similarly, clicking on any area of the pie chart will update the data table accordingly which includes a few API calls which are very well managed by React’s lifecycle processes. At first glance, this looks quite simple; listening to some events and managing actions. Next, is just taking care of event bubblings and propagations and you should be good to go. We thought so too. But things get messy when the number of events or user actions increases which can change the state of the application. You tend to mess up remembering who does what and who changes what. No, it’s not your memory which is at fault, it’s the sheer unstructured nature of Javascript and jQuery alike. That is why you need a framework/system to help you through. Luckily for us, what came handy was React. React is a declarative, efficient, and flexible JavaScript library for building user interfaces. In simple terms, React is something which allows one to build a component which is (mostly) state driven. This state can be of the application or of some other components’. This implies that any action happening in the application has to change some sort of state of the application. If the state does not change, you can assume no actions have been performed. You can read more here . WHAT DOES REACT JS OFFER? Though there are many state/data-driven libraries out there, so what’s so special about React? Here’s a list of few; which might be there in some other libraries as well but we love how React enforces/defines these rules or features: 1. The Virtual DOM. Yeah obviously. 2. React Encourages Components. This makes one create different components to be used in multiple parts of the application. In few cases, one can even re-use Project A’s component in Project B. 3. React Enforces One-way Data Flow. Unlike AngularJS (1.x) or jQuery, React only allows one to change the state of the application either from the View (the UI) or a methodology one concretely defines. This means that there are only a handful number of ways in which one can change the data/state of a component — and this helps in clean and bug-free code development. Let’s see the below code: class MyInputBox extends React.Component { constructor(props) { super(props); this.state = { value: '' }; this.handleChange = this.handleChange.bind(this); } handleChange(e) { this.setState({ value: e.target.value }); } render() { return ( <input value={this.state.value} onChange={this.handleChange} /> ); } } Here, the only way the value of the input field can change is when the user is typing something in the input box. Any other part of the application or any event in the system cannot change the value of the input field as the component strictly defines the data flow. This feature of React particularly removes lots of bugs and indeed is one of our favorite features. 4. React Allows Explicit App State. The props basically. React allows developers to control what changes the state and one can also define who changes the state. Let’s consider the below example: class MyInputBox extends React.Component { render() { return ( <input value={this.props.value} onChange={this.props.handleChange} /> ); } } Here, the component is not managing its own state but allowing the parent component to do so via Props. This allows extensibility and gives the parent full control of the child component’s state. A very useful feature indeed. 5. The Stateless Components. A stateless component in React does not follow the lifecycle methods of React. This sometimes is a benefit where one piece of code does not necessarily need the React lifecycle stretches. This results in superior syntax, testability, and readability. However, it is to be remembered that every time a stateless component is called, entire VDOM will be rendered even if nothing has changed. A stateless component in React is nothing but a pure function which can be written as below: class MyInputBox = (value, handleChange) => { return ( <input value={value} onChange={handleChange} /> ); } 6. The Community Support. React has a very widespread community support. While I was writing this blog, the GitHub repository of React had more than 81K stars along with 1.2K contributors. There are a lot of high-quality open source libraries to chose from and it turns out that for anything and everything there’s already code available. This is very helpful for startups like us who want to ship code in a fast & easy manner. But how do all these add up in real life? Let’s take the above product and assume it was built in jQuery/Javascript without the use of any framework/library. It is most likely that at one point in time we could encounter the following: 1. Written a lot of code 2. Out of which most cannot be reused 3. Very little can be tested 4. And unable to find the root cause of bugs as we are not sure what might have triggered a particular scenario. All of the above can be solved with React. In React, we are encouraged to write components which can be reused at multiple places. We can create a one-way data flow to make sure we always know what can change our component. We can test each component and be pretty confident that we are shipping bug-free code. HOW DID WE GO ABOUT CHOOSING REACT? React by default is not our first choice for any project (For eg. for one recent size constraint product we used Google’s Incremental DOM ). At Haptik, we always believe in trying out the latest technologies out there and see how those can help us improve/develop our product. Before choosing any frontend (or backend) technology stack we try to consider the following: 1. How the framework helps us build the most important features of the product? 2. Is the framework state/component based? 3. Is the team familiar with the framework? If not, how much do they need to learn? 4. The choices of open source libraries for the framework. (For e.g. Material design CSS library written in your choice of framework) 5. Community support and the core contributors. 6. Is the framework production ready? 7. How easy is it to test the framework? (Yes, we do believe in unit/integration tests) 8. How will any future major pivot/enhancements be? 9. What will be the size impact of the framework on the final product? Upon validating we found out that most of the times React was the ultimate winner for us. For one of our latest offerings, we compared React with VueJS , Inferno , Angular 2 , Ember and found that React suits us the best (although VueJS came pretty close). And with the supporting high-quality add-ons such as Flux/Redux, React-Router, Jest, Relay + GraphQL – we are ready to go tackle any kind of front end problems out there. We are quite happy with the way React is helping us build cutting-edge products. And with the introduction of React Fiber and it being MIT licensed – we could not be any happier. We are currently trying out React Fiber (16.0) in the production environment and we will soon be sharing our journey. In the meantime, if you find Haptik’s journey with React interesting and would like to join the awesome React team (or have any feedback/suggestions), do not hesitate to REACT and mail us on hello@haptik.ai Posted by Nabendu Karmakar on Nov 30, 2017 6:38:00 PM Find me on: Facebook LinkedIn", "date": "2017-11-30,"},
{"website": "Haptik-AI", "title": "How Haptik Carried Out Their Largest Python 3 Migration", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/how-haptik-carried-out-their-largest-python-3-migration", "abstract": "Tweet At Haptik, most of our backend stack revolves around Python. We use the Django framework to help us get started and build scalable applications on the go. Python, being an interpreted language, reduces the time between writing code and deploying the code in production. Python has become one of the most popular coding languages out there. Here are a few reasons why: 1. Open-Source 2. Easy to understand 3. Interpreted 4. Millions of application packages via PIP library 5. Highly Scalable and performant Reasons for migrating to Python3: 1. Haptik’s backend systems (1 of them) were running on Python2 and we needed to make a move to Python3 since Python2 was getting obsolete . We did not want to delay this any further. 2. Python3 does not only offer a lot of bug fixes, better performance & security but now helps use Python the right way with a lot of changes around the same. 3. Also, Python2 support is supposed to be dropped by the end of 2019. Instagram was arguably our role model for this process. This is how they went about it. We had this migration on our minds for some time and wanted to work towards implementing it in the first quarter of this year. We took up-gradation to Python3 as an OKR for the Jan-March quarter and this blog series will be centered around the same. The entire process of up-gradation took approximately 3 months. Our applications are highly complex since this backend handles all the chatbot information, interface user message and response systems. One important thing to note is that we had already containerized all our applications back in 2018, so right now all we needed to worry about was application code. We divided the entire task into 7 steps/phases: 1. Identify Python3 Compatible code (January 16-20th, 2019) 2. Handling new features development 3. Converting older code to being Python3 compatible 4. Add new and update older test Cases 5. Knowledge Sharing Session (March 2019) 6. Testing (Side by side) 7. Deployment (April 12th, 1 am) At Haptik, we do in-depth research before starting any project to uncover as many unknowns as possible and prevent delays. This project was a great example of the benefits of spending time up front to do research rather than directly diving deep into the execution. The plan that you see below was put in place in January. The research was done upfront and we delivered the project as per the timeline set! Identify Python3 Compatible code The best way to start with any up-gradation process is identifying how much of the old code is compatible and need not change. There are n number of packages available to do that. Of course, they are not always 100% correct, but that’s a good enough spot to start with. One such Pip package is caniusepython3 and another one is 2to3 & modernize . These helped us take note of code we would need to change and packages we would need to update if we were to move to Python3. caniusepython3 -r requirements.txt  (to check if the packages in requirements.txt have support for Python3 or not) Continuous Development (not stopping new features development) We did not want to stop the development of new features etc. so we had to figure out a way of letting people add new code while trying to reduce the amount of double work required when we would be switching our application to Python3. To start with writing new code in Python3 it was necessary to figure out what are the best practices and changes in Python3. Though we have been using Python3 internally for a lot of our applications already, most of them were not this complex. Our main Haptik Backend is sort of a monolith right now which increases complexity for such changes. Also, we could not stop people from coding and pushing changes to the repository while the up-gradation process was going on. We setup linters when PRs were created (Lint Review for Python3 for this application) which helped us know if anyone’s PR had incompatible code. That way we forced people to write new code in Python3 compatible format only. 2 code reviewers check was enforced on GitHub. Converting Old Code to Python3 Before we could even start with the conversion of old code there a few things we realized with our codebase: Developers who had worked on the features were no more working with Haptik, so documentation was the only way forward. How to go about setting owners for each piece of code that needed to be converted. Converting old code while the new code is getting written. The two main packages which helped us get there were: Python modernize – This library is a very thin wrapper around lib2to3 to utilize it to make Python 2 code more modern with the intention of eventually porting it over to Python 3. Python six – Six is a Python 2 and 3 compatibility library. It provides utility functions for smoothing over the differences between the Python versions with the goal of writing Python code that is compatible with both Python versions. We used Python modernize to find all the files which do not have python 3 compatible code. After we have seen those files, then we will decide which one of these are obvious fixes and compatible with both python 2 and python 3 without six. Then we will push these fixes. After this, we will find all the obvious fixes which require the six Library. For these, we will fix them one modernize rule at a time (e.g. except). For those rules which affect lots of files, we will fix them application-wise e.g. user, chats, ml_pipeline, etc. During this phase, we also found some more code that was not compatible with python 3. We fixed that too. Once we determined which packages do not have python 3 support, then we either upgraded the package version or found a replacement. If this required code changes, we did that on a case to case and priority basis. We started doing GitHub Tag-Based releases to keep track of changes being made to the repositories. We started to mark releases on NewRelic (APM) to keep checking performance improvements as we kept releasing changes. Test Cases One key task to all of this was to make sure we update the existing test cases and write new test cases which were Python3 compatible. We were still working on improving our coverage, so we made a decision that since we can’t cover all the lines of code, we would cover the most important modules of our application. 10% of the most core/important modules to be covered with test cases. Luckily for us, our Machine Learning team had taken it as one of their OKRs, which helped us to complete this in parallel. Knowledge Sharing Session Not working in Silos is what we promote and prefer at Haptik. Back in January, we decided that by the time all the research and basic fixes are done, we would have a knowledge sharing session for the same. So, it was time to share the plan outlined below with the Engineering team, along with the release plan: 1. Best practices for writing new code which is Python3 compatible 2. Changes being made for Python2 to Python3 and how that would affect future development activities 3. Standard logging pattern 4. Final Release/Deployment plan 5. Steps to move development environments to Python 3 Testing A lot of testing went into making our Python3 release possible in a seamless manner. We carried out multiple phases of testing. Listing out a few of these below: 1. API testing using Postman 2. Bot testing –  A lot of bots to be tested. Haptik Bot testing Tool came to the rescue and helped us check if all the bots are working fine or not. 3. New Python3 based environment 4. End-to-end QA 5. Load testing with locust + Update load test scripts Once we started writing Python3 compatible code, we started to push the code in small bits and pieces into our develop branch (we follow git-flow ). The idea was to test this code and check if we are implementing the changes in the right manner or not. The APIs we updated was tested using Postman and we checked if we were getting the right response body, headers, etc. We tried to test as many different scenarios as possible. If anything wrong went into production, it would mean a huge downtime, at least for the public-facing APIs. Another thing that helped us achieve this feat quickly was us having our entire application code & systems infrastructure as code (IAC). We could launch the entire Haptik system in a new AWS account, change the containers to be running on Python3 base images and do end-end QA on the entire setup. We used Terraform for the same and the templates were around for a while now. Production Release The last couple of weeks were supposed to be very crucial. We planned to go live entirely on Python3. Release weeks. We also wanted to do aZERO downtimedeployment. 1. Code Freeze & Sanity New code changes were stopped a few days before the release. We created a release version tag for all code in master as that would be the last working Python2 version code. All developers were asked to check their features and code and if everything was working fine in UAT. Post that, code would be merged into release & then into master . (As of today, UAT runs on the release branch, but back then it was running on develop) Try to improve code coverage even further If there are any fixes, push those to develop on priority MORE TIME WE SPEND HERE, THE LESSER WE NEED FOR PRODUCTION RELEASE 2. Switch UAT to Python3 April 5th, 11 am All Python3 transition changes to be merged into develop branch Switch Python3 Docker container image, Python3 supervisor, workers, etc. – This will mean that switch will happen in UAT at this point to Python3. 3. Python3-Day (D-day) April 11th - 11 pm All clients were informed about this Major release We asked all engineers need to be available and asked at least 1 person from every team to be in the office from where the release was happening. If anything breaks, we would have someone who knows about that feature. Things we did just before the release: Backup all Databases : We backed up each and every data store. To reduce the time we already took a manual backup every hour during the day so that before the release, the difference is less and the backup time is also less. In case of failure, we still had a way to revert code immediately to the previous release GitHub tag. Enable Blockers – We had a system in place which helped us put banners on user-facing interfaces which told the users that from x to y duration, the system might respond slowly, or the system would be under maintenance. That helped us with the Python3 release as well. It acted as a shield if anything went wrong. Release begins (Blue-green & Zero Downtime) 1. The first step was to reduce the number of nodes for all the microservices 2. Next step was to make changes to the Django migration files on our Cron VMs (we have a central system to store migration files for our Django applications) and deploy code (along with Docker container image changes) 3. The next step was to deploy code to each of the set of ECS services one by one using our Continuous Deployment pipeline. ( Read more here . ) 4. Slowly, we brought up the platform step by step. 5. Checked errors on NewRelic, if any. 6. Ran a simple load test to send messages to the Pipeline to check if everything was working normally. 7. Enable endpoints for the clients one at a time. (We have a system where we have different DNS entries for different clients we have on the platform, it helps in situations like these). And it’s a WRAP! Python 3 migration is complete. All teams being available was a boost for us to do the release even more freely. We are on Python3. The release went as smooth as it can get. We started the release at 1 am in the night and wrapped it up by 5:00 am with all sanity and testing. Things to be careful about while migrating in the next blog. Future Plans 1. Performance improvements 2. Support for threading & better scalability for millions and millions of users 3. Security improvements 4. Zero Errors shipped into Production 5. Breakdown our main monolith service just like other microservices we have 6. Go the Serverless way More details in the following blog. We will also come up with an e-book around this with more stats etc. Posted by Ranvijay Jamwal on Oct 11, 2019 7:35:00 PM Find me on: LinkedIn", "date": "2019-10-11,"},
{"website": "Haptik-AI", "title": "Datagiri Meetup: Machine Learning Event Roundup", "author": [" Admin"], "link": "https://www.haptik.ai/tech/datagiri-meetup-machine-learning-event-roundup/", "abstract": "Tweet Last Saturday, 17th June, we hosted a meet-up of Data-Scientists in collaboration with DataGiri at the 91Springboard Andheri office in Mumbai. Through this event, we got an opportunity to demonstrate and give our audience a hands-on experience of one of the machine learning modules underlying Haptik’s chatbots. Around 125 Researchers and Developers from various industry domains participated in the workshop. Apart from them, there were few entrepreneurs and business analyst, who showed up to keep in touch with the evolving technology. At Haptik, we have been building chat-bots for over 3 years now. Today, Chat-bots that powers Haptik collectively processes over 2 million messages every day. We believe that our engineering team has built best systems in place to power Chat-bots and we wanted to share our learnings with the community. The event started out with Swapan Rajdev, CTO and Co-founder of Haptik welcoming everyone and sharing insights on evolving technology in Chat-bot industry. Following that, Krupal Modi, who leads Machine Learning at Haptik, spoke about the fundamental machine learning concepts required to build an intelligent and scalable dialogue systems. He introduced everyone to Chatbot NER, one of the key modules used in Machine Learning Pipeline at Haptik. Finally, Apurva Nagvenkar, Machine Learning Scientist at Haptik, demonstrated various applications of Chatbot NER and explained the approach and architecture for the same. Participants were given access to the module hosted on Haptik’s servers and were allowed to play around with it to test it on different business use-cases. Admist all the engineering activities, Haptik Volunteers actively took feedback and inputs from participants which will be incorporated in upcoming version of the module and will help us improve our machine learning stack. The amazing turnout, the feedback and the exposure we gained from this event couldn’t have been possible without active participation and support from the DataGiri team and Haptik Volunteers who showed up on a Saturday afternoon, being up and active throughout the event. It was a great experience sharing our knowledge with the community, especially when we have some big news coming their way soon 😉 Posted by Admin on Jun 21, 2017 6:36:00 PM", "date": "2017-06-21,"},
{"website": "Haptik-AI", "title": "Announcing India’s 1st Botathon – A Hackathon For Chatbots!", "author": [" Aakrit Vaish"], "link": "https://www.haptik.ai/tech/indias-1st-botathon-a-hackathon-for-chatbots/", "abstract": "Tweet 2016 has been the breakout year for chatbots. Bot or AI is the new buzz word for every new age technology company. Chatbots present a paradigm shift from the GUI to a whole new interface for getting things done. It represents a true smartphone first interface, and a part of the unstoppable shift towards all things AI. Facebook says there are already 34,000 bots built for Messenger that allow users to buy anything from flowers to airplane tickets or get customer support. In light of this chatbot revolution, we, at Haptik are excited to bring to you India’s 1st Botathon, in collaboration with T-Labs . It will be a two day hackathon on December 10-11 at the T-Labs Office in Bangalore. We welcome you to build conversational bots over platforms such as Facebook, Slack, SMS, or any other medium of your choice. Click here to register. The difference between a hackathon & a botathon, you ask? There’s no difference, per se. A Botathon is a Hackathon, except the focus is on solving problems by using chatbots on a platform of your choice. The purpose of this Botathon is to foster innovation through developer creativity and to recognize those creations. It’s no surprise that when people put their heads together, bounce ideas off each other and work in a community spirit – the results are exceptionally interesting and who knows, we might come up with solutions to solve some of the biggest problems. Why chatbots? There has been so much noise about chatbots this year, yet not a single bot can speak of product market fit or claim to have millions of people using it. This may lead to some people dismissing them as a fad. However, we disagree with that notion. Chatbots are simply not a change in the device type that happened from websites to apps – they are a whole new UI. Such a monumental shift last occurred in the 70s with the advent of the Graphical User Interface. With the change being so transformative, it will take time for people to get used to chatbots. We, as developers, are all currently in experimentation mode – building things on the go and seeing what works. Same goes with the platforms – Facebook is rolling out new tools by the day, Google with Allo is still very new, and WhatsApp has still not opened up the platform. But ultimately when all these pieces fall into place, chatbots will replace the need to have apps with infrequent use cases. Apps should only be built for things which pass the “ toothbrush test ” – for everything else, chatbots represent a better interface. A large part of this year has been spent on taking existing apps and converting them into fast, usable chatbots. While these will eventually scale and be useful, we believe the real value will be seen through a bunch of bot-first use cases . These will be products or tasks that will lend themselves most naturally to a chatbot as opposed to any other interface. Some of these may be totally new solutions which you would have never thought technology could solve. A WhatsApp, Instagram or Snapchat could have never been built for the Web first. Keeping this in mind, we encourage all participants in this Botathon to think of unique problems whose solutions are best solved using a chat interface. And we hope that some of those take on a new journey beyond the two days of the event. What next? The event aims to bring together India’s best bot developers, so you will need to Apply to get a seat. We have partnered up with the good folks at Venturesity for developer outreach and applications. The application and other details are available here . Look forward to making India’s 1st Botathon a huge success! Bonus: 11 Commandments for Chatbot Developers Posted by Aakrit Vaish on Nov 18, 2016 6:05:00 PM Find me on: LinkedIn Twitter", "date": "2016-11-18,"},
{"website": "Haptik-AI", "title": "Animated Vector Drawables – Part I", "author": [" Raj-Dixit"], "link": "https://www.haptik.ai/tech/animated-vector-drawables-1/", "abstract": "Tweet VectorDrawables not only reduce the size of your apk, but also add a very subtle wow factor to your app when used with AnimatedVectorDrawables . They provide us with the ability to animate their individual elements using ObjectAnimators through AnimatedVectorDrawables . We will walk through an example showing how to implement some of the simplest yet effective animations to your VectorDrawables and make your app stand out in the crowd. Let’s have a look at our clock vector ’s start tag first: vector xmlns:android=\"http://schemas.android.com/apk/res/android\" android:height=\"48dp\" <strong>android:viewportHeight40000003 android:viewportWidth=\"12\"</strong> android:width=\"48dp\"&gt; The drawing commands for different paths in a vector are defined with respect to the values assigned to attributes android:viewportHeight and android:viewportWidth . We can consider it as a cartesian plane having it’s corners at (0, 0) , (0, 12), (12, 0), (12, 12), with respect to which we can write our intended drawing commands. Say, I were to draw a line from the center of my drawable to the bottom most corner of the drawable. The drawing command would something like – draw a line from the point(6, 6) to the point(12, 12) . Every vector drawable consists of multiple <path> elements. Every <path> element has a set of drawing commands assigned to it’s pathData attribute, which decides the shape of the path: &lt;!-- This draws the circle for the clock--&gt; &lt;path android:name=\"@string/circle\" android:pathData=\"M 2,6 C 2,3.8 3.8,2 6,2 C 8.2,2 10,3.8 10,6 C 10,8.2 8.2,10 6,10 C 3.8,10 2,8.2 2,6\" android:strokeColor=\"@android:color/darker_gray\" android:strokeWidth=\".1\" /&gt; The < path> element above draws the outer ring of our clock vector. Have a look at the android:pathData attribute here. Let’s put some meaning to the string assigned to it. M 2,6 is to move the pointer to point(2, 6). C 2,3.8 3.8,2 6,2 then is to draw a curve from point(2, 6) to the point(6, 2) through point(2, 3.8) & point(3.8, 2) . And this goes on till the drawing commands are exhausted. Notice the values in our drawing commands are as per the viewPortWidth and viewPortHeight values assigned to the vector . When being rendered on the device, the drawable is drawn factoring in the device’s screen size & density and these values are scaled proportionally. The attributes, android:fillColor & android:strokeColor, of the <path> element decide whether the path will be filled or stroked . If the path is filled , the inner area to of the shape drawn will be painted. If the path is stroked , the boundary of the shape will be painted.The android:fillColor attribute decides the color of the solid shape and android:strokeColor decides the color of the outline. We uniquely assign a name to every <path> element, whose attributes are to be animated. Now, Let’s have a look at the animated clock vector ’s start tag: &lt;animated-vector xmlns:android=\"http://schemas.android.com/apk/res/android\" <strong>android:drawable</strong>=\"@drawable/vd_clock\"&gt; The attribute android:drawable is where we assign the clock vector to our AnimateVectorDrawable. It’s not too difficult to animate certain attributes of the path elements of a VectorDrawable. The code below animates the path-attributes strokeWidth and strokeA lpha: &lt;!-- animates stroke Width of the outer ring--&gt; &lt;target android:name=\"@string/circle\" android:animation=\"@animator/change_width\" /&gt; &lt;!-- animates alpha of the outer ring--&gt; &lt;target android:<strong>name</strong>=\"@string/circle\" android:<strong>animation</strong>=\"@animator/change_alpha\" /&gt; The name attribute of the <target> element in the AnimatedVectorDrawable corresponds to the name of the path/group ( which will be discussed next), whose attribute is to be animated, and the animation attribute takes the animator, which is desired to run on that path/group . AnimatedVectorDrawable maps the ObjectAnimator to that name and then later runs these animators on the path/group with that name. The resulting effect is: Well, Path-Attributes animations are only limited and hence don’t serve every use case. This is where <group> element in VectorDrawable comes to our rescue. Group-Attributes include properties like rotate, translateY/X and scaleY/X .Various paths can also be clubbed inside one group to achieve these animations, If desired. Now, let’s have a look at the clock vector again: &lt;group android:name=\"@string/minute_hand\" android:pivotX=\"6\" android:pivotY=\"6\"&gt; &lt;!-- minute-hand path--&gt; &lt;path android:pathData=\"M 5.5,6 L 8.5,6\" android:strokeColor=\"@android:color/darker_gray\" android:strokeWidth=\".2\" /&gt; &lt;/group&gt; The minute-hand path of the clock is placed inside a group. The android:pivotX and android:pivotY attributes are assigned a value of 6 . This ensures that the clock’s hand will rotate with its axis at the center of the viewPort. Each group again needs a unique name for the purposes of mapping animators to the specific group. When we are done defining groups in the VectorDrawable , we can map our animators with these groups in our AnimatedVectorDrawable . Now, let’s have a look at the animated clock vector again: &lt;!-- rotates the group named second_hand of the clock vector--&gt; &lt;target android:name=\"@string/second_hand\" android:animation=\"@animator/rotate_second_hand\" /&gt; &lt;!-- rotates the group named minute_hand of the clock vector--&gt; &lt;target android:name=\"@string/minute_hand\" android:animation=\"@animator/rotate_minute_hand\" /&gt; Finally, we have our AnimatedVectorDrawable ready. We can directly reference it in our ImageView through app:srcCompat and then later call start on it by getting a reference of it from ImageView: ImageView animatedClockView; animatedClockView = (ImageView) findViewById(R.id.img_clock); ((Animatable)animatedClockView.getDrawable()).start(); We can assign our AnimatedVectorDrawable to any ImageView programmatically as well: AnimatedVectorDrawableCompat animatedClock; animatedClock = AnimatedVectorDrawableCompat .create(this, R.drawable.avd_clock_rotate); animatedClockView.setImageDrawable(animatedClock); The result is: So, Check out my github repo where the complete source code is available for you to get started with. A post on Path-Trimming and Path-Morphing will be coming soon. #BuildBetterApps #AnimatedVectorDrawable #Android Posted by Raj-Dixit on Dec 28, 2016 9:45:00 AM", "date": "2016-12-28,"},
{"website": "Haptik-AI", "title": "Probing For Clarification – A Must-Have Skill For Level 3 AI Assistant", "author": [" Krupal Modi"], "link": "https://www.haptik.ai/tech/probing-clarification-skill-ai-assistant", "abstract": "Tweet Typically, there are 5 levels of Conversational AI and Level 3 is understood as contextual assistants, where the user no longer needs to know how to use the assistant or follow the pre-designed conversational flow. Although building contextual assistant remains an overarching goal, most virtual assistants get consumed in the greed of answering as many user queries as possible . This often deviates them from focusing on key elements of two-way communication which are essential for a successful conversational experience. To understand this better, let’s take a look at some key dialogue exchanges between humans. They can be broadly categorized into one of the following categories: This blog would primarily highlight a few scenarios which explore AI assistant’s capabilities to ask post-prediction probing questions before proceeding further in the conversation. We will further dive into building blocks that need to be connected in order to build a robust probing system. Why is probing an important skill for an AI assistant? Active listening, mutual understanding, and trust are key aspects of reliable human conversations. Any gap in understanding would risk trust and credibility which can ultimately make or break the overall experience. Hence, in order to build mutual understanding and common grounds, it is essential and acceptable for humans to ask each other for clarification during conversations. This activity of asking for clarification is referred to in this blog as probing the user and objective of probing is to help an AI assistant achieve the following: 1. Granular understanding of user’s intention 2. Building credibility and reliability 3. Influence the next action taken by the user The following image tries to capture the difference in end user experience with and without probing. What is required in order to build accurate probing system? Gateway to building smart probing components in conversation flow opens up with the following checklist of building blocks. 1. Configurable Dialogue Management – Dialogue manager acts as a controller for most conversational AI systems. In order to train a good dialogue policy, availability of high quality chat transcripts is a necessity. In many cases, sufficient number of transcripts may not be available or they might not meet the quality standards needed for best user experience. Hence, well engineered, abstract and configurable dialogue state tracking becomes the key to orchestrating complex conversations. Two fundamental capabilities needed in the dialogue manager to enable probing are mentioned below: a. Recognise the need for clarification raised by any underlying NLU component. b. Identify appropriate NLG mechanism and trigger appropriate response generation needed for clarification Connecting dots of Conversational AI 2. Accurate prediction of uncertainty – Deep learning models give confidence score for every prediction they make. While there is always enough focus on correctness of predictions,quantifying uncertainty in a predictable and reliable way is also essential to achieve sensible probing. For example, if a model always predicts the score in the range (0.9, 1) for True Positives and (0,0.1) for True Negatives and does not predict anything reliable in between the range (0.1, 0.9),  then it cannot raise the sensible requirement of the probe. The following are examples of a couple of components and how their scores can be used to probe and reduce ambiguity in conversation: a. Voice to text – There are cases when the score provided by voice to text model is lower than the predefined threshold and noise corrections are not available. It is fair to trigger a probe mentioning ‘Your voice is not clear, can you speak again?’ instead of processing noisy text. b. Intent detection – In some scenarios, the intent detection model identifies multiple intents having similar scores or a single intent with a score near the boundary region. In such a scenario, it is better to reconfirm from the user before responding hastily. 3. Granular understanding of user queries – User query could be vague , incomplete and might have some hidden underlying domain-specific assumption. It’s important to understand the completeness of the query and in case if there are any assumptions or coreferences which the assistant can not resolve, it’s fair to ask the user for clarifications. You can check out a comprehensive article here to dive deeper into understanding of user queries. 4. Reasoning based response pipeline – An abstract response generator which can act on the input of Dialogue manager and support easy integration of multiple curated and automated NLG components is essential for an end to end probing mechanism. 5. Context retention from probe history – Post probing, it’s important to resolve coreferences and retain the context of the chat. Hence, a component that can interpret probe history and transform the relative user query into a resolved query is a must-have for continuing the conversation. Conclusion Regardless of the progress in Machine Learning Algorithms and availability of Training Data, boundary cases are here to stay in the near future. Hence, it’s important for an AI assistant to deal with them gracefully and accurately. Most importantly, “ What is the probability of the user dropping off against responding back to the clarification”? At Haptik, we’ve seen that close to 90 percent of the users respond back when accurately probed for clarification. This in-turn also provides us a rich channel of data to improve our virtual assistants and make user experiences better in the future. We believe that there is still a lot that AI assistants need to learn in order to understand users granularly and we will continue encouraging more research in this direction. You can also read more about our probing module ‘Smart Assist‘ in order to understand the business impact of the same. Posted by Krupal Modi on Jul 31, 2020 7:44:00 PM Find me on: LinkedIn", "date": "2020-07-31,"},
{"website": "Haptik-AI", "title": "How Haptik Uses Auto-correct To Improve User Experience", "author": [" Viraj Anchan"], "link": "https://www.haptik.ai/tech/auto-correct-to-improve-user-experience", "abstract": "Tweet There’s an old saying that goes, “You only get one chance to make a first impression.” And more often than not, that impression lasts. For a messaging app like Haptik, that first impression comes in the form of a text message from an assistant. Which is why spelling, punctuation, and grammatical mistakes can ruin the user experience. While autocorrect can be useful in preventing embarrassing mistakes, it is not always right. On Wednesday, February 29, 2012, a high school student in Gainesville, Georgia, tried to send a friend a text with the message “gunna be at west hall today.” The Autocorrect feature on this student’s iPhone changed it to “gunman.” Making it worse, it was sent to the wrong number, and that put two Georgia schools on lockdown. At Haptik, we acknowledge the challenges and consequences that develop from the use of Autocorrect technologies. In an ideal scenario, Autocorrect would consistently and correctly distinguish between what we actually type and what we intend to type with little to no user intervention. However, this utopian world is not the one we live in. Assistants at Haptik cater to millions of queries belonging to multiple domains. In today’s fast paced social world, simple mistakes spread like wildfire, especially if they are typo’s. And to prevent any such disaster, we use a combination of Autocorrect techniques. We have created our own dictionary of commonly misspelled words. Our internal chat tool autocorrects misspelled words in real time before the message is sent. The front-end autocorrects the misspelled words and then sends it to the back-end for further correction. In the back-end, we use LanguageTool to autocorrect punctuation and grammatical mistakes. LanguageTool is an Open Source Java based grammar-checker for English, French, German, Polish, Romanian, and more than 20 other languages. It takes a text and returns a list of possible errors. To detect errors, each word of the text is assigned its part-of-speech tag and each sentence is split into chunks, e.g. noun phrases. Then the text is matched against all the checker’s pre-defined error rules. If a rule matches, the text is supposed to contain an error at the position of the match. The rules describe errors as patterns of words, part-of-speech tags and chunks. Each rule also includes an explanation of the error. To improve the accuracy, we have disabled spell check rules (HUNSPELL_RULE, HUNSPELL_NO_SUGGEST_RULE, and MORFOLOGIK_RULE) in the LanguageTool. We have also created our own rules which are customized according to our use cases. We are using a Python wrapper for LanguageTool. We had to optimize it so that it takes less time to autocorrect. Earlier the Java server would restart whenever the function was called. We changed it so that the java server starts only when the function is called for the first time. The java server then runs continuously till our API server runs. Our version is available as a pip package ( grammar-check ). Here’s the basics of how we do Autocorrect using the grammar-check package. import grammar_check def grammar_check(text): tool = grammar_check.LanguageTool('en-GB') matches = tool.check(text) text = grammar_check.correct(text, matches) return text You can write your own grammar rules in 3 ways: 1) Java – You can extend LanguageTool’s Rule class and implement the match (AnalyzedSentence) method. If your rule doesn’t work on the sentence level, implement TextLevelRule instead. 2) XML – Most LanguageTool rules are contained in rules/xx/grammar.xml, whereas xx is a language code like en or de. In the source code, this folder will be found under languagetool-language-modules/xx/src/main/resources/org/languagetool/; the standalone GUI version contains them under org/languagetool/. Here’s an example of a complete rule. &lt;rule&gt; &lt;pattern&gt; &lt;marker&gt; &lt;token&gt;this&lt;/token&gt; &lt;/marker&gt; &lt;token postag=\"NNS\"&gt;&lt;exception postag=\"VBZ|NN|JJ.*\" postag_regexp=\"yes\"&gt;&lt;/exception&gt;&lt;exception&gt;data&lt;/exception&gt;&lt;/token&gt; &lt;token&gt;&lt;exception postag=\"NN|NNP|NN:.*\" postag_regexp=\"yes\"&gt;&lt;/exception&gt;&lt;/token&gt; &lt;/pattern&gt; &lt;message&gt;Did you mean &lt;suggestion&gt;these&lt;/suggestion&gt;?&lt;/message&gt; &lt;short&gt;Grammatical problem&lt;/short&gt; &lt;example type=\"correct\"&gt;These errors are easy to fix.&lt;/example&gt; &lt;example type=\"incorrect\"&gt;&lt;marker&gt;This&lt;/marker&gt; errors are easy to fix.&lt;/example&gt; &lt;example correction=\"These\" type=\"correct\"&gt;This forms a sharp contract with...&lt;/example&gt; &lt;/rule&gt; 3) Python – Create your own rules in python and apply that rule before correct() method of grammar_check is called. It is good to see that LanguageTool community is actively contributing rules in more than 20 languages. To create your own pip package, read this documentation . “One thing about open source is that even the failures contribute to the next thing that comes up. Unlike a company that could spend a million dollars in two years and fail and there’s nothing really to show for it, if you spend a million dollars on open source, you probably have something amazing that other people can build on.” ~ Matt Mullenweg Posted by Viraj Anchan on Sep 26, 2015 6:00:00 PM", "date": "2015-09-26,"},
{"website": "Haptik-AI", "title": "New Algorithm For a Personal Assistant Bot", "author": [" Aniruddha Tammewar"], "link": "https://www.haptik.ai/tech/new-algorithm-for-a-personal-assistant-bot-2/", "abstract": "Tweet Immediate response is what everyone expects from their personal assistant and so are expectations of Haptik users. In order to scale with exponentially growing user base while providing consistent and quick service, Haptik significantly relies on machine learning and natural language processing. I joined Haptik in January 2016 as a Machine Learning Scientist after completing masters from IIIT Hyderabad. Haptik so far was built using NLP techniques like parts of speech tagging, stemming, lemmatization, etc. Following rule based modules were already in place to set up the basic framework of automation. Preprocessing – It normalizes text data and has to be perfect for efficient functioning of other modules. Task Identification – This module interprets the user query and identifies what service user is exactly asking for. Entity Recognition – It identifies all the relevant details required from structured or unstructured data to get a particular task done from user. Response Generation – This module generates a natural language response either to collect user data, recommend different options or complete a transaction. There was a limitation of efficiency and scalability due to rule based or supervised nature of algorithms in place which ultimately were limited by data. But today, Haptik has a large database of messages exchanged between users and assistants collected over last two years containing chats spreading across diversified domains like restaurant reservations, shopping, travel, etc. Database is rich in terms of features like domain, entities, user details, timestamps and core messaging text. Hence we were ready to take a next step and shift our algorithms to statistical and unsupervised systems. Having core framework in place and access to rich database, advancements in deep learning and dialogue processing enabled us to create a completely unsupervised model which can predict responses by learning messaging sequence from existing data. Following is the list of some research work which was useful while building a learning mode l. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses (response generation system that can be trained end to end on large quantities of unstructured Twitter conversations) A Diversity-Promoting Objective Function for Neural Conversation Models (Maximum Mutual Information (MMI) as the objective function in neural models) Neural Responding Machine for Short-Text Conversation A Neural Conversational Model (uses sequence to sequence framework) After studying through multiple research papers, understanding existing automation architecture and considering the availability of tools and data, we came up with the new approach and named it ‘Sequence Learning for Personal Assistant’ . Concept of Sequence Learning We observed that, a typical conversation between a user and an assistant follows a sequence of messages specific to every domain. In general, what we have seen, is a conversations may start with greeting or casual message (Ex. Hi, good morning!) followed by the actual task that needs to be done and then the next couple of messages are exchanged to get all the required information(Example: date, time, destination, delivery location) . Finally we need to send a confirmation once the task is completed which may get followed by an acknowledgement or greeting from the user. We concluded that messages exchanged for every domain could be divided into finite number of meaningful clusters and every conversation could be represented by sequence of cluster ids. With thousands of conversations happening on daily basis, at Haptik we had millions of sequences to learn from and an accurate model to predict the next cluster in the ongoing sequence was generated. Following is the overview of sequence learning where every message type represents a different cluster. Training Specifications Figuring out all possible message types/clusters is a tedious task. To tackle this problem we identified all message types automatically by clustering the messages. For training sequence learning model, we used chat data from Haptik’s database. We identified and tagged each message in the database with appropriate message type and created a millions of sequence of message types for particular domain. Post sequence generation we treated it like a next word prediction problem using language model. We used RNNLM toolkit and SRILM toolkit for training a language model using combination of neural network and statistical approach to predict next cluster in the sequence. We started off by putting sequence learning in production for one domain to validate our approach and it has revealed very positive results. As a next step, we will be expanding this approach for all the domains for getting things done for Haptik users in fastest and convenient way. Wish to be a part of the amazing things we build? Look no further! Reach out to us at hello@haptik.co P.S. – A big ‘Thank You’ to Dan Roth for his inputs on this algorithm. This post is written by Aniruddha Tammewar, Machine Learning Scientist at Haptik. Posted by Aniruddha Tammewar on May 27, 2016 6:03:00 PM Find me on: LinkedIn", "date": "2016-05-27,"},
{"website": "Haptik-AI", "title": "Putting Text on Image Using Python - Part I", "author": [" Vinay Jain"], "link": "https://www.haptik.ai/tech/putting-text-on-image-using-python/", "abstract": "Tweet Computer graphics teaches us how a pixel on a screen can be manipulated to draw beautiful shapes, artistic typography, eye-catching illustrations, ‘ make-me-look-good ’ photo-filters and a lot more. Hardware manufacturers, researchers, software developers work together to build great products: smartphones, smartwatches, smart TVs, cameras all with the study of computer graphics. Despite the fact that computer graphics has evolved so fast and the development of softwares like Adobe Photoshop, Adobe Illustrator, Sketch has made our lives easier to a great extent, we still cannot generate images on-the-fly with them. In order to do that, we’ll need to reach a level where there is no drag and drop, no fancy select-all-make-bold keyboard shortcuts, no cropping and no copying-pasting. And we cannot get there by time-travel, but surely with code! Getting Started Come along, open your favourite text editor, follow me and I’ll help you draw dynamic text data on images. I assume you have Python and pip installed on your computer, but if not, follow the steps in the links to set up the development environment. After you’ve done setting up, from the shell, execute the below command to install Pillow (more details here ) and its dependencies. pip install pillow As you now have installed all dependencies, let’s move forward and write some code. Pillow is an extensive library, but for our purpose, we’ll be using the following classes: Image : to create an image object for our greeting background ImageDraw : creates a drawing context ImageFont : font of the text we will be drawing on the greeting Let’s take the following background image and initialize it with the following code: Code : # import required classes from PIL import Image , ImageDraw , ImageFont # create Image object with the input image image = Image . open ( 'background.png' ) # initialise the drawing context with # the image object as background draw = ImageDraw . Draw ( image ) For creating ImageFont objects we also need font(ttf, otf) files. You can use any font of your choice, here I’ll be using the Roboto font which can be downloaded from the Google Fonts GitHub repo . # create font object with the font file and specify # desired size font = ImageFont . truetype ( 'Roboto-Bold.ttf' , size = 45 ) # starting position of the message ( x , y ) = ( 50 , 50 ) message = \"Happy Birthday!\" color = 'rgb(0, 0, 0)' # black color # draw the message on the background draw . text ( ( x , y ) , message , fill = color , font = font ) ( x , y ) = ( 150 , 150 ) name = 'Vinay' color = 'rgb(255, 255, 255)' # white color draw . text ( ( x , y ) , name , fill = color , font = font ) # save the edited image image . save ( 'greeting_card.png' ) Below is what you get after executing the above code: With some fonts, you might have to pass an optional parameter encoding which tells the ImageFont module which encoding to use while opening the font file. Computer graphics have an inverted coordinate system, the origin(0, 0) that lies at the top-left corner of the image. x here represents the distance of the text box from the left (x=0) and y represents the distance from the top (y=0). While you save the image, you can pass optional parameters like optimize and quality to control the size of the output image. image.save('optimized.png', optimize=True, quality=20) This generates an output image optimized.png with reduced quality but smaller size. Where Are We Using Pillow With Python? While at work, I recently developed a feature which demanded the creation of a leaderboard image on-the-fly , with user-specific quiz score data. And just with a few lines of code, I was able to create an image like this: Haptik Weekly Quiz Leaderboard Voila! It looked great and we decided to use the idea of creating images on-the-go, for other use-cases as well. We currently use Pillow to generate images for Jokes, Motivational Quotes, Horoscopes, Word of the Day etc. in real time, and with data from different API responses. Haptik Motivational Quote & Word of the Day The code we used in this post is not sufficient to draw text boxes as shown in the images above. I’ll be writing another post which will focus on text alignment, splitting long text into multiple lines, controlling space between two lines and more. Please do give us your feedback if any in the comments section below. Haptik is hiring . Do visit our careers section or get in touch with us at hello@haptik.ai . Posted by Vinay Jain on Jan 11, 2018 7:22:00 PM Find me on: Facebook LinkedIn", "date": "2018-01-11,"},
{"website": "Haptik-AI", "title": "Finite State Machines To The Rescue!", "author": [" Sailesh Dev"], "link": "https://www.haptik.ai/tech/finite-state-machines-to-the-rescue/", "abstract": "Tweet Building chatbots at Haptik is our Forte & we try to follow the best practices available out there to solve complex problems in hand. Recently, we were facing a problem where our conversation flows have become very complex over a period of time and the code that is written was getting hard to manage and understand. When we started digging deeper we noticed that most of our flows follow a simple state machine where based on a decision of the previous steps the next step is decided. So What Is A Finite State Machine? “A finite state machine (FSM) or finite state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of states at any given time”. –Wikipedia Finite state machines come handy for state management. This design pattern is quite common with programmers in the gaming world. You can represent your data in a variety of state machines. You need to choose the one that best fits the use-case in hand. This programming pattern forces you to expose your data and think about different scenarios and evolutions thereby making your code concise and readable. Anyhow, this is “ not a one-size fit all approach “. How Do We Use Finite State Machines At Haptik? Let’s take a simple problem and define its scope first. At Haptik, we have a feature where a user can set a reminder for their daily tasks and we call them to remind them about it. In this flow, one of the major requirements is that a user should be able to update a reminder that they have set previously. For example: If a user has set a reminder for 25th September at 7:00 am they should be able to change the date and time of the reminder at any given point in time. What Does It Take To Update A Reminder Flow? 1. User comes to the app and says I want to update my reminder 2. Then we check whether the reminder exists or not 3. If it exists, we check if updated date & time is provided 4. If we have the updated date & time, we check if it is valid or not 5. If it is valid, we go ahead and update the reminder The above is a basic happy flow but if you look at the flow diagram there are a lot of negative cases that we need to handle as well. That’s where things start getting complex and if the code is not written well, it becomes hard to understand. First , you start to think about different entities/states for the given use case; these become the states of your finite machine. Next , you start to think about transitions/events that can be triggered which allows you to move from one state to the other. Implementation Without Using State Machines Let’s try to implement the above use-case without using state machines. First of all, we would start modeling our data and then we would start to think about the functionality. Later, we would need to map each functionality to a function which deals with a single responsibility pretty well: class Reminder: ...model data ... def welcome_message(self): print('Hi! Will help you to update reminder')\\ start_update() def start_update(self): ... check_reminder_exist() ... def check_reminder_exist(): # make db call and check if an upcoming reminder exists if exists: validate() else: display_error_message() ... You can see where this is going. One has no idea about the current state and has to deal with various conditions. This gets messier as the code base becomes larger. For someone having a look at the code for the first time, it would be a nightmare. Also, when any new change has to be made, the code has to be touched at multiple places and the possibilities of missing out edge cases and making mistakes is very likely. Implementation Using State Machines Now, let’s get our hands dirty and learn how simple it is to create a finite machine to solve the above problem. A great module called state_machine is available as a pip package . It’s a simple well-written Python Framework to create customizable finite state machines (FSMs) . You can read more about it here . Installation: pip3 install state_machine Let’s start with the Reminder class . Each update will have its own state machine. The first step to create a state machine using the state_machine module using the @acts_as_state_machine decorator: @acts_as_state_machine class Reminder: # definition goes here Next, we define the states of our FSM (Finite State Machine). We need to mention the initial state of the machine; we can do that by passing initial=True in the constructor call of State: # Below code goes inside the reminder class # An internal state which starts the update process for the reminder: start = State(initial=True) # An internal state to check if reminder exists reminder_exists = State() # An internal state to check is the updated data and time is provided by the user updated_data_exits = State() # An external state which displays error message to user display_error = State() # An internal state which checks the validity of modification: validate = State() # An external state which marks successful update: success = State() We continue defining the transitions. It is normal to execute one or more actions before or after a transition occurs. In the state_machine module, a transition has the name Event. We define the possible transitions using the arguments from_states (can be either a single state or a group of states) and to_state like in the below example: start_update = Event(from_states=start, to_state=reminder_exists) display_error_message = Event(from_states=(reminders_exist, validate), to_state=display_error) validation_success = Event(from_states=validation, to_state=success) validation_failure = Event(from_states=validation, to_state=display_error) .... Now, that we have an understanding of how the flow looks, we can go ahead with the finer implementations. For the sake of simplicity, a reminder can be modeled based on the below attributes: def __init__(self, name, timestamp, content): self.id = id self.name = name self.timestamp = timestamp self.content = content Transitions are not very useful if nothing happens when they occur. The state_machine module provides us with the @before and @after decorators that can be used to execute actions before or after a transition occurs. @before('reminder_exits') def welcome_message(self): print('Hi! Will help you to update reminder') @after('reminder_exists') def (self): switch(this.current_state): case 'display_error': # display error message to user break; case 'updated_data_exits': # ask the user to input data break; default: # default case break; Let’s define transition() function which accepts two arguments: 1. reminder 2. event def transition(reminder, event): try: event() except InvalidStateTransition as err: print('Error: transition of {} from {} to {} failed'.format(reminder.name, reminder.current_state)) That’s it! Wow! This is amazing. We eliminated a lot of conditional logic from the codebase. There’s no need to use long and error-prone if-else statements that check for every state transition and reacts upon them. The implementation is python specific but the pattern is code agnostic and can be abstracted to any relevant use case. Hope this blog helps you understand how we can effectively use the state pattern. Do let us know your valuable feedbacks & do come back for more such blogs. Also, we are hiring for various positions at Haptik, so if interested, do get in touch with us at hello@haptik.ai . More details here. Special thanks to Swapan Rajdev & Ranvijay Jamwa l for their support & help. Posted by Sailesh Dev on Sep 19, 2017 7:11:00 PM Find me on: LinkedIn Twitter", "date": "2017-09-19,"},
{"website": "Haptik-AI", "title": "Building Augmented Intelligence", "author": [" Krupal Modi"], "link": "https://www.haptik.ai/tech/building-augmented-intelligence", "abstract": "Tweet Experts at Haptik cater to millions of user queries belonging to multiple domains of products and services. Our primary goal to provide accurate solution over a messaging platform within four minutes , necessitates experts to work efficiently by optimizing their effort. In order to scale, optimize and cater to such a large volume of messages, our Natural Language Processing and Machine Learning algorithms are key for our experts to find the best solution with negligible latency. In this journey of augmenting machine intelligence and human effort, the first problem we solved was training a bot to understand user queries from an ongoing conversation. After thorough analysis we identified that most of the conversation follow the subset of below general pattern : A conversation starts with a casual greeting from the user Followed  by a specific query Sometimes the experts ask follow up questions to get more clarity of the users problem After understanding the query a message with the concrete solution is sent to the user And the conversation ends with courtesy messages from each side In order to dig deeper into identifying this pattern at a granular level, we used Natural Language Toolkit (NLTK 3.0) to pre-process incoming chats and designed a classifier to categorize our messages into the above categories. Messages which are classified as a user queries and clarifications are further processed and fed into our response recommendation algorithm. This algorithm takes the data relevant to the user’s question and looks it up across an extensive knowledge base which we carefully curated over a period of time and returns the best solution to Haptik expert all within milliseconds. Providing accurate, quick and personalized response to our users being our highest priority, our efforts will continue in this direction to make a perfect blend of machine and human intelligence. Want to join is us in scaling our technology as a part of engineering team? Just get in touch here 🙂 Posted by Krupal Modi on Mar 25, 2015 5:43:00 PM Find me on: LinkedIn", "date": "2015-03-25,"},
{"website": "Haptik-AI", "title": "Android Debugging Made Easy", "author": [" Ankur Jain"], "link": "https://www.haptik.ai/tech/android-debugging-made-easy/", "abstract": "Tweet At Haptik , one of the world’s largest conversational AI platforms, we build new features for our mobile app and SDKs almost on a daily basis. As the application development process started becoming more complex & tedious, so did the process of debugging. We found it difficult to debug things around the network layer communication of the application, databases and view hierarchies using just Android Studio. Here’s are the specific problems we were facing: 1. As we kept adding newer modules/features in our app on regular basis we found it really difficult to debug those issues in real time 2. The libraries we were using did not have in-depth logging around errors 3. There were not many good options out there to understand application behavior in depth This is when we started our hunt for a library that could help us debug in a much better way. And lo! We came across a new library open sourced by Facebook called Stetho . In this blog post, we’re going to explain what Stetho does and why it’s such an essential tool for Haptik. What is Stetho? Stetho is one of the most famous Android debugging libraries written by Facebook. What makes it stand out from other debug libraries is its deep and powerful integration with Chrome Developer Tools(DevTools for short). With it, developers have much more convenient and richer access to their app’s data. Just like we debug and inspect our web pages using Chrome DevTools, developers can now inspect their app’s data and perform operations on it using Stetho. So, if you are saving user data in an SQLite table or in shared preferences, you can use Stetho to check if data is being saved properly or to see how your app behaves with different sets of data. In simple words, you can perform all CRUD operations using Stetho GUI. Developers can also choose to enable the optional dumpapp tool which offers a powerful command-line interface to application internals. How Do I Integrate Stetho in an Application? In this section of the blog, I will tell you how to get started with Stetho and get it up and running on your local machine. Prerequisites Before we can get started, you need some tools to utilize Stetho. You’ll need to install the following: 1) Chrome Browser with DevTools for debugging. 2) Android ADB is already pre-installed if you’re using Android Studio . If not, you’ll need to install it. Dependencies Add Stetho dependencies to your build.gradle file: dependencies { debugImplementation 'com.facebook.stetho:stetho:1.5.0' } Additionally, you can also add optional Retrofit/HttpUrlConnection helper for network inspection: dependencies { // For OkHttp3 debugImplementation 'com.facebook.stetho:stetho-okhttp3:1.5.0' // For OkHttp debugImplementation 'com.facebook.stetho:stetho-okhttp:1.5.0' // For UrlConnection debugImplementation 'com.facebook.stetho:stetho-urlconnection:1.5.0' } Let’s move ahead with the integration now. Integrations Stetho integration is very seamless and straightforward for most Android applications. You just need to initialize Stetho from app’s Application class. Only include Stetho for debug builds , and create different Application classes for debugging and release builds. You can refer this doc for creating different build variants: public class MyDebugApplication extends Application { public void onCreate() { super.onCreate(); Stetho.initializeWithDefaults(this); } } The above will enable the Stetho with the default configuration in your Android Application. Now, you can inspect your database and SharedPreferences with the help of Chrome DevTools. Fire up your chrome browser and hit the URL: chrome://inspect You’ll see all the available device/emulators with the applications enabled with Stetho. You can click on the inspect button to start debugging. For network inspection, you’ll need to add network interceptor. For okhttp, it goes like this: new OkHttpClient.Builder() // Only for debug build .addNetworkInterceptor(new StethoInterceptor()) .build() After enabling Stetho in the app, the developer can debug/inspect on various resources of our Android application. Below are some key features: Persistent data inspection (SQLite/SharedPreferences) Using Stetho you can inspect your app’s data. Stetho allows you to access and modify app’s SQLite database and shared preferences. Select the Resources tab in Chrome DevTools and you will see a tree-like Hierarchy view of all databases and shared preferences available in the app. Select “Web SQL” for database and “Local Storage” for shared preferences. Clicking on one of the tables shows all the columns with the first 250 row values. Selecting one of the databases will enable the right panel as a console where SQL statements are executed: For your app’s SharedPreferences, select “Local Storage”. You will see the names of the files your app uses to store the preferences. Clicking a file displays the key-value pairs stored in that file, you can even edit the values stored in it. You can make changes to the values at runtime directly from DevTools. Note that any changes you make to the values are permanent: Network Inspection With Stetho you can see and debug real-time network requests that your app is making. By clicking on the “Network” tab, you can see the list of requests your app is making. Just like web debugging, on clicking any specific request, will give you request overview in the right pane. You can see request/response headers and the response itself: UI Hierarchy Inspection Clicking on “Elements” tab will allow you to see your Activity view hierarchy. You will see all the view elements that are currently being displayed in your App Activity. By selecting any specific view you can see that view’s properties in the right pane. You can also search any view using the small magnifying glass icon in the upper left of the DevTools. Note that Stetho provides View hierarchy support for ICS (API 15) and up: Why Did We Pick Stetho? It was very easy to integrate Stetho with our existing code. The Stetho Github repository has enough resources to help us get started. Stetho makes it so much easier to debug day-to-day issues and we spend lesser time debugging. Stetho helps us get deeper into application behaviour as well making it easy for us to know what is happening That’s how easy it is to get started with Stetho. We will soon be sharing some more integrations like these. Do let us know what you think about this blog in the comments section below. Haptik is Hiring. Do visit our careers page or email us at hello@haptik.ai. Posted by Ankur Jain on Apr 20, 2018 6:43:00 PM Find me on: LinkedIn Twitter", "date": "2018-04-20,"},
{"website": "Haptik-AI", "title": "Code Deployment in Multiple AWS Accounts Using Octopus", "author": [" Divneet Singh"], "link": "https://www.haptik.ai/tech/code-deployment-multiple-aws-accounts-using-octopus/", "abstract": "Tweet The entire Haptik backend infrastructure runs in a containerized environment. As the journey progressed we took some crucial decisions which have helped us reach where we are today, in terms of deployments and various other pipelines. This will be covered further on another blog post on CI/CD. We are fully hosted on AWS and given the nature of clients we have, we sometimes have to support a multi-tenant setup where we need to bring up a new environment in our clients AWS account. For code deployment in these accounts, we wanted to make it possible to have different streams of releases, with different steps, variables, lifecycles and code releases. We were trying to find an efficient way to deploy code to multiple AWS accounts. Of course, there are a few services for Code deployments that AWS offers from its line of Deployment services. But we wanted to keep the solution a bit agnostic to start with. Most of our Applications are deployed on AWS ECS as containers/tasks and ECS helps us Orchestrate the application containers. Spotinst also comes in to help to keep these deployment costs low. While researching we came across a tool known as Octopus Deploy . What is Octopus? Octopus is the deployment automation server, designed to make it easy to orchestrate releases and deploy applications, whether on-premises or in the cloud. Some Salient Features: 1. Multi-tenant application deployments 2. Multiple active streams of releases: Use channels to keep multiple streams of active releases – for example, a 1.x “stable” stream while you maintain what’s in production today, and a 2.x “beta” stream while you work on the next big thing for tomorrow. 3. Automated deployment from Jenkins 4. Automated deployments to AWS Why Octopus? We needed something that doesn’t put any or little overhead on the developers & could fit in with our existing Continuous Deployment pipeline. This is where we thought Octopus could come into the picture. One of the Major reasons to choose Octopus was: Keeping track of what release versions have been deployed in an ECS cluster’s service. The solution to the problem is discussed in our earlier blog – Empowering Developers at Haptik . Please give it a read if you haven’t yet. At Haptik, the DevOps practice is brought up in such a way that the developers are responsible for the code release, we needed something that would fit in the pipeline just as a puzzle piece. Moving on, the solution had to be something that would integrate with our existing Jenkins pipeline with the help of a simple plugin, building on top of the current CD pipeline. Deploying Code on ECS using Octopus A typical in-house ECS deployment for us looks like the following: For our internal AWS account deployment, we create a Docker container image with the latest code and package updates and upload the same to ECR. Now, for new AWS account deployments for the Haptik applications, we do not want to build the images again. We want to deploy the appropriate ECR image along with the precise Task definition settings. Octopus currently doesn’t have an ECS specific deployment step, but we could still make use of a multi-package script step to update our ECS Task definition and the ECS Service. This will allow us to use Octopus to control the Docker image release version deployed throughout our deployment pipeline, as well as manage the different variables, settings & environment variables that are necessary to be supplied to the ECS Task Definition. Above is what an Octopus code deployment pipeline at Haptik looks like. More or less all of it means that code can be deployed simply by firing a few Jenkins jobs. Let us briefly discuss the process of how it works behind the scene. Getting Started Signup here: https://octopus.com/ Create a new project and add a Run an AWS CLI Script step to your project: We use the Docker images pushed to **AWS ECR** during the continuous delivery pipeline and the images are tagged with release version which makes it is easier to identify which version to push out to our clients & we do a blue-green deployment with the help of Amazon ECS. Add the ECR feed to Octopus Deploy You will then need to supply your AWS credentials and region that the AWS Elastic Container Registry is in. Give necessary privileges to the repository for the secondary/client account to access the Elastic Container Registry Deploying Image to AWS ECS Enter the AWS Region that the ECR services are located in and select the AWS account that has the necessary permissions to create ECR Tasks and update the ECR services: Skip down the Referenced Packages section and add the Docker Image that we added to our ECR feed. For this image, we don’t need to do any package acquisition since that will be handled by AWS itself. So selecting the package will not be required option. We will be breaking up the AWS CLI script in 3 parts: 1. Defining the containers # Port Mapping $ PortMappings = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.PortMapping]\" $ PortMappings . Add ( $ ( New - Object - TypeName \"Amazon.ECS.Model.PortMapping\" - Property @ { HostPort = 0 ; ContainerPort = 80 ; Protocol = [ Amazon . ECS . TransportProtocol ] :: Tcp } ) ) $ MountPoints = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.MountPoint]\" $ MountPoints . Add ( $ ( New - Object - TypeName \"Amazon.ECS.Model.MountPoint\" - Property @ { ContainerPath = \"/opt/models\" ; SourceVolume = \"Models\" ; ReadOnly = \"true\" ; } ) ) # Environment Variables $ EnvironmentVariables = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.KeyValuePair]\" $ EnvironmentVariables . Add ( $ ( New - Object - TypeName \"Amazon.ECS.Model.KeyValuePair\" - Property @ { Name = \"ENVKEY\" ; Value = $ OctopusParameters [ \"envkey_token\" ] } ) ) # Ulimits $ Ulimits = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.Ulimit]\" $ Ulimits . Add ( $ ( New - Object - Type \"Amazon.ECS.Model.Ulimit\" - Property @ { HardLimit = \"64000\" ; SoftLimit = \"64000\" ; Name = \"nofile\" ; } ) ) # Logging driver $ LogOptions = New - Object \"System.Collections.Generic.Dictionary[String,String]\" $ LogOptions . Add ( \"awslogs-group\" , $ OctopusParameters [ \"aws_log_group\" ] ) $ LogOptions . Add ( \"awslogs-region\" , $ OctopusParameters [ \"aws_region\" ] ) #Define Container Write - Host \"Adding Container Definition for\" $ OctopusParameters [ \"Octopus.Action.Package[word_embeddings].Image\" ] $ ContainerDefinitions = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.ContainerDefinition]\" $ ContainerDefinitions . Add ( $ ( New - Object - TypeName \"Amazon.ECS.Model.ContainerDefinition\" - Property @ { ` Name = \"base_service\" ; ` Image = $ OctopusParameters [ \"Octopus.Action.Package[word_embeddings].Image\" ] ; ` PortMappings = $ PortMappings ; ` Essential = \"true\" ; ` Memory = $ OctopusParameters [ \"mem_hard_limit\" ] ; MemoryReservation = $ OctopusParameters [ \"mem_soft_limit\" ] ; ` MountPoints = $ MountPoints ; ` LogConfiguration = $ LogConfiguration ; ` Ulimits = $ Ulimits ; ` Environment = $ EnvironmentVariables ; } ) ) # Volume $ Volume = New - Object \"System.Collections.Generic.List[Amazon.ECS.Model.Volume]\" $ Volume . Add ( $ ( New - Object - Type \"Amazon.ECS.Model.Volume\" - Property @ { Name = \"Models\" ; Host = $ ( New - Object - Type \"Amazon.ECS.Model.HostVolumeProperties\" - Property @ { SourcePath = \"/opt/models\" ; } ) ; } ) ) We have to explicitly set the environment variables into the container definition for this task. Notice that when providing the image details, we are using the Octopus.Action.Package[web].Image variable described above. This value will be derived from the image version selected during the release. 2. Create Task with updated Container Definition # Create Task $ Region = $ OctopusParameters [ \"Octopus.Action.Amazon.RegionName\" ] $ TaskName = $ OctopusParameters [ \"taskdef_name\" ] $ ExecutionRole = $ OctopusParameters [ \"aws_iam_role_arn\" ] Write - Host \"Creating New Task Definition $TaskName\" $ TaskDefinition = Register - ECSTaskDefinition ` - ContainerDefinition $ ContainerDefinitions ` - Family $ TaskName ` - Region $ Region ` - Volumes $ Volume ` - RequiresCompatibility \"EC2\" if ( ! $ ? ) { Write - Error \"Failed to register new task definition\" Exit 0 } Write - Host \"Created Task Definition $($TaskDefinition.Family)\" Write - Host \"Created Task Definition $($TaskDefinition.TaskDefinitionArn)\" Write - Verbose $ ( $ TaskDefinition | ConvertTo - Json ) By loading the task name from an environment variable, it means that we can vary the task per-environment (and tenant if relevant) which allows us to have multiple task definitions for our different deployment contexts. 3. Upgrade the Service to Use the New Task # Update Service $ ClusterName = $ OctopusParameters [ \"ClusterName\" ] $ ServiceName = $ OctopusParameters [ \"ServiceName\" ] Write - Host \"Updating Service $ServiceName\" $ ServiceUpdate = Update - ECSService ` - Cluster $ ClusterName ` - ForceNewDeployment $ true ` - Service $ ServiceName ` - TaskDefinition $ TaskDefinition . TaskDefinitionArn ` - DesiredCount 2 ` - DeploymentConfiguration _ MaximumPercent 200 ` - DeploymentConfiguration _ MinimumHealthyPercent 50 Putting it All Together The Octopus process looks something like this: We then add the following variables which will supply configuration for both the ECS infrastructure itself and the details we want to push into the container: Deployment with Jenkins Jenkins builds the code and runs tests, while Octopus takes care of: Distributing applications to all the remote machines, securely Environment-specific configuration, like connection strings & environment variables Kicking off a deployment you should also notice that although we are using a package (the image), there is no acquisition that takes place. This is because Octopus is just providing the values describing the package for use in our scripts. When the deployment executes the ECS service will run new tasks and, based on the DesiredCount , DeploymentConfiguration_MaximumPercent , and DeploymentConfiguration_MinimumHealthyPercent configuration ensure that the correct number of tasks are active at any given point. This results in a Blue-Green style deployment. 1. Get the API key from Octopus. 2. Add the key to the octopus plugin to Jenkins 3. Under Configure System setting add the Octopus credentials: 4. Configure the Jenkins to take in the release version and the Octopus package configuration: You’re all set. An end to end pipeline is ready to deploy an ECR image with a specific tag to any AWS account’s ECS service. All that is on the developer’s plate is to trigger the Jenkin’s job with the correct GitHub release tag. Hope this blog helps you setting up an ECS deployment pipeline. We will soon come up with more such blogs. We are hiring for our Architecture DevOps Team at Haptik. Do check out our careers page . Posted by Divneet Singh on May 8, 2019 6:55:00 PM", "date": "2019-05-8,"},
{"website": "Haptik-AI", "title": "Open Sourcing Chatbot Ner", "author": [" Admin"], "link": "https://www.haptik.ai/tech/open-sourcing-chatbot-ner/", "abstract": "Tweet Chatbot? Evolution of automated messaging, which started in 1966 with first Chatbot, ELIZA , has now reached a stage where Chatbots have found their application in several industry domains like personal assistance, banking, e-commerce, healthcare, etc. With early experiments showing positive results for many enterprises, the need for building customized and domain focused Chatbots for specific applications is increasing exponentially. Building a bot which fixes real industry pain points is a combination of design, engineering and research problem. While platforms and frameworks available for addressing engineering and design problems have unidirectional focus, resources addressing research/machine learning for Chatbots are highly generic and scattered. During our journey at Haptik , we ended up building and customizing different machine learning modules specifically focused on building Chatbots on narrow domains and which are targeted at an end to end completion of a specific task such as making travel bookings, gift recommendation, and ordering, lead generation for different businesses, etc. In this blog, we will talk about one of our integral module Chatbot NER i.e. Named Entity Recognition (NER) which we have open-sourced specifically to facilitate the intelligence of Chatbots targeted at domains like personal assistance, e-commerce, insurance, healthcare, fitness, etc. There are many approaches (i.e. Generative based, Retrieval based, Heuristic based, etc.) used to build conversational bots or dialogue systems and each of these techniques make use of NER somewhere or the other in their respective pipeline as it is one of the most important modules in building the conversational bots. Apart from functionalities available in conventional NER systems, Chatbot NER contains several add on’s which are specifically aids in building Chatbots. So What Is Chatbot NER? NER is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as the name of a person, location, organization, contact detail, expressions of time, quantity, monetary value, percentage, etc. For example, ```json \"Remind me to call Mainland China day afer tommorrow at 6:00pm\" ``` In this example: - *Mainland China* is a named entity that belongs to category *restaurant* - *day after tommorrow* is a *date* - *6:00pm* is a *time* Chatbot NER is heuristic based that uses several NLP techniques to extract necessary entities from chat interface. In Chatbot, there are several entities that need to be identified and each entity has to be distinguished based on its type as a different entity has different detection logic. Following is the brief hierarchical representation of the entity classification which we have used in Chatbot NER: We have classified entities into four main types i.e. * numeral *, * pattern *, * temporal * and * textual *. ** numeral **: This type will contain all the entities that deal with the numeral or numbers. For example, number detection, budget detection, size detection, etc .** pattern **: This will contain all the detection logics where identification can be done using patterns or regular expressions. For example, email, phone_number, PNR, etc. ** temporal **: It will contain detection logics for detecting time and date. ** textual **: It identifies entities by looking at the dictionary. This detection mainly contains detection of text (like cuisine, dish, restaurants, etc.), the name of cities, the location of a user, etc. For an in-depth explanation of the approach please have a look at our Approach Documentation . Why use Chatbot NER? There are a number of NER’s available like Stanford NER, spaCy NER, etc. but none of these are designed specifically for building Chatbots. There are a lot of customizations required in existing NER’s and we have done it all for you in Chatbot NER . We have already added few entities like *restaurant names, cuisine, city list, time, date, etc.* Please, have a look at our Built-in Entities Documentation . Consistency and normalization in output format across different entities reduces the engineering effort required to plug it in your system. You can create/update entities by just mere adding data. Have a look at the documentation on How to add your own entities? Follow just a few steps to host it on a different server and run it as a separate service. We are actively working in this area if there are any doubts or issues while setting up or using our service, let us, Haptik , know and we will fix it as soon as we can 🙂 Installation Steps Please, have a look at our installation steps to install Chatbot NER on your system. Conclusion We hope that Chatbot NER helps you add more utilities to your Bot and ease out the process of detecting entities. We do expect feedback from users so that we can improve the same. Currently, our chatbot is a heuristic based that uses several NLP techniques to extract necessary entities from chat interface and we are soon going to come up with the upgraded version of this repository by integrating our ML models into it. Let’s hope that this repository comes up as a powerful resource and contributes to research and engineering community. For, any questions or support related to this repository, do leave your comments below. Also, don’t forget, we are hiring high-quality engineers. So, if you are interested reach out to us at hello@haptik.ai Posted by Admin on Jun 26, 2017 10:20:00 AM", "date": "2017-06-26,"},
{"website": "Haptik-AI", "title": "Haptik AI: Innovation must never stop", "author": [" Krupal Modi"], "link": "https://www.haptik.ai/tech/ai-innovation-must-never-stop", "abstract": "Tweet Last year exposed all of us to a global uncertainty no one ever anticipated. It cost us billions of lives and posed new challenges for economies to survive. At the same time, it also highlighted the need for innovation as healthcare workers and other professionals stepped up to adapt to the new normal & selflessly help others. We at Haptik also did our bit to help during the pandemic. We built the official MyGov Corona Helpdesk, the world’s largest WhatsApp chatbot for the Indian Government. We made this chatbot within five days to help answer queries of 25 million+ users and tackle the spread of misinformation. Innovation and disruption have always been the key to Haptik’s success. Our team showed incredible persistence in between all the chaos and challenges by putting in continuous effort into research and development to push conversational AI boundaries. The applied research focus at Haptik on the following tracks - Contextual and precise conversations Data efficiency and continuous learning Pushing boundaries of NLU We conducted several experiments in all the mentioned tracks to add significant value for our end customers. Here are our top highlights from each of the ways mentioned: Towards contextual and precise conversations Although building contextual assistants remains an overarching goal, most virtual assistants get consumed in the greed of answering as many users queries as possible . Instead of optimising higher automation at the cost of end-user experience, we started the year with an initiative to enable Haptik’s IVA to probe for clarifications and confirmations from users in a graceful way NLU models are not highly confident. These capabilities were further available to platform users in the form of Smart Assist , and it gave a significant upgrade to 9% of experiences where AI models needed assistance in disambiguation. Identifying user intent is one of the most critical components for contextual conversations. While most conventional platforms focus on improving accuracy by training intent detection models on user utterances from the training data, we explored the usage of bot utterances to improve our accuracy. By taking into account the specificity between user utterances and bot utterances, we could push our f1 score by 5% without the need for any additional training data from bot developers. Towards data efficiency and continuous learning We all know that generation of paraphrases using GPT-2 / 3 was delightful to see in Jupyter notebooks and OpenAI playground. At Haptik went a step ahead and created an algorithm to automatically rank paraphrases and select less than five paraphrases per original data point, and still maximise the intent identification model’s performance. - Haptik’s conversational AI platform does so by using paraphrases to probe the model trained                        on original data. - We also ensure the diversity in paraphrases with the help of entailment score and BLEU score and manage to keep the noise under 10% in augmented data. While we currently use the GPT-2 model fine-tuned for paraphrase generation, and it helps us bring down data requirements by 33%, we love the way progress in NLG is helping NLU, and we are on our path to reduce seed data requirement even further with the help of GPT3. Do watch this space for more as we will share our work this year as we make progress. While we ensure that our platform provides the highest accuracy with minimal seed data, it is pretty established that nothing can teach a model better than actual user queries. On production, virtual agents are exposed to diverse user queries from various demographics. These queries spread across several existing and unforeseen intents that may not be in the scope of seed training data. To ensure an efficient feedback loop, we built an AI recommendations module to discover new user intents with more than 85% accuracy and provide a faster way to improve virtual agents. We believe that this is a significant step towards the future of truly self-learning virtual agents. Towards pushing boundaries of NLU Pushing the research community towards more realistic NLU models’ benchmarking, we introduced and open-sourced a new dataset called HINT3 and presented the same at EMNLP 2020. We also introduced a method to create complex train sets to enable benchmarking over scenarios of imbalanced datasets, granular labels, and unintended correlations in the data. Furthermore, our Datasets and methods were also recognised by researchers at IBM Watson to benchmark their capabilities alongside Haptik and other global technology leaders. In a world where building larger language models and complex NLU pipelines are ubiquitous, our team truly believes that sharing weights across multiple tasks is a more sustainable and efficient way to build NLU architecture moving forward. To help the machine learning community with the same, we open-sourced the framework to quickly enable researchers and engineers to prototype their NLP tasks in a Multi-Task-NLP setup. While this framework is currently in a nascent stage, we hope it motivates the community to innovate further to build more efficient training and inference architectures. To ensure that all our customers and end-users get the most impact from the progress of our AI initiatives, our product teams migrated more than 200 Virtual Assistants from conventional dialogue systems to state-of-the-art systems. While the team at Haptik is committed and focused on pushing the boundaries of Conversational AI, we are able to do so with Fastai , Huggingface and Openai frameworks, and we are thankful for the same. With great support from our customers, the AI research community, and our fantastic engineering team, we at Haptik are confidently looking forward to pushing AI’s limits to build great conversational experiences in the time to come. If you wish to join us in our journey of enabling great conversational experiences, do explore Careers at Haptik . We are hiring! Posted by Krupal Modi on Apr 19, 2021 8:09:55 PM Find me on: LinkedIn", "date": "2021-04-19,"},
{"website": "Haptik-AI", "title": "Charles Proxy for Mobile Apps Testing", "author": [" Neha Pawar"], "link": "https://www.haptik.ai/tech/charles-proxy-mobile-apps-testing/", "abstract": "Tweet At Haptik , we have our flagship mobile app built for Android & iOS. We have also built our SDKs (both Android & iOS) which could be plugged into any other app out there in the market and they could make use of Haptik chatbots inside their apps. There is actually a lot happening inside a mobile app. There is storage, network, computation, handling user messages, giving back responses via API calls etc. On our apps we make use WebSockets to send and receive messages, make various API calls for handling various features on the app and a lot more. To find issues in our app and make the end user experience better we wanted to test out different scenarios like bad network connectivity, WebSockets not working, slow third-party API responses etc. We wanted a tool that would make testing & debugging these scenarios easier and give more visibility into the app performance.  After a lot of searching online, reading blogs & getting our hands dirty with a few tools we shortlisted two tools: Fiddler and Charles. We compared Charles Proxy to Fiddler and observed that Charles Proxy is comparatively easier to use. In terms of UI, Charles  Proxy is more user-friendly and easy to use. You can read about fiddler here . In this blog, we will discuss how to get started with Charles Proxy and how it helped us in mobile testing. Charles Proxy Some features of Charles Proxy are : Records all traffic between your browser and the Internet. Reveals the contents of all requests, responses, cookies and headers Supports SSL and HTTPS Saves valuable time Simulates slower internet connections. Yes, Charles Proxy handles this on the go. This is how you can enable handling of slower connections Download statistics Easily Configurable Find and eliminate bugs quickly. How to download and set up? Charles Proxy can be downloaded using their website. If you want to set up it up to handle HTTPS traffic, that can be done under the tools menu > SSL Proxying Settings. Testing different scenarios using Charles Proxy Throttle network to test WebSockets & APIs Create error scenarios by modifying APIs API reviews Throttle network to test WebSockets & APIs Throttle functionality helps to adjust the bandwidth and latency of the internet connection. This helps to simulate 2G/3G/4G over the high-speed network like WIFI. This enables us to test the app’s behaviour in poor network connections. Testing on slow networks is a very important scenario for chatbot app like ours as there are functionalities which require the user to be notified without any delay. For eg: Reminders bot: We have a flagship bot on our platform where users can chat with the bot and set: 1. Wake up call, reminder to call someone 2. Set drink water or medicine reminders. 3. Reminder notifications to Play quiz every day on specific time. Along with a personalized message, the user also receives a personalized call from Haptik to remind him/her about the event. We needed to test what happens during network latency, or interruptions and how the app handles it. We were definitely able to test all that using network Throttle feature. Create error scenarios by modifying APIs We can create multiple different error scenarios by rewriting the response of different APIs. This is a very handy feature when it comes to empowering the QA Engineers at Haptik as it allows them to reproduce error scenarios by using the below features : 1. Manipulating any URL requests and trigger the request again 2. Rewriting API URL 3. Modifying & Rewriting authentication headers 4. Altering user agent, to test different browsers etc, for web-views etc. By adding headers we can understand how the app would react if it receives this from a server. Steps to use the rewrite feature: 1. Select rewrite option under tools: 2. Add the name and location: 3. Select the Rewrite rule type as shown below, or eg. Body, headers, response status. 4. After selecting replace and change the value of selected type and click on ok: API reviews Before shipping any major feature we try and do a basic API review to make sure no unnecessary API calls are being made by the app. Doing this is as easy as connecting the devices to Charles Proxy and observing and recording the API calls. Charles Proxy is helpful in finding security bugs & loopholes. During API reviews this tool can help provide such insights: Verifying if API is working properly. This means that when we do an API call, what headers are sent and is the response correct with correct headers as expected. Debug if API response is incorrect. Checking if any redundant API or web requests made. For eg., We had an API to save some details on our backend. While testing the API on Charles Proxy, we came to know that the API call was being made multiple times which lead to false app behaviour. Once detected, we asked the developers to make the necessary changes. Charles Proxy can be very handy when it comes to integrating a backend with a mobile app. It helps to identify & eliminate a lot of issues and thereby improved performance & security. Conclusion The above three functionalities help developers and testers at Haptik while debugging mobile applications and native web views. It acts as a middleman and helps make these changes on the fly. The biggest advantage is that one can get a peek under the hood of the application and quickly find and identify bugs and also improve performance. Charles Proxy definitely helps save time & money. Do give it a try and comment below if you liked what we shared. Posted by Neha Pawar on Sep 26, 2018 7:25:00 PM", "date": "2018-09-26,"},
{"website": "Haptik-AI", "title": "Automated Deployment", "author": [" Moses Gangipogu"], "link": "https://www.haptik.ai/tech/automated-deployment", "abstract": "Tweet Managing large development and production environments is a long and complex process. At a startup where developers want to deploy multiple times a day managing certificates, building libraries and deploying thousands of lines of code from one environment to another manually can be time consuming and error prone. Worst of all (*my heart is sinking just writing this*) it can cause downtime, which at a company like Haptik, where we provide service 24/7 is not acceptable. To reduce risk and time of deployment, automation plays a very important role. We use a lot of cool tools to help automate our deployment processes. git: ( https://github.com/ ) Unlike the meaning of the word ( noun: an unpleasant or contemptible person ) , git is and should be every engineer’s best friend. Git is a code management and version control system which allows multiple people to work on the same code base at the same time. We use github to host all our repositories. Ansible ( http://www.ansible.com/ ) Ansible is any DevOp person’s right hand. Ansible is a config and deployment management tool written in Python. Since Haptik is a python shop on the backend, Ansible fits right in. We use Ansible to setup the servers, make sure certificates and configs are set right and deploy code from github to all the different servers required. You can build some really complex flows with just a few lines of code in Ansible. Fastlane ( https://fastlane.tools/ ) Fastlane is an awesome tool used for both iOS and Android Continuous Deployment. It helps you build custom pipelines for different environments along with integrating all third party services like gradle, cocopods, crashlytics etc. Disclaimer: We don’t use fastlane as efficiently as we would like to, but we are working on that. Rundeck ( http://rundeck.org/ ) Say what you may but developers hate typing more than what is required. That’s where Rundeck comes in. Rundeck helps with runbook automations and by using Rundeck you can allow developers to deploy at the click of a button. It’s got a nice admin interface which makes deployments look a little less tedious than they already are. Slacktree ( https://github.com/course-hero/slacktee ) Slacktree is a command line tool for Slack integration. We use Slacktree to inform the whole team about the progress of the deployments. p.s: If you don’t use Slack at your company, use it right now. It will change the way your company works. So, to sum it up o ur final pipeline for the backend looks something like this: Start release: Rundeck Update the team about a new release starting: Slacktree Take server out of LB: Ansible Setup / update environment and settle certificates: Ansible Clone the repository on the servers: Ansible Restart services: Ansible Bring server back into LB: Ansible Repeat for all different servers Update team the release is completed: Slacktree Our mobile pipeline looks similar except we use Fastlane for a lot of the automation in the middle. Using the above pipeline we take about 5 minutes to complete the releases on 12 servers and have had 0 downtime during the release. Our deployment needs improvements and as next steps we plan on integrating Jenkins and automated tests as part of the pipeline. What do you think about our pipeline? We would love to hear your feedback/suggestions in the comments below. And if you don’t what the hell Haptik is, and why automating deployment can be extremely helpful. Check the app out here. Wish to be a part of the amazing things we build? Look no further! Reach out to us at hello@haptik.co Posted by Moses Gangipogu on Jan 28, 2016 6:01:00 PM", "date": "2016-01-28,"},
{"website": "Haptik-AI", "title": "Testing Challenges In An Agile Environment", "author": [" Sanchita Mishra"], "link": "https://www.haptik.ai/tech/testing-challenges-in-an-agile-environment/", "abstract": "Tweet What is Agile? Agile management , or agile process management , or simply agile refers to an iterative, incremental method of managing the design and build activities of engineering, information technology and other business areas that aim to provide new product or service development in a highly flexible and interactive manner. A few surveys have found that adoption of Agile has almost doubled from 35 percent to 76 percent. Shows how the development across the world is changing and showing how successful it is. But do remember, Agile may or may not be suitable for your project/company. To read more about Agile, you can go here . JIRA is an Agile Project Management tool from Atlassian . At Haptik , we started using JIRA so as to help every member of our software team to plan, track and release great software. Phases We Follow In JIRA Plan a Feature Build Story for each feature Write Test cases associated with each feature Track Feature Bug Code Release Agile changed our approach to development but testing sometimes still comes as an afterthought. In some ways, testing becomes even more challenging: Due to Adhoc changes in a feature Development is delayed due to some issues in the current sprint cycle and that does not leave enough time to perform testing Sometimes silos across testers, developers and product team is another challenge+ Team Approach Towards Agile Testing We split the testing into two parts; tests directly related to the user stories developed during the sprint, and second basis everything else. Then we defer the “everything else” to the end of the release, which happens every couple of months. Here’s the truth: all we can do during the sprint is pretty much just the functional testing. We’re not doing much of the integration, regression or performance testing until right before the release. We call this final system test a “regression sprint” or “hardening sprint”. Sounds familiar? This is a very typical Agile implementation across the industry: The real goal should be to eliminate the hardening sprint and to complete all types of testing during the sprint, however, short or long it is. This way every sprint would end with a set of fully tested features that could potentially be deployed to production or shipped to the customer. There would be no need for the context switch at the end of the release, no last minute defects, and costly delays: Effective Way to Approach Challenges in Testing Environment Collaborate QA will only be successful if it operates as an integral part of the development team and larger engineering organization. Due to collaboration they would achieve the end goal easily due to understanding of objective to achieve Test in Parallel Engage all the team members in Testing. QA can work on the black box testing while developers can do White Box Testing. Include Product Managers for feedback and do UAT for the product. Avoid working in silos. Plan Realistically Include all the testing in the Workflow as To Do , In Progress , Done . If we miss this we may end up Deferring more Bug at the Regression Test. If we follow this we may the end up testing whole product till the end. We should plan stories more realistically Create Test Automation Performing Manual Component, Integration, Regression testing is not a robust idea for testing Product. Start Identifying the mission-critical workflows, Scenario automate them and run daily as part of Deployment process This is one of the Best QA Mantras: “QA is to Deliver quality product by means of Continuous Feedback and Defect prevention rather than Defect Detection” That being said, we have now started growing our QA team. I would love to hear your thoughts and suggestions around QA and how we can build quality products. I would be posting more such important blogs in the near future. Also, don’t forget, we are hiring high-quality QA engineers. So if you are interested reach out to us at hello@haptik.ai . Posted by Sanchita Mishra on Jan 20, 2017 6:33:00 PM Find me on: LinkedIn", "date": "2017-01-20,"},
{"website": "Haptik-AI", "title": "Managing Chatbot Data Using Elastisearch", "author": [" Deep Singh Baweja"], "link": "https://www.haptik.ai/tech/managing-chatbot-data-elasticsearch/", "abstract": "Tweet A good chatbot requires a lot of structured data, in order for it to carry out an enjoyable conversation with users. Entering this data, however, is not the most delightful task in the world. Being one of the world’s largest conversational AI platforms, we have built tools that help make this task not just easy, but also fun! Our chatbot platform has many tools such as a bot builder, analytics dashboard, admin portals and a desktop chat tool for humans to take over. We use a ReactJS based bot-builder tool which is one of our offerings on the Haptik platform. This tool updates various databases such as MySql, MongoDB, Elastic search and Redis which the chatbot uses in real-time. This is what it looks like: Chatbot development process takes place in our staging/UAT environment. This is where we use the bot builder tool to build the bot from scratch or a predefined bot template. While building the bot we add all the chatbot specific data and train the bot which pushes data into various data stores. Within this environment, one can test the bot (can be seen on the left bottom corner of the above screenshot) and know if it is working as expected. Next step is to move this bot to production. Re-creating the bot again in the production environment with all the data would be a nightmare. So, for this, we built a feature using which one can transfer an entire chatbot from staging to the production environment. During the above transfer process, Elasticsearch was the most time-consuming of the lot, it literally accounted for 80% of all the data that needed to be moved, mainly because that bit of logic was always shared across all our deployed chatbots. The Elasticsearch transfer alone took 15 mins. This was acceptable when there were bots transferred once on a weekly basis, but we soon found ourselves in a situation where we were shipping 10 Chatbots on a daily basis. We clearly had work to do, so let’s go ahead and understand the problem in detail, and how we fixed it using a beautiful concept called aliases in Elasticsearch . What the System looked like earlier This was our legacy system, built once during our initial stages and not upgraded for over a year. The diagram below depicts the older process of managing Elasticsearch data: When our bot builder tool requests to transfer a bot, it also means it’s time to transfer all the Elasticsearch data to our production environment. Here is a simplified algorithm we used to do the same: (refer to the above diagram) 1. We copy the current state of the data in staging live index to prod temp index 2. We take a backup of the prod live index to prod backup index 3. Delete prod live index 4. All endpoints now rely on CloudFront caching: What allowed us to delete prod live index on Elasticsearch was AWS CloudFront CDN  caching. We cache responses for various URLs and during that time the request would directly be served via CF without touching our backend application. 5. Copy prod temp index into new created prod live index 6. Prod live index now has the latest data We mainly used elasticdump to perform most of the above actions, we also make sure that we copy the settings and mappings of the source index. Take a look at the code snippet: elasticdump -- input = https : //elastic_search_host:port/index_name  --output=https://elastic_search_host:port/index_name --type=analyzer elasticdump -- input = https : //elastic_search_host:port/index_name  --output=https://elastic_search_host:port/index_name --type=mapping elasticdump -- input = https : //elastic_search_host:port/index_name  --output=https://elastic_search_host:port/index_name --type=data For more information on elasticdump, please refer this . The above process took at least 10-15 minutes, mainly because of the multiple copies and backups of a huge data set. Clear Problem Areas as Identified: 1. Data was stored in a very monolithic manner. 2. Unnecessary temp index was present on our production Elasticsearch environment. 3. Transferring all data for a single bot took 10-15 mins. 4. Transferring Multiple Bots simultaneously was not a possibility. 5. Elasticsearch Indices setup in production and staging environments were not the              same as on prod we had three indices (temp, backup and live), and on staging, we            only had one which was treated as the live index. Potential Solutions Elasticsearch Aliases As the system matured with us, we reached a point where at least 10 bots were being transferred a day, we now needed to upgrade the expensive Elasticsearch data transfer process into a seamless 10-20 seconds activity. Elasticsearch Aliases as the name suggests allows us to create a pointer of sorts, that will behave like an index when queried for, and we can point it internally to multiple indices, quickly and seamlessly. We could definitely use this since changing the index to which an alias points to is inexpensive. Thereby, we could also reduce the total amount of data being transferred and the concept had a lot of potential. To add and remove indexes for an alias, a simple query like the one below would create the alias if it does not exist, moreover, it would remove index_one if it was pointing to it, and add index_two, again if it was not already pointing to it. POST / _aliases { \"actions\" : [ { \"remove\" : { \"index\" : \"index_one\" , \"alias\" : \"alias_name\" } } , { \"add\" : { \"index\" : \"index_two\" , \"alias\" : \"alias_name\" } } ] } For complete information on what an alias is please refer here . Analyzing our data We then ran some analysis on our Elasticsearch data. Our aim was to figure out which part of our dataset changed frequently, rarely, or never. Once we were equipped with this information, we were able to use aliases to our advantage. Here is what we found: 1.  We found that around 70% of our data was not changing at all . 2. 20% of our data rarely changed, like once a month. 3. 10% data was extremely dynamic in nature. 4. We started off with a simple setup script that we could run across all environments. First, the setup script would segregate the environment’s live index into 3 small indices, and also create an alias, which would point to 3 smaller, newly-created indices. Our permanent data and rarely changing data would now be separate from our dynamic data. Thus, our systems would be ready for an improved transfer process. The basic algorithm for architecture setup: 1. Pull out 80% of the static data that does not change 2. Create an index say permanent_data and dump 70% permanent data there 3. Create another index say dynamic_data and dump 10% there 4. Create the last index for rarely changing system_data and dump 20% there 5. Create an alias that points to permanent_data, system_data and dynamic_data 6. Delete & remove the dependency from the prod index 7. Finally, rename alias to prod index name From an endpoint perspective, there were no changes required, so all underlying systems continued to work as expected. Let's put it all together now! Our permanent chatbot training data which accounts for 70% of the data is maintained in one separate index. System chatbot data (20%) which changes like once a month on average is maintained on a separate index. Dynamic chatbot data (10%) in its own dedicated index, two copies will have to be maintained, let’s say version_1 and version_2 Also, note that the current setup is consistent across dev, staging and prod owing to our setup script. To understand this, let’s consider the scenario that currently Staging alias points to version 1 and Prod alias points to version 2: 1. When the transfer button is hit on staging , the data to be transferred is pulled from staging version 1 (as that is the current live index of sorts in our staging environment). 2. Delete prod version 1 3. ES settings and mappings of staging version 1 are copied as settings in prod version 1 4. Data for prod version 2 (live) is copied into prod version 1. 5. Delete and update query (as generated in step 1) is now inserted into prod version 2. 6. Change Alias to point to prod version 2, and remove version1. 7. Version 1 is now logically considered the backup. Note: When copying settings and mappings across two different environments it is essential to first create a blank index on the destination environment, with the new mappings and settings and then copy the data into the destination index. The code snippet below shows how we copy source index’s settings, mappings and eventually the data into the destination index: # when we simply request for an index, without any query we receive its mapping and setting information. index_to_backup_url_response = requests . get ( index_to_backup_url ) source_index = json . loads ( index_to_backup_url_response . content ) [ index_to_backup ] # fields that we do not want to copy, so popping them out of the dictionary source_index [ \"settings\" ] [ \"index\" ] . pop ( \"creation_date\" , None ) source_index [ \"settings\" ] [ \"index\" ] . pop ( \"uuid\" , None ) source_index [ \"settings\" ] [ \"index\" ] . pop ( \"provided_name\" , None ) source_index [ \"settings\" ] [ \"index\" ] . pop ( \"version\" , None ) source_index . pop ( \"aliases\" , None ) # deleting the backup_index and creating and empty backup index with new mappings and settings. requests . delete ( backup_index ) requests . put ( backup_index_url , json = source_index ) # es query that will copy data from one index to another # size indicates in how many chunks the data should be copied. final_request_dict = { \"source\" : { \"index\" : index_to_backup , \"size\" : 10000 } , \"dest\" : { \"index\" : backup _ index } } reindex_response = requests . post ( '{es_url}/_reindex' . format ( * * { 'es_url' : es_url } ) , json = final_request_dict ) To understand what we mean by indices, mapping and setting, please refer here . Another part of our code that will delete and update specific records on an index, we used Elasticsearch helpers for this. Note: Elasticsearch helpers is an open source python library maintained officially by Elasticsearch itself. It is a collection of simple helper functions that abstract some specifics of the raw API, we specifically used it for its bulk update functionality. from elasticsearch import Elasticsearch , RequestsHttpConnection from elasticsearch import helpers # updated_records contains individual records with destination index information es_destination_connection = Elasticsearch ( hosts = [ { 'host' : ip , 'port' : int ( port ) } ] ) helpers . bulk ( es_destination_connection , updated_records , stats_only = True ) For more information on Elasticsearch helpers, refer here . The below diagram represents what the Elasticsearch indices architecture looks like after the systems were updated along with its underlying code. Yes, now all our environments are in sync, all our data moves seamlessly and we are capable of deploying several bots on a daily basis. Conclusion The amount of data being transferred across environments has been reduced by 70% We have also managed to reduce the total amount of data that needs to be eventually stored and maintained on our Elasticsearch servers across all three environments. No more Temp index or Backup Index Backup of only the 10% dynamic bot specific data We now transfer chatbot specific data in under 15-20 seconds as compared to the earlier 15 minutes. Now that’s an optimization task done right. Posted by Deep Singh Baweja on Aug 22, 2018 7:21:00 PM Find me on: LinkedIn Twitter", "date": "2018-08-22,"},
{"website": "Haptik-AI", "title": "Empowering Developers at Haptik", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/empowering-developers-haptik/", "abstract": "Tweet Over the last couple of years, Haptik has built and deployed 100+ chatbots on its platform. These chatbots depend on a complex backend for their functionality. Once a message is sent from the user numerous algorithms are triggered to generate a response for the user. Frequent code addition, the large number of iterations & bug fixes often need to go live as soon as possible and helps us improve quality. Over time, we have also built a lot of micro-services to make our platform featureful. Initially at Haptik, during a code release, we would all (4-5 people) sit together and start the release by logging into the servers, pulling the code and restarting the application one by one on each of the servers.  One person would monitor the logs and check for any errors, while another would check New Relic for any application errors. Due to this complicated process, we would do releases once a week. We called it the “ Release Train “, and for a single release, it took us at least a couple of hours to roll out all the code changes. It didn’t take us long to realize that this could become a bottleneck in the speed at which we wanted to ship features to the end user. The long release pipeline was slowing our growth, with even a small hotfix taking a lot of time. The solution to the problem is CI/CD (Continuous Integration/Deployment) & Monitoring, which more or less comprises of the following: 1. One click deployments and blue-green deployments for backend 2. An easy way to retrieve logs and get more insights into the application performance 3. CI tools for the mobile apps. E.g.: Bitrise & Buddybuild 4. Easy detection of bugs in the UAT environment 5. Checks for failed deployments and easy ways to rollback 6. Alerts for application errors There is a lot to go through so this will be a 3 part blog. In this post I will be covering the following in brief: Overview of CI pipeline – Lint review, Unit tests, Integration & API testing Overview of CD pipeline – One-click deployments Overview of  Monitoring and Logging We don’t have a build & release team . Firstly, at Haptik, the developers are in charge of their own releases. Secondly, all developers have their own EC2 machines where they can play around with the code as much as they want. Basically, every developer has the entire Haptik application suite setup for him/her and the data stays in isolation with others. So, a separate DB, Redis, Elasticsearch, etc. As a developer, you write your code, if your tests pass, you merge your code into develop, then master and carry out the release. Let me dig a little deeper into merging code. We follow the git-flow . Though git-flow comes with its own trade-offs, we have adapted to it very well. Below is an image which will give you a brief understanding of git-flow. Note: For a hotfix, we create a branch from master, raise a PR, and once the test cases pass and the PR is approved, we merge our code and again do the simple Jenkins clicks. I will be delving into the aforementioned aspects in greater detail in a future blog. The various coding languages that we use at Haptik are: 1. Python 2. Java 3. Objective C & Swift 4. Javascript (ReactJS) Part 1: Overview of CI Pipeline What happens when we raise a PR against develop branch? 1. Once the PR is raised, GitHub makes a webhook call to our Lint server and lint-review is triggered. ( Linting is the process of running a program that will analyze code for potential syntactical errors, wrong imports, & various coding standards). 2. Mergeability tests also run when a PR is raised. We iterate through various conditions like, does the PR have a label, does it have a description, is the changelog updated, etc. and post a status for the checks. 3. Along with that Jenkins is notified via a webhook and the Test cases job is triggered. We have a Testing environment, which is a clone of our staging environment. It has all the basic components, but is mainly used just to run the test cases, in the following manner: 1. When the Jenkins job is triggered, we pull the code in the branch for which the PR was raised, and run test cases. Most of our code has been written in Python, (Django) so we simply run tests along with tests coverage report. 2. Once the test cases finish running, we grab info from the coverage files and tests output and we push these as comments on the PR. If there is a failure, the developer would easily know what to fix. 3. We also shoot an email with the errors in test suites. 4. Only once the test cases and Lint review pass is the developer allowed to merge his/her code. 5. The PR also needs to be approved by one or more team members. This is similar across all our applications and microservices. Of course for Dockerized applications, we run tests within the containers. I will explain this further in a future blog. 6. ReactJS , which powers the frontend of our Platform Suite , also has a CI pipeline with tests running during the build time. So, once the code is ready, an automated job builds the artifacts and runs the respective tests. We are also integrating automated UI testing for this which is currently a Work In Progress. 7. For our Mobile apps , we use Bitrise for Android and Buddybuild for iOS CI. They are fairly simple tools to get started with and provide us with a great platform enabling our Mobile apps automated testing. Once a build passes all checks, we send the build to QA team via Email for them to take care of the rest. 8. We also have a pipeline setup for any new chatbot being built on the platform. We do run this during major code release so as to do sanity. The tool is totally built in-house and is called the Bot QA tool. So, that’s the CI pipeline for you. Part 2: Overview of CD pipeline How do we deploy the new code? Once you’ve been through the CI pipeline , Jenkins is your next stop. Deploying code at Haptik more or less means firing a few Jenkins jobs. Let us briefly discuss the process: 1. We have different Jenkins jobs to deploy and roll out different applications/microservices. Deploy simply means we update the code inside our Docker images, do a sanity check, push them to AWS ECR,   and if everything looks fine we do a blue-green deployment with the help of Amazon ECS. 2. These Jenkins jobs in the backend trigger Ansible playbooks which take care of pulling the code, building the images, running some sanity checks and finally reporting back with a status of success or failure. 3. For Mobile applications , the step to push latest app versions to play store and App store is manual for now. We definitely do plan to automate that in the near future. 4. Frontend code also has a deployment job which builds the artifacts from the input branch and pushes the artifacts to Amazon S3 from where they are accessed globally. 5. Release Notes – We have an automated pipeline for creating release notes. As soon as PRs go into master, our release-drafter drafts a release with a new tag. It groups the PRs under different sections like Bug Fixes, New Features & Enhancements. Once a release is published, we send an email to the entire team with the changelog using an automated Jenkins job. So, that’s the CI/CD process put simply. Part 3: Overview of Monitoring and Logging Last but not least. Various application & server logs are pushed to our Logging pipeline. We use both an ELK stack & AWS Cloudwatch Logs to handle these incoming logs and make it available to the developer. If any patterns like “5xx”, “error” “exception” etc. occur in the logs, we intimate the team so that they can look into it and implement a fix for them. Along with the logging pipeline, we have APM ( Application Monitoring) setup using New Relic agents. By making use of New Relic agent decorator in application code gives more insights into the execution of various tasks and functions. We also have a system in place where if % of errors for any application reaches more than 1% we raise alerts and these alerts also trigger PagerDuty . Haptik, being a chatbot company, we have also built various dashboards where users & developers can check how their bots are performing. Automation % –  % of chats being handled End to End by the bot , is an important metric for us and we have alerting in place for that as well. What have we achieved so far? Having implemented the processes mentioned above, we have successfully reached a point where we do 20 releases a week. And the best part is no one has to wait to push their code to production. Every application has its own pipeline setup which makes the process really simple and less time-consuming. The number of bugs in production has reduced by 90% and we have been able to achieve uptime & SLAs of the range of 99.99% Stay tuned for our future blogs where we shall discuss the following topics: 1. Continuous Integration pipeline, in-depth with all the tools we use 2. Continuous Deployment pipeline, in-depth with all tools we use 3. Monitoring, Logging & Data Lake pipelines 4. Learnings from the above 3 Do let me know in the comments how you empower the developers in your organization. Also, any feedback is welcome. Haptik is hiring enthusiastic engineers who want to help build next-generation conversational AI. Do check out our careers page for the same. Posted by Ranvijay Jamwal on Jan 18, 2019 6:50:00 PM Find me on: LinkedIn", "date": "2019-01-18,"},
{"website": "Haptik-AI", "title": "Smarter Approach To Cross-Platform Mobile Apps Testing", "author": [" Dikshit Shetty"], "link": "https://www.haptik.ai/tech/cross-platform-mobile-app-testing", "abstract": "Tweet Keeping scalability with quality in mind, it is always a good option to gradually move to automation so that we reduce the testing time and ship an effective product with minimum defects. Every organization has realized significant business benefits from intelligent automation including revenue generation, speed of execution, accuracy, compliance, and cost savings. We have also support IVAs on our iOS and Android SDK which can be shipped inside other mobile apps. In this article, we will be discussing how one can write automation tests for an App across the two most popular platforms (Android and iOS) efficiently and reducing the duplicity of code to the maximum. At Haptik , one such tool that helps us achieve all the aforementioned is the Appium testing tool. Before proceeding, let us discuss the Page Object Model (POM). The most widely used design pattern is Page Object Model (POM) which can be used with any framework in Appium or Selenium. POM has become popular in test automation for enhancing test maintenance and reducing code duplication. A page object is an object-oriented class that serves as an interface to a particular screen of the app to be automated. The tests then use the methods of this page object class whenever they need to interact with the UI of that page. The benefit with this is that if the UI for the page changes, the tests themselves need not be changed, only the code within the page object needs to be changed accordingly. A good read around POM here . Traditional Approach If you have an app across two different platforms, one approach is obviously by having two different teams to write the tests for Android and iOS respectively. Thus, in this case, we need to create two different projects for both platforms. So if you have a team of two or more, the automation could be easily maintained by writing tests on both the platforms by QA as per the team divided. Also, either of the teams can maintain different architecture for automation since there is no dependency. Smarter Approach But there could be an even better or smarter way to set up automation testing of a single app across Android and iOS. There could be cases where a single QA is handling the testing of an App on both platforms. So maintaining separate projects for Android and iOS automation test cases does not seem to be an efficient option since for the same scenarios, the QA will have to create separate page objects and write the tests twice for both the platforms and maybe also rewrite the tests separately in case of any functional or UI/UX changes made on that particular screen. So, we used a single project which has tests for both Android & iOS. In such scenarios, an even better approach would be having a single project consisting of test scripts for the app across both platforms, iOS, and Android. Here we include desired capabilities and drivers of both Android and iOS in a single class and assign capabilities for a given platform at any given point of time, thus giving the advantage of creating a single test suite that can be run on both Android and iOS with minor changes. The following is a class where we have defined both the desired capabilities in a single class: Single Class with both the desired capabilities: class TestDriver { public Driver setupAppium(String platform) { File demoAppPath = new File(\"Demo App\"); File demoApp = new File(demoAppPath, \"Demo.app\"); DesiredCapabilities capabilities = new DesiredCapabilities(); capabilities.setCapability(MobileCapabilityType.APP,    demoApp.getAbsolutePath()); // Here we’ll set capabilities according to the platform mentioned in executing testng file. if (platform.equals(\"ios\")) { capabilities.setCapability(MobileCapabilityType.DEVICE_NAME, \"iPhone 8\"); capabilities.setCapability(MobileCapabilityType.PLATFORM_NAME, \"IOS\"); capabilities.setCapability(MobileCapabilityType.PLATFORM_VERSION, \"13.2\"); capabilities.setCapability(MobileCapabilityType.AUTOMATION_NAME, AutomationName.IOS_XCUI_TEST); capabilities.setCapability(\"useNewWDA\", true); capabilities.setCapability(\"autoAcceptAlerts\", true);    } // below will check for Android else { capabilities.setCapability(MobileCapabilityType.AUTOMATION_NAME, \"UiAutomator2\"); capabilities.setCapability(MobileCapabilityType.DEVICE_NAME, \"emulator-5554\"); capabilities.setCapability(MobileCapabilityType.PLATFORM_NAME, Platform.ANDROID); capabilities.setCapability(MobileCapabilityType.APP, app.getAbsolutePath()); capabilities.setCapability(AndroidMobileCapabilityType.APP_PACKAGE, \"ai.haptik.android.sample.app\"); capabilities.setCapability(AndroidMobileCapabilityType.APP_ACTIVITY, \"ai.haptik.android.sample.app.ClientHomeActivity\"); } return driver; } } However, the challenge here would be whenever we run an automation test for the app, how will Appium know whether to run the Android test or iOS test? To overcome this, we use the @Parameter annotation in TestNg, where we pass the platform name (iOS / Android) as parameters from our test suites. See the Android example given below: android_testng.xml <?xml version = \"1.0\" encoding = \"UTF-8\"?> <!DOCTYPE suite SYSTEM \"http://testng.org/testng-1.0.dtd\" > <suite name = \"signup_suite\">     // Define value for which platform this testng file belongs <parameter name = \"platformName\" value=\"android\"/> <test name = \"guest_signup\"> // Your test classes or packages <package> <class name = \"com.test.android\" /> </package> </test> </suite> We use the same Page object class for maintaining the locators of iOS and Android keeping a common method, only in case of common UI. Suppose if we want to automate a simple sign-up test case that is common in both platforms, we will first create a page object for that particular screen. You can refer to the following sample class: SignupPage.java public class SignupPage { protected static AppiumDriver<MobileElement> driver; //Below are page object locators for Android and iOS respectively @AndroidFindBy(accessibility = \"username\") @iOSXCUITFindBy(accessibility = \"username\") MobileElement userName; public SignupPage(String platformName) { driver = new TestDrive().setupAppium(platformName); // This will init all the locators present in page object file PageFactory.initElements(new AppiumFieldDecorator(driver), this); } } Please Note: In case there are UI differences, then we create a separate Page Object Class for Android and iOS for the same feature. Now, given below is a simple test case automating a Sign-up flow where the QA just launches the app and performs the first step of tapping on the username field. SignupTest.java public class SignupTest { Private SignupPage signupPage; @BeforeClass @Parameters(\"platformName\") public void setup(String platformName) { // Initialising signup page object. signupPage = new SignupPage(platformName); } @Test public void testSignup() { signupPage.userName.click(); } } Another Good Practice – Something we learnt Although it does not matter if the naming conventions of the locators of elements are different for Android and iOS , it is always a good practice to have the same names for locators of similar elements in both the platforms. This will be of great help for a QA to write and edit the methods written in the page object class in case of any changes in functionality and thus helps a lot in maintaining the test cases with ease in such scenarios. Thus it is always advisable to include the respective platform developers in the initial phase itself during the sprint plannings where the team decides which tests need to be automated. Our Go-To-Tool – Appium Appium is the cross-platform solution for mobile Apps test automation. It is widely used by testers for automating their test cases because it can test all sorts of applications. Appium allows native, hybrid and web application testing and supports automation test on physical devices as well as an emulator or simulator both. It offers cross-platform mobile Apps testing, i.e. a single API works for both Android and iOS platform test scripts using the WebDriver protocol. It is the most widely used tool for regression testing Mobile Apps on mobile phones and tablets. Appium at its heart is a web server that exposes a REST API. It receives connections from a client(test cases), listens for commands, executes those commands on a mobile device, and responds with an HTTP response representing the result of the command execution. It supports many languages like Java, Objective-C, JavaScript , PHP, Ruby, Python, C# , etc for writing test cases. It is independent of the mobile operating system because it is essentially a wrapper that translates Selenium Webdriver commands into UIAutomation (iOS) or UIAutomator (Android) commands depending on the desired capabilities assigned through tests. Appium in Action Getting Started with Appium You need to download and install the following prerequisite to use Appium: 1. Java ( https://www.oracle.com/java/technologies/javase-downloads.html ) 2. Android SDK ( https://developer.android.com/studio ) 3. Appium Desktop ( http://appium.io/downloads.html ) 4. TestNg Framework ( https://testng.org/doc/ ) 5. Eclipse ( https://www.eclipse.org/photon/ ) Conclusion Personally, for me, at Haptik , all of the above has been a game-changer. I have been doing cross-platform testing for the last few years and now, it is so much easier and simpler. The plan is now to scale this framework, where even developers can themselves test a lot of the functionalities before merging their feature branches or before final releases. Do check it out and let me know how you do cross-platform testing using automation. Posted by Dikshit Shetty on Apr 28, 2020 7:45:00 PM Find me on: LinkedIn", "date": "2020-04-28,"},
{"website": "Haptik-AI", "title": "Automating Bot Testing at Haptik", "author": [" Lajari Patil"], "link": "https://www.haptik.ai/tech/automating-bot-testing/", "abstract": "Tweet At Haptik we design, develop and test chatbots every day. These bots are spread over multiple domains a few examples being feedback bots, supports bots, lead gen bots, etc. Some of these bots are text-based, some voice-based and some support a mix of both. Our objective is not just to make bots live in production, but also to make them robust and fail-proof. The onus is on the QA (Quality Analyst) team to deliver live bots with minimal, or zero, errors in production. Hence, for QA’s, it is a huge responsibility to ship these chatbots with minimal errors and maintain quality. Now, you guys must be wondering, what exactly does chatbot testing mean. To give you some background first, a chatbot is a machine built with a purpose to help its users over text or voice. Since the medium of communication is very open-ended, the users can say anything they feel like. At this point, the QA needs to figure out if the bot should have been able to reply to the users’ query and if so was it the right reply. A few things that go into this are: Testing of end-end user flows for the user queries without failing. Validating the business logic the bot executed General Responses (Greeting, positive feedbacks, negative feedbacks etc.) Testing functionality of Individual Tasks (tasks are nothing but use-cases supported by chatbot) Checking of UI of chat elements, images, Grammar, Personality, etc. Bot Flow Bots can be viewed or understood as a simple flow chart. The user sends a message, the message is processed on the basis of various conditions, and the best possible response is then sent back to the user. To understand what a simple bot flow looks like, let’s consider the above example where the u ser says- ” Set wake up reminder ”.  The following happens on receiving the message: 1. The sentence goes through our Machine Learning pipeline (NLU engine, consisting of our backend ML microservices which are responsible for detecting the intent, entities, correcting spelling, grammar and understanding what the user wants) 2. Once we know that, the Pipeline traverses all the way to “ wake up reminder ” through “ set reminder ”. 3. On selecting the respective and most relevant response, bot replies: ”Help me with date and time” (Response is taken care of by our ML Response Engine). Challenges While Testing a bot? 1. Wide Scope of testing i. Unlike other types of testing, chatbot testing depends on multiple factors-domain for which the chatbot is built, target users, target age group, type of conversation that a chatbot is supposed to have with the user (Casual or professional). Keeping all these factors in mind, there is a large number of permutations and combinations to think off. Building test cases for each of these scenarios is again a tricky and tedious task. ii. While testing the chatbots, the tester also has to focus on replies given by the Chatbot for various general queries. (User typing anything in a free form) iii. The response that was given by the bot when it does not understand the user queries. 2. Time consumption and increased manpower Let’s understand this with the help of the previous example. Here we have. A bot builder made the following changes – separated the date and time intents as shown below: While testing a bot for change made in flow, the main objective to keep in mind is to not break the existing flows. Slightest of changes to a bot, as in the above case, the tester has to test all three reminder types (exercise reminder, wake up reminder and drink water reminder). This leads to the testing of the whole flow again. This consumes a lot of time and manpower. 3. API failures Chatbot testing also involves testing of third-party integration API. This also includes responses from chatbot for API integration i.e -success, failures and timeout scenarios. 4. Report generation Chatbots performance tracking is one of the major challenges nowadays. There is no tool in the market which can track changes made to the bot and give a summary and detail report about test cases before and after. Not just that but also have this data available at any time. Introduction to Bot Testing Tool To overcome these day to day challenges and deploy a high-quality bot in the shortest possible time,  we knew that only automation can come to our rescue. And that was the birth of our Bot testing tool . This tool can be run on demand, and it also runs automatically through a cron script in our UAT environment every day,  to make sure the bots are running as per requirement. So, let’s take a look at how exactly the Bot testing tool works and how it has made our life easier. There are 4 important steps that are followed in order to get the bot testing tool up and running. The following is an overview of what exactly the tool does: 1. Perform the chatbot flow manually once on your app or website bot. This includes sending messages to the bot, checking the bot response and whether it is responding to all the chats and free form queries. Remember that every bot is built to automate specific business critical processes. 2. The user input sheet gets created with all the user queries/messages that one sent while manually testing the bot. Input sheet is just a simple CSV with various columns related to the chatbot flow. 3. The above input sheet is then passed as an input to the next step to get the messages from the system against the user queries, which in turn generates the System Messages sheet. 4. Once the above System sheet is created, it is simply run every day to check if the correct responses are sent to the user and if the response generated is the most relevant one. If the above is true, then the test cases are marked as passed else the bot QA tool raises an exception and sends an email with the failure scenarios. “The beauty of this tool is that it is independent of the copy of the messages.” Some add-ons: 1. On-Demand Execution: This tool can be executed through Jenkins , on the click of a single button. We just have to take the system message sheet URL for a particular bot from the S3 folder, add it to the field called file URL and provide user_id (Unique to a user) of the account on which we want to run the bot testing tool. Thereafter, we just have to click the build button and the test cases start executing. The messages that are sent by the system can be seen on the devices of the user, whose user_id is mentioned. Through that, the user can come to know if the chatbot is working as intended or not. The Bot QA tool actually simulates a real user chatting on the platform. 2. Automatic Execution : This tool runs automatically every day at with help of a cron script. For each chatbot, the system message sheet gets executed and if there are any failures, a mail gets triggered to the stakeholders of that particular chatbot. The mail received is as shown below: All these reports are available on S3 and can be accessed whenever needed to check the previous state. This helps in debugging issues very easily. “Bot testing tool runs every night starting from 1 am and gives the result for 25 bots in approximately 2 hours” Without this tool, testing and deploying 25 Bot’s would take more than 3-4 days. Limitations Bot builders / Developers changing flow breaks the tool, so communication is key whenever things are changing. Future Plans 1. To leverage this bot testing tool for Voice bot testing. 2. Build some functionality around this tool to support Multilingual chatbot testing. 3. Optimizing time to run bot testing tool from several hours to minutes. Advantages 1. Time taken to test any chatbot has reduced to 7-8 mins from 1-2 hrs. 2. Bot testing tool is capable of testing multiple chatbots all at once 3. One time effort to test chatbot manually. 4. A single bot can be running for multiple clients. This also allows testing the bot across clients. We would love to hear from you. Do let us know your feedback for our Bot testing tool. Our team is working vigorously to improve and build more features and we will be back with the updates soon. Haptik is hiring. Do check out our careers page. Posted by Lajari Patil on Mar 15, 2019 6:52:00 PM Find me on: LinkedIn", "date": "2019-03-15,"},
{"website": "Haptik-AI", "title": "Extract Spelling Mistakes Using Fasttext", "author": [" Chirag Jain"], "link": "https://www.haptik.ai/tech/extract-spelling-mistakes-fasttext/", "abstract": "Tweet At Haptik , we deal with a lot of noisy conversational data. This conversational data is generated from our 100+ chatbots that are live across various platforms like mobile apps , web-SDKs etc. Even though 12.5% of India can speak English, it ranks at 27th on the English Proficiency Index . This means any kind of textual data obtained from Internet users like ours should not only expect multilingual data but also a lot of noise in the form of grammatical errors, internet slang, spelling mistakes, etc. Noise can create significant problems both during training and testing bots. During training time, a lot of noise can prevent machine learning algorithms from learning quickly, while at test time unseen noise can lead to wrong predictions. Rare words like spelling mistakes often end up with not very useful vectors during the training process. This is because they occur so rarely that their randomly assigned vector hardly moves. Usually, rare words are marked as unknown words and discarded from the training process. If we can correct spellings accurately, we can keep more of our training data as well as reduce the total number of unique words which impact the training time and memory. In this post, we will try tackling spelling errors . Given that we have a moderately sized corpus of text data, we will extract a dictionary of spelling mistakes and their possible corrections from the data in an unsupervised way. Although we can simply borrow a list of correctly spelled words from Internet dictionaries and use something like edit distance coupled to precompute all spelling errors, we wanted to experiment with a completely data-driven approach. Also, as mentioned earlier, our data also contains a fair amount of ‘Hinglish’ (romanized Hindi) messages. In India, romanizing or transliterating words from native language and changing the language back and forth in the same sentence is quite common, especially when it comes to messaging and social media. This is referred to as code mixing or code-switching . This is mostly due to native languages being cumbersome to type on regular layout keyboards. Although such romanized words weakly follow some (unspoken) rules, there are no consistent guidelines to do so and can vary wildly. This means such sentences can have words which are not technically spelling mistakes but would be considered as such because they don’t belong in any standard dictionary. We would also like to extract such words and their variants throughout the corpus. For any natural language corpus, the vocabulary roughly follows a Zipfian distribution . It states that the frequency of any word is inversely proportional to its rank in the frequency table. Put simply, a small subset of all words occur very frequently and the rest very rarely. Working on a small subset of our chatbots data with 1.8M lines and 11M total words, there are 182K unique words and about 80% of the unique words occur 5 times or less. 1. A quick look at the vocabulary tells us that such words are either named entities (mostly proper nouns) or spelling mistakes. 2. Another interesting observation is that some spelling mistakes are more common than others and end up occurring even 100+ times across the corpus. An example is a word ‘mony’ (spelling error for ‘money’) that occurs 300+ times. Commonly used internet slangs and SMS shortened versions of words (‘know’ -> ‘knw’, ‘you’ -> ‘u’) also belong to this category. 3. There are also spelling mistakes like ‘remainder’, which even though is a legit English word, in our corpus it is often used in place of ‘reminder’. Such contextual mistakes are a bit harder to correct with an unsupervised method and out of the scope of this post. To tackle our problem we will use fastText . FastText is a way to obtain dense vector space representations for words. It modifies the Skip-gram algorithm from word2vec by including character level sub-word information. So first for any word, say “hello” it would break it down into character n-grams. FastText asks for a min_n and max_n for character n-grams. So for example, min_n = 3, max_n = 4, “hello” would be broken down into [“##h”, “#he”, “hel”, “ell”, “llo”, “lo#”, “o##”, “###h”, “##he”, “#hel”, “hell”, “ello”, “llo#”, “lo##”, “o###”] where ‘#’ represents the padding character. (Note it doesn’t necessarily have to be ‘#’. FastText takes care of padding on its own) Now, each of these character n-grams is assigned a vector instead of the main word itself. The vector for the main word itself is defined as the sum vector of all of its char n-grams. These vectors improve over the course of training via the skip-gram algorithm. Now, since we are considering char n-grams as input tokens, we can end up with a larger input space than our original vocabulary size. (In the example of min_n = 3, max_n = 4, above 27 ^ 3 + 27 ^ 4 =  551,124). To work around this problem, fastText uses hashing trick to hash these character n-grams to a fixed number of buckets. This way items hashed to the same bucket are assigned the same vector. This keeps the memory bounded without affecting the performance severely. Coming back to the task, since fastText uses sub-word level information, any two words which have a similar set of character n-grams, can be expected to have their vectors nearby in the vector space. Since most spelling mistakes are just one or two characters wrong (edit distance <= 2) such words will have vectors close enough. Getting started Okay, so let’s first read the corpus and make the word to frequency dictionary. Since I took care of preprocessing the corpus beforehand, I am simply tokenizing on whitespace: import io import collections import matplotlib . pyplot as plt import nltk import enchant ​ words = [ ] with io . open ( 'corpus.txt' , 'r' , encoding = 'utf-8' ) as f : for line in f : line = line . strip ( ) words . extend ( line . split ( ) ) ​ vocab = collections . Counter ( words ) vocab . most_common ( 10 ) We get: [ ( 'i' , 174639 ) , ( 'to' , 127111 ) , ( 'my' , 84886 ) , ( 'is' , 69504 ) , ( 'me' , 67741 ) , ( 'the' , 63488 ) , ( 'not' , 51194 ) , ( 'you' , 50830 ) , ( 'for' , 47846 ) , ( '?' , 45599 ) ] Inspecting the other end of the Counter list ( reversed ( vocab . most_common ( ) [ - 10 : ] ) ) Unsurprisingly, we find words from other languages. In fact, about 3000 words at the bottom are in the non-Latin script. We will ignore these words later in the script: [ ( '酒店在haridwar' , 1 ) , ( '谢谢' , 1 ) , ( '谈' , 1 ) , ( '看不懂' , 1 ) , ( '的人##' , 1 ) , ( '现在呢' , 1 ) , ( '王建' , 1 ) , ( '火大金一女' , 1 ) , ( '李雙鈺' , 1 ) , ( '拜拜' , 1 ) ] Okay, now let’s train a fastText model on the corpus: $ fasttext skipgram - input corpus . txt - output model - minCount 1 - minn 3 - maxn 6 - lr 0.01 - dim 100 - ws 3 - epoch 10 - neg 20 I am keeping minCount 1 to try and learn a vector for all words, ws controls the window size hyperparameter in the skip-gram algorithm, 3 means for every word we will try to predict 3 words to its left and right in the given corpus. Changing ws won’t have much dramatic effects on our task. The main parameters are minn and maxn which control the size of character n-grams as explained above. We settled with 3 to 6, larger values would mean words would need to have longer substrings in common. Okay, now we will load this model with Gensim and check some nearest neighbours: from gensim . fasttext import FastText model = FastText . load_fasttext_format ( 'model' ) print ( model . wv . most_similar ( 'recharge' , topn = 5 ) ) print ( model . wv . most_similar ( 'reminder' , topn = 5 ) ) print ( model . wv . most_similar ( 'thanks' , topn = 5 ) ) This gives, [ ( 'rechargecharge' , 0.9973811507225037 ) , ( 'rechargea' , 0.9964320063591003 ) , ( 'rechargedd' , 0.9945225715637207 ) , ( 'erecharge' , 0.9935820698738098 ) , ( 'rechargw' , 0.9932199716567993 ) ] [ ( \"reminder'⏰\" , 0.992865264415741 ) , ( 'sk-reminder' , 0.9927705526351929 ) , ( 'myreminder' , 0.992688775062561 ) , ( 'reminderw' , 0.9921447038650513 ) , ( 'ofreminder' , 0.992128312587738 ) ] [ ( 'thanksd' , 0.996020495891571 ) , ( 'thanksll' , 0.9954444169998169 ) , ( 'thankseuy' , 0.9953703880310059 ) , ( 'thankss' , 0.9946843385696411 ) , ( 'thanksb' , 0.9942326545715332 ) ] Okay, we seem to be getting spelling mistakes of our desired words very close to them (similarity scores are > 0.99 ). Now, we will walk through our vocabulary, query the fastText model for each word’s nearest neighbours and check for some conditions on each neighbour. If the neighbor passes these conditions, we will include the neighbour as a spelling mistake for the word. These conditions ensure that whatever we get at the end has less false positives: word_to_mistakes = collections . defaultdict ( list ) nonalphabetic = re . compile ( r '[^a-zA-Z]' ) for word , freq in vocab . items ( ) : if freq < 500 or len ( word ) <= 3 or nonalphabetic . search ( word ) is not None : #  To keep this task simple, we will not try finding #  spelling mistakes for words that occur less than 500 times #  or have length less than equal to 3 characters #  or have anything other than English alphabets continue # Query the fasttext model for 50 closest neighbors to the word similar_words = model . wv . most_similar ( word , topn = 50 ) for similar_word in results : if include_spell_mistake ( word , similar_word , similarity_score ) : word_to_mistakes [ word ] . append ( similar_word ) Here are the rules we use to include something like a spelling mistake for a word: enchant_us = enchant . Dict ( 'en_US' ) spell_mistake_min_frequency = 5 fasttext_min_similarity = 0.96 def include_spell_mistake ( word , similar_word , score ) : \"\" \" Check if similar word passes some rules to be considered a spelling mistake Rules: 1. Similarity score should be greater than a threshold 2. Length of the word with spelling error should be greater than 3. 3. spelling mistake must occur at least some N times in the corpus 4. Must not be a correct English word. 5. First character of both correct spelling and wrong spelling should be same. 6. Has edit distance less than 2 \" \"\" edit_distance_threshold = 1 if len ( word ) <= 4 else 2 return ( score > fasttext_min _ similarity and len ( similar_word ) > 3 and vocab [ similar_word ] >= spell_mistake_min _ frequency and not enchant_us . check ( similar_word ) and word [ 0 ] == similar_word [ 0 ] and nltk . edit_distance ( word , similar_word ) <= edit_distance_threshold ) Some rules are straightforward: Spelling mistake word vector must have high vector similarity with correct word’s vector, Spelling mistake word must occur at least 5 times in our corpus, It must have more than three characters It should not be a legit English word (we use Enchant which has a convenient dictionary check function). Other rules are based on observations like: The first character in a word with spelling mistake is usually correct so we can add a constraint that both correct and wrong spellings should have the same first character. Since most spelling errors lie within 2 edits of the correct word, we will ignore words that are more than 2 edits away. We can customize these rules according to the desired results. We can increase the edit distance threshold or ignore the first character same rule to be more lenient. At this point, most of our work is done, let’s check word_to_mistakes : print ( list ( word_to_mistakes . items ( ) ) [ : 10 ] ) [ ( 'want' , [ 'wann' , 'wanto' , 'wanr' , 'wany' ] ) , ( 'have' , [ 'havea' , 'havr' ] ) , ( 'this' , [ 'thiss' , 'thise' ] ) , ( 'please' , [ 'pleasee' , 'pleasr' , 'pleasw' , 'pleaseee' , 'pleae' , 'pleaae' ] ) , ( 'number' , [ 'numbe' , 'numbet' , 'numbee' , 'numbr' ] ) , ( 'call' , [ 'calll' ] ) , ( 'will' , [ 'willl' , 'wiill' ] ) , ( 'account' , [ 'aaccount' , 'acccount' , 'accouny' , 'accoun' , 'acount' , 'accout' , 'acoount' ] ) , ( 'match' , [ 'matche' , 'matchs' , 'matchh' , 'matcj' , 'matcg' , 'matc' , 'matcha' ] ) , ( 'recharge' , [ 'rechargr' , 'recharg' , 'rechage' , 'recharege' , 'recharje' , 'recharhe' , 'rechare' ] ) ] Nice! Now as a final step let’s create an inverted index for fast lookup: inverted_index = { } for word , mistakes in word_to_mistakes . items ( ) : for mistake in mistakes : if mistake != word : inverted_index [ mistake ] = word Now, this inverted_index dict can be used for quick lookup and correction. One extra thing I did, that might not necessarily be needed, was merging transitive links in this index. What I noticed was some word ‘A’ corrects to ‘B’ and in another entry ‘B’ corrects to ‘C’. In such cases, I chose to correct all ‘A’, ‘B’ and ‘C’ to whichever occurs the most in the corpus. I used the connected components algorithm to mark clusters and merge them. That’s it. However, this method is not entirely accurate. 1. Very common proper nouns can still slip through the rules and end up being corrected when they shouldn’t be. 2. A manual inspection must still be done once to remove errors. 3. Another drawback is spelling mistakes that never occurred in the corpus will not have a correction in the index. Nevertheless, this was a fun experiment. If you already have a list of correct spellings with you (say top 10k most frequent words from a corpus like above), you can use something like Symspell which will generate all possible mistakes within 2 deletion operations for each word and then apply same deletion operations on spelling mistakes and use specialized index for fast lookup and correction. Do let us know in comments about how you’re handling such use cases. Posted by Chirag Jain on Nov 3, 2018 6:44:00 PM Find me on: LinkedIn Twitter", "date": "2018-11-3,"},
{"website": "Haptik-AI", "title": "Setup Auto-Scaling For Aws Elasticsearch", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/auto-scaling-for-aws-elasticsearch/", "abstract": "Tweet Elasticsearch has been around for a while now and is being used in almost every other application as a search engine. We have also seen that sometimes it has also been used as a replacement to our traditional Databases, mainly depending on the use-cases are. Elasticsearch makes searching of key-value pairs from a large amount of data really easy and performs really well. Here’s what Wikipedia has to say about Elasticsearch: “ Elasticsearch is a search engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It also forms a part of a popular log management stack called ELK . The latest available version is 5.1.2 . AWS Elasticsearch is Elasticsearch + Kibana provided as a service. AWS manages the nodes and you get an endpoint through which you can access the Elasticsearch cluster. At Haptik, we use Elasticsearch for production loads and one of the challenges we faced was that AWS does not provide auto-scaling of each node. Auto-scaling is done based on a metric that is monitored and once that metric reaches a specific threshold value, a new server is spun up to balance the hike in the threshold. AWS provides us with the option of custom metrics where we can make our own metrics and set alarms based on the different values. We came up with scripts that can help us to auto-scale Elasticsearch with ease. It was as easy as putting Bash shell and AWS CLI to their proper and efficient use. What have we done for AWS Elasticsearch Auto-scaling at Haptik? Using get-metric-statistics (AWS CLI) we have pulled data from an AWS CloudWatch metric on AWS and calculated an average for the last 5 minutes. If that average threshold is higher than what we specify, we have used AWS ES CLI to modify our Elasticsearch Cluster domain settings and spin up new nodes to even out the load. A scale down script runs every 30 minutes which checks the threshold value. If the value is normal, it brings down the node count to the initial number. This initial number is passed as a parameter to the scripts. Note : Scale up brings up another set of nodes with same data and then once those are up and running AWS, brings then under the same endpoint and later on removes the older nodes; something similar to Blue-Green deployment. Let’s discuss the scripts one by one. Later you can also go to our GitHub repo , download the scripts, modify according to your use case and put them to use. Script 1 Explanation : Our web servers query ES (Elasticsearch) for some data and whenever the traffic starts increasing our ES servers slow down due to the load. When the ES servers start slowing down our web servers latency starts going high as each request starts taking more time. At this point, we want to bring up more nodes so that we can keep up with the load. Hence we scale our ES cluster based on the latency of the ELB under which we have our web servers. The below script is for scaling up the ES nodes based on a metric that we have taken for our use case, i.e. using an ELB metric; Average Latency which is a pre-defined metrics on CloudWatch . When this script runs, it gets the metric points for the last 5 minutes, takes the average and then decides whether or not the ES nodes need to be scaled up. #!/bin/bash ##################Change ELB name##################### NAME=\"elb-name\" AZ=\"ap-south-1a\" date_end=`date -u +\"%Y-%m-%dT%H:%M:%SZ\"` #date --date \"$date_end -10 min\" date_start=`date -u +\"%Y-%m-%dT%H:%M:%SZ\" --date \"$date_end -5 min\"` original_cluster_nodes_count=$1 ##################ELB STEPS########################################################### var=`/usr/local/bin/aws cloudwatch get-metric-statistics --namespace AWS/ELB --metric-name Latency --start-time $date_start --end-time $date_end --period 60 --statistics Average --dimensions '[{\"Name\":\"LoadBalancerName\",\"Value\":\"'$NAME'\"},{\"Name\":\"AvailabilityZone\",\"Value\":\"'$AZ'\"}]' --unit Seconds | awk '{print $2}' ` { echo \"${var[*]}\"; } > cat.txt total=`awk '{ SUM += $1} END { print SUM }' cat.txt` var2=$(echo \"scale=2; ($total/5.0) *100 \"|bc) delay=${var2%.*} #################500 below means 5 seconds################ function scale_up () { if [ $delay -ge 300 ]; then #################count increases by 2##################### count=2 #####################Replace name by your AWS ES domain name, not URL################## /usr/local/bin/aws es describe-elasticsearch-domain-config --domain-name name > test123.txt instance_current_count=`cat test123.txt | awk '{print $3}'| awk 'NR==7'` instance_new_auto_count=`expr $count + $instance_current_count` /usr/local/bin/aws es update-elasticsearch-domain-config --elasticsearch-cluster-config \"InstanceType=m4.large.elasticsearch,InstanceCount=$instance_new_auto_count\" --domain-name name fi } scale_up Script 2 This script is for downscaling the AWS Elasticsearch nodes. Explanation : To downscale nodes, we are using a similar script which checks the metric value of the ELB metric over the past 5 minutes, takes the average and then decides whether or not to downscale. We scale down our nodes if the latency is lower than the threshold i.e. delay <= 3 seconds (300 mili-seconds). If this condition is true, it will check for the current count of nodes and the actual regular count of nodes which is passed to it as a parameter. #!/bin/bash #####################CHANGE ELB NAME######################### NAME=\"ELB-name\" AZ=\"ap-south-1a\" date_end=`date -u +\"%Y-%m-%dT%H:%M:%SZ\"` #date --date \"$date_end -10 min\" date_start=`date -u +\"%Y-%m-%dT%H:%M:%SZ\" --date \"$date_end -5 min\"` original_cluster_nodes_count=$1 #######################################################ELB STEPS########################################################### var=`/usr/local/bin/aws cloudwatch get-metric-statistics --namespace AWS/ELB --metric-name Latency --start-time $date_start --end-time $date_end --period 60 --statistics Average --dimensions '[{\"Name\":\"LoadBalancerName\",\"Value\":\"'$NAME'\"},{\"Name\":\"AvailabilityZone\",\"Value\":\"'$AZ'\"}]' --unit Seconds | awk '{print $2}' ` { echo \"${var[*]}\"; } > cat.txt total=`awk '{ SUM += $1} END { print SUM }' cat.txt` var2=$(echo \"scale=2; ($total/5.0) *100 \"|bc) delay=${var2%.*} ###########Value of delay == 500 means 5 seconds################ function scale_down () { if [ $delay -le 300 ]; then ############check current size of cluster############### /usr/local/bin/aws es describe-elasticsearch-domain-config --domain-name name > test123.txt instance_current_count=`cat test123.txt | awk '{print $3}'| awk 'NR==7'` if [ $instance_current_count -eq $original_cluster_nodes_count ]; then echo \"size okay\" elif [ $instance_current_count -gt $original_cluster_nodes_count ]; then /usr/local/bin/aws es update-elasticsearch-domain-config --elasticsearch-cluster-config \"InstanceType=m4.large.elasticsearch,InstanceCount=$original_cluster_nodes_count\" --domain-name name echo \"Things look okay now. Cluster Scaled down to $original_cluster_nodes_count\" fi fi } scale_down Things you can change in the script: Name of ELB, if you plan to use an ELB metric Name of metric can even be a custom metric Name of ES domain Time for checking the metric date points Now, that we are done understanding the two scripts, let’s see how to use them. Following are the ways we are using them: 1. We have put Script 1 (say es_scaleup.sh) in cron or our Jenkins server. You can put it in cron on any server that has access to AWS CLI and right permissions to access your AWS Elasticsearch domain. We are running this script every 7 minutes because it takes around 5-6 minutes for the new nodes to come up: */7 * * * * bash -x /var/lib/jenkins/scripts/elasticsearch/es_scaleup.sh 6 -x : Using this to get the output of the script in debug mode. You may or may not use it. Parameter : “6” is passed as the minimum instance count. It will add the new number of nodes above this count. 2. Similarly, as above we have put our Script 2 (say es_scaledown.sh) in cron too. We are running this every 30 minutes so as to lower down the nodes count only after things are totally stable. */30 * * * * bash -x /var/lib/jenkins/scripts/elasticsearch/es_scaledown.sh 6 Parameter : “6” is passed to let this script know the minimum instance count so that it does not scale down lower than that number. To modify ES via CLI we have to Modify Access Policy of the AWS Elasticsearch domain. It’s as simple as going to ES console and click on Modifying Access Policy: The above opens some options as below: You can edit the access policy on the above page. Read more about modifying access policy here . You’re all set. Just follow this if you want your AWS Elasticsearch to be auto-healing and auto-scaling . Let me know if you face any issues while making the scripts or implementing this. Looking forward to hearing from you all. Would love to hear how you guys are handling such use-cases. Also, don’t forget, we are hiring high-quality engineers. So if you are interested reach out to us at hello@haptik.ai . Posted by Ranvijay Jamwal on Feb 7, 2017 6:41:00 PM Find me on: LinkedIn", "date": "2017-02-7,"},
{"website": "Haptik-AI", "title": "Putting Text on Images Using Python - Part 2", "author": [" Vinay Jain"], "link": "https://www.haptik.ai/tech/putting-text-on-images-using-python-part2/", "abstract": "Tweet In the first post of this series, I introduced you to the basics of text drawing in Python by adding a greeting text on an image. I also highlighted examples of how I further extended this functionality to create some complex images at work. If you haven’t already read the first part of this series (Putting Text on Images Using Python Part -1), I recommend you take a glance at it first, to get a better understanding of this post. For now, we know how to draw text, change the font, and position the text on the image. In this post, we’ll discover how to draw multiline text and also discuss the challenges of doing so. Multiline Text Often, while generating images, we come across situations where the text doesn’t fit in a single line. Python Pillow is not helpful here as it doesn’t automatically draw & push the text to a new line. In order to do this manually, we need to calculate the width and height of the text. With the text-width, we determine when we need to move to the next line and with the text-height, we can figure how much space should be left in between the two lines: The idea is to split the long sentences into multiple shorter sentences and draw each of these, one by one at the correct positions thereby making it look like a multiline text. To split a longer line, we‘ll use a Pillow function to calculate the size of the text passed to it as one of the parameters. Calculate Text Width For convenience, I’ve created a method text_wrap() to explain the line-split logic: from PIL import Image from PIL import ImageFont from PIL import ImageDraw def text_wrap ( text , font , max_width ) : lines = [ ] # If the width of the text is smaller than image width # we don't need to split it, just add it to the lines array # and return if font . getsize ( text ) [ 0 ] <= max_width : lines . append ( text ) else : # split the line by spaces to get words words = text . split ( ' ' ) i = 0 # append every word to a line while its width is shorter than image width while i < len ( words ) : line = '' while i < len ( words ) and font . getsize ( line + words [ i ] ) [ 0 ] <= max_width : line = line + words [ i ] + \" \" i += 1 if not line : line = words [ i ] i += 1 # when the line gets longer than the max width do not append the word, # add the line to the lines array lines . append ( line ) return lines def draw_text ( text ) : # open the background file img = Image . open ( 'background.png' ) # size() returns a tuple of (width, height) image_size = img . size # create the ImageFont instance font_file_path = 'fonts/Avenir-Medium.ttf' font = ImageFont . truetype ( font_file_path , size = 50 , encoding = \"unic\" ) # get shorter lines lines = text_wrap ( text , font , image_size [ 0 ] ) print lines # ['This could be a single line text ', 'but its too long to fit in one. '] if __name__ == __main__ : draw_text ( \"This could be a single line text but its too long to fit in one.\" ) This function expects three parameters – the text to draw, an ImageFont class instance and the width of the background image on which the text is to be drawn. The logic is pretty straightforward: 1. Check, if the sentence can fit in one line then just return it without splitting, else: 2. Split the sentence using spaces to fetch the words in it 3. Create shorter lines by appending words while the width is smaller than the image width When we run this script it returns an array containing 2 shorter lines which fit within the width of the background image. To draw these lines on the image we have to calculate the correct vertical position of each line. Calculate Text Height Whenever we write text, there is an equal amount of space between two lines. For example in this post, the lines have the constant spaces between them. While building this library I faced an issue of varying spaces with most of the input text: text = \"This could be a single line text but it can't fit in one line.\" lines = text_wrap ( lines , font ) for line in lines : print font . getsize ( line ) [ 1 ] # Output # 62 # 51 Finding correct height for characters like g, j, p, q, y which are drawn below the Baseline and b, d, f, h, k, l which are drawn above the Median is a little tedious due to varying heights. The best way to get the correct height of the text is to simply calculate the total height of “hg”. This trick works because h and g cover the height range of all the English characters. For languages other than English, you might have to use different characters in place of h & g. text = \"This could be a single line text but it can't fit in one line.\" lines = text_wrap ( lines , font ) line_height = font . get_size ( 'hg' ) [ 1 ] print line_height # Output # 62 Since we have our wrapped/short lines and also the text height we can draw these on the image. We can do this by keeping a reference to the vertical position of the previously drawn line and then adding to it the line height to calculate the vertical position of the new line: text = \"This could be a single line text but its too long to fit in one.\" lines = text_wrap ( text , font , image_size [ 0 ] ) line_height = font . getsize ( 'hg' ) [ 1 ] x = 10 y = 20 for line in lines : # draw the line on the image draw . text ( ( x , y ) , line , fill = color , font = font ) # update the y position so that we can use it for next line y = y + line_height # save the image img . save ( 'word2.png' , optimize = True ) This will output an image like this: The text in the latter images looks much better and readable. At Haptik , we believe in experimentation and finding out the best possible way to solve problems. The above is one such example. In my next Blog post, I will be writing about how to center align text horizontally and vertically in an image using Python. Think we did a good job? Let us know in the comments below. Also, Haptik is hiring . Visit our careers section or get in touch with us at hello@haptik.ai . Posted by Vinay Jain on Apr 4, 2018 6:51:00 PM Find me on: Facebook LinkedIn", "date": "2018-04-4,"},
{"website": "Haptik-AI", "title": "End-To-End Frontend Testing At Haptik", "author": [" Gopal"], "link": "https://www.haptik.ai/tech/end-to-end-frontend-testing-haptik", "abstract": "Tweet Another year is about to end and we will move on to the next year with a lot of learnings. One of those learnings is around our endeavor on End-to-end Frontend Testing . I will try to keep this blog as close as to what we learned during the entire journey and provide useful hyperlinks because Cypress’s own document is pretty darn good. Quarter 1 – Piloting with Selenium At Jio Haptik, we currently have three major internal tools to monitor and build bots. They are namely, Agent Chat, Analytics, and Bot builder. With our increasing frequency of pull requests every day and the ever-growing complexity of these web applications, we chose Selenium to automate integration tests and get rid of man-hours for manual testing. One of the strong reasoning behind choosing Selenium was to have the code consistency across tools, platforms and extend its capability to test mobile applications as well. Three weeks down the line, we quickly realized that keeping pace with the development cycle is extremely difficult because of these inherent reasons The learning curve for manual testers is steep Setting up a test environment with selenium is not straight forward and often requires OS-specific quirks Focus diversion: We observed that testers are spending more time figuring out the capabilities/syntax of the language instead of focusing on writing the range of test cases. Writing tests in Selenium is more like building with lego blocks; for every new feature, you’ll have to rely on a third-party add-on . For example, we used Cucumber for screenshot capturing, AutoIt for system popups. This creates a huge dependency and is prone to stale libraries. This lead us to ask the obvious question… Why does everyone still use Selenium despite the inherent problems? After an extensive interview with a couple of our QAs, putting down some of the major pointers, as to why Selenium is a goto solution for End-to-end Testing: Community support: Being one of the oldest and matured solutions around, Selenium has grown a wonderful and large community. So, getting a solution to any of your queries is just a Google search away Language support : Selenium is language-agnostic; its core API is ported in many languages namely, Java, C#, Ruby, Python, etc. Multi-browser support : This is a valid reason as many companies will save on maintenance overhead. Reports : For Sass/Service based enterprise, client test reports are vital. Selenium ecosystem provides multiple such add-ons for report generation, test case aggregation on tags, which helps in monitoring and final delivery. Quarter 2 and 3 – Cypress! The new guy At the end of the 1st quarter, we needed a solution that would enable us to increase the development and testing cycle instead of being a hindrance. We decided to try out Cypress, and I would like to highlight a few pointers which were immediately beneficial. Setting up and writing our initial test case were done in a matter of minutes. Increased collaboration : All our platform repositories in-housed the testing code, which enabled front-end developers and QAs to simultaneously build features and write tests. This new workflow also enabled us to set up contracts between the frontend team and QAs, to define better DOM selectors, which previously was an afterthought. Use of Mocha for test case structuring, Chai for the assertion along with Cypress own API, enforced consistency of code and readability . Time travel – Debugger : The power of scrubbing through different states of the application gives a broader insight into why a test might fail. Furthermore, the experience of writing tests is enhanced due to the integration of Cypress test runner with already powerful chrome dev tools. Retries and browser event wait: The core API of cypress to find DOM elements has an implicit timeout of 4 seconds. This really takes a lot of overhead code to provide explicit wait to find DOM elements. This is just one of many such browser events, that Cypress keeps watching, to describe: a. The DOM load event. b. Bootstrapping of the framework being used. c. Wait for the XHR/Asynchronous request. d. Wait for Animation to be completed. Intercept network: Cypress allows you to intercept network calls and mock requests/responses. It really helps to write robust test cases instead of relying on dynamic responses from the server end. Native access : Because Cypress operates within the browser, it has native access to its APIs. Whether it is the window, the document, a DOM element, your application instance, a function, a timer, a service worker, or anything else – you have access to it in your Cypress tests. Out of box recording for every test-run helps to find application/test loopholes faster: Other than these, which we immediately got benefited from, please do read the other great features which might relate to your domain-specific problem. Cypress.io – Our learnings During our usage of Cypress for more than two quarters, we have faced a few different challenges and learned a lot. Let’s have a look at those: Using data attributes to find DOM elements makes test cases resilient for a longer period of time and immune to development changes. Using assertion alternatively in chained commands to enable retries. cy.get('ul[data-testid=todo-list] li', { timeout : 10000 }) /*command*/ .should('have.length', 1) /*assertion*/ .find('label') /*command*/ .should('contain', 'todo A') /*assertion*/ Wait for a route instead of explicit waiting. Cypress allows you to track API calls via the route method. To verify the success or failure of an asynchronous call, waiting for the network call provides accurate waiting time. Check for events before the click. Cypress often tries to click on a DOM element even before an event listener has been attached to it. Adding an explicit wait makes the test flaky. The ideal approach would be to check if there are any listeners attached to a DOM element. This makes the test robust. Here is a great blog about it. Reference and Roadmap So far we’ve been very happy with what Cypress has to offer out-of-the-box and pretty much satisfies our needs. With its fast growth, they keep reducing the existing problems. Do keep track of their current roadmap , which might help you to take a long-term decision. Furthermore, adding some important articles that are worth pinning Best practices of Cypress Cypress Architecture Trade-offs Click retries Conclusion As I stated at the start of this article, my experience of automation testing is not a good one where a lot of money, time and pain are spent keeping thousands of hard to maintain tests afloat for not great payout. Automation testing has only ever guaranteed a long CI build in my experience. We, as developers, need to be better at automation testing. We need to write fewer tests that do more and are useful. We’ve left some of the most difficult code to write to some of the least experienced developers. We’ve made manual testing seem outdated when, for my money, this is still where the real bugs are found. We need to be sensible about what automation testing can achieve. Cypress is great because it makes things synchronous, this eliminates a whole world of pain, and for this, I am firmly on board. This, however, is not the green light to write thousands of cypress tests. The bulk of our tests are manual with a layer of integration tests before we get to a few happy path automation tests. Haptik is hiring. Do visit our careers page here . Posted by Gopal on Dec 26, 2019 7:39:00 PM", "date": "2019-12-26,"},
{"website": "Haptik-AI", "title": "Paying It Forward: Open Source @ Haptik", "author": [" Raveesh Bhalla"], "link": "https://www.haptik.ai/tech/open-source-at-haptik", "abstract": "Tweet Right from early days at Haptik, when it was only Swapan and I writing code, we had this strong urge to ensure that we would contribute to the open source communities in any way we could. We had, after all, benefitted a lot from them. I can’t deny the disappointment I feel when I look around at the wider Indian tech ecosystem’s contribution. Yes, there are a few worth highlighting such as those mentioned over at this blog . However, the prevailing culture of “ Why would we contribute to open source if we are barely able to ship what we need to ship ” needs to go. Looking into it, we realized that the best way we could help out is by listing down some of the benefits we’ve seen. If the senior management of other tech firms in India were to look at this list, we hope they’d try to fit in open source contributions into their work environment and responsibilities. Code is easier to maintain: ask any programmer and they’ll tell you that if you build something long enough, and aren’t careful, you’ll soon have a ton of spaghetti code. Open sourcing is an easy way to force your team into modular programming practices and write good, clean code that anyone can understand. Free code reviews and bug detection: as soon a project picks up steam, there’ll be others depending on your work who’ll be generous enough to help out. Security: nothing freaks the Dev Ops guys as much as knowing you’re going to go out and show the world how your apps work. So before you get the go ahead from them, you’re forced into looking at the security side of things to protect your internal systems, which has a side benefit of protecting you from other malicious attacks as well. Job satisfaction: every star that a Github repo receives makes developers feel great about their work, probably more so than a dozen 5-star reviews on an app. Hiring: probably the most undervalued and overlooked benefit. See all those people starring, forking and contributing to your work? They’re all interested in what you’re doing and are, to different degrees, involved in it. They’re prime candidates that you should be reaching out to. Branding: you’ll win the love of the community for helping out others, and are almost guaranteed to win over loyal followers as a result. Open sourcing can be as impactful to the niche audience as any expensive marketing campaign. A big part of us wants to start a “Ice Bucket Challenge” equivalent of an open source contribution movement amongst startups in India, but we’re going to relent. We don’t want to force anything on anyone, or put anyone on the spot. The best open source work comes from those who genuinely want to contribute. <strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">If you love contributing to open source and want to work at a place where it’s highly encouraged, just send your resumes to careers@haptik.co. We’d love to talk to you and fit you in.</em></strong> Our contributions: CustomType: a library that allows Android developers to set custom typefaces to TextViews right from the XML itself. Also handles memory management for TypeFace objects. Proteus: allows you to tint white drawables with any color that you need at run time, both from Java code and from XML. Helps reduce your apk size by reducing number of icons required. Atlas: The standard location API on Android doesn’t have an inbuilt timeout, so we built this. Small, but ridiculously helpful. Hermes: Until v3.0, adding GCM to your app could be painful and had a big learning curve. Hermes made it easy. Now deprecated. Pacemaker: GCM messages not being delivered on time to a number of your users? Could be a b ug that can be fixed by adding Pacemaker. Want to share your own contributions? Or benefits you think we’ve missed out? Go ahead and comment. Posted by Raveesh Bhalla on Aug 21, 2015 5:57:00 PM", "date": "2015-08-21,"},
{"website": "Haptik-AI", "title": "How To Save AWS Costs Using Spotinst", "author": [" Ranvijay Jamwal"], "link": "https://www.haptik.ai/tech/save-aws-costs-using-spotinst/", "abstract": "Tweet When we speak about migrating our infrastructure to cloud, a few questions that come up are: 1. How much will this cost? 2. Can I move my application to the cloud without much effort? 3. Is it to build new applications in the cloud? 4. How safe is it? 5. Are you locked down to a specific cloud platform? As we know, cost is one of the most important factors while moving to the cloud, in this post we want to share how we optimized costs on AWS using various techniques. We are sure that by following these techniques you can significantly reduce your operating cost of your infrastructure. Many of us can be skeptical about moving to the cloud, especially because we don’t want to end up spending more. As Haptik is fully hosted on the AWS cloud platform, our cloud infrastructure costs depend on our usage. Most of the cloud platform providers these days offer an “on demand” model of payment. On-Demand model means “pay only for what you use” without any contracts or extra cost which has literally changed the way we host our applications in the cloud. How are we saving Infrastructure Cost? We were mostly using AWS On-demand instances for all our applications which over a period of 3 years has cost us significantly lesser than having an on-premise infrastructure. We’ve also had some Reserved Instances that by definition are not flexible due to their fixed terms. At that point, we decided to start experimenting with AWS Spot Instances in order to see a significantly decreasing in our cloud costs. AWS Spot Instances are “spare” capacity for which AWS allows users to bid for, and whoever bids higher, gets the instance. These instances are available at costs of about 70-80% lowe r than the normal on-demand instances. You can read more about spot instances here . We’ve started seeing cost savings of $5000 per month while using the AWS Spot Instances. Though using spot instances saves a lot of cost managing that activity (bidding, maintaining & scaling) was a great hassle, to say the least, not to mention the risks of running production environment on it. We had some scripts that helped us handle the process, but quickly we figured out we have to find a smarter, more efficient and reliable way to manage the operation and to cut off the risks dramatically. This is where Spotinst comes in. At Haptik, we are currently saving at least 85% costs on EC2 instances using Spotinst Elastigroup. There are many other features available on the Spotinst platform which we use to improve our performance while keeping the costs down. One such example is checking for Idle resources like unused volumes, instances etc. and releasing them to save costs. Other highly effective features of Spotinst are: 1. Optimizer 2. Eco 3. Radius 4. Mr. Scaler 5. Multai 6. Functions So What is Spotinst? Spotinst is an online cloud management platform which allows companies to run their mission-critical applications and manage their infrastructure while reducing up to 85% costs. Spotinst supports AWS, Azure, Google Cloud & Packet. Elastigroup is one of Spotinst’s main product which we use at Haptik to manage our virtual servers on AWS. What is Elastigroup? Elastigroup’s goal is to manage the Spot instances and thereby providing vast savings. It also helps to manage on-demand servers. The main drawback of using spot instances market directly (not from Spotinst Elastgroup) is the overhead of bidding and managing the server count 24*7 for your critical operations . This is exactly where Spotinst comes in. Spotinst will launch Spot Instances within the AWS account, the only thing that is needed is AWS account access via IAM. When creating a Spotinst account, the onboarding screens will request general details and you simply have to follow the instructions. Visit here for a complete guide on how to create Spotinst account. Setting Up and Using Elastigroup Without much ado, let’s get started with the Spotinst console. This is how the Spotinst console dashboard looks like: The Dashboard above gives an overview of all the resources you are using, the actual costs and how much have you have saved by using Spotinst. Let’s head on to what Elastigroups is, and how easy it is to set up: go to the Elastigroups page from the left menu: simply click on CREATE, which opens a Setup page: Next, you can choose from different types of Elastigroups. We will select Web Tier Autoscaling: That gives you the options below: General Settings The above will open ‘general settings’ form for you to fill and it is displayed as below: In Elastgroup there are some simple options to fill in and few advanced options that we would like to share: In Cluster Orientation , you get to chooses one of the following: Cost-Based (Spotinst will choose Spot Instances which are the cheapest), Availability Based (Spot Instances which can be available for a longer period of time) and Balanced Based (mix of the previous two) selection of instances in your Elastigroup. Another advanced option is Fallback to On-Demand , just in case spot instances are not available you will automatically use On Demand. In Scheduling , you can set various options like Capacity change etc. Compute Settings Click on next and you will see a page as shown below: This section is Compute, the most interesting part! It shows how easily Spotinst handles all the computation options using a simple form, I was really impressed. You need to choose the Availability zones, On-demand Instance Type, and Spot Instance type. You can select multiple spot instances. After that: Here you choose the AMI through which the instance should come up. If you scroll at the bottom you will see more options as below: In Additional Configurations , you are given options to choose EBS optimized, Public IP (true or false) etc. In Stateful , You can select from various options like Persist Root Volume and Persist Data Volume. By choosing those options, Spotinst will take continues snapshots of your instance with the relevant data attached and once they detect an interruption, they will launch a new instance with the same data attached from the latest snapshot. You can also “ Maintain Private IP ” of the server. More details here . In Load Balancers , as the name suggests you can choose options related to AWS Elastic Load Balancer. Also, Spotinst has many Integrations like CodeDeploy, Chef, Kubernetes etc. that you can choose from and remain in your familiar environment. Scaling Settings Scaling policies is one important part of managing infrastructure in the cloud and with the help of Spotinst, we can do that quite easily. In autoscaling you can scale up or down as needed and determine your strategy in advance as shown below: G o through the setting, click create and your Elastigroup is ready. You can see the newly launched instances in your AWS EC2 console. The newly created Elastigroup will now be available on the main Elastigroup page. Ease of Management From the left menu, select Elastigroups and it will list you all the Elastigroups. Click on any existing Elastigroup to manage it. Once you click you will be taken to the below screen which is specific to that particular Elastigroup: The above shows some basic details for the Elastigroup with stats like Cost Saving, Running Instances, Overview, Monitoring, Instances details, Timeline (Timeline of launched and terminated instances etc.), Deployments and Elastigroup Log. It also displays basic graphs, which give you insights of the Elastigroup performance. Spotinst has also got a very easy-to-use API, which we use for automated Deployments (daily Blue-Green Deployments). The Spotinst API documentation is available here . That’s how easy it is! We have been using Spotinst for over a year now to manage our AWS account and it is going great so far. We would love to hear from you. Do leave your feedbacks in the comments section. Haptik is hiring . Do get in touch with us at hello@haptik.ai Posted by Ranvijay Jamwal on Oct 24, 2017 6:37:00 PM Find me on: LinkedIn", "date": "2017-10-24,"}
]