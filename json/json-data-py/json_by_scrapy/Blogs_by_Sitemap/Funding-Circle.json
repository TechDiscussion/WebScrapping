[
{"website": "Funding-Circle", "title": null, "author": ["Mircea Gheorghiu ", "Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/07/", "abstract": "", "date": "2019-07-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Dave Martin "], "link": "https://engineering.fundingcircle.com/blog/2019/08/27/", "abstract": "", "date": "2019-08-27,"},
{"website": "Funding-Circle", "title": null, "author": ["Dave Martin "], "link": "https://engineering.fundingcircle.com/blog/2019/08/", "abstract": "", "date": "2019-08-27,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/09/11/", "abstract": "", "date": "2019-09-11,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/09/", "abstract": "", "date": "2019-09-11,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/10/21/", "abstract": "", "date": "2019-10-21,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/10/", "abstract": "", "date": "2019-10-21,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/11/05/", "abstract": "", "date": "2019-11-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/11/", "abstract": "", "date": "2019-11-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland ", "Alexandra Staniland ", "Alexandra Staniland ", "Dave Martin ", "Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2019/", "abstract": "", "date": "2019-11-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Jobright Peter "], "link": "https://engineering.fundingcircle.com/blog/2020/01/24/", "abstract": "", "date": "2020-01-24,"},
{"website": "Funding-Circle", "title": null, "author": ["Jobright Peter "], "link": "https://engineering.fundingcircle.com/blog/2020/01/", "abstract": "", "date": "2020-01-24,"},
{"website": "Funding-Circle", "title": null, "author": ["Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2020/02/10/", "abstract": "", "date": "2020-02-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2020/02/", "abstract": "", "date": "2020-02-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2020/07/14/", "abstract": "", "date": "2020-07-14,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2020/07/", "abstract": "", "date": "2020-07-14,"},
{"website": "Funding-Circle", "title": null, "author": ["Patrick Orrell", "Danny Olson", "Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/03/page/2/", "abstract": "", "date": "2015-03-12,"},
{"website": "Funding-Circle", "title": null, "author": ["Jack Zhou"], "link": "https://engineering.fundingcircle.com/blog/2015/page/5/", "abstract": "", "date": "2015-01-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson", "Danny Olson", "Afam Agbodike", "Danny Olson", "Amy Chen"], "link": "https://engineering.fundingcircle.com/blog/2015/page/4/", "abstract": "", "date": "2015-03-2,"},
{"website": "Funding-Circle", "title": null, "author": ["Enrique Figuerola", "Robert Crim", "jspc", "Patrick Orrell", "Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/page/3/", "abstract": "", "date": "2015-03-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Afam Agbodike", "Jason Michael", "Azher Hussain", "Aaron Probus", "Shaban Karumba"], "link": "https://engineering.fundingcircle.com/blog/2015/page/2/", "abstract": "", "date": "2015-04-22,"},
{"website": "Funding-Circle", "title": null, "author": ["Rose Molina Atienza", "Oleksandr Kruk", ""], "link": "https://engineering.fundingcircle.com/blog/2018/page/2/", "abstract": "", "date": "2018-05-2,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland ", "Avi Flax ", "Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2019/page/2/", "abstract": "", "date": "2019-07-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Andy Mendelsohn"], "link": "https://engineering.fundingcircle.com/blog/2014/03/10/", "abstract": "", "date": "2014-03-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Andy Mendelsohn"], "link": "https://engineering.fundingcircle.com/blog/2014/03/", "abstract": "", "date": "2014-03-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Andy Mendelsohn"], "link": "https://engineering.fundingcircle.com/blog/2014/", "abstract": "", "date": "2014-03-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Jack Zhou"], "link": "https://engineering.fundingcircle.com/blog/2015/01/26/", "abstract": "", "date": "2015-01-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Jack Zhou"], "link": "https://engineering.fundingcircle.com/blog/2015/01/", "abstract": "", "date": "2015-01-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Amy Chen"], "link": "https://engineering.fundingcircle.com/blog/2015/02/02/", "abstract": "", "date": "2015-02-2,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/02/09/", "abstract": "", "date": "2015-02-9,"},
{"website": "Funding-Circle", "title": null, "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/02/16/", "abstract": "", "date": "2015-02-16,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/02/23/", "abstract": "", "date": "2015-02-23,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson", "Afam Agbodike", "Danny Olson", "Amy Chen"], "link": "https://engineering.fundingcircle.com/blog/2015/02/", "abstract": "", "date": "2015-02-23,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/03/02/", "abstract": "", "date": "2015-03-2,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/03/09/", "abstract": "", "date": "2015-03-9,"},
{"website": "Funding-Circle", "title": null, "author": ["jspc", "Patrick Orrell"], "link": "https://engineering.fundingcircle.com/blog/2015/03/12/", "abstract": "", "date": "2015-03-12,"},
{"website": "Funding-Circle", "title": null, "author": ["Robert Crim"], "link": "https://engineering.fundingcircle.com/blog/2015/03/13/", "abstract": "", "date": "2015-03-13,"},
{"website": "Funding-Circle", "title": null, "author": ["Enrique Figuerola"], "link": "https://engineering.fundingcircle.com/blog/2015/03/15/", "abstract": "", "date": "2015-03-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Shaban Karumba"], "link": "https://engineering.fundingcircle.com/blog/2015/03/16/", "abstract": "", "date": "2015-03-16,"},
{"website": "Funding-Circle", "title": null, "author": ["Aaron Probus"], "link": "https://engineering.fundingcircle.com/blog/2015/03/21/", "abstract": "", "date": "2015-03-21,"},
{"website": "Funding-Circle", "title": null, "author": ["Aaron Probus", "Shaban Karumba", "Enrique Figuerola", "Robert Crim", "jspc"], "link": "https://engineering.fundingcircle.com/blog/2015/03/", "abstract": "", "date": "2015-03-21,"},
{"website": "Funding-Circle", "title": null, "author": ["Jason Michael", "Azher Hussain"], "link": "https://engineering.fundingcircle.com/blog/2015/04/09/", "abstract": "", "date": "2015-04-9,"},
{"website": "Funding-Circle", "title": null, "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/04/22/", "abstract": "", "date": "2015-04-22,"},
{"website": "Funding-Circle", "title": null, "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/04/28/", "abstract": "", "date": "2015-04-28,"},
{"website": "Funding-Circle", "title": null, "author": ["Afam Agbodike", "Afam Agbodike", "Jason Michael", "Azher Hussain"], "link": "https://engineering.fundingcircle.com/blog/2015/04/", "abstract": "", "date": "2015-04-28,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/05/20/", "abstract": "", "date": "2015-05-20,"},
{"website": "Funding-Circle", "title": null, "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/05/", "abstract": "", "date": "2015-05-20,"},
{"website": "Funding-Circle", "title": null, "author": ["Juanita Lee"], "link": "https://engineering.fundingcircle.com/blog/2015/06/01/", "abstract": "", "date": "2015-06-1,"},
{"website": "Funding-Circle", "title": null, "author": ["Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2015/06/08/", "abstract": "", "date": "2015-06-8,"},
{"website": "Funding-Circle", "title": null, "author": ["Razvan Spatariu", "Juanita Lee"], "link": "https://engineering.fundingcircle.com/blog/2015/06/", "abstract": "", "date": "2015-06-8,"},
{"website": "Funding-Circle", "title": null, "author": ["Jason Michael"], "link": "https://engineering.fundingcircle.com/blog/2015/10/12/", "abstract": "", "date": "2015-10-12,"},
{"website": "Funding-Circle", "title": null, "author": ["Jason Michael"], "link": "https://engineering.fundingcircle.com/blog/2015/10/", "abstract": "", "date": "2015-10-12,"},
{"website": "Funding-Circle", "title": null, "author": ["Jason Michael", "Razvan Spatariu", "Juanita Lee", "Danny Olson", "Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/", "abstract": "", "date": "2015-10-12,"},
{"website": "Funding-Circle", "title": null, "author": ["Luis Rivas Vañó"], "link": "https://engineering.fundingcircle.com/blog/2016/01/11/", "abstract": "", "date": "2016-01-11,"},
{"website": "Funding-Circle", "title": null, "author": ["Luis Rivas Vañó"], "link": "https://engineering.fundingcircle.com/blog/2016/01/", "abstract": "", "date": "2016-01-11,"},
{"website": "Funding-Circle", "title": null, "author": ["Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/08/05/", "abstract": "", "date": "2016-08-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Emma Makinson & Edu Caselles"], "link": "https://engineering.fundingcircle.com/blog/2016/08/15/", "abstract": "", "date": "2016-08-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Emma Makinson & Edu Caselles", "Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/08/", "abstract": "", "date": "2016-08-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Dennis Ideler & Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/09/06/", "abstract": "", "date": "2016-09-6,"},
{"website": "Funding-Circle", "title": null, "author": ["Dennis Ideler & Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/09/", "abstract": "", "date": "2016-09-6,"},
{"website": "Funding-Circle", "title": null, "author": ["Dennis Ideler & Oleksandr Kruk", "Emma Makinson & Edu Caselles", "Oleksandr Kruk", "Luis Rivas Vañó"], "link": "https://engineering.fundingcircle.com/blog/2016/", "abstract": "", "date": "2016-09-6,"},
{"website": "Funding-Circle", "title": null, "author": ["Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2017/04/07/", "abstract": "", "date": "2017-04-7,"},
{"website": "Funding-Circle", "title": null, "author": ["Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2017/04/", "abstract": "", "date": "2017-04-7,"},
{"website": "Funding-Circle", "title": null, "author": ["Nacho Munoz"], "link": "https://engineering.fundingcircle.com/blog/2017/11/26/", "abstract": "", "date": "2017-11-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Nacho Munoz"], "link": "https://engineering.fundingcircle.com/blog/2017/11/", "abstract": "", "date": "2017-11-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Nacho Munoz", "Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2017/", "abstract": "", "date": "2017-11-26,"},
{"website": "Funding-Circle", "title": null, "author": [""], "link": "https://engineering.fundingcircle.com/blog/2018/01/26/", "abstract": "", "date": "2018-01-26,"},
{"website": "Funding-Circle", "title": null, "author": [""], "link": "https://engineering.fundingcircle.com/blog/2018/01/", "abstract": "", "date": "2018-01-26,"},
{"website": "Funding-Circle", "title": null, "author": ["Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2018/04/27/", "abstract": "", "date": "2018-04-27,"},
{"website": "Funding-Circle", "title": null, "author": ["Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2018/04/", "abstract": "", "date": "2018-04-27,"},
{"website": "Funding-Circle", "title": null, "author": ["Rose Molina Atienza"], "link": "https://engineering.fundingcircle.com/blog/2018/05/02/", "abstract": "", "date": "2018-05-2,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/05/10/", "abstract": "", "date": "2018-05-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren", "Rose Molina Atienza"], "link": "https://engineering.fundingcircle.com/blog/2018/05/", "abstract": "", "date": "2018-05-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/09/07/", "abstract": "", "date": "2018-09-7,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/09/", "abstract": "", "date": "2018-09-7,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/10/15/", "abstract": "", "date": "2018-10-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/10/", "abstract": "", "date": "2018-10-15,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/12/04/", "abstract": "", "date": "2018-12-4,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/12/20/", "abstract": "", "date": "2018-12-20,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren", "Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/12/", "abstract": "", "date": "2018-12-20,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren", "Anna Keren", "Avi Flax ", "Avi Flax ", "Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/", "abstract": "", "date": "2018-12-20,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2019/03/04/", "abstract": "", "date": "2019-03-4,"},
{"website": "Funding-Circle", "title": null, "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2019/03/", "abstract": "", "date": "2019-03-4,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2019/06/06/", "abstract": "", "date": "2019-06-6,"},
{"website": "Funding-Circle", "title": null, "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2019/06/", "abstract": "", "date": "2019-06-6,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/07/05/", "abstract": "", "date": "2019-07-5,"},
{"website": "Funding-Circle", "title": null, "author": ["Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2019/07/10/", "abstract": "", "date": "2019-07-10,"},
{"website": "Funding-Circle", "title": null, "author": ["Alexandra Staniland ", "Mircea Gheorghiu ", "Jobright Peter "], "link": "https://engineering.fundingcircle.com/blog/2020/", "abstract": "", "date": "2020-07-14,"},
{"website": "Funding-Circle", "title": "Container Camp San Francisco", "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/04/22/container-camp-san-francisco/", "abstract": "I just got back from the Container Camp conference here San Francisco. It was\nan informative conference with presentations covering a wide range of interesting topics in the\ncontainerization ecosystem. There was a lot of discussion about the Docker application suite\nsince they are the dominant player, but there was also good coverage of alternative approaches\nand presentations on the full spectrum of tooling required to use containers in production. Common Themes Some Things are Hard One of the common threads among many of presentations was the concept that while Docker simplifies a complex problem, it exposes other difficult issues, and in a certain sense makes\nsome of them more complex. I would put networking at the top of this list, as Docker in many ways\nmakes networking even trickier than it already is. Fortunately, the community is very aware of\nthis and there are a number of very promising projects which I believe will resolve these issues.\n(And of course expose the next hard thing.) Containerize Everything? Another big – and very much undecided – area of discussion is what to do about databases and\nother large-scale services. Most people are in favor of containerizing most of the tools in\naddition to the core application, but there is a good deal of debate about how to deal with\ndatabases and other large services that generally require one or more dedicated machines. Containerization has obvious benefits for services that are small enough to be run together on a\nsingle machine, but what about the type of services that span multiple physical machines? There\nare definitely advocates for both approaches and no clear winner at this point. At Funding Circle\nwe are happy with our current DB setup, so we will probably not move to containers in that area\nunless a very strong case presents itself. If I were working on a greenfield project I might spend\nmore thought on this subject, but even then it would not be top of mind. Tooling There were a few key concepts that were mentioned by almost every speaker and the related\napplications: etcd - Almost every presenter mentioned etcd Software Defined Networking Weave Flannel SocketPlane - (recently acquired by Docker) Cluster Management Clocker Fleet Flocker Kubernetes Paz Rancher I won’t go into further detail about these since each topic is worthy of its own series of posts. Host OS vs. Container OS Host OS Three of the presenters were developers of host OSes designed for running containers and each has\ndeveloped a unique approach: CoreOS CoreOS is one of the early movers in the containerization world and\nwas the first to focus on running lightweight containers like Docker. It is a very lightweight\nhost OS that auto-updates the base software and libraries that make up the OS in one fell\nswoop. There’s no package management system, all applications outside of the base OS are\ninstalled via containers. The CoreOS team has also built many of the most popular tools to manage containers, and have\neven built an interesting Docker alternative: rkt . Project Atomic Project Atomic is RedHat’s project to make the OSes in their\necosystem Docker-friendly by default. They say it is production ready in RHEL, and will be\nproduction-ready in CentOS by June 2015. I expect they will get good early adoption, as it\nwill be an easy migration path for many of those looking in to containerization of their\nexisting infrastructure. RancherOS I had not heard of Rancher before but they have an interesting approach\n– they take what CoreOS started to the extreme: Docker becomes both the init system and the\npackage management system. A “System Docker” runs as PID 1 which starts and manages all of the system services as Docker\ncontainers. Software is installed by Docker containers and updates are a simple pull against a\nDocker repo followed by a container restart. As far as I understand there is literally nothing\nin the base OS except for the kernel and Docker. They also provide a platform product that simplifies running Docker in a clustered\nenvironment. From my perspective, the decision on host OS can be deferred and is probably one of the less\ncritical decisions points, although some tooling decisions will lead to a particular host OS.\nIt will be interesting to watch how this area plays out over the long run. Container OS What I found notable with relation to the OS is that there was almost no one who discussed what OS\nto use in the container images themselves, and those that did mention it did so only in passing. The first thing most people do is build an image that looks just like the servers they are\nrunning. They start with a full version of CentOS, Ubuntu or the like, and include all the\ndependencies and applications they are used to. These images quickly grow to gargantuan scales.\nOur own worst offender clocked in at nearly 2.5GB. Once you realize this is not a viable approach you will likely try to bring the size down by\nstripping out unnecessary software or squashing the layers, but even these stripped down images\ncan wind up around half a gigabyte or more as many of the standard OS base images clock in at\nhundreds of MB. For our second generation Docker infrastracture we are using Alpine Linux as our base image, which clocks in at just about 5MB. This is a reasonable starting point for\ncreating an image. While it’s possible to get even smaller, this seems like a worthwhile tradeoff\nfor the additional comfort of a good set of familiar tools, a complete package management\nsystem, and almost everything you could want to run an application. Technically, you don’t require an OS at all to run services in a container. With statically linked binaries you can fairly easily do without the OS, but there are many already existing applications\nthat cannot realistically be converted to static compilation. It is even possible to build an\nimage that works with dynamically linked binaries and libraries, but this seems like more effort than its worth in most cases. I expect that for the foreseeable future most implementations will need some sort of OS on the\ncontainer, and Alpine Linux seems to be the a great option. I highly recommend looking at Alpine\nor other similar micro distributions, as early in your containerization efforts as possible. Container Adoption What surprised me the most was that at a conference dedicated entirely to containers, only about\n10% of the attendees were actually using Docker, or any type of container, in production. I\nexpected that there would be a higher level of adoption by now among those interested enough in\nthe concept to attend this type of conference. It does imply that the interest in Docker /\ncontainers is very strong, so I expect the rates of production use to quickly go up in the coming\nyear. Here at Funding Circle have been using Docker in production for about 6 months, and first\nstarted working with Docker almost a year ago. Now that we’ve gained a better understanding of\nthe role containers can play we are starting our 2nd generation Docker implementation using\nthe latest and greatest techinques and tools in the container ecosystem. Notable Presentations Cloud 66 Cloud 66 is a consulting / contracting company offering DevOps as a\nservice. The presentation by Khash Sajadi covered their experience implementing Docker on\nthousands of servers for numerous clients. He discussed the hurdles they faced with their various\nclients, both from an adoptance standpoint, as well as technically, which they’ve reduced to 11\nmajor topics that need to be addressed in each implementation: OS Vendor for Host and Container Scheduling Engine Networking Service Discovery Continuous Integration / Delivery OS package repository: should it be private or public? Should data & the DB be in a container? Logging - Active & passive logging Monitoring Load balancing: Inside / Outside Debugging Note : I believe there is 12th item in addition to the 11 identified by Sajedi that is\nimportant to consider while planning any infrastructure implementation: Auto Scaling Due to the limited time of each presentation he was only able to go into detail on the three\nbolded topics. The topics he dived into were really informative and talked about the\npracticalities of adopting containerization in a production environment. It was nice to see that we’ve started planning here at Funding Circle about almost all\nof these topics in our planning. I feel confident we are on the right path for our second\ngeneration containerization efforts, but we still have a good deal of work to do to get it\nright. Docker as a desktop application package management system My favorite presentation at Container Camp was “The Willy Wonka of Containers” by Jessie Frazelle\nfrom Docker. While it wasn’t particularly relevant to what we are doing\nat Funding Circle, her talk was unique and thought-provoking presentation. She has gone\ndown the path of using Docker as a package management system for desktop use, to the point where\nshe has packaged Chrome, and some 3D games into containers. This not something I ever considered\nusing Docker for. I’m sure other folks have already considered this use case (its an extension of the concept\nof what many are already doing on servers) but Jessie takes it to a degree that I’ve not seen\nanybody else do. It is a great experiment and shows the kind of potential containerization has when you push\nit to the extreme edges of what it’s designed for. It makes me excited to think about what\nother fascinating applications will come out of the ecosystem. She has an in-depth blog post if you are interested in more detail on this topic. Most interesting sponsor product Sysdig Sysdig has a really interesting monitoring / performance dashboard with\nrealtime streaming data of system level information alerts – you can drill down to an almost\nridiculous level of granularity but also get a 10,000 foot view of your infrastructure. It can\nalso show you the state of your entire system at a specific point-in-time and lots of other cool\nstuff I haven’t had time to look into yet. I plan to dig deeper into their offerings soon to see\nif it is something that will be applicable for us. All the presentations are now available on Container Camp’s YouTube channel . Are you interested in working on Docker in a production environment? Come join us!", "date": "2015-04-22,"},
{"website": "Funding-Circle", "title": "How to replace an inherited third-party app with one everyone loves", "author": ["Emma Makinson & Edu Caselles"], "link": "https://engineering.fundingcircle.com/blog/2016/08/15/how-to-replace-an-inherited-third-party-app-with-one-everyone-loves/", "abstract": "Some time ago, together with the Product and Design teams at Funding Circle, we started thinking about refreshing the whole user experience for Funding Circle investors - starting with the mobile app. We had an existing app for iPhone and iPad, written by an external agency in 2013, but it couldn’t meet our investors’ needs and was increasingly difficult for us to maintain. This August, after many months of careful planning and hard work, a second version of our iPad app was released to Funding Circle investors. In doing so, we brought the iPad app up-to-date with the iPhone app we released at the end of last year, and were able to completely remove the earlier version of both apps, which we’d been supporting for three years. Along the way, we’ve learned a few things about rebuilding applications - this is our perspective on when you should do it, how you should do it, and what you can expect at the end of it all. When should you rebuild an inherited application? When you keep running into problems There were multiple reasons for rebuilding the iOS app from scratch. For a start, the app felt old and users frequently suffered random crashes that were difficult to debug. The codebase also needed to be brought up-to-date with new technical requirements: it used deprecated Apple APIs, and supported four different versions of iOS. The original app was also created on a moving surface with the API changing constantly and not taking into account the mobile needs. Finally, the existing code was hard to maintain and did not reach the standard we wanted : some areas of the codebase were duplicated; everything was tightly coupled; and there were no tests across the whole app. There is nothing wrong with having an external agency build your application. It is a quick way to market, and it allows you to take a pulse of what your users really want from it before you invest more heavily in the product. However, maintaining an externally-written app can be expensive in the longer term . Ultimately, great apps need dedicated specialists (developers, designers, etc.) and great design takes time to work through. When you’ve first tried to improve it We did not take this decision lightly, and we were conscious of how rewrites can seem very exciting and then easily go wrong, so we first tried to stabilise and improve the existing app instead. We added Crashlytics and, for three months, we worked hard on optimizing the network requests and fixing all the major issues causing crashes to our users. At the same time, we introduced functional tests across the app, so we could make sure that we were not breaking anything. In three months, we managed to increase the percentage of crash-free users by more than 20%, but there was a lot of work still to do, making any change took far longer than it should, and we hadn’t been able to deliver any new features . When the existing product will hold you back After working with and exploring the codebase for three months, representatives across Product, Design and Engineering felt that the existing codebase was no longer fit for purpose, and that we should not try to extend it. While we did not want to replicate all the functionality of the desktop site, we wanted to be able to provide lots of new features as well as small delights for our users , from offering simple conveniences like Touch ID, to delivering different views and experiences for different screen sizes, to keeping up-to-date with standard UI conventions. Rebuilding the application seemed to be the obvious step, if we really wanted to achieve all these big ideas we had in mind and to transform the way our users engaged with us. How should you go about it? Listen to your customers We decided that if we were going to redefine the way our customers interacted with Funding Circle, we should ask them what they wanted. When thinking about mobile, it is easy to make the common mistake to try to match the same features that you offer on the desktop site, when in reality mobile users have different needs and different preferences. You should see mobile as an extension to your desktop experience, instead of a replacement . We focused on content first and took the approach of building constant prototypes, validating them with our customers and iterating over them until we got the user experience right. This was an exhausting but very enlightening effort that the Design team drove, but in which the whole team participated. The true value was not only in getting a better sense of what users want to do when they open the Funding Circle app on their smartphone, but also in making sure that everyone in the team understood why we were doing things the way we were doing them , so that we all knew the reason behind every single decision. Work closely with the rest of the business As developers, we’ve worked closely with the design team and the product team to build the app. When we started working on the app, we worked with a dedicated product owner and designer, both of whom were really invested in delivering a beautiful - but also technically resilient - product. Everyone had a clear role and different responsibilities, but we sat together, talked often, and made decisions as a group : product owners, designers and ourselves all bringing valuable different perspectives to the table. Take the time to get things right That close working relationship helped us prevent and deal with issues as they arose: there was a high level of trust across the team, and we were given the space and time to get things right and fix small issues before they grew. At Funding Circle, all teams are Agile. On the Mobile team, we used our weekly demos as opportunities to review our deliverables, revise our expectations, and take time to explore assumptions and refactor existing code where needed. We had broad timelines for the project as a whole, but we engaged frequently with the product team to review our expectations of scope - ultimately cutting some features from the initial release so that we could deliver something with which everyone felt comfortable. Surface and tackle difficult issues together At Funding Circle, we use pair-programming to transfer knowledge about different parts of the system, and as an opportunity for wider knowledge-sharing and coaching. On the Mobile team, we do this too: we spend roughly half of every day writing code together with another member of the team . Throughout the app development process, this approach helped us to spot complex issues early and find the right solutions - by combining the knowledge, experience, and different perspectives of the wider team. Make small changes one by one Looking back across the project, one thing we might have done differently is to rewrite and deliver changes in the existing app , rather than trying to deliver the whole rewritten app in one go. Having defined what we wanted the app to do, and how we wanted to make the changes (involving users in user testing, for example), we could have chosen a few small areas in the old app to tackle first, delivering refactors to our investors one at a time. That would have meant continuing to work with the old application, but would have allowed us to get quicker feedback from our users, as well as managing scope-creep more tightly: with smaller changes, it’s easier to see when additional features have added significant extra pressure. What should you expect? At the end of this investment of time, what can you achieve? Can you solve the problems with an existing codebase? Well, our investors are happy : app users frequently call our customer relations team to tell us how much they enjoy using our product. The app doesn’t crash on them anymore, either. We have between 99.8% and 100% crash-free users on any given day . They send us their feedback, too - much of which we can now respond to in a matter of days or weeks. And as developers, we feel much more comfortable with our codebase now, which has been reduced to 52% of the original one. We have got tests covering most of the relevant flows in the app (from unit tests to UI functional tests) and we have a completely automated Continuous Integration and Continuous Delivery pipeline (thanks to CircleCI and Fastlane ). Just to give you an example of how quickly we can deliver now, last week, we were able to react to a user’s suggestion with a new submission to the App Store within three hours .", "date": "2016-08-15,"},
{"website": "Funding-Circle", "title": "Factory Time", "author": ["Aaron Probus"], "link": "https://engineering.fundingcircle.com/blog/2015/03/21/factory-time/", "abstract": "Factory Time is an open source Clojure library for managing test data. Intro When we started building the new investor API we went down the road of keeping things as simple as possible.\nIn order to keep each test as a self contained unit test data was declared at the top of each file.\nAs the project grew larger, this resulted in duplicated test data, and worse, bugs. Inconsistent type bug we found ; file1_spec.clj ( def my-test-data { :id 1 }) ; file2_spec.clj ( def my-test-data { :id \"1\" }) At this point it was time to extract the data out of the individual test files. The first pass consisted of moving the test data definitions into a separate namespace and removing duplicates.\nNext, we created a consistent access point to the data by leveraging a multi-method that also handled overriding of values.\nThe result was: Decomplected test files: Test files became about writing tests, not wading through arbitrary data Deduplicated test data: All test files had access to the same data, so only one file had to be updated when the data changed Unified interface for creating test data: No more subtle differences between files ( build-data vs build-my-data ) Once the initial pass was completed, it was clear that the data managing functions could exist as a standalone library.\nFrom there, we extracted Factory Time into a separate library and added some extra features. Usage ; people_factory.clj ( deffactory :person { :name \"Billy Joe\" , :age 42 }) ( deffactory :child { :age 12 } :extends-factory :person :generators { :smart ( fn [ n ] ( even? n )} ; n starts at 1 and will increase by 1 every time build is called :create! save-child! ) ; people_spec.clj ( factory/build :child { :hair-color \"red\" }) ; {:name \"Billy Joe\" ;  :age 12 ;  :smart false ;  :hair-color \"red\"} Calling build performs a multi level merge with the following precendence (lowest to highest): Parent factory result Factory defaults Generated values Build overrides create! result (skipped when build is called) Conclusion By implementing Factory Time in our project, we saw several key improvements: Single source for test data: Write your test data once and use it anywhere More comprehensible tests: No need to skip over 50+ lines of test data to get to the actual tests If you want to try out Factory Time, take a look at our github page for up to date information.", "date": "2015-03-21,"},
{"website": "Funding-Circle", "title": "Onboarding during lockdown: How it feels to join a company remotely", "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2020/07/14/onboarding-during-lockdown/", "abstract": "Monday 23rd March 2020, our whole way of life changed here in the UK. Lockdown officially began. Which meant everyone that could, started working from home. We went from jumping on the train or riding a bicycle into the office every morning to adjusting to our makeshift home office. It became the most surreal experience very quickly with the weeks flying by, I’ve actually lost count of how many weeks it has now been. We quickly adapted to our “new normal” across the business but especially in engineering. During this time, we’ve been able to experiment and grow our teams in a number of ways, like working in a full remote team, see our team step up to the challenges and adapting to how we onboarded any new staff that were due to start during the lockdown. Everything became virtual — how to set up your laptop, introduction to the company and meeting your team. However the biggest challenge and thoughts were “how will it feel to join a company remotely and how do we make them feel comfortable?”. To shed some light on how it has been onboarding with us during this weird and crazy time, we caught up with one of our newest engineers, George. First of all, tell us a little bit about yourself… I work in the Platform Engineering team, looking after the infrastructure and services that allow the other engineering teams to build and run great products. We get to work with lots of cool technology, which I really enjoy. Outside of work, I love climbing and am very much looking forward to climbing gyms reopening after lockdown! You started your new role with us during the most unusual time and one you’ll definitely never forget I’m sure! How did you find starting a new job being totally remote? It was actually nowhere near as difficult as I expected it to be. The logistics were pretty seamless - my laptop arrived on the morning of my first day and the IT team helped me get everything setup quickly. Of course it’s a little strange not meeting people in person, but it quickly felt normal. I think it helps that the 100% remote working situation is relatively new to everyone. How did you think the team did in welcoming you remotely? They were great - my manager set me up with a buddy, so I had someone to ask all those little questions you have when you start a new job, and also got the rest of the team to schedule 1-to-1s with me so that I got to meet everyone individually. Were there any challenges you faced when joining? I would say it’s definitely a bit harder to get up-to-speed with things when you can’t shadow people like you would in the office. I think that just means you have to be a bit more proactive about asking questions and finding things out yourself. How will you continually build relationships whilst working remotely? We have three or four team meetings a week which are a good way of getting to know what other people are working on, and pairing up to work on something is another good way. How will you feel being able to go into the office for the first time being part of the team? I think it’s going to feel a little strange at first - meeting people without their Zoom backgrounds might take a while to get used to! But I’m looking forward to it, although perhaps not having to commute again! What is the biggest thing you’ve learnt during this time? Apart from all the technical things I’ve learnt, I think it’s probably the same as everyone else - how to work remotely 100% of the time. It takes some getting used to. And that was George’s experience joining our team. With his feedback and the one from many others who will be joining us, we will continue improving and streamlining our remote onboarding processes. Aiming at making joining the Funding Circle Engineering team the easiest and most welcoming experience possible, despite the odd circumstances. Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2020-07-14,"},
{"website": "Funding-Circle", "title": "ClojureCircle meetup", "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/05/10/clojure-circle-meetup/", "abstract": "As you may be aware, we have recently launched a new Clojure specific\nmeetup hosted at Funding Circle’s London office. The aim is to bring the\nwider Clojure community closer together to share knowledge, network\nand eat some pizza. We were back with our second ever\nevent last week,\nand it was great to see the number of attendees growing - thanks to\nall who came. For those of you couldn’t make it (it would be great to\nsee you next time) we wanted to give you access to some slides from\nthe presentations. Here are the topics that were covered. Stuart Robinson - Building serverless applications in Clojure Gilles Philippart - DDD in a Clojure event-driven application Hiram Madelaine - How we exposed a Rule Engine as a microservice using Clara If you are interested in speaking at one of the upcoming events,\nplease feel free to get in touch with Jamie Smith or Anna Keren via engineering-talks@fundingcircle.com . Naturally, it wouldn’t be a traditional ‘Events Blog post’ without a recruitment\nplug… We are currently looking for software engineers to join our team. If you\nare interested in learning more about working with Clojure at Funding Circle you\ncan read more about our current openings . Follow Funding Circle Engineering on Twitter. Spread the word!", "date": "2018-05-10,"},
{"website": "Funding-Circle", "title": "Conference Review: The Lead Developer London 2019", "author": ["Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2019/07/10/lead-dev-london-2019-review/", "abstract": "Hey! I’m Mircea and this is my second year of attending The Lead Developer London conference; I would say if you’re interested in leading and managing engineers, this is the best conference for you! This year I found it very refreshing and full of interesting topics with content for both seasoned managers and those just making the switch from individual contributor to manager. The conference has only one track for both days it runs on, which is handy as you will never miss a talk that you might be interested in. Lead Dev works by having 30 minute presentations and then 10 minute ones thrown in between to mix things up. Every year they also have 2-3 talks which are more technical to keep it interesting. Most of the topics are really interesting, so the 10 minute talks leave you wanting more and sometimes I wish they were a little bit longer, perhaps substituting the tech talks out for this could be the one thing that might improve the day. The highlights are usually the keynotes and this year they didn’t disappoint. The first key note was by Laura Hogan about team friction and the key stages in a team’s life. Laura spoke a lot about the 6 core human needs you need to fulfil to be safe and secure at work (fine, I’ll spoil it for you: belonging, improvement/progress, choice, equality/fairness, predictability and significance). This was extremely interesting and very on point when focusing on having high performing teams. She also launched [her new book, “Resilient Management” , which I would definitely recommend for both new and experienced managers. Even though I’ve been an engineering manager for the last 9 years, I still found things in the book that reminded me of good management habits. Other notable talks were from Heidi Waterhouse (“12/10, Excellent doggo: the power of positive transformation”), Melinda Seckington (“Level Up: Developing Developers”), Pat Kua (“Flavours of technical leadership”), Ola Sitarska (“Behind the scenes of an effective & inclusive hiring process”) just to name a few. My personal favourite was probably an obvious one: Nickolas Means with “Eiffel’s Tower”. Nick is a veteran of Lead Dev conferences and the way he uses storytelling to enforce engineering practices in his talks is absolutely brilliant. This year he used the story of how the Eiffel Tower was built, to show that politics and knowing how to influence upwards are everyday issues in all companies. The more you know how to navigate that, the better off you and your team will be. That made me challenge some of my beliefs that getting involved at all in politics (or “making friends and telling stories”  Nick calls it, I love this) is not something I should be doing. In all honesty, I will not be doing justice to this talk if I try to explain it, so the best thing you can do is to actually watch it yourself here . Oh did I forget to say that all the talks are recorded and available for free online? My bad! Here’s the whole playlist from this year’s conference . As parting words, I would like to mention something about the sponsors and the food: there’s loads of tech companies present in the foyer and giving away gadgets and gear (I love my Datadog t-shirt this year!) so be sure to walk around and explore it. The buffet food provided was really delicious, but just make sure to grab some before it’s gone! See you next year Lead Dev London! Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2019-07-10,"},
{"website": "Funding-Circle", "title": "Mistakes were made (VIDEO)", "author": ["Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2017/04/07/mistakes-were-made/", "abstract": "This talk was done at the March event of the London CSS Meetup , hosted in our office. I took the opportunity to talk about our internal styleguide, specifically the mistakes we’ve done while building it and what we’ve learned and how we tried to fix them. Mistakes Were Made - Sessions by Pusher", "date": "2017-04-7,"},
{"website": "Funding-Circle", "title": "Salesforce multi-org development & deployment at Funding Circle", "author": ["Jobright Peter "], "link": "https://engineering.fundingcircle.com/blog/2020/01/24/salesforce-global-deployment/", "abstract": "Our approach to developing global features for multiple similar but not identical Salesforce organisations . Our Salesforce Organisation Landscape We currently use three country-specific Salesforce organisations (orgs): UK Salesforce org - our greenfield implementation US Salesforce org - inherited via the acquisition of Endurance Lending Network Continental European (CE) Salesforce org - inherited via the acquisition of Zencap All three orgs use very similar data models to support similar business processes but they are not identical because they were built in parallel by three different companies. Each org has a different set of integrations with country-specific backend applications and third party Salesforce apps. Although the deployment process was similar for each org, they all use separate source code repositories. Any re-use of features between orgs was achieved by manually copying code and metadata from one repository to another. Goals We needed a deployment process and development methodology that facilitated the following: Build once and deploy into all three orgs without having to copy-paste code between repositories Development of global features but allow local customizations Development of local features that need/will not exist in all three orgs Gradual move to a single codebase as a long term goal Not add disproportionate overhead Single Deployment Process With these goals in mind, we designed and built the new deployment process by making the following changes. New Global Repository We added a new source code repository that would contain the global features and be org-agnostic. The global features will comprise a super-set of sub-features required by all the countries. These sub-features can be switched on or off in each org using custom settings/metadata. Country-specific features will continue to remain in the existing local repositories. New Trigger Framework A new trigger framework was introduced to handle triggers in the global codebase or the local codebases based on filters that can be configured using custom metadata. Global Deployments To deploy a global feature (usually initiated by a merge into the master branch of the global repository), three parallel jobs will be executed, deploying to the three orgs. Each job will merge the master branch of the global repository with the (unchanged) master branch of one of the country-specific repositories and validate the merged code into the relevant org. If any duplication of files is encountered during the merge, the deployment will fail and require manual investigation. After all three validations have succeeded the code would be deployed into all three orgs. Local Deployments To deploy a local feature (usually initiated by a merge into the master branch of the local repository), a single job will be executed. This job will merge the (unchanged) master branch of the global repository with the master branch of the local repository and deploy it into the relevant org. Again, if any duplication of files is encountered, the deployment will fail and require manual investigation. Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2020-01-24,"},
{"website": "Funding-Circle", "title": "ClojureCircle at Funding Circle", "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/12/04/3d-clojure-circle/", "abstract": "We hosted the third Clojure Circle meet-up this year at the Funding Circle London HQ. We were joined by fifty of London’s finest for the presentations as well as the mandatory beers, pizza and socialising. Our aim once again was to host and enable engineers from the Clojure community to share their experiences and showcase their recent projects and research. This event was kicked off by Andy Chambers, Senior Software Engineer @ Funding Circle, with an innovative solution for testing distributed systems built on Kafka using Clojure. The ideas behind Andy’s presentation can be applied to verification of any distributed software. After a quick mingling break before we cracked on with the next talk. Andrea Crotti, Senior Software Engineer @ Funding Circle, excited the audience with his presentation on using Re-Frame framework while working on one of his projects. In the final part of the meet-up, Renzo Borgatti from Droit and the author of the book “Clojure Standard Library” enlightened us with the historical perspective of LISP. The guests and speakers then had plenty of time to discuss the talks and to socialise together and talk all things Clojure! The recording of the event can be found here . The presentations can be found here:\n- A tale of Lisp - ELO - test-machine We have some big plans for Clojure Circle in 2019 and we hope to see you again at the next ClojureCircle meet-up. Follow @FC_Engineering on twitter. Spread the word! If you would be interested in speaking at a future event then please reach out to us directly .", "date": "2018-12-4,"},
{"website": "Funding-Circle", "title": "TDD in Clojure", "author": ["Luis Rivas Vañó"], "link": "https://engineering.fundingcircle.com/blog/2016/01/11/tdd-in-clojure/", "abstract": "It’s been a few months since I first approached Clojure ,\nand approaching a functional language for the first time is quite an experience\nthat makes you revisit and reevaluate many of your past experience and know-how\nwith object-oriented (OO) languages. Testing and TDD/BDD is one of the main foundations of high quality software.\nBut TDD/BDD doesn’t feel as natural in functional languages as it does in OO\nlanguages; this has been one of my main issues working with Clojure. Do I really need to test as much as before? How do I test code composed of\nfunctions only? How do I mock?  Should I mock? Are functional languages hard to\ntest or am I doing something wrong? This post is aimed at giving some advice to how to TDD in Clojure, given that\nmany OO techniques both for testing and testability, are not directly\napplicable to Clojure. This is a summary of the little information I found\nabout adapting TDD to programming in Clojure, together with my own experience.\nLooking back, I can summarize three main concerns: Differences in TDD flow What testable code means in Clojure How to implement common testing techniques (i.e. mocking) Warning: These advices come from just few months of experience in Clojure, so\nthey might be incomplete, or just plainly wrong! Differences in TDD flow The main difference I’ve found is moving from the classical top-down design, to\na bottom-up strategy. A bottom-up approach feels more natural, helps keeping your impure functions\nunder control, and removes a lot of overhead by avoiding mocking in many\nsituations. Uncle Bob Martin recommends this approach in the article TDD in Clojure but mainly because of a lack of mocking tools in Clojure by the time he wrote\nit (2010). That’s no longer the case, but I would still recommend giving a try\nto a bottom-up approach. For example, try to build your DB layer first, then\nstart going up till you reach the UI. This makes easier to keep all I/O\nisolated in a single namespace. In any case, I recommend ignoring the quite spread opinion that functional\nlanguages do not need unit testing or only a small amount. I think this belief\nis based on a misunderstanding of certain properties of some functional\nlanguages. Some functional languages do need much less testing, and people mistakenly\nconfer this to its functional nature. In the case I’m familiar with, Haskell,\nthe reduced amount of testing is due to the combination of its functional\nparadigm, its side-effect encapsulation (monads), and its static type system\nwith type inference . It’s not that it doesn’t need tests; it’s that it has a built-in testing system called type system. And it only helps with some\nlow-level testing: you still need your usual amount of business logic tests. But Clojure is a dynamic language. The compiler does not reason about whether a\ncertain function can receive an invalid type, using type inference. Not only\nthat, but Clojure does not help with keeping side effects under control.\nBecause of this, I recommend to keep your test coverage as high as you do with\nany other OO language. Even if a reduced-test approach is feasible, I don’t\nthink it’s a good approach for a Clojure newbie. Testability in Clojure I would say that most of the problems I’ve had with TDD, happened because of\nlow testability of my code. What does testability mean in Clojure? Most people agree on pure functions\nbeing the main testability enhancer in a functional language. Also, dependency\ninjection helps both testability and function pureness. Avoiding untestable\ncode in namespaces and changing your OO design mindset is also important. Pure functions A pure function can be defined as a stateless function, or a referencially\ntransparent function, or plainly as a function that has no side effects and,\nfor a given input, always returns the same output. Usually, this mean no state,\nno I/O, no side effects. For example, random() is not a pure function, because it is not\ndeterministic, you may get different output when calling it with the same\ninput. A function that writes to a database is not pure, because it has a side\neffect. A function that calls three impure functions and “branches” the\nexecution is not pure, because it has side effects (well, it depends ).\nA function that makes a decision by reading from the database, is not pure\neither because it can return different results for the same inputs depending on\nthe content of the database. So, you should always try to have a clear boundary between pure and impure\nfunctions in your code. Usually, you’ll want to keep your impure functions free\nof business logic and locked up in the lower (DB) and the upper (UI) layers,\nkeeping the rest of the code as a chain of pure functions. Just as a comment, I\nhaven’t managed to do that yet, but the closer to that approach I am, the\neasier it is to test my code. Dependency injection Dependency injection can help in two different ways. The more obvious is, it\nmakes mocking easier: just pass the mock as an injected dependency, and you are\ngood to go. The second way is, it allows you to turn an impure function into a\npure one. Dependency injection in Clojure has its own difficulties in contrast with OO\nlanguages. Where in OO languages you typically inject an object or a class as a\ndependency and store it in your state, in functional languages you only deal\nwith functions being injected into other functions, usually without a state for\nstoring the injected dependencies. This adds quite an overhead. Given that you have no internal state like an\nobject does, you cannot store your injected dependency in that state. So,\nyou need some other technique to pass the injected function to the receiver\nfunction. Five faces of dependency injection contains a compendium of five different techniques to deal with this. I would\nrecommend either using function arguments, or try a reader monad. In any case, injection adds an overhead. So, we should try using it only when\nfacing a hard to mock function, or for increasing pureness. Or try to replace dependency injection with function composition. For example: ( defn should-i-print-this? [ arg ] ( = arg :yes )) ( defn print-it [ arg decider ] ( when ( decider arg ) ( prn arg ))) ( print-it :yes should-i-print-this ) but with function composition: ( defn filter-to-print [ arg ] ( when ( = arg :yes ) arg )) ( defn print-it [ arg ] ( when arg ( prn arg ))) ( def print ( comp print-it filter-to-print )) ( print :yes ) Deciding whether the composition or the injection version is better depends on\nthe situation. It’s just another tool that might be useful! Increasing pureness with dependency injection Dependency injection can help to keep impureness under control, by extracting\nimpure behavior to functions with as little business logic as possible, and\nthen injecting them where needed. This way, we can keep an otherwise impure\nfunction pure: ( defn transform-if-exists \"This function is pure.\" [ entity checker ] ( when ( checker ( :id entity )) ( transform entity ))) ( transform-if-exists entity db/checker ) instead of ( defn transform-if-exists \"This function is not pure\" [ entity ] ( when ( db/checker ( :id entity )) ( transform entity ))) ( transform-if-exists entity ) The first function is pure. You can test it without worrying about fixtures,\nfactories or similar. The second one is not, it needs you to redefine db/checker or use some db fixtures. It’s harder to test. It’s slower to test. Assembly line vs Object interactions As OO developers we are used to have code whose execution path branches instead of keeping a single line of execution. We make our objects to speak to\na lot of other objects and our programs end up being a composition of objects\ninteracting with each other. For example, if we need to save a record, then turn some flag on it, and then\ntransform it, we tend to do this: ( defn- save [ thing ] ( db/save thing )) ( defn- mark [ thing ] ( db/update ( assoc thing :mark true ))) ( defn- transform [ thing ] ( assoc thing :name \"I'm transformed\" )) ( defn save-mark-and-transform [ awesome-thing ] ( db/save awesome-thing ) ( db/mark awesome-thing ) ( transform awesome-thing )) The problem is, this approach is not suited for functional languages because of\nthe amount of side effects. Instead, we should try to think about our programs as assembly lines , where\nthere is one single flow of execution, and the output of one element is the\ninput of the next one. The previous code example would look like this: ( defn- save [ thing ] ( db/save thing ) thing ) ( defn- mark [ thing ] ( let [ marked-thing ( assoc thing :mark true )] ( db/update marked-thing ) marked-thing )) ( defn- transform [ thing ] ( assoc thing :name \"I'm transformed\" )) ( defn save-mark-and-transform [ awesome-thing ] ( -> awesome-thing save mark transform )) The difference is not only syntactic, but structural. Each function takes\nprevious function’s output as its input. This means, we are enforcing each\nfunction to return something, instead of triggering a side effect, and we\nare keeping a single flow of execution. Also, this mean we can just use\ncomposition for building new functions: ( defn- save [ thing ] ( db/save thing ) thing ) ( defn- mark [ thing ] ( let [ marked-thing ( assoc thing :mark true )] ( db/update marked-thing ) marked-thing )) ( defn- transform [ thing ] ( assoc thing :name \"I'm transformed\" )) ( def save-mark-and-transform ( comp save mark transform )) The more I code this way in Clojure, the easier to maintain my code is (at\nleast for now: ask me in six months, maybe I’m wrong!). Untestable code in namespaces Avoid complex code to execute at namespace load time. Nothing more than that. Testing techniques I really recommend reading chapter four of Test-Driven Development in Clojure (Niclas Nilsson, 2015) It’s a good compendium of basic testing techniques for Clojure. Mocking With dependency injection If your function accepts its dependencies as arguments: ( defn awesome-function \"I'm so awesome I have my dependencies injected\" [ some-function stuff1 stuff2 ] ( let [ stuff { :a-key stuff1 :another-key stuff2 }]) ( some-function stuff )) testing it can be as simple as: ( deftest awesome-function ( let [ mock-function ( constantly \"whatever\" )] ( is ( = \"whatever\" ( awesome-function mock-function 3 5 ))))) Without dependency injection If your function does not allow to inject a certain dependency, you can still\ntest it. Assuming we have: ( defn crappy-function \"I'm so crappy I have my dependencies hardcoded\" [ stuff1 stuff2 ] ( let [ stuff { :a-key stuff1 :another-key stuff2 }]) ( some-namespace/some-function stuff )) testing it would be: ( deftest crappy-function ( with-redefs [ some-namespace/some-function ( constantly \"I am really crappy\" )] ( is ( = \"I am really crappy\" ( crappy-function mock-function 3 5 ))))) If we use Midje , a test framework for\nClojure, it looks slightly better: ( facts \"about `crappy-function`\" ( fact \"returns whatever some-namespace/some-function returns\" ( crappy-function 3 5 ) => \"I am really crappy\" ( provided ( some-namespace/some-function anything ) => \"I am really crappy\" ))) But in any case, it’s better to inject the dependency when possible. Using\nredefinitions is not always feasible —for example, with Clojure protocols— and\nusing dependency injection tends to produce more pure functions, which are\neasier to test. Spies What if you want to check whether a function has been called, and with which\nparameters? Well, in that case you should first realise you are testing a side effect, and\ndecide whether you really need it or you can somehow avoid it. Using the previous example, and using midje: ( facts \"about `crappy-function`\" ( fact \"returns whatever some-namespace/some-function returns\" ( crappy-function 3 5 ) => \"I am really crappy\" ( provided ( some-namespace/some-function { :a-key 3 :another-key 5 }) => \"I am really crappy\" :times 1 ))) Note the different provided statement, together with the number of calls we expect. You can also spy using clojure.test , using atoms to store the calls to a mock function: ( deftest crappy-function ( let [ calls ( atom [])] ( with-redefs [ some-namespace/some-function ( fn [ arg ] ( swap! calls conj arg ) \"I am really crappy\" )] ( is ( = \"I am really crappy\" ( crappy-function mock-function 3 5 ))) ( is ( = @ calls [{ :a-key 3 :another-key 5 }]))))) So, this is more or less my compendium of lessons learned during my first six\nmonths with Clojure. Many things are probably wrong, or I will change my mind\nin a few more months. But in the meantime, I hope these ideas are useful to\nsomeone else!", "date": "2016-01-11,"},
{"website": "Funding-Circle", "title": "My First Week at Funding Circle", "author": ["Rose Molina Atienza"], "link": "https://engineering.fundingcircle.com/blog/2018/05/02/my-first-week/", "abstract": "Starting at a new company is an intense period; one that, for most of us, goes by in a blur of introductions and meetings, interleaved with some moments of calm. Looking back at my first week all I can say is: it was intense, fun and well worth it. Monday Stepping through the door of my new workplace was a personally significant moment. This was the next step in my, for the moment, short career: I was nervous. I was glad to discover that I was not the only new joiner that day. It surprised me to realise everyone who started was in a different department. Previously all of my other interactions with work colleagues had been in engineering and it felt refreshing to meet people with a different kind of expertise. After the HR introduction, I was shown to my desk by Jamie, my new engineering manager. This is where I met my team. When I was thrown into my first meeting with my team and our product owner I was unable to follow exactly everything we were discussing; however, some of the technical terms were already familiar. Before starting, Anna, an engineering manager in London, had recommended I <3 logs and Making sense of stream processing . Both books had been written by developers heavily involved in the development of Kafka, Jay Krepps and Martin Kleppmann , and provided me with the high-level ideas that I needed to understand our approach. Technical details aside, what I did also see was how involved our product owner was and how my team was able to communicate their concerns. Once the meeting was over, the whole team got treated to lunch, and I had the chance to meet them a bit better. After lunch, I had some quiet time to sit down at my desk, checkout out the slack channels, sift through my email and gather my thoughts. Our team does one week sprints and Mondays are retro and planning. Retrospectives provide good insight into a team’s culture, and ours was a very considerate retro. To start with, during the first five minutes everyone writes different cards of what they would like the team to start, to stop and to continue doing. The fact that this is done at the beginning and in a quiet environment means there is some actual time for reflection and that everyone’s voice will be heard. After some planning which also went over my head, I had some rest to plan my day tomorrow and sharply at 6 everyone promptly left after a good day’s work. Tuesday Tuesday started strong with a presentation from Yoav Sharon, Global Head of Investor Product, about the life of a loan. Funding Circle’s business is providing a marketplace to small and medium-sized businesses where they can borrow money from investors, a domain of which I knew something but not too much. This session proved incredibly useful as an introduction to our business, our product and it also introduced the project that my team had been working on: a migration of our backend services. With my head spinning with information I decided to take a break and meet my company buddy for coffee; we had a relaxing conversation and agreed to have lunch on Thursday. Time to start learning Clojure! I had been given a long list of technical topics to cover by my mentor and I was excited to learn more about Clojure. The choice of language had been one of the main drivers for me to join Funding Circle. Luckily I had already covered parts of the introductory book recommended to me: Clojure for the Brave and True and decided that practice was the best way so I set up to solve some problems. In the afternoon there was a brief meeting in the funderbar (This amazing bar where you can get really good and cheap food during lunchtime - definitely a great perk!) to introduce the newbies and catch up on what other departments had achieved. I had to step in front of the whole room and let my company buddy introduce me (oof, wipe sweat from forehead - I am glad I did not have to introduce myself.) Before leaving I had a discussion with my mentor to discuss how I had solved the problems in Clojure and by then it was time to return home. Wednesday I am happily ploughing through some Clojure problems in the morning, ahh the peace of it. I choose to cover the problems in 4Clojure , a great way to practice. At mid-morning the technical lead in our team (who is not our engineering manager - great separation of concerns!) introduced me and a few fellow engineers to the current architecture of our system. Our technical lead’s exposition was clear: we covered the design, the principles behind the design, how we would be deploying the code and obtain information from the legacy system when we go live. This was not an information dump on us but rather a backwards and forwards of questions and answers. Hence, by the end of the meeting, I had a decent grasp of how we were doing things. \nAfter lunch, I had a catch up with our product owner who gave me an overview of how the different tech teams mapped onto our product. By that time I already had a decent picture but it is good to see a more product-centric perspective. I spent the end of the day pairing a few problems with one of my teammates, who enlightened me on the efficient use of the Clojure core library. Thursday By far the quieter day of the week, which I very much appreciated: it involved more learning about Clojure, setting up GitHub and other necessities, lunch with my company buddy and my first refinement session with the team. Friday By the end of the week, I had started joining my team during their stand-ups and providing updates on my learning. Our scrum rituals don’t take more than 2 hours a week in total - one for planning and retro, one for refinement and our 10 min stand-ups. This leaves plenty of time to focus on the coding side of the job. After our morning stand-up, I had my first one to one with my manager. His main advice was to pace myself and restating that no one expects me to be pushing code just yet. We discussed my learning plan, which things should I set up (Artifactory, Docker, GitHub, etc …) and I enquired about what his responsibilities as a manager were.  To me, the fact that the responsibilities of product owner, engineering manager and tech lead all fall on separate people is a welcomed change and one that helps teams be more efficient. The end of Friday was a special one, my arrival at the company couldn’t have been better timed: we had our quarterly meeting. This is a meeting where the whole company, across all of our different locations (San Francisco, London, and Berlin) gathers to get an update on the company’s latest achievements and what our future goals are.  The meeting had a particular feel to it: there was showmanship, bad-puns, and an artfully displayed metaphor comparing FC to a garden that explains all the bright bees in the presentation slides. In general, it felt earnest and endearing; people were honestly trying hard to keep the audience engaged: there was even a bee onesie on stage at one point. Overall it was insightful if a bit too long. The perfect cure for a long session of presentations is some relaxation, so I was really looking forward to my first company bonding event at Flight Club. On my way there, following where everybody was going, I got to have a good informal conversation with my mentor, trying to absorb as much wisdom as I could. Once at the place, I had the chance to grab a cold beer and mingle. Mingle I did: I met some more engineers, people from sales and I had the chance to have a chat with Anna and thank her for her tips to help me prepare before I started. Thanks to my team, to Eduardo Caselles and to Sasha Gerrand for their help in reviewing this post", "date": "2018-05-2,"},
{"website": "Funding-Circle", "title": "Getting to know our Engineers at Funding Circle: Q&A with Sagar Patil", "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/09/11/fc-engineers-sagar-qa/", "abstract": "This is part of a regular series where we catch up with our engineers to discover more about what\nlife is like in the Engineering team here at Funding Circle! Tell us about yourself! What is your name and role here at Funding Circle? My name is Sagar Patil. I work as a Senior Software Engineer in the Retail Investor team. Our \nteam’s work includes improving the Retail Investor experience as well as working on the \nalgorithm that helps match borrowers with investors to help fund their loans. How long have you been with Funding Circle? Close to 4 months now. It’s been a breeze so far. Why did you choose Funding Circle for your next role? At my previous company, I got a taste of the kind of problems that FinTechs all over the world \nare trying to solve. I loved the fact that even though you feel like you’re developing tiny \nfeatures, they had massive impact on the lives of our customers. When I felt like it was time to \nmove on (from my previous company), I wanted to work for a FinTech where there were technically \nchallenging problems but also make a difference to the community. As my luck would have it, \none of the talent partners from FundingCircle (a huge shout out to Jasmin) reached out and through \na quick interview process, I ended up here. How do you start your day? We have dashboards with metrics set up to monitor critical parts of the funding process. A quick \nlook to see everything’s A-okay and then a brief team standup to sync-up. A cup of \nginger tea (or hot chocolate) and I’m ready to go… What do you enjoy most about Funding Circle? I really like the business problem that we’re trying to solve at Funding Circle. It ticks both \ninnovation and impact that one might look for in a place of work. I get to work in one of the \ncoolest parts of the business but I think I enjoy working with my colleagues here the most. \nThey’re extremely smart, super friendly and overall, an ace bunch. What are the main challenges you face? As a new member of the team, ramping up on the domain is slightly challenging. Other than that, \noften times there are a hundred different ideas, all great, on what can be done to improve our \nsystems but I personally feel that it is challenging to decide what would be most useful to our \ncustomers. This issue is exacerbated as we are growing fast and across multiple geographies and \nwe want to provide the best possible customer experience. Talk through some of the projects you are working on? Currently, we are making improvements to the way the funding algorithm works so as to improve \nfairness in matching loans to investors. At the same time, we’re also working quite laboriously \nto improve the cash out process for our investors. What advice would you give to someone who may be interviewing with an Engineering team at Funding Circle? The idea of an interview is not just to test if you have the technical skills to be able to solve \nproblems, but also to be able to collaborate with the interviewers and see if we can reach a good \nenough solution by the end of the interview. Articulate your assumptions, ask questions if you \ndon’t understand anything and try to explain your approach to the interviewers along the way and \nanswer any questions they may have along the way in as simply as you can. Sometimes, an \ninterviewer’s question is an edge case that you may not have thought about ;) One major mistake I have made in the past was of not applying to some job because I thought I might \nnot be smart enough to clear the interview. If there are others who feel similarly while looking \nat a job posting, I would say go for it! If there’s any advice I would give to my younger self \nor any of you folks, this would be it! Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2019-09-11,"},
{"website": "Funding-Circle", "title": "Tech Leadership Event at Funding Circle", "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2018/12/20/3d-techleadershipevent/", "abstract": "We hosted a Technology Leadership Event on 12th December 2018. \nThe idea behind the evening was to bring together leaders within the\nworld of software engineering to network and share knowledge around\ninnovation and the challenges that technology managers face. Here is a summary of the talks that we heard during the evening: We started with Rija Javed, CTO of Marketinvoice , who spoke to us\nabout the importance of staying technical even as a manager. She\nhighlighted a challenge that all managers face that as your role\nbecomes bigger, you need to take a step away from doing the day to\n day work, and trust your team to deliver at the same standard you would.\nYet whilst stepping away, you also need to ensure that you understand\nthe innovations in the technical world, and can jump in when needed.\nClearly a difficult balance for anyone to get right, and Rija\nsuggested resources such as using blogs, and online reading as a way\nof keeping up to date. After some food and drinks, I talked about the innovation management \nthat FC have taken on their journey. I explained that 3 years ago FC \nneeded to design a new platform that can track financial information, \nbe future-proof and scalable, can store vast amounts of data and \nrequires low maintenance - the dream! After researching the market, \nwe decided to go with a bleeding edge technology stack made up of \nKafka’s streaming platform (to be used as an immutable system of record) \nand the functional programming language, Clojure. \nFC were pioneering something new, which required novel approaches to configuration, \nadditional tooling requirements and security protocols. This tech stack is now fully built and\nin production and meets our expectation. I went on to talk about the advantages \n of being an early adopter. It enables inventing\ntools where there are none, to give back to the\ncommunity through open source, knowledge sharing and\nlastly, it creates an incredibly exciting environment for attracting\ntop engineering talent. Our third speaker, Jamie Smith, Engineering Manager at Funding Circle,\ncontinued the theme of how we can uncover the best engineering talent\nwhen the skills you need just aren’t there in the traditional market.\nHe focused on how to drive your hiring process by the values and\ntraits you need rather than a purely tech-focused approach. The key\nvalues that we use at FC are: How do they work? We use an extended pair-programming session looking\nat their problem-solving and collaboration skills. Are they open, honest and transparent? Would they think smart and be curious about finding non-obvious solutions? Would they help us make things happen?  Do they show grit and purpose\nin what they do? Will they stand together with their team, showing emotional\nintelligence and empathy? Are they passionate?  Is this their craft? Jamie also talked about what Funding Circle have done to help retain\nthese engineers once they get through the first exciting months where\neverything is new. He showed that the key to what works at Funding\nCircle is a continual focus on growth - both creating a space for\npersonal growth and generating momentum for it to happen.  Using 1:1s\nas a key area he talked through an approach that encompassed: Use Feedback: make it actionable and specific based on a clear framework Being explicit about “experimenting”: create definitive space for a\n“Test -> Commit or Rollback” cycle Set High Standards: help engineers aim high and give support to help\nthem experiment, fail and learn to make substantial progress over\ntime. Lastly, Jamie talked through the most important aspect of this as\nEngineering Leaders is to demonstrate these ideas ourselves to create\nthe right atmosphere for others to operate in.  When we, as leaders,\ndemonstrate these behaviours we create the right cultural clues for\nothers to start to emulate.", "date": "2018-12-20,"},
{"website": "Funding-Circle", "title": "What is BDD?", "author": ["Azher Hussain"], "link": "https://engineering.fundingcircle.com/blog/2015/04/09/what-is-bdd/", "abstract": "I was introduced to Behaviour-Driven Development (BDD) about 5 years ago, about 2 years or so after I became a Quality Assurance engineer. BDD as a movement has been around for more than a decade and it has evolved a lot in the past 5 years or so. In all the organisations I have worked at, including Funding Circle, I have seen recurring problems in adopting BDD principles.  A lot of people still think BDD is a tool and is only about testing. For a long time I was one of those people too. I believe BDD is still poorly understood and frequently misappropriated. Recently, the QA team and I went to the CukeUp conference here in London. Over the years,the CukeUp conference has usually been based around automated test and QA best practices, but this time there was a big push towards BDD with many people sharing best practices, new practices and experiences of where BDD has gone wrong and right. So what did I take from the conference? Well I asked myself. BDD needs a description that makes sense. What’s in it for our Engineering and Product teams? Why should we adopt and try out BDD? This is what I came up with based on my renewed understanding since the conference: BDD is a set of principles and practices that enables technical teams to produce more valuable software with fewer bugs. Those who practice BDD should explore, discover, define, and then drive out the desired behaviour of software using conversations, examples and automated tests. BDD can lead to reduced development and maintenance costs. Let me try and break these high level definitions down to something with a bit more context. Deliberate Discovery This is about learning early to fail less. In traditional development practices, learning is amplified when functionality is tested or put in production in front of users. Useful feedback is gained but it usually requires the functionality to be built before any feedback is given. This is usually costly and causes delays in a team’s learning. Then there’s the question: “what if the developers built the functionality wrong? Can we learn more before the defective code is written? Deliberate Discovery and Exploration are principles that tell us to try to learn more before writing the software. As with any principles, the practice to support this is contextual. The practice that I have seen work well in many contexts is to carry out Discovery Workshops or, as many of us call them, Grooming Sessions. In my view of BDD, Discovery Workshops are an effective practice that I have seen work well on projects here at Funding Circle. Involving non-technical stakeholders as well as developers and QA engineers is essential to for efficiency. Talk About Examples One of the things that come out of Grooming sessions is examples . These are business related (real world) examples of how the functionality should or could work, if it existed. The purpose of examples is to trigger discovery through conversations, but also to define how the functionality should behave. Examples should show general constraints, rule and acceptance criteria. They do not have to be written in a particular form. Some teams write examples using Gherkin syntax (Given, As, When, Then, So), but this does not have to be the case. Sometimes starting using plain English is easier to have a conversation around. At the conference this was best demonstrated using a BDD process by Matt Wynne (Co-founder of Cucumber ) called “Example Mapping” . Example mapping doesn’t demonstrate  anything that we are not practising already at Funding Circle, but it gives structure and organisation to the way we practice BDD which is what makes it awesome. We have already started the process to implement Example Mapping in an existing project. Test-Driven Development BDD started out as TDD , replacing the word \"test” with “should” to help people understand that TDD is not about testing, but about design and reflection. When it’s time to write code, I believe BDD is no different from TDD: Write a failing test. Watch it fail. Write enough code to make it pass. Clean up your mess (refactor). So how does BDD differ from TDD? In a couple of areas: BDD gives you a well-defined starting point: The examples created by the deliberate discovery process. Tests that result from BDD are not unit tests, but high level tests that interact with the system at a much more granular level. I don’t consider tools like RSpec to be a BDD tool. I think of it more as a unit testing tool that is great for TDD in the spirit of BDD. A tool like RSpec (fantastic as it is) is usually used at an abstraction level below the level that the non-technical stakeholders care about, and stakeholder involvement is key to BDD in my view.", "date": "2015-04-9,"},
{"website": "Funding-Circle", "title": "A year on from CodeCraft", "author": ["Shaban Karumba"], "link": "https://engineering.fundingcircle.com/blog/2015/03/16/a-year-on-from-codecraft/", "abstract": "It’s been a year now since I joined Funding Circle as one of 8 CodeCrafters who spent three months being trained in website development and becoming a fully fledged software engineer. I was one of the three people who were hired by Funding Circle and here is a brief overview of my experience since. CodeCraft was organised and run extremely well; each week we knew exactly what we were doing. The first few weeks were classroom-based, completing simple challenges, lessons/presentations from other engineers and weekly tests to reinforce what we had learnt. We also slowly started to take on some real work, such as changing some styling or fixing a bug. We started off just a day a week, but then ended up with us completing a whole week working within a team, which gave us a chance to work with the engineers at Funding Circle. I found working with the team one of the best things about CodeCraft as it really gives you a chance to see how real software engineers work, which includes everything from daily stand-ups, planning meetings, retrospectives and demonstrations to other areas of the business to showcase our work. This was a real eye opener, as being an engineer is not just about coding all day long! Our time working with the developers not only prepared us for working within an engineering team but also gave us the opportunity to see if it’s something we would enjoy doing on a day to day basis. After 10 months of working as a software engineer I’m sometimes amazed about how far I’ve come in terms of what I’m capable of doing. I started off in what is called the “borrower team” that deals with all things related to our borrowers on the platform and we completely changed the flow of how a borrower signs up to the site. This project involved designing various API endpoints which were then consumed by an single page application in AngularJS . Building an API is great because it enables the same code base to be used by any external client such as a mobile application or even a third party company. Since completing this I have been moved to another team migrating existing functionality into a micro-service part of a service oriented architecture (SOA) . As you can imagine it’s been an interesting, exciting and challenging journey, with many learnings. Pairing with other engineers has really helped, as I’ve been able to learn from other’s experiences, perspectives, and different ways of problem-solving plus occasionally adding my new point of view to various problems helping the team solve things. I can’t recommend CodeCraft enough for when we do it again, as you won’t regret the decision at all. It will open up many avenues for you in terms of the experience you’ll gain, and there are not many programmes like it which allow you to both learn and also work side by side with an engineering team. Funding Circle as a whole is a great place for you to learn and develop, although one of the main attractions for me has been the people I work with everyday, striving to achieve more to make it an even better place to work .", "date": "2015-03-16,"},
{"website": "Funding-Circle", "title": "Getting to know our Engineers at Funding Circle: Q&A with Panos Stravopodis", "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/07/05/fc-engineers-panos-qa/", "abstract": "This is part of a regular series where we catch up with our engineers to discover more about what\nlife is like in the Engineering team here at Funding Circle! Tell us about yourself! What is your name and role here at Funding Circle? My name is Panos Stravopodis. I’m a Principal Engineer for the Platform Team. How long have you been with us and what do you enjoy the most? I have been with Funding Circle just over a year. I like that Engineering is a big team; everyone\nis so accessible which makes cross-domain work a breeze. I am also lucky to be part of one of the\ncoolest teams (go platform team go!). How do you start your day? Coffee and team stand-up! It’s important to be in-sync with the team and be able to help with\nany blockers! What are the main challenges you face? We are growing fast and we need to make sure that everything we build is future-proof,\ndelivering always the best possible experience to the end user. We operate across a range of\ngeographies and it’s imperative to have consistency in our platform, avoiding at any cost\nintroducing downtime or deteriorating the user experience. What are some of the projects you are working on? We are migrating from our existing workload orchestrator to Kubernetes. This transparent transition\nenables us to improve all aspects of our platform investing heavily in automation while\nstandardising even more the way we deliver software. Another interesting project we are\nimplementing is around observability and distributed tracing. What advice would you give to someone who may be interviewing with an Engineering team at Funding Circle? First of all to relax! We love chatting to fellow engineers and the purpose of the interview is to\nhave an in-depth, honest and friendly discussion! Try to articulate your thoughts, have in mind our\nbusiness domain, and scope your answers to the purpose of the interview - we can issue a paper\nlater on together if you want more detail. :) Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2019-07-5,"},
{"website": "Funding-Circle", "title": "How to Set Up a Service Oriented Architecture for Development", "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/02/23/how-to-set-up-a-service-oriented-architecture-for-development/", "abstract": "Service oriented architectures can have many benefits, but there are trade-offs as well. One\nimmediate annoyance is setting up multiple applications to do local development. The current workflow looks like this: cd app1 $ RACK_ENV = development rackup -p 3002 # open a new tab $ cd app2 $ RACK_ENV = development rackup -p 4000 # open a new tab $ cd app3 $ RACK_ENV = development rackup -p 5000 # etc. It’s also convenient to label the tabs to know where to look for an application’s output, so we\nneed to name each tab as well. This is boring and error prone as it’s very easy to forget to start\nan application that could prevent the entire system from working as expected. Consular to the Rescue! We just recently found out about Consular to automate terminal tasks and turn the above headache\ninto a one command solution. Setup is simple: gem install consular $ gem install consular-iterm $ rbenv rehash # if appropriate $ consular init This generates ~/.consularc for customization, and ~/.config/consular for projects. If you’re\nusing iTerm , you’ll need to require the necessary core. # ~/.consularc require 'consular/iterm' # the rest of the file Now generate a project: $ consular edit soa-setup Here is an example project that replaces the current workflow: # ~/.config/consular/soa-setup.term tab 'app1' do run 'cd ~/workspace/app1' run 'RACK_ENV=development rackup -p 3002' end tab 'app2' do run 'cd ~/workspace/app1' run 'RACK_ENV=development rackup -p 4000' end tab 'app3' do run 'cd ~/workspace/app3' run 'RACK_ENV=development rackup -p 5000' end Now all you need to do is run one command, consular start soa-setup . Some Tweaks It’s nice to not have to do this (minimal) song and dance for each development machine, so we\npackaged this up into some scripts to make the process easier. # bin/setup echo \"Installing Consular...\" bundle install\nrbenv rehash\nconsular init # prepend require to file echo \"require 'consular/iterm' \\n \" > tmpfile cat ~/.consularc >> tmpfile\ncp tmpfile ~/.consularc\nrm tmpfile # copy project(s) to home directory to use anywhere cp lib/default.term ~/.config/consular/ echo \" \\n Run bin/run to start all applications.\" # bin/run consular start default # lib/default.term tab 'app1' do run 'cd ~/workspace/app1' run 'RACK_ENV=development rackup -p 3002' end tab 'app2' do run 'cd ~/workspace/app2' run 'RACK_ENV=development rackup -p 4000' end tab 'app3' do run 'cd ~/workspace/app3' run 'RACK_ENV=development rackup -p 5000' end # Gemfile source 'https://rubygems.org' gem 'consular' gem 'consular-iterm' Finally, you can create a symlink to the run command: ln -s ~/path/to/bin/run ~/launch-apps Go forth and be lazy, I mean, virtuous !", "date": "2015-02-23,"},
{"website": "Funding-Circle", "title": "ClojureCircle at Funding Circle", "author": ["Anna Keren"], "link": "https://engineering.fundingcircle.com/blog/2019/03/04/4th-clojure-circle/", "abstract": "Funding Circle hosted its 4th ClojureCircle meet-up on 7th March 2019. \nDespite cold, rainy and windy weather on the day, \nmany Clojure community engineers came to the event to listen to the talks, \nsocialise and exchange their experiences. Our host, Sasha Gerrand (Principal Engineer at Funding Circle), kicked the\nmeetup off by welcoming attendees and presenting the speakers. The first speaker was Avi Flax (Principal Engineer at Funding Circle). Avi introduced the FC4 Framework, a\nsoftware package that allows software engineers to create, publish, and maintain\nsoftware architecture diagrams (the original announcement post for it is still\navailable ).\nThe FC4 Framework is based on Simon Brown’s C4 model .\nAvi shared his passion for clear, precise and rich diagrams as a tool to express\narchitectural ideas. Keigo Suzukawa (Software Engineer at Funding Circle), presented straight after\nAvi. He continued the broad theme of improving the effectiveness of software\nengineers and described how a sluggishly written codebase creates apathy with\nthe engineers who use it. He brought a few design analogies from everyday life\nand also examples in software engineering as well. He also encouraged the\ncommunity to think about the next developer who is going to use your codebase as\na customer and care about her/his experience and productivity. We then had a short break for some pizza, drinks and socialising. Following this break, we turned our\nattention to Mathieu Gathron, an independent tech consultant.  Mathieu focused\nhis presentation on how to improve reporting capabilities of peer2peer lenders\nusing Spark Dataframes. He demonstrated using an example peer2peer provider how\nto extract data from the files it sends to its customers, by processing them\neffectively and producing a result that will satisfy reporting requirements for\nend of tax year submission. Using a distributed data collection which is\npersisted into named columns, one can perform operations on the data like\nfilter, aggregate, and count, to produce desired reports. We finished the evening by discussing the talks, exchanging ideas and networking. We hope to host the next ClojureCircle event in a few months. If you would be\ninterested in speaking at a future event then please reach out to Sasha Gerrand or\nJamie Smith via engineering-talks@fundingcircle.com . Follow Funding Circle Engineering on Twitter. Spread the word! A recording of the meetup is available on\nYouTube . The talk slides are below Mathieu Gathron – How to use Clojure with the Spark Dataframe API to analyse a Peer2Peer lending transaction history \nto generate an accounting report Keigo Suzukawa – Death by a Thousand Cuts Avi Flax – Growing a Clojure CLI Tool", "date": "2019-03-4,"},
{"website": "Funding-Circle", "title": "Kafka Streams, the Clojure way", "author": ["Dave Martin "], "link": "https://engineering.fundingcircle.com/blog/2019/08/27/kafka-streams-the-clojure-way/", "abstract": "(Cross post from Clojure Conundrums ) In this blog post, I’ll walk you through how to create a Kafka Streams application in an idiomatic Clojure style. I won’t assume any knowledge of Kafka or Kafka Streams, but if you’ve never heard of them before this post may be a bit overwhelming - I’d check out Confluent’s introduction to Kafka Streams , and also the Kafka Streams docs . Kafka can be thought of as a persistent, highly scalable, distributed message queue. Kafka stores all messages in “topics”, which can be produced to and consumed from. Kafka Streams is an abstraction on top of Kafka, which treats topics as a reactive stream of data onto which you can apply transformations ( map , filter , etc.). It also gives you a way to perform stateful aggregations, in such a way that your application can be safely restarted. The Java API for Kafka Streams is very powerful, but has a few drawbacks in terms of code re-use and flexibility. It gives you a StreamsBuilder object, on which you call methods like .map(...) and .filter(...) . You use this builder object to build a Topology , a logical representation of your application as a graph of processing nodes. You then use the Topology to initialise a KafkaStreams object, which executes the topology’s logic. We’ll develop an application in this style using the Jackdaw library, a Clojure library for Kafka, and then evolve it into a more idiomatic, data-driven style. All the code I’m going to show you is in this walkthrough repo , so clone it now if you’d like to follow along. A Simple Example Let’s imagine that you work at Fidget-no-more Incorporated, the world’s largest purveyor of fidget spinners (or whatever the kids are buying nowadays). Your website currently records every purchase in a DB, but other departments are finding it difficult to write applications that react when a purchase is made. The sales team would like their apps to somehow be notified when a large purchase is made, so they can send the user a personalised thank you email. This sounds like the perfect job for Kafka! Here’s what we’ll do: On every purchase, produce a message to the purchase-made Kafka topic. Create a Kafka Streams app that: Reads from this topic Filters for large purchases (above £100) Removes any extraneous fields from each message (the sales team only need a user id and an amount) Writes to the large-transaction-made topic Before we start, we need to start up a Kafka “broker”, a server running Kafka. The easiest way to do this is to spin up a landoop/fast-data-dev Docker container. You can do this by running the following command (you’ll need Docker installed): # On Linux docker run --rm --net = host landoop/fast-data-dev # On Mac docker run --rm -p 2181:2181 -p 3030:3030 -p 8081-8083:8081-8083 -p 9581-9585:9581-9585 -p 9092:9092 -e ADV_HOST = localhost landoop/fast-data-dev:latest We’ll start by creating the purchase-made and large-transaction-made topics. Clone the walkthrough repo if you haven’t already, and start up your repl. Navigate to the kafka-streams-the-clojure-way.core namespace, and run these commands: ;; create the \"purchase-made\" and \"large-transaction-made\" topics ( ja/create-topics! admin-client [ purchase-made-topic large-transaction-made-topic ]) By the way - all the commands we’re going to run are in the comment block at the bottom of the kafka-streams-the-clojure-way.core namespace. We now need to produce some dummy messages onto the purchase-made topic. These messages are going to look like this: { :id 1 :user-id 1234 :amount 20 :quantity 5 } We’ve already got a make-purchase! function defined for us in the kafka-streams-the-clojure-way.core namespace, so just run these commands: ;; Make a few dummy purchases ( make-purchase! 10 ) ( make-purchase! 500 ) ( make-purchase! 50 ) ( make-purchase! 1000 ) If you’re interested in how make-purchase! works, check out the code in the walkthrough repo. It uses the Kafka (not Kafka Streams ) API, so I’ll just gloss over it here. You should see in the fast-data-dev UI that messages appear in the topic. Now we have a topic with some dummy data in it, we can start writing our Kafka Streams topology. This is already defined in the walkthrough repo, here’s the code: ( defn simple-topology [ builder ] ( -> ;; Read the purchase-made topic into a KStream ( js/kstream builder purchase-made-topic ) ;; Filter the KStream for purchases greater than £100 ( js/filter ( fn [[ _ purchase ]] ( <= 100 ( :amount purchase )))) ;; Remove all but the :amount and :user-id fields from the message. ;; Note that the function passed to map takes and returns a tuple of [key value]. ( js/map ( fn [[ key purchase ]] [ key ( select-keys purchase [ :amount :user-id ])])) ;; Write our KStream to the large-transaction-made topic ( js/to large-transaction-made-topic ))) ( defn start! [] \"Starts the simple topology\" ( let [ builder ( js/streams-builder )] ( simple-topology builder ) ( doto ( js/kafka-streams builder kafka-config ) ( js/start )))) ( defn stop! [ kafka-streams-app ] \"Stops the given KafkaStreams application\" ( js/close kafka-streams-app )) Run (def app (start!)) in the repl to start the topology. You should shortly see messages appear on the large-transaction-made topic. Magic! Try producing some more messages to the purchase-made topic, and your topology should pick them up and process them immediately. When you’re ready to move on, run (stop! app) to stop the topology. Introducing Transducers We’ve made a good start, but there are a few problems with our code at the moment. One is that our code is not as easy to test as it could be. We can’t directly test the logic of our topology, because it’s tied to the Kafka Streams API. You can use Kafka’s TopologyTestDriver to run your topology in memory, but this can be quite cumbersome. It would be much easier if we could express our topology’s logic as pure functions, and then test them. Secondly, our code isn’t very composable. If you have 2 separate topologies, there is no easy way of merging them together. Thirdly, the code is tied to a specific context - Kafka Streams. If you want to re-use the same logic for transforming, for example, a core.async channel you’d be forced to re-write it. To alleviate these problems, we’re going to use transducers . Transducers are “a powerful and composable way to build algorithmic transformations that you can reuse in many contexts”. Basically, they allow you to encapsulate the logic of transforming a stream of data, without specific knowledge of what the stream is . The stream could be a seq, a core.async channel, or a Kafka topic. If you’re not familiar with them already, you may want to check out this introductory blog post . Let’s write a transducer that captures the logic of our topology: ( def purchase-made-transducer ( comp ;; Note that each step takes a [key value] tuple ( filter ( fn [[ _ purchase ]] ( <= 100 ( :amount purchase )))) ( map ( fn [[ key purchase ]] [ key ( select-keys purchase [ :amount :user-id ])])))) We can easily test the transducer in the repl like this: ( into [] purchase-made-transducer [[ 1 { :purchase-id 1 :user-id 2 :amount 10 :quantity 1 }] [ 3 { :purchase-id 3 :user-id 4 :amount 500 :quantity 100 }]]) We’ve successfully isolated the logic of our topology as a pure function. This enables us to easily test our topology’s logic. Now we’ll update our topology to use this transducer. We’re going to use the transduce-stream function here - you don’t need to know exactly how it works, only that it applies a transducer to a KStream . ( defn build-topology-with-transducer [ builder ] ( -> ( js/kstream builder purchase-made-topic ) ( transduce-stream purchase-made-transducer ) ( js/to large-transaction-made-topic ))) Great, this seems to have made our code more testable, flexible, and composable. A more complicated Example Unfortunately, your boss now comes to you with another requirement (as they always do). Your company has just launched a new way of buying fidget spinners - The Humble Spinner Bundle™. Your customers pay whatever they think is fair for a bundle of 10 (ten!) fidget spinners. This is on a completely separate site, but luckily the team in charge of building it have created a Kafka topic for you, onto which they’ll publish all the purchases of the humble bundle - humble-donation-made . The sales team would like to send a congratulatory email to customers who pay a large amount for the bundle, in the same way as they do for regular purchases. Therefore, you’ve been tasked with consuming this topic in your application. Unfortunately, the team building the humble-donation-made topic have come up with a message schema that’s slightly different to purchase-made . humble-donation-made messages look like this: { :user-id 1234 :donation-amount-cents 1000 ;; £10 :donation-date \"2019-01-02\" } We’ll need to create a KStream from the humble-donation-made topic, apply a transducer to it, then use Jackdaw’s merge function to merge it with the KStream from our previous topology. Let’s start with the transducer for the humble-donation-made topic: ( def humble-donation-made-transducer ( comp ;; Again, each step takes a [key value] tuple ( filter ( fn [[ _ donation ]] ( <= 10000 ( :donation-amount-cents donation )))) ( map ( fn [[ key donation ]] [ key { :user-id ( :user-id donation ) :amount ( int ( / ( :donation-amount-cents donation ) 100 ))}])))) Now for the topology itself, this is what the code looks like: ( defn more-complicated-topology [ builder ] ( js/merge ( -> ( js/kstream builder purchase-made-topic ) ( transduce-stream purchase-made-transducer )) ( -> ( js/kstream builder humble-donation-made-transducer ) ( transduce-stream humble-donation-made-transducer )))) This will work, but we’ve re-introduced all the problems we had before we started using transducers. Our code is now less testable, less composable, and less portable. As Clojure programmers, we know exactly what to do when we’re confronted with this problem - express everything as data! If only there were a library that could help us out… Introducing Willa Willa is just such a library. It allows you to express your topology as data and functions, rather than using the mutable StreamsBuilder API. Full disclosure - I’m the author of Willa, so if you’re looking for an unbiased critique of it, I’d just stop reading now. Let’s see how it would affect our code. We’ll start by defining all our topics and KStream s. In Willa, these are called \"entities”. We first need to construct a map of entity id to entity config, like so: ( def entities ;; We'll define our topic entities first ;; For the values, we just need to add the ::w/entity-type to our existing topic configs { :topic/purchase-made ( assoc purchase-made-topic ::w/entity-type :topic ) :topic/humble-donation-made ( assoc humble-donation-made-topic ::w/entity-type :topic ) :topic/large-transaction-made ( assoc large-transaction-made-topic ::w/entity-type :topic ) ;; We now define our KStreams ;; This is where we define the transducers we apply to the KStream, as the ::w/xform key :stream/large-purchase-made { ::w/entity-type :kstream ::w/xform purchase-made-transducer } :stream/large-donation-made { ::w/entity-type :kstream ::w/xform humble-donation-made-transducer }}) So far so good. We now need to define how our topics and streams relate to each other - how the data flows through our topology. Willa models a topology as a graph (a DAG) of entities. You express this as a vector of tuples. Each tuple has 2 elements, in the format [:entity-id-from :entity-id-to] , and represents a directed edge on the topology graph. Willa calls this a “workflow”. Our workflow looks like this: ( def workflow [[ :topic/purchase-made :stream/large-purchase-made ] [ :topic/humble-donation-made :stream/large-donation-made ] ;; When there are multiple edges pointing to the same node, Willa will merge the inputs by default [ :stream/large-purchase-made :topic/large-transaction-made ] [ :stream/large-donation-made :topic/large-transaction-made ]]) Putting it together: ( def topology { :workflow workflow :entities entities }) That’s all we need to do to completely specify our topology. One nice advantage of doing it this way is that we can now visualise our topology. Run this command in the repl: ( wv/view-topology topology ) You should see a diagram like this: Nice! To get it running, we just need a few lines of code to compile Willa’s representation of our topology into an actual Kafka Streams topology: ;; Create the humble-purchase-made topic ( ja/create-topics! admin-client [ humble-donation-made-topic ]) ;; Start the topology ( let [ builder ( js/streams-builder )] ( w/build-topology! builder topology ) ( js/start ( js/kafka-streams builder kafka-config ))) ;; Publish a couple of messages to the input topics ( make-purchase! 200 ) ( make-humble-donation! 15000 ) You should now see some more messages appear in the large-transaction-made topic . So now we’ve completely specified our topology as data structures and transducers. What does that give us, other than being able to brag to other developers about how decomplected our code is? (Note - please don’t do that). One advantage is that, as we saw earlier, we’re able to test each transducer in isolation without knowing anything about Kafka Streams. We can also see how data flows through our topology, without interacting with Kafka at all. Willa calls this an “experiment”. Run this in the repl: ;; Run an experiment ( def experiment ( we/run-experiment topology { :topic/purchase-made [{ :key 1 :value { :id 1 :amount 200 :user-id 1234 :quantity 100 }}] :topic/humble-donation-made [{ :key 2 :value { :user-id 2345 :donation-amount-cents 15000 :donation-date \"2019-01-02\" }}]})) ;; Visualise experiment result ( wv/view-topology experiment ) ;; View results as data ( we/results-only experiment ) The experiment results should look like this: You can also verify that the topology is valid in the repl, using clojure.spec . You can do this by running: ;; Should print \"Success!!\" ( s/explain ::ws/topology topology ) ;; What happens with an invalid topology? ( s/explain ::ws/topology ;; introduce a loop in our topology, which is not allowed ( update topology :workflow conj [ :topic/large-transaction-made :topic/purchase-made ])) The real advantage of Willa is that it relies on standard Clojure data structures as much as possible. This allows you to do things like serialise your topology as EDN, programatically manipulate it, and also query it - for example, try writing a function to count the number of topics involved in a topology. Summary I hope that’s given you a brief overview of how to write a basic Kafka Streams topology. If you’re interested in learning more I’d recommend reading Kafka 101 and Designing Event Driven Systems . I’d also recommend this Kafka Streams Udemy course . We’ve barely scratched the surface of Kafka Streams, there are many more concepts to learn (😓). We haven’t even touched on things like KTable s, aggregations, or joins. Willa is just an experiment at this point, but hopefully it’s given you some food for thought. I’d also recommend checking out the ksml and Noah libraries, which have similar goals to Willa but a slightly different approach. Thanks for reading. Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2019-08-27,"},
{"website": "Funding-Circle", "title": "AWS Lambda For Great Victory", "author": ["Jason Michael"], "link": "https://engineering.fundingcircle.com/blog/2015/04/09/aws-lambda-for-great-victory/", "abstract": "Lambda is a way to run short code snippets in response to events, without the need to worry about infrastructure. The Use Case This past Tuesday, one of our PMs dropped by the engineering pit to discuss a problem he was having.  He explained that, as part of some compliance work, he needed a workflow to convert incoming HTML documents into PDFs.  This needed to happen not just once, but on an ongoing basis.  We didn’t have any natural home for such an endpoint in any of our services, and building a new one just for this workflow seemed like overkill.  What to do? Enter Lambda I had been experimenting with Amazon’s Lambda service, available as part of AWS .  Lambda provides a runtime environment for executing small, stateless code snippets (called “Lambda functions”, which irks me to no end…) in response to certain events within the AWS environment.  There is no “backend” to worry about, no EC2 instances to provision, deploy, or manage.  You simply upload your Lambda function, map it to a set of events, and AWS will handle the execution, monitoring, etc.  It seemed like the perfect fit for our use case! Our strategy to start with seemed pretty simple - HTML would be uploaded to a specific S3 bucket.  This would trigger an event, causing our Lambda function to execute.  The function would pull the HTML down from S3, and pipe it through an HTML-to-PDF binary (we are using the html-pdf module for Node.js, which itself calls into the phantomjs binary).  The resulting PDF document is uploaded into a separate, target S3 bucket.  Piece of cake, right? There are a few wrinkles we had to deal with when working with Lambda.  First of all, each Lambda function is built and deployed as a zip archive, and the max size of this archive, including modules/dependencies and any binaries, is 20MB.  The phantomjs binary clocks in at just over 36MB.  Shipping it along with our code wouldn’t work, obviously.  We worked around this problem by uploading the binary to its own S3 bucket.  Then, the first step in the Lambda function is to pull the binary down and place it in /tmp (Lambda only gives you access to /tmp on the local filesystem).  This takes maybe 3-4 seconds, so not a big deal.  I then had to get html-pdf to use the binary in such a non-standard location.  This amounted to replacing the binary in my zipfile with a symlink to /tmp/phantomjs, and preserving symlinks in the zip job.  I found out later that you can pass the binary path into the call to html-pdf in your Node.js code, which is much cleaner. Small Victories! Within a single business day, I had the workflow up and running, and was able to demo it for our PM.  He was excited to get such a fast turnaround, with no fussing about with infrastructure of any kind.  And while I’m hard-pressed to jump up and down over 20-30 lines of code, I am excited to find other uses for Lambda within the organization.  For example, we are starting to build a data warehouse, and one thing I’m interested in trying out is decomposing our ETL/data movement workflows into a series of small, stateless Lambda functions.  These would operate on data change events streaming in from our production systems, freeing us from updating the warehouse in a batch manner.  If we can make it work, we’d have the Holy Grail in warehousing - a rich, integrated dataset that is updated in near real-time!  I’m also looking forward to seeing what other runtimes besides Node.js that Amazon adds support for.  If they can provide an environment with a JVM, that opens the door for any number of fun and useful JVM-based languages, not to mention plain old Java. Sound intriguing?  Come join us !", "date": "2015-04-9,"},
{"website": "Funding-Circle", "title": "Our Interview Process", "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/03/02/our-interview-process/", "abstract": "Interviewing is difficult for both sides - the interviewee may have to recall small bits of esoterica while standing in front of a group who all know the answers to their questions, while the interviewer needs to find a way to evaluate a very important decision (whether she wants to work with the candidate or not, potentially for years), in a very short amount of time. Both sides take time out of their schedules to meet, so everyone wants to be efficient but still get the correct answers. Word of Warning We do not pretend to have the answer. This is simply our experience iterating our interview process: what has worked, what has not, and what we’re still testing. Caveat Percontator! The Phone Screen We initially set up a 30 minute phone screen with a candidate. This is not particularly technical as we want to understand the candidate’s background and motivations beyond the resume. We talk about what the candidate has done, what she’s interested in, and sell Funding Circle as a great place to work (because it is!). Assuming both sides like what they hear, we send out a coding challenge, and then schedule a half-day on-site interview. The Coding Challenge Before the candidate comes in he gets a coding challenge from us. This is nothing particularly difficult - a few hours at most - we just want to give the candidate the opportunity to write a small program how he thinks it should be written. We prefer the candidate to put this on a Github repository, but we understand that not everyone wants to have a public “job interview test” repository, so the candidate is free to email us the code as well. With the candidate on-site, we sit down with him to discuss the code. “Why did you name this class this way?” “What does this method do?” “Is there a way we can refactor this code to make it cleaner?” The goal isn’t to find the “right answer” but to understand how the candidate thinks and expresses himself in code. A quick bathroom break and we’re back to it. The Pairing Exercise The other test we like to use is a pairing exercise. We pair program most of the time, so we find this the most realistic way to gauge how we would work with the candidate. The premise is a contrived example of our domain that is partially implemented. There are failing tests for the code that needs to be written. The point isn’t to make the tests pass but to demonstrate what it is like to pair with someone on our team, and get the thought process of how the candidate writes code in real time. Since there are so many decisions to make, it is valuable to get a candidate to express how she arrives at the ones she makes, how she decides where her code fits within the existing application, how she deals with edge cases, and how she follows existing patterns. Both we and the candidate should come away with a good idea of what it is like to work together and if that is a positive experience. And the Rest Engineering doesn’t exist in a vacuum, so we want the candidate to meet others in the company. He talks to the product team, since we collaborate with them very closely, and he talks to someone in a non-technical capacity. We strive to get a full, consistent understanding of the candidate before we make a decision, as well as giving the candidate a chance to ask questions to people in many different roles, to give him a better understanding of what Funding Circle is all about. The Aftermath Post-interview, those involved vote on a four point scale “I would quit if we hired this person.” “I don’t think we should hire this person, but I could be convinced otherwise.” “I think we should hire this person, but I could be convinced otherwise.” “I would quit if we did not hire this person.” There has never been a candidate who received a 1, and there has not (yet) been a candidate who received all 4s. The scale is effectively binary, but we find it is easier for everyone to say 2 or 3 instead of thumbs up or thumbs down . If there is not consensus, we discuss until we decide “hire” or “no hire.” These discussions can last a few seconds or up to an hour. These longer conversations have gradually decreased as we hone our process and figure out how to better decide on a candidate. What Else We Tried There have been two alternative tests we tried and decided did not work. The first is a more technical phone screen. With candidates from such diverse backgrounds, it is difficult to find a list of questions that are simple enough to do over the phone and still give us insight into the candidate’s knowledge. We have found it easier to discuss what he has actually accomplished, going into detail about successful projects and areas of interest. Think questions about open source projects instead of explaining Ruby’s eigenclass. We have also tried going “higher level” and discussing service oriented architecture design. This did not work well because it is a less concrete, more abstract, conversation. The candidates generally want to whiteboard specific implementations, and it has been difficult to stay at the higher conceptual level of the overall architecture. This also takes away from the pairing exercise which we find has been the most helpful for us. Conclusion We constantly reevaluate our interview process. The need to balance thoroughness and expediency is difficult since we don’t want to spend all our time interviewing instead of solving business problems, but we know how expensive a poor decision, either to hire or not, can be for the business. We’ll keep trying, as we know there are great developers out there who would thrive in our environment. If you are interested in more specifics of our interview process, why don’t you apply ?", "date": "2015-03-2,"},
{"website": "Funding-Circle", "title": "Creating a Package Repository for Alpine Linux", "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/04/28/create-alpine-linux-repository/", "abstract": "As part of our dockerization efforts we needed some software that was not available in the\npublicly available alpine linux repositories (such as Consul and Weave) and decided to build our own\nrepository so we would have a standard place to put them. While creating the repository it became\nclear that the existing documentation was outdated, so I decided I would document the steps needed\nto get a repository set up for Alpine Linux . Some important sections of the Alpine Linux documentation are out of date and haven’t been updated\nsince 2012. I’m sure there is great knowledge available in the IRC channel, but that is less\naccessible than a good set of online documentation.  I was tasked with creating a repo for\npackaging software that is not available in the public repos, but the entry for the APKINDEX File Format documentation did not\nwork for me. Note: These instructions are for creating a new repository for custom packages, not a repository mirror . Seting up the Repository Decide where you want the repository to live and all the packages will go in this directory. For\nthis example I will create the repository in the /repo/ directory. Notes: The files should live in the repository under the appropriate architecture.\nRepositories can be a simple directory or an http, https, ftp location. You will leave out the\narchitecture when adding a repo. In this example all the apk files will be placed in /repo/x86_64/ . For my own testing I simply copied a few of the packages from one of the public\nrepositories into my repo. I suggest using just one or two for testing because when you index the\npackages you will get an error if it some dependencies are missing. Continue to copy the\ndependencies until you’ve got everything that is required. You will need to index the packages to create the repository once you have all the packages (.apk\nfiles) you want / need in the repository directory: apk index -o /repo/x86_64/APKINDEX.unsigned.tar.gz /repo/x86_64/*.apk Copy the unsigned index to where the signed index will be: cp /repo/x86_64/APKINDEX.unsigned.tar.gz /repo/x86_64/APKINDEX.tar.gz In order to add the new repository to your list of repositories you will need to edit the apk\nrepositories file ( /etc/apk/repositories ) and add the location of your repository. ( /repo in\nthis example.) Testing the Unsigned Repository Comment out any other repositories in your list of repos ( /etc/apk/repositories ) by placing a # character at beginning of any line to be commented. This is not entirely necessary but makes\nit easier to see that things are working properly. Update your available packages: apk update This should output a warning about the untrusted signature: WARNING: Ignoring /repo/x86_64/APKINDEX.tar.gz: UNTRUSTED signature\nOK: 27 distinct packages available You will notice that there are 27 packages in the snippet above. These are not packages in the\nrepository you created but instead are already pre-installed system packages. The number of\navailable packages may be different in your case. You should now re-run the update command and\nallow it to use untrusted repositories: apk update --allow-untrusted This should include the packages in your repo in the count of available packages. In my example\nthere were 11 packages in my repository, resulting in a total of 38 packages: OK: 38 distinct packages available Search for one of the packages you added to your repo: apk search <PACKAGE_NAME> --allow-untrusted This will list all packages matching the string PACKAGE_NAME . If you got the same results as the steps above then you now have a valid (but untrusted) Alpine\nLinux repository. Making the New Repository Trusted Create a Keypair: apk add abuild\nabuild-keygen -a -i It will then prompt you for a filename to save the keypair: `Enter file in which to save the key In this case we will use ‘alpine-devel@example.com-5629d7e6.rsa’ which will create the\n‘alpine-devel@example.com-5629d7e6.rsa’ and ‘alpine-devel@example.com-5629d7e6.rsa.pub’ keypair\nand copy the public key to /etc/apk/keys/ . Make sure you keep a copy of your private key somewhere safe because you will need it if you add\nany packages to the repo in the future since you will need to re-sign the updated index. Note: The standard practice for naming the key is to use the email address of the mailing list\n(‘alpine-devel@example.com’ in this example) as a prefix for the filename of the keypair followed\nby an alphanumeric suffix (‘5629d7e6’ in this example), which is generated by the abuild-keygen tool. Sign the APKINDEX abuild-sign -k ~/alpine-devel@example.com-5629d7e6.rsa /repo/x86_64/APKINDEX.tar.gz Note: You need to give it the full path of the private key or it will be unable to find it. Testing the Signed Repository Your repository should now be complete! The last thing to do is that if everything worked as\nexpected, you should no longer need to us the --allow-untrusted flag to use your packages: apk update This should include the packages in your new repo in the count of available packages: OK: 38 distinct packages available At this point, you will probably want to re-enable any of the repositories that were commented out\nof your /etc/apk/repositories during the first test above.", "date": "2015-04-28,"},
{"website": "Funding-Circle", "title": "Moving to cross-functional teams", "author": ["Mircea Gheorghiu "], "link": "https://engineering.fundingcircle.com/blog/2020/02/10/moving-to-cross-functional-teams/", "abstract": "Bit of history first In early 2019 we embarked on our biggest engineering programme we’ve had in the last 2 years: re-platforming our originations systems. These systems are responsible for the entire loan application process which our borrowers go through.  It orchestrates the whole flow: Eligibility Checks, Credit Risk Assessments, pulling Bureau Data from Third Parties and ultimately runs our Statistical Risk Models (which predict the likelihood of default and therefore how to price loans). Needless to say, this was (and still is) BIG! We had to divert a significant part of our tech and product teams to tackle this. We had teething issues and challenges with finding the right approach to deal with such a big project and eventually we found our groove and managed to deliver our first big milestone by December 2019. What are cross-functional teams for us and why are we doing it? While this was a massive success, 2020 will be even more challenging as we want to accelerate this re-platforming and be even more ambitious with our agility and ability to deliver quicker. One of the issues we have observed last year was that the engineering teams involved in the project were built around specific components of our platform (e.g. we had one team doing the front end of the application, one team building an API, 2 teams working on data connectors and back-end systems, Salesforce engineers dealing with different requests as a single team) and while we did achieve our objectives for the year we felt that there was too much handover between the teams and dependencies that slowed us down at times. So while planning the 2020 roadmap and the team allocations we decided to mix things up and create teams that will be able to build fully functional vertical features across our whole technical and business stack. We’ve come up with 6 brand new teams that have the necessary skills to tackle any problem thrown their way, be it front end, back-end, stream processing or Salesforce. Teams even came up with new names for them, although they did keep the theme of being named after animals: Spice Gulls, Wild Barcodes (basically a fancy name for Zebras), Zealous Zebras (what is this obsession with Zebras??), Flaming Puffins, Tenacious Dragons and Crimson Camels. \nWe’ve intentionally left out any data or DevOps/platform capabilities outside these teams as we want to start this concept and iterate over it before we embrace fully cross-functional teams. Of course, cross-functional teams are not a silver bullet, they will help us solve some of our inefficiencies but they will introduce some challenges along the way as well. Here are some things we already know might be hard: While the teams might be cross-functional and able to tackle problems in React, Ruby, Clojure, Kafka or Salesforce (yup, that’s pretty much our tech stack right now), most of our engineers are not cross-functional or T-shaped as we like them to be called. So there will be a big focus in the next period to ramp up people to be able to move towards that T-shape engineer that we would like to have. That will come with implications in terms of delivery times as well as career development (our career growth framework will need some adjustments for sure). With all 6 teams able to build features on any part of our platform, the code ownership becomes distributed, with no core team maintaining any particular component. This could prove challenging in terms of keeping the architecture vision and integrity of individual components. To counter that, we’ve created Chapters (no, we’re not trying the Spotify model, it’s very loosely based on that), which are groups of engineers maintaining, enhancing and supporting certain well-defined components of our platform (including code ownership for said components). I’ll expand on that in future posts, as I do feel that’s a whole topic on itself. It’s early days of this experiment, the teams have just completed their first iteration and they’re halfway through the second one but signs look pretty positive: everyone feels genuinely excited and there’s a good buzz around the engineering floor. The work completed in the first 2 weeks surpassed our expectations as well so well done to all the teams involved!! What’s next for us? Well, we will keep focusing on getting Chapters up and running properly, iterate over our process of dealing with tech work (another pain points of ours in the past) as well as starting sending surveys to the teams to gauge their engagement and happiness but also get some early feedback about the changes we’ve introduced. Stay tuned… Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2020-02-10,"},
{"website": "Funding-Circle", "title": "Conference Highlights: Write the Docs Portland 2019", "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2019/06/06/write-the-docs-portland-2019/", "abstract": "I attended my first Write the Docs﻿ conference last year, in\nPortland ﻿. I’d been passionate about\ndocumentation and knowledge sharing at work for many years, but I’d been a professional documentarian﻿ for only about six months. It was\ntherefore galvanizing and inspiriting to be surrounded by hundreds of like-minded people, people\ncreating a community with strong and clear values that resonated with me. This community is\nincredibly friendly, warm, inclusive, egalitarian, supportive, helpful, friendly, and thoughtful.\nThat’s one of the primary reasons the conference was extremely valuable for me last year, and why I\nwas eager to attend again this year. I hope sharing my highlights from the conference will be helpful to anyone working on software\ndocumentation or considering attending a similar event. Saturday On Saturday I joined the annual conference\nhike . This was super-enjoyable and a great\nway to meet new people and have interesting conversations in a beautiful environment, while being\nactive. Sunday Sunday was a community writing day for documentation\nsprints . I skipped it so I could drive\nout to the Pacific coast and walk in the surf. It was a beautiful drive, a beautiful beach, and a\nrestorative experience. Monday On Monday the only talks in the main hall that I attended were the introduction by Eric\nHolscher and the first talk, by Heather Stenson; I\nspent the rest of the day preparing a lightning talk (see Tuesday ) and attending unconference sessions. Eric Holscher The main conference started with a wonderful\nintroduction by one of the founders of Write the\nDocs , Eric Holscher . This was an\nintroduction in the most expansive sense of the term — he went to great lengths to encourage a warm,\nwelcoming, supportive, inclusive community. Heather Stenson Stenson presented Any friend of the docs is a friend of mine: Cultivating a community of\ndocumentation\nadvocates .\nGreat talk all around. My favorite aspect of this talk is that it’s about an idea that’s somewhat\nobvious, and has been discussed before, but it took a fresh approach to the topic. Stenson focused\non very concrete and practical ways of running an actual program designed to manifest, maintain,\nand harness a diverse community to work on an org’s documentation. Unconference Forming a non-profit Clay Murphy generously shared his purpose and experience in working to form his non-profit FirstRides.org which teaches underprivileged young kids how to ride\nmotorbikes in a safe, supportive, and accessible environment. I was moved by Clay’s story of how\nswitching from a car to a motorcycle enabled him to cut his costs enough to stay in college, and how\nhe wants to help others take advantage of the radically lower costs and higher efficiencies of\nriding motorcycles as compared to cars. This struck me as an extremely timely idea, as the\ncombination of the climate crisis and growing financial inequity mean that we have a doubly urgent\nneed for people to switch to more efficient modes of transportation. Real tech writing in confluence: Rock the Docs Matt Reiner kicked this off by showing a website he’s been\nworking on with a few other people: Rock the Docs , which is\nintended to collect and organize tips and techniques for maintaining “real” technical documentation\nin Confluence . (My impression is that Confluence\ntends more often to be used for wikis and knowledge bases.) Impressively, the site is itself\nauthored and published via Confluence — it essentially is a publicly viewable instance of\nConfluence. The site is impressive and looks like it’s full of highly-accessible tips on working in\nConfluence, some of which are probably applicable for any usage of Confluence. Discussion of the\nsite and its contents evolved into to a far-reaching exploration of the challenges related to using\nConfluence for technical docs — both internal and external. How to set up auto grammar/style checks with Vale I showed up late to this one, and when I arrived the crowd gathered around Jodie\nPutrino was the largest I’d ever seen for an unconference\ndiscussion. There was intense interest from the group in Jodie’s experience adopting Vale , a “free, open-source linter for prose built with speed\nand extensibility in mind.” That description is from Vale’s website , which continues with this useful explanation of its\nvalue proposition: “Unlike most writing aids, Vale’s primary purpose isn’t to provide its own\nadvice; it’s designed to enforce an existing style guide…” Being a command-line tool, Vale seems perfect for a team employing Docs as\nCode , as Jodie’s does (she spoke about her\nteam’s experience adopting Docs as Code at Write the\nDocs Portland 2017). For example, Vale could be run via an editor integration, as a Git\nhook , and/or as part of an automated test/build pipeline. In fact, that’s\nexactly what I did when I integrated Vale into my FLOSS diagramming framework only two weeks later. Show and Tell: Docs as code framework for architecture diagramming I proposed and hosted this discussion; I wanted to share the FC4\nFramework with an eye towards hopefully growing the\ncommunity of people using it and maybe even contributing to it. About eight people attended, which I\nthought was pretty good given the demographics of the conference (which have shifted over time to be\ndominated by technical writers). It was encouraging that my short demo was effective; it seemed that\nthe participants really understood the value proposition of the framework and found it compelling. Lightning Talks I’m a big fan of the lightning talk format. Whether they’re five minutes or ten, accompanied by\nslides or not, they tend to be very effective and efficient at making people aware that something\nexists, and giving them enough information to determine whether or not they want to learn more.\nSince “x exists” is often the most important and impactful result of a conference talk, lightning\ntalks probably have the best bang/buck ratio in terms of discovering new ideas, tools, topics,\nand techniques to bookmark and investigate later. Colin Torretta Torretta’s talk Succeeding as an Internal Engineering Writer… was\nextremely relevant to my interests. He discussed the significant, and potentially surprising,\ndifferences in priorities between internal customers and external customers. For example, internal\ncustomers don’t care about consistency of presentation. Tess Needham Needham’s talk Comics in Docs was fun, engaging, and compelling. As\na mega-fan of comics I didn’t need any convincing, but I was delighted to see someone using comics\nto show when, why, and how comics can be used effectively as a form of documentation. Tuesday Alicja Raszkowska When someone asked me what I’d thought of the first talk of the day, I was dismayed to realize that\nwhen I’d glanced at the schedule over coffee that morning I’d somehow missed the talk, called Draw\nthe Docs . If I’d\nnoticed it in the schedule I would have made sure to have attended, as based on the title alone it\nwas extremely relevant to my interests. I therefore made sure to download the talk (using the excellent youtube-dl ), and I was eager to watch it on the flight home\n— which I would have done, had my laptop not been dead. Once I was able to watch the talk, I was even more excited. Raszkowska is an engaging speaker, and\nthe talk is beautifully constructed. I had proposed a similar talk entitled Docs as Code for\nArchitecture Diagrams wherein I would have focused on the benefits and the workflows involved in\ndiagramming software architecture using a text-based notation. Raszkowska covered nearly the same\ntopic, except for the focus on architecture specifically. (Which makes sense given this audience.\nDocumentarians frequently need to explain what a program does, and how it does so — which means\nworkflow or data-flow diagrams are crucial.) The talk selection committee definitely made the right choice; whereas Raszkowska started by telling\nthe compelling story of how and why she came to integrate drawing and graphical documentation into\nher work and only then moved on to her more analytical segment on automation and workflow, I\nprobably would have started in the analytical mode and stayed there. I’m grateful to have learned so\nmuch by noting the differences between Raszkowska’s talk and mine. I suspect that if and when I do\nend up delivering my talk it will be substantially more effective than it would have been, had I not\nseen her talk. Matt Reiner Reiner presented Show Me the Money: How to Get Your Docs the Love and Support They\nDeserve which was a comprehensive guide on how and why to achieve and maintain alignment between “the\nbusiness” and the documentarians. His energetic, humorous presentation style breathed life into a\ntopic that is crucial but can be dangerously dry. This is a talk that I’m definitely going to be\nre-watching while I take notes. Riona MacNamara Because this was a sponsored talk I was initially concerned that it might be more than a lightly\nveiled recruiting talk for MacNamara’s employer. I should have had more faith in the conference\norganizers; they wouldn’t waste the community’s time like that. MacNamara’s talk Documentation for Good: Knowledge as a tool for equity and\ninclusion was inspiring and galvanizing. She helped put words to some beliefs I’ve long held in in a hazy,\ninarticulate way — that making knowledge shared, open, and accessible is an act of radical\negalitarianism that fundamentally levels the playing field. Knowledge is a form of power, and I\nbelieve that people shouldn’t have more or less power due to the happenstance of, for example, their\nage, tenure at an org, or level of experience in their career (nor of course their race, ethnicity,\nnational origin, marital status, parental status, gender identity, disabilities, neural differences,\netc). MacNamara’s talk comprised a powerful and useful blend of ideas and information and I expect to\nrevisit it a few times so as to internalize both. Jessica Parsons Parsons’ talk Lessons Learned in a Year of Docs-Driven\nDevelopment was an extremely informative case study in her org’s continued embrace of docs-driven development. I\nwas first introduced to this idea by Tom Preston-Warner as Readme\nDriven Development , and\nI’d thought that he’d invented the idea, so I was grateful that Parsons led with a short history of\nthe idea that questioned that story. Lightning Talks Katy Decorah Decorah’s talk Automated Testing for Prose was impressive and\ninspiriting. I now aspire to adopt and employ something like Mapbox’s copyeditor system. (It’s a\nfork of alex , and now I’m curious to compare alex with Vale .) Sandra Friesen Friesen’s talk Don’t get RSI was informative and heartening. It’s\nreassuring to be around people who clearly care about their communities and who go out of their way\nto support the wellness of their communities — people who have enlarged their circle of\nempathy . Unconference My Lightning Talk I hadn’t been planning to give a Lightning talk. During the hike on Saturday I was describing\nliterate programming to someone, and I mentioned that I tend to use Git commit messages to tell the\nstory of my work. The person I was speaking with was intrigued to learn that Git commit messages\nhave an official format so I described in in\nsome detail. Eric Holscher overheard and said something like “sounds like a lightning talk!” — which\nsounded good to me. So I spent some time on Sunday, Monday and Tuesday preparing a talk. My talk was chosen as an\nalternate for Tuesday’s batch of lightning talks, but there was no time for me in the end. The\norganizer of the lightning talks, Rose, suggested that I present the talk as an unconference\nsession, so I did that. I ended up calling the talk Storytelling with Git ( slides ). I enjoyed putting the talk\ntogether and delivering it to a small but highly engaged audience in an unconference session. It\nended up being ~15–20 minutes rather than 5, but that included some discussion. Inclusion Another highlight of the conference, for me, was the effort and attention put towards inclusion. It\nwas heart-warming to see and made me proud to be part of this community. Portland I ❤️ Portland!", "date": "2019-06-6,"},
{"website": "Funding-Circle", "title": "Testing Kafka Streams topologies with Kafka Interceptors", "author": ["Nacho Munoz"], "link": "https://engineering.fundingcircle.com/blog/2017/11/26/testing-kafka-streams-topologies-with-kafka-interceptors/", "abstract": "We rely heavily on Kafka and Kafka Streams at Funding Circle to build event-driven microservices, so, testing those Kafka Streams topologies is key to validate the correctness of our platform. The aim of this blog post is to compare two integration testing approaches for Kafka Streams topologies: one is based on Kafka TestUtils , which is commonly used in the Apache Kafka project and some of the Kafka Streams examples that can be found in Confluent’s repository ; and another one is based on Kafka Interceptors which seem to have some advantages when compared to the other approach. Kafka Interceptors were introduced in Kafka 0.10.0 as a way to make instrumentation easier for applications relying on the Kafka Client API. It is possible to chain several interceptors and utilise them either on the Consumer or Producer side which provides flexibility and a great range of scenarios where we can apply them: logging, message encrypting, distributed tracing, filtering fields with sensitive information, custom metrics and, as we are going to see soon, testing. In order to illustrate those different approaches we are going to use a toy topology as an example. As showed in the image we would like to enrich our investors’ information with their portfolios and split the output in different topics depending on the investor type. This topology will look like the code below: ( def topology ( let [ builder ( KStreamBuilder. ) ;; inputs investor ( .stream builder ( into-array String [ \"investors\" ])) portfolio ( .globalTable builder \"portfolios\" ) ;; processing [ institutions retail ] ( .. investor ( join portfolio ( key-value-mapper ( fn [ k v ] ( :portfolio-id v )) ( value-joiner ( fn [ v v ' ] ( merge v v ' )))) ( branch ( into-array Predicate [( predicate institutional? ) ( predicate ( complement institutional? ))])))] ;; outputs ( .to institutions \"enriched-institutional-investors\" ) ( .to retail \"enriched-retail-investors\" ) builder )) How do we check that the logic of this topology is correct? Let’s see some integration testing approaches that we can use to test our topology logic. We will assume that we have Kafka running for the integration tests, maybe by using a docker-compose file. Also, for this kind of tests we will consider the topology as a blackbox, so we will only care about the outputs generated by a given input. Using a dedicated consumer In this approach we need to use Kafka’s client API to create a consumer for our tests so we can check if the records in our output topics match our expectations. Putting it in the context of the Four-Phase Test testing pattern, we can describe this approach as: Setup: This step implies to populate the input topics (investors and portfolios) with some sample records. Ideally, we will choose inputs that follow different execution paths in our topology. Exercise: Running the topology and wait for it to process the inputs. How do we know when the topology has finished? We just don’t know, so either we put the thread to sleep providing enough time for the topology to finish or we poll the output topics until a condition is met (e.g. there are 3 messages) or a timeout is reached. Kafka’s test utilities include a static method TestUtils.waitForCondition which does exactly that. The thread sleep approach is simpler, however, it can slow down your tests or make them flaky if the chosen sleep time is too tight. Verify: Checking that the records in the output topics match with our expectations. As mentioned before, we need a dedicated consumer for our tests to retrieve the records from the topic. Teardown: Resetting the system state to its previous state by maybe deleting the output topics and cleaning Kafka Stream’s state folder. As an alternative, you can run Kafka inside of your process using for instance EmbeddedSingleNodeKafkaCluster . In that case, you don’t need to worry about deleting the topics but booting and shutting down this in memory instance of Kafka every time we run a test, could make the test suite slower. Besides, it makes auditability harder because the topics are only reachable while the tests are running, so you cannot use tools like kafkacat or kafka-console-consumer against your embedded cluster as you would do with a separate docker cluster. We can see these steps applied in the code excerpt below: ( deftest topology-with-consumer-test ( testing \"when institutional investor\" ;;setup ( let [ investor { :id 1000 :name \"lol\" :institutional? true :portfolio-id 1 } portfolio { :id 1 :amount 100 } output-topics [ :enriched-retail-investors :enriched-institutional-investors ] consumer ( create-consumer-for output-topics )] ( send-to :portfolios 1 ( str portfolio )) ( send-to :investors 1 ( str investor )) ;;exercise ( with-topology topology config ( TestUtils/waitForCondition ( expected-messages consumer 1 ) 30000 \"Timing out\" )) ;;verify ( is ( = 1 ( count ( consumed-messages :enriched-institutional-investors )))) ( is ( = ( merge investor portfolio ) ( -> ( consumed-messages :enriched-institutional-investors ) first :value read-string ))) ( is ( nil? ( consumed-messages :enriched-retail-investors ))) ;;teardown ( .close consumer ) ( remove-topics output-topics )))) When following this approach you need to take into account Consumer configuration settings like “GROUP ID” and “AUTO OFFSET RESET CONFIG” to avoid undesirable rebalances and to force the consumer to read from the beginning instead of using any previous committed offset. In a nutshell, there is nothing wrong with this approach aside from the fact that you need to write some utility code to retrieve the outputs from the topics and clean that state afterwards. Using Kafka Interceptors In the previous section we leaned on a dedicated consumer to verify our expectations, lets see how we can leverage interceptors to get rid of that additional consumer. Given the pluggable nature of Kafka Interceptors, we can easily add or remove them from our applications with just a configuration change. Our goal is to avoid checking the output topics in Kafka by maintaining a map in memory that associates each output topic with produced records. A possible implementation can be found below: ( deftype MyTestingInterceptor [] ProducerInterceptor ( close [ _ ]) ( configure [ _ configs ]) ( onAcknowledgement [ _ metadata exception ]) ( onSend [ _ record ] ( let [ topic ( keyword ( .topic record )) key ( deserialize ( Serdes/Long ) ( .key record )) value ( deserialize ( Serdes/String ) ( .value record ))] ( swap! output-results update-in [ topic key ] conj value ) record ))) The implementation is using an atom to accumulate the state in a map using the topic name as the key and a vector with the records written to that topic. Those records are deserialised before appending them to the map to facilitate the assertions on the test. Revisiting our description based on the Four Phase Test pattern, we would find the following steps: Setup: Same as before but with an additional change in the Kafka Streams configuration to include our interceptor. Exercise: Again running the topology and wait for the outputs but this time we don’t have to poll any topic because of the visibility gained with the interceptor. For instance, we could use a CountDownLatch initialised with the expected amount of messages and decrease the latch when the atom that contains the state changes. Still, we will need to use a timeout to wait for a bounded amount of time in case something goes wrong. Verify: We run our assertions against our in-memory map that contains the results of the execution. One nice thing about it is that we can assert the lack of messages on a given topic without having to expire a timeout. Teardown: In this case we do not need to clean the output topics, we just need to reset our atom to clean the state. However, it is still necessary to clean Kafka Stream’s state folder. Translating this into code we will have: ( deftest topology-with-interceptors-test ( testing \"when institutional investor\" ;; setup ( let [ investor { :id 1000 :name \"lol\" :institutional? true :portfolio-id 1 } portfolio { :id 1 :amount 100 } config ( assoc kafka-stream-settings \"producer.interceptor.classes\" \"blogpost.utils.MyTestingInterceptor\" ) latch ( expected-messages 1 ) _ ( add-watch output-results :test ( fn [ _ _ o n ] ( .countDown latch )))] ( send-to :portfolios 1 ( str portfolio )) ( send-to :investors 1 ( str investor )) ;; exercise ( with-topology topology config ( .await latch 30 TimeUnit/SECONDS )) ;;verify ( is ( = 1 ( count ( :enriched-institutional-investors @ output-results )))) ( is ( = ( merge investor portfolio ) ( some-> @ output-results :enriched-institutional-investors first :value read-string ))) ( is ( nil? ( :enriched-retail-investors @ output-results ))) ;;teardown ( remove-watch output-results :test ) ( reset! output-results {})))) In my opinion, avoiding to consume from the output topics to make our assertions has the following advantages: More visibility over when to transition to the Verify phase Easier state management (e.g. Kafka topics vs Clojure map) Easier to debug failing tests in topologies with intermediate topics Conclusions This blog post has presented two different approaches to test our Kafka Streams topologies. The first one is more general and it is widely utilised in Kafka Stream’s codebase and code examples. The second one shows how we can use Kafka interceptors for testing and doing some optimisation over the general approach. To sum up, being aware of Kafka Interceptors can make our life easier when testing Kafka Streams applications. Thanks to Sasha Gerrand, Niel Drummond and Shkodran Gerguri for their help reviewing this post", "date": "2017-11-26,"},
{"website": "Funding-Circle", "title": "Trip report - My visit to the London office", "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/03/09/trip-report-my-visit-to-the-london-office/", "abstract": "I recently had the good fortune to travel to the London office to work with the HQ team for a week. Funding Circle currently has two offices: the founding office in London and an office in San Francisco. There are development and business teams in both offices, and we work together on different parts of our global platform. It’s a delicate process to keep us all coordinated and moving in the same direction, and my trip was part of our effort to keep our teams aligned and share what we’ve learned. Since we’re two parts of the same team and our work overlaps different applications across our stack, we want to optimize the communication between the two offices. We need to be more than just email addresses and GitHub handles to each other, and working together in person is a simple way to do this. Lessons learned Knowledge sharing There was a great presentation by Rafa Paez on optimizing Ruby code. He presented some strategies and shared how he sped up a particular slow query. Even though I’ve been writing software for years, it was valuable to see a developer’s thought process when solving a problem. Knowledge sharing is vital. Not only do we hire junior developers who haven’t worked on sites that need to scale, but we can’t expect even the more experienced developers to have background on the same challenges we have. As the team grows, it is critical to spread knowledge, and we encourage this behavior. Consistency Code to the principle of least surprise . It’s important to agree on patterns, naming, and libraries so each developer does not need to reinvent the wheel. Here are a few examples of how we maintain consistency with existing and new applications. APIs : We discuss the best way to build our internal and external APIs so services have an easier time communicating. Naming : We share a style guide that describes the importance of naming and prescribes a set of conventions to make code easier to reason about. Libraries : Our style guide also lists the Ruby libraries we use for common tasks so we don’t need to solve the same problem in multiple ways. Empathy The biggest lesson learned: we are all on the same team striving to build a successful company. Meeting people I’ve only talked to online is powerful (and fun!). We can now use more subtlety in our words because we understand each other more, and it turns out that there are some brilliant, fun, and amazingly welcoming people at Funding Circle. I can’t thank the team enough for sharing their favorite restaurants, explaining what pudding is (not always what an American thinks it is), or helping me figure out the best way to get around London. Beyond the specifics of the code or the business requirements, we met as friends, and we’re better for it. I hope to visit again, but in the meantime, my coworkers will have the same opportunity to visit.", "date": "2015-03-9,"},
{"website": "Funding-Circle", "title": "Our Values - Delivering Value", "author": ["Afam Agbodike"], "link": "https://engineering.fundingcircle.com/blog/2015/02/16/our-values-delivering-value/", "abstract": "You might thing that delivering “value” is redundant in an article about values. But you are wrong! This value is about making sure that the work that we do is valuable to our customers. When\ndeciding how to prioritize work, or whether we should even undertake a task should be decided based\non how it can make the experience of our users better. Sustainable pace One way of delivering value is ensuring the teams that count on us, can trust the estimates that we\ngive them. Often decisions of what to prioritize are based on how long a particular feature will\ntake to implement. If we are not able to make realistic estimates on when features will be delivered,\nthen making effective prioritization decisions becomes impossible. One of the ways for us to be\nable to give effective time estimates is to work at a sustainable pace. If we are consistently\ndelivering about 20 points worth of difficulty per week, then we can be fairly confident that a\nfeature with 80 points worth of complexity will take about 4 weeks of full time effort. Delivering business value Sometimes this is quite obvious, as in the case of adding a new product to our loan offerings, but\nother times it’s not so easy to determine. How can we determine if adding a messaging queue, or refactoring a tricky bit of code, or adding a\ndata warehouse is going to “deliver value?” For the cases where determing the benefit of a task, we try to evaluate it from a number of smaller\ntopics which can help us determine if there is value in the task. Automation Automation is all about efficiency. If you can take an error prone manual process and turn it into\na consistent and repeatable process, you can get great gains in overall efficiency, as well as\neliminating the possibility of the fat finger. The one flaw in automation is the risk of\nover-automation, which leads us to… YAGNI - “You ain’t gonna need it!” One of the core principles in delivering value is making sure that somebody needs it,\nor at least wants it. No value is delivered when there is nobody that can, or will, take advantage\nof a feature. Often developers look at a problem, see that in some future condition it may need\nadditional coding, and dive in right away to developing the code about the future condition, only\nto discover several months later that either they didn’t understand the full ramifications, or have\nit decided that an entirely different approach is desired.", "date": "2015-02-16,"},
{"website": "Funding-Circle", "title": "Google I/O 2015", "author": ["Juanita Lee"], "link": "https://engineering.fundingcircle.com/blog/2015/06/01/google-i-o-2015/", "abstract": "Google I/O. That’s the developer conference Google holds every year in San Francisco to explore the latest and greatest in tech, including web, mobile and everything in between and beyond! In 2013, only 8% of attendees were women and in 2014, it rose to 20%. This year I was lucky enough to attend I/O and was a part of the record 23% in attendance! How exactly did I score an invite to Google I/O? Google has a global program for women in tech— Women Techmakers (WTM) —that seeks to empower women in tech and create an inclusive tech community. Led by the amazing Natalie Villalobos , WTM teamed up with awesome organizations like GirlDevelopIt (GDI) —which I am a part of—and extended invites and discounted tickets to their members. Community. Before I/O officially kicked off, there were plenty of opportunities to meet and connect with other female attendees. WTM created a group on the team messaging app Slack for us to connect with each other before the event and to stay connected afterwards. GDI members gathered early in the morning the day before the conference at a nearby coffee shop for hot drinks and conversations before lining up together to pick up our I/O badges. The night before I/O, WTM held multiple dinners throughout the city so everyone could connect in person over delicious food, drinks, and conversations. During dinner, we learned about the statistics of I/O female attendees throughout the years. It’s great to know that this year there were 23% in attendance AND half were attending for the first time, including myself! I forgot how many shrimp cocktails I devoured before dinner or how many oysters I slurped, but it was a great crowd and the food was spot on! Dinner with amazing #womentechmakers from all over! This year 23% of #Google #io15 attendees are females! Woot-woot! pic.twitter.com/ZcF5QMGFF1 — Juanita Lee (@thejuanitalee) May 28, 2015 The great thing is that although many of us were attending the conference alone, WTM and GDI made it possible for us to attend as a part of a community already. We were ready to take on I/O! Google I/O 2015. I can’t talk about everything that happened at I/O because then I would bore you with a very long post. I’ll highlight some of my favorite moments instead. Pre-Keynote The keynote had a limited audience and was assigned on a first-come, first-serve basis during badge pickup. Luckily, I was able to see the keynote live! Inside the keynote room, there were giant screens that almost wrapped around the entire room. This photo does zero justice but it gives you an idea of what it looked like: Everyone watched a blue whale swim across the screens: Oh you know, just hanging out underwater with a big ass whale :D #io15 #prekeynote pic.twitter.com/AEkAONV9TM — Juanita Lee (@thejuanitalee) May 28, 2015 And a few giant games, literally, of Pong: Diversity Guess what? It wasn’t all white dudes presenting everywhere! Aparna Chennapragada , the Product Director of Google Now, was a great speaker. She made me excited about having my phone take over my life. She introduced Now on Tap, which is Google Now on steroids. The important thing about Now on Tap is context; it makes your smartphone smart enough to help you with tasks right away based on the context in your screen. On the first day, it was hard getting into the sandbox talks and different sessions mainly because of three things: There was limited space. The map wasn’t super user-friendly. There wasn’t enough time to get from one place to the next. This is what a typical sandbox talk looked like: But on the second day, knowing what I learned from the first, I was able to navigate better and even get good seats! It was definitely nice to attend a sandbox session led by a woman. Like this one with Googler Alice Boxhall , who is the primary developer of the Accessibility Developer Tools extension and library in Chrome: “The Gold Standard: Accessible Web Components” with @sundress and @rob_dodson ! #web #accessibilty #io15 pic.twitter.com/E3qK3dex47 — Juanita Lee (@thejuanitalee) May 29, 2015 There were other great talks, including Engineering for the Stratosphere which had an all-women panel from Project Loon . It made me happy to see smart women on stage talking about tech and leading sandbox sessions. :) More cool stuff that happened… Check out these 3D printed hands from exiii ! Oh no biggie, just shaking hands with someone’s #bionic hand… Uh, yeah right! SUPER COOL! :D #io15 #accessibilty pic.twitter.com/amldBS56S1 — Juanita Lee (@thejuanitalee) May 28, 2015 More #exiii at #googleorg #sandbox ! 3D printed AND I got 2 experience controlling 1 of the #bionic hands! :D #io15 pic.twitter.com/Sx3TY2DhcU — Juanita Lee (@thejuanitalee) May 29, 2015 Checkout some very cool stuff from Google’s Advanced Technology and Projects (ATAP)! Woooow, #ATAP ! Gesture radar for wearables! Twist, swipe, scroll, etc. #mindBlown #Google #io15 pic.twitter.com/NerHthCpW4 — Juanita Lee (@thejuanitalee) May 29, 2015 Interactive textile! Wow! #ProjectJacquard #ATAP #Google #io15 http://t.co/LO9lQKefYe pic.twitter.com/fN8QZPgiCj — Juanita Lee (@thejuanitalee) May 29, 2015 Project Tango was pretty sweet too. It combines awesomeness (3D motion tracking with depth sensing) to let a mobile device see how we see. Below, I was a giant in a world with tiny people and houses, really pointy trees, and there were specks of light floating all over. Partnered up w/ @ggmathur ! We know it’s #VR but still tried to touch things, haha. #ATAP #ProjectTango #io15 pic.twitter.com/TXa6uJYvu3 — Juanita Lee (@thejuanitalee) May 29, 2015 And Project Ara ! Make your smartphone what you want it to be on the fly! #ProjectARA #Google #io15 pic.twitter.com/zWMtcxvXjt — Juanita Lee (@thejuanitalee) May 29, 2015 One of the best talks at I/O was Speechless—one you wouldn’t want to miss out on! It’s based on a SF comedy show but with a nerdy tech-conference twist. The Googler contestants spun a wheel to determine the type of speech (e.g. product launch, keynote, etc.), then the audience came up with a topic, and finally the contestants had to walk through a random slideshow and present their talk. The hilarious improvs ranged from opossums to Texas to even Android Underwear. Who knew that Googlers can present on stage and tell jokes on the fly?! This was probably the best last session you could have attended. Overall, Google introduced a lot of very cool projects and these were only a few of them. There’s an exciting future for mobile! Swag, swag, swag. Some people take swag very seriously. After a session I attended ended, the presenters set out some stickers for attendees and a small crowd formed. I watched a guy rudely push through a crowd and knock another guy over because he REALLY wanted those free stickers. He also took more than he should have buuuut like I said, some people take swag very seriously. Anyway, this is what I got from the WTM dinner: Sticker nametags what? #WomenTechmakers #io15 dinner #swag ! My 1st #GEMMA kit + #TCHOchocolate + #LadyAlamo pouch :D pic.twitter.com/ogTdTrvyba — Juanita Lee (@thejuanitalee) May 28, 2015 And here are some of the other things I got: Goodbye Google I/O 2015! I’m glad I received an exclusive opportunity to attend I/O. Did I mention that this was my FIRST tech conference ever?! Well, it was and overall it was a memorable experience. I met a lot of great people from all over the world, including amazing women who inspired me with their stories and backgrounds. I’m glad to hear and see Google taking the initiative to be more inclusive of women and minorities within the tech community. Increasing the female attendance rate at I/O has been awesome, but what can we all do to improve the overall statistics on women in tech and leadership? In my corner of tech—marketplace lending—my workplace Funding Circle is working to increase diversity both within and outside our offices. We have a community that promotes the professional and personal development of the women at Funding Circle, including recruiting efforts. We are also making the lending process more accessible to women entrepreneurs who traditionally lack access to affordable capital and that has been a very good thing. Learn more about it here ! If you missed out on I/O this year, be sure to check out the videos here . Again, a big thanks and shout out to Women Techmakers and GirlDevelopIt for making it happen!", "date": "2015-06-1,"},
{"website": "Funding-Circle", "title": "Our Values - Respect", "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/02/09/our-values-respect/", "abstract": "The most important aspect of a company I look for is respect because it drives every interaction\namong the engineering team, among the company, and with customers. “No Asshole” Rule We demonstrate respect in all areas of our work. The agile methodology requires collaboration as a\nteam and with stakeholders. This can only happen if we effectively communicate to each other. If I\ncan’t empathize with a stakeholder or another developer, I could easily say something that would\nhurt or offend. Now we’ve got a hostile work environment, morale drops, people quit, and the entire\ncompany is affected. Nobody wants to work with that guy . If I talk to others with an understanding of their feelings, I would never say something to create\nthis situation. I would: critique the code, not the developer never feign surprise never dismiss something as “stupid” say “yes, and” instead of “no” Optimism We are also optimistic. We understand that no code is perfect, but it can always get better. It is our code, not Developer X’s, so we must all work to improve it. We use the Boy Scout rule (Girl\nScouts need apply too!) to keep our code base clean. There are downsides though. Many of our decisions require consensus - they’re transparent and we\nhear what others have to say. This causes more overhead to getting things done on occasion, but as\nour team grows, we discover new ways to balance collaboration and delivering.", "date": "2015-02-9,"},
{"website": "Funding-Circle", "title": "When to use strong, weak and unowned reference types in Swift and why", "author": ["Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2018/04/27/swift-strong-weak-unowned/", "abstract": "We have been using Swift at Funding Circle for a couple of years now.\nOne particular subject that was interesting to get to the bottom of was the usage of weak vs unowned reference types. Why are there three ways ( strong , weak , unowned ) of referencing an object and when should we use each. First lets understand why is the reference counting important. Swift does its memory management by relying\non the ARC . As name implies, it counts references in order to understand\nif an object has to be kept in memory or not. That’s an extremely simplified explanation, to learn more\nplease refer to Apple’s ARC documentation 1 . Before we start lets make sure our communication is clear. When talking about variables, we are referring\nto the Swift reference types ( class and function ). Reference counting does not work the same way for value types ( struct , enum and basic types). Swift does a copy on write whenever a value type is being changed. This means that, in general, we can assume that in practice a value type\nhas a reference count of at most 1. In reality the values also point to one same instance up to the point when\none of the values is being changed, only at that point a copy of the original value will be done, then changed\nand persisted in its own allocated memory. Strong references This is the default reference type in Swift. Whenever we declare a variable without\nspecifying its reference type, it will always be strong . A reference being strong means that the ARC\nwill increment the reference count for the object being referenced by a variable.\nThis impacts memory management since an object’s memory can not be released while the reference count\nis greater than zero. Lets look at the following simplified example: class Balance { let amount : NSDecimalNumber let currency : String } class Lender { let name : String var availableFunds : Balance init ( name : String , availableFunds : Balance ) { self . name = name self . availableFunds = availableFunds } } Although in real world Balance entity would be a great candidate for being a struct , we made it a class for demonstration purposes.\nSo we have two entities, Balance and Lender. Lender’s property availableFunds holds a strong reference to\nan object of type Balance . At the point when we assign the Balance object to a named variable,\nthe object’s reference count is incremented by one. This means that there is one more variable pointing to the\nmemory that was allocated for that object. The implication of this assignment is that the Balance object\ncan not be deallocated while the Lender instance exists. ... self . availableFunds = availableFunds // reference count += 1 ... An important note about this example is that in Swift the arguments are constants by default and can not be\nchanged. This means that when we assign the argument to the property, a copy of the argument will be made when it’s\nbeing changed. Multiple variables pointing to the same object increment it’s reference counting as we can see in this snippet: var balance = Balance ( 100.0 , \"GBP\" ) // ref count 1 var balanceCopy : Balance ? = balance // ref count 2 let balanceConstant = balance // ref count 3 balanceCopy = nil // ref count 2 A constant will have always a strong reference type, so when we declare let balanceConstant it will\nincrement the reference counting of the Balance object in memory. Whenever a variable that points to an object\nwhich is as well referenced by a constant is changed, a copy of the object will be made and that copy is\nthe one being updated. Weak reference Contrary to strong reference, weak reference has no impact on an object’s reference count. Meaning that if\nwe declare a weak variable pointing to an object, that object’s reference count will remain the same as it\nwas before. Lets see what that means in practice with the following simple example: var balance : Balance ? = Balance ( 200.0 , \"GBP\" ) // ref count 1 weak var balanceCopy = balance // ref count 1 balance = nil // ref count 0 balanceCopy ? . amount // nil We start by creating a balance variable that will hold a strong reference to the newly created Balance object. This increments the object’s reference count and makes it equal to one.\nNext we declare a weak variable balanceCopy which will not change the object’s reference count.\nWe then remove the strong reference from the object by assigning nil to the balance variable that was holding strong reference\nto the Balance object. This brings the reference count to zero and consequently deallocates the object\nwhich means that our weak balanceCopy variable will have no object to point to and thus when we\ntry to unwrap it, the result is nil . Unowned reference Similarly to weak reference, an unowned reference does not increment the object’s reference count. But there\nare several important differences in its usage.\nOne of the differences between weak and unowned is the fact that Swift runtime keeps a secondary reference\ncount for unowned references. When the strong references count goes to zero, the object will release all the\nreferences it has but the object’s own memory won’t be released while there are unowned references pointing to it.\nThe object’s memory is marked as zombie though. It means that the user can not rely on whatever is stored in that memory\nand accessing it, without a safe unwrap, will crash the program. The check of the reference happens at runtime, that’s why accessing it is a runtime error.\nAnother difference is that unowned variables can not be of optional type. This is very important given that Swift will force us to use the variable without being able to double check if it’s pointing to a valid object.\nLets look at the following example: var balance : Balance ? = Balance ( 100.0 , \"GBP\" ) // ref count 1 unowned var balanceCopy = balance ! // ref count 1 balance = nil // Balance object ref count 1 balanceCopy . amount // runtime error To start with, we declare an optional balance variable that holds a strong reference to Balance object.\nIn the next line we declare an unowned variable balanceCopy , which will point to the same Balance object\nas the balance variable but will not change the object’s reference count.\nWhen we then assign nil to the balance variable, the Balance object is marked as zombie, so its\nmemory is not accessible and thus when we try to get the amount on balanceCopy we get a runtime error. When to use a strong reference This one is simple, given the examples that we have seen. It becomes clear that we should use a strong reference whenever we want to guarantee that we are always able to access the variable. This is specially true for things like object properties which should always exist during their owner’s lifetime. To reiterate, in our case, when we\nhave a Lender object we know he will definitely have a name and will have a balance, even if he has just\ncreated his account with £0.00. For more detailed examples check Apple’s examples 1 . So when strong reference is not advised The situation becomes more complicated when we start having bidirectional references between objects.\nSuch references are common in delegate pattern as we can see in the following example that uses\nthe LenderDepositDelegate to define a deposit operation. protocol LenderDepositDelegate : AnyObject { func deposit ( amount : DecimalNumber ) } class AvailableFundsViewController : UIViewController , LenderDepositDelegate { func deposit ( amount : DecimalNumber ) { lender . availableFunds = lender . availableFunds . adding ( amount ) } } class AvailableFundsView : UIView { var delegate : LenderDepositDelegate @IBAction func depositTenPounds ( _ sender : Any ) { delegate . deposit ( 10 ) } } Usually in this case the view receives some input event from the user and delegates the handling of that event to the controller (which is also the delegate in our case), and for doing that the view has a reference to the controller.\nThe problem is that the controller also has a reference to the view (otherwise the view would be immediately\ndeallocated after being initialized because its reference count would be zero). This creates a reference cycle as you can see in the diagram below. It clearly depicts a situation where, when strong reference is used, neither of the objects can be\ndeallocated because they always have at least one other object pointing to them. So it’s good practice in delegate pattern to never strongly reference the delegate, as we can see in the snippet\nbelow. class AvailableFundsView : UIView { weak var delegate : LenderDepositDelegate ? @IBAction func depositTenPounds ( _ sender : Any ) { delegate ? . deposit ( 10 ) } } By doing so, we break the reference cycle and allow for the AvailableFundsViewController to be deallocated\nwhen it’s no longer in use (e.g.: we navigate to a different controller). And that in turn leaves the AvailableFundsView with reference count zero and consequently makes it possible for the view to be deallocated\nas depicted in the following diagram. This is a very simple case though, in the real world things are much more complex to analyse, but this\nshould be enough to understand the general idea behind it. When to prefer weak over unowned As we have seen, there is no practical difference between weak and unowned in terms of the reference count\nof an object. The difference between the two lies in the mechanism that the Swift language uses to handle them.\nWhen we use the weak reference, Swift explicitly makes the variable an optional and doesn’t let us\nuse it unless we unwrap it. This creates a mandatory compile\ntime verification of the reference, meaning that we will not be allowed to use it without checking it first.\nThis obviously can be defeated by using force unwrap ! , which is generally discouraged unless we have\nsome kind of guarantee that the variable is pointing to a valid object.\nIn case of unowned reference, it is implicitly unwrapped, meaning that the reference will only be\nverified at runtime, which leads to a runtime crash if the reference points to deallocated object.\nThis mechanism makes the weak reference type the safe choice if we don’t want to have surprises\nwhen our program is running. So when should we use unowned ? The rule here is to use it iff we can guarantee that the lifecycle\nof the referenced object is equal or greater than the lifetime of the variable pointing to it. In that case\nwe know for sure that the object will not be deallocated and we can safely use it.\nLet’s adapt our previous example in order to demonstrate a possible usage of unowned property. class Balance { let amount : NSDecimalNumber unowned let lender : Lender init ( _ amount : NSDecimalNumber , lender : Lender ) { self . amount = amount self . lender = lender } } class Lender { let name : String var balance : Balance ? init ( aName : String ) { name = aName } } var lender = Lender ( name : Matt ) lender . balance = Balance ( 20.0 , aLender : lender ) In the example above it’s safe to declare balance owner Lender as unowned because we know that Lender will\nalways exist while Balance exists. In case lender is deallocated, the Balance gets deallocated as well.\nThis means it’s safe to use unowned since the Lender will always exist during the lifetime of Balance object. Although the unowned reference has a slight performance benefit over weak reference, it’s not\nsignificant for most of the projects out there.\nIn real scenarios it is very hard to guarantee the pre-condition of the referenced object being always available during\nthe lifecycle of the variable that is pointing to it, unless the relation between the entities is such that it\nguarantees this condition 1 . And even when it’s theoretically easy to prove such conditions,\nnothing prevents somebody from changing the code in a way that that condition does not hold anymore and\nwe unintentionally have a runtime crash. Recap We have seen that strong , weak and unowned each have their own clear use cases. The strong reference is useful\nfor declaring the attributes of an entity. The weak reference is a safe way to break reference cycles. As for unowned , it has a very specific use case, for when we know that the pointee’s lifecycle is at least as long\nas the pointer’s lifecycle. In real world though, we are always at risk of our assumptions being changed\nand no longer hold, leading our once safe code to a runtime crash. For this reason, weak reference is always\nsafer to use than unowned . Resources Apple provides a guide with examples of when each reference type makes sense and how to break reference\ncycles 1 Another useful reference is the Advanced Swift 4 book\nby Chris Eidhof , Ole Begemann and AirspeedVelocity Apple offers a good description about how the ARC works and they provide example scenarios for when\nto use each type of reference types. You can read about it here . ↩", "date": "2018-04-27,"},
{"website": "Funding-Circle", "title": "Highlights from Strange Loop 2018", "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/10/15/highlights-from-strange-loop-2018/", "abstract": "Strange Loop is an annual software conference based in St. Louis, Missouri that describes itself like so: …a multi-disciplinary conference that brings together the developers and thinkers building tomorrow’s technology in fields such as emerging languages, alternative databases, concurrency, distributed systems, security, and the web. Through consistent excellence over the years Strange Loop has become one of the most successful extant software conferences. It’s the favorite conference of some of our engineers, a few of whom have attended Strange Loop as often as they could going back to its second year in 2010. This year we sent a fairly sizable contingent to the conference, and they were generally pleased with their experiences and agreed that this was yet another successful year for Strange Loop. I asked the engineers who attended if any of them would be interested in sharing the talks that they’d recommend to their peers… and I got one volunteer. 😬 Ah well. I’ll go first. Avi I started my Strange Loop experience this year on the pre-conference day with the Software with a Mission (SWAM) mini-conference organized by Bridget Hillyer and Sean Marcia . I found the event inspiring and greatly encouraging. My two favorite talks were From Coder to Bureaucrat: How We Implemented the First Open Data Law by Becky Sweger and Kaitlin Devine , and Finding Happiness in Open Source by Sean Marcia. Both were inspiring and informative. My overall takeaway from SWAM was that I’m not alone in my idealism and that it really is possible to work towards aligning one’s idealism and career. Being in that packed room was truly, deeply encouraging, and I hope this becomes the first SWAM event of many. As for the main event, the talks that made the strongest impressions on me were: The Hard Parts of Open Source by Evan Czaplicki was a fascinating analytical and historical breakdown of the communication patterns that tend to recur in FLOSS project communities, wherein Czaplicki compellingly traced and constructed two major literary-intellectual threads from the past to the present. That alone would have made for a satisfying and thought-provoking talk, but then he also shared some fascinating forward-looking ideas of his own on how we might make communication in these communities more constructive and effective. Changing the World by Erica Joy Baker was one of those big-picture, take a step back and (re-)frame Everything kind of talks — I just love this sort of thing. Her talk was all about inspiring, energizing, and exhorting the audience to be both ambitious and diligent, and to be more conscious of whether, when, and how much to compromise on our ideals. I fear I won’t do it justice by a quick summary, but I’ll do my best: Baker pointed out that “changing the world” has become a staple of Silicon Valley rhetoric, a rallying cry used to get people excited and psyched to do Big Things. But, said Baker, in our excitement — and greed — we forgot to “finish the sentence” — we forgot to be sure that we’re changing the world for the better, for everyone . So let’s stop forgetting that — let’s stand up for what’s right; let’s each and everyone one of us insist, however we can manage in our individual circumstances, to ensure that we are changing the world for the better, for everyone . I really needed this talk, and I’m wholeheartedly +1 on every bit of it. Understanding TypeScript’s Structural Type System by Drew Colthorp was the biggest surprise for me this year. I’ve had a positive impression of TypeScript for years, despite never having worked with it — any project that furthers the cause of gradual typing has my (moral) support. That’s what led me to take a look at the talk’s description, despite my assumption that the talk wouldn’t interest me — I’m generally disinterested in type systems. But the excellent description piqued my interest and propelled me to give the talk a try — still mostly a lark, but I make it a point to take some chances at every conference. So I was surprised and delighted when this turned out to be an exciting, cogent, impressive, and informative talk. Not only did I learn a whole lot about how TypeScript’s type system works, with grounded, realistic examples — I also intriguingly observed many similarities between that system and Clojure’s Spec library . Finally, I thoroughly enjoyed the presentation of this presentation. It was fast-paced and information-dense, but meticulously crafted with many, many examples that helped me ingest a lot of information quickly and successfully. I think I’ll be re-watching this one at least a few times in the hopes that maybe I can integrate some of these techniques into my own talks. Two quick honorable mentions: first to Freeing the Software That Runs Our Elections by Roan Kattouw for giving me a glimmer of hope that we might be able to wrest our democracy in the United States from the corrosive grasp of the greedy, corrupt elections technology industry. Second: Algorithms for DNA Data Storage by Ruthie Nachmany for just plain old blowing my mind. A few talks I wanted to see but missed, and will be catching via video: Zero Downtime Migrations of Stateful Systems by Sangeeta Handa , Towards Language Support for Distributed Systems by Heather Miller , and Justice for Sale by Brittany Wald . Finally, I did have one minor disappointment with this year’s Strange Loop: there was a dearth of talks on documentation. This is something I’d like to see fixed… maybe I’ll even try to do something about it next year 😅. Paco Pre-Conference I chose the Day of Datomic Cloud pre-conference event. I’ve always been drawn to the Datomic architecture and its “too good to be true” features. I’ve played around with Datomic for a few years now. I’ve mostly used it for toy projects, mostly as a vessel to learn Datalog . My main problem with Datomic is that even with the “free” offering it was weird and cumbersome to use. I am surprised to hear there is a lot of actual Production use. It seems to require so much buy-in from all stakeholders. I don’t want to dwell too much into the reasons to pick Datomic or not. What certainly impressed me way more than I had expected is the Cloud offering that Cognitect has concocted. Being able to just press a button to get a fully managed and scalable storage system is quite cool but there is so much more under the hood. The main thing I’d like to focus on is Ions . Ions enables you to run Clojure services within your Datomic cluster and seamlessly integrate them through AWS Lambda functions. It provides a full programming model and workflow to develop, deploy and integrate with Clojure-friendly tooling. I believe that Datomic Cloud + Ions delivers a really compelling development environment for new full-stack Clojure apps or even internal systems. Main Conference Chasing the Stream Processing Utopia by Kartik Paramasivam was a talk that I found very relevant to my work at Funding Circle. My team is building next generation apps for a better financial world and naturally stream processing is something we do. Not only do we use it as part of our stack but it’s quite central to our new architecture, allowing us to use Apache Kafka as our system of record. This talk gave me a great overview of the challenges around stream processing centered around the Apache Samza framework. Paramasivam outlined the various streaming application patterns supported by Samza as well as challenges related to running these topologies in production and at scale. My main take-away was that stream processing is still very much the “wild west” of application programming paradigms. I see this reflected in our use of Kafka Streams and the Confluent Platform wherein we experience the same challenges outlined in this talk sans the massive scale LinkedIn operates at. I’m excited for the future of stream processing and am happy to work in this field with fresh challenges and boundaries that need pushing. Whether you’re just curious about stream processing or are already “knee deep” this is a good talk for you!", "date": "2018-10-15,"},
{"website": "Funding-Circle", "title": "Using Firebase alongside Google Analytics on Android", "author": ["Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/08/05/firebase-and-google-analytics/", "abstract": "During the last I/O , Google announced their new super product: Firebase Analytics . Firebase, when compared to Google Analytics, is a huge improvement for cross-platform mobile analytics, crash reports, marketing campaigns coordination, mobile notifications and other useful features. Given the amount of new useful goodies, it’s obvious why many teams want to move to Firebase ASAP. The only problem lies in a small but very important detail: you can not migrate your analytics data from Google Analytics to Firebase :(. It means that if you decide to move to Firebase you will lose all your historical analytics data and won’t be able to leverage it to improve your product. There is however a way to have the best of both worlds. A reasonable solution for the problem, appointed by Google, is to setup Firebase with Google Tag Manager (Tag Manager) for your app and use Tag Manager to “replay” the events to Google Analytics. This will allow you to take advantage of the new Firebase features and keep the important Google Analytics historical data and realtime features. In this post we will see how to setup some basic Screen View tracking and Custom User Events tracking which send data to Firebase and Google Analytics allowing you to keep your Google Analytics data and at the same time start enjoying the shiny features from Firebase. What is Google Tag Manager? Google Tag Manager can be thought of as an event listener. In summary, it receives all the events from your application and redirects them to other systems, but it also acts like a data store in which you can store variables that change your application runtime behaviour. This means at least two things: It acts like a proxy to the analytics systems, allowing the user to process the events in the most appropriate way (blocking some events, or resending to multiple tracking systems) It allows to change application runtime (e.g. change available features) using these variables without having to redeploy the application The important point for this post is the first, given that we are interested in “intercepting” the Firebase events and resending them to Google Analytics. Firebase and Tag Manager setup Adding the Firebase and Tag Manager libraries to your application is a simple process. It is well documented in the link referenced above so you can follow the instructions to set it up in the application. Configuring Screen Views Using Google Analytics you probably had something similar to the following to track page views: GoogleAnalytics analytics = GoogleAnalytics . getInstance ( this ); tracker = analytics . newTracker ( R . xml . global_tracker ); tracker . setScreenName ( MyActivity . class . getSimpleName ()); tracker . send ( new HitBuilders . ScreenViewBuilder (). build ()); To do the equivalent in Firebase Analytics you would write something like: mFirebaseAnalytics = FirebaseAnalytics . getInstance ( this ); Bundle screenView = new Bundle (); screenView . putString ( FirebaseAnalytics . Param . ITEM_NAME , MyActivity . class . getSimpleName () ); mFirebaseAnalytics . logEvent ( FirebaseAnalytics . Event . VIEW_ITEM , screenView ); Now what we need to do to make this screen view available in Google Analytics is to create a trigger in Tag Manager which extracts the data logged by Firebase and send that same data to Google Analytics.\nWe start by creating a new Tag and choosing Google Analytics as the product. The next step is to create a new variable of type Constant holding the Google Analytics Tracking ID, let’s call it GA Tracking ID . Now we select Screen View for Track Type and expand More settings and Fields to Set . This field will hold the screen name for our screen view event. For the Field Name we select screenName from the dropdown and for the Value we will create a new variable of type Event Parameter . Let’s call the variable Item Name and select view_item for the Suggested Event and item_name as Event Parameter . We can also set a default value in case our event doesn’t bring data in item_name parameter (I just set it to “not set” as usually seen in Google Analytics). Once those are in place, we can create a trigger which will be the mechanism that intercepts the type of event we are interested in. For that, in the Fire On section, we select Custom trigger type. After the trigger is created, the only thing left is to confirm the tag creation. And we have a complete integration for Screen View events in Firebase and Google Analytics using Tag Manager as the middleman :). Configuring Custom User Events Similarly to screen views, to track user actions with Google Analytics, you would have something like: tracker . send ( new HitBuilders . EventBuilder () . setCategory ( \"my_category\" ) . setAction ( \"my_action\" ) . build () ); To do the equivalent in Firebase: Bundle eventAction = new Bundle (); eventAction . putString ( FirebaseAnalytics . Param . CONTENT_TYPE , \"my_category\" ); eventAction . putString ( FirebaseAnalytics . Param . ITEM_ID , \"my_action\" ); mFirebaseAnalytics . logEvent ( FirebaseAnalytics . Event . SELECT_CONTENT , eventAction ); The configuration in Tag Manager for custom events is similar to what we just did with the screen view event. We start by creating a new Tag for Google Analytics, and we can reuse the GA Tracking ID . The tracking type in this case has to be Event and we will create two new variables, one for event category and another one for the event action.\nThe following picture shows the creation of the category variable of type Event Parameter using the Firebase suggested parameters select_content for the event type and content_type for the event parameter. To track the event action we create another variable using the same variable type and event type but this time we use the item_id for the event_parameter field. The last step is to create a Custom Trigger in the Fire On section. We set the trigger to fire when the event name is select_content The overall Tag should look like the following Now we can finally publish the container and download the JSON file to use in the app.\nThis configuration should allow you to track the screen views and user events both in Firebase and Google Analytics. Build and run your app to check that you can see the correct screen views and events in Google Analytics realtime section. Hope this helps you to get the benefit from both Firebase and Google Analytics. Happy tracking! Things to bear in mind Google Analytics library can be removed com.google.android.gms:play-services-analytics You might need to update Google Play Services and Google Repository using Android SDK Manager in order to have access to Firebase and Tag Manager libraries. Make sure you publish the container after creating the tags before you download the container JSON file. Make sure you choose the correct type of container in Tag Manager. There are two type of containers: Legacy Android and Firebase Android . The one we have to choose is Firebase Android , as shown in the following screenshot. Acknowledgements Many thanks to Edu Caselles and Dennis Ideler for the reviews :) Notes The first version of this post was originally published on the authors page .", "date": "2016-08-5,"},
{"website": "Funding-Circle", "title": "Partial Application in JavaScript using Bind", "author": ["Patrick Orrell"], "link": "https://engineering.fundingcircle.com/blog/2015/03/12/partial-application-js/", "abstract": "JavaScript is a multi-paradigm language, but its functional aspect is predominant. One feature of a functional language is the concept of partial application. This refers to supplying an incomplete argument set to a function to return a function with those arguments pre-filled. In JavaScript this can be achieved several ways, but one of the most convenient is by the use of bind() . bind() was added in ES5 as a Function prototype method, meaning all functions prototypically inherit it. The form of this function is: Function . prototype . bind ( scope , args ...) The purpose of bind is to bind a scope to the this keyword, thus the function scope. This is the first argument of bind. A function created by bind() cannot be bound itself. var objectA = { x : 5 , getX : function getX () { return this . x ; } } var objectB = { x : 10 } var boundFunction = objectA . getX . bind ( objectB ); > objectA . getX (); 5 > boundFunction (); 10 Everything after the first argument is an argument that you want to pass to the function, from left to right. var objectA = { x : 5 , getXWithLabel : function getXWithLabel ( label ) { return label + this . x ; } } var objectB = { x : 10 } var boundFunction = objectA . getXWithLabel . bind ( objectB , 'From Object B: ' ); > objectA . getXWithLabel ( 'From Object A: ' ); 'From Object A: 5' > boundFunction (); 'From Object B: 10' Here we can see boundFunction has not only had its this value bound to objectB , but also its argument bound to From Object B: . It is this feature of bind that lets you use partial application in ES5+ functions. The key aspect about these bound arguments is that you do not need to provide all the arguments: you can partially apply them, leaving the rest to be provided when the function is called. Let us take the function of form: function sum ( x , y ) { return x + y ; } > sum ( 10 , 5 ); 15 Let us say we wish to perform a large number of sums where we simply add 10. We can use partial application to create a new function where the first argument is already filled in with 10. In this case we do not want to rebind this , so we simply pass null and it is left alone. var sum10 = sum . bind ( null , 10 ); > sum10 ( 5 ); 15 > sum10 ( 10 ); 20 We can see here that the arguments have been partially applied , with x being ‘pre-filled’, leaving y to be provided. Note that the arguments you pass to bind move from left to right . Here is a function with three parameters: function triple ( x , y , z ) { return x + y + z ; } // PartialTriple now contains triple with x and y assigned to 10 and 11 var partialTriple = triple . bind ( null , 10 , 11 ); One useful practical application of this method is in pre-created callback functions, which allow you to ‘pass a variable to a callback’. function errorMessage ( serviceName , err ) { console . log ( 'Error ' + err . msg + ' occured in ' + serviceName ); } // Meanwhile in the httpService var httpErrorHandler = errorMessage . bind ( null , 'HTTPService' ); httpLibrary . httpCall ( httpSuccessHandler , httpErrorHandler ); //Or depending on your views on readability: httpLibrary . httpCall ( httpSuccessHandler , errorMessage . bind ( null , 'HTTPService' )); This shows the very useful and elegant usage of partial application. This enables you to create pure functions, which are more robust and less likely to leak memory via accidental closure.", "date": "2015-03-12,"},
{"website": "Funding-Circle", "title": "CodeCraft Lives", "author": ["Andy Mendelsohn"], "link": "https://engineering.fundingcircle.com/blog/2014/03/10/codecraft/", "abstract": "Five months ago it was an idea, but now it is a reality: CodeCraft is alive and running. Eight students intensively training for 12 weeks, learning web application software development and practices from the ground up. Learning the craft of software development in a commercial environment, with experienced developers teaching students how to write code. It’s an apprenticeship of sorts with learning that incorporates real-life, paid, professional practice and experience. We would have a better photo than this, except the codecrafters have all left for lrug ! The course is designed and run by Mal Pinder who, as well as being a great practitioner, is a great teacher and has already been running a regular in-house Ruby evening class. But, we’ve also encouraged every member of the development team to contribute a few hours of their time to presenting a class. Three members of the tech team have already contributed 1-2 hours of class teaching, presenting classes on ‘the apprentice pattern’, Javascript, and HTML & CSS. We have all levels of expereince getting involved in the teaching: the trainers so far have comprised of one Junior/Mid, one Mid/Senior, and our UK VP of Engineering. Hopefully this is a trend that will continue over the length of the course - I will be running a class next week as will Helene, our TPM (Technical Project Manager). Having our own practitioners as trainers doesn’t just benefit the course students, it also benefits the trainers. If you want to get better at something, start teaching it to someone else. to quote the oft-quoted: ‘the best way to learn is to teach’. That’s what’s so cool about Funding Circle. Come up with something that people believe ‘has legs’, and we are willing to try it.\nAnd, it’s obvious that the idea and concept of CodeCraft has legs, and it’s obvious where it works for us as a tech business and it directly enhances the Tech Team. Should we have to do this? Should we be able to find good developers without having to nurture and train them ourselves? Maybe. There are a whole ton of arguments about this. Quite clearly we (as a society) weren’t educating enough young people from infants right through to university level, in way that would fit current demand for these skills. And maybe there wasn’t enough cultural drive or peer-group social encouragement. Has there ever been? Perhaps it’s just a lack of foresight, or the fact that we are in such a fast-moving industry and opportunities have been missed, and it’s simply that the rush of businesses to be part of the ‘tech-city’ revolution has sucked the life out of the developer market. But this is the “Year of Code” , and the drive is on to better educate our young people to help feed our technological growth and digital creation needs; to get people who’ve never coded into code; to open that door of opportunity. Whilst this is a great initiative, many of the “Year of Code” initiatives aren’t going to help us today, right now, when we are so hungry for skills. So, rather than wait for the inevitable bearing of fruit, this is our contribution to the initiative because we are a Giant that needs feeding! I believe that there is great value in us giving back to the community in this way. It feels good to teach and many of us are self-taught, having learnt from sources as diverse as magazine listings, other developers, blogs and conferences (and of course, books). We want to be a centre of excellence and learning and for our growth as a tech team to be organic, maintainable and sustainable, or at the very least be highly strategic. I often compare managing a development team to managing a football team. It’s about finding the right players and putting them in the right positions and then getting them to work as a team - and it’s about depth of quality in your squad and being able to mix things up, changing them about to get the best result. And, to continue the metaphor, many of the best, most successful football teams have academies. With an academy, the team has the ability to train up players as they’d like them to be trained, identify and nurture talent, and then, select the best. At the same time, the trainee is provided with an environment that enables them to ‘learn from the pros’; to learn what they don’t know; to pursue excellence; to find out if they have what it takes and the kind of brain that does well at this. It’s a win-win situation. If the trainee isn’t quite what the team are looking for, then they’ve gained some further education and taken another step towards becoming a professional should they still want to. And, that’s as far as we’ll go with the football metaphor. I’m no Alex Ferguson, and there are no hairdryer treatments in our tech team. We don’t rule by fear (like anyone would ever be scared of me ;). The Funding Circle Tech Team are a group of smart, valuable, highly valued and respected people and I love ‘em all! And I believe CodeCraft, and similar initiatives, work for every member of the team - they all have a vested interest in our success and in working in a great environment, with smart, well-educated people. It’s the ‘want to go to work’ factor. Stuff like this helps. Will it work? I am confident it is the right thing to try for Funding Circle and the London tech community. I am confident that it will work. I’ll keep you informed.", "date": "2014-03-10,"},
{"website": "Funding-Circle", "title": "How to reinvent containers (with one weird trick)", "author": ["jspc"], "link": "https://engineering.fundingcircle.com/blog/2015/03/12/navvy/", "abstract": "Containers are at once both simple and difficult to grok. One lunchtime, while trying to understand the internals of docker, I accidentally created a container standard which must never, ever see the light of production. I called it Navvy . Linux containers are, in their purest form, made up of three technologies: chroot , cgroups and network namespaces . Some of these technologies have been around since year dot. Some of these technologies are from the last couple of years. The toolset chroot jails chroot as a concept/ system call initially appeared in V7 Unix and was first widely used in 4.2bsd in 1983. The system call, in its purest sense, changes where a process’ apparent root directory is. To the process, however, the root directory is, seemingly, still at / . This solves two problems: Isolating installations of files to preserve the Filesystem Hierarchy Standard Preventing a process accessing files outside of its scope chroot is commonly used in systems maintenance and bootstrapping distributions such as gentoo (because, let’s face it, if you’re getting deep into the internals of containerisation you almost certainly have an old tiny conputers pentium III box somewhere with an ancient gentoo installation on it). cgroups cgroups are cool. In 2006 a pair of Google engineers started a project called process containers with the purpose of throttling processes at the kernel level. The name was shortly after changed to Linux Control Groups (cgroups) because of the confusion and ambiguity around the word container in kernel-space. cgroups allow us to limit the resources a process can run. Because UNIX processes are hierarchical, children of the cgroup ‘d process are in the same cgroup and will contribute to the same limit. network namespaces network namespaces are, in effect, another distinct copy of the network stack; they have their own routes, firewall rules, devices and magic. This allows for several interesting things: Separating traffic destined for one container from others Traffic shaping at the container level The ability to route processes differently in different circumstances Linux has had this technology as part of netns (another Google project) since 2.6.27 ; which makes network namespaces the youngest member of the container toolchain. Containerising In order to play with containers we need an operating system to run. The following examples use a centos 7 container running on Arch linux. The best way to start is to unpack the filesystem from a livecd into a directory: $ mkdir centos7 squash root navvy-1 $ wget -q http://centos.openitc.uk/7.0.1406/isos/x86_64/CentOS-7.0-1406-x86_64-DVD.iso $ sudo mount -o loop,ro CentOS-7.0-1406-x86_64-DVD.iso centos7/ $ sudo mount -o loop,ro centos7/LiveOS/squashfs.img squashfs/ $ sudo mount -o loop,ro squashfs/LiveOS/rootfs.img root/ $ sudo cp -aR root/ * navvy-1/ $ sudo umount root/ $ sudo umount squashfs/ $ sudo umount centos7/ $ rm -rf centos7 squash root CentOS-7.0-1406-x86_64-DVD.iso We can use this version of Centos 7 now by issuing a simple $ sudo chroot navvy-1/ /bin/bash But this will, essentially, mean: We could run stuff here that steals resources from other resources on this machine We’re sharing the entirety of the network Instead we’re going to setup some cgroup config around memory and CPU. We do this with: $ sudo cgcreate -a jspc -g memory,cpu:navvy-1 Which gives us, for instance: $ ls -l /sys/fs/cgroup/memory/navvy-1/\ntotal 0 -rw-r--r-- 1 jspc root 0 Mar 12 15:13 cgroup.clone_children --w--w--w- 1 jspc root 0 Mar 12 15:13 cgroup.event_control -rw-r--r-- 1 jspc root 0 Mar 12 15:13 cgroup.procs -rw-r--r-- 1 jspc root 0 Mar 12 15:13 memory.failcnt --w------- 1 jspc root 0 Mar 12 15:13 memory.force_empty -rw-r--r-- 1 jspc root 0 Mar 12 15:13 memory.limit_in_bytes -rw-r--r-- 1 jspc root 0 Mar 12 15:13 memory.max_usage_in_bytes -rw-r--r-- 1 jspc root 0 Mar 12 15:13 memory.move_charge_at_immigrate We’re going to set a limit of 1GB, and 10% of the CPU. Conceptually, it would appear that by putting a container into this group and setting a per-process limit of 1GB we can get around this limitation by using many processes in this container. This is not an issue due to the way chroot , and indeed UNIX, works. Processes inherit resource pools and namespaces from their parent; because init is generally run with All The Resources available processes aren’t throttled until specifically set. When we put the chroot process under a cgroup we limit the pool that both it and its children have access to. Thus 1GB and 10% for the container. $ echo 1000000000 > /sys/fs/cgroup/memory/navvy-1/memory.limit_in_bytes $ echo 102 > /sys/fs/cgroup/cpu/navvy-1/cpu.shares n.b.: groups have 1024 shares each which cascade. Because navvy-1 is a top level cgroup 102 equates to ca. 10% To chroot with this limit we simply run our command as per: $ sudo cgexec -g memory,cpuset:navvy-1  chroot navvy-1/ /bin/bash The final task is to create a new network stack: $ sudo ip netns add navvy-1 $ sudo ip link add veth0 type veth peer name veth1 $ sudo ip link set veth1 netns navvy-1 $ sudo ip netns exec navvy-1 ifconfig veth1 10.10.10.10/24 up And to then bridge veth0 to wherever your network lives (which will vary per platform). We can then finally use our container as per: $ sudo ip netns exec navvy-1 cgexec -g memory,cpuset:navvy-1  chroot navvy-1/ /bin/bash But what does it all mean? Containers are conceptually simple to grok; they separate memory, CPU and networking off and jail a process to its own set of binaries. Easy. In practice, though, they can get difficult to run outside of an all encompassing tool such as docker or even the CoreOS offering rocket . Ultimately, though, through understanding the underlying technologies and toolchains, and how they control our containers, we can start to piece together interesting solutions to problems. Having access to the cgroup layer, for instance, means we could scale up or down an application’s access to memory as we go along; or to even start routing an application’s traffic via another host to start snarfing data.", "date": "2015-03-12,"},
{"website": "Funding-Circle", "title": "The FC4 Framework", "author": ["Avi Flax "], "link": "https://engineering.fundingcircle.com/blog/2018/09/07/the-fc4-framework/", "abstract": "I love software architecture diagrams. Good examples harness the strengths of both linguistic and visual communication to reach pinnacles of precision, clarity, and richness. I even love bad diagrams — which is most of them — for the earnest optimism they convey, that this medium of entities, labels, relationships, and composition has some magical ability to convey the Truth. I love making architecture diagrams too. I love almost everything about it — the creativity, the research, the work of naming and composing the elements, even styling. But then there are the tools. I’ve been perpetually dissatisfied by diagramming tools for years, for a bunch of reasons: Most of them: Are GUI-driven and therefore fiddly I’ve spent many hours fiddling with diagrams to get all the elements aligned just so. It was all a poor use of my time. Use proprietary binary file formats that encode the graphics as graphics Support no textual representation of diagrams Lack a comprehensive, opinionated, conceptual model of how to approach software diagramming All these dissatisfactions but the first are rooted in my drive to scale this work beyond myself, beyond individual authorship. I want to collaborate with my colleagues on these diagrams, in depth and over time. I want this work to have lasting value and impact beyond the contributions and tenure of any individual. As software designers and developers, we already have rich, robust tools and workflows to facilitate and support collaboration via files: version control systems (VCSs) such as Git and Mercurial and collaboration systems built on those VCSs such as GitHub , GitLab , BitBucket , etc. These systems, however, are optimized for collaborating on text files; because most diagramming tools use binary file formats and support no textual representation of diagrams it’s challenging and awkward to collaborate on such diagrams using most popular version control systems. The few tools that do have a textual representation and a command-driven rendering process — the most prominent being Graphviz — have never felt like a good fit for diagramming software architectures. Graphviz, for example, being focused on graphs, makes composition and nesting quite awkward, and supports only algorithmic layout when rendering diagrams, making it very difficult to tell a coherent story with the relative positioning of elements. Whenever I tried Graphviz, I’d find myself wrestling with its algorithms just to get things laid out clearly and usefully. Then there’s the lack of a conceptual model for how to author architecture diagrams. Without this, it’s extremely inefficient to onboard new collaborators — the implicit model needs to be described during in-person training sessions during which the more experienced authors attempt to extricate implicit rules and guidance from their fingertips, implicit knowledge that’s developed with time and is hard to transfer. This is not a criticism of any of these diagramming tools for not meeting my requirements. They’re general-purpose diagramming tools, and general-purpose tools have plenty of value. It’s just that, by definition, such general-purpose tools are not optimized for my specific needs. Despite my dissatisfaction with every diagramming tool I’d ever used, whenever the time came to create a new corpus of diagrams, I’d always try and fail to find a newer and better tool. So with a sigh and slumped shoulders I’d resign myself to using one of the GUI tools. So when I joined Funding Circle in October 2017 and observed that we needed some big-picture architecture diagrams of all our systems, I tried a bunch of different tools — OmniGraffle, LucidChart, Graphviz, Keynote, and even HTML+CSS. But I remained dissatisfied with all of them. Then, just as I was on the cusp of proceeding resignedly with one of the usual suspects, I stumbled across Simon Brown’s C4 Model for Software Architecture and the tools he created to support it, Structurizr and Structurizr Express — and I was delighted. As I read the docs, played with the tools, and watched some of Brown’s talks I felt a growing frisson of recognition. Brown and I had observed many of the same problems with the approach and tools commonly employed to create software architecture diagrams — but he had done something about it, something substantial and impressive. Although Brown and I had never spoken nor corresponded, I felt a powerful kinship with him and his work. Brown recently wrote an excellent introduction to the C4 model in which he states: C4 stands for context, containers, components, and code — a set of hierarchical diagrams that you can use to describe your software architecture at different zoom levels, each useful for different audiences.  Think of it as Google Maps for your code. To create these maps of your code, you first need a common set of abstractions to create a ubiquitous language to use to describe the static structure of a software system. The C4 model considers the static structures of a software system in terms of containers (applications, data stores, microservices, etc.), components , and code . It also considers the people who use the software systems that we build. I wrote above that a major problem with all diagramming tools I’ve used to create architecture diagrams is that they lack a comprehensive and opinionated conceptual model of how to approach software architecture diagramming. Well, that’s exactly what C4 is. Brown recognized this need and also realized that this model would be most useful if it were abstract — independent any particular implementation or toolset. That’s why, I surmise, he gave it a name and its own identity, independent of the tools he created to support it. Just about any diagramming tool can be used to create C4 diagrams, so anyone can create C4 diagrams with the tools with which they’re already comfortable or perhaps required to use. The C4 model alone is impressive and extremely promising, but Brown didn’t stop there. He also created Structurizr, a commercial Web app for authoring, editing, hosting, publishing, and viewing C4 diagrams. Structurizr is available as a hosted service or for on-premises installations. Structurizr has a free plan that’s quite useful, but Brown also released Structurizr Express , a single-page Web app that allows you to create a single standalone C4 diagram by defining it as text. Just to explain that distinction a bit: Structurizr hosts workspaces ; each workspace is intended to host a full model of your software ecosystem — all the systems, components, containers, people, and all the relationships between them — and a set of views “over” those elements. Those models and views are supposed to be defined and populated programmatically via code. Brown has published a cogent overview of why Structurizr encourages you to use code as a way to create software architecture models. But when I decided to try out the C4 model, I chose to start with Structurizr Express and its data-driven approach. It seemed simpler and easier, and jived with my enthusiasm for functional programming, which, as per Dmitri Sotnikov “…brings data to the forefront and it encourages us to think about our problems in terms of data transformations.” In mid-December 2017 I created my first C4 diagram: a System Landscape diagram of Funding Circle’s UK platform. My initial impressions of C4 and Structurizr Express were very positive. Here’s a blurry export of that diagram: I had only one quibble: whenever I would paste my diagram’s source YAML into Structurizr Express in order to edit the diagram graphically, Structurizr Express would regenerate the YAML and make it, to be blunt, very ugly. For example, this is a clean YAML representation of a contrived System Landscape diagram: type : System Landscape scope : Acme inc. description : Big-picture diagram showing all our systems. elements : - type : Person name : A tags : foo, bar position : ' 125,50' - type : Software System name : B position : ' 800,100' relationships : - source : A description : interacts with destination : B styles : - type : element tag : Person shape : Person size : A6_Landscape And here’s what that same diagram looks like after it’s been edited with Structurizr Express: elements : - type : Person name : A description : \" \" tags : ' foo, bar' position : ' 125,50' - type : ' Software System' name : B description : \" \" tags : \" \" position : ' 800,100' containers : [] relationships : - source : A description : ' interacts with' technology : \" \" destination : B tags : \" \" styles : - type : element tag : Person width : \" \" height : \" \" background : \" \" color : \" \" border : \" \" opacity : \" \" fontSize : \" \" shape : Person metadata : \" \" description : \" \" type : ' System Landscape' scope : ' Acme inc.' description : ' Big-picture diagram showing all our systems.' size : A6_Landscape As you can see, Structurizr Express adds all missing properties to each object, even those that are optional and have empty/blank default values. It also reorders the root entries and wraps extraneous quotes around most strings. I find the second example ugly and hard to read. Aside from this one thorn in my side, I was enjoying Structurizr Express and wanted to keep using it. Luckily, I’m a software developer, so it occurred to me that I might be able to solve this problem for myself, with code. So I opened up my terminal and whipped up a little tool to clean up and reorder Structurizr Express YAML documents. I called it Restructurizr. With the addition of Restructurizr I had all the ingredients I needed to really dig in and get to know the C4 Model and Structurizr Express, by creating the diagrams we needed within Funding Circle. As I did so, I realized that Restructurizr provided an additional benefit: it made peer review of the diagram source more viable. Making the YAML documents smaller, giving them simpler, more consistent formatting, and a stable sort (of both properties and values) meant that diffs were nearly guaranteed to show only meaningful differences. As anyone who’s ever had to review a noisy diff knows, the added noise can really slow down review and make it plain annoying. I was excited when I realized this; I find enabling and supporting collaboration to be compelling and fulfilling. As I used C4, Structurizr Express, and Restructurizr in collaboration with my coworkers to create, edit, review, revise, and publish software architecture diagrams, I also worked to share what we were up to more broadly across Product Engineering. I wanted to encourage more folks to create and publish diagrams, both because we needed them and to elicit feedback on the approach and the tools. As more people started using the model and tools it became clear that we needed to document guidance and guidelines on using them. As comprehensive as C4 and Structurizr Express are, they’re still somewhat general; any organization or team that adopts them will need to make and document some shared decisions as to semantics, workflow, etc. So we started documenting all that in our internal wiki. Over just a month or two, the guidance and guidelines became so comprehensive and thorough that I started to think of them as a methodology. Then one day I found myself at my synagogue’s kiddush , enthusiastically explaining to my friend Ilan what I was working on. When he said he’d love to check it out and maybe give it a try, I got real excited, and thought — hell yeah, why shouldn’t this all be FLOSS ‽ None of it is specific to Funding Circle, so it didn’t need to be proprietary. My only hesitation, I told Ilan, was that I wasn’t sure how to describe it all. I had a methodology and a tool, built on a conceptual model (C4) and another tool (Structurizr Express) — that was all quite a mouthful. Luckily Ilan is smarter than me — he immediately said something like “a methodology plus a tool — sounds like a framework to me.” I love that guy. Now that I had a term to describe this thing, it lacked only a name before I could unleash it on the world. Being the lazy geek that I am, and wanting to reference the C4 model, I decided to combine FC (for Funding Circle) and C4 into FC4. (I actually considered FC²4/FC24 but my colleagues thankfully talked me out of it.) Thus was named the FC4 Framework , which I first released on 9 March 2018. Since then we’ve seen steadily increasing interest and adoption of FC4 within Funding Circle, and even a little elsewhere . We’ve continued to refine and improve the framework and we don’t see that stopping any time soon — we see many opportunities to level it up significantly, such as a linting feature, automated rendering, a streamlined workflow, and possibly a new data scheme that’d decouple the static models of software systems and the views of those models. If you have a chance to give FC4 a try, please let us know what you think!", "date": "2018-09-7,"},
{"website": "Funding-Circle", "title": "Our Values - Readability", "author": ["Amy Chen"], "link": "https://engineering.fundingcircle.com/blog/2015/02/02/our-values-readability/", "abstract": "When I look at code, the first thing that pops into my head is “Is this readable?”  Can I quickly glance\nat a class or a method and understand what it does?  Is the interface clear, are side effects obvious,\nand are there consistent patterns that tell other developers how to use the software? We value readability, both in our source code and in our tests.  Cogent tests express the intent of the code,\nwhereas verbose tests with a lot of set up muddle the intent.  The beauty of TDD is that it enables you\nto write the code that you wish you had. We pair program, which lets us propagate patterns throughout the code base.  As we build a feature, we’ll often take\non minor refactorings to make the code more consistent.  The problem is that not all refactoring is “minor.”\nA convenient way to categorize refactorings is in units of time – is this a refactor that will take minutes, hours,\ndays, or weeks?  The ones that take minutes/hours are no-brainers and should be done; the ones that take days/weeks\noften involve discussion and need to be balanced with the priorities of the business. When I say that we value readability, I’m not talking about code that is pretty for pretty’s sake. The goal is not\nto stare at our code and sigh “ooooh.” The goal is to have code that satisfies existing business needs and can be\neasily adapted for new features.  Code that is simple and readable has a lower cost of change – there’s less cognitive\noverhead and less time spent untangling what something does.  Our goal is to make the cost of change cheap so that\nwe can quickly respond to business needs.", "date": "2015-02-2,"},
{"website": "Funding-Circle", "title": "Many ways to write a unit test", "author": ["Danny Olson"], "link": "https://engineering.fundingcircle.com/blog/2015/05/20/many-ways-to-write-a-unit-test/", "abstract": "Wherein we explore multiple ways to write a unit test, and further, discovering an alternative that more fully expresses the intent of the code. We all practice test-driven development (right?). We write a failing test, make it pass, refactor our code, and repeat until the code does what we want it to do . We now have regression tests to know if any changes break existing functionality, and we have documentation for the functionality we want. This means that a new developer — or us in a week — can reason about the code, and therefore know how to effectively use it. Given these benefits, and given that regression tests are easy to check (just run them!), we should optimize our tests for readability and maintainability. This means refactoring our test code in addition to our application code. Here’s an example to show how and why. Example An investor can view available listings in our marketplace to decide which one to invest in. The listing domain model can go through a few states, depending on the current time compared to its attributes: pending — when the current time is before the preview date preview — when the current time is after the preview date live — when the current time is after the live date expired — when the current time is after the expiration date How do we test these various state changes? Writing the tests first would have us create an instance with the relevant attributes set to certain times and then asserting that the state is correct. describe \"#status\" do describe \"when after the expiration date\" do let ( :listing ) { Fabricate . build ( :listing , expiration_date: 1 . day . ago ) } it \"is expired\" do expect ( listing . status ). to eq ( :expired ) end end end We would then write a test for each status, and voilà, test-driven development. Next steps Now that we have passing tests, we can refactor our code and know that it still works, as long as the tests are green. But what about our tests? Do we ever want to change them to reflect the code? In our case, what we’re trying to communicate is that a listing changes its state as time progresses, but our tests don’t show that — they are almost context-free since they only care about a specific moment in time. That’s usually a good thing, but we can do better to show our coworkers, and our future selves, what the code is trying to tell us. An alternative approach describe \"#status\" do let ( :preview_date ) { 7 . days . ago } let ( :live_date ) { 3 . days . from_now } let ( :expiration_date ) { 7 . days . from_now } let ( :listing ) { Fabricate . build ( :listing , preview_date: preview_date , live_date: live_date , expiration_date: expiration_date ) } it \"starts in pending\" do Timecop . travel ( preview_date - 1 . day ) expect ( listing . status ). to eq ( :pending ) end it \"transitions to preview after pending\" do Timecop . travel ( preview_date ) expect ( listing . status ). to eq ( :preview ) end it \"transitions to live after preview\" do Timecop . travel ( live_date ) expect ( listing . status ). to eq ( :live ) end it \"transitions to expired after the live date\" do Timecop . travel ( expiration_date ) expect ( listing . status ). to eq ( :expired ) end end These tests show us the intent and act as documentation more than our previous example. Is it better than the original version? We find these tests explain the intent better than the previous example, and they cover the same functionality, so I would say, yes, they are better. Better in the sense that Programs must be written for people to read, and only incidentally for machines to execute. Hal Abelson, Structure and Interpretation of Computer Programs", "date": "2015-05-20,"},
{"website": "Funding-Circle", "title": "JS Monthly Meetup #67", "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/10/21/js-monthly-meetup-67/", "abstract": "We were proud to host the 67th JS Monthly here at Funding Circle’s offices\nlast week! It was a great turnout and we wanted to give an insight of what went\ndown. This was the first time we had hosted JS Monthly here at Funding Circle and the\nperfect time to do so right after the summer holidays, so the turnout was great. We had talks from three presenters on very different topics, drawn from their\npersonal and team experiences. But before the talks started, we had one of our\nTechnical Leads, Igor Czerwinski, talk through some of the JavaScript\ntechnologies that we employ and how we use them. Funding Circle is a lending platform for small businesses and we are\ncontinuing to make a big impact across the UK, US, Germany and the Netherlands\nby helping contribute £6.5 billion to global GDP and creating 115,000 jobs. One\nof the biggest projects we are working on is creating a global platform for our\nborrowers using a mix of JavaScript, Clojure and Ruby. When someone joins Funding Circle as an engineer and throughout their time here,\nwe always encourage our teams to get involved in technical communities and speak\nat internal and external events. So with that being said, Andrew Collins, one of\nour software engineers and JavaScript whizz, volunteered to be the first speaker\nof the night! One of the biggest projects Andrew is working on is creating a new borrower\nexperience using React, so it was only natural that he spoke about this\nframework in a talk entitled. “React Under The Hood”. Andrew wanted to delve deeper into how React really works and how to plan for it\nas this can be hard. He spoke about some of the up and coming and currently\nunstable parts of React. Check out his full talk on YouTube Don’t worry if you missed out on this one, JS Monthly are back in our offices\nfor October and November with plenty more to learn from! If you would like to host technology community event at our awesome offices,\nplease get in touch with us. Check out our Twitter for all updates and\nmore from our Blog .", "date": "2019-10-21,"},
{"website": "Funding-Circle", "title": "A day in the life of a Funding Circle developer", "author": ["Enrique Figuerola"], "link": "https://engineering.fundingcircle.com/blog/2015/03/15/day-in-funding-circle-tech-uk/", "abstract": "I recently had the opportunity to show the world what it is like to be part\nof Funding Circle’s engineering team in the London office. Overview I’m part of the awesome Investor team at Funding Circle and we focus on\nimproving the experience for our Investors. We are a small part of a larger\nengineering team, but together, we are trying to create a better financial\nworld. People At Funding Circle I work with an amazing group of people, who provoke you to\nthink in many different ways and provide a different perspective from your own.\nThis is what makes us special and drives everybody to improve themselves\neveryday. Challenges We deal with many challenges on a daily basis. This is not always easy but it’s\nwhat makes Funding Circle an exciting place to work. Like every company we have\nour successes and failures, which we always try to learn from. We are working to\nrevolutionise the world of finance by making technology changes in favour of a\nmore open and reliable place for everyone. Technologies We use a variety of technologies in our work, such as Ruby , Rails , iOS , AngularJS , Capistrano and Chef . Agile methodologies\nsuch as TDD , BDD feature prominently in our\ndevelopment process, however we are always trying to improve as an engineering\nteam and are open to new ideas. We aim to offer the best service for all of our\nusers and constantly strive for the best. Conclusion We are changing everything … watch it yourself", "date": "2015-03-15,"},
{"website": "Funding-Circle", "title": "Shipping Software in FinTech", "author": ["Dennis Ideler & Oleksandr Kruk"], "link": "https://engineering.fundingcircle.com/blog/2016/09/06/shipping-in-fintech/", "abstract": "Funding Circle is one of Europe’s leading FinTech companies.\nOur headquarters are based in the City of London, one of the oldest financial centres in the world and still a global leader.\nNaturally there is a lot of financial regulations with which our business must comply. In this post we describe how regulation affects our agile development process, and present an open-source tool that\nhelps with some of the challenges faced. Afterwards we discuss some of the implementation details – this\nsection is more technical and is included as optional reading. Becoming a regulated environment Funding Circle has seen tremendous growth over the last year. We’ve become an\nestablished financial institution and with that comes financial regulation.\nOne of the regulatory bodies in the UK is the Financial Conduct Authority (FCA).\nWe have been operating under interim permission from the FCA since April 2014\nand continue to work closely with them to obtain a full operating license. With regulation comes process. There are a lot more checks in place now than\nwhen we were a startup, and rightfully so to protect our client money and data.\nThe increased trust that comes with such process is a win, but if the business\nisn’t careful, the tradeoff is a loss in speed from all the red tape. With our goal to become compliant, an anti-goal was slowing down. To be\nsuccessful, we have to be able to make quick decisions and deploy software multiple\ntimes a day. A non-goal was to be faster - if it happened that would be\ngreat, but we wouldn’t actively be trying to achieve that. Becoming FCA compliant affects virtually every department of our business.\nFor the tech team it meant being able to answer ‘yes’ to a very long list of\nquestions about engineering and security practices. Fortunately for us there\nwere quite a lot of practices which we already satisfied or exceeded. If we can prove\nduring an audit that we follow good practices, we can answer ‘yes’ to those.\nFor those which we cannot, we have to explain what we are doing\nto get to a ‘yes’ answer. Move fast and safe One of the sections of the compliance checklist was focused on the governance of software\ndelivery. Our approach was to build a system that would create an inspectable audit trail of\nthe software development lifecycle, highlighting practices that we follow. It would have to withstand a\nsurprise technical audit, meaning we had to create a detailed audit trail of events\nwhere we could easily view the state of the world at a specific time in history. After an inception meeting we had a quick look to see if there were any existing\ntools but couldn’t find any that met our criteria. After many discussions and diagrams in a cramped cold\nroom we dubbed “The Icebox”, we all decided on the following: Our unit of currency would be Git SHAs.\nThese are the software versions that ultimately get shipped. Track the full delivery process for all released software versions.\nFrom story inception to deployment and everything in between. Keep the tool fairly generic and not too tightly coupled to specific services.\nIn case the business decides to switch from GitHub to BitBucket for example. The tool would only observe and alert, not lock down. Though we cannot prevent\nanother process locking things down based on the information retrieved from our tool. We open sourced the tool from the beginning as it would be a lot more challenging to do after\nit’s been built. It’s called Shipment Tracker 1 and you can find it on GitHub . Use cases Shipment Tracker brings our software development processes closer to becoming fully compliant,\nthough it’s important to note that it only addresses certain needs. It’s not a\nsilver bullet – there will be other changes to your processes and tools that need\nto be made in addition to using a tool such as Shipment Tracker. Feature Reviews When a feature is ready to be reviewed by the product owner (PO), the developer presents them with a Feature\nReview. This is a page that has a checklist of various processes, such as relevant tickets and their state,\ntest results, which user acceptance testing (UAT) environment it’s been deployed to, QA approval, and so on. [fig. a] The Feature Review page as shown for a specific software version that’s under review. Some of these processes could be optional. For example, not every change may need QA to review. Each panel gets its information from events. Shipment Tracker is continuously\nreceiving and storing events from many different sources – more on this later. A Feature Review gives the change-control process more visibility. We can use it when signing off a feature to\nmake sure it’s in good state. We can also use it during an audit to see what criteria was met at various\nspecific times. Because this page is a projection of events, we can show the state of a Feature Review at any\ntime by replaying events up until a given time. If no time is specified, we show the Feature Review at its\ncurrent state by replaying all events up to the very last one. [fig. b] Feature Reviews can be viewed at specific times, down to milliseconds if needed. On every push to GitHub, Shipment Tracker creates a commit status for the associated Feature Review. [fig. c] GitHub Commit Statuses by Shipment Tracker show the state of any associated Feature Review(s). When no associated Feature Review exists, it asks you to create one and link it to at least one (JIRA) ticket.\nIf an associated Feature Review does exist for the commit, it links to it.\nOnce a Feature Review exists for the feature branch, any child commits on the same branch will have a Feature\nReview auto-created for them. A detailed description of all the Feature Review panels can be found on Shipment Tracker’s wiki . Releases Every repository tracked by Shipment Tracker has its own Releases page where you can see what’s been deployed.\nA Release is defined by its software version (Git SHA). Only commits made directly on the canonical branch 2 are considered to be Releases as that’s the source for releasable software 3 . Meaning any commits made\non feature branches will not be shown, but their merge commit will be. [fig. d] The Releases page gives a region-specific overview of a project’s deploy queue. An application can be deployed in multiple regions, with each region having a different deploy queue.\nThis is why Releases are per geography – indicated by a flag for each geography. New geographies can easily\nbe added as two-letter country codes 4 by setting an environment variable. The page is divided into two sections. Pending releases and deployed releases. Releases are by default\nconsidered to be pending. To be marked as deployed, Shipment Tracker must receive a production deploy event\nfor that software version or for one of its children. We can see if the release has been approved or not. Unapproved releases are indicated by a red row and a\nvariety of statuses, such as it being in a pre-approval state, or a code change was pushed after approval, or\nit simply lacks an associated Feature Review. [fig. e] Releases can be flagged if they haven’t gone through the proper approval process. Deployment Alerts The Shipment Tracker tool is an observer, not a gatekeeper. Sometimes things won’t follow the happy path. Unauthorised Releases will trigger a deploy alert at the time of deployment. This can be for a variety of\nreasons, such as deploying an older software version. A full list can be found on the Shipment Tracker wiki . [fig. f] Risky deploys are alerted to the business via a Slack channel. Currently such alerts go to a Slack channel, but it can easily be extended to send email alerts.\nHere the Deployer or Product Owner can justify the release or retrospectively rectify the situation. The notification links back to the Releases page. Search The Shipment Tracker landing page is the Search page. Here you can search for released features that have gone\nthrough the Shipment Tracker process. By default it shows releases for the current day. [fig. g] Tickets that have been released can be found via the Search page. The query can contain the app name or SHA of a deployed commit, or any keywords from a ticket title or\ndescription. The search criteria is weighted. For example, matches against deploy data is most relevant\nand those results will appear first. The results will show all relevant tickets with their title and snippet of their description. The bottom\nof each ticket panel shows deployment information, such as the region, time of deployment, app name, and short SHA. Implementation Tech Stack Shipment Tracker is a typical Rails application with a PostgreSQL database and background jobs.\nIt has an authenticated GUI and API. We used Auth0 for user authentication but this is\neasily configurable with environment variables. In fact, because we built Shipment Tracker as an open source\napplication from the start, it affected a lot of our design decisions. We strived to follow the guidelines of\na twelve-factor app wherever possible. A few areas where Shipment Tracker stands out from other Rails apps are\n- Interaction with Git - there’s quite a bit of this\n- PostgreSQL is used in a variety of ways, including as a NoSQL store (via JSON) and full-text search\n- Storing state as an event stream, which creates an audit trail with time travel capability Event Sourcing Shipment Tracker relies on the concept of Event Sourcing .\nIn summary, event sourcing consists of treating all the system state changes as a stream of events and\nrecording all the states changes of your application through event objects. This allows you to reconstruct\nthe state of your application at any given time by replaying all the events, assuming you have stored all the\nevents since the beginning of the application operation. Shipment Tracker stores all events in a relational database management system (RDBMS) known as PostgreSQL. [fig. h] Shipment Tracker keeps track of the software development lifecycle by receiving events from a\nvariety of external and internal sources. For brevity, not all event sources are shown here. Shipment Tracker uses the event sourcing concept for providing the traceability of all the relevant actions\ntaken during the software development and delivery process. The platform provides a webhook to which all the\nexternal services report. This webhook is the source of all the events that the system needs to create its\ncurrent state. Tracking new data is as easy as creating a new type of event with a specific endpoint. Snapshots To get the current state of the world, state has to be accumulated by applying all events, from the very\nbeginning until the specific point of interest in time. This is inefficient and doesn’t scale well.\nThe time it takes to apply events grows linearly as the number of events grows. Instead of storing events and lazily (re)applying them whenever the system has to project information,\nwe can apply events as we receive them and persist the accumulated state so it can be efficiently looked up.\nThis is called “snapshotting” and is common in systems that use event sourcing. A snapshot is a record of accumulated state from events, for a specific model. For example, events from a CI\nsource such as CircleCI, Travis, or Jenkins would be normalised into build snapshots with these attributes: 5 [fig. i] Events are normalized into persisted snapshots, which are more efficient to work with. So any information that is ultimately projected to users, is retrieved from snapshots instead of raw events. Queries – which are used in controllers – collect information to be projected in the views.\nA Query can have multiple Repositories for data retrieval.\nEvery Repository has a data store, which contains Snapshots for a specific model.\nFinally, a Snapshot normalizes a raw Event. [fig. j] Views project information from Queries. Queries use Repositories for interacting with the persistence layer.\nRepositories each have a Snapshot store. Snapshots store accumulated state of Events. Snapshotting is a continuous process. There is an infinite loop that snapshots every new event .\nTo identify new events, there is an EventCount table that keeps track of the last event applied for each\nsnapshot type. Events that have an id greater than the last event applied are considered new and are pending snapshot. [fig. k] The EventCounts table tracks the last event applied for each event repository. Sometimes we need to wipe all snapshots and recreate them. An example is adding a new attribute to a snapshot.\nWe would have to recreate all snapshots of that type so that events are re-applied and the relevant information\ncan be extracted for the new field.\nIn these cases we put the application into maintenance mode. A Capistrano task sets DATA_MAINTENANCE=true ,\nstops the background workers (so they don’t process new incoming events), then restarts the application.\nA rake task is then automatically triggered to recreate snapshots - but only up to the previous snapshot count.\nAfter resnapshotting is completed, maintenance mode is automatically turned off and any new events that were\nqueued are snapshotted as usual. Base events All events are derived from an abstract BaseEvent model and stored in the database. [fig. l] Events have a straightforward structure. Event-specific details are stored as JSON. The\nunstructured data allows for quick and easy modification of payloads from event sources. The only “custom” field is the details column which holds all event metadata. The others are common columns\nprovided by Rails. We have the typical timestamp columns which we need for event sourcing, and we also have a\ntype column that we need for Single Table Inheritance (STI). Specific type of events that are derived from the base event will have their type set. This allows us to use BaseEvent to query all events. We can also narrow down our queries by using a specific event, such as JiraEvent . Note that the details column uses a JSON data type. This makes it very easy for us to accept any type of payload\nfor event details, which is essential as we have a large amount of event sources, each being very different.\nMany of these are from third-party sources where we do not have any control over the payload. So we accept anything\nbut we only pick and choose what we need from the details payload, and that varies for different events. JIRA events Let’s look at the JIRA case. JIRA allows you to specify an endpoint for notifying on the\nspecific modifications we are interested in. In our case, we are interested in the ticket lifecycle flow, meaning\nthat whenever a ticket is moved between different states e.g: “To Do”, “In Progress”, “Ready for Deploy”,\nwe want to be notified. And the reason for this interest in a ticket’s state change is that it will allow\nus to answer questions like: “Was the code changed in context of ‘ticket A’ after the ticket was approved by PO?” “What is the current state of the ‘ticket A’? In order to explain how we answer those questions we need to introduce the Github push notifications. GitHub events GitHub, as well as JIRA, simplifies our life by allowing us to setup an endpoint to which it will send the\nevents we are interested in. For answering our questions, we are only interested in receiving a notification\nevent when a code change is pushed to a repository which we are tracking. We do not filter the destination\nof the commits, all pushes to all branches are processed.\nHow do we use the information from GitHub push events?\nFirst let’s look at some key fields in the body of the GitHub push event { repo: \"audited_repo\" , sha: \"29efedc\" , parent: \"4acde2d\" } As you probably guessed, the sha key will point to the commit’s SHA about which we are being notified\nwhile the parent key will point to the SHA of the parent of the current commit. With this information we can easily identify which repository was updated, but what about the information\nabout the branches? Well, in order to be able to quickly query the repository for relevant information,\nwe decided to maintain a copy of each repository locally on each Shipment Tracker deployed instance,\nwe’ll give more details on this later, but for now let’s assume that we have a local up-to-date copy for each\nrepository that we track. We use rugged to manage the locally cloned repositories. Rugged is a very useful\ngem (a wrapper for a C++ library libgit2) that allows us to query the repository and find out which\ncommit belongs to which branch, which commits were made between commit A and B, was commit A merged to\nthe master branch, among other important information. Now that we have an event to call us when any change is made to the code and we have an easy way to find out\nmore information about the commit by querying the local repository, we can do some interesting things.\nWhenever we receive the push event from GitHub, Shipment Tracker will check the following condition: Is there any association between the parent commit and a JIRA ticket, if so, associate that same\nticket (or tickets) to the newly pushed commit. Check the status which the associated ticket is in, and post it to GitHub as a commit status. The status can be one\nof the described in figure c . These commit statuses on GitHub are a convenient way for developers to check what status the feature related ticket is in.\nAlso it can be very useful to implement a specific “merge to master” policy, e.g.: a developer\ncan only merge the PR if the ticket in JIRA is approved (which is not the practice at Funding Circle, given\nthat we prefer to monitor and alert as opposed to restrict people’s actions and slow down the development process). How do we keep the local copies of git repositories up to date? In section GitHub events we mentioned that we keep a copy of all git repositories\nthat are under audit by Shipment Tracker. Now let’s look at how we keep the data in those copies up\nto date.\nIn order to be able to query the most recent state of the code for each local repo, we need to update them\nfrequently. We have already experimented two different solutions: Fetch the repo updates whenever there is a web request that will require querying the repository Run a background job which compares local repo against remote every minute and in case the local repo is\noutdated, fetch the updates for that repo. Let’s look at each solution closely and analyse the pros and cons. Solution 1. is quite efficient given that\nwe are not polling the repo all the time. Also the repo is only cloned when there is a clear need to query it.\nThe problem starts manifesting when we deal with larger repos which take some time to update or when we have\nto clone the repo for the first access. The size of the repo impacts quite significantly the response time of\nthe web requests which is quite frustrating for the users :(. After having this solution for a couple of\nmonths, we’ve decided that it the impact on the user experience is too big of an impact on the user experience and started developing a better\nversion which resulted in solution 2) mentioned above. We are still evaluating this second solution and we have already identified a couple of advantages and\ndisadvantages. This solution provides a much better user experience in most cases by keeping all the\nrepositories up to date in the background. All the repositories are scanned in an infinite loop. It provides quite a fast way of keeping all the repos up to date in most cases but also has an associated cost of the tireless background worker.\nOne of the problems that we’ve foreseen is scaling the number of tracked repositories. Imagine we have a thousand repositories to keep track of. Assuming that in a given update loop we have 300 outdated repositories. If we assume an update operation takes in average 2 seconds, then 2x300 will\ngive us a delay of 600s. or 6 minutes for the last repository to be updated. This can be acceptable for small\ncompanies which are less likely to reach those numbers, but when you have dozens of teams working in parallel\non a couple of repositories each, you might reach the acceptable limit quite easily. Luckily this is not the\nworst problem to have since it can be solved with threading. By using only six threads we can reduce the\nlongest outage to one minute, which looks much more acceptable, and if it’s not, we can always use more\nthreads. In fact, the most reasonable case for this problem is probably to define the maximum outage time\nand calculate the number of threads needed to satisfy that criteria. Let’s look at an example, considering\nthe exact same number of outdated repositories, 300, and an average time of 2s. per repo update, assuming we\nwant a maximum outage time of 10 seconds, the number of threads needed to ensure this would be obtained by\nthreads = 300/(10/2) = 60. Sixty is not a low number by any means, but it’s steel manageable. In a real world\nwe would of course define a maximum threshold for the number of threads and establish the maximum limit of\nrepos for which we can guarantee a maximum outage of 10s. Circle CI events In case of CircleCI we receive notification events whenever a build finishes.\nThese events allow us to determine if the build for a given commit was successful or not.\nThis information can be seen in the \"Test Results” section of the Feature Review page in the picture above. This information is relevant for the Product Owner and QA, it is a quick way of confirming that the\nimplementation for this specific feature is building correctly and passes all tests.\nAnother use case for this information, which is planned for future is to allow automated deploy once the build goes green.\nA CircleCI build would package the application version and store it in a convenient place, than with a single\nclick of a button a deploy of this version could be done. Deploy events Deploy events can come from any machine or service responsible for a deploy. In figure h the source is Jenkins\nCI, but it could also come from CircleCI, Heroku or Capistrano for example. Depending on the source, the payload can be different. A typical deploy could have the following fields: { app_name: \"my_app\" , version: \"sha\" , deployed_by: \"someone\" , locale: \"gb\" , environment: \"staging\" , servers: [ \"a.example.com\" , \"b.example.com\" ], } With that information Shipment Tracker can link a deploy to a specific software version for audited projects. Search The landing page of Shipment Tracker is a search page for Released Tickets, as seen in figure g .\nFor this we use PostgreSQL’s full text search instead of a dedicated full-text search tool such as Elasticsearch. Postgres’s text search system preprocesses documents 6 by reducing them to the tsvector format –\na compact representation of the full document. The tsvector data type stores lexemes (key words) with\nassociated rankings. It’s like a hash map specifically for weighted search. Here’s an example of a tsvector for a document: \"'accept':9B,16B,35 'api':6B,28,54 'borrow':8B 'creat':4B 'fulli':13B 'fund':14B 'goal':18 'integr':46 'loan':11B,36 'make':49 'new':53 'one':25 'place':41 'result':32 'stori':21 'test':5B,47 'updat':44\" Some of the words look weird because they’re stemmed. Postgres uses a dictionary to eliminate common words\nthat should not be considered in a search, and to stem words so that different derived forms of the same word\nwill match. For example, “jumping” and “jumped” would be stored as a lexeme like “jump”. Searching and ranking are performed entirely on the tsvector — the original text only needs to be retrieved\nwhen the document has been selected for display to a user. Search results are ranked. Certain parts of a document (e.g. titles) can be giving higher relevance by using\nweights, so that when there are any matches, the most relevant results can be shown first. Consider the following table. CREATE TABLE released_tickets ( id integer NOT NULL , key character varying , title character varying , description text , tsv tsvector , ); Full text search can be slow. It’s recommended to index the tsvector column(s) to accelerate search. CREATE INDEX index_released_tickets_on_tsv ON released_tickets USING gin ( tsv ); We use the pg_search gem to extend ActiveRecord (the default ORM\nfor Rails). It provides ActiveRecord callbacks to update the tsv indexes, but some ActiveRecord operations\nskip callbacks, so it’s much safer to have a trigger defined on the database. This trigger will set the weights for records that have been touched and index them. CREATE FUNCTION released_tickets_trigger () RETURNS TRIGGER AS $$ BEGIN new . tsv : = setweight ( to_tsvector ( coalesce ( new . title , '' )), 'A' ) || setweight ( to_tsvector ( coalesce ( new . description , '' )), 'B' ); RETURN new ; END $$ LANGUAGE plpgsql ; CREATE TRIGGER released_tickets_tsv_update BEFORE INSERT OR UPDATE ON released_tickets FOR EACH ROW EXECUTE PROCEDURE released_tickets_trigger (); Data analysis A pleasant side effect which we hadn’t thought about is that now we are collecting a lot of information regarding our development process and we can answer interesting questions about it. Some of the most interesting questions are: How many times we deploy in a day? How long, on average, does it take from development start until the deploy of a feature? How many unauthorised deploys we have per month? This information is extremely valuable when you want to improve/speed-up the process because it allows you to see where the bottlenecks are and focus immediately on the pain points without loosing too much time on investigation. Testing Shipment Tracker has various layers of testing. Unit tests Most of the tests are unit tests, using the RSpec testing framework for Ruby.\nIn many cases we stub external dependencies of the system under test so the unit is tested in isolation. Acceptance tests Our acceptance tests are integration tests. Here we almost never stub, except for cases such as communication\nwith a third-party service. These tests use Cucumber (for the Gherkin syntax), Capybara (for simulating\nuser interaction), and RSpec (for test assertions). Our acceptance tests are usually written in a BDD style,\nfocusing on the business value from a customer’s perspective. The narrative follows the style of user-story.\nFor example: Feature : Managing Repository Locations\n  As an application onboarder\n  I want to add a repository from GitHub\n  Because I want an audit trail of the application's development Scenario : Add repositories Given I am on the new repository location form When I enter a valid uri \"ssh : //github.com/new_app\" Then I should see the repository locations : | Name | URI | | new_app | ssh://github.com/new_app | From a development perspective, the outside-in approach let us focus on getting the feature working first,\nand then making it better while being confident that it doesn’t break. This prevents us from being bogged down\nin design details. Because acceptance tests are expensive, we don’t use them to test exhaustively, instead\nonly testing the business critical paths which are usually happy paths. Linters After unit tests and acceptance tests are run, we run a Ruby style linter known as RuboCop .\nThese run after because the changes are usually cosmetic, and we prefer to see that the code behaves as\nspecified before making cosmetic changes. On projects with a slower test suite, you’ll usually see such linters\nrunning first. Performance tests We introduced performance tests to benchmark projections before and after snapshotting.\nThey were used to justify implementing snapshots, as event sourcing wasn’t scalable without it - page\nrequests would very quickly start to timeout. These tests do not run by default as they considerably slow down the test suite.\nThe results can be seen in these slides . Testing with Git Testing with Git can be difficult. Git repositories need to be built up and teared down on the spot, while\nkeeping the tests easy to understand. To make it easier, we introduced some helpers. The interface for interacting with a test Git repository is the GitTestRepository class .\nThis class helps us create basic test repositories and handles all the common operations for us, such as\ncreating commits, and creating and merging branches. To reference specific commits in our tests without\nknowing what their SHAs will be beforehand, we use “ pretend commits ” which is a hash that links a\npredetermined human-readable key such as “#abc” to an actual commit. Tests that required git quickly became very verbose and difficult to read and write. So we introduced a RepositoryBuilder class that would build GitTestRepository s for us based on an ASCII\ndiagram of a Git tree . o-A-B---\n  /        \\\n-o-------o--C---o Relevant commits would be named with a letter. Other commits would simply be an ‘o’.\nThe diagrams should be read from left to right.\nHere is an example of a test that requires git interaction . Conclusion Overall the introduction of Shipment Tracker was a positive experience and it has been successfully running for almost a year.\nWe are really happy for being able to carry on our development with agility while being regulated,\ninstead of setting up blockers which would frustrate the development team and slow down the company. There are some enhancements that have to be done in the area of usability so that we reduce the distraction time to a minimum.\nFor example a typo fix to the README is not a change that affects production. Maybe there could be a commit message keyword to skip some of the strict checks.\nThere are also some lacking features, such as alerting on out-of-hour deploys or unlinking tickets from a Feature Review. As a nice to have, it would be also interesting to create a statistics page,\nwhich would represent in charts the answers to the questions mentioned in Data analysis section. We hope this project might be of use to others and would love to see some contributions given its open-source nature.\nDon’t hesitate to contact us if you are interested in exploring more of Shipment Tracker. The name “Snowden” won an internal poll but was rejected due to its controversial meaning. ↩ In most cases the canonical branch will be “origin/master”. ↩ Container-based deployment using a tool such as Docker are becoming increasingly popular.\nIn such cases we recommend setting image tags as the commit SHA.\nThat ensures the software version can be easily found when sending a deploy event. ↩ Country codes as defined in ISO 3166-1 alpha-2 . For example, ‘GB’ for Great Britain and ‘US’ for the United States of America. ↩ Notice the event_created_at field which each snapshot has. It allows the system to easily move across history at specific points in time. For example, we may want to show that a build for a software version was failing at the time of ticket approval, but a later rebuild was passing. Damn flakey tests. ↩ For text search purposes, a document is the unit of searching. In other words, the text to be searched. ↩", "date": "2016-09-6,"},
{"website": "Funding-Circle", "title": "debugging-kafka-connect-sinks", "author": [""], "link": "https://engineering.fundingcircle.com/blog/2018/01/26/debugging-kafka-connect-sinks/", "abstract": "Debugging Kafka Connect Sinks When we had to scale into billions of dollars of originations and multiple currencies at the same time here at Funding Circle, we chose Kafka for our new global marketplace lending platform. We still have a legacy stack, though, with a lot of important data. We’ve been using Kafka Connect to help legacy apps access new data and new apps access legacy data. Kafka Connect allows streaming of data between Kafka producers and consumers and databases such as PostgreSQL. A Kafka Connect job that takes data from a PostgreSQL database and publishes it to a topic is a “source connector”, and a job that creates a PostgreSQL database out of messages on a topic is a “sink connector”. Kafka Connect has thoughtful configuration options, is quick to set up and–when it works–requires little to no maintenance. Kafka Connect is maintained by Confluent and updated regularly. This information is current as of version 3.1.1. Set your sink up for success Consume the topic you want to direct into your sink. Make sure you know the forms the data take. Knowing which key.converter and value.converter you need to use will save you a lot of time and hassles. Keep in mind that Kafka Connect does not currently support many data types (e.g. UUIDs) as keys in sinks, so a string converter is a safe bet. Think about your table name. If your-topic has hyphens in the name, your table will too. “table.name.format: your_topic” will override this. “auto.evolve” is an option for sinks; it will add new columns when new data appear in the schemas and is a good one if you envision your schema changing. Unfortunately, some of the error messages we see when the sink connector doesn’t work can be unclear. Here are a few problems that we’ve run into and their error messages: The general error that you’ll get when a sink fails: Exiting WorkerSinkTask due to unrecoverable exception If your topic is a JSON topic, some not-documented pitfalls are:\nKafka Connect cannot set primary keys with JSON topics (“pk.mode”, etc.). It also can’t do “insert.mode: upsert” for this reason.\nKafka Connect cannot whitelist fields (“fields.whitelist”) with JSON topics\nIn general, the JDBC sink connector does not currently support the level of table customization for JSON topics that it does for Avro topics. Kafka expects Avro but gets data that isn’t Avro org.apache.kafka.common.errors.SerializationException: Unknown magic byte! You’ll want to check the following settings in your config to see if one or both are expecting Avro data: value.converter key.converter Consume the topics with keys and take a look at them. Sometimes keys aren’t Avro and require a different converter (in our systems this is a string converter). Sometimes there may be a bad message published to the topic, so while the rest of the messages are ok, the one bad message crashes the job. Kafka expects JSON in a certain format JsonConverter with schemas.enable requires \"schema\" and \"payload\" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration. Consume the topic with keys and check the format. Kafka is expecting something like the following: {“schema”: {“value”:“type”} “payload”: {“key”:“value”}}. If it has encountered a message without this format the job will crash. Kafka encounters bad data org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:\nUnrecognized token 'unexpected-data-here-usually-something-weird': was expecting ('true', 'false' or 'null') You have a message that contains unexpected-data-here-usually-something-weird somewhere in your pipeline and the sink doesn’t know what to do with it. If you grep for the unexpected data you can find out what the bad message was and use that to figure out which process generated it. There are a lot more errors that you may encounter but they’re usually pretty self-explanatory. Hope this helps!", "date": "2018-01-26,"},
{"website": "Funding-Circle", "title": "Getting to know our Engineers at Funding Circle: Q&A with Catalina Tudor", "author": ["Alexandra Staniland "], "link": "https://engineering.fundingcircle.com/blog/2019/11/05/fc-engineers-catalina-qa/", "abstract": "This is part of a regular series where we catch up with our engineers to discover more about what\nlife is like in the Engineering team here at Funding Circle! What is your role & team? I am a software engineer working alongside the Growth Engineering team (Silverhawks) here at Funding Circle. Our team is looking into improving the borrower experience through identifying different pain points on the user journey and focussing on making their interaction with the website as straightforward as possible. How long have you been with Funding Circle? I’ve been with Funding Circle for almost 3 years now. It’s been an interesting journey so far and it definitely feels much shorter. Why did you choose Funding Circle for your next role? I think it was quite a lucky coincidence. I was attending my first front-end related meetup in London, it was organised by two FC employees and it was hosted in our office. That’s when I found out that FC just opened a position for a Junior Engineer. I was at a point in my career when I had reached the peak of my potential in the role and was actually looking for a change to explore something new. My previous job was in eCommerce so even though it was a great opportunity to start my career and exercise my knowledge it didn’t offer many opportunities in terms of career progression and personal growth. So as you can imagine, I’ve immediately taken the opportunity and applied with Funding Circle; almost 3 years later here I am with still plenty of exciting things to explore alongside great people willing to share their knowledge. How do you start your day? It definitely has to start with a coffee. The team has daily stand-up meetings every morning and discusses the work we will be doing that day, along with any possible issues we may encounter, so I try to wrap up everything that might carry over from the previous day and prepare my update. What do you enjoy most about Funding Circle? I would have to say the people. Everyone is so knowledgeable and passionate about what they do and it’s contagious. Another great thing about FC Engineering is that everyone is willing to share their knowledge and learnings with a smile on their faces. I’ve learned so much since I’ve joined and it’s predominantly because of this. What are the main challenges you face? Because the objective of our team is ultimately improving the user experience up the end of the borrower journey by A/B testing small changes, we always have to make sure that we’re not deteriorating the current experience. That sometimes poses challenges in terms of coming up with original, creative ideas that will improve conversion and deliver the best user experience. In the end we’re focused on delivering simplicity, usability and great performance so putting ourselves into our borrower’s shoes makes a real difference. Talk though some of the projects you are working on? At the moment our main focus is to finish the work required for delivering our main objective for this quarter. The main purpose of this objective is to simplify the user experience on the details page of the application by pre populating fields with borrower information we extract from Company House. This objective was quite complex but it allowed us to explore a lot and challenged our flexibility as a team.\nBecause it’s the end of the quarter, we’re also quite busy concentrating on brainstorming     ideas for the next quarter’s objectives. What advice would you give someone interviewing with Funding Circle engineering Teams? I will answer this question from the perspective of my experience here but I think it’s still valid for the entire engineering team. One important aspect of our day to day work in FC Engineering is pairing; that involves being able to collaborate with your peers and explain your train of thought while implementing your ideas. During interviews, I think problem solving and being able to explain your approach will definitely weigh more than remembering every tiny syntax detail. Everyone is so friendly and approachable, the atmosphere is relaxed so don’t be afraid to ask questions and just try to be yourself! Thank you for reading! Make sure to follow us on Twitter and Medium .", "date": "2019-11-5,"},
{"website": "Funding-Circle", "title": "Our Values - Learning", "author": ["Jack Zhou"], "link": "https://engineering.fundingcircle.com/blog/2015/01/26/our-values-learning/", "abstract": "“Tell me and I forget, teach me and I may remember, involve me and I learn.” - Ben Franklin As a junior developer on a team full of senior engineers, I greatly value the ability to learn\nduring work. Here at Funding Circle, we pair program by default, and it has been a great tool to\nhelp me learn as quickly as possible. Pairing helps us spread knowledge of our codebase around\nto every member of the team, but more importantly it exposes everyone to different ways of thinking\nand doing things. For example, I’ve learned to use the new RSpec 3 syntax, become better at Vim, and\nlearned the painful yet sometimes rewarding process of maintaining Chef cookbooks through pairing. Due to the nature of pairing, we greatly value open-mindedness. We hand out a coding sample during\nour interview process. During the review, we typically suggest a brand new way of completing the\nchallenge, and see how enthusiastically the candidate digs into this new method. To a certain\nextent, a willingness to self-improve and learn new things is just as important as years of experience\nand depth of knowledge. Some things cannot be efficiently learned through regular feature development work. To this end,\nwe’ve started with Free Code Fridays, where we can work on a project of our choosing or just spend\nthe day learning something new. Other times, we learn through experimentation, which is how we began\nusing Docker in our deployment process. Once again, pair programming was the tool we used\nto share this knowledge with the rest of the team. In the end, our pairing process and our encouragement of learning keeps our jobs interesting and\ncontinuously improves the quality of our work.", "date": "2015-01-26,"},
{"website": "Funding-Circle", "title": "CircleCI all-time build count", "author": ["Robert Crim"], "link": "https://engineering.fundingcircle.com/blog/2015/03/13/circleci-all-time-build-count/", "abstract": "We’re big fans of CircleCI . We use it to run\ntests, of course, but also to build and continuously deploy Docker\ncontainers for our apps, the static content on fundingcircle.com , and this blog. I was asked to put together some figures (for fun) to use on a new\ncareers page. Taking a loose interpretation of fun , I wondered how\nmany builds we’ve ever run on CircleCI. Luckily, it was pretty easy to\nfind out using their API and a REPL I\nhad handy. Interestingly, without specifying Accept: application/json , it\nlooked like I was getting a Clojure data structure in the\nreponse. Telling clj-http to coerce the body :as :clojure didn’t work, so I left it at that. Long story short – it turns out we’ve done 16,721 builds across all\nour projects!", "date": "2015-03-13,"},
{"website": "Funding-Circle", "title": "Golden ratio grid system in Sass", "author": ["Razvan Spatariu"], "link": "https://engineering.fundingcircle.com/blog/2015/06/08/golden-ratio-grid-system-in-sass/", "abstract": "One of the most exciting and fun features that we implemented for our styling library was the grid system. Our design team came up with the idea of using a golden ratio grid system, where they provide the proportions for the columns and we transform them into HTML/CSS columns. What is the golden ratio? For those who don’t know what the golden ratio is, the definition given by LiveScience is: The Golden ratio is a special number found by dividing a line into two parts so that the longer part divided by the smaller part is also equal to the whole length divided by the longer part. The result of that is a constant (represented by the greek letter phi — φ): where φ = a+b/a = a/b = 1.6180… The Greek mathematicians started investigating what we know today as the golden ratio about 2,400 years ago due to its frequent appearances in geometry. Since then, this magic number has fascinated many great minds throughout history (Euclid, Pythagoras, Fibonacci, etc). One of the most interesting discoveries about the golden ratio is the fact that it can be found in the most diversified and surprising domains of the nature, beginning with the simple flower petals, shells, up to the human body and… galaxies. You can read more about the occurences of the golden ratio in nature by clicking this link Although there isn’t a definitive proof that this ratio is perceived as “natural” by our brains, the usage of golden ratio emerged in various fields like: architecture art and even in music . Lately, we have started seeing this “magic ratio” emerging more and more in UI design, the most common use being the golden ratio font sizes , but also as a grid system, like the one we have built for Funding Circle’s website. Golden ratio grid system Requirements Before implementing the new styling library we used a 3rd party solution for the grid system which served the purpose but also had some drawbacks: you had to use an admin panel to generate the CSS/Sass so it interrupted the workflow it exported a lot of CSS — every usage resulted in repeated code it was class-name dependent Therefore, we wanted to build a custom grid system for Funding Circle which would be: easy to use optimised for our problem applicable to a parent element, so that the columns do not require specific classes easy to maintain or change into a different grid system in the future Geometry and formula So, before diving into writing the Sass for this, we first needed to understand the math/geometry behind this. This is an example of a golden spiral: If we stretch that spiral into a line, the result can be translated into our grid system: And the math would be: c 1 / c 0 = φ\nc 2 / c 1 = φ\n……………………………\nc n / c n-1 = φ By using the pattern above and always replacing the c n with c n-1 × φ , we end up with the following formula: c n = c 0 × φ n , where:\n  c = column,\n  n = column proportion relative to the first column\n  φ = golden ratio (1,618) Since we don’t have a defining scale for this and because everything will be turned into percentages anyway, we can arbitrarily set c 0 to a convenient value, like 1 , so we can simplify the formula further: c n = φ n Design files / Practical example The design team provides the column widths by giving us proportions that we then transform into percentages and apply to the columns. Example: We know that the sum of the two columns must be 100% (the full width of the container). c 2 + c 1 = 100\nc 2 = φ 2 c 1 = φ 1 (keep in mind that the subscript indicates the proportion, not the column number) Using a cross-multiplication , we can say that: c 2 + c 1 ……… 100%\nc 2 ………………………… x x = c 2 × 100 / c 2 + c 1 = φ 2 × 100 / φ 2 + φ 1 = 1.6180 2 × 100 / 1.6180 2 + 1.6181\n  = 61.8029% So, in this case the width for the column with the proportion 2 is 61.8029%. As you can see, the width of one column is influenced by the others, so a column proportion will have different widths depending on the siblings’ proportions. Sass implementation After figuring out the math behind this, we can finally get to the implementation of the grid system. Step 1 Calculate c n , where n = [1, 7] (the design team decided that the largest proportion for a column can be 7) We store all the values of cn in an array that we can later access when we need those values. // Calculate the general aspect ratio starting from an arbitrary number (1)\n$ratio: 1.618;\n$colRatio: 1;\n\n@for $i from 1 through 7 {\n  $colListLast: nth($colRatio, length($colRatio));\n  $colRatio: append($colRatio, $colListLast * $ratio);\n} Step 2 We construct a function that can take the proportions for the columns as an array and returns another array with the percentages for each column, in the same order. Example: // input\n@warn colsWidth(3 6 1);\n\n// output\n(17.80062 75.39987 6.79952) /* their sum is 100 */ Sass function: @function colsWidth($cols: false) {\n  $colsWidth: ();\n\n  @if type-of($cols) == list {\n\n    // read the list and calculate the percentages\n    // retrieve the ratio corresponding to each column and add them together\n    // this will give the third value (customized total) before applying the cross-multiplication to each value\n    $colSum: 0;\n    @each $colVal in $cols {\n      $colSum: $colSum + nth($colRatio, $colVal);\n    }\n\n    // apply a cross-multiplication to find out the width for each column\n    // so far we have:\n    // - we know that all columns added together are 100%\n    // - the corresponding value for 100% is $colSum\n    // - we have the proportion for each column width\n    // we only need to apply: columns width = proportion * 100 / $colSum\n    $colsWidth: ();\n    @each $colVal in $cols {\n      $colsWidth: append($colsWidth, nth($colRatio, $colVal) * 100 / $colSum);\n    }\n  } @else {\n    $colsWidth: false;\n  }\n\n  @return $colsWidth;\n} Step 3 After dealing with the math, all we need to do is to use this in a mixin. The mixin will be applied to the wrapper of the columns and will return the width for each column using nth-child (we polyfilled this for IE8). For the columns, we decided not to use the classical float (dealing with a lot of math can lead to trouble when rounding the widths) or flex (because of it’s weak browser support ). Instead, we went for a more robust way of doing things although avoided by so many because of it’s quirky past: table layout . Before saying “Hey, hey, hey, tables, really?”, I should add that we don’t use the HTML <table> element, we just set display: table; to the wrapper and display: table-cell; to the siblings. The pros of using this over float being: you don’t get the bug where one columns falls below the others if their width exceeds 100% you can use vertical-align you can create equal columns without actually knowing the number of columns (by using the width: 1% hack on the columns) That being said, we add the display: table on the wrapper and then we iterate through the result of the array outputed by the function in step 2 and apply those values to the corresponding direct children. @mixin columnWrapper($cols: false) {\n\n// table general stuff\nwidth: 100%;\ndisplay: table;\n\n// calculate the percentages\n$cols: colsWidth($cols);\n\n  @for $i from 1 through length($cols) {\n    & *:nth-child(#{$i}) {\n      display: table-cell;\n      width: nth($cols, $i) * 1%;\n    }\n  }\n} We can then use this mixin like this: .wrapper {\n  @include columnWrapper(5 2 4);\n} and the output will be: .wrapper { display: table; }\n\n.wrapper > * { display: table-cell; }\n\n.wrapper > *:nth-child(1) { width: 53.93366%; }\n.wrapper > *:nth-child(2) { width: 12.73281%; }\n.wrapper > *:nth-child(3) { width: 33.33353%; } You can check the column mixin in action in this CodePen . Production The full mixin for our grid system is far more complex than just calculating the width of the columns. It also has support for: Responsive - between certain breakpoints it can: reset the table layout to a full-width block layout hide the columns Rows — We can introduce another level of HTML and have support for display: table-row Vertical-align Custom paddings for the columns If you want to check the full code that we use, you can find it in our styling library .", "date": "2015-06-8,"},
{"website": "Funding-Circle", "title": "Ten Things I Think I Think After AWS re:Invent 2015", "author": ["Jason Michael"], "link": "https://engineering.fundingcircle.com/blog/2015/10/12/ten-things-i-think-i-think/", "abstract": "I recently got to attend the annual AWS re:Invent conference .  It was a bit like drinking from a firehose at times, but I’ve tried to distill my thoughts down into a top ten list of takeaways. First Things First Man, what a great conference.  The folks who made it all work did a great job — the sessions were super informative and interesting, the speakers excellent, and all the logistics of herding 20,000 attendees from room to room and meal to meal went off without a hitch.  Impressive!  With that out of the way, on to the list! The List I noted lots of agreement around separating storage from compute.  This was the most prominent, recurring theme of the sessions I went to.  Most places seem to be doing this by dumping their raw events into S3 and leveraging different packages for compute (Presto, Hive, etc.). EMR is a really flexible compute platform, and with EMRFS, can use S3 objects directly and transparently in MR jobs.  In bog-standard Hadoop clusters using HDFS, when you need more HDFS, you need to add more compute nodes to the cluster, even if you don’t need the additional compute.  I’ve run into this situation at a previous copmany, and then we joked “Hey, we’re getting the compute for free!”  That’s not really true, of course; having to provision nodes that handle both storage and compute means you can’t use specialized nodes. Companies are starting to come around on schemas and the need to manage them, vis-a-vis their big data operations.  Sonos, for example, is going to take a look at Confluent Platform ‘s Schema Registry.  FINRA has opted to run a single hive metastore in RDS and then connect multiple EMR clusters to it.  This gives them a single source for schemas, as well as saving them 5-7 minutes of compute time when they bring up a new cluster (no need to populate millions of partitions in the HMS). Make the transient transient, the long running durable.  This goes to the use of S3 as primary storage layer.  If you only need to run batch EMR jobs, then you can use the Spot market to spin up a cluster when needed and release it when done for very dollar-efficient compute. There’s an absolute explosion in the types and number of tools available for doing analysis — Spark Streaming , Presto , Drill , Hive , SQL via Redshift , etc.  I saw a lot more Hive than I was expecting, given the age of the tool and its performance, but the refrain I kept hearing was that companies trust Hive for their production workloads, whereas some of the next generation tools are still a little too immature.  There’s a lot of work going in to proving them out, though, and I think the trend is to move off of Hive onto these new tools. There’s also an explosion of file/data formats, with different pros and cons. Avro has wide adoption, which is good to see, as it validates our decision to use it for our event schemas.  People like that it contains the write schema, but there is a lot of interest in columnar formats as well like Parquet and ORC . With storage being so cheap, it seems like there is an opportunity to fit the storage format to the tool and just duplicate the data.  So your “data lake” could be in S3, in its native format (Avro, etc.), and would be the ultimate source of truth.  ETL jobs could then process the data into multiple formats for the different tools. Kafka is generating lots of interest, and many companies are wrestling with the Kafka vs. Kinesis question.  Obviously, Kinesis is going to come in for a lot of praise at an AWS conference, but it seemed like Kafka was more adopted.  It will be interesting to watch the evolution of the two tools.  Kinesis’ magic is the tight integration with the AWS ecosystem, and the fact that it is cloud-native (a speaker from Netflix mentioned frustration around trying to work with Kafka in an autoscaling environment where you expect the instances to come and go with some regularity).  The new Kinesis Firehose functionality is really intriguing, and it is tools and features like that which could tip the balance for a lot comapnies, especially if they are going with an “AWS first” mentality.  Kafka, on the other hand, is seen as more mature and more developer-friendly. My current favorite Apache project, Samza , didn’t get mentioned much, but Netflix at least is doing a lot of work with it, submitting patches and fixing bugs.  One of their points of focus has been to get Samza to run outside of YARN, or any sort of resource management layer, really.  They run their Samza jobs as Docker containers, which is exciting as that’s how we’d like to run ours! Docker has basically won the day.  Literally everybody mentioned using it in some way within their organization, even on production.  It’s astonishing to me how quickly the technology has matured, as well as how quickly consensus has formed around it.  Pretty cool! Are you interested in working with tools in the AWS ecosystem?  Come join us !", "date": "2015-10-12,"}
]